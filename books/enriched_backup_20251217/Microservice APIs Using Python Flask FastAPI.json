{
  "metadata": {
    "title": "Microservice APIs Using Python Flask FastAPI",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 442,
    "conversion_date": "2025-11-08T12:43:17.826515",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "microservice_apis_using_python_flask_fastapi_open.pdf"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "chapter_number": 1,
      "summary": "This chapter covers segment 1 (pages 1-8). Key topics include apis, orders. REST APIs leverage the semantics of HTTP methods to indicate \nactions (such as POST to create resources), and they use HTTP status codes that signal the result of processing \nthe request (such as 200 for successful responses).",
      "keywords": [
        "API",
        "payload",
        "orders",
        "Manning Publications",
        "REST APIs",
        "Status code",
        "APIs",
        "API server",
        "status",
        "API client",
        "API documentation",
        "Microservice APIs",
        "code",
        "endpoints",
        "POST"
      ],
      "concepts": [
        "api",
        "apis",
        "orders",
        "ordering",
        "editor",
        "payload",
        "developers",
        "document",
        "design",
        "designations"
      ],
      "similar_chapters": [
        {
          "book": "Microservices Up and Running",
          "chapter": 7,
          "title": "Segment 7 (pages 68-75)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 34,
          "title": "Segment 34 (pages 274-281)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 8,
          "title": "Segment 8 (pages 59-69)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 6,
          "title": "Segment 6 (pages 59-67)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 4,
          "title": "Segment 4 (pages 25-32)",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "detection_method": "topic_boundary",
      "chapter_number": 2,
      "summary": "This chapter covers segment 2 (pages 9-16). Key topics include apis, implementing, and principles. 77\n■Using HTTP status codes to \nreport client errors in the request\n78\n■Using HTTP status codes to \nreport errors in the server\n82\n4.7\nDesigning API payloads\n83\nWhat are HTTP payloads, and when do we use them.",
      "keywords": [
        "BUILDING REST APIS",
        "API",
        "REST APIS",
        "APIS",
        "MICROSERVICE APIS",
        "GRAPHQL APIS",
        "BUILDING GRAPHQL APIS",
        "Testing REST APIs",
        "DEPLOYING MICROSERVICE APIS",
        "INTRODUCING MICROSERVICE APIS",
        "REST API",
        "Documenting REST APIs",
        "Designing GraphQL APIs",
        "REST API design",
        "API testing"
      ],
      "concepts": [
        "apis",
        "implementing",
        "principles",
        "contents",
        "microservice",
        "queries",
        "introducing",
        "understanding",
        "design",
        "validating"
      ],
      "similar_chapters": [
        {
          "book": "Microservices Up and Running",
          "chapter": 7,
          "title": "Segment 7 (pages 68-75)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 8,
          "title": "Segment 8 (pages 59-69)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 33,
          "title": "Segment 33 (pages 266-273)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 11,
          "title": "Segment 11 (pages 82-89)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 4,
          "title": "Segment 4 (pages 25-32)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 17-24)",
      "start_page": 17,
      "end_page": 24,
      "detection_method": "topic_boundary",
      "chapter_number": 3,
      "summary": "This chapter covers segment 3 (pages 17-24). Key topics include apis, chapters.",
      "keywords": [
        "APIs",
        "microservice APIs",
        "book",
        "microservices",
        "build REST APIs",
        "API",
        "REST APIs",
        "’ll learn",
        "Dockerizing microservice APIs",
        "explains",
        "GraphQL APIs",
        "learn",
        "’ll",
        "Kubernetes",
        "Deploying microservice APIs"
      ],
      "concepts": [
        "apis",
        "api",
        "chapters",
        "microservices",
        "books",
        "builds",
        "works",
        "security",
        "secure",
        "design"
      ],
      "similar_chapters": [
        {
          "book": "Microservices Up and Running",
          "chapter": 7,
          "title": "Segment 7 (pages 68-75)",
          "relevance_score": 0.76,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 31,
          "title": "Segment 31 (pages 305-313)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 14,
          "title": "Segment 14 (pages 106-114)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.7,
          "method": "api"
        }
      ]
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 25-34)",
      "start_page": 25,
      "end_page": 34,
      "detection_method": "topic_boundary",
      "chapter_number": 4,
      "summary": "This chapter covers segment 4 (pages 25-34). Key topics include microservice, code. In both cases, source code is formatted in a fixed-width font\nlike this to separate it from ordinary text.",
      "keywords": [
        "Microservices",
        "Microservice APIs",
        "code",
        "APIs",
        "BOOK",
        "service",
        "microservices architecture",
        "API",
        "source code",
        "build",
        "code base",
        "application",
        "’ll",
        "orders service",
        "Definition"
      ],
      "concepts": [
        "microservice",
        "api",
        "code",
        "coding",
        "services",
        "definition",
        "definitions",
        "applications",
        "application",
        "book"
      ],
      "similar_chapters": [
        {
          "book": "Microservices Up and Running",
          "chapter": 7,
          "title": "Segment 7 (pages 68-75)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 4,
          "title": "Segment 4 (pages 25-32)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.66,
          "method": "api"
        }
      ]
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 35-42)",
      "start_page": 35,
      "end_page": 42,
      "detection_method": "topic_boundary",
      "chapter_number": 5,
      "summary": "As we will see in section 1.3,\nmicroservices also have limitations and bring challenges of their own Key topics include apis, service. As a matter of fact, they can be written in completely different program-\nming languages (that does not mean they should!).",
      "keywords": [
        "API",
        "microservices",
        "microservice APIs",
        "APIs",
        "Service",
        "web APIs",
        "web API",
        "orders service",
        "order",
        "API documentation",
        "microservices architecture",
        "application",
        "web API interfaces",
        "Microservices integration tests",
        "API client"
      ],
      "concepts": [
        "api",
        "apis",
        "service",
        "applications",
        "interfaces",
        "different",
        "difference",
        "architecture",
        "architectural",
        "error"
      ],
      "similar_chapters": [
        {
          "book": "Microservice Architecture",
          "chapter": 4,
          "title": "Segment 4 (pages 25-32)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 7,
          "title": "Segment 7 (pages 68-75)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 15,
          "title": "Segment 15 (pages 115-122)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 43-51)",
      "start_page": 43,
      "end_page": 51,
      "detection_method": "topic_boundary",
      "chapter_number": 6,
      "summary": "This chapter covers segment 6 (pages 43-51). Key topics include apis, develop, and implementation. As you’ll learn in chapter 3, every\nservice owns its own database, which means they also require multiple database setups\nwith all the features needed to operate at scale.",
      "keywords": [
        "API",
        "orders API",
        "API documentation",
        "API layer",
        "APIs",
        "orders API specification",
        "REST APIs",
        "API specification",
        "API server",
        "layer",
        "orders",
        "API client",
        "API design",
        "microservice APIs",
        "API implementation"
      ],
      "concepts": [
        "apis",
        "develop",
        "implementation",
        "implement",
        "service",
        "layer",
        "data",
        "application",
        "applications",
        "order"
      ],
      "similar_chapters": [
        {
          "book": "Microservice Architecture",
          "chapter": 13,
          "title": "Segment 13 (pages 98-105)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 7,
          "title": "Segment 7 (pages 68-75)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 4,
          "title": "Segment 4 (pages 25-32)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 17,
          "title": "Segment 17 (pages 131-142)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 14,
          "title": "Segment 14 (pages 106-114)",
          "relevance_score": 0.66,
          "method": "api"
        }
      ]
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 52-59)",
      "start_page": 52,
      "end_page": 59,
      "detection_method": "topic_boundary",
      "chapter_number": 7,
      "summary": "This chapter covers segment 7 (pages 52-59). Key topics include orders, fastapi, and apis. FastAPI (https://github.com/tiangolo/fastapi) is a web API framework built on top of\nStarlette (https://github.com/encode/starlette).",
      "keywords": [
        "API",
        "order",
        "API endpoints",
        "FastAPI",
        "URL",
        "orders API",
        "HTTP request",
        "API implementation",
        "Pipenv",
        "request",
        "data",
        "object",
        "basic API",
        "endpoint",
        "URL path"
      ],
      "concepts": [
        "orders",
        "fastapi",
        "apis",
        "response",
        "endpoints",
        "listing",
        "create",
        "created",
        "data",
        "decorators"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 8,
          "title": "Segment 8 (pages 59-69)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 4,
          "title": "Segment 4 (pages 25-32)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 46,
          "title": "Segment 46 (pages 416-420)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 29,
          "title": "Segment 29 (pages 265-274)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 57,
          "title": "Segment 57 (pages 518-522)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 60-71)",
      "start_page": 60,
      "end_page": 71,
      "detection_method": "topic_boundary",
      "chapter_number": 8,
      "summary": "This chapter covers segment 8 (pages 60-71). Key topics include order, validate, and validation. Figure 2.8 illustrates\nhow the data validation flow works for CreateOrderSchema.",
      "keywords": [
        "order",
        "API",
        "Response",
        "uuid",
        "API implementation",
        "status",
        "payload",
        "pydantic",
        "List",
        "type",
        "return order",
        "size",
        "created",
        "property",
        "product"
      ],
      "concepts": [
        "order",
        "validate",
        "validation",
        "validated",
        "list",
        "type",
        "typing",
        "apis",
        "created",
        "create"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 8,
          "title": "Segment 8 (pages 59-69)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 5,
          "title": "Segment 5 (pages 33-42)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 22,
          "title": "Segment 22 (pages 195-202)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 45,
          "title": "Segment 45 (pages 408-415)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.46,
          "method": "api"
        }
      ]
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 72-80)",
      "start_page": 72,
      "end_page": 80,
      "detection_method": "topic_boundary",
      "chapter_number": 9,
      "summary": "In this chapter, you’ll learn to answer these questions and\nhow to evaluate the quality of a microservices architecture by applying a set of\ndesign principles Key topics include service, products, and production.",
      "keywords": [
        "service",
        "Products service",
        "team",
        "business",
        "Service decomposition",
        "data",
        "products",
        "orders service",
        "microservices",
        "business capability",
        "design",
        "historical data service",
        "data service",
        "ingredients service",
        "sales forecast service"
      ],
      "concepts": [
        "service",
        "products",
        "production",
        "team",
        "data",
        "business",
        "businesses",
        "microservices",
        "customers",
        "kitchen"
      ],
      "similar_chapters": [
        {
          "book": "Microservice Architecture",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 17,
          "title": "Segment 17 (pages 131-142)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 3,
          "title": "Segment 3 (pages 17-25)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 81-91)",
      "start_page": 81,
      "end_page": 91,
      "detection_method": "topic_boundary",
      "chapter_number": 10,
      "summary": "A supportive\nsubdomain represents an area of the business that is not directly related to value gener-\nation, but it is fundamental to support it Key topics include services, apis.",
      "keywords": [
        "REST APIs",
        "REST API design",
        "orders subdomain",
        "REST",
        "order",
        "subdomain",
        "REST API",
        "building REST APIs",
        "APIs",
        "customer",
        "API",
        "Service",
        "Kitchen subdomain",
        "business",
        "products subdomain"
      ],
      "concepts": [
        "services",
        "api",
        "apis",
        "steps",
        "order",
        "architecture",
        "architectural",
        "customers",
        "products",
        "production"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 15,
          "title": "Segment 15 (pages 126-147)",
          "relevance_score": 0.4,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 44,
          "title": "Segment 44 (pages 448-455)",
          "relevance_score": 0.4,
          "method": "api"
        },
        {
          "book": "Building Microservices",
          "chapter": 3,
          "title": "How to Model Services",
          "relevance_score": 0.39,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 11,
          "title": "Segment 11 (pages 89-96)",
          "relevance_score": 0.39,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 42,
          "title": "Segment 42 (pages 343-353)",
          "relevance_score": 0.39,
          "method": "api"
        }
      ]
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 92-103)",
      "start_page": 92,
      "end_page": 103,
      "detection_method": "topic_boundary",
      "chapter_number": 11,
      "summary": "This chapter covers segment 11 (pages 92-103). Key topics include apis, orders, and resources. GET requests are suitable for caching,\nsince they return data already saved in the server.",
      "keywords": [
        "REST API design",
        "API",
        "REST APIs",
        "REST API",
        "order",
        "HTTP methods",
        "APIs",
        "REST",
        "Orders API",
        "API design",
        "POST",
        "API client",
        "Orders service",
        "Resource",
        "POST request"
      ],
      "concepts": [
        "apis",
        "orders",
        "resources",
        "levels",
        "server",
        "designing",
        "method",
        "principles",
        "model",
        "patch"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 34,
          "title": "Segment 34 (pages 274-281)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 7,
          "title": "Segment 7 (pages 68-75)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 6,
          "title": "Segment 6 (pages 59-67)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 8,
          "title": "Segment 8 (pages 59-69)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 57,
          "title": "Segment 57 (pages 518-522)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 104-116)",
      "start_page": 104,
      "end_page": 116,
      "detection_method": "topic_boundary",
      "chapter_number": 12,
      "summary": "In the orders API, we have these two resource URLs:\n\n/orders—Represents a list of orders Key topics include orders, ordering, and apis. HTTP methods help us model these operations:\nPOST /orders to place orders since we use POST to create new resources.",
      "keywords": [
        "HTTP status codes",
        "HTTP status",
        "API",
        "status code",
        "orders",
        "REST API design",
        "API client",
        "REST API",
        "orders API",
        "status",
        "HTTP request",
        "request",
        "POST",
        "code",
        "REST APIs"
      ],
      "concepts": [
        "orders",
        "ordering",
        "apis",
        "code",
        "request",
        "requests",
        "payload",
        "user",
        "error",
        "responses"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 8,
          "title": "Segment 8 (pages 59-69)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 34,
          "title": "Segment 34 (pages 274-281)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 7,
          "title": "Segment 7 (pages 51-58)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 6,
          "title": "Segment 6 (pages 59-67)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 42,
          "title": "Segment 42 (pages 853-871)",
          "relevance_score": 0.5,
          "method": "api"
        }
      ]
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 117-126)",
      "start_page": 117,
      "end_page": 126,
      "detection_method": "topic_boundary",
      "chapter_number": 13,
      "summary": "90\nDocumenting REST APIs\nwith OpenAPI\nIn this chapter, you’ll learn to document APIs using OpenAPI: the most popular\nstandard for describing RESTful APIs, with a rich ecosystem of tools for testing, val-\nidating, and visualizing APIs Key topics include apis, orders.",
      "keywords": [
        "JSON Schema",
        "JSON Schema specification",
        "JSON",
        "Schema",
        "API",
        "REST APIs",
        "orders API",
        "Documenting REST APIs",
        "JSON Schema object",
        "JSON Schema documents",
        "orders API endpoints",
        "URL query parameters",
        "orders",
        "APIs",
        "OpenAPI"
      ],
      "concepts": [
        "apis",
        "api",
        "orders",
        "openapi",
        "specifications",
        "specification",
        "schema",
        "documenting",
        "document",
        "types"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 37,
          "title": "Segment 37 (pages 334-341)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 8,
          "title": "Segment 8 (pages 59-69)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 7,
          "title": "Segment 7 (pages 53-60)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 33,
          "title": "Segment 33 (pages 266-273)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 23,
          "title": "Segment 23 (pages 190-197)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 127-135)",
      "start_page": 127,
      "end_page": 135,
      "detection_method": "topic_boundary",
      "chapter_number": 14,
      "summary": "99\n5.5\nDocumenting request payloads\n{\n    \"order\": [\n        {\n            \"product\": \"cappuccino\",\n            \"size\": \"big\",\n            \"quantity\": 1\n        }\n    ]\n}\nThis payload contains an attribute order, which represents an array of items Key topics include schema, responses, and order.",
      "keywords": [
        "schema",
        "order",
        "type",
        "URL path",
        "API",
        "JSON",
        "responses",
        "URL",
        "endpoint",
        "components",
        "payload",
        "JSON pointer",
        "define",
        "REST APIs",
        "properties"
      ],
      "concepts": [
        "schema",
        "responses",
        "order",
        "ordering",
        "type",
        "properties",
        "size",
        "payload",
        "listing",
        "paths"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 39,
          "title": "Segment 39 (pages 317-324)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 37,
          "title": "Segment 37 (pages 334-341)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 23,
          "title": "Segment 23 (pages 203-210)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 23,
          "title": "Segment 23 (pages 190-197)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 33,
          "title": "Segment 33 (pages 266-273)",
          "relevance_score": 0.47,
          "method": "api"
        }
      ]
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 136-143)",
      "start_page": 136,
      "end_page": 143,
      "detection_method": "topic_boundary",
      "chapter_number": 15,
      "summary": "This chapter covers segment 15 (pages 136-143). Key topics include orders, apis.",
      "keywords": [
        "orders API",
        "URL query parameters",
        "REST APIs",
        "API",
        "orders",
        "URL query",
        "Building REST APIs",
        "APIs",
        "query parameters",
        "URL",
        "query",
        "JSON Schema",
        "orders API schema",
        "implement REST APIs",
        "implementing REST APIs"
      ],
      "concepts": [
        "orders",
        "apis",
        "api",
        "schemas",
        "implement",
        "implementing",
        "url",
        "urls",
        "type",
        "typing"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 34,
          "title": "Segment 34 (pages 274-281)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 39,
          "title": "Segment 39 (pages 317-324)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 6,
          "title": "Segment 6 (pages 59-67)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 15,
          "title": "Segment 15 (pages 120-128)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 7,
          "title": "Segment 7 (pages 53-60)",
          "relevance_score": 0.66,
          "method": "api"
        }
      ]
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 144-156)",
      "start_page": 144,
      "end_page": 156,
      "detection_method": "topic_boundary",
      "chapter_number": 16,
      "summary": "This chapter covers segment 16 (pages 144-156). Key topics include apis, kitchen, and orders. THE API CLIENT SHOULD’VE TESTED THEIR CODE.",
      "keywords": [
        "API",
        "kitchen API",
        "kitchen",
        "Building REST APIs",
        "REST APIs",
        "orders API",
        "API specification",
        "Flask",
        "API endpoints",
        "URL",
        "Kitchen API schedules",
        "URL path",
        "schedule",
        "Api object",
        "APIs"
      ],
      "concepts": [
        "apis",
        "kitchen",
        "orders",
        "flask",
        "classes",
        "listing",
        "scheduled",
        "schedule",
        "fastapi",
        "useful"
      ],
      "similar_chapters": [
        {
          "book": "Python Microservices Development",
          "chapter": 7,
          "title": "Segment 7 (pages 49-56)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 8,
          "title": "Segment 8 (pages 57-64)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 46,
          "title": "Segment 46 (pages 416-420)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 38,
          "title": "Segment 38 (pages 321-334)",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 157-164)",
      "start_page": 157,
      "end_page": 164,
      "detection_method": "topic_boundary",
      "chapter_number": 17,
      "summary": "This chapter covers segment 17 (pages 157-164). Key topics include marshmallow, schemas, and validate. Since we don’t yet have marsh-\nmallow models, flask-smorest doesn’t know how to serialize data and therefore doesn’t\nreturn payloads.",
      "keywords": [
        "URL query parameters",
        "URL query",
        "query parameters",
        "URL",
        "query",
        "schedules",
        "kitchen",
        "schema",
        "parameters",
        "API",
        "marshmallow",
        "kitchen API",
        "schedules URL query",
        "status",
        "marshmallow models"
      ],
      "concepts": [
        "marshmallow",
        "schemas",
        "validate",
        "validated",
        "field",
        "parameter",
        "classes",
        "listing",
        "schedule",
        "scheduled"
      ],
      "similar_chapters": [
        {
          "book": "Python Microservices Development",
          "chapter": 7,
          "title": "Segment 7 (pages 49-56)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 8,
          "title": "Segment 8 (pages 57-64)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 11,
          "title": "Segment 11 (pages 82-89)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 38,
          "title": "Segment 38 (pages 321-334)",
          "relevance_score": 0.46,
          "method": "api"
        }
      ]
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 165-177)",
      "start_page": 165,
      "end_page": 177,
      "detection_method": "topic_boundary",
      "chapter_number": 18,
      "summary": "This chapter covers segment 18 (pages 165-177). Key topics include scheduled, schedule, and apis. 138\nCHAPTER 6\nBuilding REST APIs with Python\nrationales to justify why marshmallow doesn’t perform validation before serialization\n(http://mng.bz/9Vwx):\nIt improves performance, since validation is slow.",
      "keywords": [
        "API",
        "API layer",
        "schedule",
        "data layer",
        "layer",
        "REST APIs",
        "business layer",
        "data",
        "core business layer",
        "service",
        "APIs",
        "web API layer",
        "Orders service",
        "business",
        "Building REST APIs"
      ],
      "concepts": [
        "scheduled",
        "schedule",
        "apis",
        "validated",
        "validation",
        "validates",
        "payload",
        "implementation",
        "implement",
        "implementations"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 4,
          "title": "Segment 4 (pages 25-32)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 46,
          "title": "Segment 46 (pages 416-420)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 7,
          "title": "Segment 7 (pages 51-58)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 8,
          "title": "Segment 8 (pages 59-69)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 23,
          "title": "Segment 23 (pages 232-240)",
          "relevance_score": 0.47,
          "method": "api"
        }
      ]
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 178-191)",
      "start_page": 178,
      "end_page": 191,
      "detection_method": "topic_boundary",
      "chapter_number": 19,
      "summary": "We start our implementation from the database since it\nwill facilitate the rest of the discussion in this chapter Key topics include orders, layer, and items.",
      "keywords": [
        "order",
        "database",
        "orders service",
        "business layer",
        "database models",
        "layer",
        "orders repository",
        "repository",
        "repository pattern",
        "API layer",
        "Service",
        "data layer",
        "business",
        "data",
        "models"
      ],
      "concepts": [
        "orders",
        "layer",
        "items",
        "services",
        "list",
        "ordersservice",
        "repositories",
        "model",
        "ids",
        "classes"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 23,
          "title": "Segment 23 (pages 203-210)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 45,
          "title": "Segment 45 (pages 408-415)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 5,
          "title": "Segment 5 (pages 36-45)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 43,
          "title": "Segment 43 (pages 433-447)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 44,
          "title": "Segment 44 (pages 448-455)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 192-200)",
      "start_page": 192,
      "end_page": 200,
      "detection_method": "topic_boundary",
      "chapter_number": 20,
      "summary": "This chapter covers segment 20 (pages 192-200). Key topics include order, apis. In the orders service, a suitable inversion of con-\ntrol container is the request object since most operations are specific to the\ncontext of a request.",
      "keywords": [
        "order",
        "orders repository",
        "API",
        "orders service",
        "Order class",
        "kitchen API",
        "Service",
        "kitchen",
        "repository",
        "orders API",
        "payments API",
        "schedule",
        "dependencies",
        "API calls",
        "API layer"
      ],
      "concepts": [
        "order",
        "api",
        "apis",
        "listing",
        "pattern",
        "file",
        "implementing",
        "implementations",
        "implement",
        "scheduling"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 34,
          "title": "Segment 34 (pages 274-281)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 24,
          "title": "Segment 24 (pages 192-199)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 23,
          "title": "Segment 23 (pages 203-210)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 45,
          "title": "Segment 45 (pages 408-415)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 6,
          "title": "Segment 6 (pages 59-67)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 201-209)",
      "start_page": 201,
      "end_page": 209,
      "detection_method": "topic_boundary",
      "chapter_number": 21,
      "summary": "This chapter covers segment 21 (pages 201-209). Key topics include orders, database, and listing. We could simply use SQLAlchemy session objects to wrap\nour calls to OrdersService, and once our operations succeed, use the session to com-\nmit, or roll back otherwise.",
      "keywords": [
        "Order",
        "unit",
        "API layer",
        "orders repository",
        "context",
        "work",
        "API",
        "layer",
        "database",
        "unit of work",
        "Service",
        "session",
        "work pattern",
        "UnitOfWork",
        "object"
      ],
      "concepts": [
        "orders",
        "database",
        "listing",
        "method",
        "pattern",
        "object",
        "committed",
        "commit",
        "context",
        "classes"
      ],
      "similar_chapters": [
        {
          "book": "Architecture Patterns with Python",
          "chapter": 6,
          "title": "Unit of Work",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 16,
          "title": "Segment 16 (pages 133-143)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 2,
          "title": "Repository Pattern",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 23,
          "title": "Segment 23 (pages 203-210)",
          "relevance_score": 0.52,
          "method": "api"
        }
      ]
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 210-219)",
      "start_page": 210,
      "end_page": 219,
      "detection_method": "topic_boundary",
      "chapter_number": 22,
      "summary": "In this chapter, you’ll learn to design a GraphQL API Key topics include graphql, product, and ingredients. When a resource is repre-\nsented by a large payload, fetching it from the server translates to a large\namount of data transfer.",
      "keywords": [
        "products API",
        "API",
        "products",
        "GraphQL",
        "GraphQL APIs",
        "products service",
        "Designing GraphQL APIs",
        "GraphQL APIs product",
        "APIs",
        "ingredients",
        "String",
        "GraphQL server products",
        "API specification",
        "price",
        "stock"
      ],
      "concepts": [
        "graphql",
        "product",
        "ingredients",
        "apis",
        "api",
        "stock",
        "string",
        "data",
        "price",
        "types"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 39,
          "title": "Segment 39 (pages 337-346)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 11,
          "title": "Segment 11 (pages 82-89)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 2,
          "title": "[ 19 ]",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 40,
          "title": "Segment 40 (pages 394-416)",
          "relevance_score": 0.44,
          "method": "api"
        }
      ]
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 220-231)",
      "start_page": 220,
      "end_page": 231,
      "detection_method": "topic_boundary",
      "chapter_number": 23,
      "summary": "8.3.1\nCreating property definitions with scalars\nThis section explains how we define the type of a property using GraphQL’s type sys-\ntem Key topics include type, properties, and property.",
      "keywords": [
        "type",
        "String",
        "ingredient",
        "Ingredient type",
        "Boolean",
        "Supplier",
        "Supplier type",
        "cake",
        "API",
        "object types",
        "GraphQL",
        "products API",
        "property",
        "Beverage",
        "object"
      ],
      "concepts": [
        "type",
        "properties",
        "property",
        "list",
        "ingredients",
        "graphql",
        "products",
        "apis",
        "api",
        "strings"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "Segment 20 (pages 181-194)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 20,
          "title": "Segment 20 (pages 392-414)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 43,
          "title": "Segment 43 (pages 863-885)",
          "relevance_score": 0.43,
          "method": "api"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 5,
          "title": "Numeric Types",
          "relevance_score": 0.43,
          "method": "api"
        }
      ]
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 232-239)",
      "start_page": 232,
      "end_page": 239,
      "detection_method": "topic_boundary",
      "chapter_number": 24,
      "summary": "This chapter covers segment 24 (pages 232-239). Key topics include apis, query. The specification of a GraphQL query looks similar to the signature definition of\na Python function: we define the query name, optionally define a list of parameters\nfor the query between parentheses, and specify the return type after a colon.",
      "keywords": [
        "API",
        "products API",
        "GraphQL API",
        "query",
        "products",
        "GraphQL",
        "type",
        "APIs",
        "API consumers",
        "server",
        "type Query",
        "API specification",
        "Boolean",
        "products API specification",
        "’ll"
      ],
      "concepts": [
        "apis",
        "api",
        "query",
        "queries",
        "type",
        "products",
        "graphql",
        "mutations",
        "mutation",
        "list"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 10,
          "title": "Segment 10 (pages 81-88)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "Segment 20 (pages 181-194)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 39,
          "title": "Segment 39 (pages 337-346)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 42,
          "title": "Segment 42 (pages 363-371)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 28,
          "title": "Segment 28 (pages 555-576)",
          "relevance_score": 0.5,
          "method": "api"
        }
      ]
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 240-252)",
      "start_page": 240,
      "end_page": 252,
      "detection_method": "topic_boundary",
      "chapter_number": 25,
      "summary": "This chapter covers segment 25 (pages 240-252). Key topics include query, list, and type. This UI shows the \nrelationships between object types \ncaptured by the queries available in \nthe API.",
      "keywords": [
        "query",
        "queries",
        "API",
        "error",
        "products",
        "products API",
        "type",
        "GraphQL APIs",
        "ingredient",
        "Listing",
        "run",
        "Consuming GraphQL APIs",
        "GraphQL",
        "ProductInterface",
        "fragment"
      ],
      "concepts": [
        "query",
        "list",
        "type",
        "products",
        "apis",
        "api",
        "ingredients",
        "error",
        "parameters",
        "running"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 39,
          "title": "Segment 39 (pages 337-346)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 42,
          "title": "Segment 42 (pages 363-371)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 6,
          "title": "Segment 6 (pages 59-67)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 11,
          "title": "Segment 11 (pages 88-98)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "Segment 20 (pages 181-194)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 253-260)",
      "start_page": 253,
      "end_page": 260,
      "detection_method": "topic_boundary",
      "chapter_number": 26,
      "summary": "9.7\nRunning GraphQL mutations\nThis section explains how we run GraphQL mutations Key topics include queries, query, and mutation. The only difference between the two is in their\nintent: queries are meant to read data from the server, while mutations are meant to\ncreate or change data in the server.",
      "keywords": [
        "query",
        "GraphQL",
        "GraphQL API",
        "query document",
        "mutation",
        "API",
        "type",
        "queries",
        "Query Variables",
        "Query document mutation",
        "mutations",
        "parameterized",
        "Consuming GraphQL APIs",
        "APIs",
        "URL"
      ],
      "concepts": [
        "queries",
        "query",
        "mutation",
        "types",
        "apis",
        "api",
        "request",
        "requests",
        "data",
        "curl"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 41,
          "title": "Segment 41 (pages 373-382)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 10,
          "title": "Segment 10 (pages 81-88)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "Segment 20 (pages 181-194)",
          "relevance_score": 0.41,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 19,
          "title": "Segment 19 (pages 158-165)",
          "relevance_score": 0.38,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 39,
          "title": "Segment 39 (pages 337-346)",
          "relevance_score": 0.38,
          "method": "api"
        }
      ]
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 261-276)",
      "start_page": 261,
      "end_page": 276,
      "detection_method": "topic_boundary",
      "chapter_number": 27,
      "summary": "After reading this chapter, you’ll have all the tools you need\nto start developing your own GraphQL APIs Key topics include query, queries, and types.",
      "keywords": [
        "products API",
        "GraphQL APIs",
        "Ariadne",
        "API",
        "GraphQL",
        "Building GraphQL APIs",
        "query",
        "resolver",
        "products",
        "products API specification",
        "GraphQL server",
        "schema",
        "type",
        "server",
        "API specification"
      ],
      "concepts": [
        "query",
        "queries",
        "types",
        "graphql",
        "schemas",
        "ariadne",
        "implement",
        "implementing",
        "implementations",
        "products"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 41,
          "title": "Segment 41 (pages 373-382)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "Segment 20 (pages 181-194)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 47,
          "title": "Segment 47 (pages 405-412)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 18,
          "title": "Segment 18 (pages 149-157)",
          "relevance_score": 0.47,
          "method": "api"
        }
      ]
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 277-289)",
      "start_page": 277,
      "end_page": 289,
      "detection_method": "topic_boundary",
      "chapter_number": 28,
      "summary": "This chapter covers segment 28 (pages 277-289). Key topics include products, types, and list. # file: web/queries.py\nfrom ariadne import QueryType\nfrom web.data import ingredients, products\nquery = QueryType().",
      "keywords": [
        "product",
        "products API",
        "type",
        "type resolver",
        "datetime",
        "resolver",
        "input",
        "API",
        "object",
        "products API datetime",
        "Python",
        "Product type",
        "query",
        "ariadne",
        "scalar"
      ],
      "concepts": [
        "products",
        "types",
        "list",
        "resolve",
        "mutations",
        "mutation",
        "query",
        "queries",
        "data",
        "parameters"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 58,
          "title": "Segment 58 (pages 491-498)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 16,
          "title": "Segment 16 (pages 129-139)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 28,
          "title": "Segment 28 (pages 555-576)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 13,
          "title": "Segment 13 (pages 104-111)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 15,
          "title": "Segment 15 (pages 120-128)",
          "relevance_score": 0.48,
          "method": "api"
        }
      ]
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 290-298)",
      "start_page": 290,
      "end_page": 298,
      "detection_method": "topic_boundary",
      "chapter_number": 29,
      "summary": "This chapter covers segment 29 (pages 290-298). Key topics include apis, ingredients. Before we close the chapter, there’s one more\ntopic we need to explore: implementing resolvers for the fields of an object type.",
      "keywords": [
        "ingredient",
        "API",
        "APIs",
        "product",
        "resolver",
        "API authentication",
        "authorization",
        "GraphQL APIs",
        "products API",
        "API authorization",
        "API specification",
        "ingredients property",
        "GraphQL",
        "ingredients field",
        "type"
      ],
      "concepts": [
        "apis",
        "api",
        "ingredients",
        "resolve",
        "resolving",
        "products",
        "production",
        "chapters",
        "implement",
        "implemented"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 7,
          "title": "Segment 7 (pages 53-60)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 39,
          "title": "Segment 39 (pages 317-324)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 42,
          "title": "Segment 42 (pages 853-871)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 33,
          "title": "Segment 33 (pages 266-273)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 42,
          "title": "Segment 42 (pages 363-371)",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 299-319)",
      "start_page": 299,
      "end_page": 319,
      "detection_method": "topic_boundary",
      "chapter_number": 30,
      "summary": "This section\nexplains how each protocol works and how they fit within the authentication and\nauthorization flows for our APIs Key topics include tokens, requesting, and request.",
      "keywords": [
        "JSON Web Tokens",
        "token",
        "access token",
        "API",
        "API server",
        "API authorization",
        "authorization",
        "JWT",
        "access",
        "Authorization server",
        "Web Tokens",
        "JSON Web",
        "Authorization code flow",
        "Authorization code",
        "server"
      ],
      "concepts": [
        "tokens",
        "requesting",
        "request",
        "authorization",
        "authorize",
        "server",
        "api",
        "apis",
        "access",
        "accessible"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 27,
          "title": "Segment 27 (pages 245-256)",
          "relevance_score": 0.77,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 29,
          "title": "Segment 29 (pages 265-274)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 28,
          "title": "Segment 28 (pages 257-264)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 35,
          "title": "Segment 35 (pages 318-325)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 8,
          "title": "Segment 8 (pages 57-64)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 320-327)",
      "start_page": 320,
      "end_page": 327,
      "detection_method": "topic_boundary",
      "chapter_number": 31,
      "summary": "This chapter covers segment 31 (pages 320-327). Key topics include orders, users, and tables. As we saw in section 11.4.2, CORS requests are sent by\nthe browser to know which headers, methods, and origins are allowed by the server.",
      "keywords": [
        "user",
        "orders",
        "user table",
        "CORS middleware",
        "CORS",
        "order table",
        "request",
        "service",
        "CORS requests",
        "middleware",
        "API",
        "column",
        "access",
        "temporary table",
        "code"
      ],
      "concepts": [
        "orders",
        "users",
        "tables",
        "request",
        "requests",
        "list",
        "items",
        "alembic",
        "authorizing",
        "authorize"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 35,
          "title": "Segment 35 (pages 318-325)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 29,
          "title": "Segment 29 (pages 265-274)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 8,
          "title": "Segment 8 (pages 57-64)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 27,
          "title": "Segment 27 (pages 245-256)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 34,
          "title": "Segment 34 (pages 309-317)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 328-342)",
      "start_page": 328,
      "end_page": 342,
      "detection_method": "topic_boundary",
      "chapter_number": 32,
      "summary": "This chapter covers segment 32 (pages 328-342). Key topics include orders, apis, and endpoints. It distinguishes four\nauthorization flows:\n– Authorization code—The API server exchanges a code with the authorization\nserver to request the user’s access token.",
      "keywords": [
        "Dredd",
        "orders API",
        "order",
        "API",
        "Dredd hooks",
        "Testing REST APIs",
        "APIs",
        "REST APIs",
        "post",
        "orders endpoint",
        "Dredd API testing",
        "API server",
        "API specification",
        "hooks",
        "orders endpoint test"
      ],
      "concepts": [
        "orders",
        "apis",
        "endpoints",
        "response",
        "hooks",
        "requests",
        "request",
        "server",
        "transaction",
        "ids"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 29,
          "title": "Segment 29 (pages 265-274)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 28,
          "title": "Segment 28 (pages 257-264)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 34,
          "title": "Segment 34 (pages 274-281)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 27,
          "title": "Segment 27 (pages 245-256)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 6,
          "title": "Segment 6 (pages 59-67)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 343-351)",
      "start_page": 343,
      "end_page": 351,
      "detection_method": "topic_boundary",
      "chapter_number": 33,
      "summary": "12.3\nIntroduction to property-based testing\nThis section explains what property-based testing is, how it works, and how it helps us\nwrite more exhaustive tests for our APIs Key topics include orders, ordered, and hypothesis.",
      "keywords": [
        "hypothesis",
        "API",
        "property-based testing",
        "orders",
        "strategy",
        "orders endpoint",
        "payload",
        "API testing",
        "POST",
        "orders API",
        "schemathesis",
        "APIs",
        "property-based",
        "Testing REST APIs",
        "code"
      ],
      "concepts": [
        "orders",
        "ordered",
        "hypothesis",
        "payloads",
        "strategy",
        "strategies",
        "api",
        "apis",
        "listing",
        "properties"
      ],
      "similar_chapters": [
        {
          "book": "Python Microservices Development",
          "chapter": 11,
          "title": "Segment 11 (pages 82-89)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 10,
          "title": "[ 329 ]",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Architecture Patterns with Python",
          "chapter": 4,
          "title": "Our First Use",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 21,
          "title": "Segment 21 (pages 209-217)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "Building Microservices",
          "chapter": 7,
          "title": "Testing",
          "relevance_score": 0.43,
          "method": "api"
        }
      ]
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 352-361)",
      "start_page": 352,
      "end_page": 361,
      "detection_method": "topic_boundary",
      "chapter_number": 34,
      "summary": "This chapter covers segment 34 (pages 352-361). Key topics include tested, apis.",
      "keywords": [
        "API",
        "Schemathesis",
        "APIs",
        "Docker",
        "GraphQL APIs",
        "orders",
        "Testing GraphQL APIs",
        "API testing",
        "POST",
        "Schemathesis test suite",
        "Testing REST APIs",
        "orders endpoint",
        "Docker image",
        "test suite",
        "GraphQL"
      ],
      "concepts": [
        "tested",
        "apis",
        "api",
        "docker",
        "run",
        "running",
        "runs",
        "link",
        "graphql",
        "contains"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 6,
          "title": "Segment 6 (pages 59-67)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 57,
          "title": "Segment 57 (pages 518-522)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 11,
          "title": "Segment 11 (pages 82-89)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 33,
          "title": "Segment 33 (pages 266-273)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 362-369)",
      "start_page": 362,
      "end_page": 369,
      "detection_method": "topic_boundary",
      "chapter_number": 35,
      "summary": "This chapter covers segment 35 (pages 362-369). Key topics include docker, order. So far, the orders service has been using a hard-\ncoded database URL, but to operate the service in different environments, we need to\nmake this setting configurable.",
      "keywords": [
        "Docker",
        "Docker Compose",
        "URL",
        "database URL",
        "AWS",
        "orders",
        "Docker Compose file",
        "Docker image",
        "run",
        "docker build",
        "run Docker Compose",
        "docker run",
        "database",
        "Docker Compose database",
        "run Docker containers"
      ],
      "concepts": [
        "docker",
        "order",
        "run",
        "runs",
        "running",
        "container",
        "databases",
        "commands",
        "file",
        "image"
      ],
      "similar_chapters": [
        {
          "book": "Python Microservices Development",
          "chapter": 36,
          "title": "Segment 36 (pages 304-311)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 35,
          "title": "Segment 35 (pages 296-303)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 14,
          "title": "Segment 14 (pages 106-114)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Microservice Architecture",
          "chapter": 13,
          "title": "Segment 13 (pages 98-105)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 370-377)",
      "start_page": 370,
      "end_page": 377,
      "detection_method": "topic_boundary",
      "chapter_number": 36,
      "summary": "Although we won’t cover it in this chapter,\nminikube is a great tool to get more familiar with Kubernetes Key topics include kubernetes, service, and cluster.",
      "keywords": [
        "Kubernetes cluster",
        "Kubernetes",
        "cluster",
        "Kubernetes service",
        "AWS",
        "Elastic Kubernetes Service",
        "Kubernetes API",
        "AWS Fargate",
        "AWS EKS",
        "EKS",
        "Kubernetes control plane",
        "service",
        "Kubernetes control",
        "Kubernetes CLI",
        "Kubernetes Services Compared"
      ],
      "concepts": [
        "kubernetes",
        "service",
        "cluster",
        "deploying",
        "deployments",
        "useful",
        "uses",
        "run",
        "running",
        "runs"
      ],
      "similar_chapters": [
        {
          "book": "Microservices Up and Running",
          "chapter": 31,
          "title": "Segment 31 (pages 305-313)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 46,
          "title": "Segment 46 (pages 390-397)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 27,
          "title": "Segment 27 (pages 263-270)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 16,
          "title": "Segment 16 (pages 157-170)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 28,
          "title": "Segment 28 (pages 271-278)",
          "relevance_score": 0.58,
          "method": "api"
        }
      ]
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 378-388)",
      "start_page": 378,
      "end_page": 388,
      "detection_method": "topic_boundary",
      "chapter_number": 37,
      "summary": "This chapter covers segment 37 (pages 378-388). Key topics include service, ingress, and deploying.",
      "keywords": [
        "AWS Load Balancer",
        "Kubernetes cluster",
        "service",
        "Kubernetes",
        "Load Balancer Controller",
        "Kubernetes load balancer",
        "load balancer",
        "AWS",
        "AWS Load",
        "AWS API",
        "cluster",
        "Kubernetes service",
        "ingress",
        "Kubernetes API",
        "pods"
      ],
      "concepts": [
        "service",
        "ingress",
        "deploying",
        "deployments",
        "create",
        "created",
        "pods",
        "pod",
        "cluster",
        "orders"
      ],
      "similar_chapters": [
        {
          "book": "Microservices Up and Running",
          "chapter": 31,
          "title": "Segment 31 (pages 305-313)",
          "relevance_score": 0.8,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 27,
          "title": "Segment 27 (pages 263-270)",
          "relevance_score": 0.76,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 28,
          "title": "Segment 28 (pages 271-278)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 35,
          "title": "Segment 35 (pages 296-303)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 42,
          "title": "Segment 42 (pages 343-353)",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 389-397)",
      "start_page": 389,
      "end_page": 397,
      "detection_method": "topic_boundary",
      "chapter_number": 38,
      "summary": "This chapter covers segment 38 (pages 389-397). Key topics include order, secrets, and creating. To be able to interact with the API, we must set up a database, which will be the\ngoal of our next section.",
      "keywords": [
        "database subnet group",
        "database",
        "AWS",
        "database subnet",
        "AWS Aurora",
        "Kubernetes",
        "Aurora",
        "Aurora database",
        "serverless database",
        "Kubernetes cluster",
        "subnet group",
        "Kubernetes secrets",
        "command",
        "VPC",
        "Aurora Serverless"
      ],
      "concepts": [
        "order",
        "secrets",
        "creating",
        "create",
        "aurora",
        "services",
        "migrations",
        "migration",
        "security",
        "secure"
      ],
      "similar_chapters": [
        {
          "book": "Microservices Up and Running",
          "chapter": 31,
          "title": "Segment 31 (pages 305-313)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 42,
          "title": "Segment 42 (pages 343-353)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 27,
          "title": "Segment 27 (pages 263-270)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 7,
          "title": "Segment 7 (pages 53-60)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 28,
          "title": "Segment 28 (pages 271-278)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 398-407)",
      "start_page": 398,
      "end_page": 407,
      "detection_method": "topic_boundary",
      "chapter_number": 39,
      "summary": "The next section explains one more change we need\nto make to finalize the deployment Key topics include apis, service. Once the migrations job has completed, the database is finally ready to be used.",
      "keywords": [
        "Kubernetes cluster",
        "Kubernetes",
        "AWS Load Balancer",
        "API",
        "aws",
        "Load Balancer",
        "SOAP",
        "ALB",
        "AWS Load",
        "APIs",
        "RPC",
        "API client",
        "Kubernetes API",
        "cluster",
        "delete"
      ],
      "concepts": [
        "apis",
        "api",
        "service",
        "orders",
        "kubernetes",
        "soap",
        "servers",
        "deploying",
        "include",
        "including"
      ],
      "similar_chapters": [
        {
          "book": "Microservices Up and Running",
          "chapter": 31,
          "title": "Segment 31 (pages 305-313)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 27,
          "title": "Segment 27 (pages 263-270)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 28,
          "title": "Segment 28 (pages 271-278)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 35,
          "title": "Segment 35 (pages 296-303)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 47,
          "title": "Segment 47 (pages 398-406)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 408-418)",
      "start_page": 408,
      "end_page": 418,
      "detection_method": "topic_boundary",
      "chapter_number": 40,
      "summary": "It provides a way of exchanging\nmessages, and it is up to the agents involved on both sides of the API to decide\nhow to make sense of such messages Key topics include apis, version.",
      "keywords": [
        "API",
        "APIs",
        "REST APIs",
        "API client",
        "REST API",
        "API version",
        "REST",
        "API deprecation",
        "data",
        "API client requests",
        "API Report",
        "API server",
        "REST API responses",
        "Header",
        "API integrations"
      ],
      "concepts": [
        "apis",
        "api",
        "version",
        "versions",
        "graphql",
        "data",
        "uses",
        "useful",
        "ingredients",
        "client"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 33,
          "title": "Segment 33 (pages 266-273)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 34,
          "title": "Segment 34 (pages 274-281)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 29,
          "title": "Segment 29 (pages 265-274)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 8,
          "title": "Segment 8 (pages 59-69)",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 419-429)",
      "start_page": 419,
      "end_page": 429,
      "detection_method": "topic_boundary",
      "chapter_number": 41,
      "summary": "Auth0 takes care of managing user accounts and\nissuing secure tokens, and it also provides easy integrations for social login with\nidentity providers such as Google, Facebook, Twitter, and others Key topics include orders, keys.",
      "keywords": [
        "API",
        "token",
        "orders API",
        "API authorization",
        "key",
        "access token",
        "CERTIFICATE",
        "access",
        "orders",
        "client",
        "APIs",
        "authorization",
        "public key",
        "API server",
        "public"
      ],
      "concepts": [
        "orders",
        "keys",
        "key",
        "apis",
        "tokens",
        "authorize",
        "authorizing",
        "authorized",
        "certificates",
        "application"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 27,
          "title": "Segment 27 (pages 245-256)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 29,
          "title": "Segment 29 (pages 265-274)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 28,
          "title": "Segment 28 (pages 257-264)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 23,
          "title": "Segment 23 (pages 183-192)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 24,
          "title": "Segment 24 (pages 193-200)",
          "relevance_score": 0.58,
          "method": "api"
        }
      ]
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 430-440)",
      "start_page": 430,
      "end_page": 440,
      "detection_method": "topic_boundary",
      "chapter_number": 42,
      "summary": "This chapter covers segment 42 (pages 430-440). Key topics include graphql, apis, and property.",
      "keywords": [
        "REST API",
        "REST API testing",
        "API",
        "REST APIs",
        "REST API design",
        "implementing API endpoints",
        "GraphQL API",
        "JSON Schema",
        "REST",
        "implementing",
        "GraphQL API testing",
        "API testing",
        "implementing API",
        "REST API implementation",
        "APIs"
      ],
      "concepts": [
        "graphql",
        "apis",
        "property",
        "properties",
        "openapi",
        "implementation",
        "rest",
        "kubernetes",
        "designing",
        "pattern"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 37,
          "title": "Segment 37 (pages 334-341)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 31,
          "title": "Segment 31 (pages 305-313)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 34,
          "title": "Segment 34 (pages 274-281)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 6,
          "title": "Segment 6 (pages 59-67)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 7,
          "title": "Segment 7 (pages 53-60)",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 441-442)",
      "start_page": 441,
      "end_page": 442,
      "detection_method": "topic_boundary",
      "chapter_number": 43,
      "summary": "Rich with proven advice and Python-based examples, this \npractical book focuses on implementation over philosophy Key topics include apis, microservice.",
      "keywords": [
        "core business layer",
        "APIs",
        "business layer",
        "Hexagonal architecture",
        "core business",
        "Microservice APIs",
        "API",
        "microservice",
        "ingredients",
        "components",
        "GraphQL",
        "core",
        "coupled components",
        "Haro Peralta",
        "business"
      ],
      "concepts": [
        "api",
        "apis",
        "microservice",
        "book",
        "service",
        "ingredients",
        "graphql",
        "examples",
        "pattern",
        "excellent"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 5,
          "title": "Segment 5 (pages 33-40)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 46,
          "title": "Segment 46 (pages 416-420)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 42,
          "title": "Segment 42 (pages 363-371)",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "M A N N I N G\nJosé Haro Peralta\nUsing Python, Flask, FastAPI, OpenAPI and more",
      "content_length": 78,
      "extraction_method": "Direct"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "Documentation-driven development is an API-first development method in which you design and document the \nAPI first; then, you build the API server and the API client against the documentation; and finally, you use the API \ndocumentation to validate the server and client implementations. Documentation-driven development helps you \nreduce the chances of API integration failure, and it gives you more control and visibility of integration errors.\n3. Test the implementation\nagainst the speciﬁcation.\nAPI speciﬁcation\nAPI server developers\nAPI client developers\n1. API design and\ndocumentation\n2. Build the client\nand the server\nagainst the API\ndocumentation.\nREST APIs are structured around endpoints. We distinguish between singleton endpoints, such as GET /orders/8, \nand collection endpoints, such as GET /orders. REST APIs leverage the semantics of HTTP methods to indicate \nactions (such as POST to create resources), and they use HTTP status codes that signal the result of processing \nthe request (such as 200 for successful responses).\nSingleton endpoints\nPUT /orders/8\n{payload}\nDELETE /orders/8\nUpdate an order\nDelete an order\nStatus code: 200\n{payload}\nStatus code: 204\nHTTP equests\nr\nHTTP esponses\nr\nPOST /orders/8/pay\n{payload}\nPOST /orders/8/cancel\n{payload}\nPay for an order\nCancel an order\nStatus code: 200\n{payload}\nStatus code: 200\n{payload}\nCollection endpoints\nPOST /orders\n{payload}\nPlace an order\nStatus code: 201\n{payload}\nGET /orders\nGet a collection\nof orders\nStatus code: 200\n{payload}\nGET /orders/8\nRetrieve an order\nStatus code: 200\n{payload}\nSingleton endpoints\nOrders\nservice\nAPI lient\nc",
      "content_length": 1618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "Microservice APIs",
      "content_length": 17,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "Microservice APIs\nUSING PYTHON, FLASK, FASTAPI,\nOPENAPI AND MORE\nJOSÉ HARO PERALTA\nM A N N I N G\nSHELTER ISLAND",
      "content_length": 111,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "For online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2023 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning Publications \nwas aware of a trademark claim, the designations have been printed in initial caps or all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nThe author and publisher have made every effort to ensure that the information in this book \nwas correct at press time. The author and publisher do not assume and hereby disclaim any \nliability to any party for any loss, damage, or disruption caused by errors or omissions, whether \nsuch errors or omissions result from negligence, accident, or any other cause, or from any usage \nof the information herein.\nManning Publications Co.\nDevelopment editor: Marina Michaels\n20 Baldwin Road\nTechnical development editor: Nick Watts\nPO Box 761\nReview editor: Mihaela Batinic´\nShelter Island, NY 11964\nProduction editor: Andy Marinkovich\nCopy editor: Michele Mitchell\nProofreader: Katie Tennant\nTechnical proofreader: Al Krinker\nTypesetter: Dennis Dalinnik\nCover designer: Marija Tudor\nISBN: 9781617298417\nPrinted in the United States of America",
      "content_length": 2125,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "To Jiwon, without whose constant support and encouragement I wouldn’t \nhave been able to write this book, and to Ivy, that boundless spark of joy \nthat makes everything I do worth it.",
      "content_length": 183,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "vii\nbrief contents\nPART 1\nINTRODUCING MICROSERVICE APIS...............................1\n1\n■\nWhat are microservice APIs?\n3\n2\n■\nA basic API implementation\n20\n3\n■\nDesigning microservices\n45\nPART 2\nDESIGNING AND BUILDING REST APIS ........................59\n4\n■\nPrinciples of REST API design\n61\n5\n■\nDocumenting REST APIs with OpenAPI\n90\n6\n■\nBuilding REST APIs with Python\n110\n7\n■\nService implementation patterns for microservices\n144\nPART 3\nDESIGNING AND BUILDING GRAPHQL APIS ...............183\n8\n■\nDesigning GraphQL APIs\n185\n9\n■\nConsuming GraphQL APIs\n210\n10\n■\nBuilding GraphQL APIs with Python\n233",
      "content_length": 596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "BRIEF CONTENTS\nviii\nPART 4\nSECURING, TESTING, AND DEPLOYING \nMICROSERVICE APIS ..................................................267\n11\n■\nAPI authorization and authentication\n269\n12\n■\nTesting and validating APIs\n302\n13\n■\nDockerizing microservice APIs\n331\n14\n■\nDeploying microservice APIs with Kubernetes\n342",
      "content_length": 307,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "ix\ncontents\npreface\nxvi\nacknowledgments\nxviii\nabout this book\nxx\nabout the author\nxxv\nabout the cover illustration\nxxvi\nPART 1\nINTRODUCING MICROSERVICE APIS.....................1\n1 What are microservice APIs?\n3\n1.1\nWhat are microservices?\n4\nDefining microservices\n4\n■Microservices vs. monoliths\n5\nMicroservices today and how we got here\n7\n1.2\nWhat are web APIs?\n8\nWhat is an API?\n8\n■What is a web API?\n9\n■How do APIs help \nus drive microservices integrations?\n9\n1.3\nChallenges of microservices architecture\n11\nEffective service decomposition\n11\n■Microservices integration \ntests\n12\n■Handling service unavailability\n12\n■Tracing \ndistributed transactions\n13\n■Increased operational complexity \nand infrastructure overhead\n14\n1.4\nIntroducing documentation-driven development\n15",
      "content_length": 773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "CONTENTS\nx\n1.5\nIntroducing the CoffeeMesh application\n17\n1.6\nWho this book is for and what you will learn\n17\n2 A basic API implementation\n20\n2.1\nIntroducing the orders API specification\n21\n2.2\nHigh-level architecture of the orders application\n22\n2.3\nImplementing the API endpoints\n23\n2.4\nImplementing data validation models with pydantic\n30\n2.5\nValidating request payloads with pydantic\n34\n2.6\nMarshalling and validating response payloads \nwith pydantic\n38\n2.7\nAdding an in-memory list of orders to the API\n41\n3 Designing microservices\n45\n3.1\nIntroducing CoffeeMesh\n46\n3.2\nMicroservices design principles\n46\nDatabase-per-service principle\n46\n■Loose coupling principle\n48\nSingle Responsibility Principle\n49\n3.3\nService decomposition by business capability\n49\nAnalyzing the business structure of CoffeeMesh\n49\n■Decomposing \nmicroservices by business capabilities\n50\n3.4\nService decomposition by subdomains\n52\nWhat is domain-driven design?\n52\n■Applying strategic analysis to \nCoffeeMesh\n53\n3.5\nDecomposition by business capability vs. decomposition by \nsubdomain\n57\nPART 2\nDESIGNING AND BUILDING REST APIS ..............59\n4 Principles of REST API design\n61\n4.1\nWhat is REST?\n62\n4.2\nArchitectural constraints of REST applications\n63\nSeparation of concerns: The client-server architecture principle\n64\nMake it scalable: The statelessness principle\n64\n■Optimize for \nperformance: The cacheability principle\n65\n■Make it simple for the \nclient: The layered system principle\n66\n■Extendable interfaces: The \ncode-on-demand principle\n66\n■Keep it consistent: The uniform \ninterface principle\n67",
      "content_length": 1583,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "CONTENTS\nxi\n4.3\nHypermedia as the engine of application state\n67\n4.4\nAnalyzing the maturity of an API with the Richardson \nmaturity model\n70\nLevel 0: Web APIs à la RPC\n71\n■Level 1: Introducing the concept \nof resource\n71\n■Level 2: Using HTTP methods and status \ncodes\n72\n■Level 3: API discoverability\n72\n4.5\nStructured resource URLs with HTTP methods\n73\n4.6\nUsing HTTP status codes to create expressive HTTP \nresponses\n77\nWhat are HTTP status codes?\n77\n■Using HTTP status codes to \nreport client errors in the request\n78\n■Using HTTP status codes to \nreport errors in the server\n82\n4.7\nDesigning API payloads\n83\nWhat are HTTP payloads, and when do we use them?\n83\n■HTTP \npayload design patterns\n84\n4.8\nDesigning URL query parameters\n87\n5 Documenting REST APIs with OpenAPI\n90\n5.1\nUsing JSON Schema to model data\n91\n5.2\nAnatomy of an OpenAPI specification\n95\n5.3\nDocumenting the API endpoints\n96\n5.4\nDocumenting URL query parameters\n97\n5.5\nDocumenting request payloads\n98\n5.6\nRefactoring schema definitions to avoid repetition\n100\n5.7\nDocumenting API responses\n102\n5.8\nCreating generic responses\n105\n5.9\nDefining the authentication scheme of the API\n107\n6 Building REST APIs with Python\n110\n6.1\nOverview of the orders API\n111\n6.2\nURL query parameters for the orders API\n112\n6.3\nValidating payloads with unknown fields\n115\n6.4\nOverriding FastAPI’s dynamically generated \nspecification\n118\n6.5\nOverview of the kitchen API\n120\n6.6\nIntroducing flask-smorest\n122",
      "content_length": 1455,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "CONTENTS\nxii\n6.7\nInitializing the web application for the API\n123\n6.8\nImplementing the API endpoints\n125\n6.9\nImplementing payload validation models \nwith marshmallow\n129\n6.10\nValidating URL query parameters\n133\n6.11\nValidating data before serializing the response\n136\n6.12\nImplementing an in-memory list of schedules\n140\n6.13\nOverriding flask-smorest’s dynamically generated API \nspecification\n142\n7 Service implementation patterns for microservices\n144\n7.1\nHexagonal architectures for microservices\n145\n7.2\nSetting up the environment and the project \nstructure\n148\n7.3\nImplementing the database models\n149\n7.4\nImplementing the repository pattern for data access\n155\nThe case for the repository pattern: What is it, and why is it \nuseful?\n155\n■Implementing the repository pattern\n157\n7.5\nImplementing the business layer\n162\n7.6\nImplementing the unit of work pattern\n172\n7.7\nIntegrating the API layer and the service layer\n177\nPART 3\nDESIGNING AND BUILDING GRAPHQL APIS......183\n8 Designing GraphQL APIs\n185\n8.1\nIntroducing GraphQL\n186\n8.2\nIntroducing the products API\n189\n8.3\nIntroducing GraphQL’s type system\n192\nCreating property definitions with scalars\n192\n■Modeling \nresources with object types\n193\n■Creating custom scalars\n194\n8.4\nRepresenting collections of items with lists\n195\n8.5\nThink graphs: Building meaningful connections between \nobject types\n196\nConnecting types through edge properties\n196\n■Creating \nconnections with through types\n198",
      "content_length": 1452,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "CONTENTS\nxiii\n8.6\nCombining different types through unions \nand interfaces\n200\n8.7\nConstraining property values with enumerations\n202\n8.8\nDefining queries to serve data from the API\n203\n8.9\nAltering the state of the server with mutations\n206\n9 Consuming GraphQL APIs\n210\n9.1\nRunning a GraphQL mock server\n211\n9.2\nIntroducing GraphQL queries\n214\nRunning simple queries\n214\n■Running queries with \nparameters\n215\n■Understanding query errors\n215\n9.3\nUsing fragments in queries\n217\n9.4\nRunning queries with input parameters\n219\n9.5\nNavigating the API graph\n219\n9.6\nRunning multiple queries and query aliasing\n221\nRunning multiple queries in the same request\n221\n■Aliasing our \nqueries\n222\n9.7\nRunning GraphQL mutations\n225\n9.8\nRunning parameterized queries and mutations\n226\n9.9\nDemystifying GraphQL queries\n229\n9.10\nCalling a GraphQL API with Python code\n230\n10 Building GraphQL APIs with Python\n233\n10.1\nAnalyzing the API requirements\n234\n10.2\nIntroducing the tech stack\n234\n10.3\nIntroducing Ariadne\n235\n10.4\nImplementing the products API\n241\nLaying out the project structure\n241\n■Creating an entry point \nfor the GraphQL server\n242\n■Implementing query resolvers\n243\nImplementing type resolvers\n246\n■Handling query parameters\n252\nImplementing mutation resolvers\n256\n■Building resolvers for custom \nscalar types\n258\n■Implementing field resolvers\n262",
      "content_length": 1345,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "CONTENTS\nxiv\nPART 4\nSECURING, TESTING, AND DEPLOYING \nMICROSERVICE APIS ........................................267\n11 API authorization and authentication\n269\n11.1\nSetting up the environment for this chapter\n270\n11.2\nUnderstanding authentication and authorization \nprotocols\n271\nUnderstanding Open Authorization\n271\n■Understanding \nOpenID Connect\n276\n11.3\nWorking with JSON Web Tokens\n278\nUnderstanding the JWT header\n279\n■Understanding JWT \nclaims\n280\n■Producing JWTs\n282\n■Inspecting JWTs\n284\nValidating JWTs\n286\n11.4\nAdding authorization to the API server\n287\nCreating an authorization module\n287\n■Creating an \nauthorization middleware\n289\n■Adding CORS \nmiddleware\n292\n11.5\nAuthorizing resource access\n293\nUpdating the database to link users and orders\n294\nRestricting user access to their own resources\n297\n12 Testing and validating APIs\n302\n12.1\nSetting up the environment for API testing\n303\n12.2\nTesting REST APIs with Dredd\n304\nWhat is Dredd?\n304\n■Installing and running Dredd’s default \ntest suite\n305\n■Customizing Dredd’s test suite with hooks\n307\nUsing Dredd in your API testing strategy\n315\n12.3\nIntroduction to property-based testing\n315\nWhat is property-based testing?\n315\n■The traditional approach to \nAPI testing\n316\n■Property-based testing with Hypothesis\n318\nUsing Hypothesis to test a REST API endpoint\n319\n12.4\nTesting REST APIs with Schemathesis\n322\nRunning Schemathesis’s default test suite\n322\n■Using links to \nenhance Schemathesis’ test suite\n323\n12.5\nTesting GraphQL APIs\n327\nTesting GraphQL APIs with Schemathesis\n327\n12.6\nDesigning your API testing strategy\n329",
      "content_length": 1588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "CONTENTS\nxv\n13 Dockerizing microservice APIs\n331\n13.1\nSetting up the environment for this chapter\n332\n13.2\nDockerizing a microservice\n333\n13.3\nRunning applications with Docker Compose\n338\n13.4\nPublishing Docker builds to a container registry\n340\n14 Deploying microservice APIs with Kubernetes\n342\n14.1\nSetting up the environment for this chapter\n343\n14.2\nHow Kubernetes works: The “CliffsNotes” version\n344\n14.3\nCreating a Kubernetes cluster with EKS\n346\n14.4\nUsing IAM roles for Kubernetes service accounts\n350\n14.5\nDeploying a Kubernetes load balancer\n351\n14.6\nDeploying microservices to the Kubernetes cluster\n353\nCreating a deployment object\n354\n■Creating a service object\n357\nExposing services with ingress objects\n359\n14.7\nSetting up a serverless database with AWS Aurora\n361\nCreating an Aurora Serverless database\n361\n■Managing secrets \nin Kubernetes\n364\n■Running the database migrations and \nconnecting our service to the database\n367\n14.8\nUpdating the OpenAPI specification \nwith the ALB’s hostname\n370\n14.9\nDeleting the Kubernetes cluster\n372\nappendix A\nTypes of web APIs and protocols\n376\nappendix B\nManaging an API’s life cycle\n387\nappendix C\nAPI authorization using an identity provider\n391\nindex\n403",
      "content_length": 1213,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "xvi\npreface\nAPIs and microservices have taken the software industry by storm. Under the pressure\nof increasing software complexity and the need to scale, more and more organizations\nare migrating from monolithic to microservices architecture. O’Reilly’s “Microservices\nAdoption in 2020” report found that 77% of respondents had adopted microservices,\na trend that is expected to continue growing in the coming years.\n Using microservices poses the challenge of driving service integrations through\nAPIs. According to Nordic APIs, 90% of developers work with APIs and they spend\n30% of their time building APIs.1 The growth of the API economy has transformed\nthe way we build applications. Today, it’s more and more common to build products\nand services that are delivered entirely over APIs, such as Twilio and Stripe. Even tradi-\ntional sectors like banking and insurance are finding new lines of business by opening\ntheir APIs and integrating within the Open Banking ecosystem. The wide availability\nof API-first products means that we can focus on our core business capabilities when\nbuilding our own applications, while using external APIs to handle common tasks\nsuch as authenticating users and sending emails.\n It’s exciting to be part of this growing ecosystem. However, before we embrace\nmicroservices and APIs, we need to know how to architect microservices, how to\ndesign APIs, how to define an API strategy, how to make sure we deliver reliable inte-\ngrations, how to choose a deployment model, and how to protect our systems. In my\n1 J. Simpson, “20 Impressive API Economy Statistics” (https://nordicapis.com/20-impressive-api-economy-statistics/\n[accessed May 26, 2022]).",
      "content_length": 1684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "PREFACE\nxvii\nexperience, most organizations struggle with one or more of these questions, and a\nrecent report by IBM found that 31% of businesses haven’t adopted microservices due\nto lack of internal expertise.2 Equally, Postman’s 2022 State of the API Report found\nthat 14% of respondents experience API integration failures 11%–25% of the time\n(http://mng.bz/Xa9v), and according to Salt Security, 94% of organizations experi-\nenced API security incidents in 2022.3\n Many books address the problems mentioned in the previous paragraph, but they\ntypically do it from a highly specific point of view: some focus on architecture, others\non APIs, and yet others on security. I felt there’s a gap for a book that brings all these\nquestions together and addresses them with a practical approach: essentially, a book\nthat can get an average developer up and running quickly with the best practices,\nprinciples, and patterns for designing and building microservice APIs. I wrote this\nbook with that goal in mind.\n Over the past years, I’ve had the opportunity to work with different clients helping\nthem to architect microservices and deliver API integrations. Working on those proj-\nects gave me a vantage view into the major hurdles that development teams face when\nworking with microservices and APIs. As it turns out, both technologies are deceivingly\nsimple. A well-designed API is easy to navigate and consume, while well-architected\nmicroservices boost developer productivity and are easily scalable. On the other side\nof the spectrum, badly designed APIs are error prone and difficult to use, and badly\narchitected microservices result in so-called distributed monoliths.\n The obvious questions arise: How do you design good APIs? And how do you archi-\ntect loosely coupled microservices? This book will help you answer these questions and\nmore. You’ll also get your hands dirty building APIs and services, and you’ll learn how to\nsecure them, test them, and deploy them. The methods, patterns, and principles that I\nteach in this book are the outcome of many years of trials and experimentation, and I’m\nvery excited about sharing them with you. I hope you find this book a valuable resource\nin your journey towards becoming a better software developer and architect.\n2 “Microservices in the enterprise, 2021: Real benefits, worth the challenges,” (https://www.ibm.com/downloads/\ncas/OQG4AJAM [accessed 26th May 2022]).\n3 Salt Security, “State of API Security Q3 2022”, p. 4 (https://content.salt.security/state-api-report.html).",
      "content_length": 2532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "xviii\nacknowledgments\nWriting this book has been one of the most fascinating journeys in my career, and I\ncouldn’t have done it without the help and support of my family and an amazing\nteam of colleagues. The book is dedicated to my wonderful wife, Jiwon, without\nwhose constant encouragement and understanding I wouldn’t have been able to\ncomplete this book, and to our daughter, Ivy, who made sure I never had a dull\nmoment in my schedule.\n I have benefited enormously from the people who contributed ideas for the book,\nhelped me better understand the tools and protocols I use in it, and provided feedback\non various chapters and drafts. Special thanks go to Dmitry Dygalo, Kelvin Meeks,\nSebastián Ramírez Montaño, Chris Richardson, Jean Yang, Gajendra Deshpande, Oscar\nIslas, Mehdi Medjaoui, Ben Hutton, Andrej Baranovskij, Alex Mystridis, Roope\nHakulinen, Steve Ardagh-Walter, Kathrin Björkelund, Thomas Dean, Marco Antonio\nSanz, Vincent Vandenborne, and the amazing maintainers of Ariadne at Mirumee.\n Since 2020, I’ve presented drafts and ideas from the book at various conferences,\nincluding EuroPython, PyCon India, API World, API Specifications Conference, and\nvarious podcasts and meetups. I want to thank everyone who attended my presenta-\ntions and gave me valuable feedback. I also want to thank the attendants to my work-\nshops at microapis.io for their thoughtful comments on the book.\n I want to thank my acquisitions editor, Andy Waldron. Andy did a brilliant job\nhelping me get my book proposal in good shape and keeping the book focused on rel-\nevant topics. He also supported me tirelessly to promote the book and helped me to\nreach a wider audience.",
      "content_length": 1672,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "ACKNOWLEDGMENTS\nxix\n The book you now have in your hands is readable and understandable thanks to\nthe invaluable work of my editor, Marina Michaels, who went far and beyond to help\nme write a better book. She did an outstanding job helping me improve my writing\nstyle, and keeping me on track and motivated.\n I want to thank my technical editor, Nick Watts, who rightly pointed out many inac-\ncuracies and always challenged me to provide better explanations and illustrations,\nand my technical proofreader, Al Krinker, who diligently checked all the code listings\nand the GitHub repository for this book, making sure the code is correct and executes\nwithout issues.\n I also want to thank the rest of the Manning team who was involved in the produc-\ntion of this book, including Candace Gillhoolley, Gloria Lukos, Stjepan Jurekovic´,\nChristopher Kaufmann, Radmila Ercegovac, Mihaela Batinic´, Ana Romac, Aira Ducˇic´,\nMelissa Ice, Eleonor Gardner, Breckyn Ely, Paul Wells, Andy Marinkovich, Katie\nTennant, Michele Mitchell, Sam Wood, Paul Spratley, Nick Nason, and Rebecca\nRinehart. Thanks also go to Marjan Bace for betting on me and giving this book a\nchance. \n While working on this book, I had the opportunity to receive detailed and out-\nstanding feedback from the most amazing group of reviewers, including Alain\nLompo, Björn Neuhaus, Bryan Miller, Clifford Thurber, David Paccoud, Debmalya\nJash, Gaurav Sood, George Haines, Glenn Leo Swonk, Hartmut Palm, Ikechukwu\nOkonkwo, Jan Pieter Herweijer, Joey Smith, Juan Jimenez, Justin Baur, Krzysztof\nKamyczek, Manish Jain, Marcus Young, Mathijs Affourtit, Matthieu Evrin, Michael\nBright, Michael Rybintsev, Michal Rutka, Miguel Montalvo, Ninoslav Cerkez, Pierre-\nMichel Ansel, Rafael Aiquel, Robert Kulagowski, Rodney Weis, Sambasiva Andaluri,\nSatej Kumar Sahu, Simeon Leyzerzon, Steven K Makunzva, Stuart Woodward, Stuti\nVerma, and William Jamir Silva. I credit them all with much of the good content that\nmade its way into the book.\n Since the book went into MEAP, I’ve been blessed by the words of encouragement\nand feedback that many of my readers sent me through various channels, such as\nLinkedIn and Twitter. I was also lucky to converse with a brilliant community of read-\ners who actively participated in the book’s forum in Manning’s liveBook platform. I’m\nheartily grateful to all of you.\n This book wouldn’t have been possible without the tireless work of thousands of\nopen source contributors who created and maintain the amazing libraries that I use in\nthis book. I’m very thankful to all of you, and I hope my book helps to make your\namazing work more visible.\n Finally, thank you, the reader, for acquiring a copy of my book. I can only hope\nthat you find this book useful and informative and that you enjoy reading it as much\nas I enjoyed writing it. I love to hear from my readers, and I’d be delighted if you\nshare your thoughts on the book with me.",
      "content_length": 2919,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "xx\nabout this book\nThe goal of this book is to teach you how to build microservices and drive their inte-\ngrations using APIs. You’ll learn to design a microservices platform and to build REST\nand GraphQL APIs to enable communication between microservices. You’ll also learn\nto test and validate your microservice APIs, to secure them, and to deploy and operate\nthem in the cloud.\nWho should read this book?\nThis book is helpful for software developers who work with microservices and APIs.\nThe book uses a very practical approach, and nearly every chapter illustrates the\nexplanations with full coding examples. Therefore, hands-on developers who work\ndirectly with microservice APIs will find the book’s contents valuable.\n The coding examples are in Python; however, knowledge of the language isn’t nec-\nessary to be able to follow along with them. Before introducing new code, every con-\ncept is explained thoroughly.\n The book contains a lot of emphasis on design strategies, best practices, and devel-\nopment workflows, and therefore it’s also useful for CTOs, architects, and VPs of engi-\nneering who need to decide whether microservices are the right architectural solution\nfor them, or who need to choose between different API strategies and how to make\nthe integrations work.",
      "content_length": 1285,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "ABOUT THIS BOOK\nxxi\nHow this book is organized: A roadmap\nThe book is divided into four sections with a total of 14 chapters.\n Part 1 introduces the concepts of microservices and APIs, shows how to build a\nsimple API, and explains how to design a microservices platform:\nChapter 1 introduces the main concepts of the book: microservices and APIs. It\nexplains how microservices differ from monolithic architecture, and when it\nmakes sense to use monoliths versus microservices. It also explains what APIs\nare and how they help us drive integrations between microservices.\nChapter 2 offers a step-by-step guide for implementing APIs using Python’s pop-\nular FastAPI framework. You’ll learn to read an API specification and under-\nstand its requirements. You’ll also learn to build APIs in gradual steps, and how\nto test your data validation models.\nChapter 3 explains how to design a microservices platform. It introduces three\nfundamental microservice design principles, and it explains how to decompose\na system into microservices, using decomposition by business capability and\ndecomposition by subdomains.\nPart 2 explains how to design, document, and build REST APIs, and how to build a\nmicroservice:\nChapter 4 explains the design principles of REST APIs. It introduces the six\nconstraints of REST architecture and the Richardson Maturity Model, and\nthen moves on to explain how we leverage the HTTP protocol to design well-\nstructured and highly expressive REST APIs.\nChapter 5 explains how to document a REST API using the OpenAPI specifica-\ntion standard. You’ll learn the basics of JSON Schema syntax, how to define\nendpoints, how to model your data, and how to refactor your documentation\nwith reusable schemas.\nChapter 6 explains how to build REST APIs using two popular Python frame-\nworks: FastAPI and Flask. You’ll learn about the differences between the two\nframeworks, but you’ll also learn how the principles and patterns for building\nAPIs remain the same and transcend the implementation details of any techni-\ncal stack.\nChapter 7 explains fundamental principles and patterns for building microser-\nvices. It introduces the concept of hexagonal architecture, and it explains how\nto enforce loose coupling between the layers of an application. It also explains\nhow to implement database models using SQLAlchemy and how to manage\ndatabase migrations using Alembic.\nPart 3 explains how to design, consume, and build GraphQL APIs:\nChapter 8 explains how to design GraphQL APIs and how the Schema Defini-\ntion Language works. It introduces GraphQL’s built-in types, and it explains",
      "content_length": 2600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "ABOUT THIS BOOK\nxxii\nhow to define custom types. You’ll learn how to create relationships between\ntypes, and how to define queries and mutations.\nChapter 9 explains how to consume GraphQL APIs. You’ll learn to run a mock\nserver and how to explore GraphQL documentation using GraphiQL. You’ll\nlearn to run queries and mutations against a GraphQL server and how to\nparametrize your operations.\nChapter 10 explains how to build GraphQL APIs using Python’s Ariadne frame-\nwork. You’ll learn to leverage the API documentation to automatically load data\nvalidation models, and also to implement resolvers for custom types, queries,\nand mutations.\nPart 4 explains how to test, secure, and deploy your microservice APIs:\nChapter 11 explains how to add authentication and authorization to your APIs\nusing standard protocols such as OpenID Connect (OIDC) and Open Authori-\nzation (OAuth) 2.1. You’ll learn how to produce and validate JSON Web Tokens\n(JWTs) and how to create an authorization middleware for your APIs.\nChapter 12 explains how to test and validate your APIs. You’ll learn what property-\nbased testing is and how to use it to test your APIs, and you’ll also learn to use\nAPI testing automation frameworks like Dredd and schemathesis.\nChapter 13 explains how to Dockerize your microservice APIs, how to run them\nlocally using Docker Compose, and how to publish your Docker builds to AWS\nElastic Container Registry (ECR).\nChapter 14 explains how to deploy your microservice APIs to AWS using Kuber-\nnetes. You’ll learn to create and operate a Kubernetes cluster using AWS EKS,\nhow to launch an Aurora serverless database into a secure network, how to\ninject application configuration securely using envelope encryption, and how\nto set up your services to operate at scale.\nAll chapters have a common theme: building components of a fictitious, on-demand\ncoffee delivery platform called CoffeeMesh. We introduce CoffeeMesh in chapter 1,\nand in chapter 3, we break the platform down into microservices. Therefore, I recom-\nmend reading chapters 1 and 3 to get a better understanding of the examples intro-\nduced in later chapters. Otherwise, every part of the book is fairly independent, and\neach chapter is pretty self-contained. For example, if you want to learn how to design\nand build REST APIs, you can jump straight to part 2, and if your interest lies with\nGraphQL APIs, you can focus on part 3. Equally, if you want to learn to add authenti-\ncation and authorization to your APIs, you can jump straight into chapter 11, or if you\nwant to learn how to test APIs, you can go directly to chapter 12.\n There’re some cross-references between chapters: for example, chapter 12 refer-\nences the API implementations from parts 2 and 3, but if you’re comfortable building\nAPIs, you should be able to skip directly to chapter 12. The same is true for the other\nchapters in part 4.",
      "content_length": 2884,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "ABOUT THIS BOOK\nxxiii\nAbout the code\nThis book contains many examples of source code both in numbered listings and in\nline with normal text. In both cases, source code is formatted in a fixed-width font\nlike this to separate it from ordinary text. Sometimes code is also in bold to high-\nlight code that has changed from previous steps in the chapter, such as when a new\nfeature adds to an existing line of code.\n In many cases, the original source code has been reformatted; we’ve added line\nbreaks and reworked indentation to accommodate the available page space in the\nbook. In some cases, even this was not enough, and listings include line-continuation\nmarkers (➥). Additionally, comments in the source code have often been removed\nfrom the listings when the code is described in the text. Code annotations accompany\nmany of the listings, highlighting important concepts.\n Except for chapters 1, 3, and 4, every chapter of the book is full of coding exam-\nples that illustrate every new concept and pattern introduced to the reader. Most of\nthe coding examples are in Python, except in chapters 5, 8, and 9, which focus on API\ndesign, and therefore contain examples in OpenAPI/JSON Schema (chapter 5) and\nthe Schema Definition Language (chapters 8 and 9). All the code is thoroughly\nexplained, and therefore it should be accessible to all readers, including those who\ndon’t know Python. \n You can get executable snippets of code from the liveBook (online) version of this\nbook at https://livebook.manning.com/book/microservice-apis. The complete code\nfor the examples in the book is available for download from the Manning website at\nwww.manning.com, and from a GitHub repository dedicated to this book at: https://\ngithub.com/abunuwas/microservice-apis. Every chapter has a corresponding folder\nin the GitHub repo, such as ch02 for chapter 2. Unless otherwise specified, all file\nreferences in each chapter are relative to their corresponding folder in GitHub.\nFor example, in chapter 2, orders/app.py refers to the ch02/orders/app.py file in\nGitHub.\n The GitHub repository for this book shows the final state of the code in every\nchapter. Some chapters show how to build features progressively, in iterative steps. In\nthose cases, the version of the code you’ll find on GitHub matches the final version of\nthe code in the chapter.\n The Python code examples in the book have been tested with Python 3.10, although\nany version of Python upwards of 3.7 should work just the same. The code and the com-\nmands that I use throughout the book have been tested on a Mac machine, but they\nshould work without problems on Windows and Linux as well. If you work on Windows,\nI recommend you use a POSIX-compatible terminal, such as Cygwin.\n I’ve used Pipenv to manage dependencies in every chapter. In each chapter’s\nfolder, you’ll find Pipfile and Pipfile.lock files that describe the exact dependencies\nthat I used to run the code examples. To avoid problems running the code, I recom-\nmend you download those files at the start of every chapter, and install the dependen-\ncies from them.",
      "content_length": 3085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "ABOUT THIS BOOK\nxxiv\nliveBook discussion forum\nPurchase of Microservice APIs includes free access to liveBook, Manning’s online reading\nplatform. Using liveBook’s exclusive discussion features, you can attach comments to\nthe book globally or to specific sections or paragraphs. It’s a snap to make notes for your-\nself, ask and answer technical questions, and receive help from the author and other\nusers. To access the forum, go to https://livebook.manning.com/book/microservice\n-apis/discussion. You can also learn more about Manning’s forums and the rules of\nconduct at https://livebook.manning.com/discussion.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the author can take\nplace. It is not a commitment to any specific amount of participation on the part of\nthe author, whose contribution to the forum remains voluntary (and unpaid). We\nsuggest you try asking him some challenging questions lest his interest stray! The\nforum and the archives of previous discussions will be accessible from the publisher’s\nwebsite for as long as the book is in print.\nOther online resources\nIf you want to learn more about microservice APIs, you can check out my blog, https://\nmicroapis.io/blog, which contains additional resources that complement the lessons\nof this book. On the same website, I also keep an up-to-date list of workshops and sem-\ninars that I organize frequently, which also complement this book.",
      "content_length": 1498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "xxv\nabout the author\nJOSÉ HARO PERALTA is a software and architecture consultant.\nWith over 10 years of experience, José has helped organizations\nbig and small to build complex systems, architect microservice\nplatforms, and deliver API integrations. He’s also the founder\nof microapis.io, a company that provides software consulting\nand training services. Recognized as a thought leader in the\nfields of cloud computing, DevOps, and software automation,\nJosé speaks regularly at international conferences and frequently\norganizes public workshops and seminars.",
      "content_length": 560,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "xxvi\nabout the cover illustration\nThe figure on the cover of Microservice APIs is captioned “L’invalide,” or “The Dis-\nabled,” and depicts a wounded French soldier who was a resident at the Hôtel\nnational des Invalides, or National House of the Disabled. This image is taken from a\ncollection by Jacques Grasset de Saint-Sauveur, published in 1797. Each illustration is\nfinely drawn and colored by hand. \n In those days, it was easy to identify where people lived and what their trade or sta-\ntion in life was just by their dress. Manning celebrates the inventiveness and initiative\nof the computer business with book covers based on the rich diversity of regional cul-\nture centuries ago, brought back to life by pictures from collections such as this one.",
      "content_length": 757,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "Part 1\nIntroducing\nMicroservice APIs\nMicroservices are an architectural style in which components of a system\nare designed as standalone and independently deployable applications. The\nconcept of microservices has been around since the early 2000s, and since the\n2010s it has gained in popularity. Nowadays, microservices are a popular choice\nfor building modern websites. As you’ll learn in chapter 1, microservices allow\nyou to leverage the power of distributed applications, scale components more\neasily, and release faster.\n However, for all their benefits, microservices also come with challenges of\ntheir own. They bring a substantial infrastructure overhead, and they’re more\ndifficult to monitor, operate, and trace. When working with microservices, the\nfirst challenge is to get their design right, and in chapter 3 you’ll learn several\nprinciples and strategies that will help you build robust microservices.\n Microservices collaborate through APIs, and in this book, you’ll learn to\ndesign and build REST and GraphQL APIs for your microservices. Chapter 2\ngives you a taste of building a REST API, and in the second part of this book,\nyou’ll learn additional patterns and principles to build robust REST APIs. The\nmost challenging aspect of working with APIs is ensuring that both the API cli-\nent and the API server follow the API specification, and in chapter 1 you’ll learn\nabout documentation-driven development and the importance of starting the\nAPI journey with a good and well-documented design.",
      "content_length": 1512,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "2\nPART 1\nIntroducing Microservice APIs\n The first part of this book teaches you foundational patterns and principles for\nbuilding microservices and driving their integrations with APIs. In the rest of this\nbook, we build on top of the concepts introduced here, and you’ll learn how to build\nrobust APIs, how to test them, how to protect them, and how to deploy your microser-\nvice APIs to the cloud. Our intrepid journey is just about to begin!",
      "content_length": 444,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "3\nWhat are\nmicroservice APIs?\nThis chapter defines the most important concepts in this book: microservices and\nAPIs. Microservices are an architectural style in which components of a system are\ndesigned as independently deployable services, and APIs are the interfaces that allow\nus to interact with those services. We will see the defining features of microservices\narchitecture and how they compare with monolithic applications. Monolithic appli-\ncations are structured around a single code base and deployed in a single build. \n We’ll discuss the benefits and the disadvantages of microservices architecture.\nThe last part of this chapter talks about the most important challenges that we face\nwhen designing, implementing, and operating microservices. This discussion is not\nto deter you from embracing microservices, but so that you can make an informed\ndecision about whether microservices are the right choice of architecture for you.\nThis chapter covers\nWhat microservices are and how they compare \nwith monolithic applications\nWhat web APIs are and how they help us drive \nintegrations between microservices\nThe most important challenges of developing \nand operating microservices",
      "content_length": 1192,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "4\nCHAPTER 1\nWhat are microservice APIs?\n1.1\nWhat are microservices?\nIn this section, we define what microservices architecture is, and we analyze how\nmicroservices compare with monolithic applications. We’ll look into the benefits and\nchallenges of each architectural pattern. Finally, we’ll also take a brief look at the his-\ntorical developments that led to the emergence of modern microservices architecture.\n1.1.1\nDefining microservices\nSo, what are microservices? Microservices can be defined in different ways, and,\ndepending on which aspect of microservices architecture we want to emphasize,\nauthors provide slightly different yet related definitions of the term. Sam Newman,\none of the most influential authors in the field of microservices, provides a minimal\ndefinition: “Microservices are small, autonomous services that work together.”1 \n This definition emphasizes the fact that microservices are applications that run\nindependently of each other yet can collaborate in the performance of their tasks.\nThe definition also emphasizes that microservices are “small.” In this context, “small”\ndoesn’t refer to the size of the microservices’ code base, but to the idea that micro-\nservices are applications with a narrow and well-defined scope, following the Single\nResponsibility Principle of doing one thing and doing it well.\n A seminal article written by James Lewis and Martin Fowler provides a more detailed\ndefinition. They define microservices as an architectural style with “an approach to\ndeveloping a single application as a suite of small services, each running in its own\nprocess and communicating with lightweight mechanisms, often an HTTP resource\nAPI” (https://martinfowler.com/articles/microservices.html). This definition empha-\nsizes the autonomy of the services by stating that they run in independent processes.\nLewis and Fowler also highlight that microservices have a narrow scope of responsibil-\nities by saying that they are “small,” and they explicitly describe how microservices\ncommunicate through lightweight protocols, such as HTTP.\nDEFINITION\nA microservice is an architectural style in which components of a\nsystem are designed as independently deployable services. Microservices are\ndesigned around well-defined business subdomains, and they talk to each\nother using lightweight protocols, such as HTTP.\nFrom the previous definitions, we can see that microservices can be defined as an\narchitectural style in which services are components that perform a small and clearly\ndefined set of related functions. As you can see in figure 1.1, this definition means\nthat a microservice is designed and built around a specific business subdomain, for\nexample, processing payments, sending emails, or handling orders from a customer.\n Microservices are deployed as independent processes, typically running in indepen-\ndent environments, and expose their capabilities through well-defined interfaces. In\nthis book, you will learn to design and build microservices that expose their capabilities\n1 Sam Newman, Building Microservices (O’Reilly, 2015), p. 2.",
      "content_length": 3087,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "5\n1.1\nWhat are microservices?\nthrough web APIs, though other types of interfaces are also possible, such as messag-\ning queues.2\n1.1.2\nMicroservices vs. monoliths\nNow that we know what microservices are, let’s see how they compare with the mono-\nlithic application pattern. In contrast with microservices, a monolith is a system where\nall functionality is deployed together as a single build and runs in the same process.\nFor example, figure 1.2 shows a food delivery application with four services: a pay-\nments service, an orders service, a delivery service, and a customer support service.\nSince the application is implemented as a monolith, all functionality is deployed\ntogether. We can run multiple instances of a monolithic application and have them\nrun in parallel for redundancy and scalability purposes, but it’s still the whole applica-\ntion running in each process.\nDEFINITION\nA monolith is an architectural pattern in which the whole applica-\ntion is deployed as a single build.\nIn some situations, the monolith is the right choice of architecture. For example, we’d\nuse a monolith when our code base is small and it isn’t expected to grow very large.3\nMonoliths also come with advantages. First, having the whole implementation in the\nsame code base makes it easier to access data and logic from different subdomains.\nAnd because everything runs within the same process, it is easy to trace errors through\nthe application: you only need to place a few breakpoints in different parts of your\ncode, and you will get a detailed picture of what happens when something goes\nwrong. Besides, because all the code falls within the scope of the same project, you\n2 For a comprehensive view of the different interfaces that can be used to enable communication between\nmicroservices, see Chris Richardson, Microservices Patterns (Manning, 2019).\n3 For a thorough analysis of strategic architectural decisions around monoliths and microservices, see Vernon,\nVaughn and Tomasz Jaskula, Strategic Monoliths and Microservices (Addison-Wesley, 2021).\nPayments service\nOrders service\nEmail service\nFood\nelivery\npp\nd\na\nFigure 1.1\nIn microservices architecture, every service implements a specific business \nsubdomain and is deployed as an independent component that runs in its own process.",
      "content_length": 2286,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "6\nCHAPTER 1\nWhat are microservice APIs?\ncan leverage the productivity features of your favorite development editor when con-\nsuming functionality from a different subdomain.\n However, as the application grows and becomes more complex, this type of architec-\nture shows limitations. This happens when the code base grows to a point where it\nbecomes difficult to manage, and when finding your way through the code becomes\narduous. Additionally, being able to reuse code from other subdomains within the same\nproject often leads to tight coupling among components. Tight coupling happens when\na component depends on the implementation details of another piece of code.\n The bigger the monolith, the longer it takes to test it. Every part of the monolith\nmust be tested, and as we add new features to it, the test suite grows larger. Conse-\nquently, deployments become slower and encourage developers to pile up changes\nwithin the same release, which makes releases more challenging. Because many\nchanges are released together, if a new bug is introduced in the release, it is often dif-\nficult to spot the specific change that caused the bug and roll it back. And because the\nwhole application runs within the same process, when you scale the resources for one\ncomponent, you are scaling for the whole application. Long story short, code changes\nbecome increasingly risky and deployments become more difficult to manage. How\ncan microservices help us address these issues?\n Microservices address some of the issues associated with monolithic applications\nby enforcing strict boundaries separating components. When you implement an\nFood delivery app\nPayments service\nOrders service\nDelivery service\nCustomer support\nservice\nFood delivery app\nPayments\nservice\nOrders service\nDelivery service\nCustomer support\nservice\nFood delivery app\nPayments\nservice\nOrders service\nDelivery service\nCustomer support\nservice\nFigure 1.2\nIn a monolithic application, all functionality is deployed together as a single build to each \nserver.",
      "content_length": 2017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "7\n1.1\nWhat are microservices?\napplication using microservices, each microservice runs in a different process, often\nin different servers or virtual machines, and can have a completely different deploy-\nment model. As a matter of fact, they can be written in completely different program-\nming languages (that does not mean they should!).\n Because microservices contain smaller code bases than a monolith, and because\ntheir logic is self-contained and defined within the scope of a specific business subdo-\nmain, it is easier to test them, and their test suites run faster. Because they do not have\ndependencies with other components of the platform at the code level (except per-\nhaps for some shared libraries), their code is clearer, and it is easier to refactor them.\nThis means the code can get better over time and become more maintainable. Con-\nsequently, we can make small changes to the code and release more often. Smaller\nreleases are more controllable, and if we spot a bug, the releases are easier to roll back.\nI’d like to emphasize that microservices are not a panacea. As we will see in section 1.3,\nmicroservices also have limitations and bring challenges of their own.\n Now that we know what microservices are and how they compare with monolithic\napplications, let’s take a step back and see what developments led to the emergence of\nthis type of architecture.\n1.1.3\nMicroservices today and how we got here\nIn many ways, microservices are not new.4 Companies were implementing and deploy-\ning components as independent applications well before the concept of microser-\nvices became popular. They just did not call it microservices. Werner Vogels, CTO of\nAmazon, explains how Amazon started to experiment with this type of architecture in\nthe early 2000s. By that time, the code base for the Amazon website had grown into a\ncomplex system without a clear architectural pattern, where making new releases and\nscaling the system had become serious pain points. To combat these issues, they\ndecided to look for independent pieces of logic within the code and separate them\nout into independently deployable components, with an API in front of them. As part\nof this process, they also identified the data that belongs to those components and\nmade sure that other parts of the system could not access the data except through an\nAPI. They called this new type of architecture service-oriented architecture (https://vimeo\n.com/29719577). Netflix also pioneered this type of architectural style at scale, and\nthey referred to it as “fine-grained Service Oriented Architecture.”5\n \n \n4 For a more comprehensive analysis of the history of microservices architecture and its precursors, see Nicola\nDragoni et al, “Microservices: Yesterday, Today, and Tomorrow,” Present and Ulterior Software Engineering (Springer,\n2017), pp. 195–216.\n5 Allen Wang and Sudhir Tonse, “Announcing Ribbon: Tying the Netflix Mid-Tier Services Together,” Netflix\nTechnology Blog, January 18, 2013, https://netflixtechblog.com/announcing-ribbon-tying-the-netflix-mid-tier\n-services-together-a89346910a62. For an excellent discussion of the difference between service-oriented archi-\ntecture (SOA) and microservices architecture, see Richardson, Microservices Patterns, pp. 13–14.",
      "content_length": 3261,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "8\nCHAPTER 1\nWhat are microservice APIs?\n The term microservice grew in popularity in the early 2010s to describe this type of\narchitecture. For example, James Lewis used this concept in a presentation at the 33rd\nDegree conference in Krakow in 2012, under the title “Micro-Services—Java, the Unix\nway” (https://vimeo.com/74452550). In 2014 the concept was consolidated with a\npaper written by Martin Fowler and James Lewis about the architectural features of\nmicroservices (https://martinfowler.com/articles/microservices.html), as well as the\npublication of Newman’s influential book Building Microservices.\n Today, microservices are a widely used architectural style. Most companies in which\ntechnology plays an important role are already using microservices or moving toward\nits adoption. It is also common for startups to begin implementing their platform\nusing a microservices approach. However, microservices are not for everyone, and\nalthough they bring substantial benefits, as we have shown, they also carry consider-\nable challenges, as we will see in section 1.3.\n1.2\nWhat are web APIs?\nIn this section, we will explain web APIs. You will learn that a web API is a specific\ninstance of the more general concept of an application programming interface (API).\nIt is important to understand that an API is just a layer on top of an application, and\nthat there are different types of interfaces. For this reason, we will begin this section\nby defining what an API is, and then we will move on to explaining how APIs help us\ndrive integrations between microservices.\n1.2.1\nWhat is an API?\nAn API is an interface that allows us to programmatically interact with an application.\nProgrammatic interfaces are those we can use from our code or from the terminal, as\nopposed to graphic interfaces, in which we use a user interface to interact with the\napplication. There are multiple types of application interfaces, such as command-line\ninterfaces (CLIs; interfaces that allow you to use an application from a terminal),\ndesktop UI interfaces, web UI interfaces, or web API interfaces. As you can see in fig-\nure 1.3, an application can have one or more of these interfaces.\nTo illustrate this idea, think of the popular client URL (cURL). cURL is a CLI to the\nlibcurl library. libcurl implements functionality that allows us to interact with URLs,\nWeb application\nprogramming interface\nCommand- ine\nl\ninterface\nWeb user\ninterface\nDesktop user\ninterface\nApplication\nlogic\nFigure 1.3\nAn application can \nhave multiple interfaces, such \nas a web API, a CLI, a web UI, \nand a desktop UI.",
      "content_length": 2585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "9\n1.2\nWhat are web APIs?\nwhile cURL exposes those capabilities through a CLI. For example, we can use cURL\nto send a GET request to a URL:\n$ curl -L http:/ /www.google.com\nWe can also use cURL with the -O option in order to download the contents of a URL\nto a file:\n$ curl -O http:/ /www.gnu.org/software/gettext/manual/gettext.html\nThe libcurl library sits behind the cURL CLI, and nothing prevents us from access-\ning it directly through the source code (if you are curious, you can pull it from\nGithub: https://github.com/curl/curl) and building additional types of interfaces for\nthis application.\n1.2.2\nWhat is a web API?\nNow that we understand what an API is, we will explain the defining features of a web\nAPI. A web API is an API that uses the Hypertext Transfer Protocol (HTTP) protocol\nto transport data. HTTP is the communication protocol that underpins the internet,\nand it allows us to exchange different kinds of media types, such as text, images, video,\nand JSON, over a network. HTTP uses the concept of a Uniform Resource Locator\n(i.e., URL) to locate resources on the internet, and it has features that can be lever-\naged by API technologies to enhance the interaction with the server, such as request\nmethods (e.g., GET, POST, PUT) and HTTP headers. Web APIs are implemented\nusing technologies such as SOAP, REST, GraphQL, gRPC, and others that are dis-\ncussed in more detail in appendix A.\n1.2.3\nHow do APIs help us drive microservices integrations?\nMicroservices communicate with each other using APIs, and therefore APIs represent\nthe interfaces to our microservices. APIs are documented using standard protocols.\nThe API documentation tells us exactly what we need to do to interact with the micro-\nservice and what kind of responses we can expect from it. The better the API docu-\nmentation, the clearer it is for the API consumer how the API works. In that sense, as\nyou can see in figure 1.4, API documentation represents a contract between services:\nFigure 1.4\nAPI specifications \nrepresent a contract between \nthe API server and the API \nclient. As long as both the \nclient and the server follow \nthe specification, the API \nintegration will work.",
      "content_length": 2175,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "10\nCHAPTER 1\nWhat are microservice APIs?\nas long as both the client and the server follow the API documentation, communica-\ntion will work as expected.\n Fowler and Lewis popularized the idea that the best strategy for integrating micro-\nservices is by exposing smart endpoints and communicating through dumb pipes (https://\nmartinfowler.com/articles/microservices.html). This idea is inspired by the design\nprinciples of Unix systems, which establish that\nA system should be made up of small, independent components that do only\none thing.\nThe output for every component should be designed in such a way that it can\neasily become the input for another component.\nUnix programs communicate with each other using pipelines, which are simple mech-\nanisms for passing messages from one application to another. To illustrate this pro-\ncess, think of the following chain of commands, which you can run from the terminal\nof a Unix-based machine (e.g., a Mac or Linux computer):\n$ history | less\nThe history command shows you the list of all commands you have run using your\nBash profile. The list of commands can be long, so you may want to paginate history’s\noutput using the less command. To pass data from one command to the another, use\nthe pipe character (|), which instructs the shell to capture the output from the history\ncommand and pipe it as the input of the less command. We say that this type of\npipe is “dumb” because its only job is passing messages from one process to another.\nAs you can see in figure 1.5, web APIs exchange data through HTTP. The data trans-\nport layer knows nothing about the specific API protocol we are using, and therefore\nit represents our “dumb pipe,” while the API itself contains all the necessary logic to\nprocess the data.\nGET /orders\nHost: Coﬀeemesh.com\nConnection: Keep-alive\nContent-type: Application/json\nContent-length: 345\nGET /orders\nOrders API\nTransport layer: TCP / UDP\n(packets)\nAPI\nrequest\nHTTP\nprotocol\nTransport\nlayer\nFigure 1.5\nMicroservices \ncommunicate over APIs using \na data transport layer, such as \nHTTP over TCP.",
      "content_length": 2073,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "11\n1.3\nChallenges of microservices architecture\nAPIs must be stable, and behind them you can change the internal implementations\nof any service provided they comply with the API documentation. This means that the\nconsumer of an API must be able to continue calling the API in the exact way as\nbefore, and it must get the same responses. This leads to another important concept\nin microservices architecture: replaceability.6 The idea is that you should be able to\ncompletely replace the code base that lies behind an endpoint, yet the endpoint, and\ntherefore communication across services, will still work. Now that we understand what\nAPIs are and how they help us drive integrations between services, let’s look at the\nmost important challenges posed by microservices.\n1.3\nChallenges of microservices architecture\nAs we saw in section 1.1.2, microservices bring substantial benefits. However, they also\ncome with significant challenges. In this section, we discuss the most important chal-\nlenges that microservices pose, which we classify into five main categories:\nEffective service decomposition\nMicroservices integration tests\nHandling service unavailability\nTracing distributed transactions\nIncreased operational complexity and infrastructure overhead\nAll the problems and difficulties that we discuss in this section can be addressed with\nspecific patterns and strategies, some of which we detail over the course of this book.\nYou’ll also find references to other resources that deal with these issues in depth. The\nidea here is to make you aware that microservices are not a magical cure for all the\nproblems that monolithic applications present.\n1.3.1\nEffective service decomposition \nOne of the most important challenges when designing microservices is service decom-\nposition. We must break down a platform into loosely coupled yet sufficiently inde-\npendent components with clearly defined boundaries. You can tell whether you have\nunreasonable coupling between your services if you find yourself changing one service\nwhenever you change another service. In such situations, either the contract between\nservices is not resilient, or there are enough dependencies between both compo-\nnents to justify merging them. Failing to break down a system into independent\nmicroservices can result in what Chris Richardson, author of Microservices Patterns,\ncalls a distributed monolith, a situation where you combine all the problems of mono-\nlithic architectures with all the problems of microservices, without enjoying the bene-\nfits of any of them. In chapter 3, you’ll learn useful design patterns and service\ndecomposition strategies that will help you break down a system into microservices.\n6 Newman, Building Microservices, pp. 7–8.",
      "content_length": 2747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "12\nCHAPTER 1\nWhat are microservice APIs?\n1.3.2\nMicroservices integration tests\nIn section 1.1.2, we said that microservices are usually easier to test, and that their test\nsuites generally run faster. Microservices integration tests, however, can be signifi-\ncantly more difficult to run, especially in cases where a single transaction involves col-\nlaboration among several microservices. When your whole application runs within the\nsame process, it is fairly easy to test the integration between different components,\nand most of it will simply require well-written unit tests. In a microservices context, to\ntest the integration among multiple services you need to be able to run all of them\nwith a setup similar to your production environment.\n You can use different strategies to test microservices integrations. The first step is\nmaking sure that each service has a well-documented and correctly implemented\nAPI. You can test the API implementation against the API specification using tools\nlike Dredd and Schemathesis, as you’ll learn in chapter 12. You must also ensure\nthat the API client is consuming the API exactly as dictated by the API documenta-\ntion. You can write unit tests for the API client using the API documentation to gener-\nate mocked responses from the service.7 Finally, none of these tests will be sufficient\nwithout a full-blown end-to-end test that runs the actual microservices making calls to\neach other.\n1.3.3\nHandling service unavailability\nWe have to make sure that our applications are resilient in the face of service unavail-\nability, connections and request timeouts, erroring requests, and so on. For example,\nwhen we place an order through a food delivery application such as Uber Eats, Deliv-\nery Hero, or Deliveroo, a chain of requests between services unfolds to process and\ndeliver the order, and any of those requests can fail at any point. Let’s take a high-level\nview of the process that takes place when a user places an order (see figure 1.6 for an\nillustration of the chain of requests):\n1\nA customer places an order and pays for it. The order is placed using the orders\nservice, and to process the payment, the orders service works together with the\npayments service.\n2\nIf payment is successful, the orders service makes a request to the kitchen ser-\nvice to schedule the order for production.\n3\nOnce the order has been produced, the kitchen service makes a request to the\ndelivery service to schedule the delivery.\nIn this complex chain of requests, if one of the services involved fails to respond as\nexpected, it can trigger a cascading error through the platform that leaves the order\nunprocessed or in an inconsistent state. For this reason, it is important to design\n7 To learn more about API development workflows and how to use API mock servers to build the client, check\nout my presentation “API Development Workflows for Successful Integrations,” Manning API Conference,\nAugust 3, 2021, https://youtu.be/SUKqmEX_uwg.",
      "content_length": 2978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "13\n1.3\nChallenges of microservices architecture\nmicroservices so that they can deal reliably with failing endpoints. Our end-to-end tests\nshould consider these scenarios and test the behavior of our services in those situations.\n1.3.4\nTracing distributed transactions\nCollaborating services must sometimes handle distributed transactions. Distributed\ntransactions are those that require the collaboration of two or more services. For\nexample, in a food delivery application, we want to keep track of the existing stock of\ningredients so that our catalogue can accurately reflect product availability. When a\nuser places an order, we want to update the stock of ingredients to reflect the new\navailability. Specifically, we want to update the stock of ingredients once the payment\nhas been successfully processed. As you can see in figure 1.7, the successful processing\nof an order involves the following actions:\n1\nProcess the payment.\n2\nIf payment is successful, update the order’s status to indicate that it’s in progress.\n3\nInterface with the kitchen service to schedule the order for production.\n4\nUpdate the stock of ingredients to reflect their current availability.\nOrders service\nKitchen service\n2. Upon successful payment, the orders service schedules the order.\nHandle error\nOrders service\nPayments service\n1. The customer places an order and pays for it.\nHandle error\nKitchen service\nDelivery service\n3. Once the order is ready, the kitchen arranges its delivery.\nHandle error\nFigure 1.6\nMicroservices must be resilient to events such as service \nunavailability, request timeouts, and processing errors from other \nservices and either retry the requests or come back to the user with \na meaningful response.",
      "content_length": 1718,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "14\nCHAPTER 1\nWhat are microservice APIs?\nAll of these operations are related, and they must be orchestrated so that they either\nall succeed or fail together. We can’t have an order successfully paid without correctly\nupdating its status, and we shouldn’t schedule its production if payment fails. We may\nwant to update the availability of the ingredients at the time of making the order, and\nif payment fails later on, we want to make sure we rollback the update. If all these\nactions happen within the same process, managing the flow is straightforward, but\nwith microservices we must manage the outcomes of various processes. When using\nmicroservices, the challenge is ensuring that we have a robust communication process\namong services so that we know exactly what kind of error happens when it does, and\nwe take appropriate measures in response to it. \n In the case of services that work collaboratively to serve certain requests, you also\nmust be able to trace the cycle of the request as it goes across the different services to\nbe able to spot errors during the transaction. To gain visibility of distributed transac-\ntions, you’ll need to set up distributed logging and tracing for your microservices.\nYou can learn more about this topic from Jamie Riedesel’s Software Telemetry (Man-\nning, 2021).\n1.3.5\nIncreased operational complexity and infrastructure overhead\nAnother important challenge that comes with microservices is the increased opera-\ntional complexity and operational overhead they add to your platform. When the\nwhole backend of your website runs within a single application build, you only need to\ndeploy and monitor one process. When you have a dozen microservices, every service\nmust be configured, deployed, and managed. And this includes not only the provi-\nsioning of servers to deploy the services, but also log aggregation streams, monitoring\nProducts and\ningredients service\nKitchen service\nOrders\nservice\nPayments\nservice\nCustomer\nPayment successful\nPayment failed\nProcess the payment.\nUpdate the\norder’s state.\nNotify the\ncustomer.\nSchedule the\norder for\nproduction.\nUpdate the\nstock of\ningredients.\nFigure 1.7\n A distributed transaction involves collaboration among multiple services. If any of these services \nfails, we must be able to handle the failure and provide a meaningful response to the user.",
      "content_length": 2336,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "15\n1.4\nIntroducing documentation-driven development\nsystems, alerts, self-recovery mechanisms, and so on. As you’ll learn in chapter 3, every\nservice owns its own database, which means they also require multiple database setups\nwith all the features needed to operate at scale. And it is not unusual that a new deploy-\nment changes the endpoint for a microservice, whether it’s the IP, the base URL, or\na specific path within a generic URL, which means its consumers must be notified of\nthe changes.\n When Amazon first started their journey toward a microservices architecture, they\ndiscovered that development teams would spend about 70% of their time managing\ninfrastructure (https://vimeo.com/29719577 at 07:53). This is a very real risk that you\nface if you do not adopt best practices for infrastructure automation from the begin-\nning. And even if you do, you are likely to spend a significant amount of time develop-\ning custom tooling to manage your services effectively and efficiently.\n1.4\nIntroducing documentation-driven development\nAs we explained in section 1.2.3, the success of an API integration depends on good\nAPI documentation, and in this section, we introduce an API development workflow\nthat puts documentation at the forefront of API development. As you can see in fig-\nure 1.8, documentation-driven development is an approach to building APIs that works\nin three stages:\n1\nYou design and document the API.\n2\nYou build the API client and the API server following the documentation.\n3\nYou test both the API client and the API server against the documentation.\nLet’s dive into each of these points. The first step involves designing and documenting\nthe specification. We build APIs for others to consume, so before we build the API, we\nmust produce an API design that meets the needs of our API clients. Just as we involve\nusers when we design an application’s user interface (UI), we must also engage with\nour API consumers when we design the API.\n Good API design delivers good developer experience, while good API documenta-\ntion helps to deliver successful API integrations. What is API documentation? API doc-\numentation is a description of the API following a standard interface description\nlanguage (IDL), such as OpenAPI for REST APIs and the Schema Definition Lan-\nguage (SDL) for GraphQL APIs. Standard IDLs have ecosystems of tools and frame-\nworks that make it easier to build, test, and visualize our APIs, and therefore it’s worth\ninvesting time in studying them. In this book, you’ll learn to document your APIs with\nOpenAPI (chapter 5) and the SDL (chapter 8).\n Once we have produced a documented API design, we move on to the second\nstage, which consists of building the API server and the API client against the API\ndocumentation. In chapters 2 and 6, you’ll learn to analyze the requirements of an\nOpenAPI specification and to build an API application against them, and in chap-\nter 10, we’ll apply the same approach to GraphQL APIs. API client developers can",
      "content_length": 3000,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "16\nCHAPTER 1\nWhat are microservice APIs?\nalso leverage the API documentation to run API mock servers and test their code\nagainst them.8\n The final stage involves testing our implementation against the API documenta-\ntion. In chapter 12, you’ll learn to use automated API testing tools such as Dredd and\nSchemathesis, which can generate a solid battery of tests for your API. Running Dredd\nand Schemathesis in combination with your application unit test suite will give you\nconfidence that your API implementation works as it should. You should run these\ntests in your continuous integration server to make sure you don’t release any code\nthat breaks the contract with the API documentation.\n8 To learn how API server and client developers can leverage API documentation in their software development\nprocess, check out my talk “Leveraging API Documentation to Deliver Reliable API Integrations,” API Specifi-\ncations Conference, September 28–29, 2021, https://youtu.be/kAWvM-CVcnw.\n1. API design and\ndocumentation\n2. Build the client\nand the server\nagainst the API\ndocumentation.\n3. Test the implementation\nagainst the speciﬁcation.\nFigure 1.8\nDocumentation-driven development works in three stages: design and document, implement, \nand validate.",
      "content_length": 1246,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "17\n1.5\nWho this book is for and what you will learn\n By putting API documentation at the forefront of the development process,\ndocumentation-driven development helps you avoid one of the most common problems\nAPI developers face: disagreements between the client and the server development\nteams about how the API should work. In the absence of robust API documentation,\ndevelopers often need to guess on implementation details of the API. In such cases,\nthe API rarely succeeds its first integration test. Although documentation-driven devel-\nopment won’t give a 100% guarantee that your API integrations will work, it will sig-\nnificantly reduce the risk of API integration failure.\n1.5\nIntroducing the CoffeeMesh application\nTo illustrate the concepts and ideas that we explain throughout this book, we’ll build\ncomponents of an application called CoffeeMesh. CoffeeMesh is a fictitious applica-\ntion that allows customers to order coffee in any location, at any time. The Cof-\nfeeMesh platform consists of a collection of microservices that implement different\ncapabilities, such as processing orders and scheduling deliveries. We’ll undertake a\nformal analysis and design of the CoffeeMesh platform in chapter 3. To give you a\ntaste of the kinds of things you’ll learn in this book, we’ll begin implementing the API\nof CoffeeMesh’s orders service in chapter 2. Before we close this chapter, I’d like to\ndedicate a section to explaining what you’ll learn from this book.\n1.6\nWho this book is for and what you will learn\nTo make the most out of this book, you should be familiar with the basics of web devel-\nopment. The code examples in the book are in Python, so a basic understanding of\nPython is beneficial but not necessary to be able to follow along with them. You do not\nneed to have knowledge of web APIs or microservices, as we will explain these technol-\nogies in depth. It is useful if you are familiar with the model-view-controller (MVC)\npattern for web development or its variants, such as the model-template-view (MTV)\npattern implemented by Python’s popular Django framework. We will draw compari-\nsons with these patterns from time to time to illustrate certain concepts. Basic familiar-\nity with Docker and cloud computing will be useful to get through the chapters about\ndeployments, but I’ll do my best to explain every concept in detail.\n This book shows you how to develop API-driven microservices with Python\nthrough a hands-on approach. You will learn\nService decomposition strategies for designing microservice architectures\nHow to design REST APIs and how to document them using the OpenAPI spec-\nification\nHow to build REST APIs in Python using popular frameworks like FastAPI and\nFlask\nHow to design and consume GraphQL APIs and how to build them using\nPython’s Ariadne framework\nHow to test your APIs using property-based testing and API testing frameworks\nsuch as Dredd and Schemathesis",
      "content_length": 2923,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "18\nCHAPTER 1\nWhat are microservice APIs?\nUseful design patterns to achieve loose coupling in your microservices\nHow to add authentication and authorization to your APIs using Open Authori-\nzation (OAuth) and OpenID Connect (OIDC) \nHow to deploy your microservices using Docker and Kubernetes to AWS\nBy the end of this book, you will be familiar with the benefits that microservices archi-\ntectures bring for web applications as well as the challenges and difficulties that come\nwith them. You will know how to integrate microservices using APIs, you will know\nhow to build and document those APIs using standards and best practices, and you\nwill be prepared to define the domain of an API with clear application boundaries.\nFinally, you’ll also know how to test, deploy, and secure your microservice APIs.\nSummary\nMicroservices are an architectural pattern in which components of a system are\ndesigned and built as independently deployed services. This results in smaller\nand more maintainable code bases and allows services to be optimized and\nscaled independently of each other.\nMonoliths are an architectural pattern in which whole applications are deployed\nin a single build and run in the same process. This makes the application easier\nto deploy and monitor, but it also makes deployments more challenging when\nthe code base grows large.\nApplications can have multiple types of interfaces, such as UIs, CLIs, and APIs.\nAn API is an interface that allows us to interact with an application program-\nmatically from our code or terminal.\nA web API is an API that runs on a web server and uses HTTP for data trans-\nport. We use web APIs to expose service capabilities through the internet.\nMicroservices talk to each other using smart endpoints and “dumb pipes.” A\ndumb pipe is a pipe that simply transfers data from one component to another.\nA great example of a dumb pipe for microservices is HTTP, which exchanges\ndata between the API client and the API server without knowing anything about\nthe API protocol being used. Therefore, web APIs are a great technology for\ndriving integrations between microservices.\nDespite their benefits, microservices also bring the following challenges:\n– Effective service decomposition—We must design services with clear boundar-\nies around specific subdomains; otherwise, we risk building a “distributed\nmonolith.”\n– Microservice integration tests—Running integration tests for all microservices is\nchallenging, but we can reduce the risk of integration failures by ensuring\nAPIs are correctly implemented.\n– Handling service unavailability—Collaborating services are vulnerable to service\nunavailability, request timeouts, and processing errors, and therefore must be\nable to handle those scenarios.",
      "content_length": 2750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "19\nSummary\n– Tracing distributed transactions—Tracing errors across multiple services is chal-\nlenging and requires software telemetry tools that allow you to centralize\nlogs, enable API visibility, and trace requests across services.\n– Increased operational complexity and infrastructure overhead—Each microservice\nrequires its own infrastructure provisioning, including servers, monitoring\nsystems, and alerts, so you need to invest additional efforts in infrastructure\nautomation.\nDocumentation-driven development is an API development workflow that works\nin three stages:\n– Design and document the API.\n– Build the API against the documentation.\n– Test the API against the documentation.\nBy putting API documentation at the forefront of the development process,\ndocumentation-driven development helps you avoid many common problems\nthat API developers face and therefore reduce the chances of API integration\nfailure.",
      "content_length": 922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "20\nA basic API\nimplementation\nIn this chapter, we implement the API for the orders service, which is one of the\nmicroservices of the CoffeeMesh website, the project we introduced in section 1.5.\nCoffeeMesh is an application that makes and delivers coffee on demand at any\ntime, wherever you are. The orders service allows customers to place orders with\nCoffeeMesh. As we implement the orders API, you will get an early look into the\nconcepts and processes that we dissect in more detail throughout this book. The\ncode for this chapter is available under the ch02 folder of the GitHub repository\nprovided with this book.\nThis chapter covers\nReading and understanding the requirements \nof an API specification\nStructuring our application into a data layer, \nan application layer, and an interface layer\nImplementing API endpoints using FastAPI\nImplementing data validation models (schemas) \nusing pydantic\nTesting the API using a Swagger UI",
      "content_length": 943,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "21\n2.1\nIntroducing the orders API specification\n2.1\nIntroducing the orders API specification\nLet’s begin by analyzing the requirements of the orders API. Using the orders API, we\ncan place orders, update them, retrieve their details, or cancel them. The orders API\nspecification is available in a file named ch02/oas.yaml in the GitHub repository for\nthis book. OAS stands for OpenAPI specification, which is a standard format for docu-\nmenting REST APIs. In chapter 5, you’ll learn to document your APIs using OpenAPI.\nAs you can see in figure 2.1, the API specification describes a REST API with four\nmain URL paths:\n\n/orders—Allows us to retrieve lists of orders (GET) and create orders (POST).\n\n/orders/{order_id}—Allows us to retrieve the details of a specific order\n(GET), to update an order (PUT), and to delete an order (DELETE).\n\n/orders/{order_id}/cancel—Allows us to cancel an order (POST).\n\n/orders/{order_id}/pay—Allows us to pay for an order (POST).\nIn addition to documenting the API endpoints, the specification also includes data\nmodels that tell us what the data exchanged over those endpoints looks like. In\nOpenAPI, we call those models schemas, and you can find them within the compo-\nnents section of the orders API specification. Schemas tell us what properties must be\nincluded in a payload and what their types are.\n For example, the OrderItemSchema schema specifies that the product and the size\nproperties are required, but the quantity property is optional. When the quantity\nproperty is missing from the payload, the default value is 1. Our API implementation\nmust therefore enforce the presence of the product and the size properties in the\npayload before we try to create the order. \n \n \n \n/orders/{order_id}\n/cancel\n/orders/{order_id}\n/pay\nGET\nReturns an order\nPUT\nUpdates an order\nDELETE\nDeletes an order\nPOST\nCancels an order\nPOST\nPays for an order\n/orders/{order_id}\n/orders\nGET\nReturns a list of orders\nPOST\nPlaces an order\nFigure 2.1\nThe orders API exposes seven endpoints structured around four URL paths. Each \nendpoint implements different capabilities, such as placing and cancelling an order.",
      "content_length": 2138,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "22\nCHAPTER 2\nA basic API implementation\n# file: oas.yaml\nOrderItemSchema:\n  type: object\n  required:\n    - product\n    - size\n  properties:\n    product:\n      type: string\n    size:\n      type: string\n      enum:\n        - small\n        - medium\n        - big\n    quantity:\n      type: integer\n      default: 1\n      minimum: 1\nNow that we understand the requirements for building the orders API, let’s look at\nthe architectural layout we will use for the implementation.\n2.2\nHigh-level architecture of the orders application\nThis section offers a high-level overview of the orders API’s architectural layout. Our\ngoal is to identify the layers of the application and to enforce clear boundaries and\nseparation of concerns between all layers.\n As you can see in figure 2.2, we organize into three layers: the API layer, the busi-\nness layer, and the data layer.\nThis way of structuring the application is an adaptation of the three-tier architecture\npattern, which structures applications into a data layer, a business layer, and a presen-\ntation layer. As you can see in figure 2.3, the data layer is the part of the application\nListing 2.1\nSpecification for OrderItemSchema\nAPI\nExposes the capabilities of the service\nand controls interactions with the user\nBusiness layer\nImplements the capabilities\nof the service\nData layer\nInterfaces with the\nsource of data\nFigure 2.2\nTo enforce separation of concerns among the different components of our service, we \nstructure our code around three layers: the data layer knows how to interface with the source of data; \nthe business layer implements the service’s capabilities; and the interface layer implements the \nservice’s API.",
      "content_length": 1676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "23\n2.3\nImplementing the API endpoints\nthat knows how to persist data so that we can retrieve it later. The data layer imple-\nments the data models required for interfacing with our source of data. For example,\nif our persistent storage is an SQL database, the models in the data layer will represent\nthe tables in the database, often with the help of an object relational mapper (ORM)\nframework.\n The business layer implements our service’s capabilities. It controls the interac-\ntions between the API layer and the data layer. For the orders service, it’s the part that\nknows what to do to place, cancel, or pay for an order.\n The API layer of a service is different from the business layer. The business layer\nimplements the capabilities of a service, while the API layer is an adapter on top of the\napplication logic that exposes the service’s capabilities to its consumers. Figure 2.2\nillustrates this relationship among the layers of a service, while figure 2.3 illustrates\nhow a user request is processed by each layer.\n The API layer is an adapter on top of the business layer. Its most important job is\nvalidating incoming requests and returning the expected responses. The API layer\ncommunicates with the business layer, passing the data sent by the user, so that\nresources can be processed and persisted in the server. The API layer is equivalent to\nthe presentation layer in three-tier architecture. Now that we know how we are going\nto structure our application, let’s jump straight into the code!\n2.3\nImplementing the API endpoints\nIn this section, you will learn to implement the API layer of the orders service. I’ll\nshow you how to break down the implementation of the API into progressive steps. In\nthe first step, we produce a minimal implementation of the endpoints with mock\nresponses. In the following sections of this chapter, we enhance the implementation\nby adding data validation and dynamic responses. You’ll also learn about the FastAPI\nlibrary and how you can use it to build a web API. \n/orders\n1. The user request\nreaches the service\ninterface, which\nvalidates the request.\n2. The business\nlayer processes\ndata from the\nuser’s request.\n3. The data layer\npersists data from\nthe user’s request.\nAPI\nBusiness layer\nData layer\nService capabilities\nFigure 2.3\nWhen a user request reaches the orders service, it’s first validated by the \ninterface layer. Then the interface layer interfaces with the business layer to process the \nrequest. After processing, the data layer persists the data contained in the request.",
      "content_length": 2540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "24\nCHAPTER 2\nA basic API implementation\nWhat is FastAPI?\nFastAPI (https://github.com/tiangolo/fastapi) is a web API framework built on top of\nStarlette (https://github.com/encode/starlette). Starlette is a high-performance, light-\nweight, asynchronous server gateway interface (ASGI) web framework, which means\nthat we can implement our services as a collection of asynchronous tasks to gain per-\nformance in our applications. In addition, FastAPI uses pydantic (https://github.com/\nsamuelcolvin/pydantic/) for data validation. The following figure illustrates how all these\ndifferent technologies fit together.\nFastAPI\npydantic\nHTTP request\nHTTP request\nHTTP request\n/orders\n/orders/{order_id}\nData validation ﬂow\nAPI endpoints\n{ data }\n{ data }\nUvicorn worker\nUvicorn worker\nHTTP request\nUvicorn worker\nHTTP request\nHTTP request\nStarlette\nrouting\nUvicorn\nAsynchronous requests handler\nUvicorn (https://github.com/encode/uvicorn) is an asynchronous web server \ncommonly used to run Starlette applications. Uvicorn handles HTTP requests and \npasses them on to Starlette, which functions within your application to call when \na request arrives in the server. FastAPI is built on top of Starlette, and it enhances \nStarlette’s routes with data validation and API documentation functionality.",
      "content_length": 1289,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "25\n2.3\nImplementing the API endpoints\nBefore we start implementing the API, we need to set up our environment for this\nproject. Create a folder named ch02 and move into it using the cd command in your\nterminal. We’ll use Pipenv to install and manage our dependencies.\nPipenv is a dependency management tool for Python that guarantees that the same\nversions of our dependencies are installed in different environments. In other words,\nPipenv makes it possible to create environments in a deterministic way. To accomplish\nthat, Pipenv uses a file called Pipfile.lock, which contains a description of the exact\npackage versions that were installed.\n$ pipenv --three      \n$ pipenv install fastapi uvicorn     \n$ pipenv shell     \nNow that our dependencies are installed, let’s build the API. First, copy the API specifi-\ncation under ch02/oas.yaml in the GitHub repository for this book in the ch02 folder\nwe created earlier. Then create a subfolder named orders, which will contain our API\nimplementation. Within the orders folder, create a file called app.py. Create another\nsubfolder called orders/api, and within that folder create a file called orders/api/\napi.py. At this point, the project structure should look like this:\n.\n├── Pipfile\n├── Pipfile.lock\n├── oas.yaml\n└── orders\n    ├── api\n    │   └── api.py\n    └── app.py\nAbout dependencies\nIf you want to make sure you use the same dependencies that I used when writing\nthis book, you can fetch the ch02/Pipfile and ch02/Pipfile.lock files from the GitHub\nrepository for this book and run pipenv install. \nPipfile describes the environment that we wish to create with Pipenv. Among other\nthings, Pipfile contains the version of Python that must be used to create the environ-\nment and the URLs of the PyPi repositories that must be used to pull the dependencies.\nPipenv also makes it easier to keep production dependencies separate from develop-\nment dependencies by providing specific installation flags for each set. For example, to\ninstall pytest we run pipenv install pytest --dev. Pipenv also exposes commands\nthat allow us to easily manage our virtual environments, such as pipenv shell to acti-\nvate the virtual environment or pipenv --rm to delete the virtual environment. \nListing 2.2\nCreating a virtual environment and installing dependencies with pipenv\nCreate a virtual environment using pipenv \nand setting the runtime to Python 3.\nInstall FastAPI \nand Uvicorn.\nActivate the virtual \nenvironment.",
      "content_length": 2466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "26\nCHAPTER 2\nA basic API implementation\nListing 2.3 shows how to create an instance of the FastAPI application in file orders/\napp.py. The instance of the FastAPI class from FastAPI is an object that represents the\nAPI we are implementing. It provides decorators (functions that add additional func-\ntionality to a function or class) that allow us to register our view functions.1\n# file: orders/app.py\nfrom fastapi import FastAPI\napp = FastAPI(debug=True)    \nfrom orders.api import api     \nListing 2.4 shows a minimal implementation of our API endpoints. The code goes\nwithin the orders/api/api.py file. We declare a static order object, and we return the\nsame data in all the endpoints except the DELETE /orders/{order_id} endpoint,\nwhich returns an empty response. Later, we’ll change the implementation to use a\ndynamic list of orders. FastAPI decorators transform the data we return in every func-\ntion into an HTTP response; they also map our functions to a specific URL in our\nserver. By default, FastAPI includes 200 (OK) status codes in our responses, but we can\noverride this behavior by using the status_code parameter in the routes decorators,\nlike we do in the POST /orders and in the DELETE /orders/{order_id} endpoints.\n# file: orders/api/api.py\nfrom datetime import datetime\nfrom uuid import UUID\nfrom starlette.responses import Response\nfrom starlette import status\nfrom orders.app import app\norder = {       \n    'id': 'ff0f1355-e821-4178-9567-550dec27a373',\n    'status': \"delivered\",\n    'created': datetime.utcnow(),\n    'order': [\n        {\n            'product': 'cappuccino',\n            'size': 'medium',\n            'quantity': 1\n        }\nListing 2.3\nCreating an instance of the FastAPI application  \n1 For a classic explanation of the decorator pattern, see Erich Gamma et al., Design Patterns (Addison-Wesley,\n1995), pp. 175–184. For a more Pythonic introduction to decorators, see Luciano Ramalho, Fluent Python\n(O’Reilly, 2015), pp. 189–222. \nListing 2.4\nMinimal implementation of the orders API\nWe create an instance of \nthe FastAPI class. This object \nrepresents our API application.\nWe import the api module so \nthat our view functions can be \nregistered at load time.\nWe define an order object to \nreturn in our responses.",
      "content_length": 2259,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "27\n2.3\nImplementing the API endpoints\n    ]\n}\n@app.get('/orders')  \ndef get_orders():\n    return {'orders': [orders]}\n@app.post('/orders', status_code=status.HTTP_201_CREATED)    \ndef create_order():\n    return order\n@app.get('/orders/{order_id}')   \ndef get_order(order_id: UUID):     \n    return order\n@app.put('/orders/{order_id}')\ndef update_order(order_id: UUID):\n    return order\n@app.delete('/orders/{order_id}', status_code=status.HTTP_204_NO_CONTENT)\ndef delete_order(order_id: UUID):\n    return Response(status_code=HTTPStatus.NO_CONTENT.value)    \n@app.post('/orders/{order_id}/cancel')\ndef cancel_order(order_id: UUID):\n    return order\n@app.post('/orders/{order_id}/pay')\ndef pay_order(order_id: UUID):\n    return order\nFastAPI exposes decorators named after HTTP methods, such as get() and post().\nWe use these decorators to register our API endpoints. FastAPI’s decorators take at\nleast one argument, which is the URL path we want to register.\n Our view functions can take any number of parameters. If the name of the param-\neter matches the name of a URL path parameter, FastAPI passes the path parameter\nfrom the URL to our view function on invocation. For example, as you can see in fig-\nure 2.4, the URL /orders/{order_id} defines a path parameter named order_id,\nand accordingly our view functions registered for that URL path take an argument\nnamed order_id. If a user navigates to the URL /orders/53e80ed2-b9d6-4c3b-b549-\n258aaaef9533, our view functions will be called with the order_id parameter set to\n53e80ed2-b9d6-4c3b-b549-258aaaef9533. FastAPI allows us to specify the type and\nformat of the URL path parameter by using type hints. In listing 2.4, we specify that\norder_id’s type is a universally unique identifier (UUID). FastAPI will invalidate any calls\nin which order_id doesn’t follow that format.\nWe register a GET \nendpoint for the \n/orders URL path.\nWe specify that \nthe response’s \nstatus code is \n201 (Created).\nWe define URL parameters, \nsuch as order_id, within \ncurly brackets.\nWe capture the \nURL parameter as a \nfunction argument.\nWe use\nHTTPStatus.NO_CONTENT.value\nto return an empty response.",
      "content_length": 2138,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "28\nCHAPTER 2\nA basic API implementation\nFastAPI responses include a 200 (OK) status code by default, but we can change this\nbehavior by setting the status_code parameter in the endpoints’ decorators. In list-\ning 2.4, we set status_code to 201 (Created) in the POST /orders endpoint, and to\n204 (No Content) in the DELETE /orders/{order_id} endpoint. For a detailed\nexplanation of status codes, see section 4.6 in chapter 4.\n You can now run the app to get a feeling of what the API looks like by executing\nthe following command from the top-level orders directory:\n$ uvicorn orders.app:app --reload\nThis command loads the server with hot reloading enabled. Hot reloading restarts your\nserver whenever you make changes to your files. Visit the http://127.0.0.1:8000/docs\nURL in a browser and you will see an interactive display of the API documentation\ngenerated by FastAPI from our code (see figure 2.5 for an illustration). This visualiza-\ntion is called Swagger UI, and it’s one of the most popular ways of visualizing REST\nAPIs. Another popular visualization is Redoc, which is also supported by FastAPI under\nthe http://127.0.0.1:8000/redoc URL.\n If you click on any of the endpoints represented in the Swagger UI, you will see\nadditional documentation about the endpoint. You will also see a Try it Out button,\nwhich gives you the opportunity to test the endpoint directly from this UI. Click that\nbutton, then click Execute, and you will get the hardcoded response we included in\nour endpoints (see figure 2.6 for an illustration).\n Now that we have the basic skeleton of our API, we’ll move on to implementing\nvalidators for our incoming payloads and our outgoing responses. The next section\nwalks you through the steps needed to accomplish that.\nFigure 2.4\nFastAPI knows how to map a request to the right function, \nand it passes any relevant parameters from the request to the function. \nIn this illustration, a GET request on the /orders/{order_id} \nendpoint with order_id set to ff0f1355-e821-4178-9567-\n550dec27a373 is passed to the get_order() function.",
      "content_length": 2067,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "29\n2.3\nImplementing the API endpoints\nFigure 2.5\nView of the Swagger UI dynamically generated by FastAPI from our code. We can use \nthis view to test the implementation of our endpoints. \nAPI endpoints\nValidation\nschemas\nClick an endpoint to see\nits description and test it.\nFigure 2.6\nTo test an endpoint, click it to expand it. You’ll see a Try it Out button on the top-right corner of \nthe endpoint’s description. Click that button, and then click the Execute button. This triggers a request to the \nserver, and you’ll be able to see the response.\nDescription of\nthe endpoint\nClick Execute to\ntest the endpoint.\nTranslation of the request\nto a cURL command, which you\ncan run from the terminal\nEndpoint of the request\nPayload of the response\nto the request",
      "content_length": 759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "30\nCHAPTER 2\nA basic API implementation\n2.4\nImplementing data validation models with pydantic\nNow that we have implemented the main layout for the URL paths of our API, we\nneed to add validation for incoming payloads and how we marshal our outgoing\nresponses. Data validation and marshalling are crucial operations in an API, and to\ndeliver a successful API integration, we need to get them right. In the following sec-\ntions, you’ll learn to add robust data validation and marshalling capabilities to your\nAPIs. FastAPI uses pydantic for data validation, so we’ll start by learning to create\npydantic models in this section.\nDEFINITION\nMarshalling is the process of transforming an in-memory data\nstructure into a format suitable for storage or transmission over a network. In\nthe context of web APIs, marshalling refers to the process of transforming an\nobject into a data structure that can be serialized into a content type of\nchoice, like XML or JSON, with explicit mappings for the object attributes\n(see figure 2.7 for an illustration).\nThe orders API specification contains three schemas: CreateOrderSchema, GetOrder-\nSchema, and OrderItemSchema. Let’s analyze these schemas to make sure we under-\nstand how we need to implement our validation models.\n# file: oas.yaml\ncomponents:\n  schemas:\n    OrderItemSchema:\n      type: object     \n      required:       \n        - product\n        - size\n      properties:     \n        product:\n          type: string\n        size:\n          type: string\n          enum:     \n            - small\n            - medium\n            - big\n        quantity:\n          type: integer\n          default: 1     \n          minimum: 1     \n    CreateOrderSchema:\n      type: object\n      required:\n        - order\nListing 2.5\nSpecification for the orders API schemas\nEvery schema has \na type, which in this \ncase is an object.\nWe list compulsory \nproperties under the \nrequired keyword.\nWe list object \nproperties under the \nproperties keyword.\nWe constrain the values \nof a property using an \nenumeration.\nAttributes can have \na default value.\nWe can also specify \na minimum value for \na property.",
      "content_length": 2133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "31\n2.4\nImplementing data validation models with pydantic\nFigure 2.7\nTo build a response payload from a Python object, we first \nmarshal the object into a serializable data structure, with explicit mapping \nof attributes between the object and the new structure. Deserializing the \npayload gives us back an object identical to the one we serialized.\norder = {\n'id': 'ﬀ0f1355-e821-4178-9567-550dec27a373',\n'status': 'completed',\n'created': 1740493805,\n'order': [\n{\n'product': 'cappuccino',\n'size': 'medium',\n'quantity': 1\n}\n]\n}\nOrder object\nid = UUID('ﬀ0f1355-e821-4178-9567-550dec27a373')\nstatus = 'completed'\ncreated = 1740493805\norder = [\nOrderItem(product='capuccino', size='medium'), quantity=1)\n]\nPython object (not directly serializable)\nPython dictionary (can be serialized)\n{\n\"id\": \"ﬀ0f1355-e821-4178-9567-550dec27a373\",\n\"status\": \"completed\",\n\"created\": 1740493805,\n\"order\": [{\n\"product\": \"cappuccino\",\n\"size\": \":medium\",\n\"quantity\": 1\n}]\n}\nJSON document (serialized data)\nUnmarshalling\nMarshalling\nSerialization\nDeserialization",
      "content_length": 1036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "32\nCHAPTER 2\nA basic API implementation\n      properties:\n        order:\n          type: array\n          items:       \n            $ref: '#/components/schemas/OrderItemSchema'    \n    GetOrderSchema:\n      type: object\n      required:\n        - order\n        - id\n        - created\n        - status\n      properties:\n        id:\n          type: string\n          format: uuid\n        created:\n          type: string\n          format: date-time\n        status:\n          type: string\n          enum:\n            - created\n            - progress\n            - cancelled\n            - dispatched\n            - delivered\n        order:\n          type: array\n          items:\n            $ref: '#/components/schemas/OrderItemSchema'\nWe use GetOrderSchema when we return the details of an order from the server and\nCreateOrderSchema to validate an order placed by a customer. Figure 2.8 illustrates\nhow the data validation flow works for CreateOrderSchema. As you can see, Create-\nOrderSchema only requires the presence of one property in the payload: the order\nproperty, which is an array of objects whose specification is defined by OrderItem-\nSchema. OrderItemSchema has two required properties, product and size, and one\noptional property, quantity, which has a default value of 1. This means that, when\nprocessing a request payload, we must check that the product and size properties are\npresent in the payload and that they have the right type. Figure 2.8 shows what hap-\npens when the quantity property is missing from the payload. In that case, we set the\nproperty to its default value of 1 in the server.\n Now that we understand our API schemas, it’s time to implement them. Create a\nnew file called orders/api/schemas.py. This file will contain our pydantic models. List-\ning 2.6 shows how we implement CreateOrderSchema, GetOrderSchema, and Order-\nItemSchema using pydantic. The code in listing 2.6 goes in the orders/api/schemas.py\nmodule. We define every schema as a class that inherits from pydantic’s BaseModel\nclass, and we specify the type of every attribute using Python type hints. For attributes\nWe specify the type of the \nitems in the array using \nthe items keyword.\nWe use a JSON pointer to\nreference another schema\nwithin the same document.",
      "content_length": 2257,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "33\n2.4\nImplementing data validation models with pydantic\nthat can only take on a limited selection of values, we define an enumeration class. In\nthis case, we define enumerations for the size and status properties. We set the type\nof OrderItemSchema’s quantity property to pydantic’s conint type, which enforces\ninteger values. We also specify that quantity is an optional property and that its values\nshould be equal or greater than 1, and we give it a default value of 1. Finally, we use\npydantic’s conlist type to define CreateOrderSchema’s order property as a list with at\nleast one element.\nFigure 2.8\nData validation flow for request payloads against the CreateOrderSchema model. The diagram \nshows how each property of the request payload is validated against the properties defined in the schema \nand how we build an object from the resulting validation.\nCreateOrderSchema\ntype: object\nrequired: [ order ]\nproperties\norder: array\nitems: #/components/schemas\n/OrderItemSchema\nOrderItemSchema\ntype: object\nrequired:\n[ product, size ]\nproduct: string\nsize: string\nquantity: integer\ndefault: 1\nminimum: 1\n{\n'order': [\n{\n'product': 'cappuccino',\n'size': 'medium',\n}\n]\n}\nRequest payload\nOrder\n+ order: List[OrderItem]\nOrderItem\n+ product: cappuccino\n+ size: medium\n+ quantity: 1\nEnum: [ small,\nmedium, big ]",
      "content_length": 1309,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "34\nCHAPTER 2\nA basic API implementation\n# file: orders/api/schemas.py\nfrom enum import Enum\nfrom typing import List\nfrom uuid import UUID\nfrom pydantic import BaseModel, Field, conlist, conint\nclass Size(Enum):     \n    small = 'small'\n    medium = 'medium'\n    big = 'big'\nclass Status(Enum):\n    created = 'created'\n    progress = 'progress'\n    cancelled = 'cancelled'\n    dispatched = 'dispatched'\n    delivered = 'delivered'\nclass OrderItemSchema(BaseModel):    \n    product: str         \n    size: Size         \n    quantity: Optional[conint(ge=1, strict=True)] = 1   \nclass CreateOrderSchema(BaseModel):\n    order:  conlist(OrderItemSchema, min_items=1)    \nclass GetOrderSchema(CreateOrderSchema):\n    id: UUID\n    created: datetime\n    status: Status\nclass GetOrdersSchema(BaseModel):\n    orders: List[GetOrderSchema]\nNow that our validation models are implemented, in the following sections we’ll link\nthem with the API to validate and marshal payloads.\n2.5\nValidating request payloads with pydantic\nIn this section, we use the models we implemented in section 2.4 to validate request\npayloads. How do we access request payloads within our view functions? We intercept\nrequest payloads by declaring them as a parameter of the view function, and to vali-\ndate them we set their type to the relevant pydantic model.\nListing 2.6\nImplementation of the validation models using pydantic\nWe declare an \nenumeration \nschema.\nEvery pydantic \nmodel inherits from \npydantic’s BaseModel.\nWe use Python-type \nhints to specify the \ntype of an attribute. \nWe constrain the values of a \nproperty by setting its type \nto an enumeration.\nWe specify \nquantity’s minimum \nvalue, and we give it \na default.\nWe use pydantic’s \nconlist type to \ndefine a list with at \nleast one element.",
      "content_length": 1773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "35\n2.5\nValidating request payloads with pydantic\n# file: orders/api/api.py\nfrom uuid import UUID\nfrom starlette.responses import Response\nfrom starlette import status\nfrom orders.app import app\nfrom orders.api.schemas import CreateOrderSchema     \n...\n@app.post('/orders', status_code=status.HTTP_201_CREATED)\ndef create_order(order_details: CreateOrderSchema):   \n    return order\n@app.get('/orders/{order_id}')\ndef get_order(order_id: UUID):\n    return order\n@app.put('/orders/{order_id}')\ndef update_order(order_id: UUID, order_details: CreateOrderSchema):\n    return order\n...\nIf you kept the application running, the changes are loaded automatically by the\nserver, so you just need to refresh the browser to update the UI. If you click the POST\nendpoint of the /orders URL path, you’ll see that the UI now gives you an example of\nthe payload expected by the server. Now, if you try editing the payload to remove any\nof the required fields, for example, the product field, and you send it to the server,\nyou’ll get the following error message:\n{\n  \"detail\": [\n    {\n      \"loc\": [\n        \"body\",\n        \"order\",\n        0,\n        \"product\"\n      ],\n      \"msg\": \"field required\",\n      \"type\": \"value_error.missing\"\n    }\n  ]\n}\nListing 2.7\nHooking validation models up with the API endpoints \nWe import the pydantic \nmodels so that we can \nuse them for validation.\nWe intercept a \npayload by declaring \nit as a parameter in \nour function, and we \nuse type hints to \nvalidate it.",
      "content_length": 1485,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "36\nCHAPTER 2\nA basic API implementation\nFastAPI generates an error message that points to where in the payload the error is\nfound. The error message uses a JSON pointer to indicate where the problem is. A\nJSON pointer is a syntax that allows you to represent the path to a specific value within\na JSON document. If this is the first time you’ve encountered JSON pointers, think of\nthem as a different way of representing dictionary syntax and index notation in Python.\nFor example, the error message \"loc: /body/order/0/product\" is roughly equivalent\nto the following notation in Python: loc['body']['order'][0]['product']. Figure 2.9\nshows you how to interpret the JSON pointer from the error message to identify the\nsource of the problem in the payload.\nYou can also change the payload so that, instead of missing a required property, it con-\ntains an illegal value for the size property:\nFigure 2.9\nWhen a request fails due a malformed payload, we get a \nresponse with an error message. The error message uses a JSON pointer \nto tell us where the error is. In this case, the error message says that the \nproperty /body/order/0/product is missing from the payload.\n{\n\"detail\": [\n{\n\"loc\": [\n\"body\",\n\"order\",\n0,\n\"product\"\n],\n\"msg\": \"ﬁeld required\",\n\"type\": \"value_error.missing\"\n}\n]\n}\nError message\nLocation: /body/order/0/product\n{ \"order\": [\n{\n\"size\": \"small\"\n}\n]\n}\nRequest payload\nBody",
      "content_length": 1388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "37\n2.5\nValidating request payloads with pydantic\n{\n  \"order\": [\n    {\n      \"product\": \"string\",\n      \"size\": \"somethingelse\"\n    }\n  ]\n}\nIn this case, you’ll also get an informative error with the following message: \"value is\nnot a valid enumeration member; permitted: 'small', 'medium', 'big'\". What hap-\npens if we make a typo in the payload? For example, imagine a client sent the follow-\ning payload to the server:\n{\n  \"order\": [\n    {\n      \"product\": \"string\",\n      \"size\": \"small\",\n      \"quantit\": 5\n    }\n  ]\n}\nIn this case, FastAPI assumes that the quantity property is missing and that the cli-\nent wishes to set its value to 1. This result could lead to confusion between the client\nand the server, and in such cases invalidating payloads with illegal properties helps\nus make the API integration more reliable. In chapter 6, you’ll learn to handle those\nsituations.\n One edge case with optional properties, such as OrderItemSchema’s quantity, is that\npydantic assumes they’re nullable and therefore will accept payloads with quantity set\nto null. For example, if we send the following payload to the POST /orders end-\npoint, our server will accept it:\n{\n  \"order\": [\n    {\n      \"product\": \"string\",\n      \"size\": \"small\",\n      \"quantity\": null\n    }\n  ]\n}\nIn terms of API integrations, optional isn’t quite the same as nullable: a property can\nbe optional because it has a default value, but that doesn’t mean it can be null. To\nenforce the right behavior in pydantic, we need to include an additional validation\nrule that prevents users from setting the value of quantity to null. We use pydantic’s\nvalidator() decorator to define additional validation rules for our models.",
      "content_length": 1693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "38\nCHAPTER 2\nA basic API implementation\n# file: orders/api/schemas.py\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import List, Optional\nfrom uuid import UUID\nfrom pydantic import BaseModel, conint, validator\n...\nclass OrderItemSchema(BaseModel):\n    product: str\n    size: Size\n    quantity: Optional[conint(ge=1, strict=True)] = 1\n    @validator('quantity')\n    def quantity_non_nullable(cls, value):\n        assert value is not None, 'quantity may not be None'\n        return value\n...\nNow that we know how to test our API implementation using a Swagger UI, let’s see\nhow we use pydantic to validate and serialize our API responses.\n2.6\nMarshalling and validating response payloads \nwith pydantic\nIn this section, we’ll use the pydantic models implemented in section 2.4 to marshal and\nvalidate the response payloads of our API. Malformed payloads are one of the most\ncommon causes of API integration failures, so this step is crucial to deliver a robust API.\nFor example, the schema for the response payload of the POST /orders endpoint is\nGetOrderSchema, which requires the presence of the id, created, status, and order\nfields. API clients will expect the presence of all these fields in the response payload and\nwill raise errors if any of the fields is missing or comes in the wrong type or format.\nNOTE\nMalformed response payloads are a common source of API integration\nfailures. You can avoid this problem by validating your response payloads\nbefore they leave the server. In FastAPI, this is easily done by setting the\nresponse_model parameter of a route decorator.\nListing 2.9 shows how we use pydantic models to validate the responses from the GET\n/orders and the POST /orders endpoints. As you can see, we set the response_model\nparameter to a pydantic model in FastAPI’s route decorators. We follow the same\napproach to validate responses from all the other endpoints except the DELETE\n/orders/{order_id} endpoint, which returns an empty response. Feel free to check\nout the code in the GitHub repository for this book for the full implementation.\nListing 2.8\nIncluding additional validation rules for pydantic models",
      "content_length": 2151,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "39\n2.6\nMarshalling and validating response payloads with pydantic\n# file: orders/api/api.py\nfrom uuid import UUID\nfrom starlette.responses import Response\nfrom starlette import status\nfrom orders.app import app\nfrom orders.api.schemas import (\n    GetOrderSchema,\n    CreateOrderSchema,\n    GetOrdersSchema,\n)\n...\n@app.get('/orders', response_model=GetOrdersSchema)\ndef get_orders():\n    return [\n        order\n    ]\n@app.post(\n    '/orders',\n    status_code=status.HTTP_201_CREATED,\n    response_model=GetOrderSchema,\n)\ndef create_order(order_details: CreateOrderSchema):\n    return order\nNow that we have response models, FastAPI will raise an error if a required property is\nmissing from a response payload. It will also remove any properties that are not part of\nthe schema, and it will try to cast each property into the right type. Let’s see this\nbehavior at work.\n In a browser, visit the http://127.0.0.1:8000/docs URL to load the Swagger UI for\nour API. Then head over to the GET /orders endpoint and send a request. You’ll get\nthe order that we hardcoded at the top of the orders/api/api.py file. Let’s make some\nmodifications to that payload to see how FastAPI handles them. To begin, let’s add an\nadditional property called updated:\n# orders/api/api.py\n...\norder = {\n    'id': 'ff0f1355-e821-4178-9567-550dec27a373',\n    'status': 'delivered',\n    'created': datetime.utcnow(),\n    'updated': datetime.utcnow(),\nListing 2.9\nHooking validation models for responses in the API endpoints",
      "content_length": 1496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "40\nCHAPTER 2\nA basic API implementation\n    'order': [\n        {\n            'product': 'cappuccino',\n            'size': 'medium',\n            'quantity': 1\n        }\n    ]\n}\n...\nIf we call the GET /orders endpoint again, we’ll get the same response we obtained\nbefore, without the updated property since it isn’t part of the GetOrderSchema model:\n[\n  {\n    \"order\": [\n      {\n        \"product\": \"cappuccino\",\n        \"size\": \"medium\",\n        \"quantity\": 1\n      }\n    ],\n    \"id\": \"ff0f1355-e821-4178-9567-550dec27a373\",\n    \"created\": datetime.utcnow(),\n    \"status\": \"delivered\"\n  }\n]\nLet’s now remove the created property from the order payload and call the GET\n/orders endpoint again:\n# orders/api/api.py\n...\norder = {\n    'id': 'ff0f1355-e821-4178-9567-550dec27a373',\n    'status': \"delivered\",\n    'updated': datetime.utcnow(),\n    'order': [\n        {\n            'product': 'cappuccino',\n            'size': 'medium',\n            'quantity': 1\n        }\n    ]\n}\nThis time, FastAPI raises a server error telling us that the required created property is\nmissing from the payload:\npydantic.error_wrappers.ValidationError: 1 validation error for GetOrderSchema\nresponse -> 0 -> created\n  field required (type=value_error.missing)",
      "content_length": 1236,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "41\n2.7\nAdding an in-memory list of orders to the API\nLet’s now change the value of the created property to a random string and run another\nrequest against the GET /orders endpoint:\n# orders/api/api.py\n...\norder = {\n    'id': 'ff0f1355-e821-4178-9567-550dec27a373',\n    'status': \"delivered\",\n    'created': 'asdf',\n    'updated': 1740493905,\n    'order': [\n        {\n            'product': 'cappuccino',\n            'size': 'medium',\n            'quantity': 1\n        }\n    ]\n}\n...\nIn this case, FastAPI raises a helpful error:\npydantic.error_wrappers.ValidationError: 1 validation error for GetOrderSchema\nresponse -> 0 -> created\n  value is not a valid integer (type=type_error.integer)\nOur responses are being correctly validated and marshalled. Let’s now add a simple\nstate management mechanism for the application so that we can place orders and\nchange their state through the API.\n2.7\nAdding an in-memory list of orders to the API\nSo far, our API implementation has returned the same response object. Let’s change\nthat by adding a simple in-memory collection of orders to manage the state of the\napplication. To keep the implementation simple, we’ll represent the collection of\norders as a Python list. We’ll manage the list within the view functions of the API layer.\nIn chapter 7, you’ll learn useful patterns to add a robust controller and data per-\nsistence layers to the application.\n Listing 2.10 shows the changes required for the view functions under api.py to man-\nage the in-memory list of orders in our view functions. The changes in listing 2.9 go into\nthe orders/api/api.py file. We represent the collection of orders as a Python list, and we\nassign it to the variable ORDERS. To keep it simple, we store the details of every order as a\ndictionary, and we update them by changing their properties in the dictionary.\n# file: orders/api/api.py\nimport time\nimport uuid\nListing 2.10\nManaging the application’s state with an in-memory list",
      "content_length": 1953,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "42\nCHAPTER 2\nA basic API implementation\nfrom datetime import datetime\nfrom uuid import UUID\nfrom fastapi import HTTPException\nfrom starlette.responses import Response\nfrom starlette import status\nfrom orders.app import app\nfrom orders.api.schemas import GetOrderSchema, CreateOrderSchema\nORDERS = []     \n@app.get('/orders', response_model=GetOrdersSchema)\ndef get_orders():\n    return ORDERS     \n@app.post(\n    '/orders',\n    status_code=status.HTTP_201_CREATED,\n    response_model=GetOrderSchema,\n)\ndef create_order(order_details: CreateOrderSchema):\n    order = order_details.dict()    \n    order['id'] = uuid.uuid4()    \n    order['created'] = datetime.utcnow()\n    order['status'] = 'created'\n    ORDERS.append(order)     \n    return order        \n@app.get('/orders/{order_id}', response_model=GetOrderSchema)\ndef get_order(order_id: UUID):\n    for order in ORDERS:           \n        if order['id'] == order_id:\n            return order\n    raise HTTPException(     \n        status_code=404, detail=f'Order with ID {order_id} not found'\n    )\n@app.put('/orders/{order_id}', response_model=GetOrderSchema)\ndef update_order(order_id: UUID, order_details: CreateOrderSchema):\n    for order in ORDERS:\n        if order['id'] == order_id:\n            order.update(order_details.dict())\n            return order\n    raise HTTPException(\n        status_code=404, detail=f'Order with ID {order_id} not found'\n    )\nWe represent our in-memory \nlist of orders as a Python list.\nTo return the list of orders, we \nsimply return the ORDERS list.\nWe\ntransform\nevery order\ninto a\ndictionary.\nWe enrich the order \nobject with server-side \nattributes, such as \nthe ID.\nTo create the order, \nwe add it to the list.\nAfter appending the order \nto the list, we return it.\nTo find an order by ID, we \niterate the ORDERS list and \ncheck their IDs.\nIf an order isn’t found, we raise an \nHTTPException with status_code set \nto 404 to return a 404 response.",
      "content_length": 1938,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "43\n2.7\nAdding an in-memory list of orders to the API\n@app.delete(\n    '/orders/{order_id}',\n    status_code=status.HTTP_204_NO_CONTENT,\n    response_class=Response,\n)\ndef delete_order(order_id: UUID):\n    for index, order in enumerate(ORDERS):   \n        if order['id'] == order_id:\n            ORDERS.pop(index)\n            return Response(status_code=HTTPStatus.NO_CONTENT.value)\n    raise HTTPException(\n        status_code=404, detail=f'Order with ID {order_id} not found'\n    )\n@app.post('/orders/{order_id}/cancel', response_model=GetOrderSchema)\ndef cancel_order(order_id: UUID):\n    for order in ORDERS:\n        if order['id'] == order_id:\n            order['status'] = 'cancelled'\n            return order\n    raise HTTPException(\n        status_code=404, detail=f'Order with ID {order_id} not found'\n    )\n@app.post('/orders/{order_id}/pay', response_model=GetOrderSchema)\ndef pay_order(order_id: UUID):\n    for order in ORDERS:\n        if order['id'] == order_id:\n            order['status'] = 'progress'\n            return order\n    raise HTTPException(\n        status_code=404, detail=f'Order with ID {order_id} not found'\n    )\nIf you play around with the POST /orders endpoint, you’ll be able to create new\norders, and using their IDs you’ll be able to update them by hitting the PUT\n/orders/{order_id} endpoint. In every endpoint under the /orders/{order_id}\nURL path, we check whether the order requested by the API client exists, and if it\ndoesn’t we return a 404 (Not Found) response with a helpful message.\n We are now able to use the orders API to create orders, update them, pay for\nthem, cancel them, and get their details. You have implemented a fully working web\nAPI for a microservice application! You’ve become familiar with a bunch of new\nlibraries to build web APIs, and you’ve seen how to add robust data validation to\nyour APIs. You’ve also learned to put it all together and run it with success. Hope-\nfully, this chapter has sparked your interest and excitement about designing and\nbuilding microservices exposing web APIs. In the coming chapters, we’ll delve\ndeeper into these topics, and you’ll learn to build and deliver robust and secure\nmicroservice API integrations.\nWe order from the list using \nthe list.pop() method.",
      "content_length": 2258,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "44\nCHAPTER 2\nA basic API implementation\nSummary\nTo structure microservices into modular layers, we use an adaptation of the\nthree-tier architecture pattern:\n– A data layer that knows how to interface with the source of data\n– A business layer that implements the capabilities of the service\n– An interface or presentation layer that exposes the capabilities of the service\nthrough an API\nFastAPI is a popular framework for building web APIs. It’s highly performant,\nand it has a rich ecosystem of libraries that make it easier to build APIs.\nFastAPI uses pydantic, a popular data validation library for Python. Pydantic\nuses type hints to create validation rules, which results in clean and easy-to-\nunderstand models.\nFastAPI generates a Swagger UI dynamically from our code. A Swagger UI is a\npopular interactive visualization UI for APIs. Using the Swagger UI, we can eas-\nily test if our implementation is correct.",
      "content_length": 922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "45\nDesigning microservices\nWhen we design a microservices platform, the first questions we face are, “How do\nyou break down a system into microservices? How do you decide where a service\nends and another one starts?” In other words, how do you define the boundaries\nbetween microservices? In this chapter, you’ll learn to answer these questions and\nhow to evaluate the quality of a microservices architecture by applying a set of\ndesign principles.\n The process of breaking down a system into microservices is called service decom-\nposition. Service decomposition is a fundamental step in the design of our microser-\nvices since it helps us define applications with clear boundaries, well-defined\nscopes, and explicit responsibilities. A well-designed microservices architecture is\nessential to reduce the risk of a distributed monolith. In this chapter, you’ll learn\ntwo service decomposition strategies: decomposition by business capability and\ndecomposition by subdomains. We’ll see how these methods work and use a practi-\ncal example to learn to apply them. Before we delve into service decomposition\nThis chapter covers\nPrinciples of microservices design\nService decomposition by business capability\nService decomposition by subdomain",
      "content_length": 1243,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "46\nCHAPTER 3\nDesigning microservices\nstrategies, we introduce the project that will guide our examples throughout this\nchapter and the rest of the book: CoffeeMesh.\n3.1\nIntroducing CoffeeMesh\nCoffeeMesh is a fictitious company that allows customers to order all sorts of products\nderived from coffee, including beverages and pastries. CoffeeMesh has one mission:\nto make and deliver the best coffee in the world on demand to its customers, no mat-\nter where they are or when they place their order. The production factories owned by\nCoffeeMesh form a dense network, a mesh of coffee production units that spans sev-\neral countries. Coffee production is fully automated, and deliveries are carried out by\nan unmanned fleet of drones operating 24/7.\n When a customer places an order through the CoffeeMesh website, the ordered\nitems are produced on demand. An algorithm determines which factory is the most\nsuitable place to produce each item based on available stock, the number of pending\norders the factory is taking care of, and distance to the customer. Once the items are\nproduced, they’re immediately dispatched to the customer. It’s part of CoffeeMesh’s\nmission statement that the customer receives each item fresh and hot.\n Now that we have an example to work with, let’s see how we design the microser-\nvices architecture for the CoffeeMesh platform. Before we learn to apply service\ndecomposition strategies for microservices, the next section teaches you three princi-\nples that will guide our designs.\n3.2\nMicroservices design principles\nWhat makes a well-designed microservice? As we established in chapter 1, microser-\nvices are designed around well-defined business subdomains, they have clearly defined\napplication boundaries, and they communicate with each other through lightweight\nprotocols. What does this mean in practice? In this section, we explore three design\nprinciples that help us test whether our microservices are correctly designed:\nDatabase-per-service principle\nLoose coupling principle\nSingle Responsibility Principle (SRP)\nFollowing these principles will help you avoid the risk of building a distributed mono-\nlith. In the following sections, we evaluate our architectural design against these prin-\nciples, and they help us spot errors in the design.\n3.2.1\nDatabase-per-service principle\nThe database-per-service principle states that each microservice owns a specific set\nof the data, and no other service should have access to such data except through an\nAPI. Despite this pattern’s name, it does not mean that each microservice should be\nconnected to a completely different database. It could be different tables in an SQL\ndatabase or different collections in a NoSQL database. The point of this pattern is",
      "content_length": 2748,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "47\n3.2\nMicroservices design principles\nto ensure that the data owned by a specific service is not accessed directly by another\nservice.\n Figure 3.1 shows how microservices share their data. In the illustration, the orders\nservice calculates the price of a customer order. To calculate the price, the orders ser-\nvice needs the price of each item in the order, which is available in the Products data-\nbase. It also needs to know whether the user has an applicable discount, which can be\nchecked in the Users database. However, instead of accessing both databases directly,\nthe orders service requests this data from the products and users services.\nProducts service\nUsers service\nOrders\nOrder 1\nOrder 2\nOrder 3\nUsers\nUser 1\nUser 2\nUser 3\nProducts\nProduct 1\nProduct 2\nProduct 3\n1. The orders service\nrequests product prices\nfrom the products service.\n2. The orders service requests\nuser-applicable discounts\nfrom the users service.\n3. The orders service calculates\nthe price for the customer.\nOrders service\nFigure 3.1\nEach microservice has its own database, and access to another service’s data \nhappens through an API.",
      "content_length": 1119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "48\nCHAPTER 3\nDesigning microservices\nWhy is this principle important? Encapsulating data access behind a service allows us\nto design our data models for optimal access for the service. It also allows us to make\nchanges to the database without breaking another service’s code. If the orders service\nin figure 3.1 had direct access to the Products database, schema changes in that\ndatabase would require updates to both the products and orders services. We’d be\ncoupling the orders service’s code to the Products database, and therefore we’d be break-\ning the loose coupling principle, which we discuss in the next section.\n3.2.2\nLoose coupling principle\nLoose coupling states that we must design services with clear separation of concerns.\nLoosely coupled services don’t rely on another’s implementation details. What does\nthis mean in practice? This principle has two practical implications:\nEach service can work independently of others. If we have a service that can’t\nfulfill a single request without calling another service, there’s no clear separa-\ntion of concerns between both services and they belong together.\nEach service can be updated without impacting other services. If changes to a\nservice require updates to other services, we have tight coupling between those\nservices, and therefore they need to be redesigned.\nFigure 3.2 shows a sales forecast service that knows how to calculate a forecast based\non historical data. It also shows a historical data service that owns historical sales data.\nTo calculate a forecast, the sales forecast service makes an API call to the historical\ndata service to obtain historical data. In this case, the sales forecast service can’t serve\nany request without calling the historical data service, and therefore there’s tight cou-\npling between both services. The solution is to redesign both services so that they\ndon’t rely on each other, or to merge them into a single service.\nHistorical data\nservice\n2. The sales forecast service\ncalculates and serves\nthe forecast.\n1. The sales forecast service\nrequests data from the\nhistorical data service.\nSales forecast\nservice\nFigure 3.2\nWhen a service can’t serve a single request without calling \nanother service, we say both are tightly coupled.",
      "content_length": 2244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "49\n3.3\nService decomposition by business capability\n3.2.3\nSingle Responsibility Principle\nThe SRP states that we must design components with few responsibilities, and ideally\nwith only one responsibility. When applied to the microservices architecture design,\nthis means we should strive for the design of services around a single business capabil-\nity or subdomain. In the following sections, you’ll learn how to decompose services by\nbusiness capability and by subdomain. If you follow any of those methods, you’ll be\nable to design microservices that follow the SRP.\n3.3\nService decomposition by business capability\nWhen using decomposition by business capability, we look into the activities a business\nperforms and how the business organizes itself to undertake them. We then design\nmicroservices that mirror the organizational structure of the business. For example, if\nthe business has a customer management team, we build a customer management ser-\nvice; if the business has a claims management team, we build a claims management\nservice; for a kitchen team, we build the corresponding kitchen service; and so on. For\nbusinesses that are structured around products, we may have a microservice per prod-\nuct. For example, a company that makes pet food may have a team dedicated to dog\nfood, another team dedicated to cat food, another team dedicated to turtle food, and\nso on. In this scenario, we build microservices for each of these teams.\n As you can see in figure 3.3, decomposition by business capability generally results\nin an architecture that maps every business team to a microservice. Let’s see how we\napply this approach to the CoffeeMesh platform.\n3.3.1\nAnalyzing the business structure of CoffeeMesh\nTo apply decomposition by business capability, we need to analyze the structure and\norganization of the business. Let’s do this analysis for CoffeeMesh. Through the Coffee-\nMesh website, customers can order different types of coffee-related products out of a\nOrganization\nCustomer team\nClaims team\nKitchen team\nCustomer\nmicroservice\nClaims\nmicroservice\nKitchen\nmicroservice\nFigure 3.3\nUsing service decomposition by business capability, we reflect the \nstructure of the business in our microservices architecture.",
      "content_length": 2235,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "50\nCHAPTER 3\nDesigning microservices\ncatalogue managed by the products team, who is in charge of creating new products.\nThe availability of products and ingredients depends on the CoffeeMesh stock of\ningredients at the time of the order, which is looked after by the inventory team.\n A sales team is dedicated to improving the experience of ordering products through\nthe CoffeeMesh website. Their goal is to maximize sales and ensure customers are\nhappy with their experience and wish to come back. A finance team makes sure that the\ncompany is profitable and looks after the financial infrastructure required to process\ncustomer payments and return their money when they cancel an order.\n Once a user places an order, the kitchen picks up its details to commence produc-\ntion. Kitchen work is fully automated, and a dedicated team of engineers and chefs\ncalled the kitchen team monitors kitchen operations to ensure no faults happen\nduring production. When the order is ready for delivery, a drone picks it up and flies\nit to the customer. A dedicated team of engineers called the delivery team monitors\nthis process to ensure the operational excellence of the delivery process.\n This completes our analysis of the organizational structure of CoffeeMesh. We’re\nnow ready to design a microservices architecture based on this analysis.\n3.3.2\nDecomposing microservices by business capabilities\nTo decompose services by business capability, we map each business team to a micros-\nervice. Based on the analysis in section 3.3.1, we can map the following business teams\nto microservices:\nProducts team maps to the products service—This service owns CoffeeMesh product\ncatalogue data. The products team uses this service to maintain CoffeeMesh’s\ncatalogue by adding new products or updating existing products through the\nservice’s interface.\nIngredients team maps to the ingredients service—This service owns data about Cof-\nfeeMesh stock of ingredients. The ingredients team uses this service to keep the\ningredients database in sync with CoffeeMesh warehouses.\nSales team maps to the sales service—This service guides customers through their\njourney to place orders and keep track of them. The sales team owns data about\ncustomer orders, and it manages the life cycle of each order. It collects data\nfrom this service to analyze and improve the customer journey.\nFinance team maps to the finance service—This service implements payment proces-\nsors, and it owns data about user payment details and payment history. The\nfinance team uses this service to keep the company accounts up to date and to\nensure payments work correctly.\nKitchen team maps to the kitchen service—This service sends orders to the auto-\nmated kitchen system and keeps track of its progress. It owns data about the\norders produced in the kitchen. The kitchen team collects data from this ser-\nvice to monitor the performance of the automated kitchen system.\nDelivery team maps to the delivery service—This service arranges the delivery of the\norder to the customer once it has been produced by the kitchen. This service",
      "content_length": 3092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "51\n3.3\nService decomposition by business capability\nknows how to translate the user location into coordinates and how to calculate\nthe best route to that destination. It owns data about every delivery made by\nCoffeeMesh. The delivery team collects data from this service to monitor the\nperformance of the automated delivery system.\nIn this microservices architecture, we named every service after the business structure\nit represents. We did this for convenience in this example, but it does not have to be\nthat way. For example, the finance service could be renamed to payments service,\nsince all user interactions with this service will be related to their payments.\n Decomposition by business capability gives us an architecture in which every ser-\nvice maps to a business team. Is this result in agreement with the principles of micros-\nervices design we learned in section 3.2? Let’s look at this question.\n From the previous analysis, it’s clear that every service owns its own data: the\nproducts service owns product data, the ingredients service owns ingredients data,\nand so on. The SRP also applies, as every service is restricted to one business area:\nthe finance service only processes payments, the delivery service only manages deliv-\neries, and so on.\n However, as you can see in figure 3.4, this solution doesn’t satisfy the loose cou-\npling principle. To serve the CoffeeMesh catalogue, the products service needs to\ndetermine the availability of each product, which depends on the available stock of\ningredients. Since the stock of ingredients data is owned by the ingredients service,\nthe products service needs to make an API call per product to the ingredients service.\nThere’s a high degree of coupling between the products and ingredients services,\nand therefore both business capabilities should be implemented within the same service.\nProducts service\nIngredients service\nCoﬀeeMesh catalogue\nIs it available?\nFigure 3.4\nTo determine whether a product is available, the products \nservice checks the stock of ingredients with the ingredients service.",
      "content_length": 2073,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "52\nCHAPTER 3\nDesigning microservices\nFigure 3.5 shows the final layout of the CoffeeMesh microservices architecture using\nthe decomposition by business capability strategy.\nNow that we know how to decompose services by business capability, let’s see how\ndecomposition by subdomain works.\n3.4\nService decomposition by subdomains\nDecomposition by subdomains is an approach that draws inspiration from the field of\ndomain-driven design (DDD)—an approach to software development that focuses on\nmodeling the processes and flows of the business with software using the same lan-\nguage business users employ. When applied to the design of a microservices platform,\nDDD helps us define the core responsibilities of each service and their boundaries.\n3.4.1\nWhat is domain-driven design?\nDDD is an approach to software that focuses on modeling the processes and flows of\nthe business users. The methods of DDD were best described by Eric Evans in his\ninfluential book Domain-Driven Design (Addison-Wesley, 2003), otherwise called “the\nbig blue book.” DDD offers an approach to software development that tries to reflect\nas accurately as possible the ideas and the language that businesses, or end users of\nthe software, use to refer to their processes and flows. To achieve this alignment,\nDDD encourages developers to create a rigorous, model-based language that software\nProducts team\nProducts service\nIngredients team\nKitchen team\nSales team\nFinance team\nDelivery team\nDelivery service\nKitchen service\nFinance service\nSales service\nFigure 3.5\nWhen we decompose \nservices by business capability, we \nmap every team to a service.",
      "content_length": 1621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "53\n3.4\nService decomposition by subdomains\ndevelopers can share with the end users. Such language must not have ambiguous\nmeanings and is called ubiquitous language.\n To create an ubiquitous language, we must identify the core domain of a business,\nwhich corresponds with the main activity an organization performs to generate value.\nFor a logistics company, it may be the shipment of products around the world. For an\ne-commerce company, it may be the sale of products. For a social media platform, it\nmay be feeding a user with relevant content. For a dating app, it may be matching\nusers. For CoffeeMesh, the core domain is to deliver high-quality coffee to customers\nas quickly as possible regardless of their location.\n The core domain is often not sufficient to cover all areas of activity in a business, so\nDDD also distinguishes supportive subdomains and generic subdomains. A supportive\nsubdomain represents an area of the business that is not directly related to value gener-\nation, but it is fundamental to support it. For a logistics company, it may be providing\ncustomer support to the users shipping their products, leasing equipment, managing\npartnerships with other businesses, and so on. For an e-commerce company, it may be\nmarketing, customer support, warehousing, and so on.\n The core domain gives you a definition of the problem space: the problem you are\ntrying to solve with software. The solution consists of a model, understood here as a\nsystem of abstractions that describes the domain and solves the problem. Ideally, there\nis only one generic model that provides a solution space for the problem, with a clearly\ndefined ubiquitous language. However, in practice, most problems are complex\nenough that they require the collaboration of different models, with their own ubiqui-\ntous languages. We call the process of defining such models strategic design.\n3.4.2\nApplying strategic analysis to CoffeeMesh\nHow does DDD work in practice? How do we apply it to decompose CoffeeMesh into\nsubdomains? To break down a system into subdomains, it helps to think about the\noperations the system has to perform to accomplish its goal. With CoffeeMesh, we\nwant to model the process of taking an order and delivering it to the customer. As you\ncan see in figure 3.6, we break down this process into eight steps:\n1\nWhen the customer lands on the website, we show them the product catalogue.\nEach product is marked as available or unavailable. The customer can filter\nthe list by availability and sort it by price (from lowest to highest and highest\nto lowest).\n2\nThe customer selects products.\n3\nThe customer pays for their order.\n4\nOnce the customer has paid, we pass on the details of the order to the kitchen.\n5\nThe kitchen picks up the order and produces it.\n6\nThe customer monitors progress on their order.\n7\nOnce the order is ready, we arrange its delivery.\n8\nThe customer tracks the drone’s itinerary until their order is delivered.",
      "content_length": 2951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "54\nCHAPTER 3\nDesigning microservices\nLet’s map each step to its corresponding subdomain (see figure 3.7 for a representa-\ntion of this analysis). The first step represents a subdomain that serves the CoffeeMesh\nproduct catalogue. We can call it the products subdomain. This subdomain tells us which\nStep 1: The customer lands\non the website, and we show\na list of products with their\navailability.\nxxxx - xxxx - xxxx\nStep 3 : The customer pays for\ntheir order.\nStep 5: The kitchen picks up\nthe order and produces it.\n60% complete\nStep 6: The customer\nmonitors progress on their order.\nStep 7: We arrange\nthe order’s delivery.\nYour order is getting closer!\nStep 8: The customer tracks\nthe order’s delivery.\nCustomer\nCappuccino\nLatte (out of stock)\nMocha\n$10\n$4\n$2\nStep 2: The customer\nselects a product.\nCappuccino\nLatte (out of stock)\nMocha\n$10\n$4\n$2\nxxx\nStep 4: We pass on the\ndetails of the order to the\nkitchen.\n1 x Mocha\nTotal: $2\n1 x Mocha\nTotal: $2\nKitchen\nFigure 3.6\nTo place an order, the customer lands on the CoffeeMesh website, selects \nitems from the product catalogue, and pays for the order. After payment, we pass the \norder’s details to the kitchen, which produces it while the customer monitors its progress. \nFinally, we arrange the order’s delivery.",
      "content_length": 1268,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "55\n3.4\nService decomposition by subdomains\nproducts are available and which are not. To do so, the products subdomain tracks the\namount of each product and ingredient in stock.\n The second step represents a subdomain that allows users to select products. This\nsubdomain manages the life cycle of each order, and we call it the orders subdomain.\nThis subdomain owns data about users’ orders, and it exposes an interface that allows\nus to manage orders and check their status. It hides the complexity of the platform so\nthat the user doesn’t have to know about different endpoints and know what to do\nwith them. The orders subdomain also takes care of the second part of the fourth\nstep: passing the details of the order to the kitchen once the payment has been suc-\ncessfully processed. It also meets the requirements for step 6: allow the user to check\nthe state of their order. As an orders manager, the orders subdomain also works with\nthe delivery subdomain to arrange the delivery.\n The third step represents a subdomain that can handle user payments. We will call it\nthe payments subdomain. This domain contains specialized logic for payment processing,\nProducts subdomain\nWhen the customer\nlands on the\nwebsite, we show\na list of products\nwith their availability.\nOrders subdomain\nThe customer\nplaces an order.\nPayments subdomain\nThe customer\npays for their order.\nOrders subdomain\nUpon successful\npayment, we pass\nthe order to\nthe kitchen.\nKitchen subdomain\nKitchen picks up\nthe order and\nstarts production.\nOrders subdomain\nThe customer keeps\ntrack of the order’s\nprogress.\nDelivery subdomain\nOnce the order\nis produced, we\narrange its delivery.\nOrders subdomain\nThe customer\nkeeps track of the\norder’s itinerary.\nFigure 3.7\nWe map to a subdomain every step in the process of placing and delivering an order. For example, \nthe process of serving the product catalogue is satisfied by the products subdomain, while the process of \ntaking an order is satisfied by the orders subdomain.",
      "content_length": 1991,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "56\nCHAPTER 3\nDesigning microservices\nincluding card validation, integration with third-party payment providers, handling\ndifferent methods of payment, and so on. The payments subdomain owns data related\nto user payments.\n The fifth step represents a subdomain that works with the kitchen to manage the\nproduction of customer orders. We call it the kitchen subdomain. The production sys-\ntem in the kitchen is fully automated, and the kitchen subdomain interfaces with the\nkitchen system to schedule the production of customer orders and track their prog-\nress. Once an order is produced, the kitchen subdomain notifies the orders subdo-\nmain, which then arranges its delivery. The kitchen subdomain owns data related to\nthe production of customer orders, and it exposes an interface that allows us to send\norders to the kitchen and keep track of their progress. The orders subdomain inter-\nfaces with the kitchen subdomain to update the order’s status to meet the require-\nments for the sixth step.\n The seventh step represents a subdomain that interfaces with the automated deliv-\nery system. We call it the delivery subdomain. This subdomain contains specialized logic\nto resolve the geolocation of a customer and to calculate the most optimal route to\nreach them. It manages the fleet of delivery drones and optimizes the deliveries, and it\nowns data related to all the deliveries. The orders subdomain interfaces with the deliv-\nery subdomain to update the itinerary of the customer’s order to meet the require-\nments for the eighth step.\n Using strategic analysis, we obtain a decomposition for CoffeeMesh in five subdo-\nmains, which can be mapped to microservices, as each encapsulates a well-defined\nand clearly differentiated area of logic that owns its own data. DDD’s strategic analysis\nresults in microservices that satisfy the design principles we enumerated in section 3.2:\nall these subdomains can perform their core tasks without relying on other microser-\nvices, and therefore we say they’re loosely coupled; each service owns its own data,\nhence complying with the database-per-service principle; finally, each service performs\ntasks within a narrowly defined subdomain, which complies with the SRP.\n As you can see in figure 3.8, strategic analysis gives us the following microservices\narchitecture:\nProducts subdomain maps to the products service—Manages CoffeeMesh’s product\ncatalogue\nOrders subdomain maps to the orders service—Manages customer orders\nPayments subdomain maps to the payments service—Manages customer payments\nKitchen subdomain maps to the kitchen service—Manages the production of orders\nin the kitchen\nDelivery subdomain maps to the delivery service—Manages customer deliveries\nIn the next section, we compare the results of DDD’s strategic analysis with the out-\ncome of service decomposition by business capability, and we evaluate the benefits\nand challenges of each approach.",
      "content_length": 2923,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "57\n3.5\nDecomposition by business capability vs. decomposition by subdomain\n3.5\nDecomposition by business capability vs. \ndecomposition by subdomain\nWhich service decomposition strategy should we use to design our microservices: decom-\nposition by business capability or decomposition by subdomains? While decomposition\nby business capability focuses on business structure and organization, decomposition by\nsubdomain analyzes business processes and flows. Therefore, both approaches give us\ndifferent perspectives on the business, and if you can spare the time, the best strategy is to\napply both approaches to service decomposition.\n Sometimes we can combine the results of both approaches. For example, the Coffee-\nMesh platform could allow customers to write reviews for each product, and CoffeeMesh\ncould leverage this information to recommend new products to other customers. The\ncompany could have an entire team dedicated to this aspect of the business. From a\ntechnical point of view, reviews could be just another table in the Products database.\nHowever, to facilitate collaboration with the business, it could make sense to build a\nreviews service. The reviews service would be able to feed new reviews into the recom-\nmendation system, and the orders service would use the reviews service’s interface to\nserve recommendations to new users.\n The advantage of decomposition by business capability is that the architecture of\nthe platform aligns with the existing organizational structure of the business. This align-\nment might facilitate the collaboration between business and technical teams. The\ndownside of this approach is that the existing organizational structure of the business is\nProducts subdomain\nDelivery subdomain\nOrders subdomain\nPayments subdomain\nKitchen subdomain\nProducts service\nDelivery service\nOrders service\nPayments service\nKitchen service\nSubdomains\nMicroservices\nFigure 3.8\nApplying DDD’s strategic \nanalysis breaks down the CoffeeMesh \nplatform into five subdomains that can \nbe mapped directly to microservices.",
      "content_length": 2048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "58\nCHAPTER 3\nDesigning microservices\nnot necessarily the most efficient one. As a matter of fact, it can be outdated and\nreflect old business processes. In that case, the inefficiencies of the business will be\nmirrored in the microservices architecture. Decomposition by business capability also\nrisks falling out of alignment with the business if the organization is restructured.\n When we applied decomposition by business capability in section 3.3.2, we obtained\nan undesirable division between the products and ingredients services. After further\nanalysis of the dependencies between both services, we concluded that both capabili-\nties should go into the same service. However, in real-life situations, this additional\nanalysis is often missing, and the resulting architecture isn’t optimal. From the analy-\nsis in sections 3.3 and 3.4, we can say that decomposition by subdomain gives you a\nbetter architectural fit to model the business processes and flows, and if you must\nchoose only one approach, decomposition by subdomain is the better strategy.\n Now that we know how to design our microservices, it’s time to design and build\ntheir interfaces. In the upcoming chapters, you’ll learn to build REST and GraphQL\ninterfaces for microservices.\nSummary\nWe call the process of breaking down a system into microservices service decom-\nposition. Service decomposition defines the boundaries between services, and we\nmust get this process right to avoid the risk of building a distributed monolith.\nDecomposition by business capability analyzes the structure of the business and\ndesigns microservices for each team in the organization. This approach aligns\nthe business with our system architecture, but it also reproduces the inefficien-\ncies of the business into the platform.\nDecomposition by subdomains applies DDD to model the processes and flows\nof the business through subdomains. By using this approach, we design a micro-\nservice for each subdomain, which results in a more robust technical design.\nTo assess the quality of our microservices architecture, we apply three design\nprinciples:\n– Database-per-service principle—Each microservice owns its own data, and access\nto that data happens through the service’s API.\n– Loose coupling principle—You must be able to update a service without impact-\ning other services, and each service should be able to work without con-\nstantly calling other services.\n– Single Responsibility Principle—We must design each service around a specific\nbusiness capability or subdomain.",
      "content_length": 2533,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "Part 2\nDesigning and\nbuilding REST APIs\nIn part 1, you learned what microservice APIs are and how to decompose a\nsystem into microservices. The natural questions now are, “How do you build a\nmicroservice?” and “How do you make your services talk to each other?”\n We make services talk to each other using APIs, and in part 2 you learn to\ndesign and build REST APIs. Representational State Transfer (REST) is the most\npopular technology for building APIs, and in chapter 4 you learn all the funda-\nmental principles of REST API design. We’ll keep the approach practical: in\nchapter 1, we introduced CoffeeMesh, an on-demand coffee delivery applica-\ntion, and in chapter 6, you learn to build CoffeeMesh’s orders and kitchen APIs\nusing Python’s popular FastAPI and Flask frameworks.\n In chapter 1 we introduced documentation-driven development and high-\nlighted the importance of API documentation, which tells your API clients how\nthe API works; therefore, good documentation is essential to deliver successful\nintegrations. We document REST APIs using the OpenAPI standard, and in\nchapter 5 you learn step by step how to document a REST API.\n Finally, in chapter 7 you learn everything you need to build microservices.\nYou learn to implement your data layer using SQLAlchemy and to manage\nmigrations using Alembic. You learn to structure your application using hexago-\nnal architecture, as well as many other useful patterns and principles to encapsu-\nlate your code and maintain loose coupling between layers.",
      "content_length": 1510,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "60\nPART 2\nDesigning and building REST APIs\n By the end of part 2 you’ll be able to design great REST APIs, produce excellent\nAPI documentation, and write highly readable and maintainable service implementa-\ntions. I can’t wait to get started!",
      "content_length": 242,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "61\nPrinciples of\nREST API design\nRepresentational state transfer (REST) describes an architectural style for applica-\ntions that communicate over a network. Originally, the concept of REST included a\nlist of constraints for the design of distributed and scalable web applications. Over\ntime, detailed protocols and specifications have emerged that give us well-defined\nguidelines for designing REST APIs. Today, REST is by far the most popular choice\nfor building web APIs.1 In this chapter, we study the design principles of REST and\nThis chapter covers\nThe design principles of REST APIs\nHow the Richardson maturity model helps us \nunderstand the advantages of REST best design \nprinciples\nThe concept of resource and the design of \nendpoints for REST APIs\nUsing HTTP verbs and HTTP status codes to \ncreate highly expressive REST APIs\nDesigning high-quality payloads and URL query \nparameters for REST APIs\n1 The 2022 “State of the API Report” by Postman found that the majority of participants in the survey (89%)\nuse REST (https://www.postman.com/state-of-api/api-technologies/#api-technologies).",
      "content_length": 1105,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "62\nCHAPTER 4\nPrinciples of REST API design\nlearn to apply them by designing the orders API of the CoffeeMesh platform, the on-\ndemand coffee delivery application we introduced in chapter 1.\n We explain the concept of a resource, and what it means for the design of REST\nAPIs. You’ll also learn to leverage features of the HTTP protocol, such as HTTP verbs\nand status codes, to create highly expressive APIs. The final part of this chapter covers\nbest practices for designing API payloads and URL query parameters.\n4.1\nWhat is REST?\nREST, a term coined by Roy Fielding in his doctoral dissertation “Architectural Styles\nand the Design of Network-based Software Architectures” (PhD diss., University of\nCalifornia, Irvine, 2000, p. 109), describes an architectural style for loosely coupled\nand highly scalable applications that communicate over a network. It refers to the abil-\nity to transfer the representation of a resource’s state. The concept of resource is fun-\ndamental in REST applications. \nDEFINITION\nREST is an architectural style for building loosely coupled and\nhighly scalable APIs. REST APIs are structured around resources, entities that\ncan be manipulated through the API.\nA resource is an entity that can be referenced by a unique hypertext reference (i.e.,\nURL). There are two types of resources: collections and singletons. A singleton rep-\nresents a single entity, while collections represent lists of entities.2 What does this mean\nin practice? It means that we use different URL paths for each type of resource. For\nexample, CoffeeMesh’s orders service manages orders, and through its API we can\naccess a specific order through the /orders/{order_id} URL path, while a collection\nof orders is available under the /orders URL path. Therefore, /orders/{order_id} is\na singleton endpoint, while /orders is a collections endpoint.\n Some resources can be nested within another resource, such as a payload for an\norder with several items listed in a nested array.\n{\n    \"id\": \"924721eb-a1a1-4f13-b384-37e89c0e0875\",\n    \"status\": \"progress\",\n    \"created\": \"2023-09-01\",\n    \"order\": [\n        {\n            \"product\": \"cappuccino\",\n            \"size\": \"small\",\n            \"quantity\": 1\n        },\n2 See Prakash Subramaniam’s excellent article “REST API Design—Resource Modeling” for an in-depth discus-\nsion of resources and resource modeling in REST APIs (https://www.thoughtworks.com/en-gb/insights/blog/\nrest-api-design-resource-modeling).\nListing 4.1\nExample of payload with nested resources",
      "content_length": 2514,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "63\n4.2\nArchitectural constraints of REST applications\n        {\n            \"product\": \"croissant\",\n            \"size\": \"medium\",\n            \"quantity\": 2\n        }\n    ]\n}\nWe can create nested endpoints to represent nested resources. Nested endpoints\nallow us to access specific details of a resource. For example, we can expose a GET\n/orders/{order_id}/status endpoint that allows us to get an order’s status without\nall the other details about the order. Using nested endpoints is a common optimiza-\ntion strategy when resources are represented by large payloads since they help us\navoid costly data transfers when we are only interested in one property.\n The resource-oriented nature of REST APIs may sometimes appear limiting. A\ncommon concern is how to model actions through endpoints while keeping our APIs\nRESTful. For example, how do we represent the action of cancelling an order? A com-\nmon heuristic is to represent actions as nested resources. For example, we can have a\nPOST /orders/{order_id}/cancel endpoint to cancel orders. In this case, we model\nthe order’s cancellation as creating a cancellation event.\n Designing clean endpoints is the first step toward building REST APIs that are easy\nto maintain and to consume. The patterns you’ve learned in this section go a long way\nto achieving clean endpoints, and in the rest of this chapter, you’ll learn additional\npatterns and principles for clean API design. In the next section, you’ll learn about\nthe six architectural constraints of REST API applications.\n4.2\nArchitectural constraints of REST applications\nIn this section, we study the architectural constraints of REST applications. These con-\nstraints were enumerated by Fielding, and they specify how a server should process\nand respond to a client request. Before we delve into the details, let’s first provide a\nbrief overview of each constraint:\nClient-server architecture—The user interface (UI) must be decoupled from the\nbackend.\nStatelessness—The server must not manage states between requests.\nCacheability—Requests that always return the same response must be cacheable.\nLayered system—The API may be architected in layers, but such complexity must\nbe hidden from the user.\nCode on demand—The server can inject code into the user interface on demand.\nUniform interface—The API must provide a consistent interface for accessing\nand manipulating resources.\nLet’s discuss each of these constraints in more detail.",
      "content_length": 2452,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "64\nCHAPTER 4\nPrinciples of REST API design\n4.2.1\nSeparation of concerns: The client-server architecture principle\nREST relies on the principle of separation of concerns, and consequently it requires\nthat user interfaces are decoupled from data storage and server logic. This allows\nserver-side components to evolve independently from UI elements. As you can see\nin figure 4.1, a common implementation of the client-server architectural pattern is\nbuilding the UI as a standalone application, for example, as a single-page applica-\ntion (SPA).\n4.2.2\nMake it scalable: The statelessness principle\nIn REST, every request to the server must contain all the information necessary to pro-\ncess it. In particular, the server must not keep state from one request to the next. As\nyou can see in figure 4.2, removing state management from server components makes\nCDN\nSPA\nAPI server\nGET /orders\n{\"orders\": [...]]}\nFigure 4.1\nREST’s client-server architecture principle states that the server implementation \nmust be decoupled from the client.\nOrders service\ninstance\nOrders service\ninstance\nOrders service\ninstance\nPOST /orders\n{id: 8, ...}\nGET /orders/8\nPOST /orders/8/pay\nAPI client\nFigure 4.2\nREST’s statelessness principle states that the server must not manage the \nstate of the client. This allows us to deploy multiple instances of the API server and respond \nto the API client with any of them.",
      "content_length": 1391,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "65\n4.2\nArchitectural constraints of REST applications\nit easier to scale the backend horizontally. This allows us to deploy multiple instances\nof the server, and because none of those instances manages the API client’s state, the\nclient can communicate with any of them. \n4.2.3\nOptimize for performance: The cacheability principle\nWhen applicable, server responses must be cached. Caching improves the perfor-\nmance of APIs because it means we don’t have to perform all the calculations\nrequired to serve a response again and again. GET requests are suitable for caching,\nsince they return data already saved in the server. As you can see in figure 4.3, by cach-\ning a GET request, we avoid having to fetch data from the source every time a user\nrequests the same information. The longer it takes to assemble the response for a\nGET request, the greater the benefits of caching it.\nFigure 4.3 illustrates the benefits of caching. As we learned in chapter 3, customers\ncan track the progress on their orders once they’ve been submitted to the kitchen.\nOrders service\nCache storage\nKitchen service\nGET /orders/8\n{\n\"id\": 8,\n\"status\": \"progress\"\n}\n{\n\"id\": 8,\n\"status\": \"progress\"\n}\nGET /kitchen/8/status\nOrders service\nGET /orders/8\nWhen we check the order’s status for the first time, the orders service requests the status from\nthe kitchen service and saves it to a cache store.\nWhen we check the order’s status the next time, the orders\nservice reads the value from the cache.\n{\n\"id\": 8,\n\"created\": \"2025-02-02\",\n\"status\": \"progress\",\n\"order\": [...]\n}\n{\n\"id\": 8,\n\"created\": \"2025-02-02\",\n\"status\": \"progress\",\n\"order\": [...]\n}\nCache the result\nFigure 4.3\nREST’s cacheability principle states that cacheable responses must be cached, which helps to boost \nthe API server performance. In this example, we cache the order’s status for a short period of time to avoid \nmultiple requests to the kitchen service.",
      "content_length": 1904,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "66\nCHAPTER 4\nPrinciples of REST API design\nThe orders service interfaces with the kitchen service to obtain information on the\norder’s progress. To save time the next time the customer checks the order’s status, we\ncache its value for a short period of time.\n4.2.4\nMake it simple for the client: The layered system principle\nIn a REST architecture, clients must have a unique point of entry to your API and must\nnot be able to tell whether they are connected directly to the end server or to an inter-\nmediary layer such as a load balancer. You can deploy different components of a server-\nside application in different servers, or you can deploy the same component across dif-\nferent servers for redundancy and scalability. This complexity should be hidden from\nthe user by exposing a single endpoint that encapsulates access to your services.\n As you can see in figure 4.4, a common solution to this problem is the API gateway\npattern, which is a component that serves as an entry point for all microservices. The\nAPI gateway knows the server addresses of each service, and it knows how to map each\nrequest to the relevant service.3\n4.2.5\nExtendable interfaces: The code-on-demand principle\nServers can extend the functionality of a client application by sending executable\ncode directly from the backend, such as JavaScript files needed to run a UI. This\nconstraint is optional and only applies to applications in which the backend serves\nthe client interface.\n3 For more information on this pattern, see Chris Richardson, Microservices Patterns (Manning, 2019, pp. 259–\n291; https://livebook.manning.com/book/microservices-patterns/chapter-8/point-8620-53-297-0).\nAPI gateway\nOrders service\n/orders\n/orders\nAPI client\nKitchen service\n/kitchen\nPayments service\n/payments\nCoﬀeeMesh\nFigure 4.4\nREST’s layered system principle states that the complexity of \nour backend must be hidden from the client. A common solution to this \nproblem is the API gateway pattern, which serves as an entry point to all the \nservices in the platform.",
      "content_length": 2033,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "67\n4.3\nHypermedia as the engine of application state\n4.2.6\nKeep it consistent: The uniform interface principle\nREST applications must expose a uniform and consistent interface to their con-\nsumers. The interface must be documented, and the API specification must be fol-\nlowed strictly by the server and the client. Individual resources are identified by a\nUniform Resource Identifier (URI),4 and each URI must be unique and always\nreturn the same resource. For example, the URI /orders/8 represents an order\nwith ID 8, and a GET request on this URI always returns the state of the order with\nID 8. If the order is deleted from the system, the ID must not be reused to repre-\nsent a different order.\n Resources must be represented using a serialization method of choice, and that\napproach should be used consistently across the API. Nowadays, REST APIs typi-\ncally use JSON as the serialization format, although other formats are also possible,\nsuch as XML.\n The architectural constraints of REST give us solid ground for designing robust\nand scalable APIs. But as we’ll see in the following sections of this chapter, there are\nmore factors we need to consider when designing an API. In the next section, you’ll\nlearn to make your APIs discoverable by enriching a resource’s description with\nrelated hypermedia links.\n4.3\nHypermedia as the engine of application state \nNow that we understand the most important design constraints of REST APIs, let’s look\nat another important concept in REST: hypermedia as the engine of application state\n(HATEOAS). HATEOAS is a paradigm in the design of REST APIs that emphasizes the\nconcept of discoverability. HATEOAS makes APIs easier to use by enriching responses\nwith all the information users need to interact with a resource. In this section, we explain\nhow HATEOAS works, and we discuss the benefits and disadvantages of this approach.\n What exactly is HATEOAS? In an article written in 2008 with the title “REST APIs\nMust Be Hypertext-Driven” (http://mng.bz/p6y5), Fielding suggested that REST APIs\nmust include related links in their responses to allow clients to navigate the API by fol-\nlowing those links.\nDEFINITION\nHypermedia as the engine of application state (HATEOAS) is a design\nparadigm of REST that emphasizes the idea of discoverability. Whenever a cli-\nent requests a resource from the server, the response must contain a list of\nrelated links to the resource. For example, if a client requests the details of an\norder, the response must include the links to cancel and pay for the order.\nFor example, as you can see in figure 4.5, when a client requests the details of an\norder, the API includes a collection of links related to the order. With those links, we\ncan cancel the order, or we can pay for it.\n4 For the latest specification on URIs, see “RFC 7320: URI Design and Ownership” by M. Nottingham (July 2004,\nhttps://tools.ietf.org/html/rfc7320).",
      "content_length": 2911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "68\nCHAPTER 4\nPrinciples of REST API design\n{\n    \"id\": 8,\n    \"status\": \"progress\",\n    \"created\": \"2023-09-01\",\n    \"order\": [\n        {\n            \"product\": \"cappuccino\",\n            \"size\": \"small\",\n            \"quantity\": 1\n        },\n        {\n            \"product\": \"croissant\",\n            \"size\": \"medium\",\n            \"quantity\": 2\n        }\n    ],\n    \"links\": [\n        {\n            \"href\": \"/orders/8/cancel\",\nListing 4.2\nRepresentation of an order including hypermedia links\n{\n\"status\": “progress\",\n\"created\": \"2022-09-01\",\n\"order\": [\n{\n\"product\": \"cappuccino\",\n\"size\": \"small\",\n\"quantity\": 1\n}\n],\n\"links\": [\n{\n\"href\": \"/orders/8/cancel\",\n\"description\": \"Cancels the order\",\n\"type\": \"POST\"\n},\n{\n\"href\": \"/orders/8/pay\",\n\"description\": \"Pays an order\",\n\"type\": \"POST\"\n}\n]\n}\nOrders service\nOrders API\nRequest: GET /orders/8\nRepresentation\nof the order\nResponse\nRelated links\nCancellation link\nPayment link\n\"id\": 8,\nFigure 4.5\nIn the HATEOAS paradigm, the API sends a representation of the requested resource with other \nlinks related to the resource.",
      "content_length": 1064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "69\n4.3\nHypermedia as the engine of application state\n            \"description\": \"Cancels the order\",\n            \"type\": \"POST\"\n        },\n        {\n            \"href\": \"/orders/8/pay\",\n            \"description\": \"Pays for the order\",\n            \"type\": \"POST\"\n        }\n    ]\n}\nProviding relational links makes APIs navigational and easier to use, since every\nresource comes with all the URLs we need to work with it. However, in practice, many\nAPIs are not implemented that way for several reasons:\nThe information supplied by hyperlinks is already available in the API docu-\nmentation. In fact, the information contained in an OpenAPI specification is\nfar richer and more structured than what you can provide in a list of related\nlinks for specific resources.\nIt’s not always clear exactly what links should be returned. Different users have\ndifferent levels of permissions and roles, which allow them to perform different\nactions and access different resources. For example, external users can use the\nPOST /orders endpoint in the CoffeeMesh API to place an order, and they are\nalso able to use the GET /orders/{order_id} endpoint to retrieve the details\nof an order. However, they cannot use the DELETE /orders/{order_id} end-\npoint to delete an order, since this endpoint is restricted to internal users of the\nCoffeeMesh platform. If the point of HATEOAS is to make the API navigational\nfrom a single point of entry, it wouldn’t make sense to return the DELETE\n/orders/{order_id} endpoint to external users since they are not able to use\nit. Therefore, it’s necessary to return different lists of related links to different\nusers according to their permissions. However, this level of flexibility introduces\nadditional complexity in our API designs and implementations and couples the\nauthorization layer with the API layer.\nDepending on the state of the resource, certain actions and resources may not\nbe available. For example, you can call the POST /orders/1234/cancel endpoint\non an active order but not on a cancelled order. This level of ambiguity makes it\nhard to define and implement robust interfaces that follow the HATEOAS\nprinciples.\nFinally, in some APIs, the list of related links may be large and therefore make the\nresponse payload too big, hence compromising the performance of the API and\nthe reliability of the connection for small devices with low network connectivity. \nWhen working on your own APIs, you can decide whether to follow the HATEOAS\nprinciples. There’s a certain level of benefit in providing lists of related resources\nin some cases. For example, in a wiki application, the linked resources section of a",
      "content_length": 2650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "70\nCHAPTER 4\nPrinciples of REST API design\npayload can be used to list content related to a specific article, links to the same\narticle in other languages, and links to actions that can be performed on the arti-\ncle. Overall, you may want to strike a balance between what your API documenta-\ntion already provides to the client in a more clear and detailed way, and what you\ncan offer in your responses to facilitate the interaction between the client and the\nAPI. If you’re building a public-facing API, your clients will benefit from relational\nlinks. However, if it’s a small internal API, it’s probably unnecessary to include rela-\ntional links.\n Now that we know how we make our APIs discoverable and when it’s worth doing\nso, let’s study the Richardson maturity model, which will help you understand to what\nextent your APIs comply with the design principles of REST.\n4.4\nAnalyzing the maturity of an API with the Richardson \nmaturity model\nThis section discusses the Richardson maturity model, a mental model developed by\nLeonard Richardson to help us think about the degree to which an API complies with\nthe principles of REST.5 The Richardson maturity model distinguishes four levels\n(from level 0 to level 3) of “maturity” in an API. Each level introduces additional ele-\nments of good REST API design (figure 4.6). Let’s discuss each level in detail.\n5  Leonard Richardson presented his maturity model in his talk “Justice Will Take Us Millions of Intricate Moves”\nat QCon San Francisco in 2008 (https://www.crummy.com/writing/speaking/2008-QCon/).\nLevel 0: RPC over HTTP\nLevel 1: Resources\nLevel 2: HTTP methods and status\ncodes\nLevel 3: Hypermedia controls\n(HATEOAS)\nGlory of REST\nFigure 4.6\nThe Richardson maturity model \ndistinguishes four levels of API maturity, where the \nhighest level represents an API design that abides by \nthe best practices and standards of REST, while the \nlowest level represents a type of API that doesn’t \napply any of the principles of REST.",
      "content_length": 1986,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "71\n4.4\nAnalyzing the maturity of an API with the Richardson maturity model\n4.4.1\nLevel 0: Web APIs à la RPC\nAt level 0, HTTP is essentially used as a transport system to carry interactions with the\nserver. The notion of API in this case is closer to the idea of a remote procedure call (RPC;\nsee appendix A). All the requests to the server are made on the same endpoint and\nwith the same HTTP method, usually GET or POST. The details of the client’s request\nare carried in an HTTP payload. For example, to place an order through the Coffee-\nMesh website, the client might send a POST request on a generic /api endpoint with\nthe following payload:\n{\n    \"action\": \"placeOrder\",\n    \"order\": [\n        {\n            \"product\": \"mocha\",\n            \"size\": \"medium\",\n            \"quantity\": 2\n        }\n    ]\n} \nThe server invariably responds with 200 status codes and an accompanying payload\nletting us know the outcome of processing the request. Similarly, to get the details of\nan order, a client might make the following POST request on the generic /api end-\npoint (assuming the ID of the order is 8):\n{\n    \"action\": \"getOrder\",\n    \"order\": [\n        {\n            \"id\": 8\n        }\n    ]\n}\n4.4.2\nLevel 1: Introducing the concept of resource\nLevel 1 introduces the concept of resource URLs. Instead of a generic /api endpoint,\nthe server exposes URLs that represent resources. For example, the /orders URL rep-\nresents a collection of orders, while the /orders/{order_id} URL represents a single\norder. To place an order, the client sends a POST request on the /orders endpoint\nwith a similar payload as in level 0:\n{\n    \"action\": \"placeOrder\",\n    \"order\": [\n        {\n            \"product\": \"mocha\",\n            \"size\": \"medium\",\n            \"quantity\": 2",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "72\nCHAPTER 4\nPrinciples of REST API design\n        }\n    ]\n} \nThis time when requesting the details of the last order, the client will make a POST\nrequest on the URI representing that order: /orders/8. At this level, the API doesn’t\ndistinguish between HTTP methods to represent different actions.\n4.4.3\nLevel 2: Using HTTP methods and status codes\nLevel 2 introduces the concept of HTTP verbs and status codes. At this level, HTTP\nverbs are used to represent specific actions. For example, to place an order, a client\nsends a POST request on the /orders endpoint with the following payload:\n{\n    \"order\": [\n        {\n            \"product\": \"mocha\",\n            \"size\": \"medium\",\n            \"quantity\": 2\n        }\n    ]\n}\nIn this case, the HTTP method POST indicates the operation we want to perform,\nand the payload only includes the details of the order we want to place. Similarly, to\nget the details of an order, we send a GET request on the order’s URI: /orders/\n{order_id}. In this case, we use the HTTP verb GET to tell the server that we want to\nretrieve details of the resource specified in the URI.\n While previous levels include the same status code (usually 200) in all responses,\nlevel 2 introduces the semantic use of HTTP status codes to report the outcome of\nprocessing the client’s request. For example, when we create a resource using a POST\nrequest, we get a 201 response status code, and a request for a nonexistent resource\ngets a 404 response status code. For more information on HTTP status codes and best\npractices using them, see section 4.6.\n4.4.4\nLevel 3: API discoverability\nLevel 3 introduces the concept of discoverability by applying the principles of\nHATEOAS and by enriching responses with links that represent the actions we can\nperform on a resource. For example, a GET request on the /orders/{order_id} end-\npoint returns a representation of an order, and it includes a list of related links.\n{\n    \"id\": 8\n    \"status\": \"progress\",\n    \"created\": \"2023-09-01\",\nListing 4.3\nRepresentation of an order, including hypermedia links",
      "content_length": 2067,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "73\n4.5\nStructured resource URLs with HTTP methods\n    \"order\": [\n        {\n            \"product\": \"cappuccino\",\n            \"size\": \"small\",\n            \"quantity\": 1\n        },\n        {\n            \"product\": \"croissant\",\n            \"size\": \"medium\",\n            \"quantity\": 2\n        }\n    ],\n    \"links\": [\n        {\n            \"href\": \"/orders/8/cancel\",\n            \"description\": \"Cancels the order\",\n            \"type\": \"POST\"\n        },\n        {\n            \"href\": \"/orders/8/pay\",\n            \"description\": \"Pays for the order\",\n            \"type\": \"GET\"\n        }\n    ]\n}\nIn the Richardson maturity model, level 3 represents the last step toward what he calls\nthe “Glory of REST.”\n What does the Richardson maturity model mean for the design of our APIs? The\nmodel gives us a framework to think about where our API designs stand within the\noverall principles of REST. This model isn’t meant to measure the degree to which an\nAPI “complies” with the principles of REST, or to otherwise assess the quality of an API\ndesign; instead, it gives us a framework to think about how well we leverage the HTTP\nprotocol to create expressive APIs that are easy to understand and to consume.\n Now that we understand the main design principles of REST APIs, it’s time to start\ndesigning the orders API! In the next section, we’ll begin by designing the API end-\npoints by learning to use HTTP methods.\n4.5\nStructured resource URLs with HTTP methods\nAs we learned in section 4.4, using HTTP methods and status codes is associated with\na mature API design in the Richardson maturity model. In this section, we learn to use\nHTTP methods correctly by applying them to the design of the CoffeeMesh applica-\ntion’s orders API.\n HTTP methods are special keywords used in HTTP requests to indicate the type of\naction we want to perform in the server. Proper use of HTTP methods makes our APIs\nmore structured and elegant, and since they’re part of the HTTP protocol, they also\nmake the API more understandable and easier to use.",
      "content_length": 2022,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "74\nCHAPTER 4\nPrinciples of REST API design\nDEFINITION\nHTTP request methods are keywords used in HTTP requests to indi-\ncate the type of action we wish to perform. For example, the GET method\nretrieves the details of a resource, while the POST method creates a new\nresource. The most important HTTP methods for REST APIs are GET, POST,\nPUT, PATCH, and DELETE. HTTP methods are also known as verbs.\nIn my experience, there’s often confusion around the proper use of HTTP methods.\nLet’s clear up that confusion by learning the semantics of each method. The most rel-\nevant HTTP methods in REST APIs are GET, POST, PUT, PATCH, and DELETE:\nGET—Returns information about the requested resource\nPOST—Creates a new resource\nPUT—Performs a full update by replacing a resource\nPATCH—Updates specific properties of a resource\nDELETE—Deletes a resource\nHTTP methods allow us to model the basic operations we can perform on a resource:\ncreate (POST), read (GET), update (PUT and PATCH), and delete (DELETE). We\nrefer to these operations with the acronym CRUD, which comes from the field of data-\nbases,6 but is very popular in the world of APIs. You’ll often hear about CRUD APIs,\nwhich are APIs designed to perform these operations on resources.\nSemantics of the PUT method\nAccording to the HTTP specification, PUT can be idempotent, and therefore we can\nuse it to create a resource if it doesn’t exist. However, the specification also high-\nlights “[a] service that selects a proper URI on behalf of the client, after receiving a\nstate-changing request, SHOULD be implemented using the POST method rather than\nPUT.” This means that, when the server is in charge of generating the URI of a new\nresource, we should create resources using the POST method, and PUT can only be\nused for updates.\nSee R. Fielding, “Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content”\n(RFC 7231, June 2014, https://tools.ietf.org/html/rfc7231#section-4.3.4).\n6 The CRUD acronym was reportedly introduced by James Martin in his influential book Managing the Data-Base\nEnvironment (Prentice-Hall, 1983, p. 381).\nPUT vs. PATCH: What’s the difference, and when do you use them?\nWe can use both PUT and PATCH to perform updates. So, what’s the difference\nbetween the two? While PUT requires the API client to send a whole new representa-\ntion of the resource (hence the replacement semantics), PATCH allows you to send\nonly those properties that changed.",
      "content_length": 2430,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "75\n4.5\nStructured resource URLs with HTTP methods\nHow do we use HTTP methods to define the endpoints of CoffeeMesh’s orders API?\nWe use HTTP methods in combination with URLs, so let’s first define the resource\nURLs. In section 4.1, we learned to distinguish between two types of resource URLs in\nFor example, imagine that an order has the following representation:\n{\n    \"id\": \"96247264-7d42-4a95-b073-44cedf5fc07d\",\n    \"status\": \"progress\",\n    \"created\": \"2023-09-01\",\n    \"order\": [\n        {\n            \"product\": \"cappuccino\",\n            \"size\": \"small\",\n            \"quantity\": 1\n        },\n        {\n            \"product\": \"croissant\",\n            \"size\": \"medium\",\n            \"quantity\": 2\n        }\n    ]\n}\nNow suppose that the user wants to make a small amendment in this order and\nupdate the size of the croissants from \"medium\" to \"small\". Although the user\nwants to change one specific field, with PUT they must send the whole payload back\nto the server. However, with PATCH they only need to send the fields that must be\nupdated in the server. PATCH requests are more optimal, since the payloads sent to\nthe server are smaller. However, as you can see in the following example, PATCH\nrequests also have a more complex structure, and sometimes they’re more difficult\nto process in the backend:\n{\n    \"op\": \"replace\",\n    \"path\": \"order/1/size\",\n    \"value\": \"medium\"\n}\nThis follows the guidelines of the JSON Patch specification:a a JSON Patch request\nmust specify the type of operation we want to perform, plus the target attribute and\nits desired value. We use JSON Patch to declare the target attribute.\nWhile implementing PATCH endpoints is good practice for public-facing APIs, internal\nAPIs often only implement PUT endpoints for updates since they’re easier to handle.\nIn the orders API, we’ll implement updates as PUT requests.\na P. Bryan and M. Nottingham, “JavaScript Object Notation (JSON) Patch” (https://www.rfc-editor \n.org/rfc/rfc6902).",
      "content_length": 1968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "76\nCHAPTER 4\nPrinciples of REST API design\nREST: singletons, which represent a single resource, and collections, which represent\na list of resources. In the orders API, we have these two resource URLs:\n\n/orders—Represents a list of orders.\n\n/orders/{orders_id}—Represents a single order. The curly braces around\n{order_id} indicates that this is a URL path parameter and must be replaced\nby the ID of an order.\nAs you can see in figure 4.7, we use the singleton URL /orders/{order_id} to per-\nform actions on an order, such as updating it, and the collections URL /orders to\nplace and to list past orders. HTTP methods help us model these operations:\nPOST /orders to place orders since we use POST to create new resources.\nGET /orders to retrieve a list of orders since we use GET to obtain information.\nGET /orders/{order_id} to retrieve the details of a particular order.\nPUT /orders/{order_id} to update an order since we use PUT to update a\nresource.\nDELETE /orders/{order_id} to delete an order since we use DELETE for\ndeletes.\nPOST /orders/{order_id}/cancel to cancel an order. We use POST to create\na cancellation.\nPOST /orders/{order_id}/pay to pay for an order. We use POST to create a\npayment.\nNow that we know how to design API endpoints by combining URL paths with HTTP\nmethods, let’s see how to leverage the semantics of HTTP status codes to return\nexpressive responses.\n/orders/{order_id}\n/cancel\n/orders/{order_id}\n/pay\nGET\nReturns an order\nPUT\nUpdates an order\nDELETE\nDeletes an order\nPOST\nCancels an order\nPOST\nPays an order\n/orders/{order_id}\n/orders\nGET\nReturns a list of orders\nPOST\nPlaces an order\nFigure 4.7\nWe combine HTTP methods with URL paths to design our API endpoints. We \nleverage the semantics of HTTP methods to convey the intention of each endpoint. For \nexample, we use the POST method to create new resources, so we use it in the POST \n/orders endpoint to place orders.",
      "content_length": 1913,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "77\n4.6\nUsing HTTP status codes to create expressive HTTP responses\n4.6\nUsing HTTP status codes to create expressive HTTP \nresponses\nThis section explains how we use HTTP status codes in the responses of a REST API.\nWe begin by clarifying what HTTP status codes are and how we classify them into\ngroups, and then we explain how to use them to model our API responses.\n4.6.1\nWhat are HTTP status codes?\nWe use status codes to signal the result of processing a request in the server. When\nproperly used, HTTP status codes help us deliver expressive responses to our APIs’\nconsumers. Status codes fall into the following five groups:\n1xx group—Signals that an operation is in progress\n2xx group—Signals that a request was successfully processed\n3xx group—Signals that a resource has been moved to a new location\n4xx group—Signals that something was wrong with the request\n5xx group—Signals that there was an error while processing the request\nNOTE\nHTTP response status codes are used to indicate the outcome of process-\ning an HTTP request. For example, the 200 status code indicates that the request\nwas successfully processed, while the 500 status code indicates that an internal\nserver error was raised while processing the request. HTTP status codes are asso-\nciated with a reasoned phrase that explains the intent of the code. For example,\nthe reasoned phrase for the 404 status code is “Not Found.” You can check out\nthe full list of status codes and learn more about them at http://mng.bz/z5lw.\nThe full list of HTTP status codes is long, and enumerating them one by one wouldn’t\ndo much to help us understand how we use them. Instead, let’s look at the most com-\nmonly used codes and see how we apply them in our API designs.\n When thinking about HTTP status codes, it’s useful to distinguish between success-\nful and unsuccessful responses. A successful response means the request was success-\nfully processed, while an unsuccessful response means that something went wrong\nwhile processing the request. For each of the endpoints that we defined in section 4.5,\nwe use the following successful HTTP status codes:\nPOST /orders: 201 (Created)—Signals that a resource has been created.\nGET /orders: 200 (OK)—Signals that the request was successfully processed.\nGET /orders/{order_id}: 200 (OK)—Signals that the request was successfully\nprocessed.\nPUT /orders/{order_id}: 200 (OK)—Signals that the resource was successfully\nupdated.\nDELETE /orders/{order_id}: 204 (No Content)—Signals that the request was\nsuccessfully processed but no content is delivered in the response. Contrary to\nall other methods, a DELETE request doesn’t require a response with payload,\nsince, after all, we are instructing the server to delete the resource. Therefore, a\n204 (No Content) code is a good choice for this type of HTTP request.",
      "content_length": 2828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "78\nCHAPTER 4\nPrinciples of REST API design\nPOST /orders/{order_id}/cancel: 200 (OK)—Although this is a POST end-\npoint, we use the 200 (OK) status code since we’re not really creating a resource,\nand all the client wants to know is that the cancellation was successfully processed.\nPOST /orders/{order_id}/pay: 200 (OK)—Although this is a POST endpoint,\nwe use the 200 (OK) status code since we’re not really creating a resource, and all\nthe client wants to know is that the payment was successfully processed.\nThat’s all good for successful responses, but what about error responses? What kinds\nof errors can we encounter in the server while processing requests, and what kinds of\nHTTP status codes are appropriate for them? We distinguish two groups of errors:\nErrors made by the user when sending the request, for example, due to a mal-\nformed payload, or due to the request being sent to a nonexistent endpoint. We\naddress this type of error with an HTTP status code in the 4xx group.\nErrors unexpectedly raised in the server while processing the request, typically\ndue to a bug in our code. We address this type of error with an HTTP status\ncode in the 5xx group.\nLet’s talk about each of these error types in more detail.\n4.6.2\nUsing HTTP status codes to report client errors in the request\nAn API client can make different types of errors when sending a request to an API.\nThe most common type of error in this category is sending a malformed payload to\nthe server. We distinguish two types of malformed payloads: payloads with invalid syn-\ntax and unprocessable entities.\n Payloads with invalid syntax are payloads that the server can neither parse nor under-\nstand. A typical example of a payload with invalid syntax is malformed JSON. As you can\nsee in figure 4.8, we address this type of error with a 400 (Bad Request) status code.\n/orders\nPOST\nPOST\nSent\nExpected\nMissing\nclosing\nbracket\nStatus code: 400 (Bad Request)\n{\n\"order\": [\n{\n\"product\": \"mocha\",\n\"size\": \"medium\",\n“quantity\": 2\n]\n}\n{\n\"order\": [\n{\n\"product\": \"mocha\",\n\"size\": \"medium\",\n\"quantity\": 2\n}\n]\n}\nFigure 4.8\nWhen a \nclient sends a malformed \npayload, we respond with \na 400 (Bad Request) \nstatus code.",
      "content_length": 2182,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "79\n4.6\nUsing HTTP status codes to create expressive HTTP responses\nUnprocessable entities are syntactically valid payloads that miss a required parameter,\ncontain invalid parameters, or assign the wrong value or type to a parameter. For\nexample, let’s say that, to place an order, our API expects a POST request on the\n/orders URL path with a payload like this:\n{\n    \"order\": [\n        {\n            \"product\": \"mocha\",\n            \"size\": \"medium\",\n            \"quantity\": 2\n        }\n    ]\n}\nThat is, we expect the user to send us a list of elements, where each element rep-\nresents an item of the order. Each item is described by the following properties:\n\nproduct—Identifies the product the user is ordering\n\nsize—Identifies the size that applies to the ordered product\n\nquantity—Tells us how many items of the same product and size the user\nwishes to order\nAs you can see in figure 4.9, an API client can send a payload missing one of the\nrequired properties, such as product. We address this type of error with the 422\n(Unprocessable Entity) status code, which signals that something was wrong with the\nrequest and it couldn’t be processed.\n/orders\nPOST\nPOST\nSent\nExpected\nStatus code: 422 (Unprocessable Entity)\nMissing\nrequired\nproperty\n{\n\"order\": [\n{\n\"size\": \"medium\",\n\"quantity\": 2\n}\n]\n}\n{\n\"order\": [\n{\n\"product\": \"mocha\",\n\"size\": \"medium\",\n\"quantity\": 2\n}\n]\n}\nFigure 4.9\nWhen an API client sends a malformed payload, the server responds \nback with a 400 (Bad Request) status code.",
      "content_length": 1495,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "80\nCHAPTER 4\nPrinciples of REST API design\nAnother common error happens when an API client requests a resource that doesn’t\nexist. For example, we know that the GET /orders/{order_id} endpoint serves the\ndetails of an order. If a client uses that endpoint with a nonexistent order ID, we\nshould respond with an HTTP status code signaling that the order doesn’t exist. As\nyou can see in figure 4.10, we address this error with the 404 (Not Found) status code,\nwhich signals that the requested resource is not available or couldn’t be found.\nAnother common error happens when API clients send a request using an HTTP\nmethod that is not supported. For example, if a user sent a PUT request on the\n/orders endpoint, we must tell them that the PUT method is not supported on that\nURL path. There are two HTTP status codes we can use to address this situation. As\nyou can see in figure 4.11, we can return a 501 (Not Implemented) if the method\nhasn’t been implemented but will be available in the future (i.e., we have a plan to\nimplement it).\nIf the requested HTTP method is not available and we don’t have a plan to imple-\nment it, we respond with the 405 (Method Not Allowed) status code, as illustrated in\nfigure 4.12.\n Two common errors in API requests have to do with authentication and authoriza-\ntion. The first happens when a client sends an unauthenticated request to a protected\nendpoint. In that case, we must tell them that they should first authenticate. As you\nGET /orders/71a620e8-20d3-47e5-b077-35a3060c865e\nStatus code: 404 (Not Found)\nThe order doesn’t exist.\nFigure 4.10\nWhen an API client requests a resource that doesn’t exist, the server responds with \nstatus code 404 (Not Found).\nPOST /orders/71a620e8-20d3-47e5-b077-35a3060c865e/pay\nStatus code: 501 (Not Implemented)\nThe POST method on the /orders/{order_id}/pay URL path is not supported.\nFigure 4.11\nWhen an API client sends a request to a URL path with an HTTP method \nthat will be exposed in the future but hasn’t been implemented, the server responds \nwith a 501 (Not Implemented) status code.",
      "content_length": 2069,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "81\n4.6\nUsing HTTP status codes to create expressive HTTP responses\ncan see in figure 4.13, we address this situation with the 401 (Unauthorized) status\ncode, which signals that the user hasn’t been authenticated.\nThe second error happens when a user is correctly authenticated and tries to use an\nendpoint or a resource they are not authorized to access. An example is a user trying\nto access the details of an order that doesn’t belong to them. As you can see in fig-\nure 4.14, we address this scenario with the 403 (Forbidden) status code, which signals\nthat the user doesn’t have permissions to access the requested resource or to perform\nthe requested operation.\nNow that we know how to use HTTP status codes to report user errors, let’s turn our\nattention to status codes for server errors.\nGET /orders/71a620e8-20d3-47e5-b077-35a3060c865e/pay\nStatus code: 405 (Method Not Allowed)\nThe GET method on this URL path is not supported and will not be supported.\nFigure 4.12\nWhen an API client makes a request on a URL path with an HTTP method \nthat is not supported and will not be supported, the server responds with a 405 (Method \nNot Allowed) status code.\nGET /orders\nStatus code: 401 (Unauthorized)\nThe API client is making an unauthenticated request.\nUnauthenticated\nclient\nFigure 4.13\nWhen an API \nclient sends an unauthenticated \nrequest to an endpoint that \nrequires authentication, the \nserver responds with a 401 \n(Unauthorized) status code.\nDELETE /orders/71a620e8-20d3-47e5-b077-35a3060c865e\nStatus code: 403 (Forbidden)\nUser is not allowed to use the DELETE method.\nFigure 4.14\nWhen an authenticated user makes a request using an HTTP method \nthey’re not allowed to use, the server responds with a 403 (Forbidden) status code.",
      "content_length": 1740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "82\nCHAPTER 4\nPrinciples of REST API design\n4.6.3\nUsing HTTP status codes to report errors in the server\nThe second group of errors are those raised in the server due to a bug in our code or to\na limitation in our infrastructure. The most common type of error within this category is\nwhen our application crashes unexpectedly due to a bug. In those situations, we\nrespond with a 500 (Internal Server Error) status code, as you can see in figure 4.15.\nA related type of error happens when our application becomes unable to service\nrequests. We usually handle this situation with the help of a proxy server or an API\ngateway (see section 4.2.4). Our API can become unresponsive when the server is\noverloaded or down for maintenance, and we must let the user know about this by\nsending an informative status code. We distinguish two scenarios:\nAs you can see in figure 4.16, when the server is unable to take on new connec-\ntions, we must respond with a 503 (Service Unavailable) status code, which sig-\nnals that the server is overloaded or down for maintenance and therefore\ncannot service additional requests.\nWhen the server takes too long to respond to the request, we respond with a\n504 (Gateway Timeout) status code, as shown in figure 4.17.\nGET /orders\nStatus code: 500 (Internal Server Error)\nAn unexpected error happens while processing the\nrequest due to a bug in our code.\nFigure 4.15\nWhen the server \nraises an error due to a bug in \nour code, we respond with a \n500 (Internal Server Error) \nstatus code.\nGET /orders\nStatus code: 503 (Service Unavailable)\nThe API backend is overloaded and can’t serve additional requests.\nFigure 4.16\nWhen the API \nserver is overloaded and can’t \nserve additional requests, we \nrespond to the client with a \n503 (Service Unavailable) \nstatus code.\nGET /orders\nStatus code: 504 (Gateway Timeout)\nThe API server is slow responding the request.\nFigure 4.17\nWhen the API \nserver is very slow responding \nto the request, a proxy server \nresponds to the client with a \n504 (Gateway Timeout) \nstatus code.",
      "content_length": 2043,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "83\n4.7\nDesigning API payloads\nThis completes our overview of the HTTP status codes most commonly used in the\ndesign of web APIs. The correct use of status codes goes a long way toward deliver-\ning a good developer experience for your API clients, but there’s one more thing\nwe need to design well: API payloads. In the next section, we turn our attention to\nthis important topic.\n4.7\nDesigning API payloads\nThis section explains best practices for designing user-friendly HTTP request and\nresponse payloads. Payloads represent the data exchanged between a client and a\nserver through an HTTP request. We send payloads to the server when we want to cre-\nate or update a resource, and the server sends us payloads when we request data. The\nusability of an API is very much dependent on good payload design. Poorly designed\npayloads make APIs difficult to use and result in bad user experiences. It’s therefore\nimportant to spend some effort designing high-quality payloads, and in this section\nyou’ll learn some patterns and best practices to help you in that task.7\n4.7.1\nWhat are HTTP payloads, and when do we use them?\nAn HTTP request is a message an application client sends to a web server, and an\nHTTP response is the server’s reply to the request. An HTTP request includes a URL,\nan HTTP method, a set of headers, and, optionally, a body or payload. HTTP headers\ninclude metadata about the request’s contents, such as the encoding format. Similarly,\nan HTTP response includes a status code, a set of headers, and, optionally, a payload.\nWe can represent payloads with different data serialization methods, such as XML and\nJSON. In REST APIs, data is typically represented as a JSON document.\nDEFINITION\nAn HTTP message body or payload is a message that contains the\ndata exchanged in an HTTP request. Both HTTP requests and responses can\ncontain a message body. The message body is encoded in one of the media\ntypes supported by HTTP, such as XML or JSON. The Content-Type header\nof the HTTP request tells us the encoding type of the message. In REST APIs,\nthe message body is typically encoded as JSON.\nHTTP requests include a payload when we need to send data to the server. For exam-\nple, a POST request typically sends data to create a resource. The HTTP specification\nallows us to include payloads in all HTTP methods, but it discourages their use in GET\n(http://mng.bz/O69K) and DELETE (http://mng.bz/YKeo) requests.\n The wording of the HTTP specification is intentionally vague on whether DELETE\nand GET requests can include a payload. It doesn’t forbid the use of payloads, but it\nstates that they don’t have any defined semantics. This allows some APIs to include\n7 In addition to learning best practices, you’ll find it useful to read about anti-patterns. My article, “How Bad\nModels Ruin an API (or Why Design-First is the Way to Go),” contains an overview of common anti-patterns\nyou should avoid (https://www.microapis.io/blog/how-bad-models-ruin-an-api).",
      "content_length": 2973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "84\nCHAPTER 4\nPrinciples of REST API design\npayloads in GET requests. A famous example is Elasticsearch, which allows clients to\nsend query documents in the body of a GET request (http://mng.bz/G14M).\n What about HTTP responses? Responses may contain a payload depending on the\nstatus code. According to the HTTP specification, responses with a 1xx status code, as\nwell as the 204 (No Content) and 304 (Not Modified) status codes, must not include a\npayload. All other responses do. In the context of REST APIs, the most important pay-\nloads are those in the 4xx and 5xx error responses, as well as 2xx success responses\nwith the exception of the 204 status code. In the next section, you’ll learn to design\nhigh-quality payloads for all those responses.\n4.7.2\nHTTP payload design patterns\nNow that we know when we use payloads, let’s learn best practices for designing them.\nWe’ll focus on the design of response payloads, since they present more variety. As we\nlearned in section 4.6.1, we distinguish between error and success responses. Error\nresponses’ payloads should include an \"error\" keyword detailing why the client is get-\nting an error. For example, a 404 response, which is generated when the requested\nresource cannot be found in the server, can include the following error message:\n{\n    \"error\": \"Resource not found\"\n} \n\"error\" is a commonly used keyword for error messages, but you can also use other\nkeywords such as \"detail\" and \"message\". Most web development frameworks han-\ndle HTTP errors and have default templates for error responses. For example,\nFastAPI uses \"detail\", so we’ll use that keyword in the orders API specification.\n Among success responses, we distinguish three scenarios: when we create a\nresource, when we update a resource, and when we get the details of a resource. Let’s\nsee how we design responses for each of these scenarios.\nRESPONSE PAYLOADS FOR POST REQUESTS\nWe use POST requests to create resources. In CoffeeMesh’s orders API, we place\norders through the POST /orders endpoint. To place an order, we send the list of\nitems we want to buy to the server, which takes responsibility for assigning a unique ID\nto the order, and therefore the order’s ID must be returned in the response payload.\nThe server also sets the time when the order was taken and its initial status. We call the\nproperties set by the server server-side or read-only properties, and we must include them\nin the response payload. As you can see in figure 4.18, it’s good practice to return a\nfull representation of the resource in the response to a POST request. This payload\nserves to validate that the resource was correctly created.",
      "content_length": 2654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "85\n4.7\nDesigning API payloads\nRESPONSE PAYLOADS FOR PUT AND PATCH REQUESTS\nTo update a resource, we use a PUT or a PATCH request. As we saw in section 4.5, we\nmake PUT/PATCH requests on a singleton resource URI, such as the PUT /orders/\n{order_id} endpoint of CoffeeMesh’s orders API. As you can see in figure 4.19, in\nthis case it’s also good practice to return a full representation of the resource, which\nthe client can use to validate that the update was correctly processed.\n{\n\"id\": \"96247264-7d42-4a95-b073-44cedf5fc07d\",\n\"created\": \"2025-02-02\",\n\"status\": \"created\",\n\"order\": [...]\n}\nOrders service\nPOST /orders\n{\"order\": [...]}\nEnrich the payload with server-generated\nproperties, like the order ID and the status.\nPlace an order with a\nPOST request.\nProcess the order and persist it\nin the database.\nReturn a full representation of\nthe order to the client.\nAPI client\nFigure 4.18\nWhen an API client sends a POST request to create a new resource, the server \nresponds with a full representation of the resource just created with its ID and any other properties \nset by the server.\nPUT /orders/cf979f89-eﬀd-4996-ae3d-8d42c79e0831\nOrders service\n{\n\"id\": \"cf979f89-eﬀd-4996-ae3d-8d42c79e0831\",\n\"created\": \"2025-02-02\",\n\"status\": \"created\",\n\"order\": [...]\n}\nFigure 4.19\nWhen an API client sends a PUT request to update a resource, the server responds \nwith a full representation of the resource.",
      "content_length": 1399,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "86\nCHAPTER 4\nPrinciples of REST API design\nRESPONSE PAYLOADS FOR GET REQUESTS\nWe retrieve resources from the server using GET requests. As we established in section\n4.5, CoffeeMesh’s orders API exposes two GET endpoints: the GET /orders and the\nGET /orders/{orders_id} endpoints. Let’s see what options we have when designing\nthe response payloads of these endpoints.\n The GET /orders returns a list of orders. To design the contents of the list, we\nhave two strategies: include a full representation of each order or include a partial\nrepresentation of each order. As you can see in figure 4.20, the first strategy gives the\nAPI client all the information they need in one request. However, this strategy may\ncompromise the performance of the API when the items in the list are big, resulting\nin a large response payload.\nThe second strategy for the GET /orders endpoint’s payload is to include a partial\nrepresentation of each order, as you can see in figure 4.21. For example, it’s common\npractice to include only the ID of each item in the response of a GET request on a col-\nlection endpoint, such as GET /orders. In this situation, the client must call the GET\n/orders/{order_id} endpoint to get a full representation of each order.\n Which approach is better? It depends on the use case. It’s preferable to send a full\nrepresentation of each resource, especially in public-facing APIs. However, if you’re\nworking on an internal API and the full details of each item aren’t needed, you can\nshorten the payload by including only the properties the client needs. Smaller pay-\nloads are faster to process, which results in a better user experience. Finally, singleton\nendpoints, such as the GET /orders/{order_id}, must always return a full represen-\ntation of the resource.\n Now that we know how to design API payloads, let’s turn our attention to URL\nquery parameters.\nOrders service\n{\n\"orders\": [\n{\n\"id\": \"96247264-7d42-4a95-b073-44cedf5fc07d\",\n\"created\": \"2025-02-02\",\n\"status\": \"created\",\n\"order\": [...]\n},\n{...}\n]\n}\nGET /orders\nFigure 4.20\nWhen an API \nclient sends a request to the \nGET /orders endpoint, the \nserver responds with a list of \norders, where each order \nobject contains full details \nabout the order.",
      "content_length": 2223,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "87\n4.8\nDesigning URL query parameters\n4.8\nDesigning URL query parameters\nNow let’s talk about URL query parameters and how, why, and when you should use\nthem. Some endpoints, such as the GET /orders endpoint of the orders API, return a\nlist of resources. When an endpoint returns a list of resources, it’s best practice to\nallow users to filter and paginate the results. For example, when using the GET\n/orders endpoint, we may want to limit the results to only the five most recent orders\nor to list only cancelled orders. URL query parameters allow us to accomplish those\ngoals and should always be optional, and, when appropriate, the server may assign\ndefault values for them.\nGET /orders/cf979f89-eﬀd-4996-ae3d-8d42c79e0831\nOrders service\nGet a list of orders.\n{\n\"orders\": [\n{\n\"id\": \"cf979f89-eﬀd-4996-ae3d-8d42c79e0831,\n},\n{\n\"id\": \"f005db16-fa8c-4e29-82a6-6210dddda554\",\n},\n...\n]\n}\nGET /orders\nOrders service\nGet the details of an order.\n{\n\"orders\": [\n{\n\"id\": \"cf979f89-eﬀd-4996-ae3d-8d42c79e0831\",\n\"created\": \"2025-02-02\",\n\"status\": \"created\",\n\"order\": [...]\n},\n{...}\n]\n}\nFigure 4.21\nWhen the API client makes a GET request on the /orders URL path, the server \nresponds with a list of order IDs. The client uses those IDs to request the details of each order on the \nGET /orders/{order_id} endpoint.",
      "content_length": 1306,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "88\nCHAPTER 4\nPrinciples of REST API design\nDEFINITION\nURL query parameters are key-value parameters in the URL.\nQuery parameters come after a question mark (?), and they’re typically\nused to filter the results of an endpoint, such as the GET /orders endpoint\nof the orders API. We can combine multiple query parameters by separat-\ning them with ampersands (&).\nURL query parameters are key-value pairs that form part of a URL but are separated\nfrom the URL path by a question mark. For example, if we want to call the GET /orders\nendpoint and filter the results by cancelled orders, we may write something like this:\nGET /orders?cancelled=true\nWe can chain multiple query parameters within the same URL by separating them with\nampersands. Let’s add a query parameter named limit to the GET /orders endpoint\nto allow us to restrict the number of results. To filter the GET /orders endpoint by can-\ncelled orders and restrict the number of results to 5, we make the following API request:\nGET /orders?cancelled=true&Limit=5\nIt’s also common practice to allow API clients to paginate results. Pagination consists\nof slicing the results into different sets and serving one set at a time. We can use sev-\neral strategies to paginate results, but the most common approach is using a page and\na per_page combination of parameters. page represents a set of the data, while\nper_page tells us how many items we want to include in each set. The server uses\nper_page’s value to determine how many sets of the data we’ll get. We combine both\nparameters in an API request as in the following example:\nGET /orders?page=1&per_page=10\nThis concludes our journey through the best practices and design principles of REST\nAPIs. You’re now equipped with the resources you need to design highly expressive\nand structured REST APIs that are easy to understand and consume. In the next chap-\nter, you’ll learn to document your API designs using the OpenAPI standard.\nSummary\nRepresentational state transfer (REST) defines the design principles of well-\narchitected REST APIs:\n– Client-server architecture—Client and server code must be decoupled.\n– Statelessness—The server must not keep state between requests.\n– Cacheability—Cacheable requests must be cached.\n– Layered system—The architectural complexity of the backend must not be\nexposed to end users.\n– Code on demand (optional)—Client applications may be able to download exe-\ncutable code from the server.\n– Uniform interface—The API must provide a uniform and consistent interface.",
      "content_length": 2517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "89\nSummary\nHypermedia as the engine of application state (HATEOAS) is a paradigm that\nstates that REST APIs must include referential links in their responses. HATEOAS\nmakes APIs navigational and easier to use.\nGood REST API design leverages features of the HTTP protocol, such as HTTP\nmethods and status codes, to create well-structured and highly expressive APIs\nthat are easy to consume.\nThe most important HTTP methods for REST APIs are\n– GET for retrieving resources from the server\n– POST for creating new resources\n– PUT and PATCH for updating resources\n– DELETE for deleting resources\nWe exchange data with an API server using payloads. A payload goes in the\nbody of an HTTP request or response. Clients send request payloads using the\nPOST, PUT, and PATCH HTTP methods. Server responses always include a pay-\nload, except when the status code is 204, 304, or one from the 1xx group.\nURL query parameters are key-value pairs in the URL, and we use them for fil-\ntering, paginating, and sorting the results of a GET endpoint.",
      "content_length": 1036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "90\nDocumenting REST APIs\nwith OpenAPI\nIn this chapter, you’ll learn to document APIs using OpenAPI: the most popular\nstandard for describing RESTful APIs, with a rich ecosystem of tools for testing, val-\nidating, and visualizing APIs. Most programming languages have libraries that sup-\nport OpenAPI specifications, and in chapter 6 you’ll learn to use OpenAPI-\ncompatible libraries from the Python ecosystem.\n OpenAPI uses JSON Schema to describe an API’s structure and models, so we\nstart by providing an overview of how JSON Schema works. JSON Schema is a spec-\nification for defining the structure of a JSON document, including the types and\nformats of the values within the document.\nThis chapter covers\nUsing JSON Schema to create validation models \nfor JSON documents\nDescribing REST APIs with the OpenAPI \ndocumentation standard\nModeling the payloads for API requests and \nresponses\nCreating reusable schemas in OpenAPI \nspecifications",
      "content_length": 947,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "91\n5.1\nUsing JSON Schema to model data\n After learning about JSON Schema, we study how an OpenAPI document is struc-\ntured, what its properties are, and how we use it to provide informative API specifica-\ntions for our API consumers. API endpoints constitute the core of the specification, so\nwe pay particular attention to them. We break down the process of defining the end-\npoints and schemas for the payloads of the API’s requests and responses, step by step.\nFor the examples in this chapter, we work with the API of CoffeeMesh’s orders ser-\nvice. As we mentioned in chapter 1, CoffeeMesh is a fictional on-demand coffee-delivery\nplatform, and the orders service is the component that allows customers to place and\nmanage their orders. The full specification for the orders API is available under\nch05/oas.yaml in the GitHub repository for this book.\n5.1\nUsing JSON Schema to model data\nThis section introduces the specification standard for JSON Schema and explains how\nwe leverage it to produce API specifications. OpenAPI uses an extended subset of\nthe JSON Schema specification for defining the structure of JSON documents and the\ntypes and formats of its properties. It’s useful for documenting interfaces that use\nJSON to represent data and to validate that the data being exchanged is correct.\nThe JSON Schema specification is under active development, with the latest version\nbeing 2020-12.1\nDEFINITION\nJSON Schema is a specification standard for defining the structure\nof a JSON document and the types and formats of its properties. OpenAPI\nuses JSON Schema to describe the properties of an API.\nA JSON Schema specification usually defines an object with certain attributes or prop-\nerties. A JSON Schema object is represented by an associative array of key-value pairs.\nA JSON Schema specification usually looks like this:\n{\n    \"status\": {    \n        \"type\": \"string\"    \n    }\n}\nIn this example, we define the schema of an object with one attribute named status,\nwhose type is string.\n JSON Schema allows us to be very explicit with respect to the data types and formats\nthat both the server and the client should expect from a payload. This is fundamental\n1 A. Wright, H. Andrews, B. Hutton, “JSON Schema: A Media Type for Describing JSON Documents” (Decem-\nber 8, 2020); https://datatracker.ietf.org/doc/html/draft-bhutton-json-schema-00. You can follow the devel-\nopment of JSON Schema and contribute to its improvement by participating in its repository in GitHub:\nhttps://github.com/json-schema-org/json-schema-spec. Also see the website for the project: https://json-schema\n.org/.\nEach property in a JSON Schema specification \ncomes as a key whose values are the \ndescriptors of the property.\nThe minimum descriptor necessary for a \nproperty is the type. In this case, we \nspecify that the status property is a string.",
      "content_length": 2842,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "92\nCHAPTER 5\nDocumenting REST APIs with OpenAPI\nfor the integration between the API provider and the API consumer, since it lets us\nknow how to parse the payloads and how to cast them into the right data types in our\nruntime.\n JSON Schema supports the following basic data types:\n\nstring for character values\n\nnumber for integer and decimal values\n\nobject for associative arrays (i.e., dictionaries in Python)\n\narray for collections of other data types (i.e., lists in Python)\n\nboolean for true or false values\n\nnull for uninitialized data\nTo define an object using JSON Schema, we declare its type as object, and we list its\nproperties and their types. The following shows how we define an object named order,\nwhich is one of the core models of the orders API.\n{\n    \"order\": {\n        \"type\": \"object\",    \n        \"properties\": {    \n            \"product\": {\n                \"type\": \"string\"\n            },\n            \"size\": {\n                \"type\": \"string\"\n            },\n            \"quantity\": {\n                \"type\": \"integer\"\n            }\n        }\n    }\n}\nSince order is an object, the order attribute also has properties, defined under the\nproperties attribute. Each property has its own type. A JSON document that com-\nplies with the specification in listing 5.1 is the following:\n{\n    \"order\": {\n        \"product\": \"coffee\",\n        \"size\": \"big\",\n        \"quantity\": 1\n    }\n}\nAs you can see, each of the properties described in the specification is used in this\ndocument, and each of them has the expected type.\nListing 5.1\nDefining the schema of an object with JSON Schema\nWe can declare the \nschema as an object.\nWe describe the object’s \nproperties under the \nproperties keyword.",
      "content_length": 1710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "93\n5.1\nUsing JSON Schema to model data\n A property can also represent an array of items. In the following code, the order\nobject represents an array of objects. As you can see, we use the items keyword to\ndefine the elements within the array.\n{\n    \"order\": {\n        \"type\": \"array\",\n        \"items\": {    \n            \"type\": \"object\",\n            \"properties\": {\n                \"product\": {\n                    \"type\": \"string\"\n                },\n                \"size\": {\n                    \"type\": \"string\"\n                },\n                \"quantity\": {\n                    \"type\": \"integer\"\n                }\n            }\n        }\n    }\n}\nIn this case, the order property is an array. Array types require an additional property\nin their schema, which is the items property that defines the type of each of the ele-\nments contained in the array. In this case, each of the elements in the array is an\nobject that represents an item in the order.\n An object can have any number of nested objects. However, when too many objects\nare nested, indentation grows large and makes the specification difficult to read. To\navoid this problem, JSON Schema allows us to define each object separately and to use\nJSON pointers to reference them. A JSON pointer is a special syntax that allows us to\npoint to another object definition within the same specification.\n As you can see in the following code, we can extract the definition of each item\nwithin the order array as a model called OrderItemSchema and use a JSON pointer to\nreference OrderItemSchema using the special $ref keyword.\n{\n    \"OrderItemSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"product\": {\n                \"type\": \"string\"\n            },\n            \"size\": {\n                \"type\": \"string\"\n            },\nListing 5.2\nDefining an array of objects with JSON Schema\nListing 5.3\nUsing JSON pointers to reference other schemas\nWe define the elements \nwithin the array using \nthe items keyword.",
      "content_length": 1987,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "94\nCHAPTER 5\nDocumenting REST APIs with OpenAPI\n            \"quantity\": {\n                \"type\": \"integer\"\n            }\n        }\n    },\n    \"Order\": {\n        \"status\": {\n            \"type\": \"string\"\n        },\n        \"order\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"$ref\": '#/OrderItemSchema'    \n            }\n        }\n    }\n}\nJSON pointers use the special keyword $ref and JSONPath syntax to point to another\ndefinition within the schema. In JSONPath syntax, the root of the document is rep-\nresented by the hashtag symbol (#), and the relationship of nested properties is\nrepresented by forward slashes (/). For example, if we wanted to create a pointer\nto the size property of the OrderItemSchema model, we would use the following syntax:\n'#/OrderItemSchema/size'.\nDEFINITION\nA JSON pointer is a special syntax in JSON Schema that allows us\nto point to another definition within the same specification. We use the spe-\ncial keyword $ref to declare a JSON pointer. To build the path to another\nschema, we use JSONPath syntax. For example, to point to a schema called\nOrderItemSchema, defined at the top level of the document, we use the fol-\nlowing syntax: {\"$ref\": \"#/OrderItemSchema\"}.\nWe can refactor our specification using JSON pointers by extracting common schema\nobjects into reusable models, and we can reference them using JSON pointers. This\nhelps us avoid duplication and keep the specification clean and succinct.\n In addition to being able to specify the type of a property, JSON Schema also\nallows us to specify the format of the property. We can develop our own custom for-\nmats or use JSON Schema’s built-in formats. For example, for a property representing\na date, we can use the date format—a built-in format supported by JSON Schema that\nrepresents an ISO date (e.g., 2025-05-21). Here’s an example:\n{\n    \"created\": {\n        \"type\": \"string\",\n        \"format\": \"date\"\n    }\n}\nIn this section, we’ve worked with examples in JSON format. However, JSON Schema\ndocuments don’t need to be written in JSON. In fact, it’s more common to write them\nWe can specify the type \nof the array’s items using \na JSON pointer.",
      "content_length": 2170,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "95\n5.2\nAnatomy of an OpenAPI specification\nin YAML format, as it’s more readable and easier to understand. OpenAPI specifica-\ntions are also commonly served in YAML format, and for the remainder of this chap-\nter, we’ll use YAML to develop the specification of the orders API.\n5.2\nAnatomy of an OpenAPI specification\nIn this section, we introduce the OpenAPI standard, and we learn to structure an API\nspecification. OpenAPI’s latest version is 3.1; however, this version still has little sup-\nport in the current ecosystem, so we’ll document the API using OpenAPI 3.0. There’s\nnot much difference between the two versions, and nearly everything you learn about\nOpenAPI 3.0 applies to 3.1.2\n OpenAPI is a standard specification format for documenting RESTful APIs (fig-\nure 5.1). OpenAPI allows us to describe in detail every element of an API, including\nits endpoints, the format of its request and response payloads, its security schemes,\nand so on. OpenAPI was created in 2010 under the name of Swagger as an open\nsource specification format for describing RESTful web APIs. Over time, this frame-\nwork grew in popularity, and in 2015 the Linux Foundation and a consortium of\nmajor companies sponsored the creation of the OpenAPI initiative, a project aimed at\nimproving the protocols and standards for building RESTful APIs. Today, OpenAPI is\nby far the most popular specification format used to document RESTful APIs,3 and it\nbenefits from a rich ecosystem of tools for API visualization, testing, and validation.\n2 For a detailed analysis of the differences between OpenAPI 3.0 and 3.1, check out OpenAPI’s migration from\n3.0 to 3.1 guide: https://www.openapis.org/blog/2021/02/16/migrating-from-openapi-3-0-to-3-1-0. \n3 According to the 2022 “State of the API” report by Postman (https://www.postman.com/state-of-api/api-\ntechnologies/#api-technologies).\nopenapi: 3.0\ninfo\nservers\npaths\ncomponents\nschemas\nFigure 5.1\nAn OpenAPI specification contains five \nsections. For example, the paths section describes the \nAPI endpoints, while the components section contains \nreusable schemas referenced across the document.",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "96\nCHAPTER 5\nDocumenting REST APIs with OpenAPI\nAn OpenAPI specification contains everything that the consumer of the API needs to\nknow to be able to interact with the API. As you can see in figure 5.1, an OpenAPI is\nstructured around five sections:\n\nopenapi—Indicates the version of OpenAPI that we used to produce the\nspecification.\n\ninfo—Contains general information, such as the title and version of the API.\n\nservers—Contains a list of URLs where the API is available. You can list more\nthan one URL for different environments, such as the production and staging\nenvironments.\n\npaths—Describes the endpoints exposed by the API, including the expected pay-\nloads, the allowed parameters, and the format of the responses. This is the most\nimportant part of the specification, as it represents the API interface, and it’s the\nsection that consumers will be looking for to learn how to integrate with the API.\n\ncomponents—Defines reusable elements that are referenced across the specifi-\ncation, such as schemas, parameters, security schemes, request bodies, and\nresponses.4 A schema is a definition of the expected attributes and types in your\nrequest and response objects. OpenAPI schemas are defined using JSON Schema\nsyntax.\nNow that we know how to structure an OpenAPI specification, let’s move on to docu-\nmenting the endpoints of the orders API. \n5.3\nDocumenting the API endpoints\nIn this section, we declare the endpoints of the orders API. As we mentioned in sec-\ntion 5.2, the paths section of an OpenAPI specification describes the interface of your\nAPI. It lists the URL paths exposed by the API, with the HTTP methods they imple-\nment, the types of requests they expect, and the responses they return, including the\nstatus codes. Each path is an object whose attributes are the HTTP methods it sup-\nports. In this section, we’ll focus specifically on documenting the URL paths and the\nHTTP methods. In chapter 4, we established that the orders API contains the follow-\ning endpoints:\nPOST /orders—Places an order. It requires a payload with the details of the\norder.\nGET /orders—Returns a list of orders. It accepts URL query parameters, which\nallow us to filter the results.\nGET /orders/{order_id}—Returns the details of a specific order.\nPUT /orders/{order_id}—Updates the details of an order. Since this is a PUT\nendpoint, it requires a full representation of the order.\nDELETE /orders/{order_id}—Deletes an order.\n4 See https://swagger.io/docs/specification/components/ for a full list of reusable elements that can be\ndefined in the components section of the API specification.",
      "content_length": 2606,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "97\n5.4\nDocumenting URL query parameters\nPOST /orders/{order_id}/pay—Pays for an order.\nPOST /orders/{order_id}/cancel—Cancels the order.\nThe following shows the high-level definitions of the orders API endpoints. We\ndeclare the URLs and the HTTP implemented by each URL, and we add an operation\nID to each endpoint so that we can reference them in other sections of the document.\npaths:\n  /orders:    \n    get:    \n      operationId: getOrders\n    post:  # creates a new order \n      operationId: createOrder\n  /orders/{order_id}:\n    get:\n      operationId: getOrder\n    put: \n      operationId: updateOrder\n    delete: \n      operationId: deleteOrder\n  /orders/{order_id}/pay:\n    post:\n      operationId: payOrder\n  /orders/{order_id}/cancel:\n    post: \n      operationId: cancelOrder\nNow that we have the endpoints, we need to fill in the details. For the GET /orders\nendpoint, we need to describe the parameters that the endpoint accepts, and for\nthe POST and PUT endpoints, we need to describe the request payloads. We also\nneed to describe the responses for each endpoint. In the following sections, we’ll\nlearn to build specifications for different elements of the API, starting with the\nURL query parameters.\n5.4\nDocumenting URL query parameters\nAs we learned in chapter 4, URL query parameters allow us to filter and sort the\nresults of a GET endpoint. In this section, we learn to define URL query parameters\nusing OpenAPI. The GET /orders endpoint allows us to filter orders using the follow-\ning parameters:\n\ncancelled—Whether the order was cancelled. This value will be a Boolean.\n\nlimit—Specifies the maximum number of orders that should be returned to\nthe user. The value for this parameter will be a number.\nListing 5.4\nHigh-level definition of the orders API endpoints\nWe declare a URL path.\nAn HTTP method supported \nby the /orders URL path",
      "content_length": 1863,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "98\nCHAPTER 5\nDocumenting REST APIs with OpenAPI\nBoth cancelled and limit can be combined within the same request to filter the results:\nGET /orders?cancelled=true&limit=5\nThis request asks the server for a list of five orders that have been cancelled. Listing\n5.5 shows the specification for the GET /orders endpoint’s query parameters. The\ndefinition of a parameter requires a name, which is the value we use to refer to it in the\nactual URL. We also specify what type of parameter it is. OpenAPI 3.1 distinguishes\nfour types of parameters: path parameters, query parameters, header parameters, and\ncookie parameters. Header parameters are parameters that go in an HTTP header\nfield, while cookie parameters go into a cookie payload. Path parameters are part of\nthe URL path and are typically used to identify a resource. For example, in /orders/\n{order_id}, order_id is a path parameter that identifies a specific order. Query\nparameters are optional parameters that allow us to filter and sort the results of an\nendpoint. We define the parameter’s type using the schema keyword (Boolean in the\ncase of cancelled, and a number in the case of limit), and, when relevant, we specify\nthe format of the parameter as well.5\npaths: \n  /orders:\n    get:\n      parameters:    \n        - name: cancelled    \n          in: query    \n          required: false    \n          schema:    \n            type: boolean\n        - name: limit\n          in: query\n          required: false\n          schema:\n            type: integer\nNow that we know how to describe URL query parameters, in the next section we’ll\ntackle something more complex: documenting request payloads.\n5.5\nDocumenting request payloads\nIn chapter 4, we learned that a request represents the data sent by a client to the\nserver through a POST or a PUT request. In this section, we learn to document the\nrequest payloads of the orders API endpoints. Let’s start with the POST /orders\nmethod. In section 5.1, we established that the payload for the POST /orders end-\npoint looks like this:\nListing 5.5\nSpecification for the GET /orders endpoint’s query parameters\n5 To learn more about the date types and formats available in OpenAPI 3.1 see http://spec.openapis.org/\noas/v3.1.0#data-types.\nWe describe URL query parameters \nunder the parameters property.\nThe parameter’s \nname\nWe use the in descriptor to specify that \nthe parameter goes in the URL path.\nWe specify whether the \nparameter is required.\nWe specify the parameter’s \ntype under schema.",
      "content_length": 2500,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "99\n5.5\nDocumenting request payloads\n{\n    \"order\": [\n        {\n            \"product\": \"cappuccino\",\n            \"size\": \"big\",\n            \"quantity\": 1\n        }\n    ]\n}\nThis payload contains an attribute order, which represents an array of items. Each\nitem is defined by the following three attributes and constraints:\n\nproduct—The type of product the user is ordering.\n\nsize—The size of the product. It can be one of the three following choices:\nsmall, medium, and big.\n\nquantity—The amount of the product. It can be any integer number equal to\nor greater than 1.\nListing 5.6 shows how we define the schema for this payload. We define request pay-\nloads under the content property of the method’s requestBody property. We can\nspecify payloads in different formats. In this case, we allow data only in JSON format,\nwhich has a media type definition of application/json. The schema for our payload\nis an object with one property: order, whose type is array. The items in the array are\nobjects with three properties: the product property, with type string; the size prop-\nerty, with type string; and the quantity property, with type integer. In addition, we\ndefine an enumeration for the size property, which constrains the accepted values to\nsmall, medium, and big. Finally, we also provide a default value of 1 for the quantity\nproperty, since it’s the only nonrequired field in the payload. Whenever a user sends a\nrequest containing an item without the quantity property, we assume that they want\nto order only one unit of that item.\npaths:\n  /orders:\n    post:\n      operationId: createOrder\n      requestBody:    \n        required: true    \n        content:    \n          application/json:\n            schema:    \n              type: object\n              properties:\n                order:\n                  type: array\n                  items:\n                    type: object\n                    properties:\nListing 5.6\nSpecification for the POST /orders endpoint\nWe describe \nrequest payloads \nunder requestBody.\nWe specify\nwhether the\npayload is\nrequired.\nWe specify the \npayload’s content \ntype.\nWe define the\npayload’s\nschema.",
      "content_length": 2141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "100\nCHAPTER 5\nDocumenting REST APIs with OpenAPI\n                      product:\n                        type: string\n                      size:\n                        type: string\n                        enum:    \n                          - small\n                          - medium\n                          - big\n                      quantity:\n                        type: integer\n                        required: false\n                        default: 1    \n                    required:\n                      - product\n                      - size\nEmbedding payload schemas within the endpoints’ definitions, as in listing 5.6, can\nmake our specification more difficult to read and understand. In the next section, we\nlearn to refactor payload schemas for reusability and for readability.\n5.6\nRefactoring schema definitions to avoid repetition\nIn this section, we learn strategies for refactoring schemas to keep the API specification\nclean and readable. The definition of the POST /orders endpoint in listing 5.6 is long\nand contains several layers of indentation. As a result, it’s difficult to read, and that means\nin the future it’ll become difficult to extend and to maintain. We can do better by moving\nthe payload’s schema to a different section of the API specification: the components sec-\ntion. As we explained in section 5.2, the components section is used to declare schemas\nthat are referenced across the specification. Every schema is an object where the key is\nthe name of the schema, and the values are the properties that describe it. \npaths:\n  /orders:\n    post:\n      operationId: createOrder\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/CreateOrderSchema'    \ncomponents:    \n  schemas:\n    CreateOrderSchema:    \n      type: object\n      properties:\n        order:\n          type: array\n          items:\nListing 5.7\nSpecification for the POST /orders endpoint using a JSON pointer\nWe can constrain the\nproperty’s values using\nan enumeration.\nWe specify a \ndefault value.\nWe use a JSON pointer to\nreference a schema defined\nsomewhere else in the document.\nSchema \ndefinitions \ngo under \ncomponents.\nEvery schema is an \nobject, where the key is \nthe name and the values \nare the properties that \ndescribe it.",
      "content_length": 2340,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "101\n5.6\nRefactoring schema definitions to avoid repetition\n            type: object\n            properties:\n              product:\n                type: string\n              size:\n                type: string\n                enum:\n                  - small\n                  - medium\n                  - big\n              quantity:\n                type: integer\n                required: false\n                default: 1\n            required:\n              - product\n              - size\nMoving the schema for the POST /orders request payload under the components sec-\ntion of the API makes the document more readable. It allows us to keep the paths\nsection of the document clean and focused on the higher-level details of the endpoint.\nWe simply need to refer to the CreateOrderSchema schema using a JSON pointer:\n#/components/schemas/CreateOrderSchema\nThe specification is looking good, but it can get better. CreateOrderSchema is a tad\nlong, and it contains several layers of nested definitions. If CreateOrderSchema grows\nin complexity, over time it’ll become difficult to read and maintain. We can make it\nmore readable by refactoring the definition of the order item in the array in the fol-\nlowing code. This strategy allows us to reuse the schema for the order’s item in other\nparts of the API.\ncomponents:\n  schemas:\n      OrderItemSchema:    \n        type: object\n        properties:\n          product:\n            type: string\n          size:\n            type: string\n            enum:\n              - small\n              - medium\n              - big\n          quantity:\n            type: integer\n            default: 1\n      CreateOrderSchema:\n        type: object\nListing 5.8\nSchema definitions for OrderItemSchema and Order\nWe introduce the \nOrderItemSchema.",
      "content_length": 1772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "102\nCHAPTER 5\nDocumenting REST APIs with OpenAPI\n        properties:\n          order:\n            type: array\n            items:\n              $ref: '#/OrderItemSchema'    \nOur schemas are looking good! The CreateOrderSchema schema can be used to create\nan order or to update it, so we can reuse it in the PUT /orders/{order_id} endpoint,\nas you can see in listing 5.9. As we learned in chapter 4, the /orders/{order_id} URL\npath represents a singleton resource, and therefore the URL contains a path parame-\nter, which is the order’s ID. In OpenAPI, path parameters are represented between\ncurly braces. We specify that the order_id parameter is a string with a UUID format (a\nlong, random string often used as an ID).6 We define the URL path parameter directly\nunder the URL path to make sure it applies to all HTTP methods.\npaths:\n  /orders:\n    get:\n      ...\n  /orders/{order_id}:    \n    parameters:    \n      - in: path    \n        name: order_id    \n        required: true    \n        schema:\n          type: string\n          format: uuid    \n    put:    \n      operationId: updateOrder\n      requestBody:    \n        required: true\n        content:\n          application/json:\n            schema:\n           $ref: '#/components/schemas/CreateOrderSchema'\nNow that we understand how to define the schemas for our request payloads, let’s\nturn our attention to the responses.\n5.7\nDocumenting API responses\nIn this section, we learn to document API responses. We start by defining the payload\nfor the GET /orders/{order_id} endpoint. The response of the GET /orders/\n{order_id} endpoint looks like this:\n6 P. Leach, M. Mealling, and R. Salz, “A Universally Unique Identifier (UUID) URN Namespace,” RFC 4112\n(https://datatracker.ietf.org/doc/html/rfc4122).\nListing 5.9\nSpecification for the PUT /orders/{order_id} endpoint\nWe use a JSON \npointer to point to \nOrderItemSchema.\nWe declare\nthe order’s\nresource URL.\nWe define the URL \npath parameter.\nThe order_id parameter \nis part of the URL path.\nThe name of the parameter\nThe order_id \nparameter is required.\nWe specify the\nparameter’s\nformat (UUID).\nWe define the HTTP \nmethod PUT for the \ncurrent URL path.\nWe document\nthe request\nbody of the\nPUT endpoint.",
      "content_length": 2213,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "103\n5.7\nDocumenting API responses\n{\n    \"id\": \"924721eb-a1a1-4f13-b384-37e89c0e0875\",\n    \"status\": \"progress\",\n    \"created\": \"2022-05-01\",\n    \"order\": [\n        {\n            \"product\": \"cappuccino\",\n            \"size\": \"small\",\n            \"quantity\": 1\n        },\n        {\n            \"product\": \"croissant\",\n            \"size\": \"medium\",\n            \"quantity\": 2\n        }\n    ]\n}\nThis payload shows the products ordered by the user, when the order was placed, and\nthe status of the order. This payload is similar to the request payload we defined in sec-\ntion 5.6 for the POST and PUT endpoints, so we can reuse our previous schemas. \ncomponents:\n  schemas:\n    OrderItemSchema:\n      ...\n  GetOrderSchema:   \n    type: object\n    properties:\n      status:\n        type: string\n        enum:    \n          - created\n          - paid\n          - progress\n          - cancelled\n          - dispatched\n          - delivered\n      created:\n        type: string\n        format: date-time    \n      order:\n        type: array\n        items:\n          $ref: '#/components/schemas/OrderItemSchema'   \nIn listing 5.10, we use a JSON pointer to point to GetOrderSchema. An alternative way\nto reuse the existing schemas is to use inheritance. In OpenAPI, we can inherit and\nextend a schema using a strategy called model composition, which allows us to combine\nthe properties of different schemas in a single object definition. The special keyword\nListing 5.10\nDefinition of the GetOrderSchema schema\nWe define the \nGetOrderSchema \nschema.\nWe constrain the values of \nthe status property with \nan enumeration.\nA string with \ndate-time format\nWe reference the \nOrderItemSchema \nschema using a \nJSON pointer.",
      "content_length": 1703,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "104\nCHAPTER 5\nDocumenting REST APIs with OpenAPI\nallOf is used in these cases to indicate that the object requires all the properties in\nthe listed schemas.\nDEFINITION\nModel composition is a strategy in JSON Schema that allows us to\ncombine the properties of different schemas into a single object. It is useful\nwhen a schema contains properties that have already been defined elsewhere,\nand therefore allows us to avoid repetition.\nThe following code shows an alternative definition of GetOrderSchema using the\nallOf keyword. In this case, GetOrderSchema is the composition of two other schemas:\nCreateOrderSchema and an anonymous schema with two keys—status and created.\ncomponents:\n  schemas:\n    OrderItemSchema:\n      ...\n    GetOrderSchema:\n      allOf:    \n        - $ref: '#/components/schemas/CreateOrderSchema'    \n        - type: object    \n          properties:\n            status:\n              type: string\n              enum:\n                - created\n                - paid\n                - progress\n                - cancelled\n                - dispatched\n                - delivered\n            created:\n              type: string\n              format: date-time\nModel composition results in a cleaner and more succinct specification, but it only\nworks if the schemas are strictly compatible. If we decide to extend CreateOrderSchema\nwith new properties, then this schema may no longer be transferable to the GetOrder-\nSchema model. In that sense, it’s sometimes better to look for common elements among\ndifferent schemas and refactor their definitions into standalone schemas.\n Now that we have the schema for the GET /orders/{order_id} endpoint’s response\npayload, we can complete the endpoint’s specification. We define the endpoint’s\nresponses as objects in which the key is the response’s status code, such as 200. We also\ndescribe the response’s content type and its schema, GetOrderSchema.\npaths:\n  /orders:\nListing 5.11\nAlternative implementation of GetOrderSchema using the allOf keyword\nListing 5.12\nSpecification for the GET /orders/{order_id} endpoint\nWe use the allOf keyword \nto inherit properties from \nother schemas.\nWe use a JSON \npointer to reference \nanother schema.\nWe define a new object to \ninclude properties that are \nspecific to GetOrderSchema.",
      "content_length": 2288,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "105\n5.8\nCreating generic responses\n    get:\n        ...\n  /orders/{order_id}:\n    parameters: \n      - in: path\n        name: order_id\n        required: true\n        schema:\n          type: string\n          format: uuid\n    put:\n      ... \n    get:    \n      summary: Returns the details of a specific order    \n      operationId: getOrder\n      responses:    \n        '200':    \n          description: OK    \n          content:    \n            application/json:\n              schema:\n                $ref: '#/components/schemas/GetOrderSchema'    \nAs you can see, we define response schemas within the responses section of the end-\npoint. In this case, we only provide the specification for the 200 (OK) successful\nresponse, but we can also document other status codes, such as error responses. The next\nsection explains how we create generic responses we can reuse across our endpoints.\n5.8\nCreating generic responses\nIn this section, we learn to add error responses to our API endpoints. As we men-\ntioned in chapter 4, error responses are more generic, so we can use the components\nsection of the API specification to provide generic definitions of those responses, and\nthen reuse them in our endpoints.\n We define generic responses within the responses header of the API’s components\nsection. The following shows a generic definition for a 404 response named NotFound.\nAs with any other response, we also document the content’s payload, which in this\ncase is defined by the Error schema.\ncomponents:\n  responses:    \n    NotFound:    \n      description: The specified resource was not found.    \nListing 5.13\nGeneric 404 status code response definition\nWe define the GET \nendpoint of the \n/orders/{order_id} \nURL path.\nWe provide a \nsummary description \nof this endpoint.\nWe define\nthis endpoint’s\nresponses.\nEach response is\nan object where\nthe key is the\nstatus code.\nA brief description \nof the response\nWe describe the content \ntypes of the response.\nWe use a \nJSON pointer \nto reference \nGetOrderSchema.\nGeneric responses go \nunder responses in the \ncomponents section.\nWe name the \nresponse.\nWe describe \nthe response.",
      "content_length": 2129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "106\nCHAPTER 5\nDocumenting REST APIs with OpenAPI\n      content:    \n        application/json:\n          schema:\n            $ref: '#/components/schemas/Error'    \n            \n  schemas:\n    OrderItemSchema:\n      ...\n    Error:    \n      type: object\n      properties:\n        detail: \n          type: string\n      required:\n        - detail\nThis specification for the 404 response can be reused in the specification of all our\nendpoints under the /orders/{order_id} URL path, since all of those endpoints are\nspecifically designed to target a specific resource.\nNOTE\nYou may be wondering, if certain responses are common to all the end-\npoints of a URL path, why can’t we define the responses directly under the\nURL path and avoid repetition? The answer is this isn’t possible as of now. The\nresponses keyword is not allowed directly under a URL path, so we must docu-\nment all the responses for every endpoint individually. There’s a request in the\nOpenAPI GitHub repository to allow including common responses directly\nunder the URL path, but it hasn’t been implemented (http://mng.bz/097p).\nWe can use the generic 404 response from listing 5.13 under the GET /orders/\n{order_id} endpoint.\npaths:\n  ...\n  /orders/{order_id}:\n    parameters:\n      - in: path\n        name: order_id\n        required: true\n        schema:\n          type: string\n          \"format\": uuid\n    get:\n      summary: Returns the details of a specific order\n      operationId: getOrder\n      responses:\n        '200':\n          description: OK\n          content:\n            application/json:\nListing 5.14\nUsing the 404 response schema under GET /orders/{order_id} \nWe define\nthe response’s\ncontent.\nWe reference the \nError schema.\nWe define the \nschema for the \nError payload.",
      "content_length": 1755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "107\n5.9\nDefining the authentication scheme of the API\n              schema:\n                $ref: '#/components/schemas/GetOrderSchema'\n        '404':    \n          $ref: '#/components/responses/NotFound'    \nThe orders API specification in the GitHub repository for this book also contains a\ngeneric definition for 422 responses and an expanded definition of the Error compo-\nnent that accounts for the different error payloads we get from FastAPI.\n We’re nearly done. The only remaining endpoint is GET /orders, which returns a\nlist of orders. The endpoint’s payload reuses GetOrderSchema to define the items in\nthe orders array.\npaths:\n  /orders:\n    get:    \n      operationId: getOrders\n      responses:\n        '200':\n          description: A JSON array of orders\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  orders:\n                    type: array    \n                    items:\n                      $ref: '#/components/schemas/GetOrderSchema'   \n                required:\n                  - order\n    post:\n      ...\n  /orders/{order_id}:\n    parameters:\n      ...\nOur API’s endpoints are now fully documented! You can use many more elements\nwithin the definitions of your endpoints, such as tags and externalDocs. These attri-\nbutes are not strictly necessary, but they can help to provide more structure to your\nAPI or make it easier to group the endpoints. For example, you can use tags to create\ngroups of endpoints that logically belong together or share common features.\n Before we finish this chapter, there’s one more topic we need to address: docu-\nmenting the authentication scheme of our API. That’s the topic of the next section!\n5.9\nDefining the authentication scheme of the API\nIf our API is protected, the API specification must describe how users need to authen-\nticate and authorize their requests. This section explains how we document our API’s\nListing 5.15\nSpecification for the GET /orders endpoint\nWe define\na 404\nresponse.\nWe reference the \nNotFound response \nusing a JSON pointer.\nWe define the new \nGET method of the \n/orders URL path.\norders is \nan array.\nEach item in the array is\ndefined by GetOrderSchema.",
      "content_length": 2245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "108\nCHAPTER 5\nDocumenting REST APIs with OpenAPI\nsecurity schemes. The security definitions of the API go within the components section\nof the specification, under the securitySchemes header.\n With OpenAPI, we can describe different security schemes, such as HTTP-based\nauthentication, key-based authentication, Open Authorization 2 (OAuth2), and OpenID\nConnect.7 In chapter 11, we’ll implement authentication and authorization using the\nOpenID Connect and OAuth2 protocols, so let’s go ahead and add definitions for\nthese schemes. Listing 5.16 shows the changes we need to make to our API specifica-\ntion to document the security schemes.\n We describe three security schemes: one for OpenID Connect, another one for\nOAuth2, and another for bearer authorization. We’ll use OpenID Connect to authorize\nuser access through a frontend application, and for direct API integrations, we’ll offer\nOAuth’s client credentials flow. We’ll explain how each protocol and each authorization\nflow works in detail in chapter 11. For OpenID Connect, we must provide a configura-\ntion URL that describes how our backend authentication works under the\nopenIdConnectUrl property. For OAuth2, we must describe the authorization flows\navailable, together with a URL that clients must use to obtain their authorization tokens\nand the available scopes. The bearer authorization tells users that they must include a\nJSON Web Token (JWT) in the Authorization header to authorize their requests.\ncomponents:\n  responses:\n    ...\n \n  schemas:\n    ...\n  securitySchemes:    \n    openId:    \n      type: openIdConnect    \n      openIdConnectUrl: https://coffeemesh-dev.eu.auth0.com/.well-\n➥ known/openid-configuration    \n    oauth2:    \n      type: oauth2    \n      flows:    \n        clientCredentials:    \n          tokenUrl: https://coffeemesh-dev.eu.auth0.com/oauth/token    \n          scopes: {}    \n    bearerAuth:\n      type: http\n      scheme: bearer\n      bearerFormat: JWT    \n  ...\n7 For a complete reference of all the security schemas available in OpenAPI, see https://swagger.io/docs/\nspecification/authentication/.\nListing 5.16\nDocumenting an API’s security scheme\nThe security schemes under\nthe securitySchemes header\nof the API’s components\nsection\nWe provide a name for \nthe security scheme (it \ncan be any name).\nThe type of \nsecurity scheme\nThe URL that describes the \nOpenID Connect configuration \nin our backend\nThe name\nof another\nsecurity\nscheme\nThe type of\nthe security\nscheme\nThe authorization\nflows available\nunder this\nsecurity scheme\nA description \nof the client \ncredentials flow\nThe URL where\nusers can request\nauthorization tokens\nThe available scopes \nwhen requesting an \nauthorization token\nThe bearer token has \na JSON Web Token \n(JWT) format.",
      "content_length": 2750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "109\nSummary\nsecurity:\n  - oauth2:\n      - getOrders\n      - createOrder\n      - getOrder\n      - updateOrder\n      - deleteOrder\n      - payOrder\n      - cancelOrder\n  - bearerAuth:\n      - getOrders\n      - createOrder\n      - getOrder\n      - updateOrder\n      - deleteOrder\n      - payOrder\n      - cancelOrder\nThis concludes our journey through documenting REST APIs with OpenAPI. And\nwhat a ride! You’ve learned how to use JSON Schema; how OpenAPI works; how to\nstructure an API specification; how to break down the process of documenting your\nAPI into small, progressive steps; and how to produce a full API specification. The\nnext time you work on an API, you’ll be well positioned to document its design using\nthese standard technologies.\nSummary\nJSON Schema is a specification for defining the types and formats of the prop-\nerties of a JSON document. JSON Schema is useful for defining data validation\nmodels in a language-agnostic manner.\nOpenAPI is a standard documentation format for describing REST APIs and\nuses JSON Schema to describe the properties of the API. By using OpenAPI,\nyou’re able to leverage the whole ecosystem of tools and frameworks built around\nthe standard, which makes it easier to build API integrations.\nA JSON pointer allows you to reference a schema using the $ref keyword. Using\nJSON pointers, we can create reusable schema definitions that can be used in dif-\nferent parts of an API specification, keeping the API specification clean and easy\nto understand.\nAn OpenAPI specification contains the following sections:\n– openapi—Specifies the version of OpenAPI used to document the API\n– info—Contains information about the API, such as its title and version\n– servers—Documents the URLs under which the API is available\n– paths—Describes the endpoints exposed by the API, including the schemas\nfor the API requests and responses and any relevant URL path or query\nparameters\n– components—Describes reusable components of the API, such as payload\nschemas, generic responses, and authentication schemes",
      "content_length": 2043,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "110\nBuilding REST APIs\nwith Python\nIn previous chapters, you learned to design and document REST APIs. In this chap-\nter, you’ll learn to implement REST APIs by working on two examples from the\nCoffeeMesh platform, the on-demand coffee delivery application that we intro-\nduced in chapter 1. We’ll build the APIs for the orders service and for the kitchen\nservice. The orders service is the main gateway to CoffeeMesh for customers of the\nplatform. Through it they can place orders, pay for those orders, update them, and\nkeep track of them. The kitchen service takes care of scheduling orders for produc-\ntion in the CoffeeMesh factories and keeps track of their progress. We’ll learn best\npractices for implementing REST APIs as we work through these examples.\n In chapter 2, we implemented part of the orders API. In the first sections of this\nchapter, we pick up the orders API where we left it in chapter 2 and implement its\nThis chapter covers\nAdding URL query parameters to an endpoint \nusing FastAPI\nDisallowing the presence of unknown properties \nin a payload using pydantic and marshmallow\nImplementing a REST API using flask-smorest\nDefining validation schemas and URL query \nparameters using marshmallow",
      "content_length": 1219,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "111\n6.1\nOverview of the orders API\nremaining features using FastAPI, a highly performant API framework for Python and\na popular choice for building REST APIs. We’ll learn how to add URL query parame-\nters to our endpoints using FastAPI. As we saw in chapter 2, FastAPI uses pydantic for\ndata validation, and in this chapter we’ll use pydantic to forbid unknown fields in a\npayload. We’ll learn about the tolerant reader pattern and balance its benefits against\nthe risk of API integration failures due to errors such as typos.\n After completing the implementation of the orders API, we’ll implement the API\nfor the kitchen service. The kitchen service schedules orders for production in the\nfactory and keeps track of their progress. We’ll implement the kitchen API using flask-\nsmorest, a popular API framework built on top of Flask and marshmallow. We’ll learn\nto implement our APIs following Flask application patterns, and we’ll define valida-\ntion schemas using marshmallow.\n By the end of this chapter, you’ll know how to implement REST APIs using FastAPI\nand Flask, two of the most popular libraries in the Python ecosystem. You’ll see how\nthe principles for implementing REST APIs transcend the implementation details of\neach framework and can be applied regardless of the technology that you use. The\ncode for this chapter is available under folder ch06 in the repository provided with\nthis book. Folder ch06 contains two subfolders: one for the orders API (ch06/orders)\nand one for the kitchen API (ch06/kitchen). With that said, and without further ado,\nlet’s get cracking!\n6.1\nOverview of the orders API\nIn this section, we recap the minimal implementation of the orders API that we under-\ntook in chapter 2. You can find the full specification of the orders API under ch06/\norders/oas.yaml in the GitHub repository for this book. Before we jump directly into the\nimplementation, let’s briefly analyze the specification and see what’s left to implement.\n In chapter 2, we implemented the API endpoints of the orders API, and we created\npydantic schemas to validate request and response payloads. We intentionally skipped\nimplementing the business layer of the application, as that’s a complex task that we’ll\ntackle in chapter 7.\n As a reminder, the endpoints exposed by the orders API are the following:\n\n/orders—Allows us to retrieve lists (GET) of orders and to place orders (POST)\n\n/orders/{order_id}—Allows us to retrieve the details of a specific order (GET),\nto update an order (PUT), and to delete an order (DELETE)\n\n/orders/{order_id}/cancel—Allows us to cancel an order (POST)\n\n/orders/{order_id}/pay—Allows us to pay for an order (POST)\nPOST /orders and PUT /orders/{order_id} require request payloads that define\nthe properties of an order, and in chapter 2 we implemented schemas for those pay-\nloads. What’s missing from the implementation is the URL query parameters for the\nGET /orders endpoint. Also, the pydantic schemas we implemented in chapter 2 don’t\ninvalidate payloads with illegal properties in the payloads. As we’ll see in section 6.3,",
      "content_length": 3078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "112\nCHAPTER 6\nBuilding REST APIs with Python\nthis is fine in some situations, but it may lead to integration issues in other cases, and\nyou’ll learn to configure the schemas to invalidate payloads with illegal properties.\n If you want to follow along with the examples in this chapter, create a folder called\nch06 and copy into it the code from ch02 as ch06/orders. Remember to install the\ndependencies and activate the virtual environment:\n$ mkdir ch06\n$ cp -r ch02 ch06/orders\n$ cd ch06/orders\n$ pipenv install --dev && pipenv shell\nYou can start the web server by running the following command:\n$ uvicorn orders.app:app --reload\nFASTAPI + UVICORN REFRESHER\nWe implement the orders API using the FastAPI\nframework, a popular Python framework for building REST APIs. FastAPI is\nbuilt on top of Starlette, an asynchronous web server implementation. To exe-\ncute our FastAPI application, we use Uvicorn, another asynchronous server\nimplementation that efficiently handles incoming requests.\nThe --reload flag makes Uvicorn watch for changes on your files so that any time you\nmake an update, the application is reloaded. This saves you the time of having to\nrestart the server every time you make changes to the code. With this covered, let’s\ncomplete the implementation of the orders API!\n6.2\nURL query parameters for the orders API\nIn this section, we enhance the GET /orders endpoint of the orders API by adding\nURL query parameters. We also implement validation schemas for the parameters. In\nchapter 4, we learned that URL query parameters allow us to filter the results of a\nGET endpoint. In chapter 5, we established that the GET /orders endpoint accepts\nURL query parameters to filter orders by cancellation and also to limit the list of\norders returned by the endpoint. \n# file: orders/oas.yaml\npaths:\n  /orders:\n    get:\n      parameters:\n        - name: cancelled\n          in: query\n          required: false\n          schema:\n            type: boolean\n        - name: limit\n          in: query\n          required: false\nListing 6.1\nSpecification for the GET /orders URL query parameters",
      "content_length": 2098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "113\n6.2\nURL query parameters for the orders API\n          schema:\n            type: integer\nWe need to implement two URL query parameters: cancelled (Boolean) and limit\n(integer). Neither are required, so users must be able to call the GET /orders end-\npoint without specifying them. Let’s see how we do that.\n Implementing URL query parameters for an endpoint is easy with FastAPI. All we\nneed to do is include them in the endpoint’s function signature and use type hints to\nadd validation rules for them. Since the query parameters are optional, we’ll mark\nthem as such using the Optional type, and we’ll set their default values to None. \n# file: orders/orders/api/api.py\nimport uuid\nfrom datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n...\n@app.get('/orders', response_model=GetOrdersSchema)\ndef get_orders(cancelled: Optional[bool] = None, limit: Optional[int] = None):\n    ...\nNow that we have query parameters available in the GET /orders endpoint, how should\nwe handle them within the function? Since the query parameters are optional, we’ll first\ncheck whether they’ve been set. We can do that by checking whether their values are\nsomething other than None. Listing 6.3 shows how we can handle URL query parame-\nters within the function body of the GET /orders endpoint. Study figure 6.1 to under-\nstand the decision flow for filtering the list of orders based on the query parameters.\n# file: orders/orders/api/api.py\n@app.get('/orders', response_model=GetOrdersSchema)\ndef get_orders(cancelled: Optional[bool] = None, limit: Optional[int] = None): \n    if cancelled is None and limit is None:    \n        return {'orders': orders}\n    query_set = [order for order in orders]     \n    if cancelled is not None:     \n        if cancelled:\n            query_set = [\n                order\n                for order in query_set\n                if order['status'] == 'cancelled'\n            ]\nListing 6.2\nImplementation of URL query parameters for GET /orders\nListing 6.3\nImplementation of URL query parameters for GET /orders\nWe include URL query\nparameters in the\nfunction signature.\nIf the parameters \nhaven’t been set, we \nreturn immediately.\nIf any of the parameters \nhas been set, we filter \nlist into a query_set.\nWe check\nwhether\ncancelled\nis set.",
      "content_length": 2288,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "114\nCHAPTER 6\nBuilding REST APIs with Python\n        else:\n            query_set = [\n                order\n                for order in query_set\n                if order['status'] != 'cancelled'\n            ]\n    if limit is not None and len(query_set) > limit:     \n        return {'orders': query_set[:limit]}\n    return {'orders': query_set}\ncancelled\nlimit\nURL query parameters\ncancelled is None\ncancelled is not\nNone\ncancelled = True\ncancelled = False\nFilter list of orders\nand select non-\ncancelled orders\nFilter list of orders\nand select cancelled\norders\nlimit is None\nlimit is not None\nReturn list of orders\nwithout limit\nRestrict the number\nof orders by the\nspeciﬁed limit\nFigure 6.1\nDecision flow for filtering orders based on query parameters. If the \ncancelled parameter is set to True or False, we use it to filter the list of \norders. After this step, we check whether the limit parameter is set. If limit \nis set, we only return the corresponding number of orders from the list.\nIf limit is set and its \nvalue is lower than the \nlength of query_set, \nwe return a subset \nof query_set.",
      "content_length": 1100,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "115\n6.3\nValidating payloads with unknown fields\nNow that we know how to add URL query parameters to our endpoints, let’s see how\nwe enhance our validation schemas.\n6.3\nValidating payloads with unknown fields\nUntil now, our pydantic models have been tolerant with the request payloads. If an\nAPI client sends a payload with fields that haven’t been declared in our schemas, the\npayload will be accepted. As you’ll see in this section, this may be convenient in some\ncases but misleading or dangerous in other contexts. To avoid integration errors, in\nthis section, we learn how to configure pydantic to forbid the presence of unknown\nfields. Unknown fields are fields that haven’t been defined in a schema.\nPYDANTIC REFRESHER\nAs we saw in chapter 2, FastAPI uses pydantic to define\nvalidation models for our APIs. Pydantic is a popular data validation library\nfor Python with a modern interface that allows you to define data validation\nrules using type hints.\nIn chapter 2, we implemented the schema definitions of the orders API following the\ntolerant reader pattern (https://martinfowler.com/bliki/TolerantReader.html), which\nfollows Postel’s law that recommends to be conservative in what you do and be liberal\nin what you accept from others.1\n In the field of web APIs, this means that we must strictly validate the payloads we\nsend to the client, while allowing for unknown fields in the payloads we receive from\nAPI clients. JSON Schema follows this pattern by default, and unless explicitly\ndeclared, a JSON Schema object accepts any kind of property. To disallow undeclared\nproperties using JSON Schema, we set additionalProperties to false. If we use\nmodel composition, a better strategy is setting unevaluatedProperties to false,\nsince additionalProperties causes conflicts between different models.2 OpenAPI\n3.1 allows us to use both additionalProperties and unevaluatedProperties, but\nOpenAPI 3.0 only accepts additionalProperties. Since we’re documenting our APIs\nusing OpenAPI 3.0.3, we’ll ban undeclared properties using additionalProperties:\n# file: orders/oas.yaml\n \n    GetOrderSchema:\n      additionalProperties: false\n      type: object\n      required:\n        - order\n        - id\n        - created\n        - status\n1 Jon Postel, Ed., “Transmission Control Protocol,” RFC 761, p. 13, https:// tools.ietf.org/html/rfc761.\n2 To understand why additionalProperties doesn’t work when using model composition, see the excellent\ndiscussion about this topic in JSON Schema’s GitHub repository: https://github.com/json-schema-org/json-\nschema-spec/issues/556.",
      "content_length": 2572,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "116\nCHAPTER 6\nBuilding REST APIs with Python\n      properties:\n        id:\n          type: string\n          format: uuid\n      ...\nCheck out the orders API specification under ch06/orders/oas.yaml in the GitHub\nrepository for this book to see additional examples of additionalProperties.\n The tolerant reader pattern is useful when an API is not fully consolidated or is\nlikely to change frequently and when we want to be able to make changes to it without\nbreaking integrations with existing clients. However, in other cases, like we saw in\nchapter 2 (section 2.5), the tolerant reader pattern can introduce new bugs or lead to\nunexpected integration issues.\n For example, OrderItemSchema has three properties: product, size, and quantity.\nproduct and size are required properties, but quantity is optional, and if missing,\nthe server assigns to it the default value of 1. In some scenarios, this can lead to con-\nfusing situations. Imagine a client sends a payload with a typo in the representation of\nthe quantity property, for example with the following payload:\n{\n  \"order\": [\n    {\n      \"product\": \"capuccino\",\n      \"size\": \"small\",\n      \"quantit\": 5\n    }\n  ]\n}\nUsing the tolerant reader implementation, we ignore the field quantit from the pay-\nload, and we assume that the quantity property is missing and set its value to the\ndefault of 1. This situation can be confusing for the client, who intended to set a dif-\nferent value for quantity.\nTHE API CLIENT SHOULD’VE TESTED THEIR CODE!\nYou can argue that the client\nshould’ve tested their code and verified that it works properly before calling\nthe server. And you’re right. But in real life, code often goes untested, or is\nnot properly tested, and a little bit of extra validation in the server will help in\nthose situations. If we check the payload for the presence of illegal properties,\nthis error will be caught and reported to the client.\nHow can we accomplish this using pydantic? To disallow unknown attributes, we need\nto define a Config class within our models and set the extra property to forbid. \n# file: orders/orders/api/schemas.py\nfrom datetime import datetime\nfrom enum import Enum\nListing 6.4\nDisallowing additional properties in models",
      "content_length": 2218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "117\n6.3\nValidating payloads with unknown fields\nfrom typing import List, Optional\nfrom uuid import UUID\nfrom pydantic import BaseModel, Extra, conint, conlist, validator\n...\nclass OrderItemSchema(BaseModel):\n    product: str\n    size: Size\n    quantity: int = Optional[conint(ge=1, strict=True)] = 1\n    class Config:     \n        extra = Extra.forbid\nclass CreateOrderSchema(BaseModel):\n    order: List[OrderItemSchema]\n    class Config:\n        extra = Extra.forbid\nclass GetOrderSchema(CreateOrderSchema):\n    id: UUID\n    created: datetime\n    status: StatusEnum\nLet’s test this new functionality. Run the following command to start the server:\n$ uvicorn orders.app:app --reload\nAs we saw in chapter 2, FastAPI generates a Swagger UI from the code, which we can\nuse to test the endpoints. We’ll use this UI to test our new validation rules with the fol-\nlowing payload:\n{\n  \"order\": [\n    {\n      \"product\": \"string\",\n      \"size\": \"small\",\n      \"quantit\": 5\n    }\n  ]\n}\nDEFINITION\nA Swagger UI is a popular style for representing interactive visual-\nizations of REST APIs. They provide a user-friendly interface that helps us\nunderstand the API implementation. Another popular UI for REST interfaces\nis Redoc (https://github.com/Redocly/redoc). \nTo get to the Swagger UI, visit http://127.0.0.1:8000/docs and follow the steps in fig-\nure 6.2 to learn how to execute a test against the POST /orders endpoint.\nWe use Config to ban \nproperties that haven’t been \ndefined in the schema.",
      "content_length": 1488,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "118\nCHAPTER 6\nBuilding REST APIs with Python\nAfter running this test, you’ll see that now FastAPI invalidates this payload and returns a\nhelpful 422 response with the following message: “extra fields not permitted.”\n6.4\nOverriding FastAPI’s dynamically generated \nspecification\nSo far, we’ve relied on FastAPI’s dynamically generated API specification to test, visual-\nize, and document the orders API. The dynamically generated specification is great to\nunderstand how we’ve implemented the API. However, our code can contain imple-\nmentation errors, and those errors can translate to inaccurate documentation. Addi-\ntionally, API development frameworks have limitations when it comes to generating\nAPI documentation, and they typically lack support for certain features of OpenAPI.\nFor example, a common missing feature is documenting OpenAPI links, which we’ll\nadd to our API specification in chapter 12.\n To understand how the API is supposed to work, we need to look at our API design\ndocument, which lives under orders/oas.yaml, and therefore is the specification we\n1. Click the\nendpoint.\n4. Click the\nExecute button.\n3. Place your\npayload.\n2. Click to try\nout the endpoint.\nFigure 6.2\nTesting the API with the Swagger UI: to test an endpoint, click the endpoint itself, then click \nthe Try it Out button, then click the Execute button.",
      "content_length": 1343,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "119\n6.4\nOverriding FastAPI’s dynamically generated specification\nwant to show when we deploy the API. In this section, you’ll learn to override\nFastAPI’s dynamically generated API specification with our API design document.\n To load the API specification document, we need PyYAML, which you can install\nwith the following command:\n$ pipenv install pyyaml\nIn the orders/app.py file, we load the API specification, and we overwrite our applica-\ntion’s object openapi property.\n# file: orders/orders/app.py\nfrom pathlib import Path\nimport yaml\nfrom fastapi import FastAPI\napp = FastAPI(debug=True)\noas_doc = yaml.safe_load(\n    (Path(__file__).parent / '../oas.yaml').read_text()\n)      \napp.openapi = lambda: oas_doc   \nfrom orders.api import api\nTo be able to test the API using the Swagger UI, we need to add the localhost URL to\nthe API specification. Open the orders/oas.yaml file and add the localhost address\nto the servers section of the specification:\n# file: orders/oas.yaml\nservers:\n  - url: http:/ /localhost:8000\n    description: URL for local development and testing\n  - url: https:/ /coffeemesh.com\n    description: main production server\n  - url: https:/ /coffeemesh-staging.com\n    description: staging server for testing purposes only\nBy default, FastAPI serves the Swagger UI under the /docs URL, and the OpenAPI\nspecification under /openapi.json. That’s great when we only have one API, but Coffee-\nMesh has multiple microservice APIs; therefore, we need multiple paths to access each\nAPI’s documentation. We’ll serve the orders API’s Swagger UI under /docs/orders, and\nits OpenAPI specification under /openapi/orders.json. We can override those paths\ndirectly in FastAPI’s application object initializer:\nListing 6.5\nOverriding FastAPI’s dynamically generated API specification\nWe load the API \nspecification using \nPyYAML.\nWe override FastAPI’s openapi \nproperty so that it returns our \nAPI specification.",
      "content_length": 1924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "120\nCHAPTER 6\nBuilding REST APIs with Python\n# file: orders/app.py\napp = FastAPI(\n    debug=True, openapi_url='/openapi/orders.json', docs_url='/docs/orders'\n)\nThis concludes our journey through building the orders API with FastAPI. It’s now\ntime to move on to building the API for the kitchen service, for which we’ll use a new\nstack: Flask + marshmallow. Let’s get on with it!\n6.5\nOverview of the kitchen API\nIn this section, we analyze the implementation requirements for the kitchen API. As\nyou can see in figure 6.3, the kitchen service manages the production of customer\norders. Customers interface with the kitchen service through the orders service when\nthey place an order or check its status. CoffeeMesh staff can also use the kitchen ser-\nvice to check how many orders are scheduled and to manage them.\nThe specification for the kitchen API is provided under ch06/kitchen/oas.yaml in the\nrepository provided with this book. The kitchen API contains four URL paths (see fig-\nure 6.4 for additional clarification):\n\n/kitchen/schedules—Allows us to schedule an order for production in the\nkitchen (POST) and to retrieve a list of orders scheduled for production (GET)\n\n/kitchen/schedules/{schedule_id}—Allows us to retrieve the details of a\nscheduled order (GET), to update its details (PUT), and to delete it from our\nrecords (DELETE)\nKitchen\nsubdomain\nKitchen\nsubdomain\nCustomer\nOrders\nsubdomain\nSchedule order\nfor production\nCheck order status\nCustomer\nPlace an order\nCheck order status\nKitchen\nsubdomain\nKitchen\nsubdomain\nAdministrator\nGet list of scheduled orders\nManage scheduled orders\nFigure 6.3\nThe kitchen service schedules orders for production, and it tracks their progress. \nCoffeeMesh staff members use the kitchen service to manage scheduled orders.",
      "content_length": 1774,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "121\n6.5\nOverview of the kitchen API\n\n/kitchen/schedules/{schedule_id}/status—Allows us to read the status of\nan order scheduled for production\n\n/kitchen/schedules/{schedule_id}/cancel—Allows us to cancel a scheduled\norder\nThe kitchen API contains three schemas: OrderItemSchema, ScheduleOrderSchema,\nand GetScheduledOrderSchema. The ScheduleOrderSchema represents the payload\nrequired to schedule an order for production, while the GetScheduledOrderSchema\nrepresents the details of an order that has been scheduled. Just like in the orders API,\nOrderItemSchema represents the details of each item in an order.\n/kitchen/schedule\n/kitchen/schedule/{schedule_id}\n/kitchen/schedule\n/{schedule_id}/cancel\nGET\nReturns a list of\nscheduled orders\nPOST\nSchedules an order\nGET\nReturns the details of a\nscheduled order\nPUT\nUpdates a scheduled\norder\nDELETE\nDeletes a scheduled\norder\nPOST\nCancels a scheduled\norder\n/kitchen/schedule/{schedule_id}/status\nGET\nGets the status of\nan order\nFigure 6.4\nThe kitchen API has four URL paths: /kitchen/schedules exposes a GET and a POST \nendpoint; /kitchen/schedules/{schedule_id} exposes PUT, GET, and DELETE endpoints; \n/kitchen/schedules/{schedule_id}/cancel exposes a POST endpoint; and \n/kitchen/schedules/{schedule_id}/status exposes a GET endpoint.",
      "content_length": 1284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "122\nCHAPTER 6\nBuilding REST APIs with Python\n Just as we did in chapter 2, we’ll keep the implementation simple and focus only\non the API layer. We’ll mock the business layer with an in-memory representation of\nthe schedules managed by the service. In chapter 7, we’ll learn service implementa-\ntion patterns that will help us implement the business layer.\n6.6\nIntroducing flask-smorest\nThis section introduces the framework we’ll use to build the kitchen API: flask-smorest\n(https://github.com/marshmallow-code/flask-smorest). Flask-smorest is a REST API\nframework built on top of Flask and marshmallow. Flask is a popular framework for\nbuilding web applications, while marshmallow is a popular data validation library that\nhandles the conversion of complex data structures to and from native Python objects.\nFlask-smorest builds on top of both frameworks, which means we implement our API\nschemas using marshmallow, and we implement our API endpoints following the\npatterns of a typical Flask application, as illustrated in figure 6.5. As you’ll see, the\nﬂask-smorest\n(Flask\nlueprint)\nb\nmarshmallow\nFlask (class-based views) routing\nHTTP request\nHTTP request\nHTTP request\n/kitchen\n/kitchen/schedules/{schedule_id}\nData validation ﬂow\nAPI endpoints\n{ data }\n{ data }\nFigure 6.5\nArchitecture of an application built with flask-smorest. Flask-smorest \nimplements a typical Flask blueprint, which allows us to build and configure our \nAPI endpoints just as we would in a standard Flask application.",
      "content_length": 1496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "123\n6.7\nInitializing the web application for the API\nprinciples and patterns we used when we built the orders API with FastAPI can be\napplied regardless of the framework, and we’ll use the same approach to build the\nkitchen API with flask-smorest.\n Building APIs with flask-smorest offers an experience similar to building them with\nFastAPI, with only two major differences:\nFastAPI uses pydantic for data validation, while flask-smorest uses marshmallow. This\nmeans that with FastAPI we use native Python-type hints to create data valida-\ntion rules, while in marshmallow we use field classes.\nFlask allows us to implement API endpoints with class-based views. This means that we\ncan use a class to represent a URL path and implement its HTTP methods as\nmethods of the class. Class-based views help you write more structured code and\nencapsulate the specific behavior of each URL path within the class. In contrast,\nFastAPI allows you only to define endpoints using functions. Notice that Star-\nlette allows you to implement class-based routes, so this limitation of FastAPI\nmay go away in the future.\nWith this covered, let’s kick off the implementation of the kitchen API!\n6.7\nInitializing the web application for the API\nIn this section, we set up the environment to start working on the kitchen API. We’ll\nalso create the entry point for the application and add basic configuration for the web\nserver. In doing so, you’ll learn how to set up a project with flask-smorest and how to\ninject configuration objects into your Flask applications.\n Flask-smorest is built on top of the Flask framework, so we’ll lay out our web appli-\ncation following the patterns of a typical Flask application. Create a folder called\nch06/kitchen for the kitchen API implementation. Within that folder, copy the\nkitchen API specification, which is available under ch06/kitchen/oas.yaml in this\nbook’s GitHub repository. oas.yaml contains the API specification for the kitchen\nAPI. Use the cd command to navigate into the ch06/kitchen folder, and run the fol-\nlowing commands to install the dependencies that we’ll need to proceed with the\nimplementation:\n$ pipenv install flask-smorest\nNOTE\nIf you want to ensure that you’re installing the same version of the\ndependencies that I used when writing this chapter, copy the ch06/kitchen/\nPipfile and the ch06/kitchen/Pipfile.lock files from the GitHub repository\nonto your local machine, and run pipenv install.\nAlso, run the following command to activate the environment:\n$ pipenv shell\nNow that we have the libraries we need, let’s create a file called kitchen/app.py. This\nfile will contain an instance of the Flask application object, which represents our web",
      "content_length": 2696,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "124\nCHAPTER 6\nBuilding REST APIs with Python\nserver. We’ll also create an instance of flask-smorest’s Api object, which will represent\nour API. \n# file: kitchen/app.py\nfrom flask import Flask\nfrom flask_smorest import Api\napp = Flask(__name__)    \nkitchen_api = Api(app)    \nFlask-smorest requires some configuration parameters to work. For example, we need\nto specify the version of OpenAPI we are using, the title of our API, and the version of\nour API. We pass this configuration through the Flask application object. Flask offers\ndifferent strategies for injecting configuration, but the most convenient method is\nloading configuration from a class. Let’s create a file called kitchen/config.py for our\nconfiguration parameters. Within this file we create a BaseConfig class, which con-\ntains generic configuration for the API.\n# file: kitchen/config.py\nclass BaseConfig:\n    API_TITLE = 'Kitchen API'    \n    API_VERSION = 'v1'     \n    OPENAPI_VERSION = '3.0.3'       \n    OPENAPI_JSON_PATH = 'openapi/kitchen.json'    \n    OPENAPI_URL_PREFIX = '/'     \n    OPENAPI_REDOC_PATH = '/redoc'      \n    OPENAPI_REDOC_URL = \n'https:/ /cdn.jsdelivr.net/npm/redoc@next/bundles/redoc.standalone.js' \n    OPENAPI_SWAGGER_UI_PATH = '/docs/kitchen'    \n    OPENAPI_SWAGGER_UI_URL = 'https:/ /cdn.jsdelivr.net/npm/swagger-ui-\n➥ dist/'     \nNow that the configuration is ready, we can pass it to the Flask application object. \n# file: kitchen/app.py\nfrom flask import Flask\nfrom flask_smorest import Api\nfrom config import BaseConfig     \nListing 6.6\nInitialization of the Flask application object and the Api object\nListing 6.7\nConfiguration for the orders API\nListing 6.8\nLoading configuration \nWe create an instance \nof the Flask application \nobject.\nWe create an instance of \nflask-smorest’s Api object.\nThe title\nof our\nAPI\nThe version \nof our API\nThe version \nof OpenAPI \nwe are using\nPath to the dynamically \ngenerated specification \nin JSON\nURL path prefix for the \nOpenAPI specification file\nPath to the\nRedoc UI\nof our API\nPath to a script to\nbe used to render\nthe Redoc UI\nPath to the Swagger \nUI of our API\nPath to a script to be \nused to render the \nSwagger UI\nWe import the \nBaseConfig class \nwe defined earlier.",
      "content_length": 2218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "125\n6.8\nImplementing the API endpoints\napp = Flask(__name__)\napp.config.from_object(BaseConfig)     \nkitchen_api = Api(app)\nWith the entry point for our application ready and configured, let’s move on to imple-\nmenting the endpoints for the kitchen API!\n6.8\nImplementing the API endpoints\nThis section explains how we implement the endpoints of the kitchen API using flask-\nsmorest. Since flask-smorest is built on top of Flask, we build the endpoints for our\nAPI exactly as we’d do any other Flask application. In Flask, we register our endpoints\nusing Flask’s route decorator:\n@app.route('/orders')\ndef process_order():\n    pass\nUsing the route decorator works for simple cases, but for more complex application\npatterns, we use Flask blueprints. Flask blueprints allow you to provide specific config-\nuration for a group of URLs. To implement the kitchen API endpoints, we’ll use the\nflask-smorest’s Blueprint class. Flask-smorest’s Blueprint is a subclass of Flask’s Blue-\nprint, so it provides the functionality that comes with Flask blueprints, enhances it\nwith additional functionality and configuration that generates API documentation,\nand supplies payload validation models, among other things.\n We can use Blueprint’s route decorators to create an endpoint or URL path. As\nyou can see from figure 6.6, functions are convenient for URL paths that only expose\none HTTP method. When a URL exposes multiple HTTP methods, it’s more conve-\nnient to use class-based routes, which we implement using Flask’s MethodView class.\n As you can see in figure 6.7, using MethodView, we represent a URL path as a class,\nand we implement the HTTP methods it exposes as methods of the class.\n For example, if we have a URL path /kitchen that exposes GET and POST end-\npoints, we can implement the following class-based view: \nclass Kitchen(MethodView):\n    \n    def get(self):\n        pass\n    def post(self):\n        pass\nListing 6.9 illustrates how we implement the endpoints for the kitchen API using\nclass-based views and function-based views. The content in listing 6.9 goes into the\nkitchen/api/api.py file. First, we create an instance of flask-smorest’s Blueprint. The\nBlueprint object allows us to register our endpoints and add data validation to them.\nWe use the from_object \nmethod to load configuration \nfrom a class.",
      "content_length": 2322,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "126\nCHAPTER 6\nBuilding REST APIs with Python\nTo instantiate Blueprint, we must pass two required positional arguments: the name\nof the Blueprint itself and the name of the module where the Blueprint’s routes are\nimplemented. In this case, we pass the module’s name using the __name__ attribute,\nwhich resolves to the name of the file.\n/kitchen/schedule\nGET\nReturns a list of\nscheduled orders\nPOST\nSchedules an order\n@blueprint.route('/kitchen/schedule')\nclass KitchenSchedules(MethodView):\ndef get(self):\nreturn {\n'schedules': [schedule]\n}, 200\ndef post(self, payload):\nreturn schedules, 201\nFigure 6.6\nWhen a URL path exposes more than one HTTP method, it’s more \nconvenient to implement it as a class-based view, where the class methods \nimplement each of the HTTP methods exposed.\n/kitchen/schedule/{schedule_id}/cancel\nPOST\nCancels a scheduled order\n@blueprint.route('/kitchen/schedule/<schedule_id>/cancel')\ndef cancel_schedule(schedule_id):\nreturn schedules[0], 200\nFigure 6.7\nWhen a URL path exposes only one HTTP method, it’s more convenient \nto implement it as a function-based view.",
      "content_length": 1092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "127\n6.8\nImplementing the API endpoints\n Once the Blueprint is instantiated, we register our URL paths with it using the\nroute() decorator. We use class-based routes for the /kitchen/schedules and the\n/kitchen/schedules/{schedule_id} paths since they expose more than one HTTP\nmethod, and we use function-based routes for the /kitchen/schedules/{schedule_\nid}/cancel and /kitchen/schedules/{schedule_id}/status paths because they\nonly expose one HTTP method. We return a mock schedule object in each endpoint\nfor illustration purposes, and we’ll change that into a dynamic in-memory collection\nof schedules in section 6.12. The return value of each function is a tuple, where the\nfirst element is the payload and the second is the status code of the response. \n# file: kitchen/api/api.py\nimport uuid\nfrom datetime import datetime\nfrom flask.views import MethodView\nfrom flask_smorest import Blueprint\nblueprint = Blueprint('kitchen', __name__, description='Kitchen API')   \nschedules = [{     \n    'id': str(uuid.uuid4()),\n    'scheduled': datetime.now(),\n    'status': 'pending',\n    'order': [\n        {\n            'product': 'capuccino',\n            'quantity': 1,\n            'size': 'big'\n        }\n    ]\n}]\n@blueprint.route('/kitchen/schedules')     \nclass KitchenSchedules(MethodView):     \n    def get(self):   \n        return {\n            'schedules': schedules\n        }, 200    \n    def post(self, payload):\n        return schedules[0], 201\n@blueprint.route('/kitchen/schedules/<schedule_id>')     \nclass KitchenSchedule(MethodView):\nListing 6.9\nImplementation of the endpoints of the orders API\nWe create an instance\nof flask-smorest’s\nBlueprint class.\nWe declare a \nhardcoded list \nof schedules.\nWe use the Blueprint’s route() \ndecorator to register a class or \na function as a URL path.\nWe implement the \n/kitchen/schedules URL \npath as a class-based view.\nEvery method view in a class-\nbased view is named after the \nHTTP method it implements.\nWe return both the \npayload and the \nstatus code.\nWe define URL \nparameters within \nangle brackets.",
      "content_length": 2059,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "128\nCHAPTER 6\nBuilding REST APIs with Python\n    def get(self, schedule_id):     \n        return schedules[0], 200\n    def put(self, payload, schedule_id):\n        return schedules[0], 200\n    def delete(self, schedule_id):\n        return '', 204\n@blueprint.route(\n    '/kitchen/schedules/<schedule_id>/cancel', methods=['POST']\n)          \ndef cancel_schedule(schedule_id):\n    return schedules[0], 200\n@blueprint.route('/kitchen/schedules/<schedule_id>/status, methods=[GET])\ndef get_schedule_status(schedule_id):\n    return schedules[0], 200\nNow that we have created the blueprint, we can register it with our API object in the\nkitchen/app.py file. \n# file: kitchen/app.py\nfrom flask import Flask\nfrom flask_smorest import Api\nfrom api.api import blueprint    \nfrom config import BaseConfig\napp = Flask(__name__)\napp.config.from_object(BaseConfig)\nkitchen_api = Api(app)\nkitchen_api.register_blueprint(blueprint)    \nUsing the cd command, navigate to the ch06/kitchen directory and run the applica-\ntion with the following command:\n$ flask run --reload\nJust like in Uvicorn, the --reload flag runs the server with a watcher over your files so\nthat the server restarts when you make changes to the code.\n If you visit the http://127.0.0.1:5000/docs URL, you’ll see an interactive Swagger\nUI dynamically generated from the endpoints we implemented earlier. You can also\nsee the OpenAPI specification dynamically generated by flask-smorest under http://\n127.0.0.1:5000/openapi.json. At this stage in our implementation, it’s not possible to\nListing 6.10\nRegistering the blueprint with the API object\nWe include the URL \npath parameter in the \nfunction signature.\nWe implement the \n/kitchen/schedules/<schedule_id>/cancel \nURL path as a function-based view.\nWe import the \nblueprint we \ndefined earlier.\nWe register the \nblueprint with the \nkitchen API object.",
      "content_length": 1859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "129\n6.9\nImplementing payload validation models with marshmallow\ninteract with the endpoints through the Swagger UI. Since we don’t yet have marsh-\nmallow models, flask-smorest doesn’t know how to serialize data and therefore doesn’t\nreturn payloads. However, it’s still possible to call the API using cURL and inspect the\nresponses. If you run curl http:/ /127.0.0.1:5000/kitchen/schedules, you’ll get\nthe mock object we defined in the kitchen/api/api.py module.\n Things are looking good, and it’s time to spice up the implementation by adding\nmarshmallow models. Move on to the next section to learn how to do that!\n6.9\nImplementing payload validation models \nwith marshmallow\nFlask-smorest uses marshmallow models to validate request and response payloads. In\nthis section, we learn to work marshmallow models by implementing the schemas of\nthe kitchen API. The marshmallow models will help flask-smorest validate our pay-\nloads and serialize our data.\n As you can see in the kitchen API specification under ch06/kitchen/oas.yaml in\nthis book’s GitHub repository, the kitchen API contains three schemas: Schedule-\nOrderSchema schema, which contains the details needed to schedule an order; Get-\nScheduledOrderSchema, which represents the details of a scheduled order; and\nOrderItemSchema, which represents a collection of items in an order. Listing 6.11\nshows how to implement these schemas as marshmallow models under kitchen/api/\nschemas.py.\n To create marshmallow models, we create subclasses of marshmallow’s Schema\nclass. We define the models’ properties with the help of marshmallow’s field classes,\nsuch as String and Integer. Marshmallow uses these property definitions to validate\na payload against a model. To customize the behavior of marshmallow’s models, we\nuse the Meta class to set the unknown attribute to EXCLUDE, which instructs marshmal-\nlow to invalidate the payload with unknown properties.\n# file: kitchen/api/schemas.py\nfrom marshmallow import Schema, fields, validate, EXCLUDE\nclass OrderItemSchema(Schema):\n    class Meta:     \n        unknown = EXCLUDE\n    product = fields.String(required=True)\n    size = fields.String(\n        required=True, validate=validate.OneOf(['small', 'medium', 'big'])\n    )\n    quantity = fields.Integer(\n        validate=validate.Range(1, min_inclusive=True), required=True\n    )\nListing 6.11\nSchema definitions for the orders API\nWe use the Meta \nclass to ban unknown \nproperties.",
      "content_length": 2439,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "130\nCHAPTER 6\nBuilding REST APIs with Python\nclass ScheduleOrderSchema(Schema):\n    class Meta:\n        unknown = EXCLUDE\n    order = fields.List(fields.Nested(OrderItemSchema), required=True)\nclass GetScheduledOrderSchema(ScheduleOrderSchema):   \n    id = fields.UUID(required=True)\n    scheduled = fields.DateTime(required=True)\n    status = fields.String(\n        required=True,\n        validate=validate.OneOf(\n            [\"pending\", \"progress\", \"cancelled\", \"finished\"]\n        ),\n    )\nclass GetScheduledOrdersSchema(Schema):\n    class Meta:\n        unknown = EXCLUDE\n    schedules = fields.List(\n        fields.Nested(GetScheduledOrderSchema), required=True\n    )\nclass ScheduleStatusSchema(Schema):\n    class Meta:\n        unknown = EXCLUDE\n    status = fields.String(\n        required=True,\n        validate=validate.OneOf(\n            [\"pending\", \"progress\", \"cancelled\", \"finished\"]\n        ),\n    )\nNow that our validation models are ready, we can link them with our views. Listing\n6.12 shows how we use the models to add validation for request and response payloads\non our endpoints. To add request payload validation to a view, we use the blueprint’s\narguments() decorator in combination with a marshmallow model. For response pay-\nloads, we use the blueprint’s response() decorator in combination with a marshmal-\nlow model.\n By decorating our methods and functions with the blueprint’s response() decora-\ntor, we no longer need to return a tuple of payload plus a status code. Flask-smorest\ntakes care of adding the status code for us. By default, flask-smorest adds a 200 status\ncode to our responses. If we want to customize that, we simply need to specify the\ndesired status code using the status_code parameter in the decorator.\n While the blueprint’s arguments() decorator validates and deserializes a request\npayload, the blueprint’s response() decorator doesn’t perform validation and only\nWe use class \ninheritance to reuse \nthe definitions of an \nexisting schema.",
      "content_length": 1989,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "131\n6.9\nImplementing payload validation models with marshmallow\nserializes the payload. We’ll discuss this feature in more detail in section 6.11, and\nwe’ll see how we can ensure that data is validated before being serialized.\n# file: kitchen/api/api.py\nimport uuid\nfrom datetime import datetime\nfrom flask.views import MethodView\nfrom flask_smorest import Blueprint\nfrom api.schemas import (\n    GetScheduledOrderSchema,\n    ScheduleOrderSchema,\n    GetScheduledOrdersSchema,\n    ScheduleStatusSchema,    \n)    \nblueprint = Blueprint('kitchen', __name__, description='Kitchen API')\n...\n@blueprint.route('/kitchen/schedulles')\nclass KitchenSchedules(MethodView):\n    @blueprint.response(status_code=200, schema=GetScheduledOrdersSchema)  \n    def get(self):\n        return {'schedules': schedules}\n    @blueprint.arguments(ScheduleOrderSchema)     \n    @blueprint.response(status_code=201, schema=GetScheduledOrderSchema)   \n    def post(self, payload):\n        return schedules[0]\n@blueprint.route('/kitchen/schedules/<schedule_id>')\nclass KitchenSchedule(MethodView):\n    @blueprint.response(status_code=200, schema=GetScheduledOrderSchema)\n    def get(self, schedule_id):\n        return schedules[0]\n    @blueprint.arguments(ScheduleOrderSchema)\n    @blueprint.response(status_code=200, schema=GetScheduledOrderSchema)\n    def put(self, payload, schedule_id):\n        return schedules[0]\n    @blueprint.response(status_code=204)\n    def delete(self, schedule_id):\n        return\nListing 6.12\nAdding validation to the API endpoints\nWe import our \nmarshmallow \nmodels.\nWe use the blueprint’s response()\ndecorator to register a marshmallow\nmodel for the response payload.\nWe use the blueprint’s arguments() \ndecorator to register a marshmallow \nmodel for the request payload.\nWe set the status_code\nparameter to the\ndesired status code.",
      "content_length": 1836,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "132\nCHAPTER 6\nBuilding REST APIs with Python\n@blueprint.response(status_code=200, schema=GetScheduledOrderSchema)\n@blueprint.route(\n    '/kitchen/schedules/<schedule_id>/cancel', methods=['POST']\n)\ndef cancel_schedule(schedule_id):\n    return schedules[0]\n@blueprint.response(status_code=200, schema=ScheduleStatusSchema)\n@blueprint.route(\n    '/kitchen/schedules/<schedule_id>/status', methods=['GET']\n)\ndef get_schedule_status(schedule_id):\n    return schedules[0]\nTo see the effects of the new changes in the implementation, visit http://127.0.0.1:\n5000/docs URL again. If you’re running the server with the --reload flag, the changes\nwill be automatically reloaded. Otherwise, stop the server and run it again. As you can\nsee in figure 6.8, flask-smorest now recognizes the validation schemas that need to be\nused in the API, and therefore they’re represented in the Swagger UI. If you play\naround with the UI now, for example by hitting the GET /kitchen/schedules end-\npoint, you’ll be able to see the response payloads.\nThe API is looking good, and we are nearly finished with the implementation. The\nnext step is adding URL query parameters to the GET /kitchen/schedules endpoint.\nMove on to the next section to learn how to do that!\nWe can inspect the schema for this payload.\nProvides a\nsample payload\nbased on\nthe schema\nTells us that\nthe request\npayload is\nrequired\nTells us that this\nendpoint doesn’t\ntake URL parameters\nFigure 6.8\nThe Swagger UI shows the schema for the request for the POST /kitchen/schedules \nendpoint’s payload and provides an example of it.",
      "content_length": 1574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "133\n6.10\nValidating URL query parameters\n6.10\nValidating URL query parameters\nIn this section, we learn how to add URL query parameters to the GET /kitchen/\nschedules endpoint. As shown in listing 6.13, the GET /kitchen/schedules end-\npoint accepts three URL query parameters:\n\nprogress (Boolean)—Indicates whether an order is in progress.\n\nlimit (integer)—Limits the number of results returned by the endpoint.\n\nsince (date-time)—Filters results by the time when the orders were scheduled.\nA date in date-time format is an ISO date with the following structure: YYYY-\nMM-DDTHH:mm:ssZ. An example of this date format is 2021-08-31T01:01:01Z.\nFor more information on this format, see https://tools.ietf.org/html/rfc3339\n#section-5.6.\n# file: kitchen/oas.yaml\npaths:\n  /kitchen/schedules:\n    get:\n      summary: Returns a list of orders scheduled for production\n      parameters:\n        - name: progress\n          in: query\n          description: >-\n            Whether the order is in progress or not.\n            In progress means it's in production in the kitchen.\n          required: false\n          schema:\n            type: boolean\n        - name: limit\n          in: query\n          required: false\n          schema:\n            type: integer\n        - name: since\n          in: query\n          required: false\n          schema:\n            type: string\n            format: 'date-time'\nHow do we implement URL query parameters in flask-smorest? To begin, we need to\ncreate a new marshmallow model to represent them. We define the URL query param-\neters for the kitchen API using marshmallow. You can add the model for the URL\nquery parameters to kitchen/api/schemas.py with the other marshmallow models.\n \n \n \nListing 6.13\nSpecification for the GET /kitchen/schedules URL query parameters",
      "content_length": 1798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "134\nCHAPTER 6\nBuilding REST APIs with Python\n# file: kitchen/api/schemas.py\nfrom marshmallow import Schema, fields, validate, EXCLUDE\n...\nclass GetKitchenScheduleParameters(Schema):\n    class Meta:\n        unknown = EXCLUDE\n    progress = fields.Boolean()     \n    limit = fields.Integer()\n    since = fields.DateTime()\nWe register the schema for URL query parameters using the blueprint’s arguments()\ndecorator. We specify that the properties defined in the schema are expected in the\nURL, so we set the location parameter to query. \n# file: kitchen/api/api.py\nimport uuid\nfrom datetime import datetime\nfrom flask.views import MethodView\nfrom flask_smorest import Blueprint\nfrom api.schemas import (\n    GetScheduledOrderSchema, ScheduleOrderSchema, GetScheduledOrdersSchema,\n    ScheduleStatusSchema, GetKitchenScheduleParameters    \n)\nblueprint = Blueprint('kitchen', __name__, description='Kitchen API')\n...\n@blueprint.route('/kitchen/schedules')\nclass KitchenSchedules(MethodView):\n    @blueprint.arguments(GetKitchenScheduleParameters, location='query') \n    @blueprint.response(status_code=200, schema=GetScheduledOrdersSchema)\n    def get(self, parameters):    \n        return schedules\n...\nIf you reload the Swagger UI, you’ll see that the GET /kitchen/schedules endpoint\nnow accepts three optional URL query parameters (shown in figure 6.9). We should\nListing 6.14\nURL query parameters in marshmallow\nListing 6.15\nAdding URL query parameters to GET /kitchen/schedules\nWe define the fields \nof the URL query \nparameters. \nWe import the marshmallow model\nfor URL query parameters.\nWe register the model using the\narguments() decorator and set the\nlocation parameter to query.\nWe capture URL query \nparameter in the function \nsignature.",
      "content_length": 1743,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "135\n6.10\nValidating URL query parameters\npass these parameters to our business layer, which will use them to filter the list of\nresults. URL query parameters come in the form of a dictionary. If the user didn’t set\nany query parameters, the dictionary will be empty and therefore and evaluate to\nFalse. Since URL query parameters are optional, we check for their presence by\nusing the dictionary’s get() method. Since get() returns None when a parameter isn’t\nset, we know that a parameter is set when its value isn’t None. We won’t be implement-\ning the business layer until chapter 7, but we can use query parameters to filter our in-\nmemory list of schedules.\n# file: kitchen/api/api.py\n...\n@blueprint.route('/kitchen/schedules')\nclass KitchenSchedules(MethodView):\n    @blueprint.arguments(GetKitchenScheduleParameters, location='query') \n    @blueprint.response(status_code=200, schema=GetScheduledOrdersSchema)\n    def get(self, parameters):\n        if not parameters:           \n            return {'schedules': schedules}\n        query_set = [schedule for schedule in schedules]     \n        in_progress = parameters.get(progress)   \n        if in_progress is not None:\n            if in_progress:\nListing 6.16\nUse filters in GET /kitchen/schedules\nURL query parameters\naccepted by the endpoint\nFigure 6.9\nThe Swagger UI shows the URL query parameters of the GET /kitchen/schedules \nendpoint, and it offers form fields that we can fill in to experiment with different values.\nIf no parameter is set, we \nreturn the full list of schedules.\nIf the user set \nany URL query \nparameters, we use \nthem to filter the \nlist of schedules.\nWe check for\nthe presence of\neach URL query\nparameter by\nusing the\ndictionary’s\nget() method.",
      "content_length": 1731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "136\nCHAPTER 6\nBuilding REST APIs with Python\n                query_set = [\n                    schedule for schedule in schedules\n                    if schedule['status'] == 'progress'\n                ]\n            else:\n                query_set = [\n                    schedule for schedule in schedules\n                    if schedule['status'] != 'progress'\n                ]\n        since = parameters.get('since')\n        if since is not None:\n            query_set = [\n                schedule for schedule in schedules\n                if schedule['scheduled'] >= since\n            ]\n        limit = parameters.get('limit')\n        if limit is not None and len(query_set) > limit:    \n            query_set = query_set[:limit]\n        return {'schedules': query_set}    \n...\nNow that we know how to handle URL query parameters with flask-smorest, there’s\none more topic we need to cover, and that is data validation before serialization. Move\non to the next section to learn more about this!\n6.11\nValidating data before serializing the response\nNow that we have schemas to validate our request payloads and we have hooked them\nup with our routes, we have to ensure that our response payloads are also validated. In\nthis section, we learn how to use marshmallow models to validate data. We’ll use this\nfunctionality to validate our response payloads, but you could use the same approach\nto validate any kind of data, such as configuration objects.\n When we send a payload in a response, flask-smorest serializes the payload using\nmarshmallow. However, as shown in figure 6.10, it doesn’t validate if it’s correctly\nformed.3 As you can see in figure 6.11, in contrast to marshmallow, FastAPI does val-\nidate our data before it’s serialized for a response.\n The fact that marshmallow doesn’t perform validation before serialization is not\nnecessarily undesirable. In fact, it can be argued that it’s a desirable behavior, as it\ndecouples the task of serializing from the task of validating the payload. There are two\n3 Before version 3.0.0, marshmallow used to perform validation before serialization (see the change log: https://\ngithub.com/marshmallow-code/marshmallow/blob/dev/CHANGELOG.rst#300-2019-08-18).\nIf limit is set \nand its value \nis lower than \nthe length of \nquery_set, we \nreturn a subset \nof query_set.\nWe return the filtered \nlist of schedules.",
      "content_length": 2366,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "137\n6.11\nValidating data before serializing the response\nOrderItemSchema\n+ id: UUID\n+ product: str\n+ size: str\n+ quantity: integer\nOrders service\nMarshal payload\nAPI interface\nflask-smorest\nForm successful\nresponse\nFigure 6.10\nWorkflow of a data \npayload with the flask-smorest \nframework. Response payloads \nare supposed to come from a \n“trusted zone,” and therefore are \nnot validated before marshalling.\nOrderItemSchema\n+ id: UUID\n+ product: str\n+ size: str\n+ quantity: integer\nOrders service\nValidation (with pydantic)\nMarshal payload\nAPI interface\nFastAPI\nPayload is valid\nRaise server error\nPayload is invalid\nForm successful\nresponse\nForm error\nresponse\nFigure 6.11\nWorkflow \nof a data payload with \nthe FastAPI framework. \nBefore marshalling a \nresponse, FastAPI \nvalidates that the \npayload conforms to the \nspecified schema.",
      "content_length": 834,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "138\nCHAPTER 6\nBuilding REST APIs with Python\nrationales to justify why marshmallow doesn’t perform validation before serialization\n(http://mng.bz/9Vwx):\nIt improves performance, since validation is slow.\nData coming from the server is supposed to be trusted and therefore shouldn’t\nrequire validation. \nThe reasons the maintainers of marshmallow use to justify this design decision are\nfair. However, if you’ve worked with APIs, and websites in general, long enough, you\nknow there’s generally very little to be trusted, even from within your own system.\nZERO-TRUST APPROACH FOR ROBUST APIS\nAPI integrations fail due to the server\nsending the wrong payload as much as they fail due to the client sending mal-\nformed payloads to the server. Whenever possible, it’s good practice to take a\nzero-trust approach to our systems design and validate all data, regardless of its\norigin.\nThe data that we send from the kitchen API comes from a database. In chapter 7, we’ll\nlearn patterns and techniques to ensure that our database contains the right data in\nthe right format. However, and even under the strictest access security measures,\nthere’s always a chance that malformed data ends up in the database. As unlikely as\nthis is, we don’t want to ruin the user experience if that happens, and validating our\ndata before serializing helps us with that.\n Thankfully, it’s easy to validate data using marshmallow. We simply need to get an\ninstance of the schema we want to validate against and use its validate() method to\npass in the data we need to validate. validate() doesn’t raise an exception if it finds\nerrors. Instead, it returns a dictionary with the errors, or an empty dictionary if no\nerrors are found. To get a feeling for how this works, open a Python shell by typing\npython in the terminal, and run the following code:\n>>> from api.schemas import GetScheduledOrderSchema\n>>> GetScheduledOrderSchema().validate({'id': 'asdf'})\n{'order': ['Missing data for required field.'], 'scheduled': ['Missing\n➥ data for required field.'], 'status': ['Missing data for required \n➥ field.'], 'id': ['Not a valid UUID.']}\nAfter importing the schema on line 1, in line 2 we pass a malformed representation of\na schedule containing only the id field, and in line 3 marshmallow helpfully reports\nthat the order, scheduled, and status fields are missing, and that the id field is not a\nvalid UUID. We can use this information to raise a helpful error message in the server,\nas shown in listing 6.17. We validate schedules in the GET /kitchen/schedules\nmethod view before building and returning the query set, and we iterate the list of\nschedules to validate one at a time. Before validation, we make a deep copy of the\nschedule so that we can transform its datetime object into an ISO date string, since\nthat’s the format expected by the validation method. If we get a validation error, we",
      "content_length": 2880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "139\n6.11\nValidating data before serializing the response\nraise marshmallow’s ValidationError exception, which automatically formats the error\nmessage into an appropriate HTTP response.\n# file: kitchen/api/api.py\nimport copy\nimport uuid\nfrom datetime import datetime\nfrom flask.views import MethodView\nfrom flask_smorest import Blueprint\nfrom marshmallow import ValidationError    \n...\n@blueprint.route('/kitchen/schedules')\nclass KitchenSchedules(MethodView):\n    @blueprint.arguments(GetKitchenScheduleParameters, location='query') \n    @blueprint.response(status_code=200, schema=GetScheduledOrdersSchema)\n    def get(self, parameters):\n        for schedule in schedules:\n            schedule = copy.deepcopy(schedule)\n            schedule['scheduled'] = schedule['scheduled'].isoformat()\n            errors = GetScheduledOrderSchema().validate(schedule)   \n            if errors:\n                raise ValidationError(errors)   \n        ...\n        return {'schedules': query_set}\n...\nPlease be aware that there are known issues with validation in marshmallow, especially\nwhen your models contain complex configurations for determining which fields\nshould be serialized and which fields shouldn’t (see https://github.com/marshmallow\n-code/marshmallow/issues/682 for additional information). Also, take into account\nthat validation is known to be a slow process, so if you are handling large payloads, you\nmay want to use a different tool to validate your data, validate only a subset of your\ndata, or skip validation altogether. However, whenever possible, you’re better off per-\nforming validation on your data.\n This concludes the implementation of the functionality of the kitchen API. How-\never, the API is still returning the same mock schedule across all endpoints. Before\nconcluding this chapter, let’s add a minimal implementation of an in-memory list of\nschedules so that we can make our API dynamic. This will allow us to verify that all\nendpoints are functioning as intended.\nListing 6.17\nValidating data before serialization\nWe import the \nValidationError class \nfrom marshmallow.\nWe capture\nvalidation\nerrors in\nthe errors\nvariable.\nIf validate() finds errors, \nwe raise a ValidationError \nexception.",
      "content_length": 2216,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "140\nCHAPTER 6\nBuilding REST APIs with Python\n6.12\nImplementing an in-memory list of schedules\nIn this section, we implement a simple in-memory representation of schedules so that\nwe can obtain dynamic results from the API. By the end of this section, we’ll be able to\nschedule orders, update them, and cancel them through the API. Because the sched-\nules are managed as an in-memory list, any time the server is restarted, we’ll lose infor-\nmation from our previous session. In the next chapter, we’ll address this problem by\nadding a persistence layer to our service.\n Our in-memory collection of schedules will be represented by a Python list, and\nwe’ll simply add and remove elements from it in the API layer. Listing 6.18 shows the\nchanges that we need to make to kitchen/api/api.py to make this possible. We initial-\nize an empty list and assign it to a variable named schedules. We also refactor our\ndata validation code into an independent function named validate_schedule() so\nthat we can reuse it in other view methods or functions. When a schedule payload\narrives in the KitchenSchedules’ post() method, we set the server-side attributes,\nsuch as the ID, the scheduled time, and the status. In the singleton endpoints, we look\nfor the requested schedule by iterating the list of schedules and checking their IDs. If\nthe requested schedule isn’t found, we return a 404 response.\n# file: kitchen/api/api.py\nimport copy\nimport uuid\nfrom datetime import datetime\nfrom flask import abort\n...\nschedules = []    \ndef validate_schedule(schedule):     \n    schedule = copy.deepcopy(schedule)\n    schedule['scheduled'] = schedule['scheduled'].isoformat()\n    errors = GetScheduledOrderSchema().validate(schedule)\n    if errors:\n        raise ValidationError(errors)\n@blueprint.route('/kitchen/schedules')\nclass KitchenSchedules(MethodView):\n    @blueprint.arguments(GetKitchenScheduleParameters, location='query')  \n    @blueprint.response(GetScheduledOrdersSchema)\n    def get(self, parameters):\n        ...\n    @blueprint.arguments(ScheduleOrderSchema)\n    @blueprint.response(status_code=201, schema=GetScheduledOrderSchema,)\nListing 6.18\nIn-memory implementation of schedules\nWe initialize \nschedules as \nan empty list.\nWe refactor our data \nvalidation code into \na function.",
      "content_length": 2279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "141\n6.12\nImplementing an in-memory list of schedules\n    def post(self, payload):\n        payload['id'] = str(uuid.uuid4())    \n        payload['scheduled'] = datetime.utcnow()\n        payload['status'] = 'pending'\n        schedules.append(payload)\n        validate_schedule(payload)\n        return payload\n@blueprint.route('/kitchen/schedules/<schedule_id>')\nclass KitchenSchedule(MethodView):\n    @blueprint.response(status_code=200, schema=GetScheduledOrderSchema)\n    def get(self, schedule_id):\n        for schedule in schedules:\n            if schedule['id'] == schedule_id:\n                validate_schedule(schedule)\n                return schedule\n        abort(404, description=f'Resource with ID {schedule_id} not found')  \n    @blueprint.arguments(ScheduleOrderSchema)\n    @blueprint.response(status_code=200, schema=GetScheduledOrderSchema)\n    def put(self, payload, schedule_id):\n        for schedule in schedules:\n            if schedule['id'] == schedule_id:\n                schedule.update(payload)     \n                validate_schedule(schedule)\n                return schedule\n        abort(404, description=f'Resource with ID {schedule_id} not found')\n    @blueprint.response(status_code=204)\n    def delete(self, schedule_id):\n        for index, schedule in enumerate(schedules):\n            if schedule['id'] == schedule_id:\n                schedules.pop(index)     \n                return\n        abort(404, description=f'Resource with ID {schedule_id} not found')\n@blueprint.response(status_code=200, schema=GetScheduledOrderSchema)\n@blueprint.route(\n    '/kitchen/schedules/<schedule_id>/cancel', methods=['POST']\n)\ndef cancel_schedule(schedule_id):\n    for schedule in schedules:\n        if schedule['id'] == schedule_id:\n            schedule['status'] = 'cancelled'     \n            validate_schedule(schedule)\n            return schedule\n    abort(404, description=f'Resource with ID {schedule_id} not found')\n@blueprint.response(status_code=200, schema=ScheduleStatusSchema)\n@blueprint.route(\n    '/kitchen/schedules/<schedule_id>/status', methods=['GET']\n)\nWe set the server-side \nattributes of a schedule, \nsuch as the ID.\nIf a schedule isn’t\nfound, we return a\n404 response.\nWhen a user updates a \nschedule, we update the \nschedule’s properties with \nthe contents of the payload.\nWe remove the schedule \nfrom the list and return \nan empty response.\nWe set the status \nof the schedule to \ncancelled.",
      "content_length": 2432,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "142\nCHAPTER 6\nBuilding REST APIs with Python\ndef get_schedule_status(schedule_id):\n    for schedule in schedules:\n        if schedule['id'] == schedule_id:\n            validate_schedule(schedule)\n            return {'status': schedule['status']}\n    abort(404, description=f'Resource with ID {schedule_id} not found')\nIf you reload the Swagger UI and test the endpoints, you’ll see you’re now able to add\nschedules, update them, cancel them, list and filter them, get their details, and delete\nthem. In the next section, you’ll learn to override flask-smorest’s dynamically generated\nAPI specification to make sure we serve our API design instead of our implementation.\n6.13\nOverriding flask-smorest’s dynamically generated API \nspecification\nAs we learned in section 6.4, API specifications dynamically generated from code are\ngood for testing and visualizing our implementation, but to publish our API, we want to\nmake sure we serve our API design document. To do that, we’ll override flask-smorest’s\ndynamically generated API documentation. First, we need to install PyYAML, which\nwe’ll use to load the API design document:\n$ pipenv install pyyaml\nWe override the API object’s spec property with a custom APISpec object. We also\noverride APISpec’s to_dict() method so that it returns our API design document.\n# file: kitchen/app.py\nfrom pathlib import Path\nimport yaml\nfrom apispec import APISpec\nfrom flask import Flask\nfrom flask_smorest import Api\nfrom api.api import blueprint\nfrom config import BaseConfig\napp = Flask(__name__)\napp.config.from_object(BaseConfig)\nkitchen_api = Api(app)\nkitchen_api.register_blueprint(blueprint)\napi_spec = yaml.safe_load((Path(__file__).parent / \"oas.yaml\").read_text())\nspec = APISpec(\n    title=api_spec[\"info\"][\"title\"],\nListing 6.19\nOverriding flask-smorest’s dynamically generated API specification",
      "content_length": 1844,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "143\nSummary\n    version=api_spec[\"info\"][\"version\"],\n    openapi_version=api_spec[\"openapi\"],\n)\nspec.to_dict = lambda: api_spec\nkitchen_api.spec = spec\nThis concludes our journey through implementing REST APIs using Python. In the\nnext chapter, we’ll learn patterns to implement the rest of the service following best\npractices and useful design patterns. Things are spicing up!\nSummary\nYou can build REST APIs in Python using frameworks like FastAPI and flask-\nsmorest, which have great ecosystems of tools and libraries that make it easier to\nbuild APIs.\nFastAPI is a modern API framework that makes it easier to build highly perfor-\nmant and robust REST APIs. FastAPI is built on top of Starlette and pydantic.\nStarlette is a highly performant asynchronous server framework, and pydantic is\na data validation library that uses type hints to create validation rules.\nFlask-smorest is built on top of Flask and works as a Flask blueprint. Flask is one\nof Python’s most popular frameworks, and by using flask-smorest you can lever-\nage its rich ecosystem of libraries to make it easier to build APIs.\nFastAPI uses pydantic for data validation. Pydantic is a modern framework that\nuses type hints to define validation rules, which results in cleaner and easy-to-\nread code. By default, FastAPI validates both request and response payloads.\nFlask-smorest uses marshmallow for data validation. Marshmallow is a battle-\ntested framework that uses class fields to define validation rules. By default,\nflask-smorest doesn’t validate response payloads, but you can validate responses\nby using marshmallow models’ validate() method.\nWith flask-smorest, you can use Flask’s MethodView to create class-based views\nthat represent URL paths. In a class-based view, you implement HTTP methods\nas methods of the class, such as get() and post().\nThe tolerant reader pattern follows Postel’s law, which recommends being toler-\nant with errors in HTTP requests and validating response payloads. When\ndesigning your APIs, you must balance the benefits of the tolerant reader pat-\ntern with risk of integration failure due to bugs like typos.",
      "content_length": 2130,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "144\nService implementation\npatterns for microservices\nIn this chapter, we’ll learn how to implement the business layer of a microservice.\nIn previous chapters, we learned how to design and implement REST APIs. In those\nimplementations, we used an in-memory representation of the resources managed\nThis chapter covers\nHow hexagonal architecture helps us design \nloosely coupled services\nImplementing the business layer for a \nmicroservice and implementing database models \nusing SQLAlchemy\nUsing the repository pattern to decouple the data \nlayer from the business layer\nUsing the unit of work pattern to ensure the \natomicity of all transactions and using the \ndependency inversion principle to build software \nthat is resilient to changes\nUsing the inversion of control principle and \nthe dependency injection pattern to decouple \ncomponents that are dependent on each other",
      "content_length": 880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "145\n7.1\nHexagonal architectures for microservices\nby the service. We took that approach to keep the implementation simple and allow\nourselves to focus on the API layer of the service.\n In this chapter, we’ll complete our implementation of the orders service by adding\na business layer and a data layer. The business layer will implement the capabilities of\nthe orders service, such as taking orders, processing their payments, or scheduling\nthem for production. For some of these tasks, the orders service requires the collabo-\nration of other services, and we’ll learn useful patterns to handle those integrations.\n The data layer will implement the data management capabilities of the service.\nThe orders service owns and manages data about orders, so we’ll implement a per-\nsistent storage solution and an interface to it. However, as a gateway to users regarding\nthe life cycle of an order, the orders service also needs to fetch data from other ser-\nvices—for example, to keep track of the order during production and delivery. We’ll\nalso learn useful patterns to handle access to those services.\n To articulate the implementation patterns of the service, we’ll also cover elements\nof the architectural layout required to keep all pieces of our microservices loosely cou-\npled. Loose coupling will help us ensure that we can change the implementation of a\nspecific component without having to make changes to other components that rely on\nit. It’ll also make our codebase generally more readable, maintainable, and testable.\nThe code for this chapter is available in the ch07 directory in the repository provided\nwith this book.\n7.1\nHexagonal architectures for microservices\nThis chapter introduces the concept of hexagonal architecture and how we’ll apply it\nto the design of the orders service. In chapter 2, we introduced the three-tier architec-\nture pattern to help us organize the components of our application in a modular and\nloosely coupled way. In this section, we’ll take this idea further by applying the con-\ncept of hexagonal architecture to our design.\n In 2005, Alistair Cockburn introduced the concept of hexagonal architecture, also\ncalled the architecture of ports and adapters, as a way to help software developers struc-\nture their code into loosely coupled components.1 As you can see in figure 7.1, the\nidea behind the hexagonal or ports-and-adapters architecture is that, in any applica-\ntion, there’s a core piece of logic that implements the capabilities of a service, and\naround that core we “attach” adapters that help the core communicate with external\ncomponents. For example, a web API is an adapter that helps the core communicate\nwith web clients over the internet. The same goes for a database, which is simply an\nexternal component that helps a service persist data. We should be able to swap the\n1 Alistair Cockburn, “Hexagonal Architecture,” https://alistair.cockburn.us/hexagonal-architecture/. You may\nbe wondering why hexagonal and not pentagonal or heptagonal. As Alistair points out, it “is not a hexagon\nbecause the number six is important,” but because it helps to visually highlight the idea of a core application\ncommunicating with external components through ports (the sides of the hexagon), and it allows us to repre-\nsent the two main sides of an application: the public-facing side (web components, APIs, etc.) and the internal\nside (databases, third-party integrations, etc.).",
      "content_length": 3433,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "146\nCHAPTER 7\nService implementation patterns for microservices\ndatabase if we want, and the service would still be the same. Therefore, the database is\nalso an adapter.\n How does this help us build loosely coupled services? Hexagonal architecture\nrequires that we keep the core logic of the service and the logic for the adapters strictly\nseparated. In other words, the logic that implements our web API layer shouldn’t inter-\nfere with the implementation of the core business logic. And the same goes for the data-\nbase: regardless of the technology we choose, and its design and idiosyncrasies, it\nshouldn’t interfere with the core business logic. How do we achieve that? By building\nports between the core business layer and the adapters. Ports are technology-agnostic\ninterfaces that connect the business layer with the adapters. Later in this chapter, we’ll\nlearn some design patterns that will help us design those ports or interfaces.\n When working out the relationship between the core business logic and the adapt-\ners, we apply the dependency inversion principle, which states that (see figure 7.2 for\nclarification)\nHigh-level modules shouldn’t depend on low-level details. Instead, both should\ndepend on abstractions, such as interfaces. For example, when saving data, we\nwant to do it through an interface that doesn’t require understanding of the\nspecific implementation details of the database. Whether it’s an SQL or a\nNoSQL database or a cache store, the interface should be the same.\nAbstractions shouldn’t depend on details. Instead, details should depend on\nabstractions.2 For example, when designing the interface between the business\nlayer and the data layer, we want to make sure that the interface doesn’t change\nbased on the implementation details of the database. Instead, we make changes\nto the data layer to make it work with the interface. In other words, the data\nlayer depends on the interface, not the other way around.\nDEFINITION\nThe dependency inversion principle encourages us to design our\nsoftware against interfaces and to make sure we don’t create dependencies\nbetween the low-level details of our components. \n2 Robert C. Martin, Agile Software Development, Principles, Patterns, and Practices (Prentice Hall, 2003), pp. 127–131.\nBusiness logic\nWeb API interface\n(adapter)\nData layer\n(adapter)\nFigure 7.1\nIn hexagonal architecture, we distinguish a core layer in our \napplication, the business layer, which implements the service’s capabilities. \nOther components, such as a web API interface or a database, are considered \nadapters that depend on the business layer.",
      "content_length": 2610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "147\n7.1\nHexagonal architectures for microservices\nThe concept of dependency inversion often appears with the concepts of inversion of\ncontrol and dependency injection. These are related but different concepts. As we’ll\nsee in section 7.5, the inversion of control principle consists of supplying code depen-\ndencies through the execution context (also called the inversion of control con-\ntainer). To supply such dependencies, we can use the dependency injection pattern,\nwhich we’ll describe in section 7.5.\n What does this mean in practice? It means we should make the adapters depend\non the interface exposed by the core business logic. That is, it’s okay for our API layer\nto know about the core business logic’s interface, but it’s not okay for our business\nlogic to know specific details of our API layer or low-level details of the HTTP proto-\ncol. The same goes for the database: our data layer should know how the application\nworks and how to accommodate the application’s needs to our choice of storage tech-\nnology, but the core business layer should know nothing specific about the database.\nOur business layer will expose an interface, and all other components will be imple-\nmented against it.\n What exactly are we inverting with the dependency inversion principle? This prin-\nciple inverts the way we think about software. Instead of the more conventional approach\nof building the low-level details of our software first, and then building interfaces on\ntop of them, the dependency inversion principle encourages us to think of the inter-\nfaces first and then build the low-level details against them.3\n As you can see in figure 7.3, when it comes to the orders service, we’ll have a core\npackage that implements the capabilities of the service. This includes the ability to\nprocess an order and its payment, to schedule its production, or to keep track of its\nprogress. The core service package will expose interfaces for other components of the\napplication. Another package implements the web API layer, and our API modules\nwill use functions and classes from the business layer interface to serve the requests of\nour users. Another package implements the data layer, which knows how to interact\nwith the database and return business objects for the core business layer.\n3 For an excellent introduction to the dependency inversion principle, see Eric Freeman, Elizabeth Robson,\nKathy Sierra, and Bert Bates, Head First Design Patterns (O’Reilly, 2014), pp. 141–143.\nBusiness logic\nData layer\n(adapter)\nDirection of the dependency: the\nbusiness logic exposes an interface\nagainst which the data layer is\nimplemented.\nInterface\n(port)\nFigure 7.2\nWe apply the dependency inversion principle to determine which \ncomponents drive the changes. In hexagonal architecture, this means that our \nadapters will depend on the interface exposed by the core business layer.",
      "content_length": 2876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "148\nCHAPTER 7\nService implementation patterns for microservices\nNow that we know how we are going to structure the application, it’s time to start\nimplementing it! In the next section, we’ll set up the environment to start working on\nthe service.\n7.2\nSetting up the environment and the project structure\nIn this section, we set up the environment to work on the orders service and lay out\nthe high-level structure of the project. As in previous chapters, we’ll use Pipenv to\nmanage our dependencies. Run the following commands to set up a Pipenv environ-\nment and activate it:\n$ pipenv --three\n$ pipenv shell\nWe’ll install our dependencies as we need in the following sections. Or if you prefer,\ncopy the Pipfile and Pipfile.lock files from the GitHub repository under the ch07\nfolder and run pipenv install.\n Our service implementation will live under a folder named orders, so go ahead\nand create it. To reinforce the separation of concerns between the core business layer\nand the API and database adapters, we’ll implement each of them in different directo-\nries, as shown in figure 7.4. The business layer will live under orders/orders_service.\nCore business logic\npackage\nData layer package\n(adapter)\nInterface\n(port)\nInterface\n(port)\nAPI layer package\n(adapter)\nFigure 7.3\nThe orders service consist of three packages: the core business logic, \nwhich implements the capabilities of the service; an API layer, which allows clients \nto interact with the service over HTTP; and a data layer, which allows the service \nto interact with the database. The core business logic exposes interfaces against \nwhich the API layer and the data layer are implemented.\nFigure 7.4\nTo reinforce the separation of concerns, we implement each layer of the \napplication in different directories: orders_service for the core business layer; repository \nfor the data layer; and web/api for the API layer.",
      "content_length": 1888,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "149\n7.3\nImplementing the database models\nSince the API layer is a web component, it will live under orders/web, which contains\nweb adapters for the orders service. In this case, we are only including one type of web\nadapter, namely, a REST API, but nothing prevents you from adding a web adapter\nthat returns dynamically rendered content from the server, as you would in a more tra-\nditional Django application.\n The data layer will live under orders/repository. “Repository” might look like an\nunlikely name for our data layer, but we’re choosing this name because we’ll implement\nthe repository pattern to interface with our data. This concept will become clearer in\nsection 7.4. In chapters 2 and 6 we covered the implementation of the API layer, so go\nahead and copy over the files from the GitHub repository under ch07/order/web\ninto your local directory. Notice that the API implementation has been adapted for\nthis chapter. \n├── Pipfile    \n├── Pipfile.lock\n└── orders     \n    ├── orders_service     \n    ├── repository     \n    └── web     \n        ├── api   \n        │   ├── api.py\n        │   └── schemas.py\n        └── app.py     \nSince the folder structure has changed, the path to our FastAPI application object has\nalso changed location, and therefore the command to run the API server is now\n$ uvicorn orders.web.app:app --reload\nDue to the new folder structure, a few import paths and file locations have also\nchanged. For the full list of changes, please refer to the ch07 folder under the GitHub\nrepository for this book.\n Now that our project is set up and ready to go, it’s time to get on with the imple-\nmentation. Move on to the next section to learn how to add database models to the\nservice!\n7.3\nImplementing the database models\nIn the previous section, we learned how we’ll structure our project into three differ-\nent layers: the core business layer, the API layer, and the data layer. This structure\nreinforces the separation of concerns among each layer, as recommended by the\nhexagonal architecture pattern that we learned in section 7.1. Now that we know\nhow we’ll structure our code, it’s time to focus on the implementation. In this section,\nListing 7.1\nHigh-level structure of the orders service\nPipfile contains the \nlist of dependencies.\nThe full implementation \nof the orders service\nThe business layer\nThe data layer \nWeb\nadapters\nREST API\nimplementation\nThis file contains the \ninstance of our web \nserver object.",
      "content_length": 2452,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "150\nCHAPTER 7\nService implementation patterns for microservices\nwe’ll define the database models for the orders service; that is, we’ll design the data-\nbase tables and their fields. We start our implementation from the database since it\nwill facilitate the rest of the discussion in this chapter. In a real-world context, you\nmight start with the business layer, mocking the data layer and iterating back and\nforth between each layer until you’re done with the implementation. Just bear in\nmind that the linear approach we take in this chapter is not meant to reflect the actual\ndevelopment process, but is instead intended to illustrate concepts that we want to\nexplain.\n To keep things simple in this chapter, we’ll use SQLite as our database engine.\nSQLite is a file-based relational database system. To use it, we don’t need to set up and\nrun a server, as we would with PostgreSQL or MySQL, and there’s no configuration\nneeded to start using it. Python’s core library has built-in support for interfacing with\nSQLite, which makes it a suitable choice for quick prototyping and experimentation\nbefore we are ready to move on to a production-ready database system.\n We won’t manage our connection to the database and our queries manually. That\nis, we won’t be writing our own SQL statements to interact with the database. Instead,\nwe’ll use SQLAlchemy—by far the most popular ORM (object relational mapper) in\nthe Python ecosystem. An ORM is a framework that implements the data mapper pat-\ntern, which allows us to map the tables in our database to objects.\nDEFINITION\nA data mapper is an object wrapper around database tables and\nrows. It encapsulates database operations in the form of class methods, and it\nallows us to access data fields through class attributes.4\nAs you can see in figure 7.5, using an ORM makes it easier to manage our data since it\ngives us a class interface to the tables in the database. This allows us to leverage the\nbenefits of object-oriented programming, including the ability to add custom meth-\nods and properties to our database models that enhance their functionality and\nencapsulate their behavior.\n Over time, our database models will change, and we need to be able to keep track\nof those changes. Changing the schema of our database is called a migration. As our\ndatabase evolves, we’ll accumulate more and more migrations. We need to keep track\nof our migrations, since they allow us to reliably replicate the database schema in dif-\nferent environments and to roll out database changes to production with confidence.\nTo manage this complex task, we’ll use Alembic. Alembic is a schema migration\nlibrary that integrates seamlessly with SQLAlchemy.\n Let’s start by installing both libraries by running the following command:\n$ pipenv install sqlalchemy alembic\nBefore we start working on our database models, let’s set up Alembic. (For additional\nhelp, please check out my video tutorial about setting up Alembic with SQLAlchemy\n4 Martin Fowler, Patterns of Enterprise Architecture (Addison-Wesley, 2003), pp. 165–181.",
      "content_length": 3060,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "151\n7.3\nImplementing the database models\nat https://youtu.be/nt5sSr1A_qw.) Run the following command to create a migra-\ntions folder, which will contain the history of all migrations in our database:\n$ alembic init migrations\nThis creates a folder called migrations, which comes with a configuration file called\nenv.py and a versions/ directory. The versions/ directory will contain the migration\nfiles. The setup command also creates a configuration file called alembic.ini. To\nmake Alembic work with an SQLite database, open alembic.ini, find a line that con-\ntains a declaration for the sqlalchemy.url variable, and replace it with the follow-\ning content:\nsqlalchemy.url = sqlite:/ / /orders.db\nCOMMIT THE FILES GENERATED BY ALEMBIC\nThe migrations folder contains all\nthe information required to manage our database schema changes, so you\nshould commit this folder, as well as alembic.ini. This will allow you to repli-\ncate the database setup in new environments.\nTable\nrder\no\nPK\nUniqueID\nid: UUID PRIMARY KEY\ncreated: TIMESTAMP\nstatus: VARCHAR\nOrderModel\n+ id: uuid\n+ created: datetime\n+ items: relationship(OrderItemModel)\n+ status: str\n+ dict(): dict\nAn ORM class maps to a\ndatabase table, and we\ncan access the values in\neach row as attributes\nof an object.\nBecause database models\nare classes, we can enhance\nthem with custom methods,\nsuch as dict().\nTable order_item\nPK\nUniqueID\nid: UUID PRIMARY KEY\norder_id: FOREIGN KEY\n( rder)\no\nproduct: VARCHAR\nsize: VARCHAR\nquantity: INTEGER\nForeign keys\nrefer to\nrecords from\nother tables.\nOrderItemModel\n+ id: uuid\n+ order_id: ForeignKey(order)\n+ product: str\n+ size: str\n+ quantity: int\n+ dict(): dict\nFigure 7.5\nUsing an ORM, we can implement our data models as classes that map to database tables. Since \nthe models are classes, we can enhance them with custom methods to add new functionality.",
      "content_length": 1849,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "152\nCHAPTER 7\nService implementation patterns for microservices\nIn addition, open migrations/env.py and find the lines with this content:5\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = None\nReplace them with the following content:\nfrom orders.repository.models import Base\ntarget_metadata = Base.metadata\nBy setting target_metadata to our Base model’s metadata, we make it possible for\nAlembic to load our SQLAlchemy models and generate database tables from them.\nNext, we’ll implement our database models. Before we jump into the implementation,\nlet’s pause for a moment to think about how many models we’ll need and the proper-\nties we should expect each model to have. The core object of the orders service is the\norder. Users place, pay, update, or cancel orders. Orders have a life cycle, and we’ll\nkeep track of it through a status property. We’ll use the following list of properties to\ndefine our order model:\nID—Unique ID for the order. We’ll give it the format of a Universally Unique\nIdentifier (UUID). Using UUIDs instead of incremental integers is quite com-\nmon these days. UUIDs work well in distributed systems, and they help to hide\ninformation about the number of orders that exist in the database from our\nusers.\nCreation date—When the order was placed.\nItems—The list of items included in the order and the amount of each product.\nSince an order can have any number of items linked to it, we’ll use a different\nmodel for items, and we’ll create a one-to-many relationship between the order\nand the items.\nStatus—The status of the order throughout the system. An order can have the\nfollowing statuses:\n– Created—The order has been placed.\n– Paid—The order has been successfully paid.\n– Progress—The order is being produced in the kitchen.\n– Cancelled—The order has been cancelled. \n– Dispatched—The order is being delivered to the user.\n– Delivered—The order has been delivered to the user.\nSchedule ID—The ID of the order in the kitchen service. This ID is created by\nthe kitchen service after scheduling the order for production, and we’ll use it to\nkeep track of its progress in the kitchen. \n5 The shape and format of this file may change over time, but for reference, at the time of this writing, those\nlines are 18–20.",
      "content_length": 2290,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "153\n7.3\nImplementing the database models\nDelivery ID—The ID of the order in the delivery service. This ID is created by\nthe delivery service after scheduling it for dispatch, and we’ll use it to keep\ntrack of its progress during delivery. \nWhen users place an order, they add any number of items to the order. Each item con-\ntains information about the product selected by the user, the size of the product, and\nthe amount of it that the user wishes to purchase. There’s a one-to-many relationship\nbetween orders and items, and therefore we’ll implement a model for items and link\nthem with a foreign key relationship. The item model will have the following list of\nattributes: \nID—A unique identifier for the item in UUID format. \nOrder ID—A foreign key representing the ID of the order the item belongs to.\nThis is what allows us to connect items and orders that belong together. \nProduct—The product selected by the user.\nSize—The size of the product.\nQuantity—The amount of the product that the user wishes to purchase. \nOur SQLAlchemy models will live under the orders/repository folder, which we cre-\nated to encapsulate our data layer, in a file called orders/repository/models.py. We’ll\nuse these classes to interface with the database and rely on SQLAlchemy to translate\nthese models into their corresponding database tables behind the scenes. Listing 7.2\nshows the definition of the database models for the orders service. First, we create a\ndeclarative base model by using SQLALchemy’s declarative_base() function. The\ndeclarative base model is a class that can map ORM classes to database tables and col-\numns, and therefore all our database models must inherit from it. We map class attri-\nbutes to specific database columns by setting them to instances of SQLAlchemy’s\nColumn class.\n To map an attribute to another model, we use SQLAlchemy’s relationship()\nfunction. In listing 7.2, we use relationship() to create a one-to-many relationship\nbetween OrderModel’s items attribute and the OrderItemModel model. This means\nthat we can access the list of items in an order through OrderModel’s items attribute.\nEach item also maps to the order it belongs to through the order_id property, which\nis defined as a foreign key column. Furthermore, relationship()’s backref argu-\nment allows us to access the full order object from an item directly through a property\ncalled order.\n Since we want our IDs to be in UUID format, we create a function that SQLAl-\nchemy can use to generate the value. If we later switch to a database engine with built-\nin support for generating UUID values, we’ll leave it to the database to generate the\nIDs. Each database model is enhanced with a dict() method, which allows us to out-\nput the properties of a record in dictionary format. Since we’ll use this method to\ntranslate database models to business objects, the dict() method only returns the\nproperties relevant to the business layer.",
      "content_length": 2937,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "154\nCHAPTER 7\nService implementation patterns for microservices\n# file: orders/repository/models.py\nimport uuid\nfrom datetime import datetime\nfrom sqlalchemy import Column, Integer, String, ForeignKey, DateTime\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship\nBase = declarative_base()   \ndef generate_uuid():   \n    return str(uuid.uuid4())\nclass OrderModel(Base): \n    __tablename__ = 'order'    \n    id = Column(String, primary_key=True, default=generate_uuid)    \n    items = relationship('OrderItemModel', backref='order')   \n    status = Column(String, nullable=False, default='created')\n    created = Column(DateTime, default=datetime.utcnow)\n    schedule_id = Column(String)\n    delivery_id = Column(String)\n    def dict(self):   \n        return {\n            'id': self.id,\n            'items': [item.dict() for item in self.items],  \n            'status': self.status,\n            'created': self.created,\n            'schedule_id': self.schedule_id,\n            'delivery_id': self.delivery_id,\n        }\nclass OrderItemModel(Base):\n    __tablename__ = 'order_item'\n    id = Column(String, primary_key=True, default=generate_uuid)\n    order_id = Column(Integer, ForeignKey('order.id'))\n    product = Column(String, nullable=False)\n    size = Column(String, nullable=False)\n    quantity = Column(Integer, nullable=False)\n    def dict(self):\n        return {\n            'id': self.id,\n            'product': self.product,\n            'size': self.size,\n            'quantity': self.quantity\n        }\nListing 7.2\nSQLAlchemy models for the orders service\nWe create our declarative \nbase model.\nCustom function to create \nrandom UUIDs for our models\nAll our \nmodels must \ninherit from \nBase.\nName of the table that \nmaps to this model\nEvery class property\nmaps to a database\ncolumn by using the\nColumn class.\nWe use relationship() to create\na one-to-many relationship\nwith the OrderItemModel model.\nCustom method \nto render our \nobjects as Python \ndictionaries\nWe call dict() \non each item to \nget its dictionary \nrepresentation.",
      "content_length": 2084,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "155\n7.4\nImplementing the repository pattern for data access\nTo apply the models to the database, run the following command from the ch07\ndirectory:\n$ PYTHONPATH=`pwd` alembic revision --autogenerate -m \"Initial migration\"\nThis will create a migration file under migrations/versions. We set the PYTHONPATH\nenvironment variable to the current directory using the pwd command so that Python\nlooks for our models relative to this directory. You should commit your migration files\nand keep them in your version control system (e.g., a Git repository) since they’ll\nallow you to re-create your database for different environments. You can look in those\nfiles to understand the database operations that SQLAlchemy will perform to apply\nthe migrations. To apply the migrations and create the schemas for these models in\nthe database, run the following command:\n$ PYTHONPATH=`pwd` alembic upgrade heads \nThis will create the desired schemas in our database. Now that our database models\nare implemented and our database contains the desired schemas, it’s time to move on\nto the next step. Go to the next section to learn about the repository pattern!\n7.4\nImplementing the repository pattern for data access\nIn the previous section, we learned to design the database models for the orders ser-\nvice and to manage changes to the database schema through migrations. With our\ndatabase models ready, we can interact with the database to create orders and manage\nthem. Now we have to decide how we make the data accessible to the business layer. In\nthis section, we’ll first discuss different strategies to connect the business layer with the\ndata layer, and we’ll learn what the repository pattern is and how we can use it to cre-\nate an interface between the business layer and the database. Then we’ll move on to\nimplementing it.\n7.4.1\nThe case for the repository pattern: What is it, \nand why is it useful?\nIn this section, we discuss different strategies for interfacing with the database from\nthe business layer, and we introduce the repository pattern as a strategy that helps us\ndecouple the business layer from the implementation details of the database.\n As shown in figure 7.6, a common strategy to enable interactions between the busi-\nness layer and the database is to use the database models directly within the business\nlayer. Our database models already contain data about the orders, so we could enhance\nthem with methods that implement business capabilities. This is called the active record\npattern, which represents objects that carry both data and domain logic.6 This pattern\n6 Fowler, Patterns of Enterprise Architecture, pp. 160–164.",
      "content_length": 2641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "156\nCHAPTER 7\nService implementation patterns for microservices\nis useful when we have one-to-one mapping between service capabilities and database\noperations, or when we don’t need the collaboration of multiple domains.\nThis approach works for simple cases; however, it couples the implementation of the\nbusiness layer to the database and to the ORM framework of choice. What happens\nif we want to change the ORM framework later on, or if we want to switch to a differ-\nent data storage technology that doesn’t involve SQL? In those cases, we’d have to\nmake changes to our business layer. This breaks the principles we introduced in sec-\ntion 7.1. Remember, the database is an adapter that the orders service uses to persist\ndata, and the implementation details of the database should not leak into the busi-\nness logic. Instead, data access will be encapsulated by our data access layer.\n To decouple the business layer from the data layer, we’ll use the repository pattern.\nThis pattern gives us an in-memory list interface of our data. This means that we can\nget, add, or delete orders from the list, and the repository will take care of translating\nthese operations into database-specific commands. Using the repository pattern means\nthe data layer exposes a consistent interface to the business layer to interact with the\ndatabase, regardless of the database technology we use to store our data. Whether we\nuse an SQL database such as PostgreSQL, a NoSQL database like MongoDB, or an in-\nmemory cache such as Redis, the repository pattern’s interface will remain the same\nand will encapsulate whichever specific operations are required to interact with the\ndatabase. Figure 7.7 illustrates how the repository pattern helps us invert the depen-\ndency between the data layer and the business layer.\nDEFINITION\nThe repository pattern is a software development pattern that pro-\nvides an in-memory list interface to our data store. This helps us decouple our\ncomponents from the low-level implementation details of the database. The\nrepository takes care of managing interactions with the database and provides\nBusiness logic\nData layer\n(adapter)\nOrderModel\n+ id: uuid\n+ created: datetime\n+ items: list\n+ status: str\n+ dict(): dict\nOrderModel\ninstance\nFigure 7.6\nA common approach to enable interactions between the data \nlayer and the business layer is by using the database models directly in \nthe business layer.",
      "content_length": 2417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "157\n7.4\nImplementing the repository pattern for data access\na consistent interface to our components, regardless of the database technol-\nogy used. This allows us to change the database system without having to\nchange our core business logic.\nNow that we know how we can use the repository pattern to allow the business layer to\ninterface with the database while decoupling its implementation from low-level details\nof the database, we’ll learn to implement the repository pattern.\n7.4.2\nImplementing the repository pattern\nHow do we implement the repository pattern? We can use different approaches to this\nas long as we meet the following constraint: none of the operations carried out by the\nrepository can be committed by the repository. What does this mean? It means that\nwhen we add an order object to the repository, the repository will add the order to a\ndatabase session, but it will not commit the changes. Instead, it will be the responsibil-\nity of the consumer of OrdersService (i.e., the API layer) to commit the changes. Fig-\nure 7.8 illustrates this process.\n Why can’t we commit database changes within the repository? First, because the\nrepository acts just like an in-memory list representation of our data, and as such it\ndoesn’t have a concept of database sessions and transactions; second, because the\nrepository is not the right place to execute a database transaction. Instead, the context\nin which the repository is invoked provides the right context for executing database\nBusiness logic\nData layer\n(adapter)\nOrderModel\n+ id: uuid\n+ created: datetime\n+ items: list\n+ status: str\n+ dict(): dict\nOrdersRepository\n+ add(payload): Order\n+ get(id): Order\n+ list(): List[Order]\n+ delete(): None\nOrder\n+ id: uuid\n+ created: datetime\n+ items: list\n+ status: str\n+ pay()\n+ schedule()\n+ cancel()\nThe business layer\naccesses the data\nlayer by interfacing\nwith the repository.\nThe repository\nencapsulates the\nimplementation\ndetails of the database\nlayer by translating\ndatabase models to\nbusiness objects\nfrom the business layer.\nFigure 7.7\nThe repository pattern encapsulates the implementation details of the data layer by \nexposing an in-memory list interface to the business layer, and it translates database models to \nbusiness objects.",
      "content_length": 2255,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "158\nCHAPTER 7\nService implementation patterns for microservices\ntransactions. In many cases, our applications will execute multiple operations that\ninvolve one or more repositories and also call to other services. For example, figure 7.9\nshows the number of operations involved in processing a payment:\n1\nThe API layer receives the request from the user and uses the OrdersService’s\npay_order() method to process the request. \n2\nOrdersService talks to the payments service to process the payment.\n3\nIf the payment is successful, OrdersService schedules the order with the kitchen\nservice.\n4\nOrdersService updates the state of the order in the database using the orders\nrepository.\n5\nIf all the previous operations were successful, the API layer commits the transac-\ntion in the database; otherwise, it rolls back the changes.\nThese steps can be taken synchronously, one after the other, or asynchronously, in no\nspecific order, but regardless of the approach, all steps must succeed or fail all together.\nAs the unit of execution context, it’s the responsibility of the API layer to ensure that\nall changes are committed or rolled back as required. In section 7.6, we’ll learn how\nexactly the API layer controls the database session and commits the transactions.\nBusiness logic\nOrdersService\n+ place_order()\nWeb API interface\n(adapter)\n1. The API layer\nuses OrdersService\nto place an order.\n2. OrdersService\nuses the orders\nrepository to add\nthe order to the\ndatabase session.\ncommit()\n3. The API layer commits the changes to the database.\nWeb API interface\n(adapter)\nData layer\n(adapter)\nOrderModel\n+ id: uuid\n+ created: datetime\n+ items: list\n+ status: str\nOrdersRepository\n+ add(payload): Order\nFigure 7.8\nUsing the repository pattern, the API layer uses the place_order() capability of \nOrdersService to place an order. To place the order, OrdersService interfaces with the orders \nrepository to add the order to the database. Finally, the API layer must commit the changes to persist \nthem in the database.",
      "content_length": 2011,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "159\n7.4\nImplementing the repository pattern for data access\nAt a minimum, a repository pattern implementation consists of a class that exposes a\nget() and an add() method, respectively, to be able to retrieve and add objects to the\nrepository. For our purposes, we’ll also implement the following methods: update(),\ndelete(), and list(). This will simplify the CRUD interface of the repository.\n The following question bears some consideration in this context: when we fetch\ndata through the repository, what kind of object should the repository return? In\nmany implementations, you’ll see repositories returning instances of the database\nmodels (i.e., the classes defined in orders/repository/models.py). We won’t do that in\nthis chapter. Instead, we’ll return objects that represent orders from the business layer\ndomain. Why is it a bad idea to return instances of the database models through the\nrepository? Because it defeats the purpose of the repository, which is to decouple the\nbusiness layer from the data layer. Remember, we may want to change our persistence\nstorage technology or our ORM framework. If that happens, the database classes we\nimplemented in section 7.2 will no longer exist, and there’s no guarantee that a new\nframework would allow us to return objects with the same interfaces. For this reason,\nwe don’t want to couple our business layer with them. Figure 7.10 illustrates the rela-\ntionship between the business layer and the orders repository.\n Our orders repository implementation will live under orders/repository/orders_\nrepository.py. Listing 7.3 shows the implementation of the orders repository. It takes\none required argument that represents the database session. Objects are added and\ndeleted from the database session. The add() and update() methods take payloads\nBusiness logic\nOrdersService\n+ pay_order()\nWeb API interface\n(adapter)\n1. The API layer uses\nOrdersService\nto process payment\nfor an order.\nData layer\n(adapter)\nOrdersRepository\nPayments service\nKitchen service\ncommit()\n5. The API layer commits the changes to the database.\nWeb API interface\n(adapter)\n2. Process the payment\nwith payment service.\n3. Schedule the\norder with the\nkitchen service.\n4. Update the state\nof the order in the\ndatabase with the\norders repository.\nFigure 7.9\nIn some situations, OrdersService has to interface with multiple repositories or services to \nperform an operation. In this example, OrdersService interfaces with the payments service to process a \npayment, then with the kitchen service to schedule the order for production, and finally updates the status of \nthe order through the orders repository. All these operations must succeed or fail together, and it’s the \nresponsibility of the API layer to commit or rollback accordingly.",
      "content_length": 2770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "160\nCHAPTER 7\nService implementation patterns for microservices\nthat represent orders in the form of a Python dictionary. Our payloads are fairly sim-\nple, so a dictionary is sufficient here, but if we have more complex payloads, we\nshould consider using objects instead.\n With the exception of the delete() method, all methods of the repository return\nOrder objects from the business layer (see section 7.5 for Order’s implementation\ndetails). To create instances of Order, we pass dictionary representations of the SQL-\nAlchemy models using our custom dict() method from listing 7.2. In the add()\nmethod, we also include a pointer to the actual SQLAlchemy model through Order’s\norder_ parameter. As we’ll see in section 7.5, this pointer will help us access the\norder’s ID after committing the database transaction.\n OrdersRepository’s get(), update(), and delete() methods use the same logic\nto pull a record before returning, updating, or deleting it, so we define a common\n_get() method that knows how to obtain a record given an ID and optional filters. We\nfetch the record using the first() method of SQLAlchemy’s query object. first()\nreturns an instance of the record if it exists, and otherwise it returns None. Alterna-\ntively, it’s also possible to use the one() method, which raises an error if the record\ndoesn’t exist. _get() returns a database record, so it’s not meant to be used by the ser-\nvice layer, and we signal that by prefixing the method’s name with an underscore.\n The list() method accepts a limit parameter and optional filters. We build our\nquery dynamically using SQLAlchemy’s query object. We also leverage SQLAlchemy’s\nfilter_by() method to include additional filters in the query as keyword arguments,\nand we limit the query results by adding the limit parameter. Finally, we transform\nBusiness logic\nData layer\n(adapter)\nOrdersRepository\n+ add(payload): Order\n+ get(id): Order\n+ list(): List[Order]\n+ delete(): None\nOrder\n+ id: uuid\n+ created: datetime\n+ items: list\n+ status: str\nPostgreSQL\nMySQL\nMongoDB\nFigure 7.10\nThe repository pattern encapsulates the implementation details of the persistent storage \ntechnology used to manage our data. Our business layer only ever deals with the repository, and therefore \nwe are free to change our persistent storage solution to a different technology without affecting our core \napplication implementation.",
      "content_length": 2385,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "161\n7.4\nImplementing the repository pattern for data access\nthe database records into Order objects for consumption by the business layer by\nusing the dict() method we implemented in listing 7.2.\n The repository implementation is tightly coupled to the methods of SQLAlchemy’s\nSession object, but it also encapsulates these details, and to the business layer the\nrepository appears as an interface to which we submit IDs and payloads, and we get\nOrder objects in return. This is the point of the repository: to encapsulate and hide\nthe implementation details of the data layer from the business layer. This means that if\nwe switch to a different ORM framework, or to a different database system, we only\nneed to make changes to the repository.\n# file: orders/repository/orders_repository.py\nfrom orders.orders_service.orders import Order\nfrom orders.repository.models import OrderModel, OrderItemModel\nclass OrdersRepository:\n    def __init__(self, session):   \n        self.session = session\n    def add(self, items):\n        record = OrderModel(\n            items=[OrderItemModel(**item) for item in items]\n        )   \n        self.session.add(record)    \n        return Order(**record.dict(), order_=record)   \n    def _get(self, id_):    \n        return (\n            self.session.query(OrderModel)\n            .filter(OrderModel.id == str(id_))\n            .filter_by(**filters)\n            .first()\n        )   \n    def get(self, id_):\n        order = self._get(id_)   \n        if order is not None:       \n            return Order(**order.dict())\n    def list(self, limit=None, **filters):     \n        query = self.session.query(OrderModel)     \n        if 'cancelled' in filters:   \n            cancelled = filters.pop('cancelled')\n            if cancelled:\n                query = query.filter(OrderModel.status == 'cancelled')\n            else:\n                query = query.filter(OrderModel.status != 'cancelled')\n        records = query.filter_by(**filters).limit(limit).all()\n        return [Order(**record.dict()) for record in records]  \nListing 7.3\nOrders repository\nThe repository’s initializer \nmethod requires a session \nobject.\nWhen creating\na record for an\norder, we also\ncreate a record\nfor each item\nin the order.\nWe add the \nrecord to the \nsession object.\nWe return an \ninstance of the \nOrder class.\nGeneric method to \nretrieve a record by ID\nWe fetch the\nrecord using\nSQLAlchemy’s\nfirst() method.\nWe retrieve a record \nusing _get().\nIf the order exists, we \nreturn an Order object.\nlist() accepts a limit parameter \nand other optional filters.\nWe build\nour query\ndynamically.\nWe filter by whether an \norder is cancelled using the \nSQLAlchemy’s filter() method.\nWe return a\nlist of Order\nobjects.",
      "content_length": 2723,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "162\nCHAPTER 7\nService implementation patterns for microservices\n    def update(self, id_, **payload):\n        record = self._get(id_)\n        if 'items' in payload:     \n            for item in record.items:\n                self.session.delete(item)\n            record.items = [\n                OrderItemModel(**item) for item in payload.pop('items')\n            ]\n        for key, value in payload.items():    \n            setattr(record, key, value)\n        return Order(**record.dict())\n    def delete(self, id_):\n        self.session.delete(self._get(id_))    \nThis completes the implementation of our data layer. We have implemented a per-\nsistent storage solution with the help of SQLAlchemy, and we have encapsulated the\ndetails of this solution with the help of the repository pattern. It’s now time to work on\nthe business layer and see how it will interact with the repository!\n7.5\nImplementing the business layer\nWe’ve done a lot of work designing the database models for the orders service and\nusing the repository pattern to build the interface to the data. It’s now time to focus\non the business layer! In this section, we’ll implement the business layer of the\norders service. That’s the core of the hexagon we introduced in section 7.1 and\nillustrated in figure 7.1, which is reproduced here as figure 7.11 for your conve-\nnience. The business layer implements the service’s capabilities. What are the business\ncapabilities of the orders service? From the analysis in chapter 3 (section 3.4.2), we\nknow that the orders service allows users of the platform to place their orders and\nmanage them.\nAs illustrated in figure 7.12, the orders service manages the life cycle of an order through\nintegrations with other services. The following list describes the capabilities of the\nTo update an order, we first delete the \nitems linked to the order and then create \nnew items from the supplied payload.\nWe dynamically update the \ndatabase object using the \nsetattr() function.\nTo delete a record, \nwe call SQLAlchemy’s \ndelete() method.\nBusiness logic\nWeb API interface\n(adapter)\nData layer\n(adapter)\nFigure 7.11\nIn hexagonal architecture, we distinguish a core layer in our \napplication, the business layer, which implements the service’s \ncapabilities. Other components, such as a web API interface or a database, \nare considered adapters that depend on the business layer.",
      "content_length": 2384,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "163\n7.5\nImplementing the business layer\norders service and highlights integrations with other services (refer to figure 7.9 for\nfurther clarification):\nPlace orders—Creates a record of an order in the system. The order won’t be\nscheduled in the kitchen until the user pays for it.\nProcess payments—Processes payment for an order with the help of the payments\nservice. If the payments service confirms the payment is successful, the orders\nservice schedules the order for production with the kitchen service.\nUpdate orders—Users can update their order any time to add or remove items\nfrom it. To confirm a change, a new payment must be made and processed with\nthe help of the payments service.\nCancel orders—Users can cancel their orders anytime. Depending on the status\nof the order, the orders service will communicate with the kitchen or the deliv-\nery service to cancel the order.\nSchedule order for production in the kitchen—After payment, the orders service\nschedules the order for production in the kitchen with the help of the kitchen\nservice.\nKeep track of orders’ progress—Users can keep track of their orders’ status through\nthe orders service. Depending on the status of the order, the orders service\nchecks with the kitchen or the delivery service to get updated information\nabout the state of the order.\nWhat’s the best way to model these actions in our business layer? We can use different\napproaches, but to make it easy for other components to interact with the business\nlayer, we’ll expose a single unified interface through a class called OrdersService.\nWe’ll define this class under orders/orders_service/orders_service.py. To fulfill its\nduties, OrdersService uses the orders repository to interface with the database. We\nPayments service\nKitchen service\nDelivery service\nOrders service\nOrdersService\n+ pay()\n+ cancel()\n+ schedule()\n+ status()\nFigure 7.12\nIn order to perform some of its functions, the orders service needs \nto interact with orders services. For example, to process payments, it must \ninteract with the payments service, and to schedule an order for production, it \nmust interact with the kitchen service.",
      "content_length": 2148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "164\nCHAPTER 7\nService implementation patterns for microservices\ncould let OrdersService import and initialize the orders repository as in the follow-\ning code:\nfrom repository.orders_repository import OrdersRepository\nclass OrdersService:\n    def __init__(self):\n        self.repository = OrdersRepository()\nHowever, doing this would place too much responsibility on the orders service since it\nwould need to know how to configure the orders repository. It would also tightly couple\nthe implementation of the orders repository and the orders service, and we wouldn’t be\nable to use different repositories if we needed to. As you can see in figures 7.13 and\n7.14, a better approach is to use dependency injection in combination with the inver-\nsion of control principle.\nBusiness logic\nData layer package\n(adapter)\nAPI layer package\n(adapter)\nOrdersRepository\nOrdersService\nInstantiates\nand\nconﬁgures\nInstantiates and\nconﬁgures\nInstantiates and\nconﬁgures\nFigure 7.13\nIn conventional software design, dependencies follow a linear \nrelationship, and each component is responsible for instantiating and configuring \nits own dependencies. In many cases, this couples our components to low-level \nimplementation details in their dependencies.\nBusiness logic\nData layer package\n(adapter)\nAPI layer package\n(adapter)\nOrdersRepository\nOrdersService\nInstantiates and\nconﬁgures\nInjection\nInjection\nInstantiates and\nconﬁgures\nInstantiates and\nconﬁgures\nFigure 7.14\nWith inversion of control, we decouple components from their dependencies by supplying \nthem at runtime using methods such as dependency injection. In this approach, it’s the responsibility \nof the context to provide correctly configured instances of the dependencies. The solid lines show \nrelationships of dependency, while the dotted lines show how dependencies are injected.",
      "content_length": 1831,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "165\n7.5\nImplementing the business layer\nDEFINITION\nInversion of control is a software development principle that encour-\nages us to decouple our components from their dependencies by supplying\nthem at runtime. This allows us to control how the dependencies are sup-\nplied. One popular pattern to accomplish this is dependency injection. The\ncontext in which the dependencies are instantiated and supplied is called an\ninversion of control container. In the orders service, a suitable inversion of con-\ntrol container is the request object since most operations are specific to the\ncontext of a request.\nThe inversion of control principle states that we should decouple the dependencies in\nour code by letting the execution context supply those dependencies at runtime. This\nmeans that, instead of letting the orders service import and instantiate the orders\nrepository, we should supply the repository at runtime. How do we do that? We can\nuse different patterns to supply dependencies to our code, but one of the most popu-\nlar, due to its simplicity and effectiveness, is dependency injection.\nDEFINITION\nDependency injection is a software development pattern whereby we\nsupply code dependencies at runtime. This helps us decouple our components\nfrom the specific implementation details of the code they depend on, since\nthey don’t need to know how to configure and instantiate their dependencies.\nTo make the orders repository injectable into the orders service, we parameterize it:\nclass OrdersService:\n    def __init__(self, orders_repository):\n        self.orders_repository = orders_repository\nIt’s now the responsibility of the caller to instantiate and configure the orders repos-\nitory correctly. As you can see in figure 7.11, this has a very desirable outcome:\ndepending on the context, we can supply different implementations of the reposi-\ntory or add different configurations. This makes the orders service easier to use in\ndifferent contexts.7\n Listing 7.4 shows the interface exposed by OrdersService. The class initializer\ntakes an instance of the orders repository as a parameter to make it injectable. As per\nthe inversion of control principle, when we integrate OrdersService with the API\nlayer, it will be the responsibility of the API to get a valid instance of the orders reposi-\ntory and pass it to OrdersService. This approach is convenient, since it allows us to\nswap repositories at will when necessary, and it’ll make it very easy to write our tests in\nthe next chapter.\n \n \n \n7 For more details on the inversion of control principle and the dependency injection pattern, see Martin\nFowler, “Inversion of Control Containers and the Dependency Injection pattern,” https://martinfowler.com/\narticles/injection.html.",
      "content_length": 2742,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "166\nCHAPTER 7\nService implementation patterns for microservices\n# file: orders/orders_service/orders_service.py\nclass OrdersService:\n    def __init__(self, orders_repository):\n        self.orders_repository = orders_repository \n    def place_order(self, items):\n        pass\n    def get_order(self, order_id):\n        pass\n    def update_order(self, order_id, items):\n        pass\n    def list_orders(self, **filters):\n        pass\n    def pay_order(self, order_id):\n        pass\n    def cancel_order(self, order_id):\n        pass\nSome of the actions listed under OrdersService, such as payment or scheduling, take\nplace at the level of individual orders. Since orders contain data, it will be useful to\nhave a class that represents orders and has methods to perform tasks related to an\norder. Within the context of the orders service, an order is a core object of the orders\ndomain. In domain-driven design (DDD), we call these objects domain objects. These\nare the objects returned by the orders repository. We’ll implement our Order class\nunder orders/orders_service/orders.py. Listing 7.5 shows a preliminary implementa-\ntion of the Order class.\n In addition to the Order class, listing 7.5 also provides an OrderItem class that rep-\nresents each of the items in an order. We’ll use the Order class to represent orders\nbefore and after saving them to the database. Some of the properties of an order, such\nas the creation time or its ID, are set by the data layer and can be known only after the\nchanges to the database have been committed. As we explained in section 7.4, com-\nmitting changes is out of the scope of a repository, which means that when we add an\norder to the repository, the returned object won’t have those properties. The order’s\nID and its creation time become available through the order’s database record after\ncommitting the transaction. For this reason, Order’s initializer binds the order’s ID,\ncreation time, and status as private properties with a leading underscore (like in\nself._id), and we use the order_ parameter in the Order class to hold a pointer to\nthe order’s database record. If we retrieve the details of an order already saved to the\ndatabase, _id, _created, and _status will have their corresponding values in the ini-\ntializer; otherwise, they’ll be None and we’ll pull their values from order_. That’s why\nwe define Order’s id, created, and status properties using the property() decorator,\nListing 7.4\nInterface of the OrdersService class",
      "content_length": 2487,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "167\n7.5\nImplementing the business layer\nsince it allows us to resolve their value depending on the state of the object. This is the\nonly degree of coupling we’ll allow between the business layer and the data layer. And\nto make sure this dependency can be easily removed if we ever have to, we’re setting\norder_ to None by default.\n# file: orders/orders_service/orders.py\nclass OrderItem:    \n    def __init__(self, id, product, quantity, size):     \n        self.id = id\n        self.product = product\n        self.quantity = quantity\n        self.size = size\nclass Order:\n    def __init__(self, id, created, items, status, schedule_id=None,\n                 delivery_id=None, order_=None):     \n        self._id = id   \n        self._created = created\n        self.items = [OrderItem(**item) for item in items]   \n        self._status = status\n        self.schedule_id = schedule_id\n        self.delivery_id = delivery_id\n    @property\n    def id(self):         \n        return self._id or self._order.id\n    @property\n    def created(self):       \n        return self._created or self._order.created\n    @property\n    def status(self):       \n        return self._status or self._order.status\nIn addition to holding data about an order, the Order class also needs to handle\ntasks such as cancelling, paying, and scheduling an order. To fulfill those tasks, we\nmust interface with external dependencies, such as the kitchen and payments ser-\nvices. As we explained in section 7.1, the goal of hexagonal architecture is to encap-\nsulate access to external dependencies through adapters. However, to keep things\nsimple in this chapter, we’ll implement the external API calls within the Order class.\nA good adapter pattern for encapsulating external API calls is the facade pattern.8\nListing 7.5\nImplementation of the Order business object class\n8 Erich Gamma, Richard Helm, Ralph Johnsohn, and John Vlissides, Design Patterns (Addison-Wesley, 1995), pp.\n185–193.\nBusiness object that \nrepresents an order item\nWe declare the \nparameters of \nOrderItem’s \ninitializer method.\nThe order_ \nparameter \nrepresents a \ndatabase model \ninstance.\nSince we resolve the ID dynamically, we store \nthe provided ID as a private property.\nWe build an\nOrderItem\nobject for each\norder item.\nWe resolve the ID \ndynamically using the \nproperty() decorator.",
      "content_length": 2335,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "168\nCHAPTER 7\nService implementation patterns for microservices\nBefore we proceed with the implementation, we should know what those API calls\nlook like.\n To build the integration between the orders service and the kitchen and payments\nservices, we’d want to run the kitchen and payments services and see how they work.\nHowever, we don’t need to run the actual services. The folder for this chapter in the\nGitHub repository for this book contains three OpenAPI files: one for the orders API\n(ch07/oas.yaml), one for the kitchen API (ch07/kitchen.yaml), and one for the pay-\nments API (ch07/payments.yaml). kitchen.yaml and payments.yaml tell us how the\nkitchen and payments APIs work, and that’s all the information we need to build our\nintegration. Make sure to pull the kitchen.yaml and payments.yaml files from GitHub\nto be able to work with the following examples.\n As it turns out, we can also use the kitchen and payments API specifications to sim-\nulate their behavior using mock servers. API mock servers replicate the server behind\nthe APIs, validating our requests and returning valid responses. We’ll use Prism CLI\n(https://github.com/stoplightio/prism), a library built and maintained by Stoplight,\nto mock the API server for the kitchen and payments services. Prism is a Node.js\nlibrary, but don’t worry, it’s just a CLI tool; you don’t need to know any JavaScript to\nuse it. To install the library, run the following command:\n$ yarn add @stoplight/prism-cli\nDEALING WITH ERRORS RUNNING PRISM\nYou may run into errors when running\nPrism. A common error is not having a compatible version of Node.js. I rec-\nommend you install nvm to manage your Node versions and use the latest sta-\nble version of Node to run Prism. Also, make sure the port you select to run\nPrism is available.\nThis command will create a node_modules/ folder within your application folder,\nwhere Prism and all its dependencies will be installed. You don’t want to commit this\nfolder, so make sure you add it to your .gitignore file. You’ll also see a new file called\npackage.json, and another one called yarn.lock within your application directory.\nThese are the files you want to commit since they’ll allow you to re-create the same\nnode_modules/ directory in any other environment.\n To see Prism in action with the kitchen API, run the following command:\n$ ./node_modules/.bin/prism mock kitchen.yaml --port 3000\nThis will start a server on port 3000 that runs a mock service for the kitchen API. To\nget a taste of what we can do with it, run the following command to hit the GET\n/kitchen/schedules endpoint, which returns a list of schedules:\n$ curl http:/ /localhost:3000/kitchen/schedules",
      "content_length": 2675,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "169\n7.5\nImplementing the business layer\nDISPLAY JSON IN THE TERMINAL LIKE A PRO WITH JQ\nWhen outputting JSON to\nthe terminal, either using cURL to interact with an API or catting a JSON file,\nI recommend you use JQ—a command-line utility that parses the JSON and\nproduces a beautiful display. You can use JQ like this: curl http:/ /localhost:\n3000/kitchen/schedules | jq.\nYou’ll see that the mock server started by Prism is able to return a perfectly valid pay-\nload representing a list of schedules. Impressive, to say the least! Now that we know\nhow to run mock servers for the kitchen and payments APIs, let’s analyze the require-\nments of the API integrations with them:\nKitchen service (kitchen.yaml)—To schedule an order with the kitchen service, we\nmust call the POST /kitchen/schedules endpoint with a payload containing\nthe list of items in the order. In the response to this call, we’ll find the sched-\nule_id, which we can use to keep track of the state of the order.\nPayments service (payments.yaml)—To process the payment for an order, we must\ncall the POST /payments endpoint with a payload containing the ID of the\norder. This is a mock endpoint for integration testing purposes.\nBefore we can cancel an order, we need to check its status. If the order is scheduled\nfor production, we must hit the POST /kitchen/schedules/{schedule_id}/cancel\nendpoint to cancel the schedule. If the order is out for delivery, we won’t allow users\nto cancel the order, and therefore we raise an exception.\n To implement the API integrations, we’ll use the popular Python requests library.\nRun the following command to install the library with pipenv:\n$ pipenv install requests\nListing 7.6 extends the implementation of the Order class by adding methods that\nimplement API calls to the kitchen and payment services. For testing purposes, we’re\nexpecting the kitchen API to run on port 3001 and the payments service to run on\nport 3000. You can accomplish this by running the following commands:\n$ ./node_modules/.bin/prism mock kitchen.yaml --port 3000\n$ ./node_modules/.bin/prism mock payments.yaml --port 3001\nIn each API call, we check that the response contains the expected status code, and if\nit doesn’t, we raise a custom APIIntegrationError exception. Also, if a user tries to\nperform an invalid action, such as cancelling an order when it’s already out for deliv-\nery, we raise an InvalidActionError exception.\n# file: orders/orders_service/orders.py\nimport requests\nListing 7.6\nEncapsulating per-order capabilities within the Order class",
      "content_length": 2546,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "170\nCHAPTER 7\nService implementation patterns for microservices\nfrom orders.orders_service.exceptions import (\n    APIIntegrationError, InvalidActionError\n)\n...\nclass Order:\n  ...\n    def cancel(self):\n        if self.status == 'progress':    \n            kitchen_base_url = \"http:/ /localhost:3000/kitchen\"\n            response = requests.post(\n                f\"{kitchen_base_url}/schedules/{self.schedule_id}/cancel\",\n                json={\"order\": [item.dict() for item in self.items]},\n            )\n               if response.status_code == 200:    \n                return\n            raise APIIntegrationError(     \n                f'Could not cancel order with id {self.id}'\n            )\n        if self.status == 'delivery':    \n            raise InvalidActionError(\n                f'Cannot cancel order with id {self.id}'\n            )\n    def pay(self):\n        response = requests.post(      \n            'http:/ /localhost:3001/payments', json={'order_id': self.id}\n        )\n        if response.status_code == 201:\n            return\n        raise APIIntegrationError(\n            f'Could not process payment for order with id {self.id}'\n        )\n    def schedule(self):\n        response = requests.post(         \n            'http:/ /localhost:3000/kitchen/schedules',\n            json={'order': [item.dict() for item in self.items]}\n        )\n        if response.status_code == 201:     \n            return response.json()['id']\n        raise APIIntegrationError(\n            f'Could not schedule order with id {self.id}'\n        )\nListing 7.7 contains the implementation of the custom exceptions we use in the order\nservice to signal that something has gone wrong. We’ll use OrderNotFoundError in\nthe OrdersService class when a user tries to fetch the details of an order that doesn’t\nexist.\n \n \nIf an order is in progress, \nwe cancel its schedule by \ncalling the kitchen API.\nIf the response from \nthe kitchen service is \nsuccessful, we return.\nOtherwise,\nwe raise an\nAPIIntegrationError.\nWe don’t allow orders that are \nbeing delivered to be cancelled.\nWe process a \npayment by calling \nthe payments API.\nWe schedule an order for \nproduction by calling the \nkitchen API.\nIf the response from \nthe kitchen service is \nsuccessful, we return \nthe schedule ID.",
      "content_length": 2278,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "171\n7.5\nImplementing the business layer\n# file: orders/orders_service/exceptions.py\nclass OrderNotFoundError(Exception):   \n    pass\nclass APIIntegrationError(Exception):     \n    pass\nclass InvalidActionError(Exception):   \n    pass\nAs we mentioned earlier, the API module won’t use the Order class directly. Instead, it\nwill use a unified interface to all our adapters through the OrdersService class, whose\ninterface we showed in listing 7.4. OrdersService encapsulates the capabilities of the\norders domain, and it takes care of using the orders repository to get orders objects\nand perform actions on them. Listing 7.8 shows the implementation of the Orders-\nService class.\n To instantiate the OrdersService class, we require an orders repository object that\nwe can use to add or delete orders from our records. To place an order, we create a\ndatabase record using the orders repository, and to retrieve the details of an order, we\nfetch the corresponding record from the database. If the requested order isn’t found,\nwe raise an OrderNotFoundError exception. The list_orders() method accepts fil-\nters in the form of a dictionary. To get a list of orders, the orders repository forces us\nto pass a specific value for the limit argument, and therefore we extract its value from\nthe filters dictionary by using the pop() method, which allows us to set a default\nvalue and also removes the key from the dictionary. In the pay_order() method, we\nprocess the payment using the payments API, and if the payment is successful, we\nschedule the order by calling the kitchen API. After scheduling the order, we update\nthe order record by setting its schedule_id attribute to the schedule ID returned by\nthe kitchen API.\n# file: orders/orders_service/orders_service.py\nfrom orders.orders_service.exceptions import OrderNotFoundError\nclass OrdersService:\n    def __init__(self, orders_repository):    \n        self.orders_repository = orders_repository\n    def place_order(self, items):\n        return self.orders_repository.add(items)   \nListing 7.7\nOrders service custom exceptions\nListing 7.8\nImplementation of the OrdersService\nException to signal that \nan order doesn’t exist\nException to signal that an API \nintegration error has taken place\nException to signal that the \naction being performed is invalid\nTo instantiate the OrdersService \nclass, we require an instance of \nthe orders repository.\nWe place an order by \ncreating a database \nrecord.",
      "content_length": 2447,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "172\nCHAPTER 7\nService implementation patterns for microservices\n   \n    def get_order(self, order_id):\n        order = self.orders_repository.get(order_id)    \n        if order is not None:     \n            return order\n        raise OrderNotFoundError(f'Order with id {order_id} not found')\n    def update_order(self, order_id, items):\n        order = self.orders_repository.get(order_id)\n        if order is None:\n            raise OrderNotFoundError(f'Order with id {order_id} not found')\n        return self.orders_repository.update(order_id, {'items': items})\n    def list_orders(self, **filters):\n        limit = filters.pop('limit', None)    \n        return self.orders_repository.list(limit, **filters)\n    def pay_order(self, order_id):\n        order = self.orders_repository.get(order_id)\n        if order is None:\n            raise OrderNotFoundError(f'Order with id {order_id} not found')\n        order.pay()\n        schedule_id = order.schedule()    \n        return self.orders_repository.update(\n            order_id, {'status': 'scheduled', 'schedule_id': schedule_id}\n        )\n    def cancel_order(self, order_id):\n        order = self.orders_repository.get(order_id)\n        if order is None:\n            raise OrderNotFoundError(f'Order with id {order_id} not found')\n        order.cancel()\n        return self.orders_repository.update(order_id, status=\"cancelled\")\nThe orders service is ready to be used in our API module. However, before we con-\ntinue with this integration, there’s one more piece in this puzzle that we need to solve.\nAs we mentioned in section 7.4, the orders repository doesn’t commit any actions to\nthe database. It’s the responsibility of the API, as the consumer of the OrdersService,\nto ensure that everything is committed at the end of an operation. How exactly does\nthat work? Move on to section 7.6 to learn how!\n7.6\nImplementing the unit of work pattern\nIn this section, we’ll learn to handle database commits and rollbacks when interacting\nwith the OrdersService. As you can see in figure 7.15, when we use the OrdersService\nclass to access any of its capabilities, we must inject an instance of the Orders-\nRepository class. We must also open an SQLAlchemy session before we perform any\nactions, and we must commit any changes to our data to persist them in the database.\nWe get the details of an order using the orders \nrepository and passing in the requested ID.\nIf the order doesn’t \nexist, we raise an \nOrderNotFoundError \nexception.\nWe capture filters as a dictionary \nby using keyword arguments.\nAfter scheduling the order, we \nupdate its schedule_id attribute.",
      "content_length": 2618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "173\n7.6\nImplementing the unit of work pattern\nWhat’s the best way to orchestrate these operations? We can use different approaches\nfor this implementation. We could simply use SQLAlchemy session objects to wrap\nour calls to OrdersService, and once our operations succeed, use the session to com-\nmit, or roll back otherwise. This would work if OrdersService only ever had to deal\nwith a single SQL database. However, what if we had to interact with a different type of\ndatabase at the same time? We’d need to open a new session for it as well. What if we\nalso had to handle integrations with other microservices within the same operation,\nand ensure we make the right API calls at the end of the transaction in case we had to\nroll back? Again, we could just add special clauses and guards to our code. The same\ncode would have to be repeated in every API function that interacts with the Orders-\nService, so wouldn’t it be nice if there was pattern that can help us put it all together\nin a single place? Enter the unit of work pattern.\nDEFINITION\nThe unit of work is a design pattern that guarantees the atomicity\nof our business transactions, ensuring that all transactions are committed at\nonce, or rolled back if any of them fails.\nBusiness logic\nData layer\n(adapter)\nOrdersRepository\n+ add(payload): Order\nOrdersService\n+ place_order()\n2. The API layer injects\nthe SQLAlchemy\nsession into the\norders repository.\n3. The API layer instantiates the orders\nrepository and injects it into\nOrdersService.\nSQLAlchemy session\n1. The API layer\ncreates an\nSQLAlchemy\nsession.\ncommit()\n5. The API layer commits the\nchanges to the database.\nWeb API interface\n(adapter)\nWeb API interface\n(adapter)\n4. The AP layer calls OrdersService’s\nplace_order() method.\nFigure 7.15\nTo persist our changes to the database, we could simply make the API layer use the \nSQLAlchemy session object to commit the transaction. In this figure, the solid lines represent calls, \nwhile the dashed lines represent injections of dependencies.",
      "content_length": 2009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "174\nCHAPTER 7\nService implementation patterns for microservices\nThe unit of work is a pattern that ensures that all objects of a business transaction are\nchanged together, and if something fails, it ensures none of them changes.9 The\nnotion comes from the world of databases, where database transactions are imple-\nmented as units of work which ensure that every transaction is\nAtomic—The whole transaction either succeeds or fails.\nConsistent—It conforms to the constrains of the database.\nIsolated—It doesn’t interfere with other transactions.\nDurable—It’s written to persistent storage.\nThese properties are known as the ACID principles in the world of databases\n(https://en.wikipedia.org/wiki/Database_transaction). When it comes to services, the\nunit of work pattern helps us apply these principles in our operations. SQLAlchemy’s\nSession object already implements the unit of work pattern for database transactions\n(http://mng.bz/jA5z). This means that we can add as many changes as we need to the\nsame session and commit them all together. If something goes wrong, we can call the\nrollback method to undo any changes. In Python, we can orchestrate these steps with\ncontext managers.\n As you can see in figure 7.16, a context manager is a pattern that allows us to lock a\nresource during an operation, ensure that any necessary cleanup jobs are undertaken\n9 Fowler, Patterns of Enterprise Architecture (pp. 184–194). \nwith\nUnitOfWork()\nas\nunit_of_work:\nunit_of_work.commit()\nclass UnifOfWork:\ndef __enter__(self):\ndef __exit__(self):\nA with statement\nallows us to\nenter a context\nmanager.\nAn as statement allows us\nto bind the return value of\n__enter__() to a variable.\nThe __init__() method is\ntriggered on initializing\nthe UnitOfWork class.\nScope of context manager\nWhen we exit the scope\nof the context manager,\nthe __exit__() method\nis triggered.\ndef __init__(self):\nFigure 7.16\nA class-based context manager has an __init__(), an __enter__(), and an \n__exit__() method. __init__() is triggered when we initialize the context manager. The \n__enter__ method allows us to enter the context, and it’s called when we use the with \nstatement. Using an as statement within the same line allows us to bind the return value of \nthe __enter__() method to a variable (unit_of_work in this case). Finally, when we exit \nthe context manager, the __exit__() method is triggered.",
      "content_length": 2379,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "175\n7.6\nImplementing the unit of work pattern\nin case anything goes wrong, and finally release the lock once the operation is fin-\nished. The key syntactical feature of a context manager is the use of the with state-\nment, as illustrated in figure 7.16. As you can see in the illustration, context managers\ncan return objects, which we can capture by using Python’s as clause. This is useful if\nthe context manager is creating access to a resource, such as a file, on which we want\nto operate.\n In Python, we can implement context managers in multiple ways, including as a\nclass or using the contextmanager() decorator from the contextlib module.10 In this\nsection, we’ll implement our unit of work context manager as a class. A context man-\nager class must implement at least the two following special methods:\n\n__enter__()—Defines the operations that must be undertaken upon entering\nthe context, such as creating a session or opening a file. If we need to perform\nactions on any of the objects created within the __enter__() method, we can\nreturn the object and capture its value through an as clause, as illustrated in fig-\nure 7.16.\n\n__exit__()—Defines the operations that must be undertaken upon exiting the\ncontext, for example, closing a file or a session. The __exit__() method cap-\ntures any exceptions raised during the execution of the context through three\nparameters in its method signature:\n– exc_type—Captures the type of exception raised\n– exc_value—Captures the value bound to the exception, typically the error\nmessage\n– traceback—A traceback object that can be used to pinpoint the exact place\nwhere the exception took place\nIf no exceptions are raised, the value of these three parameters will be None.\n Listing 7.9 shows the implementation of the unit of work pattern as a context man-\nager for the orders service. In the initializer method, we obtain a session factory object\nusing SQLAlchemy’s sessionmaker() function, which requires a connection object\nthat we produce with the help of SQLAlchemy’s create_engine() function. To keep\nthe example simple, we’re hardcoding the database connection string to point to our\nlocal SQLite database. In chapter 13, you’ll learn to parameterize this value and pull it\nfrom the environment.\n When we enter the context, we create a new database session, and we bind it to the\nUnitOfWork instance so that we can access it in other methods. We also return the con-\ntext manager object itself so that the caller can access any of its attributes, such as the\nsession object or the commit() method. On exiting the context, we check whether any\nexceptions were raised while adding or removing objects to the session, and if that’s the\ncase, we roll back the changes to avoid leaving the database in an inconsistent state.\n10 Ramalho, Fluent Python (O’Reilly, 2015), pp. 463–478.",
      "content_length": 2836,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "176\nCHAPTER 7\nService implementation patterns for microservices\nWe have access to the exception’s type (exc_type) and value (exc_val), and the trace-\nback (traceback) context, which we can use to log the details of the error. If no\nexception took place, all three parameters will be set to None. Finally, we close the\ndatabase session to release database resources and to end the scope of the transaction.\nWe also add wrappers around SQLAlchemy’s commit() and rollback() methods to\navoid exposing database internals to the business layer.\n# file: orders/repository/unit_of_work.py\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nclass UnitOfWork:\n    def __init__(self):\n        self.session_maker = sessionmaker(    \n            bind=create_engine('sqlite:/ / /orders.db')\n        )\n    def __enter__(self):\n        self.session = self.session_maker()   \n        return self     \n    def __exit__(self, exc_type, exc_val, traceback):    \n        if exc_type is not None:    \n            self.rollback()    \n            self.session.close()   \n        self.session.close()\n    def commit(self):\n        self.session.commit()   \n    def rollback(self):\n        self.session.rollback()    \nThis is all very good, but how exactly are we supposed to use the UnitOfWork in combi-\nnation with the orders repository and the OrdersService? In the next section, we’ll\ndelve more into the details of this, but before we do that, listing 7.10 gives you a tem-\nplate for how to use all these components together. We enter the unit of work context\nwith Python’s syntax for context managers using a with statement. We also use an as\nstatement to bind the return value of UnitOfWork’s __enter__() method to the\nunit_of_work variable. Then we get an instance of the orders repository passing in\nthe UnitOfWork’s database session object, and an instance of the OrdersService class\npassing in the orders repository object. Then we use the orders service object to place\nan order, and we commit the transaction using UnitOfWork’s commit() method.\n \nListing 7.9\nUnit of work pattern as a context manager\nWe obtain a \nsession factory \nobject.\nWe open a \nnew database \nsession.\nWe return\nan instance\nof the unit of\nwork object.\nOn existing the \ncontext, we have \naccess to any \nexceptions raised \nduring the context’s \nexecution.\nWe check\nwhether an\nexception\ntook place.\nIf an exception \ntook place, \nroll back the \ntransaction.\nWe close the\ndatabase\nsession.\nWrapper around SQLAlchemy’s \ncommit() method\nWrapper around SQLAlchemy’s \nrollback() method",
      "content_length": 2559,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "177\n7.7\nIntegrating the API layer and the service layer\nwith UnitOfWork() as unit_of_work:    \n    repo = OrdersRepository(unit_of_work.session)  \n    orders_service = OrdersService(repo)       \n    orders_service.place_order(order_details)     \n    unit_of_work.commit()    \nNow that we have a unit of work that we can use to commit our transactions, let’s see\nhow we put this all together by integrating the API layer with the service layer! Move\non to section 7.7 to learn how we do that.\n7.7\nIntegrating the API layer and the service layer\nIn this section, we put everything we have learned in this chapter together to integrate\nthe service layer with the API layer. We’ll make use of the template pattern we showed\nin listing 7.10 to use the UnitOfWork class in combination with OrdersRepository and\nOrdersService. When a user tries to perform an action on an order, we make sure we\nhave checks in place to verify that the order exists in the first place; otherwise, we\nreturn a 404 (Not Found) error response.\n Listing 7.11 shows the new version of the orders/web/api/api.py module. The first\nthing we do in every function is enter the context of UnitOfWork, making sure we\nbind the context object to a variable, unit_of_work. Then we create an instance of\nOrdersRepository using the session object from the UnitOfWork context object.\nOnce we have an instance of the repository, we inject it into OrdersService as we cre-\nate an instance of the service. Then we use the service to perform the operations\nrequired in each endpoint. In endpoints that perform actions on a specific order, we\nguard against the possibility of an OrderNotFoundError being raised by OrdersService\nif the requested order doesn’t exist.\n In the create_order() function, we retrieve the dictionary representation of the\norder using order.dict() before we exit the UnitOfWork context so that we can\naccess properties generated by the database during the commit process, such as the\norder’s ID. Remember that the order ID doesn’t exist until the changes are commit-\nted to the database, and therefore it’s only accessible within the scope of the database\nsession. In our implementation, that means that we must access the ID before we exit\nthe UnitOfWork context, since the database session closes right before exiting the con-\ntext. Figure 7.17 illustrates this process.\n \n \nListing 7.10\nTemplate pattern for using the unit of work and the repository\nWe enter the \nunit of work \ncontext.\nWe get an instance of the orders repository \npassing in the UnitOfWork’s session.\nWe get an instance of the \nOrdersService class passing in \nthe orders repository object.\nWe place \nan order.\nWe commit the \ntransaction.",
      "content_length": 2685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "178\nCHAPTER 7\nService implementation patterns for microservices\n# file: orders/web/api/api.py\nfrom http import HTTPStatus\nfrom typing import List, Optional\nfrom uuid import UUID\nfrom fastapi import HTTPException\nfrom starlette import status\nfrom starlette.responses import Response\nfrom orders.orders_service.exceptions import OrderNotFoundError\nfrom orders.orders_service.orders_service import OrdersService\nfrom orders.repository.orders_repository import OrdersRepository\nfrom orders.repository.unit_of_work import UnitOfWork\nfrom orders.web.app import app\nfrom orders.web.api.schemas import (\n    GetOrderSchema,\nListing 7.11\nIntegration between API layer and service layer\nBusiness logic\nData layer\n(adapter)\nOrderModel\n+ id: uuid\n+ created: datetime\n+ items: list\n+ status: str\n+ dict(): dict\nOrdersRepository\n+ add(payload): Order\n+ get(id): Order\n+ list(): List[Order]\n+ delete(): None\nOrder\n+ id: UUID\n+ order_: OrderModel\n2. The business layer\ninterfaces with the orders\nrepository to add the\norder to the database\nsession.\n3. The orders repository returns\na business object with partial\ndata. Since we haven’t\ncommitted our transaction, the\norder’s ID isn’t available. To\nget access to the ID, we bind\nthe order_ property to an\ninstance of OrderModel.\nWeb API\ninterface\n(adapter)\ncommit()\n1. The API layer\ninterfaces with\nthe business\nlayer to place\nan order.\n4. Once we commit the\ntransaction, the order’s\nID is generated and\ninjected into the Order-\nModel instance. At that\npoint, the Order object\ncan pull the ID from\norder_id.\nID\nFigure 7.17\nWhen we place an order, the object returned by the orders repository doesn’t contain an ID. \nThe ID will be available once we commit the database transaction through the OrderModel instance. \nTherefore, we bind an instance of the model to the Order object so that it can pull the ID from the model \nafter the commit.",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "179\n7.7\nIntegrating the API layer and the service layer\n    CreateOrderSchema,\n    GetOrdersSchema,\n)\n@app.get('/orders', response_model=GetOrdersSchema)\ndef get_orders(\n    cancelled: Optional[bool] = None,\n    limit: Optional[int] = None,\n):\n    with UnitOfWork() as unit_of_work:       \n        repo = OrdersRepository(unit_of_work.session)\n        orders_service = OrdersService(repo)\n        results = orders_service.list_orders(\n            limit=limit, cancelled=cancelled\n        )\n    return {'orders': [result.dict() for result in results]}\n@app.post(\n    '/orders',\n    status_code=status.HTTP_201_CREATED,\n    response_model=GetOrderSchema,\n)\ndef create_order(payload: CreateOrderSchema):\n    with UnitOfWork() as unit_of_work:\n        repo = OrdersRepository(unit_of_work.session)\n        orders_service = OrdersService(repo)\n        order = orders_service.place_order(payload.dict()['order'])\n        order = payload.dict()['order']\n        for item in order:\n            item['size'] = item['size'].value\n        order = orders_service.place_order(order)     \n        unit_of_work.commit()\n        return_payload = order.dict()    \n    return return_payload\n@app.get('/orders/{order_id}', response_model=GetOrderSchema)\ndef get_order(order_id: UUID):\n    try:         \n        with UnitOfWork() as unit_of_work:\n            repo = OrdersRepository(unit_of_work.session)\n            orders_service = OrdersService(repo)\n            order = orders_service.get_order(order_id=order_id)\n        return order.dict()\n    except OrderNotFoundError:\n        raise HTTPException(\n            status_code=404, detail=f'Order with ID {order_id} not found'\n        )\n@app.put('/orders/{order_id}', response_model=GetOrderSchema)\ndef update_order(order_id: UUID, order_details: CreateOrderSchema):\nWe enter the \nunit of work \ncontext.\nWe place \nan order.\nWe access the order’s dictionary \nrepresentation before exiting the \nunit of work context.\nWe use a \ntry/except block \nto catch the \nOrderNotFound-\nError exception.",
      "content_length": 2021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "180\nCHAPTER 7\nService implementation patterns for microservices\n    try:\n        with UnitOfWork() as unit_of_work:\n            repo = OrdersRepository(unit_of_work.session)\n            orders_service = OrdersService(repo)\n            order = order_details.dict()['order']\n            for item in order:\n                item['size'] = item['size'].value\n            order = orders_service.update_order(\n                order_id=order_id, items=order\n            )\n            unit_of_work.commit()\n        return order.dict()\n    except OrderNotFoundError:\n        raise HTTPException(\n            status_code=404, detail=f'Order with ID {order_id} not found'\n        )\n@app.delete(\n    \"/orders/{order_id}\",\n    status_code=status.HTTP_204_NO_CONTENT,\n    response_class=Response,\n)\ndef delete_order(order_id: UUID):\n    try:\n        with UnitOfWork() as unit_of_work:\n            repo = OrdersRepository(unit_of_work.session)\n            orders_service = OrdersService(repo)\n            orders_service.delete_order(order_id=order_id)\n            unit_of_work.commit()\n        return\n    except OrderNotFoundError:\n        raise HTTPException(\n            status_code=404, detail=f'Order with ID {order_id} not found'\n        )\n@app.post('/orders/{order_id}/cancel', response_model=GetOrderSchema)\ndef cancel_order(order_id: UUID):\n    try:\n        with UnitOfWork() as unit_of_work:\n            repo = OrdersRepository(unit_of_work.session)\n            orders_service = OrdersService(repo)\n            order = orders_service.cancel_order(order_id=order_id)\n            unit_of_work.commit()\n        return order.dict()\n    except OrderNotFoundError:\n        raise HTTPException(\n            status_code=404, detail=f'Order with ID {order_id} not found'\n        )\n@app.post('/orders/{order_id}/pay', response_model=GetOrderSchema)\ndef pay_order(order_id: UUID):",
      "content_length": 1862,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "181\nSummary\n    try:\n        with UnitOfWork() as unit_of_work:\n            repo = OrdersRepository(unit_of_work.session)\n            orders_service = OrdersService(repo)\n            order = orders_service.pay_order(order_id=order_id)\n            unit_of_work.commit()\n        return order.dict()\n    except OrderNotFoundError:\n        raise HTTPException(\n            status_code=404, detail=f'Order with ID {order_id} not found'\n        )\nThis concludes our journey through the implementation of the service layer for the\norders service. The patterns we learned in this chapter are not only applicable to the\nworld of APIs and microservices, but to all application models generally. In particu-\nlar, the repository pattern will always help you ensure that you keep your data access\nlayer fully decoupled from the business layer, and the unit of work pattern will help\nyou ensure that all transactions of a business operation are handled atomically and\nconsistently.\nSummary\nHexagonal architecture, or architecture of ports and adapters, is a software\narchitectural pattern that encourages us to decouple the business layer from\nthe implementation details of the database and the application interface.\nThe dependency inversion principle teaches us that the implementation details of\nour application components should depend on interfaces. This helps us decouple\nour components from the implementation details of their dependencies.\nTo interface with the database, you can use an ORM library such as SQLAlchemy,\nwhich can translate database tables and rows into classes and objects. This pro-\nvides the possibility of enhancing our database models with useful functionality\nfor our application needs.\nRepository is a software development pattern that helps to decouple the data\nlayer from the business layer by adding an abstraction layer, which exposes an\nin-memory list interface of the data. Regardless of the database engine we use,\nthe business layer will always receive the same objects from the repository.\nThe unit of work pattern helps ensure that all the business transactions that are\npart of an application operation succeed or fail together. If one of the transac-\ntions fails, the unit of work pattern ensures that all changes are rolled back.\nThis mechanism ensures that data is never left in an inconsistent state.",
      "content_length": 2336,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "Part 3\nDesigning and\nbuilding GraphQL APIs\nIn part 2, you learned that REST is an API technology that allows us to alter\nor retrieve the state of a resource from the server. When a resource is repre-\nsented by a large payload, fetching it from the server translates to a large\namount of data transfer. With the emergence of API clients running in mobile\ndevices with restricted network access and limited storage and memory capacity,\nexchanging large payloads often results in unreliable communication. In 2012,\nFacebook was acutely aware of these problems, and it developed a new technol-\nogy to allow API clients to run granular data queries on the server. This technol-\nogy was released in 2015 under the name of GraphQL.\n GraphQL is a query language for APIs. Instead of fetching full representa-\ntions of resources, GraphQL allows you to fetch one or more properties of a\nresource, such as the price of a product or the status of an order. With GraphQL,\nwe can also model the relationship between different objects, which allows us to\nretrieve, in a single request, the properties of various resources from the server,\nsuch as a product’s ingredients and its stock availability.\n Despite its benefits, many developers aren’t familiar with GraphQL or don’t\nknow how it works, and therefore it isn’t usually the first choice of technology\nfor building an API. In part 3, you learn everything you need to know to design\nand build high-quality GraphQL APIs and how to consume them. After reading\npart 3, you’ll know what GraphQL is, how it works, and when to use it so that you\ncan make better decisions in your API strategy.",
      "content_length": 1626,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "185\nDesigning\nGraphQL APIs\nGraphQL is one of the most popular protocols for building web APIs. It’s a suitable\nchoice for driving integrations between microservices and for building integrations\nwith frontend applications. GraphQL gives API consumers full control over the\ndata they want to fetch from the server and how they want to fetch it.\n In this chapter, you’ll learn to design a GraphQL API. You’ll do it by working on\na practical example: you’ll design a GraphQL API for the products service of the\nCoffeeMesh platform. The products service owns data about CoffeeMesh’s prod-\nucts as well as their ingredients. Each product and ingredient contains a rich list of\nproperties that describe their features. However, when a client requests a list of\nproducts, they are most likely interested in fetching only a few details about each\nThis chapter covers\nUnderstanding how GraphQL works\nProducing an API specification using the Schema \nDefinition Language (SDL)\nLearning GraphQL’s built-in scalar types and data \nstructures and building custom object types\nCreating meaningful connections between \nGraphQL types\nDesigning GraphQL queries and mutations",
      "content_length": 1160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "186\nCHAPTER 8\nDesigning GraphQL APIs\nproduct. Also, clients may be interested in being able to traverse the relationships\nbetween products, ingredients, and other objects owned by the products service. For\nthese reasons, GraphQL is an excellent choice for building the products API.\n As we build the specification for the products API, you’ll learn about GraphQL’s\nscalar types, designing custom object types, as well as queries and mutations. By the\nend of this chapter, you’ll understand how GraphQL compares with other types of\nAPIs and when it makes the most sense to use it. We’ve got a lot to cover, so without\nfurther ado, let’s start our journey!\n To follow along with the specification we develop in this chapter, you can use the\nGitHub repository provided with this book. The code for this chapter is available\nunder the folder named ch08.\n8.1\nIntroducing GraphQL\nThis section covers what GraphQL is, what its advantages are, and when it makes sense\nto use it. The official website of the GraphQL specification defines GraphQL as a\n“query language for APIs and a runtime for fulfilling those queries with your existing\ndata.”1 What does this really mean? It means that GraphQL is a specification that\nallows us to run queries in an API server. In the same way SQL provides a query lan-\nguage for databases, GraphQL provides a query language for APIs.2 GraphQL also\nprovides a specification for how those queries are resolved in a server so that anyone\ncan implement a GraphQL runtime in any programming language.3\n Just as we can use SQL to define schemas for our database tables, we can use\nGraphQL to write specifications that describe the type of data that can be queried\nfrom our servers. A GraphQL API specification is called a schema, and it’s written in a\nstandard called Schema Definition Language (SDL). In this chapter, we will learn how\nto use the SDL to produce a specification for the products API.\n GraphQL was first released in 2015, and since then it’s gained traction as one of\nthe most popular choices for building web APIs. I should say there’s nothing in the\nGraphQL specification saying that GraphQL should be used over HTTP, but in prac-\ntice, this is the most common type of protocol used in GraphQL APIs.\n What’s great about GraphQL? It shines in giving users full control over which data\nthey want to obtain from the server. For example, as we’ll see in the next section, in\nthe products API we store many details about each product, such as its name, price,\navailability, and ingredients, among others. As you can see in figure 8.1, if a user\nwishes to get a list of just product names and prices, with GraphQL they can do that.\nIn contrast, with other types of APIs, such as REST, you get a full list of details for each\nproduct. Therefore, whenever it’s important to give the client full control over how\nthey fetch data from the server, GraphQL is a great choice.\n1 This definition appears in the home page of the GraphQL specification: https://graphql.org/.\n2 I owe the comparison between GraphQL and SQL to Eve Porcello and Alex Banks, Learning GraphQL, Declar-\native Data Fetching for Modern Web Apps (O’Reilly, 2018), pp. 31–32.\n3 The GraphQL website maintains a list of runtimes available for building GraphQL servers in different lan-\nguages: https://graphql.org/code/.",
      "content_length": 3314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "187\n8.1\nIntroducing GraphQL\nAnother great advantage of GraphQL is the ability to create connections between dif-\nferent types of resources, and to expose those connections to our clients for use in\ntheir queries. For example, in the products API, products and ingredients are differ-\nent but related types of resources. As you can see in figure 8.2, if a user wants to get a\nlist of products, including their names, prices, and their ingredients, with GraphQL\nthey can do that by leveraging the connections between these resources. Therefore, in\nservices where we have highly interconnected resources, and where it’s useful for our\nclients to explore and query those connections, GraphQL makes an excellent choice.\nproducts {\nname,\nprice\n}\nRequest\nGraphQL server\nproducts: [\n{\nname: Mocha,\nprice: $9.59\n},\n{\nname: Cappuccino,\nprice: $12.59\n}\n]\nResponse\nGraphQL server\nClient\nClient\nProduct\nid: 2f1c48e6-07e2\nname: Cappuccino\nprice: 12.59\navailable: true\nsize: MEDIUM\nlastUpdated: 2022/07/14\ningredients: [\nea2ac48d-40e3,\n3166c9a1-c7fc,\n75d8c361-fce1\n]\nProduct\nid: f262441b-a3de\nname: Mocha\nprice: 9.59\navailable: true\nsize: SMALL\nlastUpdated: 2022/06/03\ningredients: [\nea2ac48d-40e3,\n3166c9a1-c7fc,\n528b3a61-14d7\n]\nDatabase\nFigure 8.1\nUsing a GraphQL API, a client can request a list of items with specific details. In \nthis example, a client is requesting the name and price of each product in the products API.",
      "content_length": 1412,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "188\nCHAPTER 8\nDesigning GraphQL APIs\nIngredient\nid: ea2ac48d-40e3\nname: coffee\nstock: 55.4 KG\nlastUpdated: 2022/07/14\nProduct\nid: f262441b-a3de\nname: Mocha\nprice: 9.59\navailable: true\nsize: SMALL\nlastUpdated: 2022/06/03\ningredients: [\nea2ac48d-40e3,\n3166c9a1-c7fc,\n528b3a61-14d7\n]\nDatabase\nIngredient\nid: 3166c9a1-c7fc\nname: milk\nstock: 119.2 L\nlastUpdated: 2022/07/14\nIngredient\nid: 528b3a61-14d7\nname: chocolate\nstock: 67.3 KG\nlastUpdated: 2022/07/14\nClient\nproducts {\nname,\nprice,\ningredients {\nname\n}\n}\nRequest\nGraphQL server\nproducts: [\n{\nname: Mocha,\nprice: $9.59,\ningredients: [\n{ name: coﬀee },\n{ name: milk },\n{ name: chocolate }\n}\n]\nResponse\nGraphQL server\nClient\nFigure 8.2\nUsing \nGraphQL, a client can \nrequest the details of a \nresource and other \nresources linked to it. In \nthis example, the products \nAPI has two types of \nresources: products and \ningredients, both of which \nare connected through \nproduct’s ingredients \nfield. Using this connection, \na client can request the \nname and price of each \nproduct, as well as the \nname of each product’s \ningredient.",
      "content_length": 1079,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "189\n8.2\nIntroducing the products API\nIn the sections that follow, we’ll learn how to produce a GraphQL specification for the\nproducts service. We’ll learn how to define the types of our data, how to create meaning-\nful connections between resources, and how to define operations for querying the data\nand changing the state of the server. But before we do that, we ought to understand the\nrequirements for the products API, and that’s what we do in the next section!\n8.2\nIntroducing the products API\nThis section discusses the requirements of the products API. Before working on an\nAPI specification, it’s important to gather information about the API requirements. As\nyou can see in figure 8.3, the products API is the interface to the products service. To\ndetermine the requirements of the products API, we need to know what users of the\nproducts service can do with it.\nThe products service owns data about the products offered by the CoffeeMesh plat-\nform. As you can see in figure 8.4, the CoffeeMesh staff must be able to use the products\nservice to manage the available stock of each product, as well as to keep the products’\ningredients up to date. In particular, they must be able to query the stock of a product\nProducts\nAPI\nProducts service\nFigure 8.3\nTo interact with the \nproducts service, clients use the \nproducts API.\nProducts\nAPI\nProducts service\ningredient(id=\"75d8c361-fce1|) { stock }\nupdateIngredient(id=75d8c361-fce1) { stock }\naddProduct(name=\"Mocha\") { id }\ndeleteProduct(id=\"45f80fc0-d1af\") {}\nFigure 8.4\nThe CoffeeMesh staff uses the products service to manage products and ingredients.",
      "content_length": 1612,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "190\nCHAPTER 8\nDesigning GraphQL APIs\nor ingredient, and to update them when new stock arrives to the warehouse. They must\nalso be able to add new products or ingredients to the system and delete old ones. This\ninformation already gives us a complex list of requirements, so let’s break it down into\nspecific technical requirements.\n Let’s start with by modeling the resources managed by the products API. We want to\nknow which type of resources we should expose through the API and the products’ prop-\nerties. From the description in the previous paragraph, we know that the products service\nmanages two types of resources: products and ingredients. Let’s analyze products first.\n The CoffeeMesh platform offers two types of products: cakes and beverages. As\nyou can see in figure 8.5, both cakes and beverages have a common set of properties,\nincluding the product’s name, price, size, list of ingredients, and its availability. Cakes\nhave two additional properties:\n\nhasFilling—Indicates whether the cake has a filling\n\nhasNutsToppingOption—Indicates whether the customer can add a topping of\nnuts to the cake\nBeverages have the following two additional properties:\n\nhasCreamOnTopOption—Indicates whether the customer can top the beverage\nwith cream\n\nhasServeOnIceOption—Indicates whether the customer can choose to get the\nbeverage served on ice\nCake<Product>\nid: ID\nname: String\nprice: Float\navailable: Boolean\nsize: String\nlastUpdated: Datetime\ningredients: [Ingredient]\nhasFilling: Boolean\nhasNutsToppingOption: Boolean\nBeverage<Product>\nid: ID\nname: String\nprice: Float\navailable: Boolean\nsize: String\nlastUpdated: Datetime\ningredients: [Ingredient]\nhasCreamOnTopOption: Boolean\nhasServeOnIceOption: Boolean\nProduct\nid: ID\nname: String\nprice: Float\navailable: Boolean\nsize: String\nlastUpdated: Datetime\ningredients: [Ingredient]\nFigure 8.5\nCoffeeMesh exposes two types of products: Cake and Beverage, both of which share a \ncommon list of properties.",
      "content_length": 1961,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "191\n8.2\nIntroducing the products API\nWhat about ingredients? As you can see in figure 8.6, we can represent all ingredients\nthrough one entity with the following attributes:\n\nname—The ingredient’s name.\n\nstock—The ingredient’s available stock. Since different ingredients are mea-\nsured with different units, such as kilograms or liters, we express the available\nstock in terms of amounts of per unit of measure.\n\ndescription—A collection of notes that CoffeeMesh employees can use to\ndescribe and qualify the product.\n\nsupplier—Information about the company that supplies the ingredient to Cof-\nfeeMesh, including their name, address, contact number, and email.\nNow that we’ve modeled the main resources managed by the products service, let’s\nturn our attention to the operations we must expose through the API. We’ll distin-\nguish read operations from write/delete operations. This distinction will make sense\nwhen we look more closely at these operations in sections 8.8 and 8.9.\n Based on the previous discussion, we’ll expose the following read operations:\n\nallProducts()—Returns the full list of products available in the CoffeeMesh\ncatalogue\n\nallIngredients()—Returns the full list of ingredients used by CoffeeMesh to\nmake their products\n\nproducts()—Allows users to filter the full list of products by certain criteria\nsuch as availability, maximum price, and others\n\nproduct()—Allows users to obtain information about a single product\n\ningredient()—Allows users to obtain information about a single ingredient\nIngredient\nid: ID\nname: String\nstock: Stock\nsupplier: Supplier\ndescription: [String]\nlastUpdated: Datetime\nStock\nquantity: Float\nunit: String\nSupplier\nid: ID\nname: String\naddress: String\ncontactNumber: String\nemail: String\nFigure 8.6\nList of properties that describe an ingredient. The \ningredient’s supplier is described by a resource called Supplier, \nwhile the ingredient’s stock is described through a Stock object.",
      "content_length": 1947,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "192\nCHAPTER 8\nDesigning GraphQL APIs\nIn terms of write/delete operations, from the previous discussion it’s clear that we\nshould expose the following capabilities:\n\naddIngredient()—To add new ingredients\n\nupdateStock()—To update an ingredient’s stock\n\naddProduct()—To add new products\n\nupdateProduct()—To update existing products\n\ndeleteProduct()—To delete products from the catalogue\nNow that we understand the requirements of the products API, it’s time to move on to\ncreating the API specification! In the following sections, we’ll learn to create a GraphQL\nspecification for the products API, and along the way we’ll learn how GraphQL works.\nOur first stop is GraphQL’s type system, which we’ll use to model the resources man-\naged by the APIs.\n8.3\nIntroducing GraphQL’s type system\nIn this section, we introduce GraphQL’s type system. In GraphQL, types are defini-\ntions that allow us to describe the properties of our data. They’re the building blocks\nof a GraphQL API, and we use them to model the resources owned by the API. In this\nsection, you’ll learn to use GraphQL’s type system to describe the resources we\ndefined in section 8.2.\n8.3.1\nCreating property definitions with scalars\nThis section explains how we define the type of a property using GraphQL’s type sys-\ntem. We distinguish between scalar types and object types. As we’ll see in section 8.3.2,\nobject types are collection of properties that represent entities. Scalar types are types\nsuch as Booleans or integers. The syntax for defining a property’s type is very similar\nto how we use type hints in Python: we include the name of the property followed by a\ncolon, and the property’s type to the right of the colon. For example, in section 8.2 we\ndiscussed that cakes have two distinct properties: hasFilling and hasNutsTopping-\nOption, both of which are Booleans. Using GraphQL’s type system, we describe these\nproperties like this:\nhasFilling: Boolean\nhasNutsToppingOption: Boolean\nGraphQL supports the following types of scalars:\nStrings (String)—For text-based object properties.\nIntegers (Int)—For numerical object properties.\nFloats (Float)—For numerical object properties with decimal precision. \nBooleans (Boolean)—For binary properties of an object.\nUnique identifiers (ID)—For describing an object ID. Technically, IDs are strings,\nbut GraphQL checks and ensures that the ID of each object is unique.",
      "content_length": 2395,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "193\n8.3\nIntroducing GraphQL’s type system\nIn addition to defining the type of a property, we can also indicate whether the prop-\nerty is non-nullable. Nullable properties are properties that can be set to null when\nwe don’t know their value. We mark a property as non-nullable by placing an exclama-\ntion point at the end of the property definition:\nname: String!\nThis line defines a property name of type String, and it marks it as non-nullable by\nusing an exclamation point. This means that, whenever we serve this property from\nthe API, it will always be a string.\n Now that we’ve learned about properties and scalars, let’s see how we use this\nknowledge to model resources!\n8.3.2\nModeling resources with object types\nThis section explains how we use GraphQL’s type system to model resources. Resources\nare the entities managed by the API, such as the ingredients, cakes, and beverages that\nwe discussed in section 8.2. In GraphQL, each of these resources is modeled as an\nobject type. Object types are collections of properties, and as the name indicates, we use\nthem to define objects. To define an object type, we use the type keyword followed\nby the object name, and the list of object properties wrapped between curly braces.\nA property is defined by declaring the property name followed by a colon, and its\ntype on the right side of the colon. In GraphQL, ID is a type with a unique value. An\nexclamation point at the end of a property indicates that the property is non-nullable.\nThe following illustrates how we describe the cake resource as an object type. The\nlisting contains the basic properties of the cake type, such as the ID, the name, and\nits price.\ntype Cake {    \n  id: ID!   \n  name: String!\n  price: Float\n  available: Boolean!\n  hasFilling: Boolean!\n  hasNutsToppingOption: Boolean!\n}\nTYPES AND OBJECT TYPES\nFor convenience, throughout the book, we use the\nconcepts of type and object type interchangeably unless otherwise stated.\nSome of the property definitions in listing 8.1 end with an exclamation point. In\nGraphQL, an exclamation point means that a property is non-nullable, which means\nthat every cake object returned by our API will contain an ID, a name, its availability,\nas well as the hasFilling and hasNutsToppingOption properties. It also guarantees\nthat none of these properties will be set to null. For API client developers, this\nListing 8.1\nDefinition of the Cake object type\nWe define an object type.\nWe define a non-nullble \nID property.",
      "content_length": 2483,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "194\nCHAPTER 8\nDesigning GraphQL APIs\ninformation is very valuable because they know they can count on these properties\nto always be present and build their applications with that assumption. The following\ncode shows the definitions for the Beverage and Ingredient types. It also shows the\ndefinition for the Supplier type, which contains information about the business that\nsupplies a certain ingredient, and in section 8.5.1 we’ll see how we connect it with the\nIngredient type.\ntype Beverage {\n  id: ID!\n  name: String!\n  price: Float\n  available: Boolean!\n  hasCreamOnTopOption: Boolean!\n  hasServeOnIceOption: Boolean!\n}\ntype Ingredient {\n  id: ID!\n  name: String!\n}\ntype Supplier {\n  id: ID!\n  name: String!\n  address: String!\n  contactNumber: String!\n  email: String!\n}\nNow that we know how to define object types, let’s complete our exploration of\nGraphQL’s type system by learning how to create our own custom types!\n8.3.3\nCreating custom scalars\nThis section explains how we create custom scalar definitions. In section 8.3.1, we\nintroduced GraphQL’s built-in scalars: String, Int, Float, Boolean, and ID. In many\ncases, this list of scalar types is sufficient to model our API resources. In some cases,\nhowever, GraphQL’s built-in scalar types might prove limited. In such cases, we can\ndefine our own custom scalar types. For example, we may want to be able to represent\na date type, a URL type, or an email address type.\n Since the products API is used to manage products and ingredients and make\nchanges to them, it is useful to add a lastUpdated property that tells us the last time a\nrecord changed. lastUpdated should be a Datetime scalar. GraphQL doesn’t have a\nbuilt-in scalar of that type, so we have to create our own. To declare a custom date-\ntime scalar, we use the following statement:\nscalar Datetime\nListing 8.2\nDefinitions of the Beverage and Ingredient object types",
      "content_length": 1893,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "195\n8.4\nRepresenting collections of items with lists\nWe also need to define how this scalar type is validated and serialized. We define the\nrules for validation and serialization of a custom scalar in the server implementation,\nwhich will be the topic of chapter 10. \nscalar Datetime   \ntype Cake {\n  id: ID!\n  name: String!\n  price: Float\n  available: Boolean!\n  hasFilling: Boolean!\n  hasNutsToppingOption: Boolean!\n  lastUpdated: Datetime!    \n}\nThis concludes our exploration of GraphQL scalars and object types. You’re now in a\nposition to define basic object types in GraphQL and create your own custom scalars.\nIn the following sections, we’ll learn to create connections between different object\ntypes, and we’ll learn how to use lists, interfaces, enumerations, and more!\n8.4\nRepresenting collections of items with lists\nThis section introduces GraphQL lists. Lists are arrays of types, and they’re defined by\nsurrounding a type with square brackets. Lists are useful when we need to define prop-\nerties that represent collections of items. As discussed in section 8.2, the Ingredient\ntype contains a property called description, which contains collections of notes\nabout the ingredient, as shown in the following code.\ntype Ingredient {\n  id: ID!\n  name: String!\n  description: [String!]    \n}\nLook closely at the use of exclamation points in the description property: we’re\ndefining it as a nullable property with non-nullable items. What does this mean?\nWhen we return an ingredient from the API, it may or may not contain a description\nfield, and if that field is present, it will contain a list of strings.\n When it comes to lists, you must pay careful attention to the use of exclamation\npoints. In list properties, we can use two exclamation points: one for the list itself and\nanother for the item within the list. To make both the list and its contents non-nullable,\nwe use exclamation points for both. The use of exclamation points for list types is one\nof the most common sources of confusion among GraphQL users. Table 8.1 summarizes\nListing 8.3\nUsing a custom Datetime scalar type\nListing 8.4\nRepresenting a list of strings\nWe declare a custom \nDatetime scalar.\nWe declare a non-\nnullable property \nwith type Datetime.\nWe define a list of \nnon-nullable items.",
      "content_length": 2281,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "196\nCHAPTER 8\nDesigning GraphQL APIs\nthe possible return values for each combination of exclamation points in a list prop-\nerty definition.\nUSE EXCLAMATION POINTS AND LISTS CAREFULLY!\nIn GraphQL, an exclamation\npoint indicates that a property is non-nullable, which means that the property\nneeds to be present in an object and its value cannot be null. When it comes\nto lists, we can use two exclamation points: one for the list itself and another\nfor the item within the list. Different combinations of the exclamation points\nwill yield different representations of the property. Table 8.1 shows which rep-\nresentations are valid for each combination. \nNow that we’ve learned about GraphQL’s type system and list properties, we’re ready\nto explore one of the most powerful and exciting features of GraphQL: connections\nbetween types.\n8.5\nThink graphs: Building meaningful connections \nbetween object types\nThis section explains how we create connections between objects in GraphQL. One of\nthe great benefits of GraphQL is being able to connect objects. By connecting objects,\nwe make it clear how our entities are related. As we’ll see in the next chapter, this\nmakes our GraphQL API more easily consumed.\n8.5.1\nConnecting types through edge properties\nThis section explains how we connect types by using edge properties: properties that\npoint to another type. Types can be connected by creating a property that points to\nanother type. As you can see in figure 8.7, a property that connects with another\nobject is called an edge. The following code shows how we connect the Ingredient\ntype with the Supplier type by adding a property called supplier to Ingredient that\npoints to Supplier.\ntype Ingredient {\n  id: ID!\n  name: String!\nTable 8.1\nValid return values for list properties\n[Word]\n[Word!]\n[Word]!\n[Word!]!\nnull\nValid\nValid\nInvalid\nInvalid\n[]\nValid\nValid\nValid\nValid\n[\"word\"]\nValid\nValid\nValid\nValid\n[null]\nValid\nInvalid\nValid\nInvalid\n[\"word\", null]\nValid\nInvalid\nValid\nInvalid\nListing 8.5\nEdge for one-to-one connection",
      "content_length": 2029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "197\n8.5\nThink graphs: Building meaningful connections between object types\n  supplier: Supplier!    \n  description: [String!]\n}\nThis is an example of one-to-one connection: a property in an object that points to exactly\none object. The property in this case is called an edge because it connects the Ingre-\ndient type with the Supplier type. It’s also an example of a directed connection: as you\ncan see in figure 8.7, we can reach the Supplier type from the Ingredient type, but\nnot the other way around, so the connection only works in one direction.\n To make the connection between Supplier and the Ingredient bidirectional,4 we\nneed to add a property to the Supplier type that points to the Ingredient type. Since a\nsupplier can provide more than one ingredient, the ingredients property points to a\nlist of Ingredient types. This is an example of a one-to-many connection. Figure 8.8 shows\nwhat the new relationship between the Ingredient and the Supplier types looks like.\ntype Supplier {\n  id: ID!\n  name: String!\n  address: String!\n  contactNumber: String!\n  email: String!\n4 In the literature about GraphQL, you’ll often find a digression about how GraphQL is inspired by graph the-\nory, and how we can use some of the concepts from graph theory to illustrate the relationships between types.\nFollowing that tradition, the bidirectional relationship we refer to here is an example of an undirected graph,\nsince the Supplier type can be reached from the Ingredient type, and vice versa. For a good discussion of\ngraph theory in the context of GraphQL, see Eve Porcello and Alex Banks, Learning GraphQL, Declarative Data\nFetching for Modern Web Apps (O’Reilly, 2018), pp. 15–30.\nListing 8.6\nBidirectional relationship between Supplier and Ingredient\nWe use an edge property to connect \nthe Ingredient and the Supplier types.\nIngredient\nid: ID\nname: String\nstock: Stock\nsupplier: Supplier\ndescription: [String]\nlastUpdated: Datetime\nSupplier\nid: ID\nname: String\naddress: String\ncontactNumber: String\nemail: String\nEdge ﬁeld\nconnecting\ntwo objects\nFigure 8.7\nTo connect the Ingredient type with the Supplier type, \nwe add a property to Ingredient called supplier, which points to the \nSupplier type. Since the Ingredient’s supplier property is \ncreating a connection between two types, we call it an edge.",
      "content_length": 2310,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "198\nCHAPTER 8\nDesigning GraphQL APIs\n  ingredients: [Ingredient!]!    \n}\nNow that we know how to create simple connections through edge properties, let’s see\nhow we create more complex connections using dedicated types.\n8.5.2\nCreating connections with through types\nThis section discusses through types: types that tell us how other object types are con-\nnected. They add additional information about the connection itself. We’ll use\nthrough types to connect our products, cakes, and beverages, with their ingredients.\nWe could connect them by adding a simple list of ingredients to Cake and Beverage,\nas shown in figure 8.9, but this wouldn’t tell us how much of each ingredient goes\ninto a product’s recipe.\nWe create a bidirectional relationship \nbetween the Ingredient and the \nSupplier types.\nIngredient\nid: ID\nname: String\nstock: Stock\nsupplier: Supplier\ndescription: [String]\nlastUpdated: Datetime\nSupplier\nid: ID\nname: String\naddress: String\ncontactNumber: String\nemail: String\ningredients: [Ingredient]\nFigure 8.8\nTo create a bidirectional relationship between two types, we add \nproperties to each of them that point to each other. In this example, the \nIngredient’s supplier property points to the Supplier type, while the \nSupplier’s ingredients property points to a list of ingredients.\nCake<Product>\nid: ID\nname: String\nprice: Float\navailable: Boolean\nsize: String\nlastUpdated: Datetime\ningredients: [Ingredient]\nhasFilling: Boolean\nhasNutsToppingOption: Boolean\nIngredient\nid: ID\nname: String\nstock: Stock\nsupplier: Supplier\ndescription: [String]\nlastUpdated: Datetime\nFigure 8.9\nWe can express \nCake’s ingredients field as \na list of Ingredient types, but \nthat wouldn’t tell us how much \nof each ingredient goes into a \ncake recipe.",
      "content_length": 1749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "199\n8.5\nThink graphs: Building meaningful connections between object types\nTo connect cakes and beverages with their ingredients, we’ll use a through type\ncalled IngredientRecipe. As you can see in figure 8.10, IngredientRecipe has\nthree properties: the ingredient itself, its amount, and the unit in which the amount\nis measured. This gives us more meaningful information about how our products\nrelate to their ingredients. \ntype IngredientRecipe {    \n  ingredient: Ingredient!\n  quantity: Float!\n  unit: String!\n}\ntype Cake {\n  id: ID!\n  name: String!\n  price: Float\n  available: Boolean!\n  hasFilling: Boolean!\n  hasNutsToppingOption: Boolean!\n  lastUpdated: Datetime!\n  ingredients: [IngredientRecipe!]!  \n}\ntype Beverage {\n  id: ID!\n  name: String!\n  price: Float\n  available: Boolean!\nListing 8.7\nThrough types that represent a relationship between two types\nCake<Product>\nid: ID\nname: String\nprice: Float\navailable: Boolean\nsize: String\nlastUpdated: Datetime\ningredients: [IngredientRecipe]\nhasFilling: Boolean\nhasNutsToppingOption: Boolean\nIngredient\nid: ID\nname: String\nstock: Stock\nsupplier: Supplier\ndescription: [String]\nlastUpdated: Datetime\nIngredientRecipe\ningredient: Ingredient\nquantity: Float\nunit: String\nFigure 8.10\nTo express how an Ingredient is connected with a Cake, we use the IngredientRecipe \nthrough type, which allows us to detail how much of each ingredient goes into a cake recipe.\nWe declare the \nIngredientRecipe \nthrough type.\nWe declare ingredients as \na list of IngredientRecipe \nthrough types.",
      "content_length": 1531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "200\nCHAPTER 8\nDesigning GraphQL APIs\n  hasCreamOnTopOption: Boolean!\n  hasServeOnIceOption: Boolean!\n  lastUpdated: Datetime!   \n  ingredients: [IngredientRecipe!]!\n}\nBy creating connections between different object types, we give our API consumers\nthe ability to explore our data by just following the connecting edges in the types. And\nby creating bidirectional relationships, we give users the ability to traverse our data\ngraph back and forth. This is one of the most powerful features of GraphQL, and it’s\nalways worth spending the time to design meaningful connections across our data.\n More often than not, we need to create properties that represent multiple types.\nFor example, we could have a property that represents either cakes or beverages. This\nis the topic of the next section.\n8.6\nCombining different types through unions \nand interfaces\nThis section discusses how we cope with situations where we have multiple types of the\nsame entity. You’ll often have to deal with properties that point to a collection of mul-\ntiple types. What does this mean in practice, and how does it work? Let’s look at an\nexample from the products API!\n In the products API, Cake and Beverage are two types of products. In section 8.4.2, we\nsaw how we connect Cake and Beverage with the Ingredient type. But how do we con-\nnect Ingredient to Cake and Beverage? We could simply add a property called products\nto the Ingredient type, which points to a list of Cakes and Beverages, like this:\nproducts: [Cake, Beverage]\nThis works, but it doesn’t allow us to represent Cakes and Beverages as a single prod-\nuct entity. Why would we want to do that? Because of the following reasons:\n\nCake and Beverage are the same thing: a product, and as such, it makes sense to\ntreat them as the same entity.\nAs we’ll see in sections 8.8 and 8.9, we’ll have to refer to our products in\nother parts of the code, and it will be very helpful to be able to use one single\ntype for that.\nIf we add new types of products to the system in the future, we don’t want to\nhave to change all parts of the specification that refer to products. Instead, we\nwant to have a single type that represents them all and update only that type.\nGraphQL offers two ways to bring various types together under a single type: unions\nand interfaces. Let’s look at each in detail.\n Interfaces are useful when we have types that share properties in common. This is\nthe case for the Cake and the Beverage types, which share most of their properties.\nGraphQL interfaces are similar to class interfaces in programming languages, such as",
      "content_length": 2584,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "201\n8.6\nCombining different types through unions and interfaces\nPython: they define a collection of properties that must be implemented by other\ntypes. Listing 8.8 shows how we use an interface to represent the collection of proper-\nties shared by Cake and Beverage. As you can see, we declare interface types using the\ninterface keyword. The Cake and Beverage types implement ProductInterface,\nand therefore they must define all the properties defined in the ProductInterface\ntype. By looking at the ProductInterface type, any user of our API can quickly get an\nidea of which properties are accessible on both the Beverage and Cake types.\ninterface ProductInterface {   \n  id: ID!\n  name: String!\n  price: Float\n  ingredients: [IngredientRecipe!]\n  available: Boolean!\n  lastUpdated: Datetime!\n}\ntype Cake implements ProductInterface {   \n  id: ID!\n  name: String!\n  price: Float\n  available: Boolean!\n  hasFilling: Boolean!    \n  hasNutsToppingOption: Boolean!\n  lastUpdated: Datetime!\n  ingredients: [IngredientRecipe!]!\n}\ntype Beverage implements ProductInterface {    \n  id: ID!\n  name: String!\n  price: Float\n  available: Boolean!\n  hasCreamOnTopOption: Boolean!\n  hasServeOnIceOption: Boolean!\n  lastUpdated: Datetime!\n  ingredients: [IngredientRecipe!]!\n}\nBy creating interfaces, we make it easier for our API consumers to understand the\ncommon properties shared by our product types. As we’ll see in the next chapter,\ninterfaces also make the API easier to consume.\n While interfaces help us define the common properties of various types, unions\nhelp us bring various types under the same type. This is very helpful when we want to\ntreat various types as a single entity. In the products API, we want to be able to treat\nthe Cake and Beverage types as a single Product type, and unions allow us to do that.\nA union type is the combination of different types using the pipe (|) operator. \nListing 8.8\nRepresenting common properties through interfaces \nWe declare the \nProductInterface \ninterface type.\nThe Cake type \nimplements the \nProductInterface \ninterface.\nWe define properties \nspecific to Cake.\nBeverage \nimplements the \nProductInterface \ninterface.",
      "content_length": 2164,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "202\nCHAPTER 8\nDesigning GraphQL APIs\ntype Cake implements ProductInterface {\n  id: ID!\n  name: String!\n  price: Float\n  available: Boolean!\n  hasFilling: Boolean!\n  hasNutsToppingOption: Boolean!\n  lastUpdated: Datetime!\n  ingredients: [IngredientRecipe!]!\n}\ntype Beverage implements ProductInterface {\n  id: ID!\n  name: String!\n  price: Float\n  available: Boolean!\n  hasCreamOnTopOption: Boolean!\n  hasServeOnIceOption: Boolean!\n  lastUpdated: Datetime!\n  ingredients: [IngredientRecipe!]!\n}\nunion Product = Beverage | Cake    \nUsing unions and interfaces makes our API easier to maintain and to consume. If we\never add a new type of product to the API, we can make sure it offers a similar inter-\nface to Cake and Beverage by making it implement the ProductInterface type. And\nby adding the new product to the Product union, we make sure it’s available on all\noperations that use the Product union type.\n Now that we know how to combine multiple object types, it’s time to learn how we\nconstrain the values of object type properties through enumerations.\n8.7\nConstraining property values with enumerations\nThis section covers GraphQL’s enumeration type. Technically, an enumeration is a spe-\ncific type of scalar that can only take on a predefined number of values. Enumerations\nare useful in properties that can accept a value only from a constrained list of choices.\nIn GraphQL, we declare enumerations using the enum keyword followed by the enu-\nmeration’s name, and we list its allowed values within curly braces.\n In the products API, we need enumerations for expressing the amounts of the\ningredients. For example, in section 8.5.2, we defined a through type called Ingre-\ndientRecipe, which indicates the amount of each ingredient that goes into a prod-\nuct. IngredientRecipe expresses amounts in terms of quantity per unit of measure.\nWe can measure ingredients in different ways. For example, we can measure milk in\npints, liters, ounces, gallons, and so on. For the sake of consistency, we want to ensure\nthat everyone uses the same units to describe the amounts of our ingredients, so\nwe’ll create an enumeration type called MeasureUnit that can be used to constrain\nthe values for the unit property.\nListing 8.9\nA union of different types\nWe create a union \nof the Beverage and \nthe Cake types.",
      "content_length": 2308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "203\n8.8\nDefining queries to serve data from the API\nenum MeasureUnit {    \n  LITERS    \n  KILOGRAMS\n  UNITS\n}\ntype IngredientRecipe {\n    ingredient: Ingredient!\n    quantity: Float!\n    unit: MeasureUnit!  \n}\nWe also want to use the MeasureUnit enumeration to describe the available stock of\nan ingredient. To do so, we define a Stock type, and we use it to define the stock\nproperty of the Ingredient type.\ntype Stock {    \n  quantity: Float!\n  unit: MeasureUnit!    \n}\ntype Ingredient {\n  id: ID!\n  name: String!\n  stock: Stock    \n  products: [Product!]!\n  supplier: Supplier!\n  description: [String!]\n}\nEnumerations are useful to ensure that certain values remain consistent through the\ninterface. This helps avoid errors that happen when you let users choose and write\nthose values by themselves.\n This concludes our journey through GraphQL’s type system. Types are the building\nblocks of an API specification, but without a mechanism to query or interact with them,\nour API is very limited. To perform actions on the server, we need to learn about\nGraphQL queries and mutations. Those will be the topic of the rest of the chapter!\n8.8\nDefining queries to serve data from the API\nThis section introduces GraphQL queries: operations that allow us to fetch or read data\nfrom the server. Serving data is one of the most important functions of any web API,\nand GraphQL offers great flexibility to create a powerful query interface. Queries cor-\nrespond to the group of read operations that we discussed in section 8.2. As a\nreminder, these are the query operations that the products API needs to support:\n\nallProducts()\n\nallIngredients()\nListing 8.10\nUsing the MeasureUnit enumeration type \nListing 8.11\nUsing the Stock enumeration type \nWe declare an \nenumeration.\nWe list the allowed values \nwithin this enumeration.\nunit is a non-nullable property \nof type MeasureUnit.\nWe declare the Stock type to help \nus express information about the \navailable stock of an ingredient. \nStock’s unit property \nis an enumeration.\nWe connect the Ingredient type \nwith the Stock type through \nIngredient’s stock property.",
      "content_length": 2112,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "204\nCHAPTER 8\nDesigning GraphQL APIs\n\nproducts()\n\nproduct()\n\ningredient()\nWe’ll work on the allProducts() query first since it’s the simplest, and then move on to\nthe products() query. As we work on products(), we’ll see how we add arguments to\nour query definitions, we’ll learn about pagination, and, finally, we’ll learn how to refac-\ntor our query parameters into their own type to improve readability and maintenance.\n The specification of a GraphQL query looks similar to the signature definition of\na Python function: we define the query name, optionally define a list of parameters\nfor the query between parentheses, and specify the return type after a colon. The\nfollowing code shows the simplest query in the products API: the allProducts()\nquery, which returns a list of all products. allProducts() doesn’t take any parame-\nters and simply returns a list of all products that exist in the server.\ntype Query {    \n  allProducts: [Products!]!    \n}\nallProducts() returns a list of all products that exist in the CoffeeMesh database.\nSuch a query is useful if we want to run an exhaustive analysis of all products, but in\nreal life our API consumers want to be able to filter the results. They can do that by\nusing the products() query, which, according to the requirements we gathered in sec-\ntion 8.2, returns a filtered list of products.\n Query arguments are defined within parentheses, similar to how we define the\nparameters of a Python function. Listing 8.13 shows how we define the products()\nquery. It includes arguments that allows our API consumers to filter products by availabil-\nity, or by maximum and minimum price. All the arguments are optional. API consumers\nare free to use any or all of the query arguments, or none. If they don’t specify any of the\narguments when using the products() query, they’ll get a list of all the products.\ntype Query {\n  products(available: Boolean, maxPrice: Float, minPrice: Float):\n    [Product!]    \n}\nIn addition to filtering the list of products, API consumers will likely want to be able to\nsort the list and paginate the results. Pagination is the ability to deliver the result of\na query in different sets of a specified size, and it’s commonly used in APIs to ensure\nthat API clients receive a sensible amount of data in each request. As illustrated in\nListing 8.12\nSimple GraphQL query to return a list of products\nListing 8.13\nSimple GraphQL query to return a list of products\nAll queries are defined under \nthe Query object type.\nWe define the allProducts() query. \nAfter the colon, we indicate what \nthe return type of the query is.\nQuery parameters are \ndefined within parentheses.",
      "content_length": 2654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "205\n8.8\nDefining queries to serve data from the API\nfigure 8.11, if the result of a query yields 10 or more records, we can divide the query\nresult into groups of five items each and serve one set at a time. Each set is called a page. \nWe enable pagination by adding a resultsPerPage argument to the query, as well as a\npage argument. To sort the result set, we expose a sort argument. The following snip-\npet shows in bold the changes to the products() query after we add these arguments:\ntype Query {\n  products(available: Boolean, maxPrice: Float, minPrice: Float, sort: String, \n      resultsPerPage: Int, page: Int): [Product!]!\n}\nOffering numerous query arguments gives a lot of flexibility to our API consumers, but\nit can be cumbersome to set values for all of them. We can make our API easier to use by\nsetting default values for some of the arguments. We’ll set a default sorting order, as well\nas a default value for the resultsPerPage argument and a default value for the page\nargument. The following code shows how we assign default values to some of the argu-\nments in the products() query and includes a SortingOrder enumeration that con-\nstrains the values of the sort argument to either ASCENDING or DESCENDING.\nenum SortingOrder {    \n  ASCENDING\n  DESCENDING\n}\ntype Query {\n  products(\n    maxPrice: Float\n    minPrice: Float\n    available: Boolean = true     \n    sort: SortingOrder = DESCENDING      \n    resultsPerPage: Int = 10\n    page: Int = 1\nListing 8.14\nSetting default values for query arguments \n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nrecordsPerPage: 5\npage: 1\n...\nPage 1\nPage 2\nPage 3\nrecordsPerPage\ndivides the\nsequence into\nblocks called pages.\nSelects page 1\nFigure 8.11\nA more common approach to pagination is to let users decide how many results per page they \nwant to see and let them select the specific page they want to get.\nWe declare the SortingOrder \nenumeration.\nWe assign default \nvalues for some of \nthe parameters.\nWe constrain sort’s values \nby setting its type to the \nSortingOrder enumeration.",
      "content_length": 2025,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "206\nCHAPTER 8\nDesigning GraphQL APIs\n  ): [Product!]!\n}\nThe signature of the products() query is becoming a bit cluttered. If we keep adding\narguments to it, it will become difficult to read and maintain. To improve readability,\nwe can refactor the arguments out of the query specification into their own type. In\nGraphQL, we can define lists of parameters by using input types, which have the same\nlook and feel as any other GraphQL object type, but they’re meant for use as input for\nqueries and mutations. \ninput ProductsFilter {    \n  maxPrice: Float    \n  minPrice: Float\n  available: Boolean = true,    \n  sort: SortingOrder = DESCENDING\n  resultsPerPage: Int = 10\n  page: Int = 1\n}\ntype Query {\n  products(input: ProductsFilter): [Product!]!    \n}\nThe remaining API queries, namely, allIngredients(), product(), and ingredient(),\nare shown in listing 8.16 in bold. allIngredients() returns a full list of ingredients and\ntherefore takes no arguments, as in the case of the allProducts() query. Finally, prod-\nuct() and ingredient() return a single product or ingredient by ID, and therefore have\na required id argument of type ID. If a product or ingredient is found for the provided\nID, the queries will return the details of the requested item; otherwise, they’ll return null.\ntype Query {\n  allProducts: [Product!]!\n  allIngredients: [Ingredient!]!\n  products(input: ProductsFilter!): [Product!]!\n  product(id: ID!): Product    \n  ingredient(id: ID!): Ingredient\n}\nNow that we know how to define queries, it’s time to learn about mutations, which are\nthe topic of the next section.\n8.9\nAltering the state of the server with mutations\nThis section introduces GraphQL mutations: operations that allow us to trigger actions\nthat change the state of the server. While the purpose of a query is to let us fetch data\nfrom the server, mutations allow us to create new resources, to delete them, or to alter\nListing 8.15\nRefactoring query arguments into input types\nListing 8.16\nSpecification for all the queries in the products API\nWe declare the \nProductsFilter input type.\nWe define ProductsFilter’s \nparameters.\nWe assign default values \nto some parameters.\nWe set the input parameter’s \ntype to ProductsFilter.\nproduct() returns a nullable \nresult of type Product.",
      "content_length": 2271,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "207\n8.9\nAltering the state of the server with mutations\ntheir state. Mutations have a return value, which can be a scalar, such as a Boolean, or\nan object. This allows our API consumers to verify that the operation completed suc-\ncessfully and to fetch any values generated by the server, such as IDs.\n In section 8.2, we discussed that the products API needs to support the following\noperations for adding, deleting, and updating resources in the server:\n\naddIngredient()\n\nupdateStock()\n\naddProduct()\n\nupdateProduct()\n\ndeleteProduct()\nIn this section, we’ll document the addProduct(), updateProduct(), and delete-\nProduct() mutations. The specification for the other mutations is similar to these,\nand you can check them out in the GitHub repository provided with this book.\n A GraphQL mutation looks similar to the signature of a function in Python: we\ndefine the name of the mutation, describe its parameters between parentheses, and\nprovide its return type after a colon. Listing 8.17 shows the specification for the\naddProduct() mutation. addProduct() accepts a long list of arguments, and it returns\na Product type. All the arguments are optional except name and type. We use type to\nindicate what kind of product we’re creating, a cake or a beverage. We also include a\nProductType enumeration to constrain the values of the type argument to either\ncake or beverage. Since this mutation is used to create cakes and beverages, we allow\nusers to specify properties of each type, namely hasFilling and hasNutsTopping-\nOption for cakes, as well as hasCreamOnTopOption and hasServeOnIceOption for bev-\nerages, but we set them by default to false to make the mutation easier to use.\nenum ProductType {    \n  cake\n  beverage\n}\ninput IngredientRecipeInput {\n  ingredient: ID!\n  quantity: Float!\n  unit: MeasureUnit!\n}\nenum Sizes {\n  SMALL\n  MEDIUM\n  BIG\n}\ntype Mutation {    \n  addProduct(\nListing 8.17\nDefining a GraphQL mutation \nWe declare a \nProductType \nenumeration.\nWe declare mutations under \nthe Mutation object type.",
      "content_length": 2028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "208\nCHAPTER 8\nDesigning GraphQL APIs\n    name: String!\n    type: ProductType!\n    price: String\n    size: Sizes\n    ingredients: [IngredientRecipeInput!]! \n    hasFilling: Boolean = false\n    hasNutsToppingOption: Boolean = false\n    hasCreamOnTopOption: Boolean = false\n    hasServeOnIceOption: Boolean = false\n  ): Product!    \n}\nYou’d agree that the signature definition of the addProduct() mutation looks a bit\ncluttered. We can improve readability and maintainability by refactoring the list of\nparameters into their own type. Listing 8.18 shows how we refactor the addProduct()\nmutation by moving the list of parameters into an input type. AddProductInput con-\ntains all the optional parameters that can be set when we create a new product. We set\naside the name parameter, which is the only required parameter when we create a new\nproduct. As we’ll see shortly, this allows us to reuse the AddProductInput input type in\nother mutations that don’t require the name parameter.\ninput AddProductInput {    \n  price: String    \n  size: Sizes \n  ingredients: [IngredientRecipeInput!]!\n  hasFilling: Boolean = false   \n  hasNutsToppingOption: Boolean = false\n  hasCreamOnTopOption: Boolean = false\n  hasServeOnIceOption: Boolean = false\n}\ntype Mutation {\n  addProduct(\n    name: String!\n    type: ProductType!\n    input: AddProductInput!\n  ): Product!    \n}\nInput types not only help us make our specification more readable and maintainable, but\nthey also allow us to create reusable types. We can reuse the AddProductInput input type\nin the signature of the updateProduct() mutation. When we update the configuration\nfor a product, we may want to change only some of its parameters, such as the name, the\nprice, or its ingredients. The following snippet shows how we reuse the AddProductInput\nparameters in updateProduct(). In addition to AddProductInput, we also include a\nListing 8.18\nRefactoring parameters with input types\nWe specify the return \ntype of addProduct().\nWe declare the\nAddProductInput input type.\nWe list AddProductInput’s \nparameters.\nWe assign default values \nto some parameters.\naddProduct()’s input \nparameter has the \nAddProduct input type.",
      "content_length": 2164,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "209\nSummary\nmandatory product id parameter, which is necessary to identify the product we want to\nupdate. We also include the name parameter, which in this case is optional:\ntype Mutation {\n  updateProduct(id: ID!, input: AddProductInput!): Product!\n}\nLet’s now look at the deleteProduct() mutation, which removes a product from the\ncatalogue. To do that, the user must provide the ID for the product they want to\ndelete. If the operation is successful, the mutation returns true; otherwise, it returns\nfalse. The next snippet shows the specification for the deleteProduct() mutation:\ndeleteProduct(id: ID!): Boolean!\nThis concludes our journey through GraphQL’s SDL! You’re now equipped with\neverything you need to define your own API schemas. In chapter 9, we’ll learn how to\nlaunch a mock server using the products API specification and how to consume and\ninteract with the GraphQL API.\nSummary\nGraphQL is a popular protocol for building web APIs. It shines in scenarios\nwhere it’s important to give API clients full control over the data they want to\nfetch and in situations where we have highly interconnected data.\nA GraphQL API specification is called a schema, and it’s written using the Schema\nDefinition Language (SDL).\nWe use GraphQL’s scalar types to define the properties of an object type: Bool-\neans, strings, floats, integers, and IDs. In addition, we can also create our own\ncustom scalar types.\nGraphQL’s object types are collections of properties, and they typically repre-\nsent the resource or entities managed by the API server.\nWe can connect objects by using edge properties, namely, properties that point\nto another object, and by using through types. Through types are object types\nthat add additional information about how two objects are connected.\nTo constrain the values of a property, we use enumeration types.\nGraphQL queries are operations that allow API clients to fetch data from the\nserver.\nGraphQL mutations are operations that allow API clients to trigger actions that\nchange the state of the server.\nWhen queries and mutations have long lists of parameters, we can refactor\nthem into input types to increase readability and maintainability. Input types\ncan also be reused in more than one query or mutation.",
      "content_length": 2254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "210\nConsuming\nGraphQL APIs\nThis chapter teaches you how to consume GraphQL APIs. As we learned in chapter 8,\nGraphQL offers a query language for web APIs, and in this chapter you’ll learn how\nto use this language to run queries on the server. In particular, you’ll learn how to\nmake queries against a GraphQL API. You’ll learn to explore a GraphQL API to dis-\ncover its available types, queries, and mutations. Understanding how GraphQL APIs\nwork from the client side is an important step toward mastering GraphQL.\n Learning to interact with GraphQL APIs will help you learn to consume the\nAPIs exposed by other vendors, it’ll let you run tests against your own APIs, and it’ll\nhelp you design better APIs. You’ll learn to use the GraphiQL client to explore and\nvisualize a GraphQL API. As you’ll see, GraphiQL offers an interactive query panel\nthat makes it easier to run queries on the server.\nThis chapter covers\nRunning a GraphQL mock server to test our \nAPI design\nUsing the GraphiQL client to explore and \nconsume a GraphQL API\nRunning queries and mutations against a \nGraphQL API\nConsuming a GraphQL API programmatically \nusing cURL and Python",
      "content_length": 1154,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "211\n9.1\nRunning a GraphQL mock server\n To illustrate the concepts and ideas behind GraphQL’s query language, we’ll run\npractical examples using the products API we designed in chapter 8. Since we haven’t\nimplemented the API specification for the products API, we’ll learn to run a mock\nserver—an important part of the API development process, as it makes testing and val-\nidating an API design so much easier. Finally, you’ll also learn to run queries against a\nGraphQL API programmatically using tools such as cURL and Python.\n9.1\nRunning a GraphQL mock server\nIn this section, we explain how we can run a GraphQL mock server to explore and test\nour API. A mock server is a fake server that emulates the behavior of the real server,\noffering the same endpoints and capabilities, but using fake data. For example, a\nmock server for the products API is a server that mimics the implementation of the\nproducts API and offers the same interface that we developed in chapter 8.\nDEFINITION\nMock servers are fake servers that mimic the behavior of a real\nserver. They are commonly used for developing API clients while the backend\nis being implemented. You can launch a mock server using the specification\nfor an API. Mock servers return fake data and typically don’t persist data.\nMock servers are instrumental in the development of web APIs since they allow our\nAPI consumers to start working on the client-side code while we work on the backend\nimplementation. In this section, we’ll run a mock server on the products API. The\nonly thing we need to run a mock server is the API specification, which we developed\nin chapter 8. You’ll find the API specification under ch08/schema.graphql in the\nGitHub repository for this book.\n You can choose from among many different libraries to run a GraphQL mock server.\nIn this chapter, we’ll use GraphQL Faker (https://github.com/APIs-guru/graphql\n-faker), which is one of the most popular GraphQL mocking tools. To install GraphQL\nFaker, run the following command:\n$ npm install graphql-faker\nThis will create a package-lock.json file under your current directory, as well as a node_\nmodules folder. package-lock.json contains information about the dependencies\ninstalled together with graphql-faker, while node_modules is the directory where those\ndependencies are installed. To run the mock server, execute the following command:\n$ ./node_modules/.bin/graphql-faker schema.graphql\nGraphQL Faker normally runs on port 9002, and it exposes three endpoints:\n\n/editor—An interactive editor where you can develop your GraphQL API.\n\n/graphql—A GraphiQL interface to your GraphQL API. This is the interface\nwe’ll use to explore the API and run our queries.\n\n/voyager—An interactive display of your API, which helps you understand the\nrelationships and dependencies between your types (see figure 9.1).",
      "content_length": 2836,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "212\nCHAPTER 9\nConsuming GraphQL APIs\nFigure 9.1\nVoyager UI for the \nproducts API. This UI shows the \nrelationships between object types \ncaptured by the queries available in \nthe API. By following the connecting \narrows, you can see which objects we \ncan reach from each query.",
      "content_length": 277,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "213\n9.1\nRunning a GraphQL mock server\nTo start exploring and testing the products API, visit the following address in your\nbrowser: http:/ /localhost:9002/graphql (if you’re running GraphQL Faker in a differ-\nent port, your URL will look different). This endpoint loads a GraphiQL interface for\nour products API. Figure 9.2 illustrates what this interface looks like and highlights\nthe most important elements in it.\nTo discover the queries and mutations exposed by the API, click the Docs button on the\ntop-right corner of the UI. Upon clicking the Docs button, a side navigation bar will pop\nup offering two choices: queries or mutations (see figure 9.3 for an illustration). If you\nQuery panel\nAPI documentation\nexplorer\nQuery variables panel for\nparameterized queries\nQuery execution\nbutton\nFigure 9.2\nAPI documentation explorer and query panel interface in GraphiQL\nFigure 9.3\nBy clicking through the Documentation Explorer in GraphiQL, you can inspect all the queries and \nmutations available in the API, as well as the types they return and their properties.",
      "content_length": 1065,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "214\nCHAPTER 9\nConsuming GraphQL APIs\nselect queries, you’ll see the list of queries exposed by the server with their return\ntypes. You can click the return types to explore their properties, as you can see in fig-\nure 9.3. In the next section, we’ll start testing the GraphQL API!\n9.2\nIntroducing GraphQL queries\nIn this section, we learn to consume a GraphQL API by running queries using\nGraphiQL. We’ll start with simple queries that don’t require any parameters, and then\nwe’ll move on to queries with parameters.\n9.2.1\nRunning simple queries\nIn this section, we introduce simple queries that don’t take any parameters. The prod-\nucts API offers two queries of this type: allProducts(), which returns a list of all\nproducts CoffeeMesh offers, and allIngredients(), which returns a list of all the\ningredients.\n We’ll use GraphiQL to run queries against the API. To run the query, go to the\nquery editor pane in the GraphiQL UI, which is illustrated in figure 9.2. Listing 9.1\nshows how we run the allIngredients() query. As you can see, to run a query we\nmust use the name of the query operation followed by curly braces. Within the curly\nbraces, we declare the selection of properties we want to get from the server. The\nblock within curly braces is called a selection set. GraphQL queries must always include\na selection set. If you don’t include it, you’ll get an error response from the server.\nHere, we select only the name of each ingredient. The text representing the query is\ncalled a query document.\n{     \n  allIngredients {    \n    name     \n  }\n}\nA response to a successful query from a GraphQL API contains a JSON document with\na “data” field, which wraps the query result. An unsuccessful query results in a JSON\ndocument that contains an “error” key. Since we’re running a mock server, the API\nreturns random values.\n{\n  \"data\": {       \n    \"allIngredients\": [     \n      {\n        \"name\": \"string\"\n      },\nListing 9.1\nQuery document running the allIngredients() query\nListing 9.2\nExample of successful response for the allIngredients() query\nWe wrap queries \nwithin curly braces.\nWe run the \nallIngredients() \nquery.\nWe query \nthe name \nproperty.\nA successful response \nincludes a \"data\" key.\nThe result of the query is \nindexed under a key named \nafter the query itself.",
      "content_length": 2292,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "215\n9.2\nIntroducing GraphQL queries\n      {\n        \"name\": \"string\"\n      }\n    ]\n  }\n}\nNow that we know the basics of GraphQL queries, let’s spice up our queries by adding\nparameters!\n9.2.2\nRunning queries with parameters\nThis section explains how we use parameters in GraphQL queries. allIngredients()\nis a simple query that doesn’t take any parameters. Now let’s see how we can run a\nquery that requires a parameter. One example of such a query is the ingredient()\nquery, which requires an id parameter. The following code shows how we can call the\ningredient() query with a random ID. As you can see, we include the query parame-\nters as key-value pairs separated by a colon within parentheses.\n{\n  ingredient(id: \"asdf\") {    \n    name\n  }\n}\nNow that we know how to run queries with parameters, let’s look at the kinds of prob-\nlems we can run into when running queries and how to deal with them.\n9.2.3\nUnderstanding query errors\nThis section explains some of the most common errors you’ll find when running\nGraphQL queries, and it teaches you how to read and interpret them.\n If you omit the required parameter when running the ingredient() query, you’ll\nget an error from the API. Error responses include an error key pointing to a list of all\nthe errors found by the server. Each error is an object with the following keys:\n\nmessage—Includes a human-readable description of the error\n\nlocations—Specifies where in the query the error was found, including the\nline and column\nListing 9.4 shows what happens when you run the query with empty parentheses. As\nyou can see, we get a syntax error with a somewhat cryptic message: Expected Name,\nfound ). This is a common error that occurs whenever you make a syntax error in\nGraphQL. In this case, it means that GraphQL was expecting a parameter after the\nopening parenthesis, but instead it found a closing parenthesis.\n \n \nListing 9.3\nRunning a query with a required parameter\nWe call ingredient() \nwith the ID parameter \nset to \"asdf\".",
      "content_length": 1993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "216\nCHAPTER 9\nConsuming GraphQL APIs\n# Query:\n{\n  ingredient() {   \n    name\n  }\n}\n# Error:\n{\n  \"errors\": [     \n    {\n      \"message\": \"Syntax Error: Expected Name, found )\",   \n      \"locations\": [    \n        {\n          \"line\": 2,    \n          \"column\": 14   \n        }\n      ]\n    }\n  ]\n}\nOn the other hand, if you run the ingredient() query without any parentheses at all,\nas shown in listing 9.5, you’ll get an error specifying that you missed the required\nparameter id.\nUSE OF PARENTHESES IN GRAPHQL QUERIES AND MUTATIONS\nIn GraphQL, the\nparameters of a query are defined within parentheses. If you run a query with\nrequired parameters, such as ingredient, you must include the parameters\nwithin parentheses (see listing 9.3). Failing to do so will throw an error (see\nlistings 9.4 and 9.5). If you run a query without parameters, you must omit the\nparentheses. For example, when we run the allIngredients() query, we\nomit parentheses (see listing 9.1), since allIngredients() doesn’t require\nany parentheses.\n# Query:\n{\n  ingredient {    \n    name\n  }\n}\n# Error:\n{\n  \"errors\": [\n    {\n      \"message\": \"Field \\\"ingredient\\\" argument \\\"id\\\" of type \\\"ID!\\\" is \n➥ required, but it was not provided.\",   \nListing 9.4\nMissing query parameter errors\nListing 9.5\nMissing query parameter errors\nWe run the ingredient() \nquery without the required \nparameter id.\nAn unsuccessful \nresponse includes \nan \"errors\" key.\nWe get \na generic \nsyntax error.\nThe precise location of \nthe error in our query\nThe error was found in the second \nline of our query document.\nThe error was found at the 14th \ncharacter in the second line.\nWe run the ingredient() \nquery without the \nparentheses.\nThe error message says\nthat the id parameter is\nmissing in the query.",
      "content_length": 1751,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "217\n9.3\nUsing fragments in queries\n      \"locations\": [\n        {\n          \"line\": 2,    \n          \"column\": 3    \n        }\n      ]\n    }\n  ]\n}\nNow that we know how to read and interpret error messages when we make mistakes\nin our queries, let’s explore queries that return multiple types. \n9.3\nUsing fragments in queries\nThis section explains how we run queries that return multiple types. The queries\nthat we’ve seen so far in this chapter are simple since they only return one type,\nwhich is Ingredient. However, our product-related queries, such as allProducts()\nand product(), return the Product union type, which is the combination of the Cake\nand Beverage types. How do we run our queries in this case?\n When a GraphQL query returns multiple types, we must create selection sets for\neach type. For example, if you run the allProducts() query with a single selector, you\nget the error message saying that the server doesn’t know how to resolve the proper-\nties in the selection set.\n# Query\n{\n  allProducts {     \n    name     \n  }\n}\n# Error message\n{\n  \"errors\": [    \n    {\n      \"message\": \"Cannot query field \\\"name\\\" on type \\\"Product\\\". Did you \n➥ mean to use an inline fragment on \\\"ProductInterface\\\", \\\"Beverage\\\", \n➥ or \\\"Cake\\\"?\",     \n      \"locations\": [\n        {\n          \"line\": 3,      \n          \"column\": 5    \n        }\n      ]\n    }\n  ]\n}\nListing 9.6\nCalling allProducts() with a single selector set\nThe error was found in the second \nline of our query document.\nThe error was found at \nthe third character of \nthe second line.\nWe run the allProducts() \nquery without parameters.\nWe include the \nname property in \nthe selection set.\nWe get an error \nresponse.\nThe server doesn’t know how \nto resolve the properties in \nthe selection set.\nThe error was found in the \nthird line of the query \ndocument.\nThe error was found at the \nfifth position of the third line.",
      "content_length": 1893,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "218\nCHAPTER 9\nConsuming GraphQL APIs\nThe error message in listing 9.6 asks you whether you meant to use an inline fragment\non either ProductInterface, Beverage, or Cake. What is an inline fragment? An inline\nfragment is an anonymous selection set on a specific type. The syntax for inline frag-\nments includes three dots (the spread operator in JavaScript) followed by the on key-\nword and the type on which the selection set applies, as well as a selection of properties\nbetween curly braces:\n...on ProductInterface {\n      name\n    }\nListing 9.7 fixes the allProducts() query by adding inline fragments that select prop-\nerties on the ProductInterface, Cake, and Beverage types. allProducts()’s return\ntype is Product, which is the union of Cake and Beverage, so we can select properties\nfrom both types. From the specification, we also know that Cake and Beverage imple-\nment the ProductInterface interface type, so we can conveniently select properties\ncommon to both Cake and Beverage directly on the interface.\n{\n  allProducts {\n    ...on ProductInterface {     \n      name\n    }\n    ...on Cake {    \n      hasFilling\n    }\n    ...on Beverage {     \n      hasCreamOnTopOption\n    }\n  }\n}\nListing 9.7 uses inline fragments, but the real benefit of fragments is we can define\nthem as standalone variables. This makes fragments reusable, and it also makes our\nqueries more readable. Listing 9.8 shows how we can refactor listing 9.7 to use stand-\nalone fragments. The queries are so much cleaner! In real-life situations, you’re likely\nto work with large selection sets, so organizing your fragments into standalone, reus-\nable pieces of code will make your queries easier to read.\n{\n  allProducts {\n    ...commonProperties\n    ...cakeProperties\n    ...beverageProperties\n  }\n}\nListing 9.7\nAdding inline fragments for each return type\nListing 9.8\nUsing standalone fragments\nInline fragment with \na selection set on the \nProductInterface type\nInline fragment with a \nselection set on the Cake type\nInline fragment with \na selection set on the \nBeverage type",
      "content_length": 2059,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "219\n9.5\nNavigating the API graph\nfragment commonProperties on ProductInterface {\n  name\n}\nfragment cakeProperties on Cake {\n  hasFilling\n}\nfragment beverageProperties on Beverage {\n  hasCreamOnTopOption\n}\nNow that we know how to deal with queries that return multiple object types, let’s take\nour querying skills to the next level. In the next section, we’ll learn to run queries with\na specific type of parameter called an input parameter.\n9.4\nRunning queries with input parameters\nThis section explains how we run queries with input type parameters. In section 8.8,\nwe learned that input types are similar to object types, but they’re meant for use as\nparameters for a GraphQL query or mutation. One example of an input type in the\nproducts API is ProductsFilter, which allows us to filter products by factors such as\navailability, minimum or maximum price, and others. ProductsFilter is the parame-\nter of the products() query. How do we call the products() query?\n When a query takes parameters in the form of an input type, the query’s input type\nparameter must be passed in the form of an input object. This may sound compli-\ncated, but it’s actually very simple. We call the products() query using Products-\nFilter’s maxPrice parameter. To use any of the parameters in the input type, we\nsimply wrap them with curly braces.\n{\n  products(input: {maxPrice: 10}) {      \n    ...on ProductInterface {     \n      name\n    }\n  }\n}\nNow that we know how to call queries with input parameters, let’s take a deeper look\nat the relationships between the objects defined in the API specification and see how\nwe can build queries that allow us to traverse our data graph.\n9.5\nNavigating the API graph\nThis section explains how we select properties from multiple types by leveraging their\nconnections. In section 8.5, we learned to create connections between object types\nby using edge properties and through types. These connections allow API clients to\nListing 9.9\nCalling a query with a required parameter\nWe specify \nProductFilter’s \nmaxPrice parameter.\nInline fragment on the\nProductInterface type",
      "content_length": 2095,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "220\nCHAPTER 9\nConsuming GraphQL APIs\ntraverse the graph of relationships between the resources managed by the API. For\nexample, in the products API, the Cake and Beverage types are connected with the\nIngredient type by means of a through type called IngredientRecipe. By leveraging\nthis connection, we can run queries that fetch information about the ingredients\nrelated to each product. In this section, we’ll learn to build such queries.\n In our queries, whenever we add a selector for a property that points to another\nobject type, we must include a nested selection set for said object type. For example, if\nwe add a selector for the ingredient property on the ProductInterface type, we have\nto include a selection set with any of the properties in IngredientRecipe nested within\nthe ingredients property. We include a nested selection set for the ingredients prop-\nerty of ProductInterface in the allProducts() query. The query selects the name of\neach product as well as the name of each ingredient in the product’s recipe.\n{\n  allProducts {\n    ...on ProductInterface {    \n      name,\n      ingredients {     \n        ingredient {     \n          name\n        }\n      }\n    }\n  }\n}\nListing 9.10 leverages the connection between the ProductInterface and Ingredient\ntypes to fetch information from both types in a single query, but we can take this fur-\nther. The Ingredient type contains a supplier property, which points to the Supplier\ntype. Say we want to get a list of products, including their names and ingredients,\ntogether with the supplier’s name of each ingredient. (I encourage you to head over\nto the Voyager UI generated by graphql-faker to visualize the relationships captured\nby this query; figure 9.1 is an illustration of the Voyager UI.)\n{\n  allProducts {\n    ...on ProductInterface {     \n      name\n      ingredients {     \n        ingredient {    \n          name\n          supplier {    \n            name\n          }\n        }\nListing 9.10\nQuerying nested object types\nListing 9.11\nTraversing the products API graph through connections between types\nInline fragment on the \nProductInterface type\nSelector for ProductInterface’s \ningredients property\nSelector for \nIngredientRecipe’s \ningredient property\nInline fragment on the \nProudctInterface type\nSelector for ProductInterface’s \ningredients property\nSelector for \nIngredientRecipe’s \ningredient property\nSelector for\nIngredient’s\nsupplier\nproperty",
      "content_length": 2428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "221\n9.6\nRunning multiple queries and query aliasing\n      }\n    }\n  }\n}\nListing 9.11 is traversing our graph of types. Starting from the ProductInterface type,\nwe are able to fetch details about other objects, such as Ingredient and Supplier, by\nleveraging their connections.\n Here lies one the most powerful features of GraphQL, and one of its main advan-\ntages in comparison with other types of APIs, such as REST. Using REST, we’d need to\nmake multiple requests to obtain all the information we were able to fetch in one\nrequest in listing 9.11. GraphQL gives you the power to obtain all the information you\nneed, and just the information you need, in a single request.\n Now that we know how to traverse the graph of types in a GraphQL API, let’s take\nour querying skills to the next level by learning how to run multiple queries within a\nsingle request!\n9.6\nRunning multiple queries and query aliasing\nThis section explains how to run multiple queries per request and how to create\naliases for the responses returned by the server. Aliasing our queries means changing\nthe key under which the dataset returned by the server is indexed. As we’ll see, aliases\ncan improve the readability of the results returned by the server, especially when we\nmake multiple queries per request. \n9.6.1\nRunning multiple queries in the same request\nIn previous sections, we ran only one query per request. However, GraphQL also allows\nus to send several queries in one request. This is yet another powerful feature of\nGraphQL that can help us save unnecessary network round-trips to the server, improv-\ning the overall performance of our applications and therefore user experience.\n Let’s say we wanted to obtain a list of all the products and ingredients available in the\nCoffeeMesh platform, as shown in figure 9.4. To do that, we can run allIngredients()\nwith the allProducts() queries. Listing 9.12 shows how we include both operations\nwithin the same query document. By including multiple queries within the same\nquery document, we make sure all of them are sent to the server in the same request,\nand therefore we save round-trips to the server. The code also includes a named frag-\nment that selects properties on the ProductInterface type. Named fragments are\nuseful to keep our queries clean and focused.\n{\n  allProducts {    \n    ...commonProperties     \n  }\nListing 9.12\nMultiple queries per request\nWe run the allProducts() \nquery without parameters.\nWe select properties \nusing a named fragment.",
      "content_length": 2493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "222\nCHAPTER 9\nConsuming GraphQL APIs\n  allIngredients {     \n    name\n  }\n}\nfragment commonProperties on ProductInterface {    \n  name\n}\n9.6.2\nAliasing our queries\nAll the queries we’ve run in previous sections are anonymous queries. When we make\nan anonymous query, the data returned by the server appears under a key named after\nthe name of the query we’re calling.\n# Query:\n{\n  allIngredients {   \n    name\n  }\n}\n# Result:\n{\n  \"data\": {     \n    \"allIngredients\": [     \n      {\n        \"name\": \"string\"\n      },\nListing 9.13\nResult of an anonymous query\nWe run the \nallIngredients() query.\nNamed fragment with \nselection set on the \nProductInterface type\nQuery document\nallProducts\nallIngredients\nQuery results\nProduct\nid: 1\nProduct\nid: 2\nProduct\nid: 3\nIngredient\nid: 1\nIngredient\nid: 2\nIngredient\nid: 3\nIngredients\nProducts\nFigure 9.4\nIn GraphQL, we can run multiple queries within the same request, and \nthe response will contain one dataset for each query.\nWe run the \nallIngredients() \nquery.\nSuccessful \nresponse from \nthe query\nQuery \nresult",
      "content_length": 1051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "223\n9.6\nRunning multiple queries and query aliasing\n      {\n        \"name\": \"string\"\n      }\n    ]\n  }\n}\nRunning anonymous queries can sometimes be confusing. allIngredients() returns\na list of ingredients, so it is helpful to index the list of ingredients under an ingredients\nkey, instead of allIngredients(). Changing the name of this key is called query alias-\ning. We can make our queries more readable by using aliasing. The benefits of aliasing\nbecome clearer when we include multiple queries in the same request. For example,\nthe query for all products and ingredients shown in listing 9.12 becomes more read-\nable if we use aliases. The following code shows how we use aliases to rename the\nresults of each query: the result of allProducts() appears under the product alias,\nand the result of the allIngredients() query appears under the ingredients alias.\n{\n  products: allProducts {     \n    ...commonProperties     \n  }\n  ingredients: allIngredients {     \n    name\n  }\n}\nfragment commonProperties on ProductInterface {   \n  name\n}\nIn some cases, using query aliases is necessary to make our requests work. For exam-\nple, in listing 9.15, we run the products() query twice to select two datasets: one for\navailable products and another for unavailable products. Both datasets are produced\nby the same query: products. As you can see, without query aliasing, this request\nresults in conflict error, because both datasets return under the same key: products.\n{\n  products(input: {available: true}) {    \n    ...commonProperties      \n  }\n  products(input: {available: false}) {     \n    ...commonProperties\n  }\n}\nListing 9.14\nUsing query aliasing for more readable queries\nListing 9.15\nError due to calling the same query multiple times without aliases\nAlias for the \nallProducts() query\nWe select properties \nusing a named fragment.\nAlias for the \nallIngredients() query\nNamed fragment \nwith selection set on \nthe ProductInterface\nWe run the products() query\nfiltering for available products.\nWe select properties using \nthe commonProperties \nfragment.\nWe run the products() \nquery filtering for \nunavailable products.",
      "content_length": 2129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "224\nCHAPTER 9\nConsuming GraphQL APIs\nfragment commonProperties on ProductInterface {    \n  name\n}\n# Error\n{\n  \"errors\": [    \n    {\n      \"message\": \"Fields \\\"products\\\" conflict because they have differing \n➥ arguments. Use different aliases on the fields to fetch both if this \n➥ was intentional.\",    \n      \"locations\": [\n        {\n          \"line\": 2,     \n          \"column\": 3\n        },\n        {\n          \"line\": 5,\n          \"column\": 3\n        }\n      ]\n    }\n  ]\n}\nTo resolve the conflict created by the queries in listing 9.15, we must use aliases. List-\ning 9.16 fixes the query by adding an alias to each operation: availableProducts for\nthe query that filters for available products and unavailableProducts for the query\nthat filters for unavailable products.\n{\n  availableProducts: products(input: {available: true}) {   \n    ...commonProperties\n  }\n  unavailableProducts: products(input: {available: false}) {   \n    ...commonProperties\n  }\n}\nfragment commonProperties on ProductInterface {\n  name\n}\n# Result (datasets omitted for brevity)\n{\n  \"data\": {     \n    \"availableProducts\": [...],     \n    \"unavailableProducts\": [...]    \n  }\n}\nListing 9.16\nCalling the same query multiple times with aliases\nNamed fragment with \nselection set on the \nProductInterface type.\nThe query returns an unsuccessful \nresponse, so the payload includes \nan error key.\nThe error message says \nthat the query document \ncontains a conflict.\nThe server found errors \nin lines 2 and 5 from the \nquery document.\nAlias for the available \nproducts() query\nunavailableProducts alias for the\nunavailable products() query\nSuccessful \nresponse from \nthe server\nResult of the available \nproducts() query\nResult of the unavailable \nproducts() query",
      "content_length": 1738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "225\n9.7\nRunning GraphQL mutations\nThis concludes our overview of GraphQL queries. You’ve learned to run queries with\nparameters, with input types, with inline and named fragments, and with aliases, and\nyou’ve learned to include multiple queries within the same request. We’ve come a\nlong way! But no overview of the GraphQL query language would be complete with-\nout learning how to run mutations.\n9.7\nRunning GraphQL mutations\nThis section explains how we run GraphQL mutations. Mutations are GraphQL func-\ntions that allow us to create resources or change the state of the server. Running a\nmutation is similar to running a query. The only difference between the two is in their\nintent: queries are meant to read data from the server, while mutations are meant to\ncreate or change data in the server.\n Let’s illustrate how we run a mutation with an example. Listing 9.17 shows how we\nrun the deleteProduct() mutation. When we use mutations, we must start our query\ndocument by qualifying our operation as a mutation. The deleteProduct() mutation\nhas one required argument, a product ID, and its return value is a simple Boolean, so\nin this case, we don’t have to include a selection set.\nmutation {                    \n  deleteProduct(id: \"asdf\")   \n}\nLet’s now look at a more complex mutation, like addProduct(), which is used to add new\nproducts to the CoffeeMesh catalogue. addProduct() has three required parameters:\n\nname—The product name.\n\ntype—The product type. The values for this parameter are constrained by the\nProductType enumeration, which offers two choices: cake and beverage.\n\ninput—Additional product properties, such as its price, size, list of ingredients,\nand others. The full list of properties is given by the AddProductInput type.\naddProduct() returns a value of type Product, which means, in this case, we must\ninclude a selection set. Remember that Product is the union of the Cake and Beverage\ntypes, so our selection set must use fragments to indicate which type’s properties we\nwant to include in our return payload. In the following example, we select the name\nproperty on the ProductInterface type.\nmutation {    \n  addProduct(name: \"Mocha\", type: beverage, input: {price: 10, size: BIG, \ningredients: [{ingredient: 1, quantity: 1, unit: LITERS}]}) {    \nListing 9.17\nCalling a mutation\nListing 9.18\nCalling a mutation with input parameters and complex return type\nWe qualify the operation \nwe’re going to run as a \nmutation.\nWe call the deleteProduct() mutation,\npassing in the required id parameter.\nWe qualify the operation we’re \ngoing to run as a mutation.\nWe call the addProduct()\nmutation.",
      "content_length": 2631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "226\nCHAPTER 9\nConsuming GraphQL APIs\n    ...commonProperties    \n  }\n}\nfragment commonProperties on ProductInterface {\n  name\n}\nNow that we know how to run mutations, it’s time to learn how we write more struc-\ntured and readable query documents by parameterizing the arguments. \n9.8\nRunning parameterized queries and mutations\nThis section introduces parameterized queries and explains how we can use them to\nbuild more structured and readable query documents. In previous sections, when using\nqueries and mutations that require parameters, we defined the values for each param-\neter in the same line we called the function. In queries with lots of arguments, this\napproach can lead to query documents, which are cluttered and difficult to read and\nmaintain. GraphQL offers a solution for this, which is to use parameterized queries.\n Parameterized queries allow us to decouple our query/mutation calls from the\ndata. Figure 9.5 illustrates how we parameterize the call to the addProduct() mutation\nusing GraphiQL (the code for the query is also shown in listing 9.19 so that you can\ninspect it and copy it more easily). There’re two things we need to do when we parame-\nterize a query or mutation: create a function wrapper around the query/mutation, and\nWe select properties \nusing a named \nfragment.\nQuery document\nMutation parameters\nResponse from\nthe server\nFigure 9.5\nGraphiQL offers a Query Variables panel where we can include the input values for our \nparameterized queries.",
      "content_length": 1484,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "227\n9.8\nRunning parameterized queries and mutations\nassign values for the query/mutation parameters in a query variables object. Figure 9.6\nillustrates how all these pieces fit together to bind the parameterized values to the\naddProduct() mutation call.\n# Query document\nmutation CreateProduct(    \n  $name: String!\n  $type: ProductType!\n  $input: AddProductInput!\n) {\n  addProduct(name: $name, type: $type, input: $input) {     \n    ...commonProperties     \n  }\n}\nfragment commonProperties on ProductInterface {\n  name\n}\n# Query variables\n{\n  \"name\": \"Mocha\",     \n  \"type\": \"beverage\",    \n  \"input\": {     \n    \"price\": 10,\n    \"size\": \"BIG\",\n    \"ingredients\": [{\"ingredient\": 1, \"quantity\": 1, \"unit\": \"LITERS\"}]\n  }\n}\nLet’s look at each of these steps in detail.\n1\nCreating a query/mutation wrapper. To parameterize our queries, we create a func-\ntion wrapper around the query or mutation. In figure 9.5, we call the wrapper\nCreateProduct(). The syntax for the wrapper looks very similar to the syntax\nwe use to define a query. Parameterized arguments must be included in the\nwrapper’s function signature. In figure 9.5, we parameterize the name, type, and\ninput parameters of the addProduct() mutation. The parameterized argument\nis marked with a dollar sign ($). In the wrapper’s signature (i.e., in Create-\nProduct()), we specify the expected type of the parameterized arguments.\n2\nParameterizing through a query variables object. Separately, we define our query vari-\nables as a JSON document. As you can see in figure 9.5, in GraphiQL we define\nquery variables within the Query Variables panel. For further clarification on\nhow parameterized queries work, look at figure 9.6.\nIn figure 9.5, we used parameterized syntax to wrap only one mutation, but nothing pre-\nvents us from wrapping more mutations within the same query document. When we\nwrap multiple queries or mutations, all the parameterized arguments must be defined\nListing 9.19\nUsing parameterized syntax\nWe create a \nwrapper named \nCreateProduct().\nWe call the \naddProduct() \nmutation.\nWe select properties \nusing a named \nfragment.\nWe assign a value to \nthe name parameter.\nWe assign a value to \nthe type parameter.\nWe assign a value to \nthe input parameter.",
      "content_length": 2232,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "228\nCHAPTER 9\nConsuming GraphQL APIs\nwithin the wrapper’s function signature. The following code shows how we extend the\nquery from listing 9.19 to include a call to the deleteProduct() mutation. Here, we call\nthe wrapper CreateAndDeleteProduct() to better represent the actions in this request. \n# Query document\nmutation CreateAndDeleteProduct(     \n  $name: String!\n  $type: ProductType!\n  $input: AddProductInput!\n  $id: ID!\n) {\n  addProduct(name: $name, type: $type, input: $input) {     \n    ...commonProperties      \n  }\n  deleteProduct(id: $id)     \n}\nfragment commonProperties on ProductInterface {\n  name\n}\nListing 9.20\nUsing parameterized syntax\nQuery wrapper for\nparameterization\nmutation CreateProduct( $name: String!, $type: ProductType!, $input: AddProductInput! ) {\n...commonProperties\n}\n}\n{\nQuery parameters\n\"type\": \"beverage\",\n\"name\": “Mocha\",\naddProduct( name: $name type: $type, input: $input, ) {\nWe declare the operation as a mutation.\nName of the\nfunction wrapper\nThe signature of the function\nwrapper includes the\nparameterized arguments\nwith their types.\nThe parameterized\narguments\nCall to the addProduct mutation\nwith parameterized arguments\nThe parameterized arguments are\nused when calling the mutation.\n\"input\": {\n\"price\": 10,\n\"size\": \"BIG\",\n\"ingredients\": [1, 2]\n}\n}\nFigure 9.6\nTo parameterize queries and mutations, we create a function wrapper around the query or \nmutation. In the wrapper’s signature we include the parameterized arguments. Parameterized variables \ncarry a leading dollar ($) sign.\nWe created a wrapper named \nCreateAndDeleteProduct().\nWe call the \naddProduct() \nmutation.\nWe select properties \nusing a named fragment.\nWe call the deleteProduct() \nmutation.",
      "content_length": 1708,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "229\n9.9\nDemystifying GraphQL queries\n# Query variables\n{\n  \"name\": \"Mocha\",      \n  \"type\": \"beverage\",\n  \"input\": {\n    \"price\": 10,\n    \"size\": \"BIG\",\n    \"ingredients\": [{\"ingredient\": 1, \"quantity\": 1, \"unit\": \"LITERS\"}]\n  },\n  \"id\": \"asdf\"     \n}\nThis completes our journey through learning how to consume GraphQL APIs. You\ncan now inspect any GraphQL API, explore its types, and play around with its queries\nand mutations. Before we close this chapter, I’d like to show you how a GraphQL API\nrequest works under the hood. \n9.9\nDemystifying GraphQL queries\nThis section explains how GraphQL queries work under the hood in the context of\nHTTP requests. In previous sections, we used the GraphiQL client to explore our\nGraphQL API and to interact with it. GraphiQL translates our query documents into\nHTTP requests that the GraphQL server understands. GraphQL clients such as\nGraphiQL are interfaces that make it easier to interact with a GraphQL API. But noth-\ning prevents you from sending an HTTP request directly to the API, say, from your ter-\nminal, using something like cURL. Contrary to a popular misconception, you don’t\nreally need any special tools to work with GraphQL APIs.1\n To send a request to a GraphQL API, you can use either of the GET or POST\nmethods. If you use GET, you send your query document using URL query parame-\nters, and if you use POST, you include the query in the request payload. GraphQL\nFaker’s mock server only accepts GET requests, so I’ll illustrate how you send a query\nusing GET.\n Let’s run the allIngredients() query, selecting only the name property of each\ningredient. Since this is a GET request, our query document must be included in the\nURL as a query parameter. However, the query document contains special characters,\nsuch as curly braces, which are considered unsafe and therefore cannot be included\nin a URL. To deal with special characters in URLs, we URL encode them. URL encod-\ning is the process of translating special characters, such as braces, punctuation marks,\nand others, into a suitable format for URLs. URL-encoded characters start with a\n1 Unless you want to use subscriptions (connections with the GraphQL server that allow you to receive notifica-\ntions when something happens in the server, e.g., when the state of a resource changes). Subscriptions require\na two-way connection with the server, so you need something more sophisticated than cURL. To learn more\nabout GraphQL subscriptions, see Eve Porcello and Alex Banks, Learning GraphQL, Declarative Data Fetching for\nModern Web Apps (O’Reilly, 2018), pp. 50–53 and 150–160.\nWe assign values \nto addProduct()’s \nparameters.\nWe set the value for \ndeleteProduct()’s id \nparameter.",
      "content_length": 2702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "230\nCHAPTER 9\nConsuming GraphQL APIs\npercent sign, so this type of encoding is also known as percent encoding.2 cURL takes care\nof URL encoding our data when we use the --data-urlencode option. By using --data-\nurlencode, cURL translates our command into a GET request with the following\nURL: http:/ /localhost:9002/graphql?query=%7BallIngredients%7Bname%7D%7D.\nThe following snippet shows the cURL command you need to run to make this call:\n$ curl http:/ /localhost:9002/graphql --data-urlencode \\\n'query={allIngredients{name}}'\nNow that you understand how GraphQL API requests work under the hood, let’s see\nhow we can leverage this knowledge to write code in Python that consumes a\nGraphQL API. \n9.10\nCalling a GraphQL API with Python code\nThis section illustrates how we can interact with a GraphQL API using Python.\nGraphQL clients like GraphiQL are useful to explore and get familiar with a\nGraphQL API, but in practice, you’ll spend most of your time writing applications that\nconsume those APIs programmatically. In this section, we learn to consume the prod-\nucts API using a GraphQL client written in Python.\n To work with GraphQL APIs, the Python ecosystem offers libraries such as gql\n(https://github.com/graphql-python/gql) and sgqlc (https://github.com/profusion/\nsgqlc). These libraries are useful when we want to use advanced features of GraphQL,\nsuch as subscriptions. You’ll rarely need those features in the context of microservices,\nso for the purposes of this section, we’ll take a simpler approach and use the popular\nrequests library (https://github.com/psf/requests). Remember that GraphQL que-\nries are simply GET or POST requests with a query document.\n Listing 9.21 shows how we call the allIngredients() query, adding a selector\nfor Ingredient’s name property. The listing is also available under ch09/client.py in\nthis book’s GitHub repository. Since our GraphQL mock server only accepts GET\nrequests, we send the query document in the form of URL-encoded data. With requests,\nwe accomplish this by passing the query document through the get method’s params\nargument. As you can see, the query document looks the same as what we wrote in\nthe GraphiQL query panel, and the result from the API also looks the same. This is\ngreat news, because it means that, when working out your queries, you can start\nworking with GraphiQL, leveraging its great support for syntax highlighting and\nquery validation, and when you’re ready, you can move your queries directly to your\nPython code.\n \n \n \n2 Tim Berners-Lee, R. Fielding, and L. Masinter, “Uniform Resource Identifer (URI): Generic Syntax,” RFC\n3986, section 2.1, https://datatracker.ietf.org/doc/html/rfc3986#section-2.1.",
      "content_length": 2695,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "231\nSummary\n# file: ch09/client.py \nimport requests   \nURL = 'http:/ /localhost:9002/graphql'    \nquery_document = '''     \n{\n  allIngredients {\n    name\n  }\n}\n'''\nresult = requests.get(URL, params={'query': query_document})    \nprint(result.json())    \n# Result\n{'data': {'allIngredients': [{'name': 'string'}, {'name': 'string'}, \n➥ {'name': 'string'}]}}\nThis concludes our journey through GraphQL. You went from learning about the basic\nscalar types supported by GraphQL in chapter 8 to making complex queries using tools\nas varied as GraphiQL, cURL, and Python in this chapter. Along the way, we built the\nspecification for the products API, and we interacted with it using a GraphQL mock\nserver. That’s no small feat. If you’ve read this far, you’ve learned a great deal of things\nabout APIs, and you should be proud of it!\n GraphQL is one of the most popular protocols in the world of web APIs, and its\nadoption grows every year. GraphQL is a great choice for building microservices APIs\nand for integration with frontend applications. In the next chapter, we’ll undertake\nthe actual implementation of the products API and its service. Stay tuned!\nSummary\nWhen we call a query or mutation that returns an object type, our query must\ninclude a selection set. A selection set is a list of the properties we want to fetch\nfrom the object returned by the query.\nWhen a query or mutation returns a list of multiple types, our selection set must\ninclude fragments. Fragments are selections of properties on a specific type,\nand they’re prefixed by the spread operator (three dots).\nWhen calling a query or mutation that includes arguments, we can parameter-\nize those arguments by building a wrapper around the query or queries. This\nallows us to write more readable and maintainable query documents.\nWhen designing a GraphQL API, it’s a good idea to put it to work with a mock\nserver, which allows us to build API clients while the server is implemented.\nListing 9.21\nCalling a GraphQL query using Python\nWe import the \nrequests library.\nThe base URL of our \nGraphQL server\nThe query \ndocument \nWe send a GET request to the\nserver with the query document\nas a URL query parameter.\nWe parse and print the JSON \npayload returned by the server.",
      "content_length": 2246,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "232\nCHAPTER 9\nConsuming GraphQL APIs\nYou can run a GraphQL mock server using graphql-faker, which also creates a\nGraphiQL interface to the API. This is useful to test that our design conforms\nto our expectations.\nBehind the scenes, a GraphQL query is a simple HTTP request that uses either\nof the GET or POST methods. When using GET, we must ensure our query\ndocument is URL encoded, and when using POST, we include it in the request\npayload.",
      "content_length": 444,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "233\nBuilding GraphQL\nAPIs with Python\nIn chapter 8, we designed a GraphQL API for the products service, and we pro-\nduced a specification detailing the requirements for the products API. In this chap-\nter, we implement the API according to the specification. To build the API, we’ll\nuse the Ariadne framework, which is one of the most popular GraphQL libraries in\nthe Python ecosystem. Ariadne allows us to leverage the benefits of documentation-\ndriven development by automatically loading data validation models from the spec-\nification. We’ll learn to create resolvers, which are Python functions that imple-\nment the logic of a query or mutation. We’ll also learn to handle queries that\nreturn multiple types. After reading this chapter, you’ll have all the tools you need\nto start developing your own GraphQL APIs!\nThis chapter covers\nCreating GraphQL APIs using the Ariadne web \nserver framework\nValidating request and response payloads\nCreating resolvers for queries and mutations\nCreating resolvers for complex object types, \nsuch as union types\nCreating resolvers for custom scalar types \nand object properties",
      "content_length": 1124,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "234\nCHAPTER 10\nBuilding GraphQL APIs with Python\n The code for this chapter is available in the GitHub repository provided with this\nbook, under the folder ch10. Unless otherwise specified, all the file references within\nthis chapter are relative to the ch10 folder. For example, server.py refers to the\nch10/server.py file, and web/schema.py refers to the ch10/web/schema.py file. Also,\nto ensure all the commands used in this chapter work as expected, use the cd com-\nmand to move the ch10 folder in your terminal.\n10.1\nAnalyzing the API requirements\nIn this section, we analyze the requirements of the API specification. Before jumping\ninto implementing an API, it’s worth spending some time analyzing the API specifica-\ntion and what it requires. Let’s do this analysis for the products API!\n The products API specification is available under ch10/web/products.graphql in\nthe GitHub repository for this book. The specification defines a collection of object\ntypes that represent the data we can retrieve from the API and a set of queries and\nmutations that expose the capabilities of the products service. We must create vali-\ndation models that faithfully represent the schemas defined in the specification, as\nwell as functions that correctly implement the functionality of the queries and muta-\ntions. We’ll work with a framework that can handle schema validation automatically\nfrom the specification, so we don’t need to worry about implementing validation\nmodels.\n Our implementation will focus mainly on the queries and mutations. Most of the\nqueries and mutations defined in the schema return either an array or a single\ninstance of the Ingredient and Product types. Ingredient is simpler since it’s an\nobject type, so we’ll look at queries and mutations that use this type first. Product is\nthe union of the Beverage and Cake types, both of which implement the Product-\nInterface type. As we’ll see, implementing queries and mutations that return union\ntypes is slightly more complex. A query that returns a list of Product objects contains\ninstances of both the Beverage and Cake types, so we need to implement additional\nfunctionality that makes it possible for the server to determine which type each ele-\nment in the list belongs to.\n With that said, let’s analyze the tech stack that we’ll use for this chapter, and then\nmove straight into the implementation! \n10.2\nIntroducing the tech stack\nIn this section, we discuss the tech stack that we’ll use to implement the products API.\nWe discuss which libraries are available for implementing GraphQL APIs in Python,\nand we choose one of them. We also discuss the server framework that we’ll use to run\nthe application.\n Since we’re going to implement a GraphQL API, the first thing we want to look for\nis a good GraphQL server library. GraphQL’s website (https://graphql.org/code/) is\nan excellent resource for finding tools and frameworks for the GraphQL ecosystem.\nAs the ecosystem is constantly evolving, I recommend you check out that website every",
      "content_length": 3017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "235\n10.3\nIntroducing Ariadne\nonce in a while for any new additions. The website lists four Python libraries that sup-\nport GraphQL: \nGraphene (https://github.com/graphql-python/graphene) is one of the first\nGraphQL libraries built for Python. It’s battle tested and one of the most widely\nused libraries.\nAriadne (https://github.com/mirumee/ariadne) is a library built for schema-\nfirst (or documentation-driven) development. It’s a highly popular framework,\nand it handles schema validation and serialization automatically.\nStrawberry (https://github.com/strawberry-graphql/strawberry) is a more recent\nlibrary that makes it easy to implement GraphQL schema models by offering a\nclean interface inspired by Python data classes.\nTartiflette (https://github.com/tartiflette/tartiflette) is another recent addition\nto the Python ecosystem that allows you to implement a GraphQL server using a\nschema-first approach, and it’s built on top of asyncio, which is Python’s core\nlibrary for asynchronous programming.\nFor this chapter, we’ll use Ariadne, since it supports a schema-first or documentation-\ndriven development approach, and it’s a mature project. The API specification is\nalready available, so we don’t want to spend time implementing each schema model in\nPython. Instead, we want to use a library that can handle schema validation and serial-\nization directly from the API specification, and Ariadne can do that.\n We’ll run the Ariadne server with the help of Uvicorn, which we encountered in\nchapters 2 and 6 when we worked with FastAPI. To install the dependencies for this\nchapter, you can use the Pipfile and Pipfile.lock files available under the ch10 folder\nin the repository provided with this book. Copy the Pipfile and Pipfile.lock files into\nyour ch10 folder, cd into it, and run the following command:\npipenv install\nIf you prefer to install the latest versions of Ariadne and Uvicorn, simply run\npipenv install ariadne uvicorn\nNow that we have the dependencies installed, let’s activate the environment:\npipenv shell\nWith all the dependencies installed, now we are ready to start coding, so let’s do it! \n10.3\nIntroducing Ariadne\nIn this section, we introduce the Ariadne framework, and we learn how it works by\nusing a simple example. We’ll learn how to run a GraphQL server with Ariadne, how\nto load a GraphQL specification, and how to implement a simple GraphQL resolver.\nAs we saw in chapter 9, users interact with GraphQL APIs by running queries and",
      "content_length": 2477,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "236\nCHAPTER 10\nBuilding GraphQL APIs with Python\nmutations. A GraphQL resolver is a function that knows how to execute one of those\nqueries or mutations. In our implementation, we’ll have as many resolvers as queries\nand mutations there are in the API specification. As you can see from figure 10.1,\nresolvers are the pillars of a GraphQL server since it’s through resolvers that we can\nreturn actual data to the API users.\nLet’s start by writing a very simple GraphQL schema. Open the server.py file and copy\nthe following content into it:\n# file: server.py\nschema = '''\n  type Query {\n    hello: String\n  }\n'''\nWe define a variable called schema, and we point it to a simple GraphQL schema. This\nschema defines only one query, named hello(), which returns a string. The return\nvalue of the hello() query is optional, which means null is also a valid return value.\nTo expose this query through our GraphQL server, we need to implement a resolver\nusing Ariadne.\nUser\nGraph server\nquery {\nallIngredients {\nname\n}\n}\nResolvers\nallIngredients\nallProducts\nproducts\ningredients\n...\nRequest\nResponse\n{\n“data”: {\n“allIngredients”: [\n{\n“name”: “string”\n}\n]\n}\n}\nFigure 10.1\nTo serve data to a user, a GraphQL server uses resolvers, which are functions that \nknow how to build the payload for a given query.",
      "content_length": 1296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "237\n10.3\nIntroducing Ariadne\n Ariadne can run a GraphQL server from this simple schema definition. How do\nwe do that? First, we need to load the schema using Ariadne’s make_executable_\nschema() function. make_executable_schema() parses the document, validates our\ndefinitions, and builds an internal representation of the schema. As you can see in\nfigure 10.2, Ariadne uses the output of this function to validate our data. For exam-\nple, when we return the payload for a query, Ariadne validates the payload against\nthe schema.\nOnce we’ve loaded the schema, we can initialize our server using Ariadne’s GraphQL\nclass (listing 10.1). Ariadne provides two implementations of the server: a synchronous\nimplementation, which is available under the ariande.wsgi module, and an asyn-\nchronous implementation, which is available under the ariande.asgi module. In this\nchapter, we’ll use the asynchronous implementation. \nmake_executable_schema()\nGraphQL chema\ns\nproducts.graphql\nAriadne resolvers\nGraphQL server\nExecutable schema\n(resolvers and data validation)\nHTTP layer\nUser\nController\nValidates data sent\nby the user\nValidates data sent\nto the user\nDatabase\nFigure 10.2\nTo run the GraphQL server with Ariadne, we produce an executable \nschema by loading the GraphQL schema for the API and a collection of resolvers \nfor the queries and mutations. Ariadne uses the executable schema to validate \ndata the user sent to the server, as well as data sent from the server to the user.",
      "content_length": 1476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "238\nCHAPTER 10\nBuilding GraphQL APIs with Python\n# file: server.py\nfrom ariadne import make_executable_schema\nfrom ariadne.asgi import GraphQL\nschema = '''\n  type Query {     \n    hello: String\n  }\n'''\nserver = GraphQL(make_executable_schema(schema), debug=True)  \nTo run the server, execute the following command from the terminal:\n$ uvicorn server:server --reload\nYour application will be available on http:/ /localhost:8000. If you head over to that\naddress, you’ll see an Apollo Playground interface to the application. As you can see\nin figure 10.3, Apollo Playground is similar to GraphiQL, which we learned in chapter 8.\nOn the left-side panel, we write our queries. Write the following query:\n{\n  hello\n}\nListing 10.1\nInitializing a GraphQL server using Ariadne\nWe declare a \nsimple schema.\nWe instantiate \nthe GraphQL \nserver.\nQuery panel\nResults panel\nDocumentation panel\nPlay button\nFigure 10.3\nThe Apollo Playground interface contains a query panel where we execute queries and \nmutations; a results panel where the queries and mutations are evaluated; and a documentation panel \nwhere we can inspect the API schemas.",
      "content_length": 1129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "239\n10.3\nIntroducing Ariadne\nThis query executes the query function that we defined in listing 10.1. If you press the\nexecute button, you’ll get the results of this query on the right-side panel:\n{\n  \"data\": {\n    \"hello\": null\n  }\n}\nThe query returns null. This shouldn’t come as a surprise, since the return value of\nthe hello() query is a nullable string. How can we make the hello() query return a\nstring? Enter resolvers. Resolvers are functions that let the server know how to produce\na value for a type or an attribute. To make the hello() query return an actual string,\nwe need to implement a resolver. Let’s create a resolver that returns a string of 10 ran-\ndom characters.\n In Ariadne, a resolver is a Python callable (e.g., a function) that takes two posi-\ntional parameters: obj and info. \nResolver parameters in Ariadne\nAriadne’s resolvers always have two positional-only parameters, which are commonly\ncalled obj and info. The signature of a basic Ariadne resolver is\ndef simple_resolver(obj: Any, info: GraphQLResolveInfo):\n  pass\nAs you can see in the figure, obj will normally be set to None, unless the resolver has\na parent resolver, in which case obj will be set to the value returned by the parent\nresolver. We encounter the latter case when a resolver doesn’t return an explicit type.\nFor example, the resolver for the allProducts() query, which we’ll implement in\nsection 10.4.4, doesn’t return an explicit type. It returns an object of type Product,\nwhich is the union of the Cake and Beverage types. To determine the type of each\nobject, Ariadne needs to call a resolver for the Product type.\nsimple_resolver(obj=None,\ninfo={...})\ntype_resolver(obj={...}, info={...})\nParent resolver: None\nParent resolver:\nresolve_all_products(obj, info)\nWhen the parent\nresolver is None,\nobj will be set to\nNone.\nIf there’s a parent\nresolver, obj will be\nset to the return\nvalue of the parent\nresolver.\nWhen a resolver \ndoesn't have a parent \nresolver, the obj \nparameter is set to \nNone. When there's a \nparent resolver, obj \nwill be set to the \nvalue returned by the \nparent resolver.",
      "content_length": 2097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "240\nCHAPTER 10\nBuilding GraphQL APIs with Python\nA resolver needs to be bound to its corresponding object type. Ariadne provides bind-\nable classes for each GraphQL type:\n\nObjectType for object types.\n\nQueryType for query types. In GraphQL, the query type represents the collec-\ntion of all queries available in a schema. As we saw in chapter 8 (section 8.8), a\nquery is a function that reads data from a GraphQL server.\n\nMutationType for mutation types. As we saw in chapter 8 (section 8.9), a muta-\ntion is a function that alters the state of the GraphQL server.\n\nUnionType for union types.\n\nInterfaceType for interface types.\n\nEnumType for enumeration types.\nSince hello() is a query, we need to bind its resolver to an instance of Ariadne’s Query-\nType. Listing 10.2 shows how we do that. We first create an instance of the QueryType\nclass and assign it to a variable called query. We then use QueryType’s field() decora-\ntor method to bind our resolver, which is available on most of Ariadne’s bindable\nclasses and allows us to bind a resolver to a specific field. By convention, we prefix our\nresolvers’ names with resolve_. Ariadne’s resolvers always get two positional-only\nparameters by default: obj and info. We don’t need to make use of those parameters\nin this case, so we use a wildcard followed by an underscore (*_), which is a conven-\ntion in Python to ignore a list of positional parameters. To make Ariadne aware of our\nresolvers, we need to pass our bindable objects as an array to the make_executable_\nschema() function. The changes go under server.py.\n# file: server.py\nimport random\nimport string\nfrom ariadne import QueryType, make_executable_schema\nfrom ariadne.asgi import GraphQL\nquery = QueryType()     \n(continued)\nThe info parameter is an instance of GraphQLResolveInfo, which contains informa-\ntion required to execute a query. Ariadne uses this information to process and serve\neach request. For the application developer, the most interesting attribute exposed\nby the info object is info.context, which contains details about the context in\nwhich the resolver is called, such as the HTTP context. To learn more about the obj\nand info objects, check out Ariadne’s documentation: https://ariadnegraphql.org/\ndocs/resolvers.html.\nListing 10.2\nImplementing a GraphQL resolver with Ariadne\nInstance of \nQueryType",
      "content_length": 2345,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "241\n10.4\nImplementing the products API\n@query.field('hello')  \ndef resolve_hello(*_):     \n    return ''.join(\n        random.choice(string.ascii_letters) for _ in range(10)    \n    )\nschema = '''\ntype Query {     \n        hello: String\n    }\n'''\nserver = GraphQL(make_executable_schema(schema, [query]), debug=True)    \nSince we’re running the server with the hot reloading flag (--reload), the server\nautomatically reloads once you save the changes to the file. Go back to the Apollo\nPlayground interface in http://127.0.0.1:8000 and run the hello() query again. This\ntime, you should get a random string of 10 characters as a result.\n This completes our introduction to Ariadne. You’ve learned how to load a GraphQL\nschema with Ariadne, how to run the GraphQL server, and how to implement a resolver\nfor a query function. In the rest of the chapter, we’ll apply this knowledge as we build\nthe GraphQL API for the products service.\n10.4\nImplementing the products API\nIn this section, we’ll use everything we learned in the previous section to build the\nGraphQL API for the products service. Specifically, you’ll learn to build resolvers for\nthe queries and mutations of the products API, to handle query parameters, and to\nstructure your project. Along the way, we’ll learn additional features of the Ariadne\nframework and various strategies for testing and implementing GraphQL resolvers. By\nthe end of this section, you’ll be able to build GraphQL APIs for your own microser-\nvices. Let the journey begin!\n10.4.1 Laying out the project structure\nIn this section, we structure our project for the products API implementation. So far,\nwe’ve included all our code under the server.py file. To implement a whole API, we\nneed to split our code into different files and add structure to the project; otherwise,\nthe codebase would become difficult to read and to maintain. To keep the implemen-\ntation simple, we’ll use an in-memory representation of our data.\n If you followed along with the code in the previous section, delete the code we\nwrote earlier under server.py, which represents the entry point to our application and\ntherefore will contain an instance of the GraphQL server. We’ll encapsulate the web\nWe bind a resolver for the hello() query \nusing QueryType’s field() decorator.\nWe skip \npositional-only \nparameters.\nWe return a list of\nrandomly generated\nASCII characters.\nWe declare \nour GraphQL \nschema.\nInstance of\nthe GraphQL\nserver",
      "content_length": 2447,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "242\nCHAPTER 10\nBuilding GraphQL APIs with Python\nserver implementation within a folder called web/. Create this folder, and within it,\ncreate the following files:\ndata.py will contain the in-memory representation of our data.\nmutations.py will contain resolvers for the mutations in the products API.\nqueries.py will contain resolvers for queries.\nschema.py will contain all the code necessary to load an executable schema.\ntypes.py will contain resolvers for object types, custom scalar types, and object\nproperties.\nThe products.graphql specification file also goes under the web folder, since it’s han-\ndled by the code under the web/schema.py file. You can copy the API specification\nfrom the ch10/web/products.graphql file in the GitHub repository for this book. The\ndirectory structure for the products API looks like this:\n.\n├── Pipfile\n├── Pipfile.lock\n├── server.py\n└── web\n    ├── data.py\n    ├── mutations.py\n    ├── products.graphql\n    ├── queries.py\n    ├── schema.py\n    └── types.py\nThe GitHub repository for this book contains an additional module called excep-\ntions.py, which you can check for examples of how to handle exceptions in your\nGraphQL APIs. Now that we have structured our project, it’s time to start coding!\n10.4.2 Creating an entry point for the GraphQL server\nNow that we have structured our project, it’s time to work on the implementation. In\nthis section, we’ll create the entry point for the GraphQL server. We need to create an\ninstance of Ariadne’s GraphQL class and load an executable schema from the products\nspecification.\n As we mentioned in section 10.4.1, the entry point for the products API server lives\nunder server.py. Include the following content in this file:\n# file: server.py\nfrom ariadne.asgi import GraphQL\nfrom web.schema import schema\nserver = GraphQL(schema, debug=True)\nNext, let’s create the executable schema under web/schema.py:",
      "content_length": 1897,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "243\n10.4\nImplementing the products API\n# file: web/schema.py\nfrom pathlib import Path\nfrom ariadne import make_executable_schema\nschema = make_executable_schema(\n    (Path(__file__).parent / 'products.graphql').read_text()\n)\nThe API specification for the products API is available under the web/products.graphql\nfile. We read the schema file contents and pass them on to Ariadne’s make_executable_\nschema() function. We then pass the resulting schema object to Ariadne’s GraphQL\nclass to instantiate the server. If you haven’t started the server, you can do it now by\nexecuting the following command:\n$ uvicorn server:server --reload\nLike before, the API is available on http:/ /localhost:8000. If you visit this address\nagain, you’ll see the familiar Apollo Playground UI. At this point, we could try run-\nning any of the queries defined in the products API specification; however, most of\nthem will fail since we haven’t implemented any resolvers. For example, if you run the\nfollowing query\n{\n  allIngredients {\n    name\n  }\n}\nyou’ll get the following error message: “Cannot return null for non-nullable field\nQuery.allProducts.” The server doesn’t know how to produce a value for the Ingredient\ntype since we don’t have a resolver for it, so let’s build it!\n10.4.3 Implementing query resolvers\nIn this section, we learn to implement query resolvers. As you can see from figure 10.4, a\nquery resolver is a Python function that knows how to return a valid payload for a\ngiven query. We’ll build a resolver for the allIngredients() query, which is one of\nthe simplest queries in the products API specification (listing 10.3).\n To implement a resolver for the allIngredients() query, we simply need to cre-\nate a function that returns a data structure with the shape of the Ingredient type,\nwhich has four non-nullable properties: id, name, stock, and products. The stock\nproperty is, in turn, an instance of the Stock object type, which, as per the specifica-\ntion, must contain the quantity and unit properties. Finally, the products property\nmust be an array of Product objects. The contents of the array are non-nullable, but\nan empty array is a valid return value.",
      "content_length": 2169,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "244\nCHAPTER 10\nBuilding GraphQL APIs with Python\n# file: web/products.graphql\ntype Stock {     \n    quantity: Float!     \n    unit: MeasureUnit!\n}\ntype Ingredient {\n    id: ID!\n    name: String!\n    stock: Stock!\n    products: [Product!]!     \n    supplier: Supplier      \n    description: [String!]\n    lastUpdated: Datetime!\n}\nLet’s add a list of ingredients to the in-memory list representation of our data under\nthe web/data.py file:\nListing 10.3\nSpecification for the Ingredient type\nSchema\nPayload\nResolver\nQuery\nallIngredients: [Ingredient!]!\ntype Ingredient {\nid: ID!\nname: String!\nstock: Stock!\nproducts: [Product!]!\nsupplier: Supplier\ndescription: [String]\nlastUpdated: Datetime!\n}\ndef resolve_all_ingredients(obj, info):\npass\n{\n‘id’: ‘602f2ab3-97bd-468e-a88b-bb9e00531fd0’,\n‘name’: ‘Milk’,\n‘stock’: {\n‘quantity’: 100.00,\n‘unit’: ‘LITRES’,\n},\n‘products’: [],\n‘lastUpdated’: ‘2025-07-05T18:58:02’\n},\nresolve_all_ingredients\nis a resolver for the\nallIngredients query.\nThe allIngredients query\nmust return an array of\nIngredient objects.\nThe data generated by the\nresolver matches the\nrequirements of the schema.\nFigure 10.4\nGraphQL uses resolvers to serve the query requests sent by the user to the \nserver. A resolver is a Python function that knows how to return a valid payload for a given \nquery.\nWe declare the \nStock type.\nquantity is a non-\nnullable float.\nproducts is a non-\nnullable list of \nproducts.\nsupplier is a nullable \nthrough type that points \nto the Supplier type.",
      "content_length": 1491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "245\n10.4\nImplementing the products API\n# file: web/data.py\nfrom datetime import datetime\ningredients = [\n    {\n        'id': '602f2ab3-97bd-468e-a88b-bb9e00531fd0',\n        'name': 'Milk',\n        'stock': {\n            'quantity': 100.00,\n            'unit': 'LITRES',\n        },\n        'supplier': '92f2daae-a4f8-4aae-8d74-51dd74e5de6d',\n        'products': [],\n        'lastUpdated': datetime.utcnow(),\n    },\n]\nNow that we have some data, we can use it in the allIngredients()’ resolver. List-\ning 10.4 shows what allIngredients()’ resolver looks like. As we did in section 10.3,\nwe first create an instance of the QueryType class, and we bind the resolver with this\nclass. Since this is a resolver for a query type, the implementation goes under the\nweb/queries.py file.\n# file: web/queries.py\nfrom ariadne import QueryType\nfrom web.data import ingredients\nquery = QueryType()\n@query.field('allIngredients')   \ndef resolve_all_ingredients(*_):\n    return ingredients    \nTo enable the query resolver, we have to pass the query object to the make_executable_\nschema() function under web/schema.py:\n# file: web/schema.py\nfrom pathlib import Path\nfrom ariadne import make_executable_schema\nfrom web.queries import query\nschema = make_executable_schema(\n    (Path(__file__).parent / 'products.graphql').read_text(), [query]\n)\nListing 10.4\nA resolver for the allIngredients() query\nWe bind allIngredients()’ \nresolver using the \ndecorator.\nWe return a \nhardcoded response.",
      "content_length": 1473,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "246\nCHAPTER 10\nBuilding GraphQL APIs with Python\nIf we go back to the Apollo Playground UI and we run the query\n{\n  allIngredients {\n    name\n  }\n}\nwe get a valid payload. The query selects only the ingredient’s name, which in itself is\nnot very interesting, and it doesn’t really tell us whether our current resolver works as\nexpected for other fields. Let’s write a more complex query to test our resolver more\nthoroughly. The following query selects the id, name, and description of an ingredi-\nent, as well as the name of each product it’s related to:\n{\n  allIngredients {\n    id,\n    name,\n    products {\n      ...on ProductInterface {\n        name\n      }\n    },\n    description\n  }\n}\nThe response payload to this query is also valid:\n{\n  \"data\": {\n    \"allIngredients\": [\n      {\n        \"id\": \" \"602f2ab3-97bd-468e-a88b-bb9e00531fd0\",\n        \"name\": \"Milk\",\n        \"products\": [],\n        \"description\": null\n      }\n    ]\n  }\n}\nThe products list is empty because we haven’t associated any products with the ingre-\ndient, and description is null because this is a nullable field. Now that we know how\nto implement resolvers for simple queries, in the next section, we’ll learn to imple-\nment resolvers that handle more complex situations.\n10.4.4 Implementing type resolvers\nIn this section, we’ll learn to implement resolvers for queries that return multiple\ntypes. The allIngredients() query is fairly simple since it only returns one type of\nobject: the Ingredient type. Let’s now consider the allProducts() query. As you can",
      "content_length": 1537,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "247\n10.4\nImplementing the products API\nsee from figure 10.5, allProducts() is more complex since it returns the Product\ntype, which is a union of the Beverage and Cake types, both of which implement the\nProductInterface type.\nLet’s begin by adding a list of products to our in-memory list of data under the\nweb/data.py file. We’ll add two products: one Beverage and one Cake. What fields\nshould we include in the products? As you can see in figure 10.6, since Beverage and\nCake implement the ProductInterface type, we know they both require an id, a\nname, a list of ingredients, and a field called available, which signals if the product\nis available. On top of these common fields inherited from ProductInterface, Beverage\nrequires two additional fields: hasCreamOnTopOption and hasServeOnIceOption,\nallIngredients: [Ingredient!]!\nallProducts: [Product!]!\ntype Ingredient {\nid: ID!\nname: String!\nstock: Stock!\nproducts: [Product!]!\nsupplier: Supplier\ndescription: [String!]\nlastUpdated: Datetime!\n}\ntype Beverage implements ProductInterface {\nid: ID!\nname: String!\nprice: Float\nsize: Sizes\ningredients: [IngredientRecipe!]!\navailable: Boolean!\nlastUpdated: Datetime!\nhasCreamOnTopOption: Boolean!\nhasServeOnIceOption: Boolean!\n}\ntype Cake implements ProductInterface {\nid: ID!\nname: String!\nprice: Float\nsize: Sizes\ningredients: [IngredientRecipe!]\navailable: Boolean!\nlastUpdated: Datetime!\nhasFilling: Boolean!\nhasNutsToppingOption: Boolean!\n}\nunion Product = Beverage | Cake\nFigure 10.5\nThe allIngredients() query returns an array of Ingredient \nobjects, while the allProducts() query returns an array of Product objects, \nwhere Product is the union of two types: Beverage and Cake.",
      "content_length": 1686,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "248\nCHAPTER 10\nBuilding GraphQL APIs with Python\nboth of which are Booleans. In turn, Cake requires the properties hasFilling and\nhasNutsToppingOption, which are also Booleans.\n# file: web/data.py\n...\nproducts = [\n    {\n        'id': '6961ca64-78f3-41d4-bc3b-a63550754bd8',\n        'name': 'Walnut Bomb',\n        'price': 37.00,\n        'size': 'MEDIUM',\nListing 10.5\nResolver for the allProducts() query\ntype\nimplements\n{\nBeverage\nProductInterface\n}\nunion\n= Beverage | Cake\nProduct\ninterface\n{\nProductInterface\nid: ID!\nname: String!\nprice: Float\nsize: Sizes\ningredients: [IngredientRecipe!]\navailable: Boolean!\nlastUpdated: Datetime!\n}\nhasCreamOnTopOption: Boolean!\nhasServeOnIceOption: Boolean!\nid: ID!\nname: String!\nprice: Float\nsize: Sizes\ningredients: [IngredientRecipe!]\navailable: Boolean!\nlastUpdated: Datetime!\ntype\nimplements\n{\nCake\nProductInterface\n}\nhasFilling: Boolean!\nhasNutsToppingOption: Boolean!\nid: ID!\nname: String!\nprice: Float\nsize: Sizes\ningredients: [IngredientRecipe!]\navailable: Boolean!\nlastUpdated: Datetime!\nFigure 10.6\nProduct is the union of the Beverage and the Cake types, both of which implement the \nProductInterface type. Since Beverage and Cake implement the same interface, both types \nshare the properties inherited from the interface. In addition to those properties, each type has its own \nspecific properties, such as hasFilling in the case of the Cake type.",
      "content_length": 1400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "249\n10.4\nImplementing the products API\n        'available': False,\n        'ingredients': [\n            {\n                'ingredient': '602f2ab3-97bd-468e-a88b-bb9e00531fd0',   \n                'quantity': 100.00,\n                'unit': 'LITRES',\n            }\n        ],\n        'hasFilling': False,\n        'hasNutsToppingOption': True,\n        'lastUpdated': datetime.utcnow(),\n    },\n    {\n        'id': 'e4e33d0b-1355-4735-9505-749e3fdf8a16',\n        'name': 'Cappuccino Star',\n        'price': 12.50,\n        'size': 'SMALL',\n        'available': True,\n        'ingredients': [\n            {\n                'ingredient': '602f2ab3-97bd-468e-a88b-bb9e00531fd0',\n                'quantity': 100.00,\n                'unit': 'LITRES',\n            }\n        ],\n        'hasCreamOnTopOption': True,\n        'hasServeOnIceOption': True,\n        'lastUpdated': datetime.utcnow(),\n    },\n]\nNow that we have a list of products, let’s use it in the allProducts()’ resolver. \n# file: web/queries.py\nfrom ariadne import QueryType\nfrom web.data import ingredients, products\nquery = QueryType()\n...\n@query.field('allProducts')    \ndef resolve_all_products(*_):\n    return products      \nLet’s run a simple query to test the resolver:\n{\n  allProducts {\nListing 10.6\nAdding the allProducts() resolver\nThis ID references\nthe ID of the milk\ningredient we added\nearlier to web/data.py.\nWe bind allProducts()’ \nresolver using the field() \ndecorator.\nWe return a \nhardcoded response.",
      "content_length": 1470,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "250\nCHAPTER 10\nBuilding GraphQL APIs with Python\n    ...on ProductInterface {\n      name\n    }\n  }\n}\nIf you run this query, you’ll get an error saying that the server can’t determine what\ntypes each of the elements in our list are. In these situations, we need a type resolver.\nAs you can see in figure 10.7, a type resolver is a Python function that determines what\ntype an object is, and it returns the name of the type.\nWe need type resolvers in queries and mutations that return more than one object\ntype. In the products API, this affects all queries and mutations that return the Prod-\nuct type, such as allProducts(), addProduct(), and product().\nRETURNING MULTIPLE TYPES\nWhenever a query or mutation returns multiple\ntypes, you’ll need to implement a type resolver. This applies to queries and\nmutations that return union types and object types that implement interfaces.\nListing 10.7 shows how we implement a type resolver for the Product type in Ariadne.\nThe type resolver function takes two positional parameters, the first of which is an\nobject. We need to determine the type of this object. As you can see in figure 10.8,\nsince we know that Cake and Beverage have different required fields, we can use this\ninformation to determine their types: if the object has a hasFilling property, we\nknow it’s a Cake; otherwise, it’s a Beverage.\nQuery resolver\nallProducts: [Product!]!\ndef resolve_all_products(obj, info):\npass\n{\n‘id’: ‘6961ca64-78f3-41d4-bc3b-a63550754bd8’,\n‘name’: ‘Walnut Bomb’,\n‘available’: False,\n‘ingredients’: [],\n‘hasFilling’: False,\n‘hasNutsToppingOption’: True\n}\nType resolver\nunion Product = Beverage | Cake\ndef resolve_product_type(obj, info):\npass\nFigure 10.7\nA type resolver is a function that determines the type of an object. This \nexample shows how the resolve_product_type() resolver determines the type of \nan object returned by the resolve_all_products() resolver.",
      "content_length": 1903,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "251\n10.4\nImplementing the products API\nThe type resolver must be bound to the Product type. Since Product is a union type,\nwe create a bindable object of it using the UnionType class. Ariadne guarantees\nthat the first argument in a resolver is an object, and we inspect this object to resolve\nits type. We don’t need any other parameters, so we ignore them with Python’s\n*_ syntax, which is standard for ignoring positional parameters. To resolve the type\nof the object, we check if it has a hasFilling attribute. If it does, we know it’s a\nCake object; otherwise, it’s a Beverage. Finally, we pass the product bindable to the\nmake_executable_schema() function. Since this is a type resolver, this code goes\ninto the web/types.py.\n# file: web/types.py\nfrom ariadne import UnionType\nproduct_type = UnionType('Product')    \n@product_type.type_resolver      \ndef resolve_product_type(obj, *_):   \n    if 'hasFilling' in obj:\n        return 'Cake'\n    return 'Beverage'\nListing 10.7\nImplementing a type resolver for the Product union type\nBeverage\nCake\n{\n‘id’: ‘6961ca64-78f3-41d4-bc3b-a63550754bd8’,\n‘name’: 'Walnut Bomb’,\n‘available’: False,\n‘ingredients’: [],\n‘hasFilling’: False,\n‘hasNutsToppingOption’: True\n}\nType resolver\nunion Product = Beverage | Cake\ndef resolve_product_type(obj, info):\npass\n{\n‘id’: ‘6961ca64-78f3-41d4-bc3b-a63550754bd8’,\n‘name’: 'Walnut Bomb’,\n‘available’: False,\n‘ingredients’: [],\n‘hasCreamOnTopOption’: False,\n‘hasServeOnIceOption’: True\n}\nFigure 10.8\nA type resolver inspects the properties of a payload to determine its type. In this \nexample, resolve_product_type() looks for distinguishing properties that differentiate a Cake \nfrom a Beverage type. \nWe create a bindable object \nfor the Product type using \nthe UnionType class.\nWe bind Product’s \nresolver using the \nresolver() decorator.\nWe capture the resolver’s \nfirst positional argument \nas obj.",
      "content_length": 1884,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "252\nCHAPTER 10\nBuilding GraphQL APIs with Python\nTo enable the type resolver, we need to add the product object to the make_executable_\nschema() function under web/schema.py:\n# file: web/schema.py\nfrom pathlib import Path\nfrom ariadne import make_executable_schema\nfrom web.queries import query\nfrom web.types import product_type\nschema = make_executable_schema(\n    (Path(__file__).parent / 'products.graphql').read_text(), \n    [query, product_type]\n)\nLet’s run the allProducts() query again:\n{\n  allProducts {\n    ...on ProductInterface {\n      name\n    }\n  }\n}\nYou’ll now get a successful response. You have just learned to implement type resolv-\ners and to handle queries that return multiple types! In the next section, we continue\nexploring queries by learning how to handle query parameters.\n10.4.5 Handling query parameters\nIn this section, we learn to handle query parameters in the resolvers. Most of the que-\nries in the products API accept filtering parameters, and all the mutations require at\nleast one parameter. Let’s see how we access parameters by studying one example\nfrom the products API: the products() query, which accepts an input filter object\nwhose type is ProductsFilter. How do we access this filter object in a resolver?\n As you can see in figure 10.9, when a query or mutation takes parameters, Ariadne\npasses those parameters to our resolvers as keyword arguments. Listing 10.8 shows\nhow we access the input parameter for the products() query resolver. Since the input\nparameter is optional and therefore nullable, we set it by default to None. The input\nparameter is an instance of the ProductsFilter input type, so when it’s present in the\nquery, it comes in the form of a dictionary. From the API specification, we know that\nProductsFilter guarantees the presence of the following fields:\n\navailable—Boolean field that filters products by whether they’re available\n\nsortBy—An enumeration type that allows us to sort products by price or name",
      "content_length": 1978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "253\n10.4\nImplementing the products API\n\nsort—Enumeration type that allows us to sort the results in ascending or descend-\ning order\n\nresultsPerPage—Indicates how many results should be shown per page\n\npage—Indicates which page of the results we should return\nIn addition to these parameters, ProductsFilter may also include two optional\nparameters: maxPrice, which filters results by maximum price, and minPrice, which\nfilters results by minimum price. Since maxPrice and minPrice are not required\nfields, we check for their presence using the Python dictionary’s get() method, which\nreturns None if they’re not found. Let’s implement the filtering and sorting functional-\nity first, and deal with pagination afterwards. The following code goes under web/\nqueries.py.\n# file: web/queries.py\n...\nQuery = QueryType()\n...\n@query.field('products')     \ndef resolve_products(*_, input=None):     \n    filtered = [product for product in products]     \n    if input is None:    \n        return filtered\n    filtered = [     \nListing 10.8\nAccessing input parameters in a resolver\ninput\n{\n“available”: true,\n“sortBy” “price”,\n‘sort’: “DESCENDING”,\n“resultsPerPage”: 5,\n“page”: 1\n}\nresolve_products(\nobj, info,\ninput={’available’: True,\n‘sortBy’: ‘price’,\n‘sort’: ‘DESCENDING’,\n‘resultsPerPage’: 5,\n‘page’: 1}\n)\nFigure 10.9\nQuery parameters are passed to our resolvers \nas keyword arguments. This example illustrates how the \nresolve_products() resolver is called, with the input \nparameter passed as a keyword argument. The parameter \ninput is an object of type ProductsFilter, and \ntherefore it comes in the form of a dictionary.\nWe bind products()’ \nresolver using the \nfield() decorator.\nWe ignore the default positional \narguments and instead capture \nthe input parameter.\nWe copy the list \nof products.\nIf input is None, we return \nthe whole dataset. \nWe filter\nproducts by\navailability.",
      "content_length": 1886,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "254\nCHAPTER 10\nBuilding GraphQL APIs with Python\n        product for product in filtered\n        if product['available'] is input['available']\n    ]\n    if input.get('minPrice') is not None:     \n        filtered = [\n            product for product in filtered\n            if product['price'] >= input['minPrice']\n        ]\n    if input.get('maxPrice') is not None:\n        filtered = [\n            product for product in filtered\n            if product['price'] <= input['maxPrice']\n        ]\n    filtered.sort(     \n        key=lambda product: product.get(input['sortBy'], 0],\n        reverse=input['sort'] == 'DESCENDING'\n    )\n    return filtered    \nLet’s run a query to test this resolver:\n{\n  products(input: {available: true}) {\n    ...on ProductInterface {\n      name\n    }\n  }\n}\nYou should get a valid response from the server. Now that we have filtered the results, we\nneed to paginate them. Listing 10.9 adds a generic pagination function called get_page()\nto web/queries.py. Just a word of warning: in normal circumstances, you’ll be storing\nyour data in a database and delegating filtering and pagination to the database. The\nexamples here are to illustrate how you use the query parameters in the resolver. We\npaginate the results using the islice() function from the itertools module.\n As you can see in figure 10.10, islice() allows us to extract a slice of an iterable\nobject. islice() requires us to provide the start and stop indices of the portion that\nWe filter products \nby minPrice.\nWe sort the \nfiltered dataset.\nWe return the \nfiltered dataset.\nFigure 10.10\nThe islice() function from \nthe itertools module allows you to get a \nslice of an iterable object by selecting the \nstart and stop indices of the subset that \nyou want to slice.",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "255\n10.4\nImplementing the products API\nwe want to slice. For example, a list of 10 items comprising the numbers 0 to 9, provid-\ning a start index of 2 and a stop index of 6, would give us a slice with the following\nitems: [2, 3, 4, 5]. The API paginates results starting at 1, while islice() uses zero-\nbased indexing, so get_page() subtracts one unit from the page parameter to account\nfor that difference.\n# file: web/queries.py\nFrom itertools import islice      \nfrom ariadne import QueryType\nfrom web.data import ingredients, products\n \n...\ndef get_page(items, items_per_page, page):\n    page = page - 1\n    start = items_per_page * page if page > 0 else page     \n    stop = start + items_per_page     \n    return list(islice(items, start, stop))     \n@query.field('products')\ndef resolve_products(*_, input=None):\n    ...\n    return get_page(filtered, input['resultsPerPage'], input['page'])    \nOur hardcoded dataset only contains two products, so let’s test the pagination with\nresutlsPerPage set to 1, which will split the list into two pages:\n{\n  products(input: {resultsPerPage: 1, page: 1}) {\n    ...on ProductInterface {\n      name\n    }\n  }\n}\nYou should get exactly one result. Once we implement the addProduct() mutation in\nthe next section, we’ll be able to add more products through the API and make more\nuse of the pagination parameters.\n You just learned how to handle query parameters! We’re now in a good position to\nlearn how to implement mutations. Mutation resolvers are similar to query resolvers,\nbut they always have parameters. But that’s enough of a spoiler; move on to the next\nsection to learn more about mutations.\nListing 10.9\nPaginating results\nWe import islice().\nWe resolve the \nstart index.\nWe\ncalculate\nthe stop\nindex.\nWe return \na slice of \nthe list.\nWe paginate\nthe results.",
      "content_length": 1814,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "256\nCHAPTER 10\nBuilding GraphQL APIs with Python\n10.4.6 Implementing mutation resolvers\nIn this section, we learn to implement mutation resolvers. Implementing a mutation\nresolver follows the same guidelines we saw for queries. The only difference is the class\nwe use to bind the mutation resolvers. While queries are bound to an instance of the\nQueryType class, mutations are bound to an instance of the MutationType class.\n Let’s have a look at implementing the resolver for the addProduct() mutation.\nFrom the specification, we know that the addProduct() mutation has three required\nparameters: name, type, and input. The shape of the input parameter is given by the\nAddProductInput object type. AddProductInput defines additional properties that\ncan be set when creating a new product, all of which are optional and therefore nul-\nlable. Finally, the addProduct() mutation must return a product type.\n Listing 10.10 shows how we implement the resolver for the addProduct() muta-\ntion (see figure 10.11 for an illustration). We first import the MutationType bindable\nclass and instantiate it. We then declare our resolver and bind it to MutationType\nusing its field() decorator. We don’t need to use Ariadne’s default positional param-\neters obj and info, so we skip them using a wildcard followed by an underscore (*_).\nWe don’t set default values for addProduct()’s parameters, since the specification\nstates they’re all required. addProduct() must return a valid Product object, so we\ninput\n{\n“ingredients”: []\n“hasCreamOnTopOption”: true\n“hasServeOnIceOption”: false\n}\nresolve_add_product(\nobj, info,\nname=‘Mocha’,\ntype=‘beverage’,\ninput={\n'ingredients': [],\n'hasCreamOnTopOption': True,\n'hasServeOnIceOption': False\n}\n)\n“beverage”\ntype\n“Mocha”\nname\nFigure 10.11\nMutation parameters are passed to our resolvers as keyword \narguments. This example illustrates how the resolve_add_product() \nresolver is called, with the name, type, and input parameters passed as \nkeyword arguments.",
      "content_length": 1988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "257\n10.4\nImplementing the products API\nbuild the object with its expected attributes in the body of the resolver. Since Product\nis the union of the Cake and Beverage types, and each type requires different sets of\nproperties, we check the type parameter to determine which fields we should add to\nour object. The following code goes into the web/mutations.py file. \n# file: web/mutations.py\nimport uuid\nfrom datetime import datetime\nfrom ariadne import MutationType\nfrom web.data import products\nmutation = MutationType()    \n@mutation.field('addProduct')      \ndef resolve_add_product(*_, name, type, input):       \n    product = {    \n        'id': uuid.uuid4(),      \n        'name': name,\n        'available': input.get('available', False),    \n        'ingredients': input.get('ingredients', []),\n        'lastUpdated': datetime.utcnow(),\n    }\n    if type == 'cake':   \n        product.update({\n            'hasFilling': input['hasFilling'],\n            'hasNutsToppingOption': input['hasNutsToppingOption'],\n        })\n    else:\n        product.update({\n            'hasCreamOnTopOption': input['hasCreamOnTopOption'],\n            'hasServeOnIceOption': input['hasServeOnIceOption'],\n        })\n    products.append(product)     \n    return product\nTo enable the resolver implemented in listing 10.10, we need to add the mutation\nobject to the make_executable_schema() function in web/schema.py:\n# file: web/schema.py\nfrom pathlib import Path\nfrom ariadne import make_executable_schema\nfrom web.mutations import mutation\nfrom web.queries import query\nfrom web.types import product_type\nListing 10.10\nResolver for the addProduct() mutation\nBindable \nobject for \nmutations\nWe bind addProduct()’s \nresolver using the \nfield() decorator.\nWe capture \naddProduct()’s \nparameters.\nWe declare \nthe new \nproduct as a \ndictionary.\nWe set server-side \nproperties such as the ID.\nWe parse optional \nparameters and set \ntheir default values.\nWe check whether the product \nis a Beverage or a Cake.\nWe return the newly \ncreated product.",
      "content_length": 2027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "258\nCHAPTER 10\nBuilding GraphQL APIs with Python\nschema = make_executable_schema(\n    (Path(__file__).parent / 'products.graphql').read_text(), \n    [query, mutation, product_type]\n)\nLet’s put the new mutation to work by running a simple test. Go to the Apollo Play-\nground running on http://127.0.0.1:8000, and run the following mutation:\nmutation {\n  addProduct(name: \"Mocha\", type: beverage, input:{ingredients: []}) {\n    ...on ProductInterface {\n      name,\n      id\n    }\n  }\n}\nYou’ll get a valid response, and a new product will be added to our list. To verify things\nare working correctly, run the following query and check that the response contains\nthe new item just created:\n{\n  allProducts {\n    ...on ProductInterface {\n      name\n    }\n  }\n}\nRemember that we are running the service with an in-memory list representation of\nour data, so if you stop or reload the server, the list will be reset and you’ll lose any\nnewly created data.\n You just learned how to build mutations! This is a powerful feature: with muta-\ntions, you can create and update data in a GraphQL server. We’ve now covered nearly\nall the major aspects of the implementation of a GraphQL server. In the next section,\nwe’ll take this further by learning how to implement resolvers for custom scalar types.\n10.4.7 Building resolvers for custom scalar types\nIn this section, we learn how to implement resolvers for custom scalar types. As we saw\nin chapter 8, GraphQL provides a decent amount of scalar types, such as Boolean,\ninteger, and string. And in many cases, GraphQL’s default scalar types are sufficient to\ndevelop an API. Sometimes, however, we need to define our own custom scalars. The\nproducts API contains a custom scalar called Datetime. The lastUpdated field in both\nthe Ingredient and Product types have a Datetime scalar type. Since Datetime is a\ncustom scalar, Ariadne doesn’t know how to handle it, so we need to implement a\nresolver for it. How do we do that?",
      "content_length": 1959,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "259\n10.4\nImplementing the products API\nAs you can see in figures 10.12 and 10.13, when we encounter a custom scalar type in\na GraphQL API, we need to make sure we can perform the following three actions on\nthe custom scalar:\nSerialization—When a user requests data from the server, Ariadne has to be able\nto serialize the data. Ariadne knows how to serialize GraphQL’s built-in scalars,\nbut for custom scalars, we need to implement a custom serializer. In the case of\nthe Datetime scalar in the products API, we have to implement a method to\nserialize a datetime object.\nDeserialization—When a user sends data to our server, Ariadne deserializes the\ndata and makes it available to us as a Python native data structure, such as a\ndictionary. If the data includes a custom scalar, we need to implement a\nmethod that lets Ariadne know how to parse and load the scalar into a native\nPython data structure. For the Datetime scalar, we want to be able to load it as a\ndatetime object.\nIngredient\n+ name: “Mocha”\n+ lastUpdated: datetime(2021, 01, 01)\n{\n“name”: “Mocha”,\n“lastUpdated”: “2022-01-01”\n}\nJSON payload\nGraphQL server\nValidation and deserialization\n“name”: “Mocha” -> “Mocha”\n“lastUpdated”: “2021-01-01” -> datetime(2021, 01, 01)\nPython native object\nFigure 10.12\nWhen a GraphQL server receives data from the user, it validates and deserializes the data into \nnative Python objects. In this example, the server deserializes the name \"Mocha\" into a Python string, and \nthe date \"2021-01-01\" into a Python datetime.\nIngredient\n+ name: “Mocha”\n+ lastUpdated: datetime(2021, 01, 01)\n{\n“name”: “Mocha”,\n“lastUpdated”: “2022-01-01”\n}\nJSON payload\nGraphQL server\nSerialization\n“name”: “Mocha” -> “Mocha”\nPython native object\n“lastUpdated”: datetime(2021, 01, 01) -> “2021-01-01”\nFigure 10.13\nWhen the GraphQL server sends data to the user, it transforms native Python objects into \nserializable data. In this example, the server serializes the both the name and the date as strings.",
      "content_length": 1980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "260\nCHAPTER 10\nBuilding GraphQL APIs with Python\nValidation—GraphQL enforces validation of each scalar and type, and Ariadne\nknows how to validate GraphQL’s built-in scalars. For custom scalars, we have to\nimplement our own validation methods. In the case of the Datetime scalar, we\nwant to make sure it has a valid ISO format.\nAriadne provides a simple API to handle these actions through its ScalarType class.\nThe first thing we need to do is create an instance of this class:\nfrom ariadne import ScalarType\ndatetime_scalar = ScalarType('Datetime') \nScalarType exposes decorator methods that allow us to implement serialization,\ndeserialization, and validation. For serialization, we use ScalarType’s serializer()\ndecorator. We want to serialize datetime objects into ISO standard date format, and\nPython’s datetime library provides a convenient method for ISO formatting, the\nisoformat() method: \n@datetime_scalar.serializer\ndef serialize_datetime(value):\n  return value.isoformat()\nFor validation and deserialization, ScalarType provides the value_parser() decorator.\nWhen a user sends data to the server containing a Datetime scalar, we expect the date\nto be in ISO format and therefore parsable by Python’s datetime.fromisoformat()\nmethod:\nfrom datetime import datetime\n@datetime_scalar.value_parser\ndef parse_datetime_value(value):\n  return datetime.fromisoformat(value)\nIf the date comes in the wrong format, fromisoformat() will raise a ValueError,\nwhich will be caught by Ariadne and shown to the user with the following message:\n“Invalid isoformat string.” The following code goes under web/types.py since it imple-\nments a type resolver.\n# file: web/types.py\nimport uuid\nfrom datetime import datetime\nfrom ariadne import UnionType, ScalarType\n...\nListing 10.11\nSerializing and parsing a custom scalar",
      "content_length": 1813,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "261\n10.4\nImplementing the products API\ndatetime_scalar = ScalarType('Datetime')   \n@datetime_scalar.serializer      \ndef serialize_datetime_scalar(date):    \n    return date.isoformat()    \n@datetime_scalar.value_parser   \ndef parse_datetime_scalar(date):     \n    return datetime.fromisoformat(date)    \nTo enable the Datetime resolvers, we add datetime_scalar to the array of bindable\nobjects for the make_executable_schema() function under web/schema.py:\nfrom pathlib import Path\nfrom ariadne import make_executable_schema\nfrom web.mutations import mutation\nfrom web.queries import query\nfrom web.types import product_type, datetime_scalar\nschema = make_executable_schema(\n    (Path(__file__).parent / 'products.graphql').read_text(),\n    [query, mutation, product_type, datetime_scalar]\n)\nLet’s put the new resolvers to the test! Go back to the Apollo Playground running on\nhttp://127.0.0.1:8000 and execute the following query:\n# Query document\n{\n  allProducts {\n    ...on ProductInterface {\n      name,\n      lastUpdated\n    }\n  }\n}\n# result:\n{\n  \"data\": {\n    \"allProducts\": [\n      {\n        \"name\": \"Walnut Bomb\",\n        \"lastUpdated\": \"2022-06-19T18:27:53.171870\"\n      },\n      {\n        \"name\": \"Cappuccino Star\",\nWe create a bindable object \nfor the Datetime scalar \nusing the ScalarType class.\nWe bind Datetime’s serializer \nusing the serializer() decorator.\nWe capture the serializer’s \nargument as date.\nWe\nserialize\nthe date\nobject.\nWe bind Datetime’s parser using \nthe value_parser() decorator.\nWe capture the \nparser’s argument.\nWe parse\na date.",
      "content_length": 1565,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "262\nCHAPTER 10\nBuilding GraphQL APIs with Python\n        \"lastUpdated\": \"2022-06-19T18:27:53.171871\"\n      }\n    ]\n  }\n}\nYou should get a list of all products with their names, and with an ISO-formatted date\nin the lastUpdated field. You now have the power to implement your own custom sca-\nlar types in GraphQL. Use it wisely! Before we close the chapter, there’s one more\ntopic we need to explore: implementing resolvers for the fields of an object type. \n10.4.8 Implementing field resolvers\nIn this section, we learn to implement resolvers for the fields of an object type. We’ve\nimplemented nearly all the resolvers that we need to serve all sorts of queries on the\nproducts API, but there’s still one type of query that our server can’t resolve: queries\ninvolving fields that map to other GraphQL types. For example, the Products type has a\nfield called ingredients, which maps to an array of IngredientRecipe objects. Accord-\ning to the specification, the shape of the IngredientRecipe type looks like this:\n# file: web/products.graphql\ntype IngredientRecipe {\n    ingredient: Ingredient!\n    quantity: Float!\n    unit: String!\n}\nEach IngredientRecipe object has an ingredient field, which maps to an Ingredient\nobject type. This means that, when we query the ingredients field of a product, we\nshould be able to pull information about each ingredient, such as its name, descrip-\ntion, or supplier information. In other words, we should be able to run the following\nquery against the server:\n{\n  allProducts {\n    ...on ProductInterface {\n      name,\n      ingredients {\n        quantity,\n        unit,\n        ingredient{\n          name\n        }\n      }\n    }\n  }\n}\nIf you run this query in Apollo Playground at this juncture, you’ll get an error with the\nfollowing message: “Cannot return null for non-nullable field Ingredient.name.”",
      "content_length": 1843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "263\n10.4\nImplementing the products API\n Why is this happening? If you look at the list of products in listing 10.5, you’ll notice\nthat the ingredients field maps to an array of objects with three fields: ingredient,\nquantity, and unit. For example, the Walnut Bomb has the following ingredients:\n# file: web/data.py\ningredients = [\n  {\n    'ingredient': '602f2ab3-97bd-468e-a88b-bb9e00531fd0',\n    'quantity': 100.00,\n    'unit': 'LITRES',\n  }\n]\nThe ingredient field maps to an ingredient ID, not a full ingredient object. This is\nour internal representation of the product’s ingredients. It’s how we store product\ndata in our database (in-memory list in this implementation). And it’s a useful represen-\ntation since it allows us to identify each ingredient by ID. However, the API specification\ntells us that the ingredients field should map to an array of IngredientRecipe objects\nand that each ingredient should represent an Ingredient object, not just an ID.\n How do we solve this problem? We can use different approaches. For example, we\ncould make sure that each ingredient payload is correctly built in the resolvers for\neach query that returns a Product type. For example, listing 10.12 shows how we can\nmodify the allProducts() resolver to accomplish this. The snippet modifies every\nproduct’s ingredients property to make sure it contains a full ingredient payload.\nSince every product is represented by a dictionary, we make a deep copy of each prod-\nuct to make sure the changes we apply in this function don’t affect our in-memory list\nof products.\n# file: web/queries.py\n...\n@query.field('allProducts')\ndef resolve_all_products(*_):\n    products_with_ingredients = [deepcopy(product) for product in products]\n    for product in products_with_ingredients:\n        for ingredient_recipe in product['ingredients']:\n            for ingredient in ingredients: \n                if ingredient['id'] == ingredient_recipe['ingredient']:\n                    ingredient_recipe['ingredient'] = ingredient    \n    return products_with_ingredients    \nListing 10.12\nUpdating products to contain full ingredient payloads, not just IDs\nWe make a deep copy\nof each object in the\nproducts list.\nWe update the ingredient\nproperty with a full\nrepresentation of\nthe ingredient.\nWe return the list of\nproducts with\ningredients.",
      "content_length": 2320,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "264\nCHAPTER 10\nBuilding GraphQL APIs with Python\nThe approach in listing 10.12 is perfectly fine, but as you can see, it makes the code\ngrow in complexity. If we had to do this for a few more properties, the function would\nquickly become difficult to understand and to maintain.\n As you can see in figure 10.14, GraphQL offers an alternative way of resolving\nobject properties. Instead of modifying the product payload within the allProducts()\nresolver, we can create a specific resolver for the product’s ingredients property and\nmake any necessary changes within that resolver. Listing 10.13 shows what the resolver\nfor the product’s ingredients property looks like and goes under web/types.py since\nit implements a resolver for object properties.\n# file: web/types.py\n...\n@product_interface.field('ingredients')\ndef resolve_product_ingredients(product, _):\n    recipe = [   \nListing 10.13\nImplementing a field resolver\nQuery resolver\nallProducts: [Product!]!\ndef resolve_all_products(obj, info):\npass\n{\n‘id’: ‘6961ca64-78f3-41d4-bc3b-a63550754bd8’,\n‘name’: 'Walnut Bomb’,\n‘available’: False,\n‘ingredients’: [\n{\n‘ingredient’: ‘602f2ab3-97bd-468e-a88b-bb9e00531fd0’,\n‘amount’: 100.00,\n‘unit’: ‘LITRES’,\n}\n],\n‘hasFilling’: False,\n‘hasNutsToppingOption’: True\n}\nField resolver\nProduct\ndef resolve_product_ingredients(obj, info):\npass\n[\n{\n‘id’: ‘602f2ab3-97bd-468e-a88b-bb9e00531fd0’,\n‘name’: ‘Milk’,\n‘stock’: {\n‘amount’: 100.00,\n‘unit’: ‘LITRES’,\n},\n}\n]\nFigure 10.14\nGraphQL allows us to create resolvers for specific fields of an object. In this example, the \nresolve_product_ingredients() resolver takes care of returning a valid payload for the \ningredients property of a product.\nWe create a deep \ncopy of each \ningredient.",
      "content_length": 1726,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "265\nSummary\n        copy.copy(ingredient)\n        for ingredient in product.get(\"ingredients\", [])\n    ]\n    for ingredient_recipe in recipe:\n        for ingredient in ingredients:\n            if ingredient['id'] == ingredient_recipe['ingredient']:\n                ingredient_recipe['ingredient'] = ingredient\n    return recipe\nObject property resolvers help us keep our code more modular because every resolver\ndoes only one thing. They also help us avoid repetition. By having a single resolver\nthat takes care of updating the ingredients property in product payloads, we avoid\nhaving to perform this operation in every resolver that returns a product type. On the\ndownside, property resolvers may be more difficult to trace and debug. If something is\nwrong with the ingredients payload, you won’t find the bug within the allProducts()\nresolver. You have to know that there’s a resolver for products’ ingredients and look\ninto that resolver. Application logs will help to point you in the right direction when\ndebugging this kind of issues, but bear in mind that this design will not be entirely\nobvious to other developers who are not familiar with GraphQL. As with everything\nelse in software design, make sure that code reusability doesn’t impair the readability\nand ease of maintenance of your code.\nSummary\nThe Python ecosystem offers various frameworks for implementing GraphQL\nAPIs. See GraphQL’s official website for the latest news on available frame-\nworks: https://graphql.org/code/.\nYou can use the Ariadne framework to implement GraphQL APIs following a\nschema-first approach, which means we first design the API, and then we imple-\nment the server against the specification. This approach is beneficial since it\nallows the server and client development teams to work in parallel. \nAriadne can validate request and response payloads automatically using the\nspecification, which means we don’t have to spend time implementing custom\nvalidation models.\nFor each query and mutation in the API specification, we need to implement a\nresolver. A resolver is a function that knows how to process the request for a\ngiven query or mutation. Resolvers are the code that allow us to expose the\ncapabilities of a GraphQL API and therefore represent the backbone of the\nimplementation.\nTo register a resolver, we use one of Ariadne’s bindable classes, such as Query-\nType or MutationType. These classes expose decorators that allow us to bind a\nresolver function.\nGraphQL specifications can contain complex types, such as union types, which\ncombine two or more object types. If our API specification contains a union",
      "content_length": 2624,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "266\nCHAPTER 10\nBuilding GraphQL APIs with Python\ntype, we must implement a resolver that knows how to determine the type of an\nobject; otherwise, the GraphQL server doesn’t know how to resolve it.\nWith GraphQL, we can define custom scalars. If the specification contains a cus-\ntom scalar, we must implement resolvers that know how to serialize, parse, and\nvalidate the custom scalar type; otherwise, the GraphQL server doesn’t know\nhow to handle them.",
      "content_length": 453,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "Part 4\nSecuring, testing,\nand deploying\nmicroservice APIs\nAs we learned in chapter 1, APIs are programmatic interfaces to our appli-\ncations, and making our APIs public allows other organizations to build integra-\ntions with our own APIs. The growing offering of APIs as a means of delivering\nsoftware products has given rise to the API economy. APIs open new opportuni-\nties for business growth, but they also represent a security risk. Lack of proper\ntesting or wrongly implemented security protocols render our APIs vulnerable.\nPart 4 of this book will get you up and running on the major topics of API test-\ning, security, and operations.\n The modern standard for API authentication is OpenID Connect, and for\nAPI authorization it’s Open Authorization (OAuth) 2.1. Chapter 11 kicks off\npart 4 by introducing these standards. In my experience, this is one of the most\nmisunderstood areas of API development, which leads to security vulnerabilities\nand breaches. Chapter 11 teaches you everything you need to know to imple-\nment a robust API authentication and authorization strategy for your APIs.\n When you drive integrations using APIs, you need a reliable API testing and\nvalidation method. You must ensure that your API backend serves the interface\ndefined in your API specification. How do we do that? As you’ll learn in chapter\n12, a powerful approach to API testing is using contract-testing tools, such as",
      "content_length": 1416,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "268\nPART 4\nSecuring, testing, and deploying microservice APIs\nDredd and Schemathesis, and applying property-based testing. With these strategies,\nyou can test and validate your code with confidence before releasing it to production.\n Finally, what about deployments and operations? The final chapters of this book\nteach you how to Dockerize and deploy your microservice APIs using Kubernetes.\nYou’ll learn to deploy and operate a Kubernetes cluster using AWS EKS, one of the\nmost popular solutions for running Kubernetes in the cloud. After reading part 4,\nyou’ll be ready to test, protect, and operate your microservice APIs at scale.",
      "content_length": 635,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "269\nAPI authorization\nand authentication\nIn 2018, a weakness in the API authentication system of the US postal system\n(https://usps.com) allowed hackers to obtain data from 60 million users, including\ntheir email addresses, phone numbers, and other personal details.1 API security\nThis chapter covers\nUsing Open Authorization to allow access to \nour APIs\nUsing OpenID Connect to verify the identity of \nour API users\nWhat kinds of authorization flows exist, \nand which flow is more suitable for each \nauthorization scenario\nUnderstanding JSON Web Tokens (JWT) and \nusing Python’s PyJWT library to produce and \nvalidate them\nAdding authentication and authorization \nmiddleware to our APIs\n1 The issue was reported first by Brian Krebs, “USPS Site Exposed Data on 60 Million Users,” KrebsOnSecurity,\nNovember 21, 2018, https://krebsonsecurity.com/2018/11/usps-site-exposed-data-on-60-million-users/.",
      "content_length": 902,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "270\nCHAPTER 11\nAPI authorization and authentication\nattacks like this have become more and more common, with an estimated growth of\nover 300% in the number of attacks in 2021.2 API vulnerabilities don’t only risk expos-\ning sensitive data from your users; they can also put you out of business!3 The good\nnews is there are steps you can take to reduce the risk of an API breach. The first line\nof defense is a robust authentication and authorization system. In this chapter, you’ll\nlearn to prevent unauthorized access to your APIs by using standard authentication\nand authorization protocols.\n In my experience, API authentication and authorization are two of the most con-\nfusing topics for developers, and they’re also areas where implementation mistakes\nhappen often. Before you implement the security layer of your API, I highly recom-\nmend you read this chapter to make sure you know what you’re doing and know how\nto do it correctly. I’ve done my best to provide a comprehensive summary of how API\nauthentication and authorization work, and by the end of this chapter you should be\nable to add a robust authorization flow to your own APIs.\n Authentication is the process of verifying the identity of a user, while authorization\nis the process of determining whether a user has access to certain resources or opera-\ntions. The concepts and standards about authentication and authorization that you’ll\nlearn in this chapter are applicable to all types of web APIs.\n You’ll learn different authentication and authorization protocols and flows and\nhow to validate authorization tokens. You’ll also learn to use Python’s PyJWT library to\nproduce signed tokens and to validate them. We’ll walk through a practical example\nof adding authentication and authorization to the orders API. We’ve got a lot to cover,\nso let’s get started!\n11.1\nSetting up the environment for this chapter\nLet’s set up the environment for this chapter. The code for this chapter is available\nunder the directory called ch11 in the GitHub repository for this book. In chapter 7,\nwe implemented a fully functional orders service, complete with a business layer, data-\nbase, and API. This chapter picks up the orders service from where we left it in chap-\nter 7. If you want to follow along with the changes in this chapter, copy over the code\nfrom chapter 7 into a new folder called ch11:\n$ cp -r ch07 ch11\ncd into ch11 and install the dependencies by running pipenv install. For this chapter,\nwe need a few additional dependencies, so run the following command to install them:\n$ pipenv install cryptography pyjwt \n2 Bill Doerfeld, “API Attack Traffic Grew 300+% In the Last Six Months,” Security Boulevard, July 30, 2021,\nhttps://securityboulevard.com/2021/07/api-attack-traffic-grew-300-in-the-last-six-months/.\n3 Joe Galvin, “60 Percent of Small Businesses Fold Within 6 Months of a Cyber Attack,” Inc., May 7, 2018,\nhttps://www.inc.com/joe-galvin/60-percent-of-small-businesses-fold-within-6-months-of-a-cyber-attack-heres-how\n-to-protect-yourself.html.",
      "content_length": 3032,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "271\n11.2\nUnderstanding authentication and authorization protocols\nPyJWT is a Python library that allows us to work with JSON Web Tokens, while cryptog-\nraphy will allow us to verify the tokens’ signatures. (For a list of alternative JWT librar-\nies in the Python ecosystem, check out https://jwt.io/libraries?language=Python.)\n Our environment is now ready, so let’s begin our quest through the wondrous\nworld of user authentication and authorization. It’s a journey full of pitfalls, but a nec-\nessary one. Hold tight, and watch carefully as we go along!\n11.2\nUnderstanding authentication and authorization protocols\nWhen it comes to API authentication, the two most important protocols you need to\nknow are OAuth (Open Authorization) and OpenID Connect (OIDC). This section\nexplains how each protocol works and how they fit within the authentication and\nauthorization flows for our APIs.\n11.2.1 Understanding Open Authorization\nOAuth is a standard protocol for access delegation.4 As you can see in figure 11.1,\nOAuth allows a user to grant a third-party application access to protected resources\nthey own in another website without having to share their credentials.\nDEFINITION\nOAuth is an open standard that allows users to grant access to\nthird-party applications to their information on other websites. Typically,\naccess is granted by issuing a token, which the third-party application uses to\naccess the user’s information.\n4 https://oauth.net/ is a pretty good website with tons of resources to learn more about the OAuth specification.\nFacebook\nLinkedIn\nSusan wants to import her\nFacebook list of contacts\ninto her LinkedIn account.\nLinkedIn asks Facebook for access\nto Susan’s contacts.\nFacebook asks Susan to conﬁrm her identity\nand that she wants to grant LinkedIn access.\nSusan conﬁrms her identity with Facebook\nand grants access to LinkedIn.\nFacebook issues a token that LinkedIn\ncan use to access Susan's contacts.\nSusan\nLinkedIn loads Susan's Facebook list\nof contacts.\nFigure 11.1\nWith OAuth, a user can grant a third-party application access to their information on another \nwebsite.",
      "content_length": 2102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "272\nCHAPTER 11\nAPI authorization and authentication\nFor example, let’s say Susan has a list of contacts in her Facebook account. One day,\nSusan signs into LinkedIn, and she wants to import her list of contacts from Facebook.\nTo allow LinkedIn to import her Facebook contacts, Susan has to grant LinkedIn\naccess to that resource. How can she grant LinkedIn access to her list of contacts? She\ncould give LinkedIn her Facebook credentials to access her account. But that would\nbe a major security risk. Instead, OAuth defines a protocol that allows Susan to tell\nFacebook that LinkedIn can access her list of contacts. With OAuth, Facebook issues a\ntemporary token LinkedIn can use to import Susan’s contacts.\n OAuth distinguishes various roles in the process of granting access to a resource:\nResource owner—The user who’s granting access to the resource. In the previous\nexample, Susan is the resource owner.\nResource server—The server hosting the user’s protected resources. In the previ-\nous example, Facebook is the resource server.\nClient—The application or server requesting access to the user’s resources. In\nthe previous example, LinkedIn is the client.\nAuthorization server—The server that grants the client access to the resources. In\nthe previous example, Facebook is the authorization server.\nOAuth offers four different flows to grant authorization to a user depending on the\naccess conditions. It’s important to know how each flow works and in which scenarios\nyou can use it in. In my experience, OAuth flows are one of the biggest areas of confu-\nsion around authorization, and one of the biggest sources of security problems in\nmodern websites. These are the OAuth flows:\nAuthorization code flow\nPKCE flow\nClient credentials flow\nRefresh token flow\nOAuth\nOAuth flows are the strategies that a client application uses to authorize their access\nto an API. Best practices in OAuth change over time as we learn more about applica-\ntion vulnerabilities and we improve the protocol. Current best practices are described\nin IETF’s “OAuth 2.0 Security Best Current Practice” (http://mng.bz/o58v), written by\nT. Lodderstedt, J. Bradley, A. Labunets, and D. Fett. If you read about OAuth 2.0, you\nmay encounter references to two flows that we don’t describe in this chapter: the\nresource owner password flow and the implicit flow. Both are now deprecated since\nthey expose serious vulnerabilities, and therefore you shouldn’t use them. \nAnother popular extension that we don’t discuss in this chapter is the device authoriza-\ntion grant (http://mng.bz/5mZD), which allows input-constrained devices such as smart\nTVs to obtain access tokens. The latest version of OAuth is 2.1, which is described in\nthe IETF’s “The OAuth 2.1 Authorization Framework” (http://mng.bz/69m6).",
      "content_length": 2784,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "273\n11.2\nUnderstanding authentication and authorization protocols\nLet’s delve into each flow to understand how they work and when we use them!\nAUTHORIZATION CODE FLOW\nIn the authorization code flow, the client server exchanges a secret with the authoriza-\ntion server to produce a signing URL. As you can see in figure 11.2, after the user\nsigns in using this URL, the client server obtains a one-time code it can exchange for\nan access token. This flow uses a client secret, and therefore is only appropriate for\napplications in which the code is not publicly exposed, such as traditional web applica-\ntions where the user interface is rendered in the backend. OAuth 2.1 recommends\nusing the authorization code flow in combination with PKCE, which is described in\nthe next section.\nPROOF OF KEY FOR CODE EXCHANGE FLOW\nThe Proof of Key for Code Exchange (PKCE, pronounced “pixie”) is an extension of the\nauthorization code flow designed to protect applications whose source code is publicly\nAuthorization\nserver\nAPI server\nThe server exchanges a secret with the\nauthorization server to produce a signing\nURL where the user can prove their\nidentity and grant access.\nThe authorization server serves the\nsigning URL.\nThe server request the user's data.\nUser logs in and grants access.\nThe authorization server issues a one-time\ncode that can be exchanged for an access\ntoken.\nThe server exchanges the one-time code\nfor an access token.\nThe authorization server issues an\naccess token.\nServer-side\nrendered\napplication\nFigure 11.2\nIn the authorization code flow, the authorization server produces a signing URL, which \nthe user can use to prove their identity and grant access to the third-party application.",
      "content_length": 1705,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "274\nCHAPTER 11\nAPI authorization and authentication\nexposed, such as mobile applications and single-page applications (SPAs).5 Since the\nsource is publicly exposed, the client cannot use a secret because it would also be pub-\nlicly exposed.\n As you can see in figure 11.3, in the PKCE flow, the client generates a secret called\nthe code verifier, and it encodes it. The encoded code is called the code challenge. When\nsending an authorization request to the server, the client includes both the code veri-\nfier and the code challenge in the request. In return, the server produces an authoriza-\ntion code, which the client can exchange for an access token. To get the access token,\nthe client must send both the authorization code and the code challenge.\n5 N. Sakimura, J. Bradley, and N. Agarwal, “Proof Key for Code Exchange by OAuth Public Clients,” IETF RFC\n7636, September 2015, https://datatracker.ietf.org/doc/html/rfc7636.\nAuthorization\nserver\nAPI server\nSPA\nGenerate a code veriﬁer and code challenge\nAuthorization request with code challenge\nAuthorization code response\nAuthorization code + code veriﬁer\nAccess token\nRequest user’s data\nFigure 11.3\nIn the PKCE flow, an SPA served by the client requests access to the user’s data directly \nfrom the authorization server by exchanging a code verifier and a code challenge.",
      "content_length": 1331,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "275\n11.2\nUnderstanding authentication and authorization protocols\nThanks to the code challenge, the PKCE flow also prevents authorization code injec-\ntion attacks, in which a malicious user intercepts the authorization code and uses it to\nget hold of an access token. Due to the security benefits of this flow, PKCE is also rec-\nommended for server-side applications. We’ll see an example of this flow using an SPA\nin appendix C. \nCLIENT CREDENTIALS FLOW\nThe client credentials flow is aimed for server-to-server communication, and as you\ncan see in figure 11.4, it involves the exchange of a secret to obtain an access token.\nThis flow is suitable for enabling communication between microservices over a secure\nnetwork. We’ll see an example of this flow in appendix C.\nREFRESH TOKEN FLOW\nThe refresh token flow allows clients to exchange a refresh token for a new access\ntoken. For security reasons, access tokens are valid for a limited period of time. How-\never, API clients often need to be able to communicate with the API server after an\naccess token has expired, and to obtain the new token they use the refresh token flow.\n As you can see in figure 11.5, API clients typically receive both an access token and\na refresh token when they successfully gain access to the API. Refresh tokens are usu-\nally valid for a limited period of time, and they’re valid for one-time use. Every time\nyou refresh your access token, you’ll get a new refresh token.\n Now that we understand how OAuth works, let’s turn our attention to OpenID\nConnect!\n \n \nAuthorization\nserver\nThe server application\nexchanges a secret with the\nauthorization server to obtain\nan access token.\nThe authorization server\nissues the access token.\nServer\napplication\nAPI server\nAccess the API\nFigure 11.4\nIn the client credentials flow, a server application exchanges a secret with \nthe authorization server to obtain an access token.",
      "content_length": 1901,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "276\nCHAPTER 11\nAPI authorization and authentication\n11.2.2 Understanding OpenID Connect\nOpenID Connect (OIDC) is an open standard for identity verification that’s built on\ntop of OAuth. As you can see in figure 11.6, OIDC allows users to authenticate to a\nwebsite by using a third-party identity provider. If you’ve used your Facebook, Twitter,\nor your Google account to sign into other websites, you’re already familiar with OIDC.\nIn this case, Facebook, Twitter, and Google are identity providers. You use them to\nbring your identity to a new website. OIDC is a convenient authentication system since\nit allows users to use the same identity across different websites without having to cre-\nate and manage new usernames and passwords.\nAuthorization\nserver\nAPI server\nSPA\nAuthorization request\nAccess token + efresh token\nr\nRequest user’s data\nRefresh access token\nFigure 11.5\nTo allow API clients to use refresh tokens to continue communicating with the API server after \nthe access token has expired, the authorization server issues a new refresh token every time the client \nrequests a new access token.\ncoﬀeemesh.io\nOpenID Connect\nserver\n1. The user signs in with the\nOpenID Connect server.\nID token\nAccess token\nAccess token\n2. The OpenID Connect\nserver issues an ID token\nand an Access token.\n3. The user uses the ccess\na\ntoken to access the\napplication.\nSusan\nFigure 11.6\nWith OIDC, a user signs in \nwith an OIDC server. The OIDC server \nissues an ID token and an access \ntoken, which the user can use to \naccess an application.",
      "content_length": 1536,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "277\n11.2\nUnderstanding authentication and authorization protocols\nDEFINITION\nOpenID Connect (OIDC) is an identity verification protocol that\nallows users to bring their identity from one website (the identity provider) to\nanother. OIDC is built on top of OAuth, and we can use the same flows defined\nby OAuth to authenticate users.\nSince OIDC is built on top of OAuth, we can use any of the authorization flows\ndescribed in the previous section to authenticate and authorize users. As you can see\nin figure 11.6, when we authenticate using the OIDC protocol, we distinguish two\ntypes of tokens: ID tokens and access tokens. Both tokens come in the form of JSON\nWeb Tokens, but they serve different purposes: ID tokens identify the user, and they\ncontain information such as the user’s name, their email, and other personal\ndetails. You use ID tokens only to verify the user identity, and never to determine\nwhether a user has access to an API. API access is validated with access tokens. Access\ntokens typically don’t contain user information but a set of claims about the access rights\nof the user.\nID TOKENS VS. ACCESS TOKENS\nA common security problem is the misuse of\nID tokens and access tokens. ID tokens are tokens that carry the identity of\nthe user. They must be used exclusively for verifying the user’s identity and\nnot for validating access to an API. API access is validated through access\ntokens. Access tokens rarely contain a user’s identity details, and instead con-\ntain claims about the user’s right to access the API. A fundamental difference\nbetween ID tokens and access tokens is the audience: the ID token’s audience is\nthe authorization server, while the access token’s audience is our API server.\nIdentity providers that offer OIDC integrations expose a /.well-known/openid-\nconfiguration endpoint (with a leading period!), also known as the discovery end-\npoint, which tells the API consumer how to authenticate and obtain their access\ntokens. For example, the OIDC’s well-known endpoint for Google Accounts is https://\naccounts.google.com/.well-known/openid-configuration. If you call this endpoint,\nyou’ll obtain the following payload (the example is truncated with an ellipsis):\n{\n  \"issuer\": \"https:/ /accounts.google.com\",\n  \"authorization_endpoint\": \"https:/ /accounts.google.com/o/oauth2/v2/auth\",\n  \"device_authorization_endpoint\": \n➥ \"https:/ /oauth2.googleapis.com/device/code\",\n  \"token_endpoint\": \"https:/ /oauth2.googleapis.com/token\",\n  \"userinfo_endpoint\": \"https:/ /openidconnect.googleapis.com/v1/userinfo\",\n  \"revocation_endpoint\": \"https:/ /oauth2.googleapis.com/revoke\",\n  \"jwks_uri\": \"https:/ /www.googleapis.com/oauth2/v3/certs\",\n  \"response_types_supported\": [\n    \"code\",\n    \"token\",\n    \"id_token\",\n    \"code token\",\n    \"code id_token\",\n    \"token id_token\",",
      "content_length": 2810,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "278\nCHAPTER 11\nAPI authorization and authentication\n    \"code token id_token\",\n    \"none\"\n  ],\n  ...\n}\nAs you can see, the well-known endpoint tells us which URL we must use to obtain the\nauthorization access token, which URL returns user information, or which URL we\nuse to revoke an access token. There are other bits of information in this payload,\nsuch as available claims or the JSON Web Keys URI (JWKS). Typically, you use a library\nto handle these endpoints on your behalf, or you use an identity-as-a-service provider\nto take care of these integrations. If you want to learn more about OpenID Connect, I\nrecommend Prabath Siriwardena’s OpenID Connect in Action (Manning, 2022).\n Now that we know how OAuth and OpenID Connect work, it’s time get into the\ndetails of how authentication and authorization work. We’ll start by studying what\nJSON Web Tokens are in the next section.\n11.3\nWorking with JSON Web Tokens\nIn OAuth and OpenID Connect, user access is verified by means of a token known as\nJSON Web Token, or JWT. This section explains what JSON Web Tokens are, how they’re\nstructured, what kinds of claims they contain, and how to produce and validate them.\n A JWT is a token that represents a JSON document. The JSON document contains\nclaims, such as who issued the token, the audience of the token, or when the token\nexpires. The JSON document is typically encoded as a Base64 string. JWTs are nor-\nmally signed with a private secret or a cryptographic key.6 A typical JSON Web Token\nlooks like this:\neyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJpc3MiOiJodHRwczovL2F1dGguY29mZmVlbW\n➥ VzaC5pby8iLCJzdWIiOiJlYzdiYmNjZi1jYTg5LTRhZjMtODJhYy1iNDFlNDgzMWE5NjIiL\n➥ CJhdWQiOiJodHRwOi8vMTI3LjAuMC4xOjgwMDAvb3JkZXJzIiwiaWF0IjoxNjM4MjI4NDg2\n➥ LjE1Otg4MSwiZXhwIjoxNjM4MzE0Odg2LjE1Otg4Mswic2NvcGUiOiJvcGVuaWQifQ.oblJ\n➥ 5wV9GqrhIDzNSzcClrpEQTMK8hZGzn1S707tDtQE__OCDsP9J2Wa70aBua6X81-\n➥ zrvWBfzrcX--nSyT-\n➥ A9uQxL5j3RHHycToqSVi87I9H6jgP4FEKH6ClwZfabVwzNIy52Zs7zRdcSI4WRz1OpHoCM-\n➥ 2hNtZ67dMJQgBVIlrXcwKAeKQWP8SxSDgFbwnyRTZJt6zijRnCJQqV4KrK_M4pv2UQYqf9t\n➥ Qpj2uflTsVcZq6XsrFLAgqvAg-YsIarYw9d63rs4H_I2aB3_T_1dGPY6ic2R8WDT1_Axzi-\n➥ crjoWq9A51SN-kMaTLhE_v2MSBB3A0zrjbdC4ZvuszAqQ\nIf you look closely at the example, you’ll see the string contains two periods. The peri-\nods act as delimiters that separate each component of the JSON Web Token. As you\ncan see in figure 11.7, a JSON Web Token document has three sections:\n6 The full specification for how JSON Web Tokens should be produced and validated is available under J. Jones,\nJ. Bradley, and N. Sakimura, “JSON Web Token (JWT),” RFC-7519, May 2015, https://datatracker.ietf.org/\ndoc/html/rfc7519.",
      "content_length": 2642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "279\n11.3\nWorking with JSON Web Tokens\nHeader—Identifies the type of token as well as the algorithm and the key that\nwere used sign the token. We use this information to apply the right algorithm\nto verify the token’s signature.\nPayload—Contains the document’s set of claims. The JWT specification includes a\nlist of reserved claims that identify the issuer of the token (the authorization\nserver), the token’s audience or intended recipient (our API server), and its\nexpiry date, among other details. In addition to JWT’s standard claims, a pay-\nload can also include custom claims. We use this information to determine\nwhether the user has access to the API.\nSignature—A string representing the token’s signature.\nNow that we understand what a JWT is and what its structure looks like, let’s delve\ndeeper into its properties. The next sections explain the main types of claims and\nproperties we can find in JWT payloads and headers and how we use them.\n11.3.1 Understanding the JWT header\nJWTs contain a header that describes the type of token, as well as the algorithm and\nthe key used to sign the token. JWTs are commonly signed using the HS256 and the\nRS256 algorithms. HS256 uses a secret to encrypt the token, while RS256 uses a pri-\nvate/public key pair to sign the token. We use this information to apply the right algo-\nrithm to verify the token’s signature.\n \n \n \n{\n“typ”: “JWT”,\n“alg”: “RS256”\n}\n{\n“iss”: “https://auth.coﬀeemesh.io/”,\n“sub”: “ec7bbccf-ca89-4af3-82ac-b41e4831a962”,\n“aud”: “http://127.0.0.1:8000/orders”,\n“iat”: 1638228486.159881,\n“exp”: 1638314886.159881,\n“scope”: “openid”\n}\nHeader\nPayload (claims)\nSignature\nFigure 11.7\nA JWT is composed of three \nparts: a header that contains information \nabout the token itself, a payload with claims \nabout the user’s access to the website, and \na signature that proves the authenticity of \nthe token.",
      "content_length": 1871,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "280\nCHAPTER 11\nAPI authorization and authentication\nA typical JWT header is the following:\n{\n  \"alg\": \"RS256\",\n  \"typ\": \"JWT\",\n  \"kid\": \"ZweIFRR4l1dJlVPHOoZqf\"\n}\nLet’s analyze this header:\n\nalg—Tells us that the token was signed using the RS256 algorithm\n\ntyp—Tells us that this is a JWT token\n\nkid—Tells us that the key used to sign the token has the ID \nZweIFRR4l1dJlVPHOoZqf\nA token’s signature can only be verified using the same secret or key that was used to\nsign it. For security, we often use a collection of secrets or keys to sign the tokens. The\nkid field tells us which secret or key to use to sign the token so that we can use the\nright value when verifying the token’s signature.\n Some tokens also contain a nonce field in the header. If you see one of those\ntokens, chances are the token isn’t for your API server unless you’re the creator of the\ntoken and you know what the value for nonce is. The nonce field typically contains an\nencrypted secret that adds an additional layer of security to the JWT. For example, the\ntokens issued by the Azure Active Directory to access its Graph API contain a nonce\ntoken, which means you shouldn’t use those tokens to authorize access to your custom\nAPIs. Now that we understand the properties of a token’s header, the next section\nexplains how to read the token’s claims.\n11.3.2 Understanding JWT claims\nThe payload of a JWT contains a set of claims. Since a JWT payload is a JSON docu-\nment, the claims come in the form of key-value pairs.\nSigning algorithms for JWTs\nThe two most common algorithms used for signing JWTs are HS256 and RS256.\nHS256 stands for HMAC-SHA256, and it’s a form of encryption that uses a key to\nproduce a hash. \nRS256 stands for RSA-SHA256. RSA (Rivest-Shamir-Adleman) is a form of encryption\nthat uses a private key to encrypt the payload. In this case, we can verify that the\ntoken’s signature is correct by using a public key. \nYou can learn more about HMAC and RSA in David Wong’s Real-World Cryptography\n(Manning, 2021).",
      "content_length": 2011,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "281\n11.3\nWorking with JSON Web Tokens\n There are two types of claims: reserved claims, which are part of the JWT specifica-\ntion, and custom claims, which are claims we can add to enrich the tokens with addi-\ntional information.7 The JWT specification defines seven reserved claims:\n\niss (issuer)—Identifies the issuer of the JWT. If you use an identity-as-a-service\nprovider, the issuer identifies that service. It typically comes in the form of an ID\nor a URL.\n\nsub (subject)—Identifies the subject of the JWT (i.e., the user sending the\nrequest to the server). It typically comes in the form of an opaque ID (i.e., an ID\nthat doesn’t disclose the user’s personal details).\n\naud (audience)—Indicates the recipient for which the JWT is intended. This is\nour API server. It typically comes in the form of an ID or a URL. It’s crucial to\ncheck this field to validate that the token is intended for our APIs. If we don’t\nrecognize the value in this field, it means the token isn’t for us, and we must dis-\nregard the request.\n\nexp (expiration time)—A UTC timestamp that indicates when the JWT expires.\nRequests with expired tokens must be rejected.\n\nnbf (not before time)—A UTC timestamp that indicates the time before which the\nJWT must not be accepted.\n\niat (issued at time)—A UTC timestamp that indicates when the JWT was issued.\nIt can be used to determine the age of the JWT.\n\njti (JWT ID)—A unique identifier for the JWT.\nThe reserved claims are not required in the JWT payload, but it’s recommended to\ninclude them to ensure interoperability with third-party integrations. \n{\n  \"iss\": \"https:/ /auth.coffeemesh.io/\",\n  \"sub\": \"ec7bbccf-ca89-4af3-82ac-b41e4831a962\",\n  \"aud\": \"http:/ /127.0.0.1:8000/orders\",\n  \"iat\": 1667155816,\n  \"exp\": 1667238616,\n  \"azp\": \"7c2773a4-3943-4711-8997-70570d9b099c\",\n  \"scope\": \"openid\"\n}\nLet’s dissect the claims in listing 11.1:\n\niss tells us that the token has been issued by the https://auth.coffeemesh.io\nserver identity service.\n\nsub tell us that the user has the identifier ec7bbccf-ca89-4af3-82ac-\nb41e4831a962. The value of this identifier is owned by the identity service. Our\nAPIs can use this value to control access to the resources owned by this user in\n7 You can see a full list of the most commonly used JWT claims under https://www.iana.org/assignments/\njwt/jwt.xhtml.\nListing 11.1\nExample of JWT payload claims",
      "content_length": 2374,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "282\nCHAPTER 11\nAPI authorization and authentication\nan opaque way. We say this ID is opaque because it doesn’t disclose any personal\ninformation about the user.\n\naud tells us that this token has been issued to grant access to the orders API. If\nthe value of this field is a different URL, the orders API will reject the request.\n\niat tells us that the token was issued on the 30th of October of 2022 at 6:50 p.m.\nUTC.\n\nexp tells us that the token expires on the 31st of October of 2022 at 5:50 p.m.\nUTC.\n\nazp tells us that the token has been requested by an application with identifier\n7c2773a4-3943-4711-8997-70570d9b099c. This is typically a frontend applica-\ntion. This claim is common in tokens that have been issued using the OpenID\nConnect protocol.\nThe scope field tells us that this token was issued using the OpenID Connect\nprotocol.\nNow that we know how to work with token claims, let’s see how we produce and vali-\ndate tokens!\n11.3.3 Producing JWTs\nTo form the final JWT, we encode the header, the payload, and the signature using\nbase64url encoding. As documented in RFC 4648 (http://mng.bz/aPRj), base64url\nencoding is similar to Base64, but it uses non-alphanumeric characters and omits pad-\nding. The header, payload, and signature are then concatenated using periods as sep-\narators. Libraries like PyJWT take care of the heavy lifting of producing a JWT. Let’s\nsay we want to produce a token for the payload we saw in listing 11.1: \npayload = {\n  \"iss\": \"https:/ /auth.coffeemesh.io/\",\n  \"sub\": \"ec7bbccf-ca89-4af3-82ac-b41e4831a962\",\n  \"aud\": \"http:/ /127.0.0.1:8000/orders\",\n  \"iat\": 1667155816,\n  \"exp\": 1667238616,\n  \"azp\": \"7c2773a4-3943-4711-8997-70570d9b099c\",\n  \"scope\": \"openid\"\n}\nTo produce a signed token with this payload, we use PyJWT’s encode() function, pass-\ning in the token, the key to sign the token, and the algorithm we want to use to sign\nthe token:\n>>> import jwt\n>>> jwt.encode(payload=payload, key='secret', algorithm='HS256')\n➥ 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2F1dGguY29mZ\n➥ mVlbWVzaC5pby8iLCJzdWIiOiJlYzdiYmNjZi1jYTg5LTRhZjMtODJhYy1iNDFlNDgzMWE5\n➥ NjIiLCJhdWQiOiJodHRwOi8vMTI3LjAuMC4xOjgwMDAvb3JkZXJzIiwiaWF0IjoxNjY3MTU\n➥ 1ODE2LCJleHAiOjE2NjcyMzg2MTYsImF6cCI6IjdjMjc3M2E0LTM5NDMtNDcxMS04Otk3Lt\n➥ cwNTcwZDliMDk5YyIsInNjb3BlIjoib3BlbmlkIn0.sZEXZVitCv0iVrbxGN54GJr8QecZf\n➥ HA_pdvfEMzT1dI'",
      "content_length": 2362,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "283\n11.3\nWorking with JSON Web Tokens\nIn this case, we’re signing the token with a secret keyword using the HS256 algorithm.\nFor a more secure encryption, we use a private/public key pair to sign the token with\nthe RS256 algorithm. To sign JWTs, we typically use certificates that follow the X.509\nstandard, which allows us to bind an identity to a public key. To generate a private/\npublic key pair, run the following command from your terminal:\n$ openssl req -x509 -nodes -newkey rsa:2048 -keyout private_key.pem \\\n-out public_key.pem -subj \"/CN=coffeemesh\"\nThe minimum input for an X.509 certificate is the subject’s common name (CN),\nwhich in this case we set to coffeemesh. If you omit the -subj flag, you’ll be prompted\nwith a series of questions about the identity you want to bind the certificate to. This\ncommand produces a private key under a file named private_key.pem, and the corre-\nsponding public certificate under a file named public_key.pem. If you’re unable to\nrun these commands, you can find a sample key pair in the GitHub repository pro-\nvided with this book, under ch11/private_key.pem and ch11/public_key.pem.\n Now that we have a private/public key pair, we can use them to sign our tokens\nand to validate them. Create a file named jwt_generator.py and paste into it the con-\ntents of listing 11.2, which shows how to generate JWT tokens signed with a private\nkey. The listing defines a function, generate_jwt(), which generates a JWT for the\npayload defined within the function. In the payload, we set the iat and the exp prop-\nerties dynamically: iat is set to the current UTC time; exp is set to 24 hours from now.\nWe load the private key using cryptography’s serialization() function, passing in\nas parameters the content of our private key file encoded in bytes, as well as the pass-\nphrase encoded in bytes. Finally, we encode the payload using PyJWT’s encode() func-\ntion, passing in the payload, the loaded private key, and the algorithm we want to use\nto sign the token (RS256).\n# file: jwt_generator.py\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nimport jwt\nfrom cryptography.hazmat.primitives import serialization\ndef generate_jwt():\n    now = datetime.utcnow()\n    payload = {\n        \"iss\": \"https:/ /auth.coffeemesh.io/\",\n        \"sub\": \"ec7bbccf-ca89-4af3-82ac-b41e4831a962\",\n        \"aud\": \"http:/ /127.0.0.1:8000/orders\",\n        \"iat\": now.timestamp(),\n        \"exp\": (now + timedelta(hours=24)).timestamp(),\n        \"scope\": \"openid\",\n    }\nListing 11.2\nGenerating JWTs signed with a private key",
      "content_length": 2560,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "284\nCHAPTER 11\nAPI authorization and authentication\n    private_key_text = Path(\"private_key.pem\").read_text()\n    private_key = serialization.load_pem_private_key(\n        private_key_text.encode(),\n        password=None,\n    )\n    return jwt.encode(payload=payload, key=private_key, algorithm=\"RS256\")\nprint(generate_jwt())\nTo see this code at work, activate your virtual environment by running pipenv shell,\nand execute the following command:\n$ python jwt_generator.py\n➥ eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJpc3MiOiJodHRwczovL2F1dGguY29mZm\n➥ VlbWVzaC5pby8iLCJzdWIiOiJlYzdiYmNjZi1jYTg5LTRhZjMtODJhYy1iNDFlNDgzMWE5N\n➥ jIiLCJhdWQiOiJodHRwOi8vMTI3LjAuMC4xOjgwMDAvb3JkZXJzIiwiaWF0IjoxNjM4MDMx\n➥ LjgzOTY5ODczOTEsImV4cCI6MTYzODExOC4yMzk2Otg5OTMsInNjb3BlIjoib3BlbmlkIn0\n➥ .GipMvEvZG8ErmMA99geYUq5IkeWpRrnHoViLb1CkRufqC5vgM9555re4IsLLa7yVxNAXIp\n➥ FVFBqaoWrloJl6dSQ5r00dvUBSM1EM78KMZ7f0gQqUDFWNoKWCeyQu1QCBzuHTouS4l_mzz\n➥ Ii75Sal3DJLTaj4zr6c_bQdUuDU1GyrIOJiPSCHSlnKPgg9tjrX8eOcB_ESGSo9ipnCbPAl\n➥ uWp0cDjPRPBNRuiU53sbli-\n➥ dTy7WoCD1mXAbqhztwO39kG3DZBkysB4vTnKU4Eul2yNNYK2hHVZQEvAqq8TJjETUS7iekf\n➥ 0NSt1qQArJ7cxg6Jh5D7y5pbKmYYsBlFohPg\nNow you know how to generate JWTs! The JWT generator from listing 11.2 is handy\nfor running tests, and we’ll use it in the upcoming sections to test our code. Now that\nwe understand how JWTs are generated, let’s see how to inspect their payloads and\nhow to validate them.\n11.3.4 Inspecting JWTs\nOften when working with JWTs you’ll run into validation issues. To understand why a\ntoken validation is failing, it’s useful to inspect the payload and verify whether its\nclaims are correct. In this section, you’ll learn to inspect JWTs using three different\ntools: jwt.io (https://jwt.io), the terminal’s base64 command, and with Python. To try\nout these tools, run the jwt_generator.py script we created in section 11.3.3 to issue a\nnew token.\n jwt.io is an excellent tool that offers an easy way to inspect a JWT. As you can see in\nfigure 11.8, all you need to do is paste the JWT in the input panel on the left. The dis-\nplay panel on the right will show you the contents of the token’s header and payload.\nYou can also verify the token’s signature by providing your public key. To extract the\npublic key from our public certificate, you can use the following command:\n$ openssl x509 -pubkey -noout < public_key.pem > pubkey.pem\nThis command outputs the public key to a file named pubkey.pem. You need to copy\nthe contents of that file into the public key input panel in jwt.io to verify the token’s\nsignature.",
      "content_length": 2538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "285\n11.3\nWorking with JSON Web Tokens\nYou can also inspect the contents of the JWT by decoding the header and payload in\nthe terminal using the base64 command. For example, to decode the token’s header\nin the terminal, run the following command:\n$ echo eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9 | base64 --decode\n{\"alg\":\"RS256\",\"typ\":\"JWT\"}\nWe can also inspect the contents of a JWT using Python’s base64 library. To decode a\nJWT header with Python, open a Python shell and run the following code:\n>>> import base64\n>>> base64.decodebytes('eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9'.encode())\nb'{\"alg\":\"RS256\",\"typ\":\"JWT\",}'\nSince the JWT payload is also base64url encoded, we use the same methods for\ndecoding it. Now that we know how to inspect JWT payloads, let’s see how we vali-\ndate them!\nInput\npanel\nPublic and private\nkey input for\nsignature veriﬁcation\nDecoded\nheader\nDecoded\npayload\nSignature veriﬁcation\nFigure 11.8\njwt.io is a tool that helps you to easily inspect and visualize JWTs. Simply paste the \ntoken on the left-side panel. You can also verify the token’s signature by pasting the public key in \nthe VERIFY SIGNATURE box on the right.",
      "content_length": 1145,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "286\nCHAPTER 11\nAPI authorization and authentication\n11.3.5 Validating JWTs\nThere’re two parts to validating a JWT. On one hand, you must validate its signature,\nand on the other hand, you must validate that its claims are correct, for example, by\nensuring that the token isn’t expired and that the audience is correct. This process\nmust be clear; both steps of the validation process are required. An expired token with\na valid signature shouldn’t be accepted by the API server, while an active token with an\ninvalid signature isn’t any good either. Every user request to the server must carry a\ntoken, and the token must be validated on each request.\nVALIDATE JWTS ON EACH REQUEST\nWhen a user interacts with our API server,\nthey must send a JWT in each request, and we must validate the token on\neach request. Some implementations, especially those that use the authoriza-\ntion code flow we discussed in section 11.2.1, store tokens in a session cache\nand check the request’s token against the cache. That’s not how JWTs are\nmeant to be used. JWTs are designed for stateless communication between\nthe client and the server, and therefore must be validated using the methods\nwe describe in this section.\nAs we saw in section 11.3.3, tokens can be signed with a secret key or with a pri-\nvate/public key pair. For security, most websites use tokens that are signed with pri-\nvate/public keys, and to validate the signature of such tokens, we use the public key.\n Let’s see how we validate a token in code. We’ll use the signing key we created in\nsection 11.3.3 to produce and validate the token. Activate your Pipenv environment by\nrunning pipenv shell, and execute the jwt_generator.py script to issue a new token.\n To validate the token, we must first load the public key using the following code:\n>>> from cryptography.x509 import load_pem_x509_certificate\n>>> from pathlib import Path\n>>> public_key_text = Path('public_key.pem').read_text()\n>>> public_key = load_pem_x509_certificate(public_key_text.encode('utf-\n➥ 8')).public_key()\nNow that we have the public key available, we can use it to validate a token with the\nfollowing code:\n>>> import jwt\n>>> access_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ...\"\n>>> jwt.decode(access_token, key=public_key, algorithms=['RS256'], \n➥ audience=[\"http:/ /127.0.0.1:8000/orders\"])\n{'iss': 'https:/ /auth.coffeemesh.io/', 'sub': 'ec7bbccf-ca89-4af3-82ac-\n➥ b41e4831a962', 'aud': 'http:/ /127.0.0.1:8000/orders', 'iat': \n➥ 1638114196.49375, 'exp': 1638200596.49375, 'scope': 'openid'}\nAs you can see, if the token is valid, we’ll get back the JWT payload. If the token is\ninvalid, this code will raise an exception. Now that we know how to work with and vali-\ndate JWTs, let’s see how we authorize requests in an API server.",
      "content_length": 2769,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "287\n11.4\nAdding authorization to the API server\n11.4\nAdding authorization to the API server\nNow that we know how to validate access tokens, let’s put all this code together in our\nAPI server. In this section, we add authorization to the orders API. Some endpoints of\nthe orders API are protected, while others must be accessible to everyone. Our goal is\nto ensure that our server checks for valid access tokens under the protected endpoints.\n We’ll allow public access to the /docs/orders and the /openapi/orders.json end-\npoints since they serve the API documentation that must be available for all consum-\ners. All other endpoints require valid tokens. If the token is invalid or is missing in the\nrequest, we must reject the request with a 401 (Unauthorized) status code, which indi-\ncates that credentials are missing.\n How do we add authorization to our APIs? There’re two major strategies: handling\nvalidation in an API gateway or handling validation in each service. An API gateway is a\nnetwork layer that sits in front of our APIs.8 The main role of an API gateway is to facil-\nitate service discovery, but it can also be used to authorize user access, validate access\ntokens, and enrich the request with custom headers that add information about the\nuser.\n The second method is to handle authorization within each API. You’ll handle\nauthorization at the service level when your API gateway can’t handle authorization or\nwhen an API gateway doesn’t fit in your architecture. In this section, we’ll learn to\nhandle authorization within the service since we don’t have an API gateway.\n A question that often comes up is, where exactly in our code do we handle autho-\nrization? Since authorization is needed to validate user access to the service through\nthe API, we implement it in the API middleware. As you can see in figure 11.9, mid-\ndleware is a layer of code that provides common functionality to process all our\nrequests. Most web servers have a concept of middleware or request preprocessors,\nand that’s where our authorization code goes. Middleware components are usually\nexecuted in order, and typically we can choose the order in which they’re executed.\nSince authorization controls access to our server, the authorization middleware\nmust be executed early.\n11.4.1 Creating an authorization module\nLet’s first create a module to encapsulate our authorization code. Create a file\nnamed orders/web/api/auth.py and copy the code in listing 11.3 into it. We start by\nloading the public key we created in section 11.3.3. To validate the token, we first\nretrieve the headers and load the public key. We use PyJWT’s decode() function to\nvalidate the token, passing in as parameters the token itself, the public key required\nto validate the token, the expected list of audiences, and the algorithms used to sign\nthe key.\n \n8 See Chris Richardson, “Pattern: API Gateway/Backends for Frontends,” https://microservices.io/patterns/\napigateway.html.",
      "content_length": 2954,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "288\nCHAPTER 11\nAPI authorization and authentication\n# file: orders/web/api/auth.py\nfrom pathlib import Path\nimport jwt\nfrom cryptography.x509 import load_pem_x509_certificate\npublic_key_text = (\n    Path(__file__).parent / \"../../../public_key.pem\"\n).read_text()\npublic_key = load_pem_x509_certificate(\n    public_key_text.encode()\n).public_key()\ndef decode_and_validate_token(access_token):\n    \"\"\"\n    Validates an access token. If the token is valid, it returns the token payload.\n    \"\"\"\n    return jwt.decode(\n        access_token,\nListing 11.3\nAdding an authorization module to the API\nRequest\nHeaders:\n- Authorization: Bearer eyJ0eXAiOiJKV1QiLCJ...\n- Access-Control-Allow-Origin: coﬀeemesh.com\nGET /orders\nCORS middleware\nAuth middleware\nRequest\nHeaders:\n- Authorization: Bearer eyJ0eXAiOiJKV1QiLCJ...\nGET /orders\nRouter\nGET /orders view\nPOST /orders view\nGET /orders/{order_id} view\n...\nThe CORS middleware enriches\nthe request headers.\nThe Auth middleware authorizes\nthe request by checking the\nauthorization token.\nMiddleware\nRequest\nHeaders:\n- Authorization: Bearer eyJ0eXAiOiJKV1QiLCJ...\n- Access-Control-Allow-Origin: coﬀeemesh.com\nGET /orders\nThe request\narrives in the server.\nThe request is\npreprocessed by\nthe middleware.\nThe request is\nhandled by the\nrouter.\nFigure 11.9\nA request is first processed by the server middleware, such as the CORS and auth middleware, \nbefore making it to the router, which maps the request to the corresponding view function.",
      "content_length": 1473,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "289\n11.4\nAdding authorization to the API server\n        key=public_key,\n        algorithms=[\"RS256\"],\n        audience=[\"http:/ /127.0.0.1:8000/orders\"],\n    )\nNow that we created a module that encapsulates the functionality necessary to validate\na JWT, let’s incorporate it into the API by adding a middleware that uses it to validate\naccess to the API.\n11.4.2 Creating an authorization middleware\nTo add authorization to our API, we create an authorization middleware. Listing 11.4\nshows how to implement the authorization middleware. The code in listing 11.4 goes\ninto the orders/web/app.py file, with the newly added code in bold. We implement\nthe middleware as a simple class called AuthorizeRequestMiddleware, which inherits\nfrom Starlette’s BaseHTTPMiddleware class. The entry point for the middleware must\nbe implemented in a function called dispatch().\n We use a flag to determine whether we should enable authorization. The flag is an\nenvironment variable called AUTH_ON, and we set it to False by default. Often when\nworking on a new feature or when debugging an issue in our API, it’s convenient to\nrun the server locally without authorization. Using a flag allows us to switch authenti-\ncation on and off according to our needs. If authorization is off, we add the default ID\ntest for the request user.\n Next, we check whether the user is requesting the API documentation. In that\ncase, we don’t block the request since we want to make the API documentation visible\nto all users; otherwise, they wouldn’t know how to form their requests correctly.\n We also check the request’s method. If it’s an OPTIONS request, we won’t attempt\nto authorize the request. OPTIONS requests are preflight requests, also known as\ncross-origin resource sharing (CORS) requests. The purpose of a preflight request is\nto check which origins, methods, and request headers are accepted by the API server,\nand according to W3’s specification, CORS requests must not require credentials\n(https://www.w3.org/TR/2020/SPSD-cors-20200602/). CORS requests are typically\nhandled by the web server framework.\nDEFINITION\nCORS requests, also known as preflight requests, are requests sent\nby the web browser to understand which methods, origins, and headers are\naccepted by the API server. If we don’t process CORS requests correctly, the\nweb browser will abort communication with the API. Fortunately, most web\nframeworks contain plug-ins or extensions that handle CORS requests cor-\nrectly. CORS requests aren’t authenticated, so when we add authorization to\nour server, we must ensure that preflight requests don’t require credentials.\nIf it’s not a CORS request, we attempt to capture the token from the request headers.\nWe expect the token under the Authorization header. If the Authorization header\nisn’t found, we reject the request with a 401 (Unauthorized) status code response.",
      "content_length": 2865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "290\nCHAPTER 11\nAPI authorization and authentication\nThe format of the Authorization header’s value is Bearer <ACCESS_TOKEN>, so if the\nAuthorization header is found, we capture the token by splitting the header value\naround the space, and we attempt to validate it. If the token is invalid, PyJWT will raise\nan exception. In our middleware, we capture PyJWT’s invalidation exceptions to make\nsure we can return a 401 status code response. If no exception is raised, it means the\ntoken is valid, and therefore we can process the request, so we return a call to the next\ncallback. We also store the user ID from the token payload in the request’s state\nobject so that we can access it later in the API views. Finally, to register the middle-\nware, we use FastAPI’s add_middleware() method.\nWHERE DO JSON WEB TOKENS GO?\nJWTs go in the request headers, typically\nunder the Authorization header. An Authorization header with a JWT usually\nhas the following format: Authorization: Bearer <JWT>.\n# file: orders/web/app.py\nimport os\nfrom fastapi import FastAPI\nfrom jwt import (\n    ExpiredSignatureError,\n    ImmatureSignatureError,\n    InvalidAlgorithmError,\n    InvalidAudienceError,\n    InvalidKeyError,\n    InvalidSignatureError,\n    InvalidTokenError,\n    MissingRequiredClaimError,\n)\nfrom starlette import status\nfrom starlette.middleware.base import (\n    RequestResponseEndpoint,\n    BaseHTTPMiddleware,\n)\nfrom starlette.requests import Request\nfrom starlette.responses import Response, JSONResponse\nfrom orders.api.auth import decode_and_validate_token\napp = FastAPI(debug=True)\nclass AuthorizeRequestMiddleware(BaseHTTPMiddleware):    \n    async def dispatch(    \n        self, request: Request, call_next: RequestResponseEndpoint\n    ) -> Response:\nListing 11.4\nAdding an authorization middleware to the orders API\nWe create a middleware class\nby inheriting from Starlette’s\nBaseHTTPMiddleware base class.\nWe implement the \nmiddleware’s entry point.",
      "content_length": 1953,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "291\n11.4\nAdding authorization to the API server\n        if os.getenv(\"AUTH_ON\", \"False\") != \"True\":    \n            request.state.user_id = \"test\"    \n            return await call_next(request)   \n        if request.url.path in [\"/docs/orders\", \"/openapi/orders.json\"]:    \n            return await call_next(request)\n        if request.method == \"OPTIONS\":\n            return await call_next(request)\n        bearer_token = request.headers.get(\"Authorization\")   \n        if not bearer_token:       \n            return JSONResponse(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                content={\n                    \"detail\": \"Missing access token\",\n                    \"body\": \"Missing access token\",\n                },\n            )\n        try:\n            auth_token = bearer_token.split(\" \")[1].strip()    \n            token_payload = decode_and_validate_token(auth_token)     \n        except (    \n            ExpiredSignatureError,\n            ImmatureSignatureError,\n            InvalidAlgorithmError,\n            InvalidAudienceError,\n            InvalidKeyError,\n            InvalidSignatureError,\n            InvalidTokenError,\n            MissingRequiredClaimError,\n        ) as error:\n            return JSONResponse(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                content={\"detail\": str(error), \"body\": str(error)},\n            )\n        else:\n            request.state.user_id = token_payload[\"sub\"]    \n        return await call_next(request)\napp.add_middleware(AuthorizeRequestMiddleware)   \nfrom orders.api import api\nOur server is ready to start validating requests with JWTs! Let’s run a test to see our\nauthorization code at work. Activate the virtual environment by running pipenv shell,\nand start the server with the following command:\n$ AUTH_ON=True uvicorn orders.web.app:app --reload\nFrom a different terminal, make an unauthenticated request using cURL (some of the\noutput is truncated) with the -i flag, which displays additional information, such as\nthe response status code:\nWe authorize\nthe request if\nAUTH_ON is\nset to True.\nIf authorization is off, we \nbind a default user named \ntest to the request.\nWe return\nby calling the\nnext callback.\nThe documentation endpoints\nare publicly available, so we\ndon’t authorize them.\nWe attempt\nto fetch the\nAuthorization\nheader.\nIf the Authorization \nheader isn’t set, we \nreturn a 401 \nresponse.\nWe capture\nthe token\nfrom the\nAuthorization\nheader.\nWe validate and\nretrieve the token’s\npayload.\nIf the token is \ninvalid, we return \na 401 response.\nWe capture the \nuser ID from the \ntoken’s sub field.\nWe register the \nmiddleware using FastAPI’s \nadd_middleware() method.",
      "content_length": 2691,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "292\nCHAPTER 11\nAPI authorization and authentication\n$ curl -i http:/ /localhost:8000/orders\nHTTP/1.1 401 Unauthorized\n[...]\n{\"detail\":\"Missing access token\",\"body\":\"Missing access token\"}\nAs you can see, a request with a missing token is rejected with a 401 error and a mes-\nsage telling us that the access token is missing. Now generate a token using the jwt_\ngenerator.py script we implemented in section 11.3.3, and use the token to make a\nnew request:\ncurl http:/ /localhost:8000/orders -H 'Authorization: Bearer \n➥ eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6ImI3NTQwM2QxLWUzZDktNDgzYy0\n➥ 5MjZhLTM4NDRhM2Q4OWY1YyJ9.eyJpc3MiOiJodHRwczovL2F1dGguY29mZmVlbWVzaC5pb\n➥ y8iLCJzdWIiOiJlYzdiYmNjZi1jYTg5LTRhZjMtODJhYy1iNDFlNDgzMWE5NjIiLCJhdWQi\n➥ OiJodHRwOi8vMTI3LjAuMC4xOjgwMDAvb3JkZXJzIiwiaWF0IjoxNjM4MTE3MjEyLjc5OTE\n➥ 3OSwiZXhwIjoxNjM4MjAzNjEyLjc5OTE3OSwic2NvcGUiOiJvcGVuaWQifQ.F1bmgYm1acf\n➥ i1NMm5JGkbYQYWFNvG1-7BAXEnIqNdF0th_DYcnEm_p3YZ5hQ93v4QWxDx9muit6InKs-\n➥ MHqhChP2k6DakpSocaqbgJ_IHpqNhTaEzByqZjoNfZFyQLZMo3yEaQB8S_x0LcKOOqeoPYl\n➥ GSWM1eAUy7VFBXmvMUZrUj-yoK721U9vevgM-wdVyYFVtpTRuyjCoWMjJEVadNn-\n➥ Zrxr0ghlRQnwEx-YdTbbEMkk_vVLWoWeEgj7mkBE167fr-\n➥ fyGUKBqa2F71Zwh8DaDQz79Ph_STOY6BTlCnAVL8XwnlIOhJWpSHuc90Kynn_RX49_yJrQH\n➥ KF-xLoflWg'\n{\"orders\":[]}\nIf the token is valid, this time you’ll get a successful response with a list of orders. Our\nauthorization code is working! The next step is to ensure that users can access only\ntheir own resources in the server. Before we do that, though, let’s add one more piece\nof middleware to handle CORS requests.\n11.4.3 Adding CORS middleware\nSince we’re going to allow interactions with a frontend application, we also need to\nenable the CORS middleware. As we saw in section 11.4.2, CORS requests are sent by\nthe browser to know which headers, methods, and origins are allowed by the server.\nFastAPI’s CORS middleware takes care of populating our responses with the right\ninformation. Listing 11.5 shows how to modify the orders/web/app.py file to register\nthe CORS middleware, with the newly added code in bold and omitting some of the\ncode in listing 11.5 with an ellipsis.\n As we did previously, we use FastAPI’s add_middleware() method to register the\nCORS middleware, and we pass along the necessary configuration. For testing pur-\nposes, we’re using wildcards to allow all origins, methods, and headers, but in your\nproduction environment you must be more specific. In particular, you must restrict\nthe allowed origins to your website’s domain and other trusted origins.\n The order in which we register our middleware matters. Middleware is executed in\nreverse order of registration, so the latest registered middleware is executed first.\nSince the CORS middleware is required for all interactions between the frontend cli-\nent and the API server, we register it last, which ensures it’s always executed.",
      "content_length": 2855,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "293\n11.5\nAuthorizing resource access\n# file: orders/web/app.py\nimport os\nfrom fastapi import FastAPI\nfrom jwt import (\n    ExpiredSignatureError,\n    ImmatureSignatureError,\n    InvalidAlgorithmError,\n    InvalidAudienceError,\n    InvalidKeyError,\n    InvalidSignatureError,\n    InvalidTokenError,\n    MissingRequiredClaimError,\n)\nfrom starlette import status\nfrom starlette.middleware.base import RequestResponseEndpoint, \nBaseHTTPMiddleware\nfrom starlette.middleware.cors import CORSMiddleware    \nfrom starlette.requests import Request\nfrom starlette.responses import Response, JSONResponse\nfrom orders.api.auth import decode_and_validate_token\napp = FastAPI(debug=True)\n...\napp.add_middleware(AuthorizeRequestMiddleware)\napp.add_middleware(\n    CORSMiddleware,     \n    allow_origins=[\"*\"],   \n    allow_credentials=True,    \n    allow_methods=[\"*\"],    \n    allow_headers=[\"*\"],    \n)\nfrom orders.api import api\nWe’re almost ready! Our server can now authorize users and handle CORS requests.\nThe next step is to ensure each user can only access their data. \n11.5\nAuthorizing resource access\nWe’ve protected our API by making sure only authenticated users can access it. Now\nwe must ensure that the details of each order are only accessible to the user who\nplaced it; we don’t want to allow users to access each other’s data. We call this type of\nvalidation authorization, and in this section, you’ll learn to add it to your APIs.\nListing 11.5\nAdding CORS middleware\nWe import \nStarlette’s \nCORSMiddleware \nclass.\nWe register CORSMiddleware \nusing FastAPI’s add_middleware() \nmethod.\nWe allow\nall origins.\nWe support cookies for \ncross-origin requests.\nWe allow all \nHTTP methods.\nWe allow all \nheaders.",
      "content_length": 1708,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "294\nCHAPTER 11\nAPI authorization and authentication\n11.5.1 Updating the database to link users and orders\nWe’ll start by removing the orders currently present in the database. Those orders are\nnot associated with a user and therefore won’t work once we enforce an association\nbetween each order and a user. cd into the ch11 directory, activate the virtual environ-\nment by running pipenv shell, and open a Python shell by running the python com-\nmand. Within the Python shell, run the following code:\n>>> from orders.repository.orders_repository import OrdersRepository\n>>> from orders.repository.unit_of_work import UnitOfWork\n>>> with UnitOfWork() as unit_of_work:\n...     orders_repository = OrdersRepository(unit_of_work.session)\n...     orders = orders_repository.list()\n...     for order in orders: order.delete(order.id)\n...     unit_of_work.commit()\nOur database is now clean, so we’re ready to get rolling. How do we associate each\norder with a user? A typical strategy is to create a user table and link our orders to user\nrecords via foreign keys. But does it really make sense to create a user table for the\norders service? Do we want to have a user table per service?\n No, we don’t want to have a user table per service since it would involve lots of\nduplication. As you can see in figure 11.10, we want to have just one user table, and\nthat table must be owned by the user service. Our user service is our identity-as-a-service\nprovider, and therefore our user table already exists. Each user has already an ID, and\nas we saw in section 11.3.1, the ID is present in the JWT payload under the sub field.\nAll we need to do is add a new column to the orders table to store the ID of the user\nwho created the order.\nIdentity service\nprovider\nOrders service\nUsers database\n+ID: 3229cd59-68ec-42a3-a0cd-06bf40f4533e\n+Name: Joe Preston\n+email: joe@gmail.com\nOrders database\n+ID: 3aa36c25-17bd-447a-8f07-d9b495431530\n+Items: [{product: latte, quantity: 1}]\n+User: 3229cd59-68ec-42a3-a0cd-06bf40f4533e\nFigure 11.10\nTo avoid duplication, we keep only one user table under the identity service provider. \nAnd to avoid tight coupling between services, we avoid foreign keys between the tables owned by \ndifferent services.",
      "content_length": 2224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": "295\n11.5\nAuthorizing resource access\nLINKING USERS TO THEIR RESOURCES\nTwo common anti-patterns in microser-\nvices architecture is to create one user table per service and to have a shared\nuser table that is directly accessed by multiple services to create foreign keys\nbetween users and other resources. Having a user table per service is unneces-\nsary and involves duplicates, while a shared user table across multiple services\ncreates tight coupling between the services and risks breaking them the next\ntime you change the user table’s schema. Since JWTs already contain opaque\nuser IDs under the sub field, it’s good practice to rely on that identifier to link\nusers to their resources.\nListing 11.6 shows how we add a user_id field to the OrderModel class. The following\ncode goes in the orders/repository/models.py file, and the newly added code is high-\nlighted in bold.\n# file: orders/repository/models.py\nclass OrderModel(Base):\n    __tablename__ = 'order'\n    id = Column(String, primary_key=True, default=generate_uuid)\n    user_id = Column(String, nullable=False)      \n    items = relationship('OrderItemModel', backref='order')\n    status = Column(String, nullable=False, default='created')\n    created = Column(DateTime, default=datetime.utcnow)\n    schedule_id = Column(String)\n    delivery_id = Column(String)\nNow that we’ve updated the models, we need to update the database by running a\nmigration. As we saw in chapter 7, running a migration is the process of updating the\ndatabase schema. As we did in chapter 7, we use Alembic to manage our migrations,\nwhich is Python’s best database migration management library. Alembic checks the\ndifference between the OrderModel model and the order table’s current schema, and\nit performs the necessary updates to add the user_id column.\nALTERING TABLES IN SQLITE\nSQLite has limited support for ALTER statements.\nFor example, SQLite doesn’t support adding a new column to a table through\nan ALTER statement. As you can see in figure 11.11, to work around this prob-\nlem, we need to copy the table’s data to a temporary table and drop the origi-\nnal table. Then we re-create the table with the new fields, copy the data from\nthe temporary table, and drop the temporary table. Alembic handles these\noperations with its batch operations strategy.\nBefore we can run the migration, we need to update the Alembic configuration. The\nchange in listing 11.6 adds a new column to the order table, which translates into an\nALTER TABLE SQL statement. For local development, we’re working with SQLite,\nwhich has limited support for ALTER statements. To ensure that Alembic generates the\nListing 11.6\nAdding a user ID foreign key to the order table\nWe add a new \ncolumn called \nuser_id.",
      "content_length": 2732,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "296\nCHAPTER 11\nAPI authorization and authentication\nright migrations for SQLite, we need to update its configuration to run batch opera-\ntions. You only need to do this if you work with SQLite.\n To update the Alembic configuration so that we can run the migration, open the\nmigrations/env.py file and search for a function called run_migrations_online().\nThis is the function that runs the migrations against our database. Within that func-\ntion, search for the following block:\n# file: migrations/env.py\nwith connectable.connect() as connection:\n    context.configure(\n        connection=connection,\n        target_metadata=target_metadata\n    )\nAnd add the following line (highlighted in bold) within the call to the configure()\nmethod:\n# file: migrations/env.py\nwith connectable.connect() as connection:\n    context.configure(\n        connection=connection,\n        target_metadata=target_metadata,\n        render_as_batch=True\n    )\nTable\nItem 1\nItem 2\nItem 3\nTemporary Table\nItem 1\nItem 2\nItem 3\n1. Copy records to a\ntemporary table.\n2. Drop the\ntable.\nTable\n3. Re-create the table\nwith the new ﬁelds\n4. Copy data from the\ntemporary table.\nFigure 11.11\nWhen working with SQLite, we use batch operations to make changes to our tables. In a \nbatch operation, we copy data from the original table to a temporary table; then, we drop the original \ntable and re-create it with the new fields; and, finally, we copy back data from the temporary table.",
      "content_length": 1450,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "297\n11.5\nAuthorizing resource access\nNow we can generate the Alembic migration and update the database. Run the follow-\ning command to create the new migration:\n$ PYTHONPATH=`pwd` alembic revision --autogenerate -m \"Add user id to order table\"\nNext, we run the migration with the following command:\n$ PYTHONPATH=`pwd` alembic upgrade heads\nOur database is now ready to start linking orders and users. The next section explains\nhow we fetch the user ID from the request object and feed it to our data repositories.\n11.5.2 Restricting user access to their own resources\nNow that our database is ready, we need to update our API views to capture the user\nID when creating or updating an order, or when retrieving the list of orders. Since the\nchanges that we need to make to our view functions are all quite similar, we’ll illus-\ntrate how to apply the changes to some of the views. You can refer to the GitHub\nrepository for this book for the full list of changes.\n Listing 11.7 shows how to update the create_order() view function to capture the\nuser ID when placing the order. The newly added code is highlighted in bold. As we\nsaw in section 11.4.2, we store the user ID under the request’s state property, so the\nfirst change we make is changing the signature of the create_order() function to\ninclude the request object. The second change is passing the user ID to the Order-\nService’s place_order() method.\n# file: orders/web/api/api.py\n@app.post(\n    \"/orders\", status_code=status.HTTP_201_CREATED, \nresponse_model=GetOrderSchema\n)\ndef create_order(request: Request, payload: CreateOrderSchema):     \n    with UnitOfWork() as unit_of_work:\n        repo = OrdersRepository(unit_of_work.session)\n        orders_service = OrdersService(repo)\n        order = payload.dict()[\"order\"]\n        for item in order:\n            item[\"size\"] = item[\"size\"].value\n        order = orders_service.place_order(order, request.state.user_id)   \n        unit_of_work.commit()\n        return_payload = order.dict()\n    return return_payload\nWe also need to change the OrdersService and the OrdersRepository to ensure they\ntoo capture the user ID. The following code shows how to update the OrdersService\nto capture the user ID:\nListing 11.7\nCapturing the user ID when placing an order\nWe capture the\nrequest object in the\nfunction signature.\nWe capture the user\nID from the request’s\nstate object.",
      "content_length": 2383,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "298\nCHAPTER 11\nAPI authorization and authentication\n# file: orders/orders_service/orders_service.py\nclass OrdersService:\n    def __init__(self, orders_repository: OrdersRepository):\n        self.orders_repository = orders_repository\n    def place_order(self, items, user_id):\n        return self.orders_repository.add(items, user_id)\nAnd the following code shows how to update the OrdersRepository to capture the\nuser ID:\n# file: orders/repository/orders_repository.py\nclass OrdersRepository:\n    def __init__(self, session):\n        self.session = session\n    def add(self, items, user_id):\n        record = OrderModel(\n            items=[OrderItemModel(**item) for item in items],\n            user_id=user_id\n        )\n        self.session.add(record)\n        return Order(**record.dict(), order_=record)\nNow that we know how to save an order with the user ID, let’s see how we make sure a\nuser gets only a list of their own orders when they call the GET /orders endpoint.\nListing 11.8 shows the changes required to the get_orders() function, which imple-\nments the GET /orders endpoint. The newly added code is shown in bold. As you can\nsee, in this case we also need to change the function’s signature to capture the request\nobject. Then we simply pass on the user ID as one of the query filters. No additional\nchanges are required anywhere else in the code since both OrdersService and\nOrdersRepository are designed to accept arbitrary dictionaries of filters.\n# file: orders/web/api/api.py\n@app.get(\"/orders\", response_model=GetOrdersSchema)\ndef get_orders(\n    request: Request,\n    cancelled: Optional[bool] = None,\n    limit: Optional[int] = None\n):\n    with UnitOfWork() as unit_of_work:\n        repo = OrdersRepository(unit_of_work.session)\n        orders_service = OrdersService(repo)\n        results = orders_service.list_orders(\n            limit=limit, cancelled=cancelled, user_id=request.state.user_id\n        )\n    return {\"orders\": [result.dict() for result in results]}\nListing 11.8\nEnsuring a user only gets a list of their own orders",
      "content_length": 2055,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "299\n11.5\nAuthorizing resource access\nLet’s now turn our attention to the GET /orders/{order_id} endpoint. What hap-\npens if a user tries to retrieve the details of an order that doesn’t belong to them? We\ncan respond with two strategies: return a 404 (Not Found) response indicating that\nthe requested order doesn’t exist, or respond with a 403 (Forbidden) response, indi-\ncating that the user doesn’t have access to the requested resource.\n Technically, a 403 response is more correct than a 404 when a user is trying to\naccess a resource that doesn’t belong to them. But it also exposes unnecessary infor-\nmation. A malicious user who has valid credentials could leverage our 403 responses\nto build a map of the existing resources in the server. To avoid that problem, we opt\nfor disclosing less information and return a 404 response. The user ID will become an\nadditional filter when we attempt to retrieve an order from the database.\n The following code shows the changes required to the get_order() function to\ninclude the user ID in our queries, with the newly added code in bold. Again, we\ninclude the request object in the function signature, and we pass on the user ID to the\nOrderService’s get_order() method.\n# file: orders/web/api/api.py\n@app.get(\"/orders/{order_id}\", response_model=GetOrderSchema)\ndef get_order(request: Request, order_id: UUID):\n    try:\n        with UnitOfWork() as unit_of_work:\n            repo = OrdersRepository(unit_of_work.session)\n            orders_service = OrdersService(repo)\n            order = orders_service.get_order(\n                order_id=order_id, user_id=request.state.user_id\n            )\n        return order.dict()\n    except OrderNotFoundError:\n        raise HTTPException(\n            status_code=404, detail=f\"Order with ID {order_id} not found\"\n        )\nTo be able to query orders by user ID as well, we also need to update the Orders-\nService and the OrdersRepository classes. We’ll change their methods to accept an\noptional dictionary of arbitrary filters. The OrdersService’s get_order() method\nchanges like this:\n# file: orders/orders_service/orders_service.py\ndef get_order(self, order_id, **filters):\n    order = self.orders_repository.get(order_id, **filters)\n    if order is not None:\n        return order\n    raise OrderNotFoundError(f\"Order with id {order_id} not found\")\nListing 11.9\nFiltering orders with order ID and user ID",
      "content_length": 2401,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "300\nCHAPTER 11\nAPI authorization and authentication\nAnd the OrdersRepository’s get() and _get() methods require the following changes:\n# file: orders/repository/orders_repository.py\ndef _get(self, id_, **filters):\n    return (\n        self.session.query(OrderModel)\n        .filter(OrderModel.id == str(id_)).filter_by(**filters)\n        .first()\n    )\ndef get(self, id_, **filters):\n    order = self._get(id_, **filters)\n    if order is not None:\n        return Order(**order.dict())\nThe rest of the view functions in the orders/web/api/api.py file require changes simi-\nlar to the ones we’ve seen in this section, and the same goes for the remaining meth-\nods of the OrdersService and the OrdersRepository classes. As an exercise, I\nrecommend you try to complete the changes necessary to add authorization to the\nremaining API endpoints. The GitHub repository for this book contains the full list of\nchanges, so feel free to check it out for guidance. \n This concludes our journey through API authentication and authorization, and\nwhat a journey! You’ve learned what OAuth and OpenID Connect are and how they\nwork. You’ve learned about OAuth flows and when to use each flow. You’ve learned\nwhat JWTs are, how to inspect their payloads, and how to produce and validate them.\nFinally, you’ve learned how to authorize API requests and how to authorize user access\nto specific resources. You’ve got all you need to start adding robust authentication and\nauthorization to your own APIs!\n Appendix C teaches you how to integrate with an identity provider such as Auth0.\nYou’ll also see practical examples of how to use the PKCE and client credentials flows,\nand you’ll learn to authorize your requests using a Swagger UI.\nSummary\nWe authorize access to our APIs using the standard protocols OAuth and OpenID\nConnect.\nOAuth is an access delegation protocol that allows a user to grant an applica-\ntion access to resources they own in a different website. It distinguishes four\nauthorization flows:\n– Authorization code—The API server exchanges a code with the authorization\nserver to request the user’s access token.\n– PKCE—The client application, typically an SPA, uses a code verifier and a\ncode challenge to obtain an access token from the authorization server.",
      "content_length": 2261,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "301\nSummary\n– Client credentials—The client, typically another microservice, exchanges a pri-\nvate secret in return for an access token.\n– Refresh token—A client obtains a new access token in exchange for a refresh\ntoken.\nOpenID Connect is an identity verification protocol that builds on top of\nOAuth. It helps users easily authenticate to new websites by bringing their iden-\ntity from other websites, such as Google or Facebook.\nJWTs are JSON documents that contain claims about the user’s access permis-\nsions. JWTs are encoded using base64url encoding and are typically signed\nusing a private/public key.\nTo authenticate a request, users send their access tokens in the request’s Autho-\nrization header. The expected format of this header is Authorization: Bearer\n<ACCESS_TOKEN>.\nWe use PyJWT to validate access tokens. PyJWT checks that the token isn’t\nexpired, that the audience is correct, and that the signature can be verified with\none of the available public keys. If the token is invalid, we reject the request\nwith a 401 (Unauthorized) response.\nTo link users to their resources, we use the user ID as represented in the sub\nclaim of the JWT.\nIf a user tries to access a resource that doesn’t belong to them, we respond with\na 403 (Forbidden) response.\nOPTIONS requests are known as CORS requests or preflight requests. CORS\nrequests must not be protected by credentials.",
      "content_length": 1391,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "302\nTesting and\nvalidating APIs\nThis chapter teaches you how to test and validate API implementations. Thus far,\nwe’ve learned to design and build APIs to drive integrations between microservices.\nAlong the way, we did some manual tests to ensure our implementations exhibited\nthe correct behavior. However, those tests were minimal, and most importantly,\nthey were purely manual and therefore not repeatable in an automated fashion.\n In this chapter, we learn how to run an exhaustive test suite against our API\nimplementations using tools such as Dredd and Schemathesis, tools for API testing\nthat are part of every API developer’s tool kit. Both Dredd and Schemathesis work\nby looking at the API specification and automatically generating tests against our\nThis chapter covers\nGenerating automatic tests for REST APIs using \nDredd and Schemathesis\nWriting Dredd hooks to customize the behavior \nof your Dredd test suite\nUsing property-based testing to test APIs\nLeveraging OpenAPI links to enhance your \nSchemathesis test suite\nTesting GraphQL APIs with Schemathesis",
      "content_length": 1074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "303\n12.1\nSetting up the environment for API testing\nAPI server. For an API developer, this is very handy because it means you can focus\nyour efforts on building your APIs instead of testing them.\n By using tools such as Dredd and Schemathesis, you can save time and energy\nwhile resting assured that the implementation you’re delivering is correct. You can\nrun Dredd and Schemathesis in combination, or you can choose one of them. As\nyou’ll see, Dredd runs a more basic test suite that is very useful in the early stages of\nyour API development cycle, while Schemathesis runs a robust test suite that is useful\nbefore you release your APIs to production.\n To illustrate how we test REST APIs, we’ll use the orders API, which we imple-\nmented in chapters 2 and 6. To illustrate how we test GraphQL APIs, we’ll use the\nproducts API, which we implemented in chapter 10. As a recap, both APIs are part of\nCoffeeMesh, the fictional on-demand coffee delivery platform that we’re building in\nthis book. The orders API is the interface to the orders service, which manages cus-\ntomers’ orders, while the products API is the interface to the products service, which\nmanages the catalogue of products CoffeeMesh offers.\n The code for this chapter is available in GitHub, under the folder named ch12. In\nsection 12.1, we set up the folder structure and the environments to work on this\nchapter’s examples, so make sure you go through that section if you want to follow\nalong with the examples in this chapter.\n12.1\nSetting up the environment for API testing\nIn this section, we set up the environment to follow along with the examples in this\nchapter. Let’s start by setting up the folder structure. Create a new folder called ch12\nand cd into it. Within this folder, we’ll copy the orders API and the products API. To\nkeep things simple in this chapter, we use the implementation of the orders API as we\nleft it in chapter 6. Chapter 6 contains a full implementation of the orders API, but it\nlacks a real database and integration with other services (those features were added in\nchapter 7). Since the goal of this chapter is to learn how to test APIs, the implementa-\ntion in chapter 6 is sufficient and will help us stay focused, as we won’t have to set up the\ndatabase and run additional services. In real life, you’d want to test the API layer in isola-\ntion and run integration tests, including on the database. See the README.md file\nunder the ch12/orders folder in the GitHub repository for this chapter for instructions\non running the tests against the state of the application after chapters 7 and 11.\n Within the ch12 folder, copy the implementation of the orders API from\nch06/orders by running the following command:\n$ cp -r ../ch06/orders orders\ncd into ch12/orders and run the following command to install the dependencies:\n$ pipenv install --dev\nDon’t forget to include the --dev flag when you run pipenv install, which tells pipenv\nto install both production and development dependencies. In this chapter, we’ll use",
      "content_length": 3022,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "304\nCHAPTER 12\nTesting and validating APIs\ndevelopment packages to test the orders API. To run the tests, we’ll need pytest,\ndredd_hooks, and schemathesis, which you can install with the following command:\n$ pipenv install --dev dredd_hooks pytest schemathesis\nTo run the tests, we’ll use a slightly modified version of the orders API specification\nwithout the bearerAuth security scheme, which you can find under the ch12/orders/\noas.yaml file in the GitHub repository for this book. In this chapter, we’ll focus on test-\ning that the API implementation complies with the API specification, namely, ensuring\nthe API uses the right schemas, the right status codes, and so on. API security testing is a\nwhole different topic, and for that I recommend chapter 11 of Mark Winteringham’s Test-\ning Web APIs (Manning, 2022) and Corey J. Ball’s Hacking APIs (No Starch Press, 2022).\n Let’s now copy the implementation of the products API from chapter 10. Go\nback to the top level of the ch12 directory by running cd .. and then execute the\nfollowing command:\n$ cp -r ../ch10 products\ncd into ch12/products and run pipenv install --dev to install the dependencies.\nWe’ll use pytest and schemathesis to test the products API, which you can install by\nrunning the following command:\n$ pipenv install pytest schemathesis\nWe’re now all set up to start testing the APIs. We’ll start our journey by learning about\nthe Dredd API testing framework.\n12.2\nTesting REST APIs with Dredd\nThis section explains what Dredd is and how we use it to test REST APIs. Dredd is an\nAPI testing framework that automatically generates tests to validate the behavior of\nour API server. It generates tests by parsing the API specification and learning from it\nhow the API is expected to work. Using Dredd is very helpful during development\nbecause it means we can focus our efforts on building the API while Dredd ensures\nthat our work is going in the right direction. Dredd was released by Apiary in 2017 as\nthe first tool of its kind (http://mng.bz/5maq), and ever since it’s been part of every\nAPI developer’s essential tool kit.\n In this section, we’ll learn how Dredd works by using it to validate the implementa-\ntion of the orders API. We’ll start by first running a basic test suite against the API, and\nthen we’ll explore more advanced features of the framework.\n12.2.1 What is Dredd?\nBefore we start working with Dredd, let’s take a moment to understand what Dredd is\nand how it works. Dredd is an API testing framework. As shown in figure 12.1, Dredd\nworks by parsing the API specification and discovering the available URL paths and\nthe HTTP methods they accept.",
      "content_length": 2641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "305\n12.2\nTesting REST APIs with Dredd\nTo test the API, Dredd sends requests to each of the endpoints defined in the API\nspecification with the expected payloads, if any, as well as any query parameters\naccepted by the endpoint. Finally, it checks whether the responses the API receives\nconform to the schemas declared in the API specification and whether they carry the\nexpected status codes.\n Now that we understand how Dredd works, let’s start using it! The next section\nexplains how to install Dredd.\n12.2.2 Installing and running Dredd’s default test suite\nIn this section, we install Dredd and run its default test suite against the orders API. cd\ninto ch12/orders and run pipenv shell to activate the environment. Dredd is an npm\npackage, which means you need to have a Node.js runtime available in your machine, as\nwell as a package management tool for JavaScript, such as npm or Yarn. To install Dredd\nwith npm, run the following command from your ch12/orders directory:\n$ npm install dredd\nThis will install Dredd under a folder called node_modules/. Once the installation is\ncomplete, we can start using Dredd to test the API. Dredd comes with a CLI that is\navailable under the following directory: node_modules/.bin/dredd. The Dredd CLI\nexposes optional arguments that give us great flexibility in how we want to run our\nAPI server\npaths:\n/orders:\nget:\n[...]\npost:\nrequestBody:\ncontent:\napplication/json:\nschema:\n$ref: ‘#/components/schemas/CreateOrderSchema’\n[...]\nAvailable paths\n/orders path\nGET /orders endpoint\nPOST /orders endpoint\nRequest payload for the\nPOST /orders endpoint\nGET /orders\nPOST /orders\nBody {}\nGenerated test cases\nOpenAPI speciﬁcation for the orders API\nFigure 12.1\nDredd works by parsing the API specification, discovering the available endpoints, and \nlaunching tests for each endpoint.",
      "content_length": 1823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "306\nCHAPTER 12\nTesting and validating APIs\ntests. We’ll make use of some of those arguments later in this section. For now, let’s\nexecute the simplest Dredd command to run a test:\n$ ./node_modules/.bin/dredd oas.yaml http:/ /127.0.0.1:8000 --server \\\n \"uvicorn orders.app:app\"\nThe first argument for the Dredd CLI is the path to the API specification file, while\nthe second argument represents the base URL of the API server. With the --server\noption, we tell Dredd which command needs to be used to start the orders API\nserver. If you run this command now, you’ll get a few warnings from Dredd with the\nfollowing message (the ellipsis omits the path to the API specification file, which will\nbe different in your machine):\nwarn: [...] (Orders API > /orders/{order_id}/cancel > Cancels an order > \n➥ 200 > application/json): Ambiguous URI parameter in template: \n➥ /orders/{order_id}/cancel\nNo example value for required parameter in API description document: \n➥ order_id\nDredd is complaining because we haven’t provided an example of the URL parameter\norder_id, which is required in some of the URL paths. Dredd complains about the\nmissing example because it’s unable to generate random values from the specifica-\ntion. To address Dredd’s complaint, we add an example of the order_id parameter in\neach URL where it’s used. For example, for the /orders/{order_id} URL path, we\nmake the modification shown in listing 12.1 (the ellipses represent omitted code).\nThe /orders/{order_id}/pay and the /orders/{order_id}/cancel URLs also con-\ntain descriptions of the order_id parameter, so add examples to them as well. Dredd\nwill use the exact value provided in the examples to test the API.\n# file: orders/oas.yaml\n[...]\n  /orders/{order_id}:\n    parameters:\n      - in: path\n        name: order_id\n        required: true\n        schema:\n          type: string\n        example: d222e7a3-6afb-463a-9709-38eb70cc670d    \n    get:\n      [...]\nOnce we’ve added examples for the order_id parameter, we can run the Dredd CLI\nagain. This time, the tests suite runs without problems, and you’ll get a result like this:\ncomplete: 7 passing, 5 failing, 0 errors, 0 skipped, 12 total\ncomplete: Tests took 90ms\nListing 12.1\nAdding examples for the order_id URL path parameter\nWe add an example \nfor the order_id URL \nparameter.",
      "content_length": 2311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": "307\n12.2\nTesting REST APIs with Dredd\nINFO:     Shutting down\nINFO:     Finished server process [23593]\nThis summary tells us that Dredd ran 18 tests, of which 7 passed and 11 failed. The full\noutcome of the test is too long to reproduce here, but if you scroll up in the terminal,\nyou’ll see that the failing tests are on endpoints that target specific resources:\nGET, PUT, and DELETE /orders/{order_id}\nPOST /orders/{order_id}/pay\nPOST /orders/{order_id}/cancel\nDredd runs three tests for each of those endpoints, and it expects to obtain one suc-\ncessful response per endpoint. However, in the previous execution, Dredd only\nobtained 404 responses, which means the server couldn’t find the resources Dredd\nrequested. Dredd is using the ID we provided as an example in listing 12.1 when\ntesting those endpoints. To address this problem, we could add a hardcoded order\nwith that ID to our in-memory list of orders (we’d add it to the database if we were\nusing one for the tests). As we’ll see in the next section, however, a better approach\nis to use Dredd hooks.\n There’s also a failing test for the POST /orders endpoint in which Dredd expects\na 422 response. The failed tests for 422 responses happen because Dredd doesn’t\nknow how to create tests that generate those responses, and Dredd hooks will also\nhelp us address this problem.\n12.2.3 Customizing Dredd’s test suite with hooks\nDredd’s default behavior can be limited. As we’ve seen in section 12.2.1, Dredd\ndoesn’t know how to handle endpoints with URL path parameters, such as order_id\nin the /orders/{order_id} URL. Dredd doesn’t know how to produce a random\nresource ID, and if we provide an example, it expects the sample ID to be present in\nthe system during the execution of the test suite. This expectation is unhelpful, since\nit means our API is only testable when it’s in a certain state—when certain resources\nor fixtures have been loaded into the database.\nDEFINITION\nIn software testing, fixtures are the preconditions required to run a\ntest. Typically, fixtures are data that we load into a database for testing, but they\ncan also include configuration, directories and files, or infrastructure resources.\nInstead of using fixtures, we can take a better approach by using Dredd hooks. This\nsection explains what Dredd hooks are and how we use them. Dredd hooks are scripts\nthat allow us to customize Dredd’s behavior during the execution of the test suite.\nUsing Dredd hooks, we can create resources for use during the test, save their IDs,\nand clean them up after finishing the test.\n Dredd hooks allow us to trigger actions before and after the whole test suite, and\nbefore and after each endpoint-specific test. They are useful for stateful tests that",
      "content_length": 2731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": "308\nCHAPTER 12\nTesting and validating APIs\ninvolve creating resources and performing operations on them. For example, we can\nuse hooks to place an order using the POST /orders endpoint, save the ID of the\norder, and reuse the ID to perform operations on the order, such as payments and\ncancellations, with other endpoints. Using this approach, we can test that the POST\n/orders endpoint fulfills its job of creating a resource, and we can test other end-\npoints with a real resource. As illustrated in figures 12.2, 12.3, and 12.4, we’ll create\nthe following hooks with these steps:\n1\nAfter the POST /orders test, we use a hook to save the ID returned by the\nserver for the newly created order.\n2\nBefore the GET, PUT, and DELETE /orders/{order_id} tests, we use hooks\nto tell Dredd to use the ID from the order created at point (1). These end-\npoints are used to retrieve the details of the order (GET), to update the\norder (PUT), and to remove the order from the server (DELETE). There-\nfore, after running the DELETE /orders/{order_id} test, the order will no\nlonger exist in the server.\n3\nBefore the POST /orders/{order_id}/pay and the POST /orders/ {order_\nid}/cancel endpoints, we use hooks to create new orders for use in these tests.\nWe won’t be able to reuse the ID from point (1), since the DELETE /orders/\n{order_id} test from point (2) deletes the order from the server.\n4\nFor the 422 responses, we need a strategy that generates a 422 response from\nthe server. We’ll use two approaches: for the POST /orders endpoint, we’ll\nsend an invalid payload, while for the remaining endpoints, we’ll modify the\norder’s URI and include an invalid identifier.\nFigure 12.2\nAfter the POST /orders endpoint test, the save_created_order()hook saves the ID from \nthe server response body in the response_stash. The before_get_order(), before_put_order(), \nand before_delete_order() hooks use the ID from response_stash to form their resource URLs.\nPOST /orders\nBody {}\nsave_created_order()\nAPI server\nGET /orders/f299294e-db31-4c16-904d-b6baed7e997e\nPUT /orders/f299294e-db31-4c16-904d-b6baed7e997e...\nDELETE /orders/f299294e-db31-4c16-904d-b6baed7e997e\n1. The POST /orders endpoint test creates a new resource.\n2. The save_created_order() hook\nsaves the ID from the created\nresource in the response_stash.\n3. The before_get_order(),\nbefore_put_order(), and\nbefore_delete_order()\nhooks use the ID from\nthe response_stash to\nform the resource URL\nfor their tests.\nServer response\n{\n\"id\": \"f299294e-db31-4c16-904d-b6baed7e997e\",\n...\n}\nresponse_stash\nbefore_get_order()\nbefore_put_order()\nbefore_delete_order()",
      "content_length": 2602,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": "309\n12.2\nTesting REST APIs with Dredd\nUSING DREDD HOOKS TO SAVE THE ID OF A CREATED RESOURCE\nNow that we know what we want to do, let’s write our hooks! First, if you haven’t done\nit yet, cd into ch12/orders and activate the virtual environment by running pipenv\nFigure 12.3\nBefore executing the test, the before_pay_order() and the before_cancel_ \norder() hooks use the POST /orders endpoint to place a new order and use the ID from the \nresponse payload form their resource URLs.\nPOST /orders/23de3420-2e49-4a10-857c-328e1023fbdb/pay\nPOST /orders/23de3420-2e49-4a10-857c-328e1023fbdb/cancel\nbefore_cancel_order()\nbefore_pay_order()\nPOST /orders\nBody {}\nAPI server\n{\n\"id\": \"23de3420-2e49-4a10-857c-328e1023fbdb\",\n...\n}\n1. Before executing the test, the before_pay_order() and the\nbefore_cancel_order() hooks use the POST /orders endpoint\nto create a new resource for use during their tests.\n2. Once the resource is created, the hooks\nuse the ID from the response payload to\nform the resource URL.\nPOST /orders/8/cancel\nfail_target_speciﬁc_order()\nfail_create_order()\ntransaction[\"fullPath\"]\nBefore executing the test, the fail_create_order() hook injects an invalid payload\nfor the POST /orders endpoint’s test, while the faill_target_speciﬁc_order() hook\ninjects an invalid order identiﬁer for the singleton endpoints’ tests.\ntransaction[\"request\"][\"uri\"]\n8\n8\ntransaction[\"request\"][\"body\"]\nPOST /orders\n{'order': [{'product': 'string', 'size': 'asdf'}]}\nGET /orders/8\nPOST /orders/8/pay\nPUT /orders/8\nDELETE /orders/8\n{'order': [{'product': 'string', 'size': 'asdf'}]}\nAPI server\nFigure 12.4\nThe fail_create_order() and the fail_target_specific_order() hooks inject \ninvalid payloads and invalid order identifiers to trigger a 422 response from the server.",
      "content_length": 1759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": "310\nCHAPTER 12\nTesting and validating APIs\nshell. Create a file called orders/hooks.py, where we’ll write our hooks. Although\nDredd is an npm package, we can write our hooks in Python by using the dredd-hooks\nlibrary. In section 12.1, we set up the environments for this chapter, so dredd-hooks\nhas already been installed.\n To understand how Dredd hooks work, let’s look at one of them in detail. Listing 12.2\nshows the implementation of an after hook for the POST /orders endpoint. This\ncode goes into the orders/hooks.py file. We first declare a variable called response_\nstash, which we’ll use to store data from the POST /orders request. dredd-hooks pro-\nvides decorator functions, such as dredd_hooks.before() and dredd_hooks.after(),\nthat allow us to bind a function to a specific operation. dredd-hooks’ decorators\naccept an argument, which represents the path to the specific operation that we want\nto bind the hook to. As you can see in figure 12.5, in Dredd, an operation is defined as\na URL endpoint with its response status code and its content-encoding format. In list-\ning 12.2, we bind the save_created_order() hook to the 201 response of the POST\n/orders endpoint.\nDEFINING OPERATION PATHS IN DREDD HOOKS\nWhen defining the path for an\noperation using dredd-hooks, you can’t use HTTP methods as part of the\noperation path; that is, the following syntax won’t work: /orders > post > 201\n> application/json. Instead, we use other properties of the POST endpoint,\nsuch as summary or operationId, as in the following example: /orders >\nCreates an order > 201 > application/json.\npaths:\n/orders:\n[...]\npost:\nsummary: Creates an order\nresponses:\n‘201’:\ncontent:\napplication/json:\nschema:\n$ref: ‘#/components/schemas/GetOrderSchema’\n[...]\n/orders path\nSummary of the POST\n/orders operation\nSuccess response\nstatus code\nContent format of the\nresponse\n‘/orders > Creates an order > 201 > application/json’\nDredd hook path for the POST /orders endpoint test\nFigure 12.5\nTo form the path for a specific operation in a Dredd hook, you use the URL path with the \noperation’s summary, response status code, and content encoding of the response.",
      "content_length": 2145,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": "311\n12.2\nTesting REST APIs with Dredd\nDredd hooks take an argument that represents the transaction Dredd performed\nduring the test. The argument comes in the form of a dictionary. In listing 12.2, we\nname the hook’s argument transaction. Since our goal in the save_created_\norder() hook is to fetch the ID of the created order, we inspect the payload returned\nby the POST /orders endpoint, which can be found under transaction['real']\n['body']. Since our API returns JSON payloads, we load its contents using Python’s\njson library. Once we get hold of the order’s ID, we save it for later use in our global\nstate dictionary, which we named response_stash.\n# file: orders/hooks.py\nimport json\nimport dredd_hooks    \nresponse_stash = {}    \n@dredd_hooks.after('/orders > Creates an order > 201 > application/json')\ndef save_created_order(transaction):\n    response_payload = transaction['real']['body']    \n    order_id = json.loads(response_payload)['id']    \n    response_stash['created_order_id'] = order_id    \nUSING HOOKS TO MAKE DREDD USE CUSTOM URLS\nNow that we know how to save the ID of the order created in a POST request, let’s see\nhow we use the ID to form the order’s resource URL. Listing 12.3 shows how we build\nhooks for the order resource endpoints. The code shown in listing 12.3 goes into the\norders/hooks.py file. The code from listing 12.2 is omitted using ellipses, while the\nnew additions are shown in bold.\n To specify which URL Dredd should use when testing the /orders/{order_id} path,\nwe need to modify the transaction payload. In particular, we need to modify the transac-\ntion’s fullPath and its request’s uri properties and make sure they point to the right\nURL. To form the URL, we access the order’s ID from the response_stash dictionary.\n# file: orders/hooks.py\nimport json\nimport dredd_hooks\nresponse_stash = {}\n[...]\nListing 12.2\nImplementation of an after hook for the POST /orders endpoint\nListing 12.3\nUsing before hooks to tell Dredd which URL to use\nWe import the \ndredd_hooks library.\nWe create a global object to store and \nmanage the state of the test suite.\nWe create a hook to be\ntriggered after the POST\n/orders endpoint test.\nWe access the response \npayload from the POST \n/orders endpoint.\nWe load the response using\nPython’s json library and\nretrieve the order’s ID.\nWe store the order ID in our \nglobal response_stash object.",
      "content_length": 2373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": "312\nCHAPTER 12\nTesting and validating APIs\n@dredd_hooks.before(\n    '/orders/{order_id} > Returns the details of a specific order > 200 > '\n    'application/json'\n)    \ndef before_get_order(transaction):\n    transaction[\"fullPath\"] = (\n        \"/orders/\" + response_stash[\"created_order_id\"]\n    )    \n    transaction['request']['uri'] = (\n        '/orders/' + response_stash['created_order_id']\n    )\n@dredd_hooks.before(\n    '/orders/{order_id} > Replaces an existing order > 200 > '\n    'application/json'\n)\ndef before_put_order(transaction):\n    transaction['fullPath'] = (\n        '/orders/' + response_stash['created_order_id']\n    )\n    transaction['request']['uri'] = (\n        '/orders/' + response_stash['created_order_id']\n    )\n@dredd_hooks.before('/orders/{order_id} > Deletes an existing order > 204')\ndef before_delete_order(transaction):\n    transaction['fullPath'] = (\n        '/orders/' + response_stash['created_order_id']\n    )\n    transaction['request']['uri'] = (\n        '/orders/' + response_stash['created_order_id']\n    )\nUSING DREDD HOOKS TO CREATE RESOURCES BEFORE A TEST\nThe DELETE /orders/{order_id} endpoint deletes the order from the database,\nso we can’t use the same order ID to test the /orders/{order_id}/pay and\n/orders/{order_id}/cancel endpoints. Instead, we’ll use hooks to create new orders\nbefore testing those endpoints. Listing 12.4 shows how we accomplish that. The code\nin listing 12.4 goes into the orders/hooks.py file. The new code is shown in bold,\nwhile the code from previous listings is omitted with ellipses.\n To create new orders, we’ll call the POST /orders endpoint using the requests\nlibrary, which makes it easy to make HTTP requests. To launch a POST request, we\nuse requests’ post() function, passing in the target URL for the request and the\nJSON payload required to create an order. In this case, we hardcode the server base\nURL to http://127.0.0.1:8000, but you may want to make this value configurable if you\nwant to be able to run the test suite in different environments. Once we’ve created the\norder, we fetch its ID from the response payload and use the ID to modify the trans-\naction’s fullPath and its request’s uri properties.\nWe create a hook to be triggered before \nthe GET /orders/{order_id} endpoint test.\nWe change the GET \n/orders/{order_id} \nendpoint test’s URL \nto include the ID of \nthe order we created \nearlier.",
      "content_length": 2394,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": "313\n12.2\nTesting REST APIs with Dredd\n# file: orders/hooks.py\nimport json\nimport dredd_hooks\nimport requests    \nresponse_stash = {}\n[...]\n@dredd_hooks.before(\n    '/orders/{order_id}/pay > Processes payment for an order > 200 > '\n    'application/json'\n)\ndef before_pay_order(transaction):\n    response = requests.post(   \n        \"http:/ /127.0.0.1:8000/orders\",\n        json={\n            \"order\": [{\"product\": \"string\", \"size\": \"small\", \"quantity\":1}]\n        },\n    )\n    id_ = response.json()['id']    \n    transaction['fullPath'] = '/orders/' + id_ + '/pay'    \n    transaction['request']['uri'] = '/orders/' + id_ + '/pay'\n@dredd_hooks.before(\n    '/orders/{order_id}/cancel > Cancels an order > 200 > application/json'\n)\ndef before_cancel_order(transaction):\n    response = requests.post(\n        \"http:/ /127.0.0.1:8000/orders\",\n        json={\n            \"order\": [{\"product\": \"string\", \"size\": \"small\", \"quantity\":1}]\n        },\n    )\n    id_ = response.json()['id']\n    transaction['fullPath'] = '/orders/' + id_ + '/cancel'\n    transaction['request']['uri'] = '/orders/' + id_ + '/cancel'\nUSING HOOKS TO GENERATE 422 RESPONSES\nSome of the endpoints in the orders API accept request payloads or URL path param-\neters. If an API client sends an invalid payload or uses an invalid URL path parameter,\nthe API responds with a 422 response. As we saw earlier, Dredd doesn’t know how to\ngenerate 422 responses from the server, so we’ll create hooks for that.\n As you can see in listing 12.5, we only need two functions:\nListing 12.4\nUsing before hooks to create resources before a test\nWe import the \nrequests library.\nWe place a \nnew order.\nWe fetch the newly \ncreated order’s ID.\nWe change the POST /orders/{order_id}/pay endpoint test’s \nURL by to include the ID of the order we created earlier.",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": "314\nCHAPTER 12\nTesting and validating APIs\n\nfail_create_order() intercepts the request for the POST /orders endpoint\nbefore it reaches the server, and it modifies its payload with an invalid value for\nthe size property.\n\nfail_target_specific_order() modifies the order’s URI with an invalid iden-\ntifier. Since we know that Dredd fires this test using the example ID we provided\nin the API specification, we simply need to replace that ID with an invalid value.\nThe type of the order_id path parameter is a UUID, so by replacing it with an\ninteger, the server will respond with a 422 status code.\nThese hooks are a good opportunity to test how the server behaves with different\ntypes of payloads and parameters, and if you need to, you can create specific tests for\neach endpoint for more comprehensive test coverage.\n# file: orders/hooks.py\n@dredd_hooks.before('/orders > Creates an order > 422 > application/json')\ndef fail_create_order(transaction):\n    transaction[\"request\"][\"body\"] = json.dumps(\n        {\"order\": [{\"product\": \"string\", \"size\": \"asdf\"}]}\n    )\n@dredd_hooks.before(\n    \"/orders/{order_id} > Returns the details of a specific order > 422 > \"\n    \"application/json\"\n)\n@dredd_hooks.before(\n    \"/orders/{order_id}/cancel > Cancels an order > 422 > application/json\"\n)\n@dredd_hooks.before(\n    \"/orders/{order_id}/pay > Processes payment for an order > 422 > \"\n    \"application/json\"\n)\n@dredd_hooks.before(\n    \"/orders/{order_id} > Replaces an existing order > 422 > \"\n    \"application/json\"\n)\n@dredd_hooks.before(\n    \"/orders/{order_id} > Deletes an existing order > 422 > \"\n    \"application/json\"\n)\ndef fail_target_specific_order(transaction):\n    transaction[\"fullPath\"] = transaction[\"fullPath\"].replace(\n        \"d222e7a3-6afb-463a-9709-38eb70cc670d\", \"8\"\n    )\n    transaction[\"request\"][\"uri\"] = transaction[\"request\"][\"uri\"].replace(\n        \"d222e7a3-6afb-463a-9709-38eb70cc670d\", \"8\"\n    )\nListing 12.5\nGenerating 422 responses with Dredd hooks",
      "content_length": 1977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": "315\n12.3\nIntroduction to property-based testing\nRUNNING DREDD WITH CUSTOM HOOKS\nNow that we have Dredd hooks to make sure that each URL is correctly formed, we\ncan run the Dredd test suite again. The following command shows how to run Dredd\nusing a hooks file:\n$ ./node_modules/.bin/dredd oas.yaml http:/ /127.0.0.1:8000 --server \\ \n \"uvicorn orders.app:app\" --hookfiles=./hooks.py --language=python\nAs you can see, we simply need to pass the path to our hooks file using the --hookfiles\nflag. We also need to specify the language in which the hooks are written by using the\n--language flag. If you run the command now, you’ll see now that all tests pass.\n12.2.4 Using Dredd in your API testing strategy\nDredd is a fantastic tool for testing API implementations, but its test suite is limited.\nDredd only tests the happy path of each endpoint. For example, to test the POST\n/orders endpoint, Dredd sends only a valid payload to the endpoint and expects it\nto be processed correctly. It doesn’t send malformed payloads, so by using Dredd\nalone, we don’t know how the server reacts in those situations. This is fine when\nwe’re in the early stage development of our service and we don’t want to be carried\naway by the API layer.\n However, before we release our code, we must ensure it works as expected in all\nsituations, and to run tests that go beyond the happy path, we need to use a differ-\nent library: schemathesis. We’ll learn about Schemathesis in section 12.4, but\nbefore we do that, we need to understand the core approach to testing that Sche-\nmathesis uses: property-based testing. That’s the topic of our next section, so move\non to learn more about it!\n12.3\nIntroduction to property-based testing\nThis section explains what property-based testing is, how it works, and how it helps us\nwrite more exhaustive tests for our APIs. Along the way, you’ll also lean about Python’s\nexcellent property-based testing library, hypothesis. As you’ll see, property-based test-\ning helps us create robust test suites for APIs, allowing us to easily generate hundreds\nof test cases with multiple combinations of properties and types. This section paves\nthe way for the upcoming sections of this chapter, where we’ll learn about Schemathe-\nsis, an API testing framework that uses property-based testing.\n12.3.1 What is property-based testing?\nAs you can see in figure 12.6, property-based testing is a testing strategy in which we\nfeed test data to our code and design our tests to make claims about the properties of\nthe result of running our code.1 Typically, a property-based framework generates test\ncases for us given a set of conditions that we define.\n1 See the excellent article by David R. MacIver for a more detailed explanation of what property-based testing\nis: “What is Property Based Testing?,” https://hypothesis.works/articles/what-is-property-based-testing/.",
      "content_length": 2870,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": "316\nCHAPTER 12\nTesting and validating APIs\nDEFINITION\nProperty-based testing is an approach to testing in which we make\nclaims about the properties of the return value of our functions or methods.\nInstead of manually writing lots of different tests with various inputs, we let a\nframework generate the inputs for us, and we define how we expect our code to\nhandle them. In Python, an excellent library for property-based testing is\nHypothesis (https://github.com/HypothesisWorks/hypothesis).\n12.3.2 The traditional approach to API testing\nLet’s say we want to test our POST /orders endpoint to ensure it only accepts valid\npayloads. As you can see from the OpenAPI specification for the orders API under the\nch012/orders/oas.yaml file, a valid payload for the POST /orders endpoint contains\na key named order, which represents an array of ordered items. Each item has two\nrequired keys: product and size. \n# file: orders/oas.yaml\ncomponents:\n  schemas:\n    OrderItemSchema:\n      type: object\n      additionalProperties: false\n      required:\n        - product\n        - size\n      properties:\n        product:\n          type: string\n        size:\n          type: string\n          enum:\n            - small\n            - medium\nListing 12.6\nSchema for the POST /orders endpoint’s request payload\n2. We run our code on the test data.\n3. We verify that our code runs correctly\nby making assertions on the properties\nof the result.\n1. We use a framework to generate\ntest data for our code.\nFigure 12.6\nIn property-based testing, we use a framework to generate test cases for \nour functions, and we make assertions on the result of running our code on such cases.",
      "content_length": 1659,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": "317\n12.3\nIntroduction to property-based testing\n            - big\n        quantity:\n          type: integer\n          format: int64\n          default: 1\n          minimum: 1\n    CreateOrderSchema:\n      type: object\n      additionalProperties: false\n      required:\n        - order\n      properties:\n        order:\n          type: array\n          minItems: 1\n          items:\n            $ref: '#/components/schemas/OrderItemSchema'\nIn a traditional approach, we’d write various payloads manually, then submit them to\nthe POST /orders endpoint and write the expected result for each payload. Listing 12.7\nillustrates how we test the POST /orders endpoint with two different payloads. If you\nwant to try out the code in listing 12.7, create a file called orders/test.py and run the\ntests with the following command: pytest test.py.\n In listing 12.7, we define two test cases: one with an invalid payload missing the\nrequired size property of an order item and another with a valid payload. In both\ncases, we use FastAPI’s test client to send the payloads to our API server, and we test\nthe server’s behavior by checking the status code from the response. We expect the\nresponse for an invalid payload to carry the 422 status code (Unprocessable Entity),\nand the response for the valid payload to carry the 201 status code (Created). FastAPI\nuses pydantic to validate our payloads, and it automatically generates a 422 response\nfor malformed payloads. Therefore, this test serves to validate that our pydantic mod-\nels are correctly implemented.\n# file: orders/test.py\nfrom fastapi.testclient import TestClient    \nfrom orders.app import app\ntest_client = TestClient(app=app)    \ndef test_create_order_fails():    \n    bad_payload = {\n        'order': [{'product': 'coffee'}]    \n    }\nListing 12.7\nTesting the POST /orders endpoint with different payloads\nWe import FastAPI’s \nTestClient class.\nWe instantiate \nthe test client.\nWe create a test.\nWe define a bad payload for \nthe POST /orders endpoint.",
      "content_length": 2000,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": "318\nCHAPTER 12\nTesting and validating APIs\n    response = test_client.post('/orders', json=bad_payload)    \n    assert response.status_code == 422    \ndef test_create_order_succeeds():\n    good_payload = {\n        'order': [{'product': 'coffee', 'size': 'big'}]    \n    }\n    response = test_client.post('/orders', json=good_payload)\n    assert response.status_code == 201    \n12.3.3 Property-based testing with Hypothesis\nThe testing strategy in listing 12.7, where we write all the test cases manually, is a com-\nmon approach to API testing. The problem with this approach is that it’s quite limited\nunless we’re willing to spend many hours writing exhaustive test suites. The test suite\nin listing 12.7 is far from complete: it’s not testing what happens if the size property\ncontains an invalid value, or if the quantity property is present with a negative value,\nor if the list of order items is empty.\n For a more comprehensive approach to API testing, we want to be able to use a\nframework that can generate all possible types of payloads and test them against our\nAPI server. This is exactly what property-based testing allows us to do. In Python, we\ncan run property-based tests with the help of the excellent hypothesis library.\n Hypothesis uses the concept of strategy to generate test data. For example, if we\nwant to generate random integers, we use Hypothesis’s integers() strategy, and if\nwe want to generate text data, we use Hypothesis’s text() strategy. Hypothesis’s\nstrategies expose a method called example() that you can use to get an idea of the\nvalues they produce. You can get a feeling of how Hypothesis’s strategies work by\nplaying with them in a Python shell (since Hypothesis produces random values,\nyou’ll see different results in your shell):\n>>> from hypothesis import strategies as st\n>>> st.integers().example()\n0\n>>> st.text().example()\n'r'\nAs you can see in figure 12.7, Hypothesis also allows us to combine various strategies\nusing the pipe operator (|). For example, we can define a strategy that produces\neither integers or text:\n>>> strategy = st.integers() | st.text()\n>>> strategy.example()\n-2781\nTo test the POST /orders endpoint with Hypothesis, we want to define a strategy that\nproduces dictionaries with random values. To work with dictionaries, we can use\nWe test the \npayload.\nWe confirm that the \nresponse status code is 422.\nWe define a valid \npayload for the POST \n/orders endpoint.\nWe confirm that \nthe response status \ncode is 201.",
      "content_length": 2484,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": null,
      "content": "319\n12.3\nIntroduction to property-based testing\neither Hypothesis’s dictionaries() or fixed_dictionaries() strategies. For exam-\nple, if we want to generate a dictionary with two keys, such as product and size, where\neach key can be either an integer or a text, we’d use the following declaration:\n>>> strategy = st.fixed_dictionaries(\n    {\n        \"product\": st.integers() | st.text(),\n        \"size\": st.integers() | st.text(),\n    }\n)\n>>> strategy.example()\n{'product': -7958791642907854994, 'size': 16875}\n12.3.4 Using Hypothesis to test a REST API endpoint\nLet’s put all of this together to create an actual test for the POST /orders endpoint.\nFirst, let’s define a strategy for all the values that a property in our payload can take.\nWe’ll keep it simple for illustration purposes and assume properties can only be null,\nBooleans, text, or integers:\n>>> values_strategy = (\n        st.none() |\n        st.booleans() |\n        st.text() |\n        st.integers()\n)\nNow, let’s define a strategy for the schema that represents an order item. To keep it sim-\nple, we use a fixed dictionary with valid keys, that is, product, size, and quantity. Since\nthe size property can only take on values from an enumeration whose choices are\nsmall, medium, or big, we define a strategy that allows Hypothesis to choose a value either\nfrom that enumeration or from the values_strategy strategy that we defined earlier:\n>>> order_item_strategy = st.fixed_dictionaries(\n    {\n        \"product\": values_strategy,\nProduces any of the\nvalues above\nFigure 12.7\nWe can combine various Hypothesis strategies into one. The resulting \nstrategy will produce a value from any of the combined strategies at random.",
      "content_length": 1690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": null,
      "content": "320\nCHAPTER 12\nTesting and validating APIs\n        \"size\": st.one_of(st.sampled_from((\"small\", \"medium\", \"big\")))\n        | values_strategy,\n        \"quantity\": values_strategy,\n    }\n)\nFinally, as you can see in figure 12.8, we put all of this together in a strategy for the\nCreateOrderSchema schema. From listing 12.4, we know that CreateOrderSchema\nrequires a property called order, whose value is a list of order items. Using Hypothesis,\nwe can define a strategy that generates payloads to test the CreateOrderSchema\nschema like this:\n>>> strategy = st.fixed_dictionaries({\n    'order': st.lists(order_item_strategy)\n})\n>>> strategy.example()\n{'order': [{'product': None, 'size': 'small', 'quantity': None}]}\nWe’re now ready to rewrite our test suite from listing 12.6 into a more generic and\ncomprehensive test for the POST /orders endpoint. Listing 12.7 shows how we inject\nHypothesis strategies into a test function. The code in listing 12.7 goes into the\n{...\nst.ﬁxed_dictionaries(\n'product': values_strategy,\n'size': st.one_of(st.sampled_from(('small', 'medium', 'big'))) | values_strategy,\n'quantity': values_strategy\n)\nstrategy = st.ﬁxed_dictionaries({...\nProduces None, True/False,\nrandom text, or random integers\nProduces 'small', 'medium', or 'big'\nProduces a dictionary with the\nfollowing keys: 'product', 'size', and\n'quantity';\n{'product': None, 'size': 'small', 'quantity': None}]\nstrategy = st.ﬁxed_dictionaries({\n'order': st.lists(order_item_strategy)\n})\n{'order': [{'product': None, 'size': 'small', 'quantity': None}]}\n{\n}\nFigure 12.8\nBy combining Hypothesis’s fixed_dictionaries() strategy with the lists() strategy and \nthe values_strategy, we can produce payloads that resemble the CreateOrderSchema schema.",
      "content_length": 1732,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": null,
      "content": "321\n12.3\nIntroduction to property-based testing\norders/test.py file. I’ve omitted the definitions of some variables in listing 12.7, such\nas values_strategy and order_item_strategy, since we already came across them in\nthe previous examples.\n The testing strategy in listing 12.8 uses the jsonschema library to validate the pay-\nloads generated by Hypothesis. To validate payloads with the jsonschema library, we\nfirst load the OpenAPI specification for the orders API, which lives under ch012/\norders/oas.yaml. We read the file contents using pathlib’s Path().read_text()\nmethod, and we parse them using Python’s yaml library. To check whether a payload\nis valid, we create a utility function called is_valid_payload(), which returns True if\nthe payload is valid and, otherwise, returns False. \n We validate the payload using jsonschema’s validate() function, which requires\ntwo arguments: the payload that we want to validate and the schema that we want to\nvalidate against. Since CreateOrderSchema contains a reference to another schema\nwithin the API specification, namely, the OrderItemSchema schema, we also provide a\nresolver, which jsonschema can use to resolve references to other schemas within the\ndocument. jsonschema’s validate() function raises a ValidationError if the pay-\nload is invalid, so we call it within a try/except block, and we return True or False\ndepending on the result.\n To inject data into our test functions, Hypothesis provides the given() decorator,\nwhich takes a Hypothesis strategy as an argument and uses it to feed test cases to our\ntest function. If the payloads are valid, we expect our API to return a response with the\n201 status code, while for bad payloads we expect a 422 status code.\n# file: orders/test.py\nfrom pathlib import Path\nimport hypothesis.strategies as st\nimport jsonschema\nimport yaml\nfrom fastapi.testclient import TestClient\nfrom hypothesis import given, Verbosity, settings\nfrom jsonschema import ValidationError, RefResolver\nfrom orders.app import app\norders_api_spec = yaml.full_load(\n    (Path(__file__).parent / 'oas.yaml').read_text()    \n)\ncreate_order_schema = ( \norders_api_spec['components']['schemas']['CreateOrderSchema']    \n)\ndef is_valid_payload(payload, schema):   \n    try:\n        jsonschema.validate(\nListing 12.8\nUsing hypothesis to run property-based tests against an API \nWe load the API\nspecification.\nPointer to the\nCreateOrderSchema\nschema\nHelper function to determine \nwhether a payload is valid",
      "content_length": 2481,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": null,
      "content": "322\nCHAPTER 12\nTesting and validating APIs\n            payload, schema=schema,\n            resolver=RefResolver('', orders_api_spec)    \n        )\n    except ValidationError:\n        return False\n    else:\n        return True\ntest_client = TestClient(app=app)    \nvalues_strategy = [...]\norder_item_strategy = [...]\nstrategy = [...]\n@given(strategy)    \ndef test(payload):    \n    response = test_client.post('/orders', json=payload)    \n    if is_valid_payload(payload, create_order_schema):    \n        assert response.status_code == 201\n    else:\n        assert response.status_code == 422\nAs it turns out, Hypothesis is very suitable for generating datasets based on JSON\nSchema schemas, and there’s already a library that translates schemas into Hypothesis\nstrategies, so you don’t have to do it yourself: hypothesis-jsonschema (https://github\n.com/Zac-HD/hypothesis-jsonschema). I strongly encourage you to look at this\nlibrary before trying to generate your own Hypothesis strategies for testing web APIs.\nNow that we understand what property-based testing is and how Hypothesis works,\nwe’re ready to learn about Schemathesis, which is the topic of our next section!\n12.4\nTesting REST APIs with Schemathesis\nThis section introduces Schemathesis and explains how it works and how we use it to\ntest REST APIs. Schemathesis is an API testing framework that uses property-based\ntesting to validate our APIs. It uses the hypothesis library under the hood, and thanks\nto its approach, it’s capable of running a more exhaustive test suite than Dredd. Once\nyou’re getting ready to release your APIs to production, I recommend you test them\nwith Schemathesis to make sure you cover all edge cases.\n12.4.1 Running Schemathesis’s default test suite\nIn this section, we’ll get familiar with Schemathesis by running its default test suite.\nSince we already installed our dependencies in section 12.1, all we need to do is cd\ninto the orders folder and activate our environment by running pipenv shell. In con-\ntrast with Dredd, Schemathesis requires you to have your API server running before\nyou run your test suite. You can start the server by opening a new terminal window and\nWe validate a payload \nwith jsonschema’s \nvalidate() function.\nWe instantiate \nthe test client.\nWe feed the hypothesis \nstrategies into our test function.\nWe capture each test case \nthrough the payload argument.\nWe send the \npayload to the POST \n/orders endpoint.\nWe assert the expected \nstatus code depending on \nwhether the payload is valid.",
      "content_length": 2517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": null,
      "content": "323\n12.4\nTesting REST APIs with Schemathesis\nrunning the server there or by starting the server and pushing it to the background\nwith the following command:\n$ uvicorn orders.app:app &\nThe & symbol pushes the process to the background. Then you can run Schemathesis\nwith the following command:\n$ schemathesis run oas.yaml --base-url=http://localhost:8000 \\\n--hypothesis-database=none\nHypothesis, the library that Schemathesis uses to generate test cases, creates a folder\ncalled .hypothesis/ where it caches some of its tests. In my experience, Hypothesis’s\ncache sometimes causes misleading results in subsequent test executions, so until this\nis fixed, my recommendation is to avoid caching the tests. We set the --hypothesis-\ndatabase flag to none so that Schemathesis doesn’t cache test cases. \n After executing the command, you’ll see that Schemathesis runs around 700\ntests against the API, testing all possible combinations of parameters, types, and for-\nmats. All tests should pass correctly. Once Schemathesis has finished, you can bring\nthe Uvicorn process to the foreground by running the fg command, and stop it if\nyou wish. (I’m sure know you know, but remember that to stop a process you use the\nCtrl-C key combination).\n12.4.2 Using links to enhance Schemathesis’ test suite\nThe test suite we just ran with Schemathesis has one major limitation: it doesn’t test\nwhether the POST /orders endpoint is creating orders correctly nor if we can per-\nform the expected operations, such as payments and cancellations, on an order. It’s\nsimply launching independent and unrelated requests to each of the endpoints in\nthe orders API. To check whether we are creating resources correctly, we need to\nenhance our API specification with links. As you can see in figure 12.9, in the\nOpenAPI standard, links are declarations that allow us to describe the relationships\nbetween different endpoints.2\n For example, using links, we can specify that the POST /orders endpoint returns a\npayload with an ID, and that we can use that ID to form the resource URL of the order\njust created under the GET /orders/{order_id} endpoint. We use operation IDs to\ndescribe the relationships between our endpoints. As we learned in chapter 5 (section\n5.3), operation IDs are unique identifiers for each endpoint in the API. Listing 12.9\nshows how we enhance the orders API with a link that describes the relationship\nbetween the POST /orders endpoint and the GET /orders/{order_id} endpoint.\nFor the full list of links, please see the ch12/orders/oas_with_links.yaml file in the\nGitHub repository for this book. Ellipses are used to hide parts of the code that are\nnot relevant to the example, and newly added code is in bold.\n2 For a good explanation of how links work and how you leverage them in your API documentation, see\nhttps://swagger.io/docs/specification/links/.",
      "content_length": 2854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": null,
      "content": "324\nCHAPTER 12\nTesting and validating APIs\nIn listing 12.9, we name the link between the POST /orders and the GET\n/order/{order_id} endpoints GetOrder. GetOrder’s operationId property identifies\nthe endpoint this link refers to (getOrder). The GET /order/{order_id} endpoint\nhas an URL parameter named order_id, and GetOrder’s parameters property tells us\nthat the response body from the POST /orders endpoint contains an id property,\nwhich we can use to replace order_id in the GET /order/{order_id} endpoint.\n \n \n \npaths:\n/orders:\nget:\n[...]\npost:\n[...]\nresponses:\n[...]\nlinks:\nGetOrder:\noperationId: getOrder\nparameters:\norderId: ‘$response.body#/id’\ndescription: >\nThe ‘id’ value returned in the response can be used as\nthe ‘orderId’ parameter in ‘GET /orders/{orderId}.\n[...]\n/orders/{orderId}:\nparameters:\n- in: path\nname: orderId\nrequired: true\nschema:\ntype: string\nexample: d222e7a3-6afb-463a-9709-38eb70cc670d\nget:\noperationId: getOrder\n[...]\nThese are link descriptors for the POST\n/orders endpoint’s response.\nLink relationship with the getOrder\noperation, which is the GET /orders\n/{orderId} endpoint\nThe response payload from the POST\n/orders endpoint contains an id property\nthat can be used to substitute the\norderId parameter in the /orders\n/{orderId} URL path.\nFigure 12.9\nIn OpenAPI, we can use links to describe the relationships between endpoints. For \nexample, the POST /orders response contains an id property that we can use to replace the \norder_id parameter in the /orders/{order_id} URL.",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": null,
      "content": "325\n12.4\nTesting REST APIs with Schemathesis\n# file: orders/oas.yaml\npaths:\n  /orders:\n    get:\n      [...]\n    post:\n      operationId: createOrder\n      summary: Creates an order\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/CreateOrderSchema'\n      responses:\n        '201':\n          description: A JSON representation of the created order\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/GetOrderSchema'\n          links:    \n            GetOrder:\n              operationId: getOrder    \n              parameters:\n                order_id: '$response.body#/id'   \n              description: >    \n                The `id` value returned in the response can be used as\n                the `order_id` parameter in `GET /orders/{order_id}`\n            [...]\n  /orders/{order_id}:\n    [...]\n    get:\n      operationId: getOrder\n      [...]\nWe can now run Schemathesis and take advantage of our links by running the follow-\ning command:\n$ schemathesis run oas_with_link.yaml --base-url=http://localhost:8000 \\\n--stateful=links\nThe --stateful=links flag instructs Schemathesis to look for links in our documen-\ntation and use them to run tests on the resources created through the POST /orders\nendpoint. If you run Schemathesis now, you’ll see that it runs well over a thousand tests\nagainst the API. Since Schemathesis generates random tests, the exact number of tests\ncases may differ from time to time. Listing 12.10 shows the output of the Schemathesis\nListing 12.9\nAdding links to create relationships between endpoints in OpenAPI\nWe add links\nto the POST\n/orders\nendpoint.\nWe define a link with the GET \n/orders/{order_id} endpoint.\nThe order_id URL\nparameter in the\ngetOrder endpoint can\nbe replaced with the\nresponse payload’s\nid property.\nWe explain how \nthis link works.",
      "content_length": 1947,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": null,
      "content": "326\nCHAPTER 12\nTesting and validating APIs\ntest suite after running it with the --stateful parameter set to links. The listing\nomits the first few lines of the test suite as they contain only system-specific metadata.\nNotice that some of the tests appear nested within the POST /orders endpoint (the\nlines starting with the -> symbol). Nested tests are tests that leverage links from our\nAPI documentation. If the tests on the POST /orders endpoint’s links pass, we can\nrest assured that our resources are being created correctly.\n[...]\nBase URL: http:/ /localhost:8000    \nSpecification version: Open API 3.0.3    \nWorkers: 1    \nCollected API operations: 7    \nGET /orders .                                                        [ 14%]\nPOST /orders .                                                       [ 28%]\n    -> GET /orders/{order_id} .                                      [ 37%]\n    -> PUT /orders/{order_id} .                                      [ 44%]\n    -> DELETE /orders/{order_id} .                                   [ 50%]\n    -> POST /orders/{order_id}/cancel .                              [ 54%]\n    -> POST /orders/{order_id}/pay .                                 [ 58%]\nGET /orders/{order_id} .                                             [ 66%]\nPUT /orders/{order_id} .                                             [ 75%]\nDELETE /orders/{order_id} .                                          [ 83%]\nPOST /orders/{order_id}/pay .                                        [ 91%]\nPOST /orders/{order_id}/cancel .                                     [100%]\n================================ SUMMARY ==================================\nPerformed checks:\n    not_a_server_error        1200 / 1200 passed          PASSED    \n========================== 12 passed in 57.57s ============================\nThe output from the previous test says that our API passed all checks in the not_a_\nserver_error category. By default, Schemathesis only checks that the API doesn’t\nraise server errors, but it can be configured to also verify that our API uses the right\nstatus codes, content types, headers, and schemas as documented in the API specifica-\ntion. To apply all these checks, we use the --checks flag and we set it to all:\n$ schemathesis run oas_with_link.yaml --base-url=http://localhost:8000 \\\n--hypothesis-database=none --stateful=links --checks=all\nListing 12.10\nOutput of a Schemathesis test suite\nThe server’s\nbase URL\nThe version of OpenAPI \nused by our server\nNumber of processes running \nthe test suite in parallel\nNumber of \noperations defined in \nthe API specification\nTest for the GET /orders endpoint\nTest for the GET /orders/{order_id} \nendpoint linked to the POST \n/orders endpoint test\nThe test suite runs 1,200\ntests, and all of them pass.",
      "content_length": 2767,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": null,
      "content": "327\n12.5\nTesting GraphQL APIs\nAs you can see, this time Schemathesis runs over a thousand test cases per check:\n================================ SUMMARY ==================================\nPerformed checks:\n    not_a_server_error              1200 / 1200 passed          PASSED\n    status_code_conformance         1200 / 1200 passed          PASSED\n    content_type_conformance        1200 / 1200 passed          PASSED\n    response_headers_conformance    1200 / 1200 passed          PASSED\n    response_schema_conformance     1200 / 1200 passed          PASSED\n========================== 12 passed in 70.54s ============================\nIn some cases, Schemathesis may complain that it takes too long to generate test\ncases. You can suppress that warning by using the --hypothesis-suppress-health-\ncheck=too_slow flag. By running the whole set of Schemathesis checks against your\nAPI, you can be certain that it works as expected and complies with the API specifi-\ncation. If you’d like to extend the tests with additional custom payloads or scenarios,\nyou can do that as well. Since schemathesis is a Python library, it’s very easy to add\nadditional custom tests. Check the documentation for examples on how to do that\n(http://mng.bz/69Q5).\n This concludes our journey through testing REST APIs. It’s now time to move on\nto the world of GraphQL API testing, which is the topic of the next section!\n12.5\nTesting GraphQL APIs\nThis section explains how we test and validate GraphQL APIs so that we can ensure\nthey work as expected before we release them to production. We’ll use the products\nAPI, which we implemented in chapter 10, as a guiding example. To work through the\nexamples in this section, cd into ch12/products and activate the environment by run-\nning pipenv shell.\n In sections 12.2 and 12.4, we learned about Dredd and Schemathesis, which auto-\nmatically generate tests for REST APIs based on the API specification. For GraphQL,\nthere’s less support for automatic test generation. In particular, Dredd doesn’t sup-\nport GraphQL APIs, while Schemathesis only provides partial support. However, this\nis an active area of development, so expect to see increasing support for automatic\nGraphQL testing in the future.\n12.5.1 Testing GraphQL APIs with Schemathesis\nThis section explains how we use Schemathesis to test and validate a GraphQL API. As\nwe explained in section 12.4, Schemathesis is an API testing framework that uses an\napproach known as property-based testing to validate our APIs. Schemathesis can be\nused to test both REST and GraphQL APIs. In both cases, as you can see in figure 12.10,\nSchemathesis looks at the API specification to learn about its endpoints and schemas,\nand to decide which tests to run.",
      "content_length": 2731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": null,
      "content": "328\nCHAPTER 12\nTesting and validating APIs\nTo generate tests for a GraphQL API, Schemathesis uses hypothesis-graphql\n(http://mng.bz/o5Pj), a library that generates Hypothesis strategies from a GraphQL\nschema. Before we run our test, we need to start the GraphQL API server. You can do\nthat in a different terminal window, or you can run the process in the background\nwith the following command:\n$ uvicorn server:server &\nThe & symbol pushes the Uvicorn process to the background. To test a GraphQL API\nwith Schemathesis, we simply need to give it the URL where our API specification is\nhosted. In our case, the GraphQL API is hosted under the following URL: http://\n127.0.0.1:8000/graphql. Armed with this information, we can now run our tests:\n$ schemathesis run --hypothesis-deadline=None http:/ /127.0.0.1:8000/graphql\nThe --hypothesis-deadline=None flag instructs Schemathesis to avoid timing the\nrequests. This is useful in cases where our queries may be slow, which sometimes hap-\npens with GraphQL APIs. The following shows the output of the test suite, omitting\nAPI server\ntype Ingredient {\nid: ID!\nname: String!\nstock: Stock!\nproducts: [Product!]!\nsupplier: Supplier\ndescription: [String!]\nlastUpdated: Datetime!\n}\ntype Query {\nallProducts: [Product!]!\nallIngredients: [Ingredient!]!\nproducts(input: ProductsFilter!): [Product!]!\nproduct(id: ID!): Product\ningredient(id: ID!): Ingredient\n}\nallIngredients {\nname,\ndescription\n}\nGenerated test case\nGraphQL speciﬁcation for the products API\nAvailable\nqueries\nThe allIngredients\nquery returns a list of\nIngredient objects.\nFigure 12.10\nSchemathesis parses a GraphQL API specification in search of available operations and \ngenerates query documents with both valid and invalid parameters and selection sets to test the \nserver’s response.",
      "content_length": 1794,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": null,
      "content": "329\n12.6\nDesigning your API testing strategy\nthe first few lines that contain platform-specific metadata. As you can see, Schemathe-\nsis tests all of the  queries and mutations exposed by the products API, generating a\nvery solid battery of tests: 1,100 test cases!\n[...]\nSchema location: http:/ /127.0.0.1:8000/graphql\nBase URL: http:/ /127.0.0.1:8000/graphql\nSpecification version: GraphQL\nWorkers: 1\nCollected API operations: 11\nQuery.allProducts .                                                  [  9%]\nQuery.allIngredients .                                               [ 18%]\nQuery.products .                                                     [ 27%]\nQuery.product .                                                      [ 36%]\nQuery.ingredient .                                                   [ 45%]\nMutation.addSupplier .                                               [ 54%]\nMutation.addIngredient .                                             [ 63%]\nMutation.addProduct .                                                [ 72%]\nMutation.updateProduct .                                             [ 81%]\nMutation.deleteProduct .                                             [ 90%]\nMutation.updateStock .                                               [100%]\n================================ SUMMARY ==================================\nPerformed checks:\n    not_a_server_error.           1100 / 1100 passed          PASSED\n========================== 11 passed in 36.82s ============================\nAfter running the Schemathesis test suite against the products API, we can be certain\nthat our queries and mutations work as expected. You can further customize your tests\nto make sure the application works correctly under certain conditions. To learn\nhow to add custom tests cases, check out Schemathesis’ excellent documentation\n(https://schemathesis.readthedocs.io/en/stable/).\n12.6\nDesigning your API testing strategy\nYou’ve learned a lot in this chapter. You’ve learned to use frameworks such as Dredd\nand Schemathesis, which run automated test suites against your APIs based on the API\ndocumentation. You’ve also learned about property-based testing and how to use\nHypothesis to automatically generate test cases to test your REST and GraphQL APIs.\n As we saw in section 12.2, Dredd runs a simple test suite against your APIs. Dredd\nonly tests the happy path: it makes sure your API accepts the expected payloads and\nresponds with the expected payloads. It doesn’t test what happens when the wrong\npayloads are sent to your server.\n Dredd’s testing strategy is useful in the early development stage of your API, when\nyou want to be able to focus on the overall functionality of your application rather\nthan get bogged down with specific corner cases of your API integration. However,\nListing 12.11\nOutput of a Schemathesis test suite for a GraphQL API",
      "content_length": 2864,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": null,
      "content": "330\nCHAPTER 12\nTesting and validating APIs\nbefore you release your APIs to production, you want to make sure your APIs are\ntested with Schemathesis. Schemathesis runs a more comprehensive test suite, which\nensures that your API works exactly as expected.\n I recommend you run Dredd and Schemathesis locally during development, and\nalso in your continuous integration (CI) server before releasing your code. For an\nexample of how you can incorporate Dredd and Schemathesis into your CI server,\ncheck out my talk, “API Development Workflows for Successful Integrations,” at Man-\nning’s API Conference (August 3 2021, https://youtu.be/SUKqmEX_uwg).\n Some of the technologies and skills that you’ve learned in this chapter are still very\nnew and experimental, so you’ve got an edge in your team and in the job market. Use\nyour new powers wisely!\nSummary\nDredd and Schemathesis are API testing tools that automatically generate vali-\ndation tests for APIs from the documentation. This helps you to avoid the effort\nof writing tests manually and to focus on building your APIs and services.\nDredd is a REST API testing framework. It runs a basic test suite against your\nAPI without covering edge cases, and therefore it’s convenient in the early stages\nof your API cycle.\nYou can customize Dredd’s behavior by adding Dredd hooks to your tests.\nAlthough Dredd is an npm package, you can write your hooks in Python. Dredd\nhooks are useful for saving information from one test for reuse in another test,\nand for creating or deleting resources before and after each test.\nSchemathesis is a more generic API test framework that runs an exhaustive test\nsuite against your APIs. Before releasing your APIs to production, you want to\nmake sure you’ve tested them with Schemathesis. You can use Schemathesis to\ntest both REST and GraphQL APIs.\nTo test that your POST endpoints are creating resources correctly, you can\nenrich your OpenAPI specification with links and instruct Schemathesis to use\nthem in its test suite. Links are properties that describe the relationship\nbetween different operations in an OpenAPI specification.\nProperty-based testing is an approach in which you let a framework generate\nrandom test cases, and you validate the behavior of your code by making asser-\ntions about the properties of the test result. This approach saves you the time of\nhaving to write test cases manually. In Python, you can run property-based tests\nwith the excellent hypothesis library.",
      "content_length": 2479,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": null,
      "content": "331\nDockerizing\nmicroservice APIs\nDocker is a virtualization technology that allows us to run our applications any-\nwhere by simply having a Docker execution runtime. Docker takes away the pain\nand effort required to tune and configure an environment to run code. It also\nmakes deployments more predictable since it produces replicable artifacts (con-\ntainer images) that we can run locally as well as in the cloud.\n In this chapter, you’ll learn to Dockerize a Python application. Dockerizing is\nthe process of packaging an application as a Docker image. You can think of a\nDocker image as a build or artifact that is ready to be deployed and executed. To\nexecute an image, Docker creates running instances of the image, known as con-\ntainers. To deploy Docker images, we typically use a container orchestrator, such as\nKubernetes, which takes care of managing the life cycle of a container. In the next\nchapter, you’ll learn to deploy Docker builds with Kubernetes. We’ll illustrate how\nThis chapter covers\nHow to Dockerize an application\nHow to run Docker containers\nHow to run an application with Docker Compose\nPublishing a Docker image to AWS Elastic \nContainer Registry",
      "content_length": 1180,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": null,
      "content": "332\nCHAPTER 13\nDockerizing microservice APIs\nto Dockerize an application using the orders service of the CoffeeMesh platform.\nYou’ll also learn to publish your Docker builds to a container registry by uploading\nimages to AWS’s Elastic Container Registry (ECR).\n All the code examples are available under folder ch13 in the GitHub repository\nfor this book. We’ll begin by setting up the environment to work on this chapter in\nsection 13.1.\n13.1\nSetting up the environment for this chapter\nIn this section, we set up the environment so that you can follow along with the exam-\nples in the rest of the chapter. We continue the implementation of the orders service\nwhere we left it in chapter 11, where we added the authentication and authorization\nlayers. First, copy over the code from chapter 11 into a new folder called ch13:\n$ cp -r ch11 ch13\ncd into ch13, and install the dependencies and activate the virtual environment by\nrunning the following commands:\n$ cd ch13 && pipenv install --dev && pipenv shell\nWhen we deploy the application, we use a PostgreSQL engine, which is one of the most\npopular SQL engines for running applications in production. To communicate with the\ndatabase, we use psycopg2, which is one of Python’s most popular PostgreSQL drivers:\n$ pipenv install psycopg2\nINSTALLING PSYCOPG2\nIf you run into issues installing and compiling psy-\ncopg2, try installing the compiled package by running pipenv install psy-\ncopg2-binary, or pull ch13/Pipfile and ch13/Pipfile.lock from this book’s\nGitHub repository and run pipenv install --dev. Two other powerful Post-\ngreSQL drivers are asyncpg (https://github.com/MagicStack/asyncpg) and\npscycopg3 (https://github.com/psycopg/psycopg), both of which support\nasynchronous operations. I encourage you to check them out!\nTo build and run Docker containers, you’ll need a Docker runtime on your machine.\nInstallation instructions are platform specific, so please see the official documentation\nto learn how to install Docker on your system (https://docs.docker.com/get-docker/).\n Since we’re going to publish our Docker images to AWS’s ECR, we need to\ninstall the AWS CLI:\n$ pipenv install --dev awscli\nNext, go to https://aws.amazon.com/. Create an AWS account and obtain an access\nkey to be able to access AWS services programmatically. The user profile you use to\ncreate the AWS account is the account’s root user. For security, it is recommended that\nyou don’t use the root user to generate your access key. Instead, create an IAM user",
      "content_length": 2501,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": null,
      "content": "333\n13.2\nDockerizing a microservice\nand generate an access key for that user. IAM is AWS’s Identity Access Management\nservice, which allows you to create users, roles, and granular policies for granting\naccess to other services in your account. Follow the AWS documentation to learn how\nto create an IAM user (http://mng.bz/neP8) and to learn how to generate your access\nkeys and configure the AWS CLI (http://mng.bz/vXxq).\n Now that our environment is ready, it’s time to Dockerize our applications!\n13.2\nDockerizing a microservice\nWhat does Dockerizing an application mean? Dockerizing is the process of packaging\nan application as a Docker image. You can think of a Docker image as a build or arti-\nfact that can be deployed and executed in a Docker runtime. All the system depen-\ndencies are already installed in the Docker image, and to run the image, we only need\na Docker runtime. To execute the image, the Docker runtime creates a container,\nwhich is a running instance of the image. As you can see in figure 13.1, working with\nDocker is very convenient since it allows us to run our applications in isolated pro-\ncesses. There are different options for installing a Docker runtime depending on your\nplatform, so please see the official documentation to determine which option works\nbest for you (https://docs.docker.com/get-docker/).\nIn this section, we create an optimized Docker image of the orders service. Along the\nway, you’ll learn how to write a Dockerfile, which is a document that contains all the\ninstructions required to build a Docker image. You’ll also learn how to run Docker\ncontainers and to map ports from the container to the host operating system so that\nyou can interact with the application running inside the container. Finally, you’ll also\nlearn how to manage containers with the Docker CLI.\nDOCKER FUNDAMENTALS\nIf you want to know more about how Docker works\nand how it interacts with the host operating system, check out Prabath Siri-\nwardena and Nuwan Dias’s excellent “Docker Fundamentals” from their book\nMicroservices Security in Action (Manning, 2020, http://mng.bz/49Ag).\nHost operating system\nDocker runtime\nContainer\nApplication\nApplication\nApplication\nApplication\nContainer\nContainer\nContainer\nFigure 13.1\nDocker containers run in isolated processes on top of the host operating \nsystem.",
      "content_length": 2330,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": null,
      "content": "334\nCHAPTER 13\nDockerizing microservice APIs\nBefore we build the image, we need to make two small changes to our application\ncode to get it ready for deployment. So far, the orders service has been using a hard-\ncoded database URL, but to operate the service in different environments, we need to\nmake this setting configurable. The following code shows the changes needed to the\norders/repository/unit_of_work.py file to pull the database URL from the environ-\nment, with the newly added code in bold characters. We use an assert statement to exit\nthe application immediately if no database URL is provided.\n# file: orders/repository/unit_of_work.py\nimport os\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nDB_URL = os.getenv('DB_URL')    \nassert DB_URL is not None, 'DB_URL environment variable needed.'    \nclass UnitOfWork:\n    def __init__(self):\n        self.session_maker = sessionmaker(bind=create_engine(DB_URL))    \n    def __enter__(self):\n        self.session = self.session_maker()\n        return self\n    ...\nWe also need to update our Alembic files to pull the database URL from the environ-\nment. The following code shows the changes required to migrations/env.py to accom-\nplish that, with the newly added code in bold. We omitted nonrelevant parts of the\ncode with ellipses to make it easier to observe the changes.\n# file: migrations/env.py\nimport os\nfrom logging.config import fileConfig\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import pool\nfrom alembic import context\n...\nListing 13.1\nPulling the database URL from the environment\nListing 13.2\nPulling the database URL from the environment for alembic\nWe pull the database \nURL from the DB_URL \nenvironment variable.\nWe exit the application\nif DB_URL isn’t set.\nWe use the value from\nDB_URL to connect to\nthe database.",
      "content_length": 1831,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": null,
      "content": "335\n13.2\nDockerizing a microservice\ndef run_migrations_online():\n    \"\"\"...\n    \"\"\"\n    url = os.getenv('DB_URL')    \n    assert url is not None, 'DB_URL environment variable needed.'    \n    connectable = create_engine(url)\n    \n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n    ...\nNow that our code is ready, it’s time to Dockerize it! To build a Docker image, we need\nto write a Dockerfile. Create a file named Dockerfile. Listing 13.3 shows this file’s con-\ntents. We use the slim version of the official Python 3.9 Docker image as our base\nimage. Slim images contain just the dependencies that we need to run our applica-\ntions, which results in lighter images. To use a base image, we use Docker’s FROM direc-\ntive. Then we create the folder for the application code called /orders/orders. To run\nbash commands, such as mkdir in this case, we use Docker’s RUN directive. We also set\n/orders/orders as the working directory using Docker’s WORKDIR directive. The work-\ning directory is the directory from which the application runs.\n Next, we install pipenv, copy our Pipenv files, and install the dependencies. We use\nDocker’s COPY directive to copy files from our filesystem into the Docker image. Since\nwe’re running in Docker, we don’t need a virtual environment, so we install the depen-\ndencies using pipenv’s --system flag. We also use pipenv’s --deploy flag, which checks\nthat our Pipenv files are up to date. Finally, we copy over our source code and specify\nthe command that needs to be executed to get the orders service up and running.\nThe command that Docker must use to execute our application is specified using\nDocker’s CMD directive. We also use Docker’s EXPOSE directive to make sure the running\ncontainer listens on port 8000, the port on which our API runs. If we don’t expose the\nport, we can’t interact with the API.\n The order of our statements in the Dockerfile matters because Docker caches each\nstep of the build. Docker will only execute a step again if the previous step changed,\nfor example, if we installed a new dependency, or if one of our files changed. Since\nour application code is likely to change more often than our dependencies, we copy\nthe code at the end of the build. That way, Docker will only install the dependencies\nonce and cache the step until they change.\n \n \nWe pull the database \nURL from the DB_URL \nenvironment variable.\nWe exit the application\nif DB_URL isn’t set.",
      "content_length": 2538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": null,
      "content": "336\nCHAPTER 13\nDockerizing microservice APIs\n# file: Dockerfile\nFROM python:3.9-slim    \nRUN mkdir -p /orders/orders    \nWORKDIR /orders             \nRUN pip install -U pip && pip install pipenv\nCOPY Pipfile Pipfile.lock /orders/    \nRUN pipenv install --system --deploy    \nCOPY orders/orders_service /orders/orders/orders_service/    \nCOPY orders/repository /orders/orders/repository/\nCOPY orders/web /orders/orders/web/\nCOPY oas.yaml /orders/\nCOPY public_key.pem /orders/public_key.pem\nCOPY private.pem /orders/private.pem\nEXPOSE 8000    \nCMD [\"uvicorn\", \"orders.web.app:app\", \"--host\", \"0.0.0.0\"]    \nTo build the Docker image from listing 13.3, you need to run the following command\nfrom the ch13 directory:\n$ docker build -t orders:1.0 .\nThe -t flag stands for tag. A Docker tag has two parts: the image name on the left of\nthe colon and the tag name on the right of the colon. The tag name is typically the\nversion of the build. In this case, we’re naming the image orders and tagging it with\n1.0. Make sure you don’t miss the period at the end of the build statement: it rep-\nresents the path to the source code for the build (the context in Docker parlance). A\nperiod means the current directory.\n Once the image has built, you can execute it with the following command: \n$ docker run --env DB_URL=sqlite:///orders.db \\\n-v $(pwd)/orders.db:/orders/orders.db -p 8000:8000 -it orders:1.0\nAs you can see in figure 13.2, the --env flag allows us to set environment variables in\nthe container, and we use it to set the URL of the database. To make the application\naccessible to the host machine, we use the -p flag, which allows us to bind the port on\nwhich the application is running inside the container to a port in the host machine.\nWe also use the -v flag to mount a volume on the SQLite database file. Docker vol-\numes allow containers to access files from the host machine’s file system. \nListing 13.3\nDockerfile for the orders service\nThe base image\nBase folder structure \nfor our application\nWorking directory from \nwhich we’ll run the code\nWe copy our pipenv files.\nWe install the dependencies.\nWe copy the \nrest of the \napplication \nfiles.\nWe expose the application’s \nport to the host machine.\nThe API \nserver’s \nstartup \ncommand",
      "content_length": 2245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": null,
      "content": "337\n13.2\nDockerizing a microservice\nYou can now access the application on the following URL: http://127.0.0.1:8000/\ndocs/orders. The previous command executes the container attached to your current\nterminal session, which allows you to see the logs unfold as you interact with the appli-\ncation. In this case, you can stop the container just like any other process by pressing\nthe Ctrl-C key combination.\n You can also run containers in detached mode, which means the process isn’t\nlinked to your terminal session, so when you close your terminal, the process will con-\ntinue running. This is convenient if you just want to run a container to interact with it,\nand you don’t need to watch the logs. We typically run containerized databases in\ndetached mode. To run the container in detached mode, you use the -d flag:\n$ docker run -d –-env DB_URL=sqlite:///orders.db \\\n-v $(pwd)/orders.db:/orders/orders.db -p 8000:8000 orders:1.0\nIn this case, you’ll need to stop the container with the docker stop command. First,\nyou need to figure out the ID of the running container with the following command: \n$ docker ps\nThis command will list all currently running containers in your machine. The output\nlooks like this (output truncated with ellipses):\nCONTAINER ID   IMAGE       COMMAND       CREATED         STATUS...    \n83e6189a02ee   orders:1.0  \"uvicorn...\"  7 seconds ago   Up 6 seconds\nContainer\norders.db\nexport DB_URL=sqlite:///orders.db\ndocker run -env DB_URL=sqlite:///orders.db -v $(pwd)/orders.db:/orders/orders.db\n-p 8000:8000 -it orders:1.0\nUser\nport 8000\nport 8000\nHost operating system\nMounting a volume on a ﬁle or folder\nallows the container to access it.\nBinding a port in the\ncontainer to a port in the\nhost operating system\nallows users to interact\nwith the application.\nWe can set environment\nvariables for the container.\nFigure 13.2\nWhen we run a container, we can include various configurations to set environment variables \nwithin the container or to allow it to access files in the host operating system.",
      "content_length": 2025,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": null,
      "content": "338\nCHAPTER 13\nDockerizing microservice APIs\nPick up the container ID (in this case 83e6189a02ee) and use it to stop the process\nwith the following command:\n$ docker stop 83e6189a02ee\nThat’s all it takes to build and run Docker containers! There’s a lot more to Docker\nthan we’ve seen in this section, and if you’re interested in learning more about this\ntechnology, I recommend you look at Docker in Practice by Ian Miell and Aidan Hob-\nson Sayers (Manning, 2019) and Docker in Action by Jeff Nickoloff and Stephen\nKuenzli (Manning, 2019).\n13.3\nRunning applications with Docker Compose\nIn the previous section, we ran the orders service’s container by mounting it on our\nlocal SQLite database. This is fine for a quick test, but it doesn’t really tell us whether\nour application will work as expected with a PostgreSQL database. A common strategy\nto connect our containerized applications to a database is using Docker Compose,\nwhich allows us to run multiple containers within a shared network, so they can talk to\neach other. In this section, you’ll learn how to run the orders service with a Post-\ngreSQL database using docker-compose.\n To use Docker Compose, first we need to install it. It is a Python package, so we\ninstall it with pip:\n$ pip install docker-compose\nNext, let’s write our Docker Compose file—a declaration of the resources we need to\nrun our application. Listing 13.4 shows the docker-compose file for the orders service.\nWe use Docker Compose’s latest specification format, version 3.9, and we declare two\nservices: database and api. database runs PostgreSQL’s official Docker image, while\napi runs the orders service. We use the build keyword to point to the Docker build con-\ntext, and we give it a period value (.). By using a period, we instruct Docker Compose to\nlook for a Dockerfile and build the image relative to the current directory. Through the\nenvironment keyword, we configure the environment variables required to run our\napplications. We expose database’s 5432 port so that we can connect to the database\nfrom our host machine, as well as api’s 8000 port so that we can access the API. Finally,\nwe use a volume called database-data, which docker-compose will use to persist our\ndata. This means that if you restart docker-compose, you won’t lose your data. \n# file: docker-compose.yaml\nversion: \"3.9\"    \nservices:    \nListing 13.4\ndocker-compose file for the orders service\nThe version of \ndocker-compose’s \nformat for this file.\nWe declare our services.",
      "content_length": 2495,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": null,
      "content": "339\n13.3\nRunning applications with Docker Compose\n  database:    \n    image: postgres:14.2    \n    ports:    \n      - 5432:5432\n    environment:    \n      POSTGRES_PASSWORD: postgres\n      POSTGRES_USER: postgres\n      POSTGRES_DB: postgres\n    volumes:    \n      - database-data:/var/lib/postgresql/data\n  api:    \n    build: .    \n    ports:    \n      - 8000:8000\n    depends_on:    \n      - database\n    environment:    \n      DB_URL: postgresql:/ /postgres:postgres@database:5432/postgres\nvolumes:    \n  database-data:\nExecute the following command to run our Docker Compose file:\n$ docker-compose up --build\nThe --build flag instructs Docker Compose to rebuild your images if your files\nchanged. Once the web API is up and running, you can access it on http://localhost:\n8000/docs/orders. If you try any of the endpoints, your tables don’t exist. That’s\nbecause we haven’t run the migrations against our fresh PostgreSQL database! To run\nthe migrations, open a new terminal window, cd into the ch13 folder, activate your\npipenv environment, and run the following command:\n$ PYTHONPATH=`pwd` \\\nDB_URL=postgresql://postgres:postgres@localhost:5432/postgres alembic \\\nupgrade heads\nOnce the migrations have been applied, you can hit the API endpoints again, and\neverything should work. To stop docker-compose, run the following command from\nanother terminal window and inside the ch13 folder:\n$ docker-compose down\nThis is all it takes to run Docker Compose! You’ve just learned to use one of the most\npowerful automation tools. Docker Compose is often used to run integration tests\nThe database service\nThe database service’s Docker image\nWe expose the database \nports to the host machine.\nDatabase environment \nconfiguration\nWe mount our \ndatabase’s data folder \non a local volume.\nThe API service\nThe API’s build context\nWe expose the API’s port \nto the host machine.\nThe API depends on the database.\nThe API’s environment configuration\nThe database’s \nvolume",
      "content_length": 1964,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": null,
      "content": "340\nCHAPTER 13\nDockerizing microservice APIs\nand to have an easy way to run the backend for developers who work on client appli-\ncations, such as SPAs.\n With our Docker stack ready and our images tested, it’s time to learn how to push\nimages to a container registry. Move on to the next section to learn how!\n13.4\nPublishing Docker builds to a container registry\nTo deploy our Docker builds, we need to publish them first to a Docker container reg-\nistry. A container registry is a repository of Docker images. In the next chapter, we will\ndeploy our applications to AWS’s Elastic Kubernetes Service, so we publish our builds\nto AWS’s ECR. Keeping our Docker images within AWS will make it easier to deploy\nthem to EKS.\n First, let’s create an ECR repository for our images with the following command:\n$ aws ecr create-repository --repository-name coffeemesh-orders\n{\n    \"repository\": {\n        \"repositoryArn\": \n➥ \"arn:aws:ecr:<aws_region>:<aws_account_id>:repository/coffeemesh-orders\",\n        \"registryId\": \"876701361933\",\n        \"repositoryName\": \"coffeemesh-orders\",\n        \"repositoryUri\": \n➥ \"<aws_account_id>.dkr.ecr.<aws_region>.amazonaws.com/coffeemesh-orders\",\n        \"createdAt\": \"2021-11-16T10:08:42+00:00\",\n        \"imageTagMutability\": \"MUTABLE\",\n        \"imageScanningConfiguration\": {\n            \"scanOnPush\": false\n        },\n        \"encryptionConfiguration\": {\n            \"encryptionType\": \"AES256\"\n        }\n    }\n}\nIn this command, we create a ECR repository named coffeemesh-orders. The output\nfrom the command is a payload describing the repository we just created. When you\nrun the command, the placeholder for <aws_account_id> in the output payload will\ncontain your AWS account ID, and <aws_region> will contain your default AWS\nregion. To publish our Docker build to ECR, we need to tag our build with the name\nof the ECR repository. Get hold of the repository.repositoryArn property of the\nprevious command’s output (in bold), and use it to tag the Docker build we created in\nsection 13.2 with the following command:\n$ docker tag orders:1.0 \\\n<aws_account_id>.dkr.ecr.<aws_region>.amazonaws.com/coffeemesh-orders:1.0\nTo publish our images to ECR, we need to obtain login credentials with the follow-\ning command:",
      "content_length": 2248,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": null,
      "content": "341\nSummary\n$ aws ecr get-login-password --region <aws_region> | docker login \\\n--username AWS --password-stdin \\ \n<aws_account_id>.dkr.ecr.<region>.amazonaws.com\nMake sure you replace <aws_region> in this command for the AWS region where you\ncreated the Docker repository, such as eu-west-1 for Europe (Ireland) or us-east-2 for\nUS East (Ohio). Also replace <aws_account_id> with your AWS account ID. Check\nout the AWS documentation to learn how to find your AWS account ID (http://mng\n.bz/Qnye).\nAWS REGIONS\nWhen you deploy services to AWS, you deploy them to specific\nregions. Each region has an identifier, such as eu-west-1 for Ireland and eu-\neast-2 for Ohio. For an up-to-date list of the regions available in AWS, see\nhttp://mng.bz/XaPM.\nThe aws ecr get-login-password command produces an instruction that Docker\nknows how to use to log in to ECR. We’re now ready to publish our build! Run the fol-\nlowing command to push the image to ECR:\n$ docker push \\\n<aws_account_id>.dkr.ecr.<aws_region>.amazonaws.com/coffeemesh-orders:1.0\nVoila! Our Docker build is now in ECR. In the next chapter, you’ll learn how to deploy\nthis build to a Kubernetes cluster in AWS.\nSummary\nDocker is a virtualization technology that allows us to run our applications any-\nwhere by simply having a Docker execution runtime. A Docker build is called\nan image, which is executed in processes called Docker containers.\nDocker Compose is a container orchestration framework that allows you to run\nmultiple containers simultaneously, such as databases and APIs. Using Docker\nCompose is an easy and effective way to run your whole backend without having\nto install and configure additional dependencies.\nTo deploy Docker images, we publish them to a container registry, such as\nAWS’s ECR—a robust and secure container registry that makes it easy to deploy\nour containers to AWS services.",
      "content_length": 1869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": null,
      "content": "342\nDeploying\nmicroservice APIs\nwith Kubernetes\nKubernetes is an open source container orchestration framework, and it’s fast\nbecoming a standard way for deploying and managing applications across plat-\nforms. You can deploy Kubernetes yourself to your own servers, or you can use a\nmanaged Kubernetes service. In either case, you’ll get a consistent interface to your\nservices, which means moving across cloud providers becomes less disruptive for\nyour operations. You can also deploy a Kubernetes cluster in your machine and run\nyour tests locally in much the same way you’d do in the cloud.\nRUN KUBERNETES LOCALLY WITH MINIKUBE\nYou can run a Kubernetes clus-\nter locally using minikube. Although we won’t cover it in this chapter,\nminikube is a great tool to get more familiar with Kubernetes. Check out\nThis chapter covers\nCreating a cluster with AWS’s Elastic Kubernetes \nService (EKS)\nExposing services using the AWS Load Balancer \nController\nDeploying services to a Kubernetes cluster\nManaging secrets securely in Kubernetes\nDeploying an Aurora Serverless database",
      "content_length": 1076,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": null,
      "content": "343\n14.1\nSetting up the environment for this chapter\nthe official documentation for minikube (https://minikube.sigs.k8s.io/docs/\nstart/).\nDeploying Kubernetes yourself is a good exercise to get more familiar with the technol-\nogy, but in practice most companies use a managed service. In this chapter, we’ll use a\nKubernetes managed service to deploy our cluster. Plenty of vendors offer Kubernetes\nmanaged services. The major players are Google Cloud’s Google Kubernetes Engine\n(GKE), Azure’s Kubernetes Service (AKS), and AWS’s Elastic Kubernetes Service\n(EKS). All three services are very robust and offer similar features.1 In this chapter,\nwe’ll use EKS, which is currently the most popular managed Kubernetes service.2\n To illustrate how to deploy applications to a Kubernetes cluster, we’ll use the exam-\nple of the orders service. We’ll also create an Aurora Serverless database, and we’ll see\nhow to securely feed the database connection credentials to the service using Kuber-\nnetes secrets. \n The chapter doesn’t assume previous knowledge of AWS or Kubernetes. I’ve\nmade an effort to explain every Kubernetes and AWS concept in detail so that you\ncan follow along with the examples, even if you have no previous experience with\neither technology. Entire books have been written on these topics, so this chapter is\njust an overview, and I provide references to other resources you can use to dive\ndeeper into these matters.\n Before proceeding, please bear in mind that EKS and other AWS services used in\nthis chapter are for-fee services, so this is the only chapter in the book that will cost\nyou some money if you follow along with the examples. The base cost of a Kubernetes\ncluster in AWS EKS is $0.10 per hour, which amounts to $2.40 per day and roughly $72\nper month. If budget is an issue, my recommendation is to read the chapter first to get\nan understanding of what we’re doing and then try out the EKS examples afterward.\nIf this is your first time working with EKS and Kubernetes, it may take you one or two\ndays to work through the examples, so try to schedule this time to work on the exam-\nples. Section 14.9 describes how to delete the EKS cluster, and all the other resources\ncreated in this chapter, to make sure you don’t incur additional costs.\n Without further ado, let’s get started! We’ll begin by setting up the environment.\n14.1\nSetting up the environment for this chapter\nIn this section, we set up the environment so that you can follow along with the\nexamples in the rest of the chapter. Even if you’re not planning to try out the exam-\nples, I recommend you take at least a quick look at this section to learn about the\ntools we’re going to use. This chapter is heavy in tooling, so here we install the most\nimportant dependencies, and in the coming sections you’ll find additional instruc-\ntions for other tools.\n1 For a quick comparison between GKE, AKS, and EKS, see Alexander Postasnick, “AWS vs EKS vs GKE: Man-\naged Kubernetes Services Compared,” June 9, 2021, https://acloudguru.com/blog/engineering/aks-vs-eks-vs-\ngke-managed-kubernetes-services-compared.\n2 Flexera, “2022 State of the Cloud Report” (pp. 52–53), https://info.flexera.com/CM-REPORT-State-of-the-Cloud.",
      "content_length": 3214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": null,
      "content": "344\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\n First, copy over the code from chapter 13 into a new folder called ch14 by running\nthe following command:\n$ cp -r ch13 ch14\ncd into ch14, install the dependencies, and activate the virtual environment by run-\nning the following commands:\n$ cd ch14 && pipenv install --dev && pipenv shell\nSince we’ll deploy to AWS, we need to be able to access AWS services programmati-\ncally. In chapter 13, we installed and configured the AWS CLI. If you haven’t done\nso, please go back to section 13.1 and follow the steps to install and configure the\nAWS CLI. \n You’re going to learn how to deploy services to Kubernetes, so you also need to\ninstall the Kubernetes CLI, known as kubectl. There’re different ways to install\nkubectl depending on the platform that you’re using, so please refer to the official\ndocumentation to see which option works best for you (https://kubernetes.io/docs/\ntasks/tools/).\n Finally, in this chapter we will make heavy use of jq—a CLI tool that helps us parse\nand query JSON documents. jq is not strictly necessary to follow along with the exam-\nples in this chapter, but it does make everything easier, and if you haven’t used the\ntool before, I highly encourage you to learn about it. We’ll use jq mostly for filtering\nJSON payloads and retrieving specific properties from them. As with Kubernetes,\nthere are different installation options depending on your platform, so please refer to\nthe official documentation to find out which strategy is best for you (https://stedolan\n.github.io/jq/download/).\n Now that our environment is ready, it’s deployment time! Before we create the\ncluster, the next section explains some of the main concepts related to Kubernetes to\nmake sure you can follow the upcoming sections. If you have previous experience with\nKubernetes, you can skip section 14.2.\n14.2\nHow Kubernetes works: The “CliffsNotes” version\nSo, what is Kubernetes? If you don’t have previous experience with Kubernetes or are\nstill confused about how it works, this section offers a hyper-compressed introduction\nto its main components.\n Kubernetes is an open source container orchestration tool. Container orchestration is\nthe process of running containerized applications. In addition to container orchestra-\ntion, Kubernetes also helps us automate deployments, and it handles graceful rollouts\nand rollbacks, scaling applications, and more.\n Figure 14.1 offers a high-level overview of the main components of a Kubernetes\ncluster. The core of a Kubernetes cluster is the control plane, a process that runs the\nKubernetes API for our cluster, controls its state, and manages the available resources,",
      "content_length": 2683,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": null,
      "content": "345\n14.2\nHow Kubernetes works: The “CliffsNotes” version\namong many other tasks. It’s also possible to install add-ons on the control plane,\nincluding specific DNS servers such as CoreDNS.\nDEFINITION\nThe Kubernetes control plane is a process that runs the Kubernetes\nAPI and controls the state of the cluster and manages the available resources,\nscheduling, and many other tasks. For more information about the control\nplane, see chapters 11 (http://mng.bz/yayE) and 12 (http://mng.bz/M0dm)\nfrom Core Kubernetes by Jay Vyas and Chris Love (Manning, 2022).\nThe smallest unit of computing in Kubernetes is the pod: a wrapper around containers\nthat can include one or more containers. The most common practice is to run one\ncontainer per pod, and in this chapter, we deploy the orders service as a single con-\ntainer per pod.\n To deploy pods into the cluster, we use workloads. Kubernetes has four types of\nworkloads: Deployment, StatefulSet, DaemonSet, and Job/CronJob. Deployment is\nthe most common type of Kubernetes workload and is useful for running stateless dis-\ntributed applications. StatefulSet is used for running distributed applications whose\nControl\nlane\np\nKubernetes API\netcd\ncoreDNS\nDeployment\norkload\nw\nJob\norkload\nw\nNamespace: orders\nNode\nPod\nPod\nPod\nNode\nPod\nPod\nPod\nService\nLoad balancer\nIngress\nReplicaSet\nDeployments use\nReplicaSets to control the\ndesired number of pods.\nPods are executed in nodes.\nThe Kubernetes control plane\ncontrols the state of the cluster and\nexposes the Kubernetes API.\nA service exposes a\ndeployment as a web\nservice.\nThe load balancer accepts\nincoming requests from the\ninternet and forwards them to\na service using ingress rules.\nFigure 14.1\nHigh-level architecture of a Kubernetes cluster showing how all components of a cluster \ncome together.",
      "content_length": 1792,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": null,
      "content": "346\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\nstate needs to be synchronized. You use DaemonSet to define processes that should run\non all or most of the nodes in the cluster, such as log collectors. Job and CronJob help\nus to define one-off processes or applications that need to be run on a schedule, such\nas once a day or once a week.\n To deploy a microservice, we use either a Deployment or a StatefulSet. Since our\nservices are all stateless, in this chapter we deploy the orders service as a Deployment.\nTo manage the number of pods, deployments use the concept of a ReplicaSet, a pro-\ncess that maintains the desired number of pods in the cluster.\n Workloads are normally scoped within namespaces. In Kubernetes, namespaces\nare logical groupings of resources that allow us to isolate and scope our deploy-\nments. For example, we can create a namespace for each service in our platform.\nNamespaces make it easier to manage our deployments and to avoid name conflicts:\nthe names of our resources must be unique within a namespace but don’t have to be\nacross namespaces.\n To run our applications as web services, Kubernetes offers the concept of services—\nprocesses that manage the interfaces of our pods and enable communication between\nthem. To expose our services through the internet, we use a load balancer, which sits\nin front of the Kubernetes cluster, and forwards traffic to the services based on\ningress rules.\n The final piece of the Kubernetes system is the node, which represents the actual\ncomputing resources in which our services run. We define nodes as computing\nresources since they can be anything from physical servers to virtual machines. For\nexample, when running a Kubernetes cluster in AWS, our nodes will be represented\nby EC2 machines.\n Now that we understand what the main parts of Kubernetes are, let’s create a cluster!\n14.3\nCreating a Kubernetes cluster with EKS\nIn this section, you’ll learn how to create a Kubernetes cluster using the AWS EKS. We\nlaunch the Kubernetes cluster using eksctl, which is the recommended tool for man-\naging Kubernetes in AWS.\n eksctl is an open source tool created and maintained by Weaveworks. It uses Cloud-\nFormation behind the scenes to create and manage changes to our Kubernetes clus-\nters. This is excellent news, because it means we can reuse the CloudFormation\ntemplates to replicate the same infrastructure across different environments. It also\nmakes all our changes to the cluster visible through CloudFormation.\nDEFINITION\nCloudFormation is AWS’s infrastructure-as-code service. With Cloud-\nFormation, we can declare our resources in YAML or JSON files called tem-\nplates. When we submit the templates to CloudFormation, AWS creates a\nCloudFormation stack, the collection of resources defined in the templates.\nCloudFormation templates shouldn’t contain sensitive information and can\nbe committed in our code repositories, which makes changes to our infra-\nstructure very visible and replicable across different environments.",
      "content_length": 3017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 375,
      "chapter": null,
      "content": "347\n14.3\nCreating a Kubernetes cluster with EKS\nThere are various ways to install eksctl depending on the platform you’re using, so\nplease refer to the official documentation to find out which strategy works best for you\n(https://github.com/weaveworks/eksctl).\n To run the containers in the Kubernetes cluster, we use AWS Fargate. As you can see\nin figure 14.2, Fargate is AWS’s serverless container service that allows us to run contain-\ners in the cloud without having to provision servers. With AWS Fargate, you don’t need\nto worry about scaling your servers up or down, since Fargate takes care of that.\nTo create a Kubernetes cluster using eksctl, run the following command:\n$ eksctl create cluster --name coffeemesh --region <aws_region> --fargate \\\n--alb-ingress-access\nThe creation process takes approximately 30 minutes to complete. Let’s look at each\nflag in this command:\n\n--name—The name of the cluster. We’re calling the cluster coffeemesh.\n\n--region—The AWS region where you want to deploy the cluster. This region\nshould be the same you used to create the ECR repository in section 13.4.\n\n--fargate—Creates a Fargate profile to schedule pods in the default and the\nkube-system namespaces. Fargate profiles are policies that determine which\npods must be launched by Fargate.\n\n--alb-ingress-access—Enables access to the cluster through an Application\nLoad Balancer.\nKubernetes cluster\nPool of nodes provisioned by AWS Fargate\nOur pods are automatically\ndeployed to the available nodes.\nPods\nKubernetes\nengine\nAWS Fargate automatically scales up or down the number\nof servers required to operate our Kubernetes cluster.\nFigure 14.2\nAWS Fargate automatically provisions the servers required to operate our \nKubernetes cluster.",
      "content_length": 1741,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": null,
      "content": "348\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\nFigure 14.3 illustrates the architecture of the stack created by eksctl when launching\nthe Kubernetes cluster. By default, eksctl creates a dedicated Virtual Private Cloud\n(VPC) for the cluster.\nKUBERNETES NETWORKING\nTo make advanced use of Kubernetes, you need to\nunderstand how networking works in Kubernetes. To learn more about Kuber-\nnetes networking, check out Networking and Kubernetes: A Layered Approach by\nJames Strong and Vallery Lancey (O’Reilly, 2021).\nIt’s also possible to launch the cluster within an existing VPC by specifying the subnets\nin which you want to run the deployment. If launching within an existing VPC, you must\nmake sure the VPC and the provided subnets are correctly configured for operating the\nKubernetes cluster. See the eksctl documentation to learn about the networking\nrequirements of a Kubernetes cluster (https://eksctl.io/usage/vpc-networking/) and\nPublic network 1\nPublic network 2\nPublic network 3\nPrivate network 1\nPrivate network 2\nCIDR reservation 1\nVPC\nKubernetes control plane\nKubernetes API\netcd\ncoreDNS\nPrivate network 3\nCIDR reservation 2\nSecurity group that\nallows communication\nbetween nodes\nSecurity group that allows\ncommunication between the\ncontrol plane and the nodes\nKubernetes cluster\nFigure 14.3\neksctl creates a VPC with three public networks, three private networks, two CIDR \nreservations, and two VPC security groups. It also deploys the Kubernetes cluster within the VPC.",
      "content_length": 1497,
      "extraction_method": "Direct"
    },
    {
      "page_number": 377,
      "chapter": null,
      "content": "349\n14.3\nCreating a Kubernetes cluster with EKS\nthe official AWS documentation on the VPC networking requirements for a Kuberne-\ntes cluster (http://mng.bz/aPRY).\n As you can see in figure 14.3, eksctl creates six subnets by default: three public and\nthree private, with their corresponding NAT gateways and routing tables. A subnet is a\nsubset of the IP addresses available in a VPC. Public subnets are accessible through\nthe internet, while private subnets are not. eksctl also creates two subnet CIDR reser-\nvations for internal use by Kubernetes, as well as two security groups; one of them\nallows communication between all nodes in the cluster, and the other allows commu-\nnication between the control plane and the nodes.\nDEFINITION\nCIDR stands for Classless Inter-Domain Routing, and it’s a notation\nused for representing ranges of IP addresses. CIDR notation includes an IP\naddress followed by a slash and a decimal number, where the decimal number\nrepresents the range of addresses. For example, 255.255.255.255/32 represents\na range for one address. To learn more about CIDR notation, see Wikipedia’s\narticle: https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing.\nOnce we’ve created the cluster, we can configure kubectl to point to it, which will\nallow us to manage the cluster with the command line. Use the following command to\npoint kubectl to the cluster:\n$ aws eks update-kubeconfig --name coffeemesh --region <aws_region>\nNow that we’re connected to the cluster, we can inspect its properties. For example,\nwe can get a list of running nodes with the following command:\n$ kubectl get nodes\n# output truncated:\nNAME                                       STATUS  ROLES  AGE    VERSION\nfargate-ip-192-168-157-75.<aws_region>...  Ready  <none>  4d16h  v1.20.7...\nfargate-ip-192-168-170-234.<aws_region>... Ready  <none>  4d16h  v1.20.7...\nfargate-ip-192-168-173-63.<aws_region>...  Ready  <none>  4d16h  v1.20.7...\nTo get the list of pods running in the cluster, run the following command:\n$ kubectl get pods -A\nNAMESPACE    NAME                      READY  STATUS   RESTARTS  AGE\nkube-system  coredns-647df9f975-2ns5m  1/1    Running  0         2d15h\nkube-system  coredns-647df9f975-hcgjq  1/1    Running  0         2d15h\nThere are many more useful commands you can run to learn more about your cluster.\nCheck out the official documentation about the Kubernetes CLI for additional com-\nmands and options (https://kubernetes.io/docs/reference/kubectl/). A good start-\ning point is the kubectl cheat sheet (https://kubernetes.io/docs/reference/kubectl/\ncheatsheet/). Now that our cluster is up and running, in the next section, we’ll create\nan IAM role for our Kubernetes service accounts.",
      "content_length": 2709,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": null,
      "content": "350\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\n14.4\nUsing IAM roles for Kubernetes service accounts\nEvery process that runs in your Kubernetes cluster has an identity, and that identity\nis given by a service account. Service accounts determine the access privileges of a pro-\ncess within the cluster. Sometimes, our services need to interact with AWS resources\nusing the AWS API. To give access to the AWS API, we need to create IAM roles—\nentities that give applications access to the AWS API—for our services. As you can\nsee in figure 14.4, to link a Kubernetes service account to an IAM role, we use OpenID\nConnect (OIDC). By using OIDC, our pods can obtain temporary credentials to access\nthe AWS API.\nTo check if your cluster has an OIDC provider, run the following command, replacing\n<cluster_name> with the name of your cluster:\n$ aws eks describe-cluster --name coffeemesh \\\n--query \"cluster.identity.oidc.issuer\" --output text\nYou’ll get an output like the following:\nhttps:/ /oidc.eks.<aws_region>.amazonaws.com/id/BE4E5EE7DCDF9FB198D06FC9883F\n➥ F1BE\nIn \nthis \ncase, \nthe \nID \nof \nthe \ncluster’s \nOIDC \nprovider \nis\nBE4E5EE7DCDF9FB198D06FC9883FF1BE. Grab the OIDC provider’s ID and run the fol-\nlowing command:\n$ aws iam list-open-id-connect-providers | \\\ngrep BE4E5EE7DCDF9FB198D06FC9883FF1BE\nThis command lists all the OIDC providers in your AWS account, and it uses grep to\nfilter by the ID of your cluster’s OIDC provider. If you get a result, it means you\nalready have an OIDC provider for your cluster. If you don’t get any output, it means\nyou don’t have an OIDC provider, so let’s create one! To create an OIDC provider for\nPod\nOpenID Connect\nrovider\np\nService\nccount\na\nIAM role\nAWS API\nPods can authenticate with an\nOpenID Connect rovider.\np\nA successfully authenticated pod gets\naccess to an IAM role, which gives the\npod access to the AWS API.\nFigure 14.4\nPods can authenticate with an OIDC provider to assume an IAM role, which gives them \naccess to the AWS API, and therefore gives them access to AWS services.",
      "content_length": 2045,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": null,
      "content": "351\n14.5\nDeploying a Kubernetes load balancer\nyour cluster, run the following command, replacing <cluster_name> with the name of\nyour cluster:\n$ eksctl utils associate-iam-oidc-provider --cluster <cluster_name> \\\n--approve\nThat’s all it takes. Now we can link IAM roles to our service accounts! In the next sec-\ntion, we deploy a Kubernetes load balancer to enable external traffic to the cluster.\n14.5\nDeploying a Kubernetes load balancer\nRight now, our cluster is not accessible from outside of the VPC. If we deploy our\napplications, they’ll only get internal IPs and therefore won’t be accessible to the\nexternal world. To enable external access to the cluster, we need an ingress controller. As\nyou can see in figure 14.5, an ingress controller accepts traffic from outside of the\nKubernetes cluster and load balances it among our pods. To redirect traffic to specific\npods, we create ingress resources for each service. The ingress controller takes care of\nmanaging ingress resources.\nIn this section, we’ll deploy a Kubernetes ingress controller as an AWS Load Balancer\nController.3 As you can see in figure 14.5, the AWS Load Balancer Controller deploys\nan AWS Application Load Balancer (ALB), which sits in front of our cluster, captures\nincoming traffic, and forwards it to our services. To forward traffic to our services, the\n3 The AWS Load Balancer Controller is an open source project hosted on GitHub (https://github.com/kubernetes\n-sigs/aws-load-balancer-controller/). The project was originally created by Ticketmaster and CoreOS.\nNode\nPod\nPod\nPod\nService\nApplication\nLoad\nBalancer\nIngress\ncontroller\nThe ALB accepts trafﬁc from the\ninternet and forwards it to the\ningress controller.\nKubernetes cluster\nIngress\nresource\nThe ingress controller redirects trafﬁc to the\npods using an ingress resource per service.\nFigure 14.5\nAn ingress controller accepts traffic from outside of the Kubernetes cluster and forwards it to \nthe pods according to rules defined by ingress resources.",
      "content_length": 1995,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": null,
      "content": "352\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\nALB uses the concept of target groups—a rule for how traffic should be forwarded from\nthe ALB to a specific resource. For example, we can have target groups based on IPs,\nservices IDs, and other factors. The load balancer monitors the health of its registered\ntargets and makes sure traffic is only redirected to healthy targets.\n To install the AWS Load Balancer Controller, we need to have an OIDC provider in\nthe cluster, so make sure you’ve gone through section 14.4 before proceeding. The\nfirst step to deploying an AWS Load Balancer Controller is to create an IAM policy\nthat gives the controller access to the relevant AWS APIs. The open source community\nthat maintains the AWS Load Balancer Controller project provides a sample of the\npolicy that we need, so we simply need to fetch it:\n$ curl -o alb_controller_policy.json \\\nhttps:/ /raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-\n➥ controller/main/docs/install/iam_policy.json\nAfter running this command, you’ll see a file called alb_controller_policy.json in your\ndirectory. Now we can create the IAM policy using this file:\n$ aws iam create-policy \\\n--policy-name ALBControllerPolicy \\\n--policy-document file:/ /alb_controller_policy.json\nThe next step is to create an IAM role associated to a Kubernetes service account for\nthe load balancer with the following command:\n$ eksctl create iamserviceaccount \\\n  --cluster=coffeemesh \\\n  --namespace=kube-system \\\n  --name=alb-controller \\\n  --attach-policy-arn=arn:aws:iam::<aws_account_id>:policy/ALBControllerPolicy \\\n  --override-existing-serviceaccounts \\\n  --approve\nThis command creates a CloudFormation stack, which includes an IAM role associated\nwith the policy we created earlier, as well as a service account named alb-controller\nwithin the kube-system namespace reserved for system components of the Kuber-\nnetes cluster.\n Now we can install the Load Balancer Controller. We’ll use Helm to install the con-\ntroller, a package manager for Kubernetes. If you don’t have Helm available in your\nmachine, you need to install it. There are different strategies for installing Helm\ndepending on your platform, so make sure you check out the documentation to see\nwhich option works best for you (https://helm.sh/docs/intro/install/).\n Once Helm is available on your machine, you need to update it by adding the EKS\ncharts repository to your local helm (in Helm, packages are called charts). To add the\nEKS charts, run the following command:\n$ helm repo add eks https:/ /aws.github.io/eks-charts",
      "content_length": 2580,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": null,
      "content": "353\n14.6\nDeploying microservices to the Kubernetes cluster\nNow let’s update helm to make sure we pick up the most recent updates to the charts:\n$ helm repo update\nNow that helm is up to date, we can install the AWS Load Balancer Controller. To\ninstall the controller, we need to get hold of the ID of the VPC eksctl created when we\nlaunched the cluster. To find the VPC ID, run the following command:\n$ eksctl get cluster --name coffeemesh -o json | \\\njq '.[0].ResourcesVpcConfig.VpcId'\n# output: \"vpc-07d35ccc982a082c9\"\nTo run the previous command successfully, you need to have jq installed. Please refer\nto section 14.1 to learn how to install it. Now we can install the controller by running\nthe following command:\n$ helm install aws-load-balancer-controller eks/aws-load-balancer-\n➥ controller \\\n  -n kube-system \\\n  --set clusterName=coffeemesh \\\n  --set serviceAccount.create=false \\\n  --set serviceAccount.name=alb-controller \\\n  --set vpcId=<vpc_id>\nSince the controller is an internal Kubernetes component, we install it within the kube-\nsystem namespace. We make sure that the controller is installed for the coffeemesh\ncluster. We also instruct Helm not to create a new service account for the controller,\nand instead use the alb-controller service account we created earlier. \n It’ll take a few minutes until all the resources are created. To verify that the deploy-\nment went well, run the following command:\n$ kubectl get deployment -n kube-system aws-load-balancer-controller\nNAME              READY    UP-TO-DATE    AVAILABLE    AGE\nalb-controller    2/2      2             2            84s\nYou’ll know that the controller is up and running when the READY column shows 2/2,\nwhich means the desired number of resources are up. Our cluster is now ready, so it’s\ntime to deploy the orders service!\n14.6\nDeploying microservices to the Kubernetes cluster\nNow that our Kubernetes cluster is ready, it’s time to start deploying our services! In\nthis section, we walk through the steps required to deploy the orders service. You can\nfollow the same steps to deploy other services of the CoffeeMesh platform.\n As you can see in figure 14.6, we deploy the orders service to a new namespace\ncalled orders-service. This allows us to logically group and isolate all the resources\nrequired to operate the orders service. To create a new namespace, we run the follow-\ning command:\n$ kubectl create namespace orders-service",
      "content_length": 2424,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": null,
      "content": "354\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\nSince we’ll run the orders service in the new namespace, we also need to create a new\nFargate profile configured to schedule jobs within the orders-service namespace.\nTo create the new Fargate profile, run the following command:\n$ eksctl create fargateprofile --namespace orders-service --cluster \\\ncoffeemesh --region <aws_region>\nWith the orders-service namespace and the Fargate profile ready, we can deploy the\norders service. To make the deployment, we take the following steps:\n1\nCreate a deployment object for the orders service.\n2\nCreate a service object.\n3\nCreate an ingress resource to expose the service.\nThe following sections explain in detail how to proceed in each step.\n14.6.1 Creating a deployment object\nLet’s begin by creating a deployment for the orders service using a service manifest\nfile. As you can see in figure 14.7, deployments are Kubernetes objects that operate\nour pods and provision them with everything they need to run, including a Docker\nimage and port configuration. Create a file named orders-service-deployment.yaml\nand copy the contents of listing 14.1 into it.\n We use Kubernetes’ API version apps/v1 and declare this object a Deployment. In\nmetadata, we name the deployment orders-service, we specify its namespace, and\nNode\nPod\nPod\nPod\nService\nIngress\nresource\nDeployment object\nFargate proﬁle\norders-service namespace\nA deployment ensures we\nhave the desired number\nof pods.\nThe Fargate proﬁle ensures our\npods can run on AWS servers.\nA service\nexposes our\npods as web\napplications.\nAn ingress resource\naccepts HTTP trafﬁc\nfrom outside the cluster.\nFigure 14.6\nTo deploy a microservice, we create a new namespace, and within this namespace we deploy \nall the components needed to operate the microservice, such as a Deployment object and a Service \nobject.",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": null,
      "content": "355\n14.6\nDeploying microservices to the Kubernetes cluster\nwe add the label app: orders-service. Labels are custom identifiers for Kubernetes\nobjects, and they can be used for monitoring, tracing, or scheduling tasks, among\nother uses.4\n In the spec section, we define a selector rule that matches pods with the label app:\norders-service, which means this deployment will only operate pods with this label.\nWe also declare that we’d like to run only one replica of the pod. \n Within the spec.template section, we define the pod operated by this deploy-\nment. We label the pod with the app: orders-service key-value pair in agreement\nwith the deployment’s selector rule. Within the pod’s spec section, we declare the\ncontainers that belong in the pod. In this case, we want to run just one container,\nwhich is the orders service application. Within the definition of the orders service con-\ntainer, we specify the image that must be used to run the application with the port on\nwhich the application runs.\n# file: orders-service-deployment.yaml\napiVersion: apps/v1     \nkind: Deployment    \n4 To learn more about labels and how to use them, see the official documentation, https://kubernetes.io/docs/\nconcepts/overview/working-with-objects/labels/, and Zane Hitchcox’s “matchLabels, Labels, and Selectors\nExplained in Detail, for Beginners,” Medium (July 15, 2018), https://medium.com/@zwhitchcox/matchlabels\n-labels-and-selectors-explained-in-detail-for-beginners-d421bdd05362.\nListing 14.1\nDeclaring a deployment manifest\nPod\nPod\nPod\nElastic Container\nRegistry (ECR)\ncontainerPort: 8000\nDeployment object\nA Deployment object\nsupplies the required\nDocker images for\nour pods.\nA Deployment object provides\nport conﬁguration for the pods.\nIt also ensures we have\nthe desired number of\npods.\nFigure 14.7\nA Deployment object provides necessary configuration for the pods, such as their \nDocker image and port configuration, and ensures we have the desired number of pods running.\nVersion of the Kubernetes \nAPI used in this manifest\nThis manifest defines a \nDeployment object.",
      "content_length": 2072,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": null,
      "content": "356\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\nmetadata:\n  name: orders-service     \n  namespace: orders-service   \n  labels:     \n    app: orders-service\nspec:     \n  replicas: 1     \n  selector:\n    matchLabels:\n      app: orders-service     \n  template:    \n    metadata:\n      labels:\n        app: orders-service    \n    spec:      \n      containers:\n      - name: orders-service\n        image: <aws_account_id>.dkr.ecr.<aws_region>.amazonaws.com/\ncoffeemesh-orders:1.0      \n        ports:\n          - containerPort: 8000    \n        imagePullPolicy: Always\nTo create the deployment, we run the following command:\n$ kubectl apply -f orders-service-deployment.yaml\nThis command creates the deployment and launches the pods we defined in the man-\nifest file. It’ll take a few seconds for the pods to become available. You can check their\nstate with the following command:\n$ kubectl get pods -n orders-service\nThe initial state of the pods will be Pending, and once they’re up and running their\nstate will change to Running.\nWhat is a Kubernetes manifest file?\nIn Kubernetes, we can create objects using manifest files. Objects are resources\nsuch as namespaces, deployments, services, and so on. A manifest file is a YAML\nfile that describes the properties of the object and its desired state. Using manifest\nfiles is convenient because they can be tracked in source control, which helps us\ntrace changes to our infrastructure.\nEach manifest file contains, at a minimum, the following properties:\n\napiVersion—The version of the Kubernetes API that we want to use. Each\nKubernetes object has its own stable version. You can check the latest stable\nversion of each object for your Kubernetes cluster by running the following\ncommand: kubectl api-resources.\nThe deployment’s \nname\nThe namespace within which the \ndeployment must be located\nA label\nfor the\ndeployment\nThe deployment’s \nspecification\nHow many\npods must\nbe deployed\nA label selector \nfor pods\nTemplate\nfor the\npods\nA label for \nthe pods\nSpecification\nfor the pods\nThe pods’\nimage\nThe port on which \nthe API runs",
      "content_length": 2087,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": null,
      "content": "357\n14.6\nDeploying microservices to the Kubernetes cluster\n14.6.2 Creating a service object\nNow that our deployment is ready, we will create a service object for the orders service.\nAs we learned in section 14.2, services are Kubernetes objects that allow us to expose\nour pods as networking services. As you can see in figure 14.8, a service object exposes\nour applications as web services and redirects traffic from the cluster to our pods on\nthe specified ports. Create a file named orders-service.yaml, and copy into it the con-\ntents of listing 14.2, which shows how to configure a simple service manifest.\nWe use version v1 of the Kubernetes API to declare our service. In metadata, we spec-\nify that the service’s name is orders-service and that it’s to be launched within the\norders-service namespace. We also add a label: app: orders-service. In the ser-\nvice’s spec section, we configure a ClusterIP type, which means the pod will only be\naccessible from within the cluster. There are other types of services in Kubernetes,\n\nkind—The kind of object that we’re creating. Possible values include Service,\nIngress, and Deployment, among others.\n\nmetadata—A collection of properties that provide identifying information\nabout the object, such as its name, its namespace, and additional labels.\n\nspec—The specification for the object. For example, if we’re creating a ser-\nvice, we use this section to specify the type of service we’re creating (e.g.,\nNodePort) and selector rules.\nTo create an object from a manifest file, we use the kubectl apply command. For\nexample, if we have a manifest file called deployment.yaml, we apply it using the fol-\nlowing command:\n$ kubectl apply -f deployment.yaml\nPod\nPod\nPod\nService\nKubernetes cluster\nTCP port 80\nPort 8000\nPort 8000\nPort 8000\nFigure 14.8\nA service object redirects traffic from the cluster to the pods on the specified \nports. In this example, incoming traffic to the cluster on port 80 is redirected to port 8000 \nin the pods.",
      "content_length": 1990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": null,
      "content": "358\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\nsuch as NodePort and LoadBalancer. (To learn more about the types of services and\nwhen to use each type, see the sidebar, “Which type of Kubernetes service should I use?”)\n We also create a forwarding rule to redirect traffic from port 80 to port 8000, which\nis the port on which our containers run. Finally, we specify a selector for the app:\norders-service label, which means this service will only operate pods with that label.\n# file: orders-service.yaml\napiVersion: v1\nkind: Service    \nmetadata:\n  name: orders-service\n  namespace: orders-service\n  labels:\n    app: orders-service\nspec:\n  selector:\n    app: orders-service\n  type: ClusterIP     \n  ports:\n    - protocol: http     \n      port: 80   \n      targetPort: 8000     \n To deploy this service, run the following command:\n$ kubectl apply -f orders-service.yaml\nListing 14.2\nDeclaring a service manifest\nWhich type of Kubernetes service should I use?\nKubernetes has four types of services. Here we discuss the features of each service\ntype and their use cases:\n\nClusterIP—Exposes services on the cluster’s internal IP and therefore\nmakes them accessible only within the cluster\n\nNodePort—Exposes services on the node’s external IP and therefore makes\nthem available on the cluster’s network\n\nLoadBalancer—Exposes the service directly through a dedicated cloud load\nbalancer\n\nExternalName—Exposes the service through an internal DNS record within\nthe cluster\nWhich of these types should you use? It depends on your needs. NodePort is useful if\nyou want to be able to access your services externally on the IP of the node in which\nthey’re running. The downside is the service uses the static port of the node, so you\ncan only run one service per node. ClusterIP is useful if you’d rather access the ser-\nvice on the cluster’s IP. ClusterIP services are not directly reachable from outside the\ncluster, but you can expose them by creating ingress rules that forward traffic to them.\nThis manifest defines \na Service object.\nThis is a ClusterIP \ntype of Service.\nThe service \ncommunicates \nover HTTP.\nThe service must \nbe mapped to \nport 80.\nThe service\nruns internally\non port 8000.",
      "content_length": 2204,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": null,
      "content": "359\n14.6\nDeploying microservices to the Kubernetes cluster\n14.6.3 Exposing services with ingress objects\nThe final step is to expose the service through the internet. To expose the service, we\nneed to create an ingress resource that routes traffic to the service. As you can see in\nfigure 14.9, an ingress resource is a service that redirects HTTP traffic to the pods run-\nning in our Kubernetes cluster on the specified ports and URL paths. Create a file\nnamed orders-service-ingress.yaml and copy the content of listing 14.3 to it.\nIn the ingress manifest, we use version networking.k8s.io/v1 of the Kubernetes API,\nand we declare the object as an Ingress type. In metadata, we name the ingress object\norders-service-ingress, and we specify that it should be deployed within the orders-\nservice namespace. We use annotations to bind the ingress object to the AWS Load\nBalancer we deployed in section 14.5. Within the spec section, we define the forward-\ning rules of the ingress resource. We declare an HTTP rule that forwards all traffic\nunder the /orders path to the orders service and additional rules to access the ser-\nvice’s API documentation.\n# file: orders-service-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress    \nLoadBalancer is useful if you’d like to use one cloud load balancer per service.\nUsing a load balancer per service makes configuration somewhat simpler, as you\nwon’t have to configure multiple ingress rules. However, load balancers are usually\nthe most expensive components of your cluster, so if budget is a factor, you may not\nwant to use this option. Finally, ExternalName is useful if you want to be able to\naccess your services from within the cluster using custom domains.\nListing 14.3\nDeclaring an ingress manifest\nPod\nPod\nPod\nService\nKubernetes cluster\nHTTP traﬃc\nIngress object\nRules:\n- path: /orders\nport: 80\nPort 80\n/orders\nURL path preﬁx\nFigure 14.9\nAn ingress object allows us to redirect HTTP traffic on a specific port and URL path to a \nservice object.\nThe manifest \ndefines an \nIngress object.",
      "content_length": 2049,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": null,
      "content": "360\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\nmetadata:\n  name: orders-service-ingress\n  namespace: orders-service\n  annotations:      \n    kubernetes.io/ingress.class: alb     \n    alb.ingress.kubernetes.io/target-type: ip         \n    alb.ingress.kubernetes.io/scheme: internet-facing     \nspec:\n  rules:    \n  - http:\n      paths:\n      - path: /orders    \n        pathType: Prefix    \n        backend:     \n          service:\n            name: orders-service     \n            port:\n              number: 80    \n      - path: /docs/orders\n        pathType: Prefix\n        backend:\n          service:\n            name: orders-service\n            port:\n              number: 80\n      - path: /openapi/orders.json\n        pathType: Prefix\n        backend:\n          service:\n            name: orders-service\n            port:\n              number: 80\nTo create this ingress resource, we run the following command:\n$ kubectl apply -f orders-service-ingress.yaml\nThe orders API is now accessible. To call the API, we first need to find out the endpoint\nfor the ingress rule we just created. Run the following command to get the details of the\ningress resource:\n$ kubectl get ingress/orders-service-ingress -n orders-service\n# output truncated:\nNAME                     CLASS    HOSTS   ADDRESS...\norders-service-ingress   <none>   *       k8s-ordersse-ordersse-3c391193...\nThe value under the ADDRESS field is the URL of the load balancer. You can also get\nhold of this value by running the following command:\n$ kubectl get ingress/orders-service-ingress -n orders-service -o json | \\\njq '.status.loadBalancer.ingress[0].hostname'\n\"k8s-ordersse-ordersse-3c39119336-236890178.<aws_region>.elb.amazonaws.com\"\nAWS configuration for the Ingress\nThe Ingress exposes an \nApplication Load Balancer.\nTraffic is routed \nto the pods \nbased on IP.\nThe Ingress is \navailable to external \nconnections.\nTraffic forwarding \nrules\nA rule for\nthe /orders\nURL path\nThe rule applies to \nrequests starting with \nthe /orders prefix.\nThe backend\nservice that\nhandles this\ntraffic\nTraffic must be \nrouted to the orders-\nservice Service.\nThe orders-service \nService is available \non port 80.",
      "content_length": 2178,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": null,
      "content": "361\n14.7\nSetting up a serverless database with AWS Aurora\nWe can use this URL to call the orders service API. Since the database isn’t yet ready,\nthe API itself won’t work, but we can access the API documentation:\n$ curl http:/ /k8s-ordersse-ordersse-3c39119336-\n➥ 236890178.<aws_region>.elb.amazonaws.com/openapi/orders.json\nIt may take some time for the load balancer to become available, and in the meantime\ncurl won’t be able to resolve the host. If that happens, wait a few minutes and try\nagain. To be able to interact with the API, we must set up a database, which will be the\ngoal of our next section!\n14.7\nSetting up a serverless database with AWS Aurora\nThe orders service is almost ready: the application is up and running, and we can\naccess it through the internet. Only one component is missing: the database. We have\nmultiple choices for setting up the database. We can set up the database as a deploy-\nment within our Kubernetes cluster with a mounted volume, or we can choose one of\nthe many managed database services that cloud providers offer.\n To keep it simple and cost-effective, in this section, we’ll set up an Aurora Server-\nless database in AWS—a powerful database engine that is cost-effective since you only\npay for what you use and is very convenient since you don’t have to worry about man-\naging or scaling the database.\n14.7.1 Creating an Aurora Serverless database\nWe’ll launch our Aurora database within the Kubernetes cluster’s VPC. To be able to\nlaunch a database within an existing VPC, we need to create a database subnet group: a\ncollection of subnets within the VPC. As we learned in section 14.3, eksctl divides the\nKubernetes cluster’s VPC into six subnets: three public and three private. The six sub-\nnets are distributed across three availability zones (data centers within an AWS region),\nwith one public and one private subnet per availability zone.\n When choosing the subnets for our database subnet group, we need to consider\nthe following constraints:\nAurora Serverless only supports one subnet per availability zone. \nWhen creating a database subnet group, the subnets must all be either private\nor public.5\nFor security, it’s best practice to use private subnets in database subnet groups as it\nensures that the database server is not accessible from outside of the VPC, which\nmeans external and unauthorized users are unable to connect to it directly. To find\n5 For more information on this point, see the official AWS documentation: https://docs.aws.amazon.com/\nAmazonRDS/latest/UserGuide/USER_VPC.WorkingWithRDSInstanceinaVPC.html.",
      "content_length": 2587,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": null,
      "content": "362\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\nthe list of private subnets in the VPC, we first need to obtain the ID of the Kubernetes\ncluster’s VPC with the following command:\n$ eksctl get cluster --name coffeemesh -o json | \\\njq '.[0].ResourcesVpcConfig.VpcId'\nThen use the following command to get the IDs of the private subnets in the VPC:\n$ aws ec2 describe-subnets --filters Name=vpc-id,Values=<vpc_id> \\\n--output json | jq '.Subnets[] | select(.MapPublicIpOnLaunch == false) | \\\n.SubnetId'\nThe previous command lists all the subnets in the Kubernetes cluster’s VPC, and it\nuses jq to filter the public subnets. Armed with all this information, we can now create\nthe database subnet group using the following command:\n$ aws rds create-db-subnet-group --db-subnet-group-name \\\ncoffeemesh-db-subnet-group --db-subnet-group-description \"Private subnets\" \\\n--subnet-ids \"<subnet_id>\" \"<subnet_id>\" \"<subnet_id>\"\nAs you can see in figure 14.10, this command creates a database subnet group named\ncoffeemesh-db-subnet-group. When running the command, make sure you replace\nthe <subnet_id> placeholders with the IDs of your private subnets. We’ll deploy our\nAurora database within this database subnet group.\nNext, we need to create a VPC security group—a set of rules that define what incoming\nand outgoing traffic is allowed from the VPC—that allows traffic to the database so\nthat our applications can connect to it. The following command creates a security\ngroup called db-access:\nPublic network 1\nPublic network 2\nPublic network 3\nPrivate network 1\nPrivate network 2\nVPC\nPrivate network 3\ncoﬀeemesh-db-subnet-group\n(database subnet group)\nFigure 14.10\nWe deploy an Aurora database within a database subnet group named coffeemesh-\ndb-subnet-group. The database subnet group is created on top of the three private subnets of our \nVPC to prevent unauthorized access.",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": null,
      "content": "363\n14.7\nSetting up a serverless database with AWS Aurora\n$ aws ec2 create-security-group --group-name db-access --vpc-id <vpc-id> \\\n--description \"Security group for db access\"\n# output:\n{\n    \"GroupId\": \"sg-00b47703a4299924d\"\n}\nIn the previous command, replace <vpc-id> with the ID of your Kubernetes cluster’s\nVPC. The output from the previous command is the ID of the security group we just\ncreated. We’ll allow traffic from all IP addresses on PostgreSQL’s default port, which is\n5432. Since we’re going to deploy the database into private subnets, it’s okay to listen\non all IPs, but for additional security, you may want to restrict the range of addresses\nto those of your pods. We use the following command to create an inbound traffic\nrule for our database access security group:\n$ aws ec2 authorize-security-group-ingress --group-id \\\n<db-security-group-id> --ip-permissions \\ \n'FromPort=5432,IpProtocol=TCP,IpRanges=0.0.0.0/0'\nIn this command, replace <db-security-group-id> with the ID of your database\naccess security group.\n Now that we have a database subnet group and a security group that allows our pods\nto connect to it, we can use the subnet group to launch an Aurora Serverless cluster\nwithin our VPC! Run the following command to launch an Aurora Serverless cluster:\n$ aws rds create-db-cluster --db-cluster-identifier coffeemesh-orders-db \\\n--engine aurora-postgresql --engine-version 10.14 \\\n--engine-mode serverless \\\n--scaling-configuration MinCapacity=8,MaxCapacity=64,\n➥ SecondsUntilAutoPause=1000,AutoPause=true \\\n--master-username <username> \\\n--master-user-password <password> \\\n--vpc-security-group-ids <security_group_id> \\\n--db-subnet-group <db_subnet_group_name>\nLet’s take a close look at the command’s parameters:\n\n--db-cluster-identifier—The name of the database cluster. We’re naming\nthe cluster coffeemesh-orders-db.\n\n--engine—The database engine you want to use. We’re using a PostgreSQL-\ncompatible engine, but you can also choose a MySQL-compatible engine if\nyou prefer.\n\n--engine-version—The version of the Aurora engine you want to use. We’re\nchoosing version 10.14, which is the only version available for Aurora Post-\ngreSQL serverless right now. See the AWS documentation to keep up to date\nwith new versions (http://mng.bz/gRyn).",
      "content_length": 2281,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": null,
      "content": "364\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\n\n--engine-mode—The database engine mode. We’re choosing serverless to keep\nthe example simple and cost-effective.\n\n--scaling-configuration—The autoscaling configuration for the Aurora clus-\nter. We configure the cluster with minimum Aurora capacity units (ACU) of 8\nand a maximum of 64. Each ACU provides approximately 2 GB of memory. We\nalso configure the cluster to scale down to 0 ACUs automatically after 1,000 sec-\nonds without activity.6\n\n--master-username—The username of the database master user.\n\n--master-user-password—The password of the database master user.\n\n--vpc-security-group-ids—The ID of the database access security group we\ncreated in the previous step.\n\n--db-subnet-group—The name of the database security group we created earlier.\nAfter running this command, you’ll get a large JSON payload with details about the\ndatabase. To connect to the database, we need the value of the DBCluster.Endpoint\nproperty of the payload, which represents the database’s hostname. We’ll use this\nvalue in the next sections to connect to the database.\n14.7.2 Managing secrets in Kubernetes\nTo connect our services to the database, we need a secure way to pass the connection\ncredentials. The native way to manage sensitive information in Kubernetes is using\nKubernetes secrets. This way, we avoid having to expose sensitive information through\nthe code or through our image builds. In this section, you’ll learn how to manage\nKubernetes secrets securely.\n AWS EKS offers two secure ways to manage Kubernetes secrets: we can use the AWS\nSecrets & Configuration Provider for Kubernetes,7 or we can use AWS Key Manage-\nment Service (KMS) to secure our secrets with envelope encryption. In this section,\nwe’ll use envelope encryption to protect our secrets.8\n As you can see in figure 14.11, envelope encryption is the practice of encrypting\nyour data with a data encryption key (DEK) and encrypting the DEK with a key\nencryption key (KEK).9 It sounds complicated, but it’s simple to use since AWS does\nthe heavy lifting for us.\n6 See the official documentation for more information on how Aurora Serverless works and the autoscaling con-\nfiguration parameters: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-\nv2.html.\n7 You can learn more about this option through Tracy Pierce’s article “How to use AWS Secrets & Configuration\nProvider with Your Kubernetes Secrets Store CSI driver,” https://aws.amazon.com/blogs/security/how-to-use\n-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/.\n8 Managing Kubernetes securely is a big and important topic, and to learn more about, it you can check out Alex\nSoto Bueno and Andrew Block’s Securing Kubernetes Secrets (Manning, 2022), https://livebook.manning.com/\nbook/securing-kubernetes-secrets/chapter-4/v-3/point-13495-119-134-1.\n9 Ibid.",
      "content_length": 2905,
      "extraction_method": "Direct"
    },
    {
      "page_number": 393,
      "chapter": null,
      "content": "365\n14.7\nSetting up a serverless database with AWS Aurora\nTo use envelope encryption, first we need to generate an AWS KMS key. You can use\nthe following command to create the key:\n$ aws kms create-key\nThe output of this command is a payload with metadata about the newly created key.\nFrom this payload, we want to use the KeyMetadata.Arn property, which represents\nthe key’s ARN, or Amazon Resource Name. The next step is to enable secrets encryp-\ntion in our Kubernetes cluster using eksctl:\n$ eksctl utils enable-secrets-encryption --cluster coffeemesh \\\n--key-arn=<key_arn> --region <aws_region>\nMake sure you replace <key_arn> with the ARN of your KMS key and <aws_region>\nwith the region where you deployed the Kubernetes cluster. The operation triggered\nby the previous command can take up to 45 minutes to complete. The command runs\nuntil the cluster is created, so just wait until it finishes. Once it’s done, we can create\nKubernetes secrets. Let’s create a secret that represents the database connection\nstring. A database connection string has the following structure:\n<engine>://<username>:<password>@<hostname>:<port>/<database_name>\nLet’s look at each component of the connection string:\n\nengine—The database engine, for example, postgresql.\n\nusername—The username we chose earlier when creating the database.\n\npassword—The password we chose earlier when creating the database.\n\nhostname—The database’s hostname, which we obtained in the previous sec-\ntion from the DBCluster.Endpoint property of the payload returned by the aws\nrds create-db-cluster command.\n\nport—The port on which the database is running. Each database has its own\ndefault port, such as 5432 for PostgreSQL and 3306 for MySQL.\n\ndatabase_name—The name of the database we’re connecting to. In PostgreSQL,\nthe default database is called postgres.\nFor example, for a PostgreSQL database, a typical connection string looks like this:\npostgresql:/ /username:password@localhost:5432/postgres\nData encryption key\nKey encryption key\nData\nA key encryption key encrypts\nthe data encryption key.\nThe data encryption key\nencrypts the data.\nEncrypted data\nFigure 14.11\nEnvelope encryption is the practice of encrypting data with a data encryption key \n(DEK) and encrypting the DEK with a key encryption key.",
      "content_length": 2284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": null,
      "content": "366\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\nTo store the database connection string as a Kubernetes secret, we run the following\ncommand:\n$ kubectl create secret generic -n orders-service db-credentials \\\n--from-literal=DB_URL=<connection_string>\nThe previous command creates a secret object named db-credentials within the\norders-service namespace. To get the details of this secret object, you can run the\nfollowing command:\n$ kubectl get secret db-credentials -n orders-service -o json\n# output:\n{\n    \"apiVersion\": \"v1\",\n    \"data\": {\n        \"DB_URL\": \"cG9zdGdyZXNxbDovL3VzZXJuYW1lOnBhc3N3b3JkQGNvZmZlZW1lc2gtZG\nIuY2x1c3Rlci1jYn\n➥ Y0YWhnc2JjZWcuZXUtd2VzdC0xLnJkcy5hbWF6b25hd3MuY29tOjU0MzIvcG9zdGdyZXM=\"\n    },\n    \"kind\": \"Secret\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-11-19T15:21:42Z\",\n        \"name\": \"db-credentials\",\n        \"namespace\": \"orders-service\",\n        \"resourceVersion\": \"599258\",\n        \"uid\": \"d2c210e7-c61c-46b7-9f43-9407766e147c\"\n    },\n    \"type\": \"Opaque\"\n}\nThe secrets are listed under the data property of the payload, and they’re Base64\nencoded. To obtain their values, you can run the following command:\n$ echo <DB_URL> | base64 --decode\nwhere <DB_URL> is the Base64 encoded value of the DB_URL key.\n To make the secret available to the orders service, we need to update the order ser-\nvice deployment to consume the secret and expose it as an environment variable. \n# file: orders-service-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orders-service\n  namespace: orders-service\n  labels:\n    app: orders-service\nspec:\n  replicas: 1\nListing 14.4\nConsuming secrets as environment variables in a deployment",
      "content_length": 1692,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": null,
      "content": "367\n14.7\nSetting up a serverless database with AWS Aurora\n  selector:\n    matchLabels:\n      app: orders-service\n  template:\n    metadata:\n      labels:\n        app: orders-service\n    spec:\n      containers:\n      - name: orders-service\n        image: \n➥ <aws_account_id>.dkr.ecr.<aws_region>.amazonaws.com/coffeemesh-orders:1.0\n        ports:\n          - containerPort: 8000\n        imagePullPolicy: Always\n        envFrom:    \n          - secretRef:      \n              name: db-credentials     \nLet’s apply the changes by running the following command:\n$ kubectl apply -f orders-service-deployment.yaml\nOur service can now connect to the database! We’re almost done. The final step is to\napply the database migrations, which we’ll accomplish in the next section.\n14.7.3 Running the database migrations and connecting our service \nto the database\nOur database is up and running, and now we can connect the orders service with it.\nHowever, before we can create records and run queries, we must ensure the database\nhas the expected schemas. As we saw in chapter 7, the process of creating the database\nschemas is called migration. Our application’s migrations are available under the\nmigrations folder. In this section, we’ll run the migrations against the Aurora Server-\nless database.\n In the previous section, we deployed the Aurora database to our private subnets,\nwhich means we can’t access our database directly to run the migrations. We have two\nmain options to connect to the database: connect through a bastion server or create a\nKubernetes Job that applies the migrations. Since we’re working with Kubernetes and\nour cluster is already up and running, using a Kubernetes Job is a suitable option for us.\nDEFINITION\nA bastion server is a server that allows you to establish a secure con-\nnection with a private network. By connecting to the bastion server, you are\nable to access other servers within the private network.\nTo create the Kubernetes job, we first need to create a Docker image for running the\ndatabase migrations. Create a file named migrations.dockerfile, and copy the con-\ntents of listing 14.5 into it. This Dockerfile installs both the production and the devel-\nopment dependencies and copies over the migrations and the Alembic configuration\nEnvironment \nconfiguration \nfor the pods\nConfiguration\nfor identifying\nthe secret\nEnvironment must be \nloaded from the secret \nnamed db-credentials.",
      "content_length": 2420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": null,
      "content": "368\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\ninto the container. As we saw in chapter 7, we use Alembic to manage our database\nmigrations. The command for this container is a one-off alembic upgrade.\n# file: migrations.dockerfile\nFROM python:3.9-slim\nRUN mkdir -p /orders/orders\nWORKDIR /orders\nRUN pip install -U pip && pip install pipenv\nCOPY Pipfile Pipfile.lock /orders/\nRUN pipenv install --dev --system --deploy\nCOPY orders/repository /orders/orders/repository/\nCOPY migrations /orders/migrations\nCOPY alembic.ini /orders/alembic.ini\nENV PYTHONPATH=/orders     \nCMD [\"alembic\", \"upgrade\", \"heads\"]\nTo build the Docker image, run the following command:\n$ docker build -t \n➥ <aws_account_number>.dkr.ecr.<aws_region>.amazonaws.com/coffeemesh-\n➥ orders-migrations:1.0 -f migrations.dockerfile .\nWe’re naming the image coffeemesh-orders-migrations and tagging it with version\n1.0. Make sure you replace <aws_account_id> with your AWS account ID and\n<aws_region> with the region where you want to store your Docker builds. Before we\npush the image to the container registry, we need to create a repository:\n$ aws ecr create-repository --repository-name coffeemesh-orders-migrations\nNow let’s push the image to the container registry:\n$ docker push \n➥ <aws_account_id>.dkr.ecr.<aws_region>.amazonaws.com/coffeemesh-orders-\n➥ migrations:1.0\nIf your ECR credentials have expired, you can refresh them by running the following\ncommand again:\n$ aws ecr get-login-password --region <aws_region> | docker login \\\n--username AWS --password-stdin \\\n<aws_account_id>.dkr.ecr.<aws_region>.amazonaws.com\nListing 14.5\nDockerfile for the database migrations job\nWe set the PYTHONPATH \nenvironment variable.",
      "content_length": 1707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": null,
      "content": "369\n14.7\nSetting up a serverless database with AWS Aurora\nNow that our image is ready, we need to create a Kubernetes Job object. We use a\nmanifest file to create the Job. Create a file named orders-migrations-job.yaml and\ncopy the contents of listing 14.6 into it. Listing 14.6 defines a Kubernetes object of\ntype Job using the batch/v1 API. Just as we did in the previous section for the orders\nservice, we expose the database connection string in the environment by loading the\ndb-credentials secret using the envFrom property of the container’s definition. We\nalso set the ttlSecondsAfterFinished parameter to 30 seconds, which controls how\nlong the pod will last in the orders-service namespace once it’s finished the job.\n# file: orders-migrations-job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: orders-service-migrations\n  namespace: orders-service\n  labels:\n    app: orders-service\nspec:\n  ttlSecondsAfterFinished: 30    \n  template:\n    spec:\n      containers:\n      - name: orders-service-migrations\n        image: \n➥ <aws_account_id>.dkr.ecr.<aws_region>.amazonaws.com/coffeemesh-orders-\n➥ migrations:1.0\n        imagePullPolicy: Always\n        envFrom:\n          - secretRef:\n              name: db-credentials\n      restartPolicy: Never\nLet’s create the Job by running the following command:\n$ kubectl apply -f orders-migrations-job.yaml\nIt’ll take a few seconds until the job’s pod is up and running. You can check its status\nby running the following command:\n$ kubectl get pods -n orders-service\nOnce the pod’s status is Running or Completed, you can check the job’s logs by run-\nning the following command:\n$ kubectl logs -f jobs/orders-service-migrations -n orders-service\nWatching the pod’s logs in this way is useful to check how the process is going and to\nspot any issues raised in its execution. Since the migration job is ephemeral and will\nListing 14.6\nCreating a database migrations job\nThe pod must be \ndeleted 30 seconds \nafter completing.",
      "content_length": 1976,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": null,
      "content": "370\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\nbe deleted after completion, make sure you check the logs while the process is run-\nning. Once the migrations job has completed, the database is finally ready to be used!\nWe can finally interact with the orders service—the moment we’ve been waiting for!\nOur service is now ready for use. The next section explains one more change we need\nto make to finalize the deployment.\n14.8\nUpdating the OpenAPI specification \nwith the ALB’s hostname\nNow that our service is ready and the database is deployed and configured, it’s time to\nplay around with the application! In chapters 2 and 6, we learned to interact with our\nAPIs using a Swagger UI. To use the Swagger UI in our deployment, we need to update\nthe API specification with the hostname of our Kubernetes cluster’s ALB. In this section,\nwe update the order’s API specification, make a new deployment, and test it.\n# file: oas.yaml\nopenapi: 3.0.0\ninfo:\n  title: Orders API\n  description: API that allows you to manage orders for CoffeeMesh\n  version: 1.0.0\nservers:\n  - url: <alb-hostname>\n    description: ALB's hostname\n  - url: https:/ /coffeemesh.com\n    description: main production server\n  - url: https:/ /coffeemesh-staging.com\n    description: staging server for testing purposes only\n  - url: http:/ /localhost:8000\n    description: URL for local testing\n...\nIn listing 14.8, replace <alb-hostname> with the hostname of your own ALB. As we\nlearned in section 14.6, you obtain the ALB’s hostname by running the following\ncommand:\n$ kubectl get ingress/orders-service-ingress -n orders-service -o json | \\\njq '.status.loadBalancer.ingress[0].hostname'\n# output:\n# \"k8s-ordersse-ordersse-8cf837ce7a-1036161040.<aws_region>.elb.amazonaws.com\"\nNow we need to rebuild our Docker image:\n$ docker build -t \n<aws_account_number>.dkr.ecr.<aws_region>.amazonaws.com/coffeemesh-\n➥ orders:1.1 .\nListing 14.7\nAdding the ALB’s hostname as a server",
      "content_length": 1951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": null,
      "content": "371\n14.8\nUpdating the OpenAPI specification with the ALB’s hostname\nThen, we publish the new build to AWS ECR:\n$ docker push \n<aws_account_number>.dkr.ecr.<aws_region>.amazonaws.com/coffeemesh-\n➥ orders:1.1\nNext, we need to update the orders service deployment manifest.\n# file: orders-service-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orders-service\n  namespace: orders-service\n  labels:\n    app: orders-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: orders-service\n  template:\n    metadata:\n      labels:\n        app: orders-service\n    spec:\n      containers:\n      - name: orders-service\n        image: \n➥ <aws_account_id>.dkr.ecr.<aws_region>.amazonaws.com/coffeemesh-orders:1.1\n        ports:\n          - containerPort: 8000\n        imagePullPolicy: Always\nFinally, we apply the new deployment configuration by running the following command:\n$ kubectl apply -f orders-service-deployment.yaml\nMonitor the rollout by running the following command: \nkubectl get pods -n orders-service\nOnce the old pod is terminated and the new one is up and running, load the order’s\nservice Swagger UI by pasting the ALB’s hostname in a browser and visiting the\n/docs/orders page. You can play around with the API using the same approach you\nlearned in chapters 2 and 6: creating orders, modifying them, and fetching their\ndetails from the server.\n And the journey is finally complete! If you’ve been able to follow up to this point\nand managed to get your Kubernetes cluster up and running, please accept my most\nListing 14.8\nDeclaring a deployment manifest",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": null,
      "content": "372\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\nsincere congratulations! You’ve made it! Figure 14.12 shows a high-level overview of\nthe architecture you’ve deployed in this chapter.\nThe overview of Kubernetes in this chapter is a brief one, but it’s enough to get an\nunderstanding of how Kubernetes works, and it’s sufficient to get a cluster up and run-\nning in your production environment. If you work or intend to work with Kubernetes,\nI strongly encourage you to continue reading about this technology. You can check all\nthe references I’ve cited in this chapter, to which I’d like to add Marko Lukša’s funda-\nmental Kubernetes in Action (2nd ed., Manning, expected 2023).\n In the next section, we’ll delete all the resources we created during this chapter.\nDon’t miss it if you don’t want to be charged more than needed!\n14.9\nDeleting the Kubernetes cluster\nThis section explains how to delete all the resources we created in this chapter. This\nstep is crucial to make sure you don’t get billed for the Kubernetes cluster once\nyou’ve finished working through the examples. As you can see in figure 14.13, we\nVPC\nKubernetes control plane\nKubernetes API\netcd\ncoreDNS\nKubernetes cluster\nPublic network 1\nPrivate network 1\ncoﬀeemesh-db-subnet-group\n(database subnet group)\nNode\nPod\nPod\nPod\nService\nApplication Load Balancer (ALB)\nIngress\ncontroller\nIngress\nresource\nPublic network 2\nPublic network 3\nPrivate network 2\nPrivate network 3\nAWS Fargate\nThe deployment object\nmanages the desired\nnumber of pods.\nPool of nodes\nHTTP\ntraﬃc\nFigure 14.12\nHigh-level overview of the architecture deployed in this chapter",
      "content_length": 1621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": null,
      "content": "373\n14.9\nDeleting the Kubernetes cluster\nhave dependency relationships among some of our resources. To successfully delete\nall resources, we must delete them in reverse order of their dependencies. For exam-\nple, the database cluster depends on the database subnet group, which depends on\nthe VPC subnets, which depend on the VPC. In this case, we’ll start by deleting the\ndatabase cluster, and in the last step we’ll delete the VPC.\nLet’s delete the database cluster with the following command:\n$ aws rds delete-db-cluster --db-cluster-identifier coffeemesh-db \\\n--skip-final-snapshot\nThe --skip-final-snapshot flag instructs the command not to create a snapshot of\nthe database before deletion. It takes a few minutes for the database to be deleted.\nOnce it’s deleted, we can delete the database subnet group with the following command:\n$ aws rds delete-db-subnet-group --db-subnet-group-name \\\ncoffeemesh-db-subnet-group\nNext, let’s delete the AWS Load Balancer Controller. Deleting the AWS Load Balancer\nController is a two-step process: first we uninstall the controller using helm, and then\nKubernetes cluster\nApplication\noad\nalancer\nL\nB\nKMS key\nPublic\nsubnet\nPublic\nsubnet\nPublic\nsubnet\nPrivate\nsubnet\nPrivate\nsubnet\nPrivate\nsubnet\nVPC\nDatabase subnet group\nVPC private\nsubnets\nVPC public\nsubnets\nAWS Load Balancer\nController\nFigure 14.13\nThe resources in our stack have relationships of dependency. The direction of dependency is \nindicated by the direction of the arrows. To delete the resources, we start by deleting those that have no arrows \npointing to them.",
      "content_length": 1571,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": null,
      "content": "374\nCHAPTER 14\nDeploying microservice APIs with Kubernetes\nwe delete the ALB that was created when we installed the controller. To delete the\nALB we need its URL, so let’s fetch that value first (make sure you run this step before\nuninstalling with helm):\n$ kubectl get ingress/orders-service-ingress -n orders-service -o json | \\\njq '.status.loadBalancer.ingress[0].hostname'\n# output: \"k8s-ordersse-ordersse-8cf837ce7a-\n➥ 1036161040.<aws_region>.elb.amazonaws.com\"\nNow let’s uninstall the controller with the following command:\n$ helm uninstall aws-load-balancer-controller -n kube-system\nAfter running this command, we need to delete the ALB. To delete the ALB, we need\nto find its ARN. We’ll use the AWS CLI to list the load balancers in our account and fil-\nter them out by their DNS name. The following command fetches the ARN of the\nload balancer whose DNS name matches the ALB’s URL, which we obtained earlier:\n$ aws elbv2 describe-load-balancers | jq '.LoadBalancers[] | \\\nselect(.DNSName == \"<load_balancer_url>\") | .LoadBalancerArn'\n# output: \"arn:aws:elasticloadbalancing:<aws_region>:<aws_account_id>:\n➥ loadbalancer/app/k8s-ordersse-ordersse-8cf837ce7a/cf708f97c2485719\"\nMake sure you replace <load_balancer_url> with your load balancer’s URL, which\nwe obtained in an earlier step. This command gives us the load balancer’s ARN, which\nwe can use to delete it:\n$ aws elbv2 delete-load-balancer --load-balancer-arn \"<load_balancer_arn>\"\nNow we can delete the Kubernetes cluster with following command:\n$ eksctl delete cluster coffeemesh\nFinally, let’s delete the KMS key we created earlier to encrypt our Kubernetes secrets.\nTo delete the key, we run the following command:\n$ aws kms schedule-key-deletion --key-id <key_id>\nwhere <key_id> is the ID of the key we created earlier.\nSummary\nKubernetes is a container orchestration tool that’s becoming a standard for\ndeploying microservices at scale. Using Kubernetes helps us to move across\ncloud providers while keeping a consistent interface to our services.\nThe three major managed Kubernetes services are Google’s Kubernetes Engine\n(GKE), Azure’s Kubernetes Service (AKS), and AWS’s Elastic Kubernetes Ser-\nvice (EKS). In this chapter, we learned to deploy a Kubernetes cluster with EKS,\nwhich is the most widely adopted Kubernetes managed service.",
      "content_length": 2314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": null,
      "content": "375\nSummary\nWe can deploy a Kubernetes cluster in AWS using the console, CloudFormation,\nor the eksctl command-line tool. In this chapter, we used the eksctl CLI since\nit’s the AWS recommended way to manage a Kubernetes cluster.\nTo make our Kubernetes cluster reachable from the internet, we use an ingress\ncontroller such as the AWS Load Balancer Controller.\nTo deploy a microservice to a Kubernetes cluster, we create the following\nresources: \n– A Deployment, which manages the desired state of the pods, processes that\nrun the Docker build\n– A Service that allows us to expose our application as a web service\n– An Ingress object bound to the ingress controller (the AWS Load Balancer\nController) that forwards traffic to the service\nAurora Serverless is a powerful database engine and a convenient choice for\nmicroservices. With Aurora Serverless, you only pay for what you use, and you\ndon’t need to worry about scaling the database, thereby reducing your costs and\nthe time you spend managing it.\nTo securely feed sensitive configuration details to your applications in Kuberne-\ntes, we use Kubernetes secrets. With EKS, we have two strategies for managing\nKubernetes secrets securely: \n– Using the AWS Secrets & Configuration Provider for Kubernetes\n– Using Kubernetes secrets in combination with the AWS Key Managed Service",
      "content_length": 1336,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": null,
      "content": "376\nappendix A\nTypes of web APIs\nand protocols\nIn this appendix, we study the API protocols we can use to implement application\ninterfaces. Each of these protocols evolved to address specific problems in the inte-\ngration between API consumers and producers. We discuss the benefits and the\nlimitations of each protocol so that we can make the best choice when designing\nand building our own APIs. We will discuss the following protocols:\nRPC and its variants, JSON-RPC and XML-RPC\nSOAP\ngRPC\nREST\nGraphQL\nChoosing the right type of API is fundamental for the performance and integration\nstrategy of our microservices. The factors that will condition our choice of API pro-\ntocol include these:\nWhether the API is public or private\nType of API consumer: small devices, mobile applications, browsers, or other\nmicroservices\nThe capabilities and resources we wish to expose; for example, whether it is a\nhierarchical data model that can be organized around endpoints or a highly\ninterconnected net of resources with cross-references among them\nWe take these factors into consideration when discussing the benefits and con-\nstraints of each protocol in the following sections to assess their suitability for dif-\nferent scenarios.",
      "content_length": 1234,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": null,
      "content": "377\nA.1\nThe dawn of APIs: RPC, XML-RPC, and JSON-RPC\nA.1\nThe dawn of APIs: RPC, XML-RPC, and JSON-RPC\nLet’s begin by explaining a remote procedure call and its two most common imple-\nmentations, namely, XML-RPC and JSON-RPC. As you can see in figure A.1, a remote\nprocedure call (RPC) is a protocol that allows a client to invoke a procedure or subrou-\ntine in a different machine. The origins of this form of communication go back to the\n1980s, with the emergence of distributed computing systems, and over time it has\nevolved into standard implementations.1 Two popular implementations are XML-RPC\nand JSON-RPC.\nXML-RPC is an RPC protocol that uses Extensible Markup Language (XML) over\nHTTP to exchange data between a client and a server. It was created by Dave Winer\nin 1998, and it eventually grew into what later came to be known as SOAP (see sec-\ntion A.2).\n With the increasing popularity of JavaScript Object Notation (JSON) as a data seri-\nalization format, an alternative implementation of RPC came in the form of JSON-\nRPC. It was introduced in 2005 and offers a simplified way for exchanging data\nbetween an API client and the server. As you can see in figure A.2, JSON-RPC payloads\nusually include three properties:\n\nmethod—The method or function that the client wishes to invoke in the remote\nserver\n\nparams—The parameters that must be passed to the method or function on\ninvocation\n\nid—A value to identify the request\nIn turn, JSON-RPC response payloads include the following parameters:\n\nresult—The value returned by the invoked method or function\n\nerror—An error code raised during the invocation, if any\n\nid—The ID of the request which is being handled\nRPC is a lightweight protocol that allows you to drive API integrations without having\nto implement complex interfaces. An RPC client only needs to know the name of the\n1 Bruce Jay Nelson is credited with the introduction of the term remote procedure call in his doctoral dissertation\n(Technical Report CSL-81-9, Xero Palo Alto Research Center, Palo Alto CA, 1981). For a more formal descrip-\ntion of the implementation requirements of RPC, see Andrew B. Birrell and Bruce Jay Nelson, “Implementing\nRemote Procedure Calls,” ACM Transactions on Computer Systems, vol. 2, no. 1, 1984, pp. 39–59. \nAPI client\nRPC server\nsubroutine\ngetProductList()\nInvoke\nFigure A.1\nUsing RPC, a program \ninvokes a function or subroutine \nfrom the API server.",
      "content_length": 2417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": null,
      "content": "378\nAPPENDIX A\nTypes of web APIs and protocols\nfunction it needs to invoke in the remote server, with its signature. It doesn’t need to\nlook for different endpoints and comply with their schemas as in REST. However, the\nlack of a proper interface layer between the API consumer and the producer inevita-\nbly tends to create tight coupling between the client and the implementation details\nof the server. As a consequence, a small change in implementation details risks break-\ning the integration. For this reason, RPC is recommended mostly for internal API\nintegrations, where you’re in full control of both the client and the server.\nA.2\nSOAP and the emergence of API standards\nThis section discusses the Simple Object Access Protocol (SOAP). SOAP enables com-\nmunication with web services through the exchange of XML payloads. It was intro-\nduced in 1998 by Dave Winer, Don Box, Bob Atkisnon, and Mohsen Al-Ghosein for\nMicrosoft, and after a number of iterations, it became a standard protocol for web\napplications in 2003. SOAP was conceived as a messaging protocol, and it runs on top\nof a data transport layer, such as HTTP.\n SOAP was designed to meet three major goals:\nExtensibility—SOAP can be extended with capabilities found in other messaging\nsystems.\nNeutrality—It can operate over any data transfer protocol of choice, including\nHTTP, or directly over TCP or UDP, among others.\nIndependence—It enables communication between web applications regardless\nof their programming models.\n{\n\"method\": \"calculate_price\",\n\"params\": {\n\"product\": \"cappuccino\",\n\"size\": \"medium\"\n}\n}\n{\n\"result\": 10.7,\n\"id\": 1\n}\nAPI client\nAPI server\nRequest\nResponse\n\"id\": 1,\nFigure A.2\nUsing JSON-RPC, an API client sends a request to an API server invoking the \ncalculate_price() function to get the price of a medium cup of cappuccino. The server \nresponds with the result of the invocation: $10.70.",
      "content_length": 1888,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": null,
      "content": "379\nA.2\nSOAP and the emergence of API standards\nThe payloads exchanged with a SOAP endpoint are represented in XML, and as illus-\ntrated in figure A.3, they include the following properties:\n\nEnvelope (required)—Identifies the XML document as a SOAP payload\n\nHeader (optional)—Includes additional information about the data contained in\nthe message, for example, the type of encoding\n\nBody (required)—Contains the payload (actual message being exchanged) of the\nrequest/response\n\nFault (optional)—Contains errors that occurred while processing the request\nSOAP was a major contribution to the field of APIs. The availability of a standard pro-\ntocol for communication across web applications led to the emergence of vendor\nAPIs. Suddenly, it was possible to sell digital services by simply exposing an API that\neverybody could understand and consume.\n In recent years, SOAP has been superseded by newer protocols and architectures.\nThe factors that contributed to the decline of SOAP include these:\nThe payloads exchanged through SOAP contain large XML documents, which\nconsume a large amount of bandwidth.\nBody\n(required)\nHeader\n(optional)\nEnvelope\n(required)\nFault\n(optional)\nIdentiﬁes the message as a SOAP payload\nIndicates why the\nrequest failed\nPayload instructing the server what to do,\nfor example:\n<SOAP-ENV:Body xsi:type=\"NorthwindBody\">\n<PlaceOrder>\n<product>cappuccino</product>\n<size>big</size>\n<quantity>2</quantity>\n</PlaceOrder>\n</SOAP-ENV:Body>\nIncludes additional\ninformation, such\nas the request ID\nFigure A.3\nAt the top of a SOAP message, we find a section called Envelope that tells us that this is a \nSOAP payload. An optional Header section includes metadata about the message, such as the type of \nencoding. The Body section includes the actual payload of the message: the data being exchanged between \nthe client and the server. Finally, a section called Fault includes details of any errors raised while \nprocessing the payload.",
      "content_length": 1959,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": null,
      "content": "380\nAPPENDIX A\nTypes of web APIs and protocols\nXML is difficult to read and maintain, and it requires careful parsing, which\nmakes exchanging messages structured in XML less convenient.\nSOAP does not provide a clear framework for organizing the data and capabili-\nties that we want to expose through an API. It provides a way of exchanging\nmessages, and it is up to the agents involved on both sides of the API to decide\nhow to make sense of such messages.\nA.3\nRPC strikes again: Fast exchanges over gRPC\nThis section discusses a specific implementation of the RPC protocol called gRPC,2\nwhich was developed by Google in 2015. This protocol uses HTTP/2 as a transport\nlayer and exchanges payloads encoded with Protocol Buffers (Protobuf)—a method\nfor serializing structured data. As we explained in chapter 2, serialization is the pro-\ncess of translating data into a format that can be stored or transferred over a network.\nAnother process must be able to pick up the saved data and restore it to its original\nformat. The process of restoring serialized data is also known as unmarshalling.\n Some serialization methods are language specific, such as pickle for Python. Some\nothers, like the popular JavaScript Object Notation (JSON) format, are language agnos-\ntic and can be translated into the native data structures of other languages.\n An obvious shortcoming of JSON is that it only allows for the serialization of sim-\nple data representations consisting of strings, Booleans, arrays, associative arrays, and\nnull values. Because JSON is language agnostic and must be strictly transferable\nacross languages and environments, it cannot allow for the serialization of language-\nspecific features, like NaN (not a number) in JavaScript, tuples or sets in Python, or\nclasses in object-oriented languages.\n Python’s pickle format allows you to serialize any type of data structure running in\nyour Python programs, including custom objects. The shortcoming, though, is that\nthe serialized data is highly specific to the version of Python that you were running at\nthe time of dumping the data. Due to slight changes in the internal implementation\nof Python between different releases, you cannot expect a different process to be able\nto reliably parse a pickled file.\n Protobuf comes somewhere in between: it allows you to define more complex\ndata structures than JSON, including enumerations, and it is able to generate native\nclasses from the serialized data, which you can extend to add custom functionality.\nAs you can see in figure A.4, in gRPC you must first define the schema for the data\nstructures that you want to exchange over the API using the Protobuf specification\nformat, and then use the Protubuf CLI to automatically generate code for both the\nclient and the API server.\n2 You’re surely wondering what the “g” in gRPC stands for. According to the official documentation, it stands\nfor a different word in every release. For example, in version 1.1 it stands for “good,” while in version 1.2 it\nstands for “green,” and so on (https://grpc.github.io/grpc/core/md_doc_g_stands_for.html). Some people\nbelieve that the “g” stands for Google, as this protocol was invented by Google (see “Is gRPC the Future of Client-\nServer Communication?” by Bleeding Edge Press, Medium, July 19, 2018, https://medium.com/@EdgePress/\nis-grpc-the-future-of-client-server-communication-b112acf9f365).",
      "content_length": 3396,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": null,
      "content": "381\nA.3\nRPC strikes again: Fast exchanges over gRPC\nThe data structures generated from the Protobuf specifications are called stubs. The\nstubs are implemented in code native to the language we use to build the API client\nand the server. As you can see in figure A.5, the stubs take care of parsing and validat-\ning the data exchanged between client and server.\ngRPC offers a more reliable approach for API integrations than plain RPC. The use of\nProtobuf serves as an enforcement mechanism that ensures the data exchanged\nbetween the client and the server comes in the expected format. It also helps to make\nsure that communication over the API is highly optimized, since the data is exchanged\ndirectly in binary format. For this reason, gRPC is an ideal candidate for the implemen-\ntation of internal API integrations where performance is a relevant factor.3\n3 According to Postman’s 2022 State of the API Report, 11% of the surveyed developers use gRPC (https://\nwww.postman.com/state-of-api/api-technologies/#api-technologies).\nProtobuf\ndeﬁnitions\nprotoc compiler (CLI)\nPython stubs for server API\nJavaScript stubs for user-facing\nUI client\nPython stubs for client-side\ncomponents\nFigure A.4\ngRPC uses Protobuf to encode the data exchanged through the API. Using the \nprotoc CLI, we can generate code (stubs) for both the client and the server from a Protobuf \nspecification.\nAPI client\ngRPC stubs\nAPI server\ngRPC stubs\nJavaScript classes\nPython classes\n{payloads}\nFigure A.5\nThe stubs \ngenerated with Protobuf \ntake care of parsing the \npayloads exchanged \nbetween the API client \nand the API server and \ntranslating them into \nnative code.",
      "content_length": 1644,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": null,
      "content": "382\nAPPENDIX A\nTypes of web APIs and protocols\nA.4\nHTTP-native APIs with REST\nThis section explains Representational State Transfer (REST) and its main features.\nREST is an architectural style for the design of web services and their interfaces. As we\nsaw in chapter 4, REST APIs are structured around resources. We distinguish two types\nof resources, collections and singletons, and we use different URL paths to repre-\nsent them. For example, in figure A.6, /orders represents a collection of orders,\nwhile /orders/{order_id} represents the URI of a single order. We use /orders to\nretrieve a list of orders and to place new orders, and we use /orders/{order_id} to per-\nform actions on a single order.\n Good REST API design leverages features from the HTTP protocol to deliver\nhighly expressive APIs. For example, as you can see in figure A.7, we use HTTP meth-\nods to define API endpoints and express their intent (POST to create resources and\nGET to retrieve resources); we use HTTP status codes to signal the result of process-\ning a request; and we use HTTP payloads to carry exchange data between the client\nand the server.\n We document REST APIs using the OpenAPI standard, which was originally cre-\nated in 2010 by Tony Tam under the name Swagger API. The project gained in popu-\nlarity, and in 2015 the OpenAPI Initiative was launched to maintain the specification.\nIn 2016, the specification was officially released under the name OpenAPI Specifica-\ntion (OAS).\n The data exchanged through a REST API goes in the body of an HTTP request/\nresponse. This data can be encoded in any type of format the producer of the API\nwishes to enforce, but it is common practice to use JSON.\n Thanks to the possibility of creating API documentation with a high level of detail\nin a standard specification format, REST is an ideal candidate for enterprise API inte-\ngrations and for building public APIs with a large and diverse range of consumers.\nA.5\nGranular queries with GraphQL\nThis section explains GraphQL and how it compares to REST. GraphQL is a query\nlanguage based on graphs and nodes. As of the time of this writing, it is one of the\nmost popular choices for the implementation of web APIs.4 It was developed by Face-\nbook in 2012 and publicly released in 2015.\n GraphQL is designed to address some of the limitations of REST APIs, such as the\ndifficulty of representing certain operations through HTTP endpoints. For example,\nlet’s say you ordered a cup of coffee through the CoffeeMesh website, and later you\nchange your mind and decide to cancel the order. Which HTTP method is most\nappropriate to represent this action? You can argue that cancelling an order is akin to\ndeleting it, so you could use the DELETE method. But, is cancelling really the same as\ndeleting? Are you going to delete the order from your records after cancellation?\n4 According to Postman’s 2022 State of the API Report, 28% of the surveyed developers use GraphQL (https://\nwww.postman.com/state-of-api/api-technologies/#api-technologies).",
      "content_length": 3024,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": null,
      "content": "383\nA.5\nGranular queries with GraphQL\nAPI client\nGET /orders/8\nPOST /orders\n{payload}\nPUT /orders/8\n{payload}\nPATCH /orders/8\n{payload}\nDELETE /orders/8\nGET retrieves a speciﬁc resource.\nPOST creates a resource.\nPUT replaces a resource.\nPATCH updates details of a resource.\nDELETE deletes a resource.\nstatus code: 200\n{payload}\nstatus code: 200\n{payload}\nstatus code: 200\n{payload}\nstatus code: 201\n{payload}\nstatus code: 204\nHTTP requests\nHTTP responses\nGET /orders\nGET retrieves a collection of resources.\nstatus code: 200\n{payload}\nOrders service\nFigure A.6\nREST APIs are structured around endpoints. We distinguish between singleton endpoints, such \nas GET /orders/8, and collection endpoints, such as GET /orders. Leveraging the semantics of HTTP \nmethods, REST API responses include HTTP status codes that signal the result of processing the request.",
      "content_length": 856,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": null,
      "content": "384\nAPPENDIX A\nTypes of web APIs and protocols\nProbably not. You could argue that it should be a PUT or a PATCH request since you\nare changing the state of the order to cancelled. Or you could say it should be a\nPOST request since the user is triggering an operation that involves more than simply\nupdating a record. However you look at it, HTTP does present some limitations when\nit comes to modeling user actions, and GraphQL gets around this problem by not\nconstraining itself to using elements of the HTTP protocol exclusively.\n Another limitation of REST is the inability for clients to make granular requests\nof data, technically known as overfetching. For example, imagine that an API exposes\n/products and /ingredients resources. As you can see in figure A.7, with /products\nwe can get a list of products, including the IDs of their ingredients. However, if we\nwant to get the name of each ingredient, we must request the details of each ingredi-\nent to the /ingredients API. The result is the API client needs to send various\nrequests to the API to obtain a simple representation of a product. The API client also\nProducts API\n/products\n/ingredients\n1. Retrieve details of product Mocha.\nProduct\n2. Request the details of\neach ingredient.\nIngredient\nID: 1\nname: Milk\nstock: 10\nsupplier: Arlington Milk\nIngredient\nID: 3\nname: Chocolate\nstock: 30\nsupplier: Fourier Chocolates\nIngredient\nID: 2\nname: Coﬀee\nstock: 20\nsupplier: Nested Coﬀee\nIngredient\nID: 4\nname: Sugar\nstock: 80\nsupplier: Sudoku Sugar\nID: 1\nname: Mocha\nprice: $10\ningredients: [\n1,\n2,\n3,\n4\n]\nFigure A.7\nA limitation of REST APIs is the inability of API clients to make granular requests of data, \notherwise known as overfetching. In the figure, the /products endpoint returns a list of products with \nthe IDs of their ingredients. To obtain the ingredients’ names, the client must request the details of each \ningredient from the /ingredients endpoint. As a result, the API client ends up making too many requests \nto the server and receiving more data than it needs.",
      "content_length": 2039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": null,
      "content": "385\nA.5\nGranular queries with GraphQL\nreceives more information than it needs: in each request against the /ingredients\nAPI, the client receives a full description of each ingredient, when it only needs the\nname. Overfetching is a challenge for small devices such as mobile phones, which may\nnot be able to handle and store large amounts of data and may have more limited net-\nwork access.\n GraphQL avoids these problems by allowing clients to make granular queries on\nthe server. With GraphQL, we can create relationships between different data models,\nallowing API clients to fetch data from related entities. For example, in figure A.8,\nan API client can request a list of products and the names of their ingredients in a sin-\ngle request. By allowing clients to retrieve the data they need from the server in a sin-\ngle request, GraphQL is an ideal candidate for APIs, which are consumed by clients\nwith limited network access or limited storage capabilities, such as mobile devices.\nGraphQL is also a good choice for APIs with highly interconnected resources, in\nwhich users are likely to fetch data from related entities, such as products and ingredi-\nents in figure A.8.\nDespite its benefits, GraphQL also comes with constraints. A major limitation of\nGraphQL is that it doesn’t provide great support for custom scalar types. GraphQL\nships with a basic set of built-in scalars, such as integer (Int) and string (String).\nGraphQL allows you to declare your own custom scalars, but you can’t document their\nshape or how they’re validated using the SDL. In the words of GraphQL’s official docu-\nmentation, “It’s up to our implementation to define how that type should be serialized,\ndeserialized, and validated” (https://graphql.org/learn/schema/). Since one of the\ncornerstones of robust API integrations is great documentation, GraphQL is a chal-\nlenging choice for public APIs that must be reliably consumed by external clients.\n Another limitation of GraphQL is that all the queries are typically done with POST\nrequests, which makes it more difficult to cache the responses. In my experience,\nmost developers also find it more difficult to interact with a GraphQL API. In fact,\nPostman’s 2022 State of the API Report found that only 28% of the surveyed develop-\ners use GraphQL, and up to 14% of them hadn’t heard of it. While interacting with a\nClient\nproducts {\nname,\nprice,\ningredients: {\nname\n}\n}\nRequest\nGraphQL server\nFigure A.8\nUsing a GraphQL API, we can query data from related entities, such as \nproducts and ingredients. In this figure, an API client requests a list of products with \nthe names of their ingredients.",
      "content_length": 2636,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": null,
      "content": "386\nAPPENDIX A\nTypes of web APIs and protocols\nREST API may be as simple as hitting a GET endpoint, with GraphQL you must know\nhow to build query documents and how to send them to the server. Since developers\nare less familiar with GraphQL, choosing this technology may make your APIs less\nlikely to be consumed.",
      "content_length": 312,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": null,
      "content": "387\nappendix B\nManaging\nan API’s life cycle\nAPIs are very rarely static. As your product evolves, you need to expose new capabil-\nities and features through your API, and this means that you will need to create new\nendpoints or change your schemas to introduce new entities or fields. Often, API\nchanges are backward incompatible, which means clients who are unaware of the\nnew changes will get failed responses to their requests. Part of managing an API is\nmaking sure that any changes you make don’t break the integrations that already\nexist with other applications, and API versioning serves that purpose. In this appen-\ndix, we study API versioning strategies to manage API changes.\n In addition to evolving and changing, APIs also sometimes come to an end. Per-\nhaps you’re migrating a REST API to GraphQL, or you’re ceasing a product alto-\ngether. If you’re planning to deprecate an API, you must let your clients know when\nand how it’ll happen, and in the second part of this appendix, you’ll learn to\nbroadcast this information to your users.\nB.1\nVersioning strategies for evolving APIs\nLet’s see how we use versioning to manage API changes. We use two major types of\nversioning systems for APIs:\nSemantic versioning (SemVer, https://semver.org/)—This is the most common\ntype of versioning, and it is widely used to manage software releases. It has\nthe following format: MAJOR.MINOR.PATCH, for example, 1.1.0. The first\nnumber indicates the major version of the release, the second number indi-\ncates the minor version, and the third number indicates the patch version.\nThe major version changes whenever you make a breaking change to the\nAPI, for example, when a new field is required in a request payload. Minor",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": null,
      "content": "388\nAPPENDIX B\nManaging an API’s life cycle\nversions represent nonbreaking changes to the API, such as the introduction of\na new optional query parameter. Your API consumers expect to be able to keep\ncalling your endpoints in the same way on different minor versions and con-\ntinue to obtain responses. Patch versions indicate bug fixes.\nIn the context of APIs, we typically only use the major version, so we may\nhave v1 and v2 of an API. Minor changes and patches that improve the API can\ngenerally be rolled out without the risk of breaking existing integrations.\nCalendar versioning (CalVer, https://calver.org/)—Calendar versioning uses\ncalendar dates to version releases. This system is useful when your APIs change\nvery often, or when your releases are time sensitive. An increasing number of\nsoftware products use calendar versioning, including Ubuntu (https://ubuntu\n.com/). AWS also uses calendar versioning in some of its products, such as Cloud-\nFormation (http://mng.bz/epQZ) and the S3 API (http://mng.bz/p6B0).\nCalVer does not provide a full specification about how to format your versions;\nit only emphasizes the use of dates. Some projects use the format YYYY.MM.DD,\nwhile others use YY.MM. If you make several releases per day, you can use an\nadditional counter to keep track of each release, for example, 2022.12.01.3,\nwhich means this is the third release made on the 12th of December in 2022.\n(For more details on calendar versioning, see http://mng.bz/O6MO.)\nWhich type of versioning system is better? It depends on your specific needs and your\noverall API management strategy. SemVer is more commonly used since it’s more\nintuitive. However, if your product rollouts are time sensitive, CalVer is a better fit.\nYour choice of versioning system will also be affected by your versioning manage-\nment strategy, so let’s take a look at the different methods we use to indicate the ver-\nsion of our APIs:\nVersioning using the URL—You can embed the API version in the URL, for exam-\nple, https://coffeemesh.com/api/v1/coffee. This is very convenient because\nconsumers of your API know that they will always be able to call the same end-\npoint and get the same results. If you release a new version of your API, that ver-\nsion will go into a different URL path (/api/v2) and therefore will not conflict\nwith your previous releases. It also makes your API easier to explore, since, to\ndiscover and test different versions of it, API consumers only need to change\nthe version field in the URL. On the downside, when working with REST APIs,\nusing the URL to manage versions is considered a violation of the principles of\nREST since every resource should be represented by one and only one URI.\nVersioning using the Accept Header field—An API consumer uses the Accept\nHTTP request Header field to advertise the type of content they can parse. In\nthe context of APIs, the typical value of the Accept Header is application/\njson, which means the client only accepts data in JSON format. Since the API\nversion also influences the type of content we receive from the server, we can\nuse the Header field to advertise which API version we want to use. An example",
      "content_length": 3170,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": null,
      "content": "389\nB.2\nManaging the life cycle of your APIs\nof a Header field that specifies the content type and API version is Accept:\napplication/json;v1.\nThis approach is more harmonious with the principles of REST since it does\nnot modify the resource endpoints, but it requires careful parsing. Introducing\nadditional characters in the header’s value, as in the following snippet, can\ncause errors at runtime:\nAccept: application/json; v1  # note the additional space after the \n➥ semicolon\nSince we’re using the Accept header, we respond with a 415 (Unsupported\nMedia Type) to any errors in the API version declaration, or when the client\nrequests an unavailable version of the API.\nVersioning using custom Request Header fields—In this approach, you use a cus-\ntom Request Header field such as Accept-version to specify the version of the\nAPI you want to use. This approach is the least preferred, since some frame-\nworks may not accept nonstandard Header fields, thus leading to integration\nissues with your clients.\nEach versioning strategy comes with its own benefits and challenges. URL versioning\nis the most adopted strategy since it’s intuitive and easy to use. However, indicating the\nAPI version in the URL also means that our resource URIs change depending on the\nversion of the API, which may be confusing for some clients.\n Using the Accept header is another popular option, but it couples the logic for\nhandling our media types with the logic for handling our API versions. Also, using the\nsame error status code for both media types and API versions may be confusing for\nour API clients. The best strategy is to carefully consider the needs of our application\nand to agree with your API clients on the most preferred solution.\nB.2\nManaging the life cycle of your APIs\nIn this section, we study strategies to gracefully deprecate our APIs. APIs don’t last for-\never; as the products and services that you offer through APIs evolve and change,\nsome of your APIs will become deprecated, and you will eventually retire them. How-\never, you may have external consumers whose systems depend on your APIs, so you\ncannot just take them down without causing disruption to your clients. You must\norchestrate your API deprecation process, and as you’ll see, we use specific HTTP\nheaders to give notice of API deprecation. Let’s see how that works!\n Before you retire an API, you should deprecate it first. A deprecated API is still in\nservice, but it lacks maintenance, enhancements, and fixes. Once you deprecate your\nAPIs, your users won’t expect further changes to them. Deprecation serves as a grace\nperiod for your users to give them time to migrate their systems to a new API without\ndisrupting their operations.\n As soon as you decide to deprecate your API, you should announce it to your API\nconsumers through a standard communication channel, such as by email or in a",
      "content_length": 2873,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": null,
      "content": "390\nAPPENDIX B\nManaging an API’s life cycle\nnewsletter. At the same time, you should set the Deprecation header in your responses.1\nIf the API is going to be deprecated in the future, we set the Deprecation header to\nthe date when the API will be deprecated:\nDeprecation: Friday, 22nd March 2025 23:59:59 GMT\nOnce the API is deprecated, we set the Deprecation header to true:\nDeprecation: true\nYou can also use the Link header to provide additional information about your API\ndeprecation process. For example, you can provide a link to your deprecation policy:\nLink: <https:/ /coffeemesh.com/deprecation>; rel=”deprecation”; \n➥ type=”text/html”\nIn this case, we are telling the user that they can follow the link https://coffeemesh\n.com/deprecation to find additional information about the deprecation of the API.\n If you’re deprecating an old version of your API, you can use the Link header to\nprovide the URL that replaces or supersedes the current API version:\nLink: <https:/ /coffeemesh.com/v2.0.0/coffee>; rel=”successor-version”\nIn addition to broadcasting the deprecation of your APIs, you should also announce\nwhen the API will be retired. We use the Sunset header to signal when the API will\nbe retired:2\nSunset: Friday, 22nd June 2025 23:59:59 GMT\nThe date of the Sunset header must be later or the same as the date given in the\nDeprecation header. Once you’ve retired an API, you must let your API clients know\nthat the old endpoints are no longer available. You may use any combination of 3xx\nand 4xx status codes when a user calls the old API. A good option is the 410 (Gone)\nstatus code. We use the 410 status code to signal that the requested resource no longer\nexists for a known reason. In some circumstances, 301 (Moved Permanently) might be\nuseful. We use the 301 status code to signal that the requested resource has been\nassigned a new URI, and therefore it may be useful when you migrate your API to a\nnew endpoint.\n Proper management of API changes and deprecations is a crucial yet often over-\nlooked ingredient necessary to deliver high-quality and reliable API integrations. By\napplying the recommendations from this appendix, you’ll be able to evolve your APIs\nwith confidence and without breaking integrations with your clients.\n1 Sanjay Dalal and Erik Wilde, “The Deprecation HTTP Header Field,” https://datatracker.ietf.org/doc/html/\ndraft-ietf-httpapi-deprecation-header-02.\n2 Erik Wilde, “The Sunset HTTP Header Field,” RFC 8594, https://tools.ietf.org/html/rfc8594.",
      "content_length": 2499,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": null,
      "content": "391\nappendix C\nAPI authorization\nusing an identity provider\nIn chapter 11, you learned how the Open Authorization (OAuth) and the OpenID\nConnect (OIDC) protocols work. You also learned how to produce, inspect, and\nvalidate JSON Web Tokens (JWTs). Finally, you learned a pattern for adding autho-\nrization middleware to your APIs. The question we still need to answer is, how do\nwe build an end-to-end authentication and authorization system?\n You can use various strategies to handle authentication and authorization. You\ncan build your own authentication service, or you can use an identity-as-a-service\nprovider, such as Auth0, Okta, Azure Active Directory, or AWS Cognito. Unless\nyou’re an expert in web security and authentication protocols and have sufficient\nresources to build the system correctly, I recommend you use an identity service\nprovider. In this appendix, you’ll learn to add authentication to your APIs with\nAuth0, which is one of the most popular identity management systems.\n We’ll use Auth0’s free plan. Auth0 takes care of managing user accounts and\nissuing secure tokens, and it also provides easy integrations for social login with\nidentity providers such as Google, Facebook, Twitter, and others. Auth0’s authenti-\ncation system is built on standards, so everything you learn about authenticating\nwith Auth0 applies to any other provider. If you use a different authentication sys-\ntem in your own projects or at work, you’ll be able to take the lessons from this\nappendix and apply them to whichever other system you use.\n The code for this appendix is available under the appendix_c folder in the\nGitHub repository for this book. I recommend you pull this code to follow along\nwith the examples, in particular, the folder named appendix_c/ui since you’ll need\nit to run the examples in section C.2.",
      "content_length": 1826,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": null,
      "content": "392\nAPPENDIX C\nAPI authorization using an identity provider\nC.1\nUsing an identity as a service provider\nThis section explains how to integrate our code with an identity-as-a-service (IDaaS) pro-\nvider. An IDaaS provider is a service that takes care of handling user authentication and\nissuing access tokens for our users. Using an IDaaS provider is convenient, since it means\nwe can focus our time and efforts on building our APIs. Good IDaaS providers are built\non standards and with strong security protocols, which also reduces the security risks of\nour servers. In this section, you’ll learn how to build an integration with Auth0, which is\none of the most popular IDaaS providers.\n To work with Auth0, first create an account, and then create a tenant following Auth0’s\ndocumentation (https://auth0.com/docs/get-started). As a first step, go to your dash-\nboard and create an API to represent the orders API. Configure it as shown in figure C.1,\nFigure C.1\nTo create a new API, click the Create API button, and fill in the form with the API’s name, \nits URL identifier, and the signing algorithm you want to use for its access tokens.",
      "content_length": 1139,
      "extraction_method": "Direct"
    },
    {
      "page_number": 421,
      "chapter": null,
      "content": "393\nC.1\nUsing an identity as a service provider\ngiving it http://127.0.0.1:8000/orders as the identifier’s value and selecting the RS256\nsigning algorithm.\n Once you’ve created the API, go to Permissions and add a permission scope to the\nAPI, as shown in figure C.2.\nNext, click Settings on the left-side bar, and then click the Custom Domains tab, as\nshown in figure C.3.\nYour APIs page\nthe API using this form.\nAdd permission scopes to\nThis API’s permissions tab\nFigure C.2\nTo add permission scopes to the API, click on the Permissions tab, and fill in the Add a \nPermission (Scope) form.\nTenant’s settings page\nTenant’s default domain\nTenant’s domains page\nFigure C.3\nTo find out your tenant’s default domain, go to the tenant’s settings page and click the \nCustom Domains tab.",
      "content_length": 780,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": null,
      "content": "394\nAPPENDIX C\nAPI authorization using an identity provider\nYou can add a custom domain if you want, or you can use Auth0’s default domain\nfor your tenant. We use this domain to build the well-known URL of our authentica-\ntion service:\nhttps:/ /<tenant>.<region>.auth0.com/.well-known/openid-configuration\nFor example, for CoffeeMesh, the tenant’s domain is https://coffeemesh-dev.eu.auth0\n.com/.well-known/openid-configuration.\n Now make a call to this URL, and capture the jwks_uri property, which represents\nthe URL that returns the public keys we can use to verify Auth0’s tokens. Here’s an\nexample:\n$ curl https:/ /coffeemesh.eu.auth0.com/.well-known/openid-configuration \\\n| jq .jwks_uri\n# output:\n\"https:/ /coffeemesh-dev.eu.auth0.com/.well-known/jwks.json\"\nIf you call this URL, you’ll get an array of objects, each of which contains information\nabout each of your tenant’s public keys. Each object looks like this:\n{\n  \"alg\": \"RS256\",\n  \"kty\": \"RSA\",\n  \"use\": \"sig\",\n  \"n\": \"sV2z9AApyKK-\n➥ Zo9vrzHbonNsHTgYiIOx1dHx3U102fUhPFzUcdnjb7li960iTKyTbFlMRbsN2fFZOHa5_4Q\n➥ 3C7UzjkVw__jK3AcPZ-0cCiLBS-HQzE_6ii-OPo84-\n➥ W9Pp2ScKdAlJIqBimDtNv8vuOEMr5c5YbJz1HlppFY_hA71dgc101SHp0n9GZYqP5HV713m\n➥ 6smE5b7abHLqrUSz9eVbSOrTUOcSd5_LUHvQqFb5Wt7kRalIiHnQFob-\n➥ cyM1AmxDNsX1qR2cX_jqjWCRO2iK5DTG--ure8GQUTCMPZ0LkBKSDelTwHuEn_r4z-\n➥ x30wf-2lA0yzMSlcxcJIojpQ\",\n  \"e\": \"AQAB\",\n  \"kid\": \"ZweIFRR4l1dJlVPHOoZqf\",\n  \"x5t\": \"OJXBmAMkfObrQ9YkfUb4O20l_us\",\n  \"x5c\": [\n    \"MIIDETCCAfmgAwIBAgIJUbXpEMz8nlmXMA0GCSqGSIb3DQEBCwUAMCYxJDAiBgNVBAMTG2NvZm\n➥ ZlZW1lc2gtZGV2LmV1LmF1dGgwLmNvbTAeFw0yMTEwMjkyMjQ4MjBaFw0zNTA3MDgyMjQ4M[\n➥ jBaMCYxJDAiBgNVBAMTG2NvZmZlZW1lc2gtZGV2LmV1LmF1dGgwLmNvbTCCASIwDQYJKoZI\n➥ hvcNAQEBBQADggEPADCCAQoCggEBALFds/QAKciivmaPb68x26JzbB04GIiDsdXR8d1NdNn\n➥ 1ITxc1HHZ42+5YvetIkysk2xZTEW7DdnxWTh2uf+ENwu1M45FcP/4ytwHD2ftHAoiwUvh0M\n➥ xP+oovjj6POPlvT6dknCnQJSSKgYpg7Tb/L7jhDK+XOWGyc9R5aaRWP4QO9XYHNdNUh6dJ/\n➥ RmWKj+R1e9d5urJhOW+2mxy6q1Es/XlW0jq01DnEnefy1B70KhW+Vre5EWpSIh50BaG/nMj\n➥ NQJsQzbF9akdnF/46o1gkTtoiuQ0xvvrq3vBkFEwjD2dC5ASkg3pU8B7hJ/6+M/sd9MH/tp\n➥ QNMszEpXMXCSKI6UCAwEAAaNCMEAwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUWrl+q/\n➥ l4wp/MWDdYrhjxns0iP2wwDgYDVR0PAQH/BAQDAgKEMA0GCSqGSIb3DQEBCwUAA4IBAQA+Y\n➥ H+sxcMlBzEOJ5hJgZw1upRroCgmeQzEh+Cx73sTKw+vi8u70bdkDt9sBLKlGK9xbPJt3+QW\n➥ ZDJF9rwx4vXbfFvxZD+dthIvn4NH4/sLQXG20JN/b6GtHdVllbJIGUeWb8DBsx94wXYMwag\n➥ 0gXUk5spgaGGdoc16uSrrbxt/rmzFk3VMQ8qG5i8E33N/DZb88P4u3WJMNMsmujw9Q8meg4\n➥ ygEFadXBcfJPHuiriLWi0j1Gm+m6DZQM51OtpQ/cvcZXRNPogqj7wsZXH4za9DJjnQf8ZOK\n➥ Q86WKl/9CE5AvHBTTTr810DviJIqv8sqC866+2t2euxcfOYMIw5E42o\"\n  ]\n}",
      "content_length": 2554,
      "extraction_method": "Direct"
    },
    {
      "page_number": 423,
      "chapter": null,
      "content": "395\nC.1\nUsing an identity as a service provider\nThe two most important fields in this payload are kid and x5c. kid is the ID of the key,\nand we use it to match the kid field of the JWT’s header section. It tells us which key\nwe need to use to verify the token’s signature. The x5c field contains an array of pub-\nlic keys in the form of X.509 certificates, the first of which we use to verify the JWT’s\nsignature.\n This is all the information we need to integrate our code with Auth0. We’ll imple-\nment our Auth0 integration in the orders/web/api/auth.py module, which we cre-\nated in chapter 11 (section 11.4.1) to encapsulate our authorization code. Delete the\ncontents of orders/web/api/auth.py, and replace them with the contents of listing\nC.1. We first import the necessary dependencies, create a template for the X.509 cer-\ntificate, and load the public keys from the well-known endpoint. X.509 certificates are\nwrapped between -----BEGIN CERTIFICATE----- and -----END CERTIFICATE-----\nstatements, so our template includes both statements with a template variable named\nkey, which we’ll replace with the actual key.\n Since Auth0 uses several keys to sign the tokens, we load the public keys by calling\nthe JWKS endpoint, and we dynamically load the right key for the given token. As you\ncan see in figure C.4, the kid property in the token’s headers tells us which key we\nneed to use, and our custom function _get_certificate_for_kid() finds the X.509\ncertificate for the token’s kid. To load the key, we use cryptography’s load_pem_x-\n509_certificate() function, passing in the public key formatted into our X.509 byte-\nencoded certificate.\n Since tokens can be signed with different algorithms, we fetch the algorithms\ndirectly from the token’s headers. Auth0 issues tokens that can access both our API\nand the user information API, so we include both services in the audience.\n# file: orders/web/api/auth.py\nimport jwt\nimport requests\nfrom cryptography.x509 import load_pem_x509_certificate\nX509_CERT_TEMPLATE = (\n       \"-----BEGIN CERTIFICATE-----\\n{key}\\n-----END CERTIFICATE-----\"  \n   )\npublic_keys = requests.get(\n    \"https:/ /coffeemesh-dev.eu.auth0.com/.well-known/jwks.json\"\n).json()[\"keys\"]    \ndef _get_certificate_for_kid(kid):      \n    \"\"\"\n    Return the public key whose ID matches the provided kid.\nListing C.1\nAdding an authorization module to the API\nTemplate for a\nX509 certificate\nWe pull the list of signing \nkeys from the tenant’s \nwell-known endpoint.\nFunction that returns \nthe certificate for a \ngiven key ID",
      "content_length": 2544,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": null,
      "content": "396\nAPPENDIX C\nAPI authorization using an identity provider\neyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9...\nhttps://coffeemesh-dev.eu.auth0.com/.well-known/jwks.json\nJWKS endpoint\n{\n“alg”: “RS256”,\n“kty”: “RSA”,\n“use”: “sig”,\n“n”: “sV2z9AApyKK-Zo9vrzHbonNsHTgYiIO...,\n“e”: “AQAB”,\n“kid”: “ZweIFRR4l1dJlVPHOoZqf”,\n“x5t”: “OJXBmAMkfObrQ9YkfUb4O20l_us”,\n“x5c”: [\n“MIIDETCCAfmgAwIBAgIJUb...”\n]\n}\n{\n“alg”: “RS256”,\n“typ”: “JWT”,\n“kid”: “ZweIFRR4l1dJlVPHOoZqf”\n}\nHeaders\n{\n“iss”: “https://auth.coﬀeemesh.io/”,\n“sub”: “ec7bbccf-ca89-4af3-82ac-b41e4831a962”,\n“aud”: “http://127.0.0.1:8000/orders”,\n“iat”: 1667155816,\n“exp”: 1667238616,\n“azp”:\n“7c2773a4-3943-4711-8997-70570d9b099c”,\n“scope”: “openid”\n}\nSIGNATURE\nPayload claims\nJSON Web Token\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\nCER\nTE\nTIFICA\nByte-encoded\ncryptography.load_pem_x509_certiﬁcate()\nPublic Key\njwt.decode()\n1. We fetch the list of\nsigning keys from the\nJWKS endpoint.\n2. We ﬁnd the signing\nkey for the JWT by\ninspecting the kid\nproperty.\n4. We obtain the public key object\nusing cryptography’s\nload_pem_x509_certiﬁcate() function.\n5. We validate the\ntoken using PyJWT’s\ndecode() function.\n3. We produce\na byte-\nencoded\ncertiﬁcate\nfor the\nsigning key.\nFigure C.4\nTo validate a JWT, we verify its signature using its corresponding signing key. The signing key is \navailable in the JWKS endpoint.",
      "content_length": 1358,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": null,
      "content": "397\nC.2\nUsing the PKCE authorization flow\n    If no match is found, an exception is raised.\n    \"\"\"\n    for key in public_keys:\n        if key[\"kid\"] == kid:    \n            return key[\"x5c\"][0]\n    raise Exception(f\"Not matching key found for kid {kid}\")    \ndef load_public_key_from_x509_cert(certificate):    \n    \"\"\"\n    Loads the public signing key into a RSAPublicKey object. To do that,\n    we first need to format the key into a PEM certificate and make sure\n    it's utf-8 encoded. We can then load the key using cryptography's\n    convenient `load_pem_x509_certificate` function.\n    \"\"\"\n    return load_pem_x509_certificate(certificate).public_key()    \ndef decode_and_validate_token(access_token):     \n    \"\"\"\n    Validates an access token. If the token is valid, it returns the token \n    payload.\n    \"\"\"\n    unverified_headers = jwt.get_unverified_header(access_token)   \n    x509_certificate = _get_certificate_for_kid(\n        unverified_headers[\"kid\"]     \n    )\n    public_key = load_public_key_from_x509_cert(   \n        X509_CERT_TEMPLATE.format(key=x509_certificate).encode(\"utf-8\")\n    )\n    return jwt.decode(   \n        access_token,\n        key=public_key,\n        algorithms=unverified_headers[\"alg\"],     \n        audience=[    \n            \"http:/ /127.0.0.1:8000/orders\",\n            \"https:/ /coffeemesh-dev.eu.auth0.com/userinfo\",\n        ],\n    )\nWe’re ready to go! The orders service is now able to validate tokens issued by Auth0.\nThe following sections illustrate how to leverage this integration to make our API\nserver accessible to a single-page application (SPA) and to another microservice.\nC.2\nUsing the PKCE authorization flow\nIn the PKCE flow, the API client requests an ID token and an access token directly\nfrom the authorization server. As we explained in chapter 11, we must use the access\ntoken to interact with the API server. The ID token can be used in the UI to show the\ndetails of the user, but it must never be sent to our API server.\n To illustrate how this flow works, I’ve included an SPA under the appendix_c/ui\ndirectory in the GitHub repository for this book. The SPA is a simple application built\nWe look for the certificate that \nmatches the supplied key ID.\nIf a match \nisn’t found, \nwe raise an \nexception.\nFunction that loads \nthe public key object \nfor a given certificate\nWe load\nthe public\nkey.\nFunction that decodes \nand validates a JWT\nWe fetch\nthe token’s\nheaders\nwithout\nverification.\nWe fetch the certificate \ncorresponding to the \ntoken’s key ID.\nWe load the\ncertificate’s\npublic key\nobject.\nWe validate and \ndecode the token.\nWe verify the token’s signature \nusing the algorithm indicated in \nthe token’s header.\nWe pass the\nlist of expected\naudiences for\nthe token.",
      "content_length": 2742,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": null,
      "content": "398\nAPPENDIX C\nAPI authorization using an identity provider\nwith Vue.js that talks to the orders API, and it’s configured to authenticate with an\nAuth0 server.\n We’ll first configure the application. Go to your Auth0 account, and create a new\napplication. Select Single Page Web Applications, and give it a name, then click Cre-\nate. In the application’s settings page, under the Application URIs section, give the\nvalue of http:/ /localhost:8000 to the Allowed Callback URLs, the Allowed Logout\nURLs, the Allowed Web Origins, and the Allowed Origins (CORS) fields. From the\napplication’s settings, we need two values to configure our application: the domain\nand the client ID. Open the ui/.env.local file, and replace the value for VUE_APP_\nAUTH_CLIENT_ID with the client ID and VUE_APP_AUTH_DOMAIN with the domain from\nyour application’s settings page in Auth0.\n To run the UI, you need an up-to-date version of Node.js and npm, which you can\ndownload from the node.js website (https://nodejs.org/en/). Once you’ve installed\nthese, you need to install yarn with the following command:\n$ npm install -g yarn\nNext, cd into the ui/ folder, and install the dependencies by running the following\ncommand:\n$ yarn\nOnce the application is configured, you can run it by executing the following command:\n$ yarn serve --mode local\nThe application will become available under the http:/ /localhost:8080 address. Make\nsure the orders API is also running, since the Vue.js application talks to it. To run the\norders API, run the following command from the orders folder:\n$ AUTH_ON=True uvicorn orders.web.app:app –-reload\nOnce you register a user through the UI, you’ll be able to see your authorization\ntoken in the UI. You can use this token to call the API directly from the terminal. For\nexample, you can get a list of orders for your user by calling the API with the follow-\ning command:\n$ curl http:/ /localhost:8000/orders \\\n-H 'Authorization: Bearer <ACCESS_TOKEN>'\nThrough the Vue.js application, you can create new orders and display the orders\nplaced by the user by clicking the Show My Orders button.\n The PKCE flow works for users accessing your APIs through the browser. However,\nthis flow isn’t convenient for machine-to-machine communication. To allow more pro-\ngrammatic access to your APIs, you need to support the client credentials flow. In the\nnext section, we explain how to enable that flow!",
      "content_length": 2402,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": null,
      "content": "399\nC.3\nUsing the client credentials flow\nC.3\nUsing the client credentials flow\nThis section explains how to implement the client credentials flow for server-to-server\ncommunication. We use the server-to-server flow when we must authenticate our own\nservices to access other APIs, or when we want to allow programmatic access to our\nAPIs. In the client credentials flow, our services request an access token from the\nauthentication service by providing a shared secret with the client ID and the desired\naudience. We can then use this access token to access the API of the target audience.\n To use this authorization flow, you need to register a server-to-server client with\nyour IDaaS provider. In your Auth0 dashboard’s applications page, click Create Appli-\ncation and select Machine to Machine Applications. Give it a name, and click Create.\nOn the next screen, where you’re asked to select the API you want to authorize this cli-\nent for, select the orders API, and then select the permission we created in chapter 11\n(section 11.6). Once you’ve registered the client, you get a client ID and a client\nsecret, which you can use to obtain access tokens.\n Listing C.2 shows how to implement server-to-server authorization to obtain an\naccess token and make a call to the orders API. The code in listing C.2 is available in\nthe book’s GitHub repository under the machine_to_machine_test.py file. We create a\nfunction to obtain the access token from the authorization server by calling the POST\nhttps:/ /coffeemesh-dev.eu.auth0.com/oauth/token endpoint. In the payload, we\nprovide the client ID and the client secret, and we specify the audience for which we\nwant to generate the access token. We also declare that we want to use the client cre-\ndentials flow under the grant_type property. If the client is correctly authenticated,\nwe get back an access token, which we then use to call the orders API.\n# file: machine_to_machine_test.py\nimport requests\ndef get_access_token():\n    payload = {\n        \"client_id\": \"<client_id>\",\n        \"client_secret\": \"<client_secret>\",\n        \"audience\": \"http:/ /127.0.0.1:8000/orders\",\n        \"grant_type\": \"client_credentials\"\n    }\n    response = requests.post(\n        \"https:/ /coffeemesh-dev.eu.auth0.com/oauth/token\",\n        json=payload,\n        headers={'content-type': \"application/json\"}\n    )\n    return response.json()['access_token']\nListing C.2\nAuthorizing a client for machine-to-machine access to the orders API",
      "content_length": 2472,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": null,
      "content": "400\nAPPENDIX C\nAPI authorization using an identity provider\ndef create_order(token):\n    order_payload = {\n        'order': [{\n            'product': 'cappuccino',\n            'size': 'small',\n            'quantity': 1\n        }]\n    }\n    order = requests.post(\n        'http:/ /127.0.0.1:8000/orders',\n        json=order_payload,\n        headers={\n            \"content-type\": \"application/json\", \n            \"authorization\": f\"Bearer {token}\",\n        }\n    )\n    return order.json()\naccess_token = get_access_token()\nprint(access_token)\norder = create_order(access_token)\nprint(order)\nThat’s all it takes to use the client credentials flow! In the next section, you’ll learn\nto authenticate your requests using a Swagger UI so that you can test your API more\neasily.\nC.4\nAuthorizing requests in the Swagger UI\nOver the course of this book, you’ve learned to test your APIs using a Swagger UI.\nYou can use the Swagger UI to test your API authorization as well, and in this section\nyou’ll learn how. First, cd into appendix_c/orders and up the API server with autho-\nrization on:\n$ AUTH_ON=True uvicorn orders.web.app:app --reload\nYou can now access the Swagger UI on http:/ /localhost:8000/docs/orders. As you can\nsee in figure C.5, if you try any of the endpoints, you’ll get a 401 response since we\nhaven’t authorized our requests.\n To authorize a request, click the Authorize button on the top-right corner of the\nscreen. You’ll get a pop-up menu with the security schemes documented in the API\nspecification: openId (authorization code and PKCE flows), oauth2 (client credentials\nflow), and bearerAuth. The easiest way to test the API’s authorization layer is using the\nbearerAuth security scheme, since it only requires you to feed the authorization token.\nYou can produce a token with the Vue.js application under appendix_c/ui or using the",
      "content_length": 1849,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": null,
      "content": "401\nC.4\nAuthorizing requests in the Swagger UI\nmachine_to_machine_test.py script. For example, if you run the machine_to_machine_\ntest.py script, you’ll get a token and the result of creating an order:\n$ python machine_to_machine_test.py\n# output:\neyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6Ilp3ZUlGUlI0bDFkSmxWUEhPb1px...\n➥ {'order': [{'product': 'latte', 'size': 'small', 'quantity': 1}], 'id': \n➥ '6e420d2e-b213-4d15-bc46-0c680e590154', 'created': '2022-06-\n➥ 07T09:01:47.757223', 'status': 'created'}\nCopy the token, and paste it into the value input field of the bearerAuth’s security\nscheme, as shown in figure C.6, and then click Authorize. If you send a request to the\nGET /orders endpoint now, you’ll get a successful response. While the token is valid\nFigure C.5\nIf we make an unauthorized request with the Swagger UI, we’ll get a 401 \nresponse.",
      "content_length": 856,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": null,
      "content": "402\nAPPENDIX C\nAPI authorization using an identity provider\n(i.e., before it expires), you can try any other endpoint, and your requests will be suc-\ncessfully processed.\n This is all it takes to test your API authorization with a Swagger UI. You just learned\nhow to add a robust authentication and authorization layer by integrating with an\nexternal identity provider, how to test the PKCE and the client credentials flows, and\nhow to test your API authorization implementation with Swagger. You’re all set to go\nand build secure APIs!\nFigure C.6\nTo authorize a request, paste the authorization token into the value input from \nbearerAuth’s form.",
      "content_length": 647,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": null,
      "content": "403\nindex\nSymbols\n/.well-known/openid-configuration endpoint 277\n$ref keyword, JSON Schema/OpenAPI 93–94\nA\naccess tokens, OAuth 277\nACID principles 174\nactive record pattern 155\nACU (Aurora Capacity Units) 364\nadapters, hexagonal architecture 145\nadditionalProperties, JSON schema/OpenAPI 115–116\nadd_middleware() method, FastAPI \nframework 290, 292\nAKS (Azure’s Kubernetes Service) 343\nALB (Application Load Balancer), AWS\ndeploying 351–353\nAlembic 150–152\nbatch operations 295–297\nconfigure() method 296\ninitializing 151\nrun_migrations_online() function 296\nrunning migrations with 155, 339\nsqlalchemy.url 151\ntarget_metadata 152\nalg header, JWT 280\nallOf keyword, JSON Schema/OpenAPI 104\nALTER statements 295\nAPI authorization 293–300\nrestricting user access to their own resources\n297–300\nupdating database to link users and orders 294–297\nAPI Gateway 287\nAPIIntegrationError exception, orders service 169\nAPI layer, integrating with service layer 177–181\nAPI life cycle 390\nmanaging 389–390\nAPIs, defined 8–9\nAPISpec object, apispec library 142\nAPI testing and validation 302–330\ndesigning testing strategy 329–330\nGraphQL API testing 327–329\nwith Schemathesis 327–329\nproperty-based testing 315–322\ndefined 315\ntraditional approach to testing 316–317\nwith hypothesis 318–322\nREST API testing with Dredd 304–315\ncustomizing test suite with hooks 307–315\noverview 304–305\nrunning default test suite 305–307\nusing in testing strategy 315\nREST API testing with Schemathesis 322–327\nrunning default test suite 322–323\nusing - -checks=all 326–327\nusing - -stateful=links 323–327\nAPI validation models (REST)\nimplementing with marshmallow 129–130\nimplementing with pydantic 30–34\nAPI versioning 387–389\napiVersion property, Kubernetes manifest 356\narguments() decorator, flask-smorest 130, 134\nAriadne framework 235–241\nbuilding resolvers for custom scalars 258–262\ncreating entry point for GraphQL server 242–243\nhandling query parameters 252–255\nimplementing field resolvers 262–265\nimplementing mutation resolvers 256–258\nimplementing query resolvers 243–246\nimplementing resolvers for custom scalars 258–262\nimplementing type resolvers 246–252",
      "content_length": 2146,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": null,
      "content": "INDEX\n404\narray type, JSON Schema/OpenAPI 92, 99\nASGI (asynchronous server gateway interface) 24\nasynchronous server gateway interface (ASGI) 24\naud reserved claim, JWT 281–282\nAurora Capacity Units (ACU) 364\nauthorization and authentication 269–301\nadding authorization to server 287–293\ncreating authorization middleware 289–292\ncreating authorization modules 287–289\nauthorizing requests in Swagger UI 400–402\nauthorizing resource access 293–300\nrestricting user access to their own \nresources 297–300\nupdating database to link users and orders\n294–297\nclient credentials flow 399–400\nidentity providers 391–402\nusing identity as service provider 392–397\nJWTs 278–286\ninspecting 284–285\nJWT claims 280–282\nJWT header 279–280\nproducing 282–284\nvalidating 286\nOAuth 271–275\nauthorization code flow 273\nclient credentials flow 275\nPKCE flow 273–275\nrefresh token flow 275\nOIDC 276–278\nPKCE flow 397–398\nauthorization code, OAuth 274\nAuthorization HTTP header 108\nAuthorization server, OAuth 272\nAuthorizeRequestMiddleware class, orders API 289\nAWS Aurora 361–370\ncreating serverless databases 361–364\nrunning database migrations and connecting ser-\nvice to database 367–370\naws ecr get-login-password command 341\nAWS KMS (Key Managed Service) 364\nAWS Load Balancer Controller, installing 353\naws rds create-db-cluster command 365\nazp reserved claim, JWT, OIDC 282\nB\nbase64 encoding 284–285\nbase64 library, Python 285\nBaseConfig class, flask-smorest 124\nBaseHTTPMiddleware class, FastAPI/Starlette 289\nBaseModel class, Pydantic 32\nbastion server 367\nbearerAuth security scheme, OpenAPI 304, 400–401\nBlueprint class (flask-smorest) 125\nBody property, SOAP 379\nBoolean scalar (GraphQL) 194\nboolean type, JSON Schema/OpenAPI 92\nbuild context, Docker Compose 338\nBuilding Microservices (Newman) 8\nbusiness layer, implementing 162–172\nC\ncacheability principle (REST) 65–66\ncalendar versioning (CalVer) 388\ncharts (Helm) 352\nCIDR (Classless Inter-Domain Routing) 349\nclass-based views 122–123, 125–127\nclient credentials flow, OAuth 275, 399–400\nclient-server architecture principle (REST) 64\nCLIs (command-line interfaces) 8\nCloudFormation 346\nClusterIP (Kubernetes service) 357–358\nCMD directive, Docker 335\nCockburn, Alistair 145\ncode challenge, OAuth 274\ncode-on-demand principle (REST) 66\ncode verifier, OAuth 274\nCoffeeMesh 17\nanalyzing business structure of 49–50\napplying strategic analysis to 53–56\ndecomposing microservices 50–52\nkitchen API (REST)\nflask-smorest 122–123\nimplementing endpoints 125–129\nimplementing in-memory list of schedules\n140–142\nimplementing payload validation models\n129–132\ninitializing web application 123–125\noverriding dynamically generated \nspecification 142–143\noverview 120–122\nvalidating data before serializing response\n136–139\nvalidating URL query parameters 133–136\norders API (REST)\nhigh-level architecture 22–23\nimplementing API endpoints 23–28\nimplementing data validation models with \npydantic 30–34\nimplementing in-memory list of orders 41–43\nmarshalling and validating response payloads \nwith pydantic 38–41\noverriding FastAPI’s dynamically generated \nspecification 118–120\noverview 111–112\nspecification 21–22\nURL query parameters for 112–115\nvalidating payloads with unknown fields 115–118\nvalidating request payloads with pydantic 34–38",
      "content_length": 3282,
      "extraction_method": "Direct"
    },
    {
      "page_number": 433,
      "chapter": null,
      "content": "INDEX\n405\nCoffeeMesh (continued)\noverview 46\nproducts API (GraphQL) 241–265\ncombining types through unions and \ninterfaces 200–202\nconstraining property values with \nenumerations 202–203\ncreating entry point for GraphQL server\n242–243\ncustom scalars 194–195\ndefining queries 203, 206\nGraphQL 186–189\nhandling query parameters 252–255\nimplementing field resolvers 262–265\nimplementing mutation resolvers 256–258\nimplementing query resolvers 243–246\nimplementing resolvers for custom scalars\n258–262\nimplementing type resolvers 246–252\nlaying out project structure 241–242\nmutations 206–209\noverview 189–192\ncollections resources, REST 62\nColumn class (SQLAlchemy) 153\ncommit() method (SQLAlchemy) 175–176\nConfig class, Pydantic 116\nconfigure() method (Alembic) 296\nconint type, Pydantic 33\nconlist type, Pydantic 33\ncontainer registry, publishing Docker builds \nto 340–341\ncontainers, Docker 331\ncontent property, OpenAPI 99\ncontext managers 174–175\nCOPY directive, Docker 335\ncore domain, domain-driven design 53\nCore Kubernetes (Vyas and Love) 345\nCORS (cross-origin resource sharing)\nadding CORS middleware 292–293\nCORS (Cross-Origin Resource Sharing) \nmiddleware 292–293\ncreate_engine() function (SQLAlchemy) 175\ncryptography library 271, 283, 286, 288, 395–397\ncustom claims, JWT 281\nD\nDaemonSet, Kubernetes workload 345\ndatabase-data volume, Docker Compose 338\ndatabase models, implementing 149–155\ndatabase-per-service principle 46–48\ndatabase subnet group, AWS 361\ndata encryption key (DEK) 364\ndata mapper pattern 150\nDatetime custom scalar (GraphQL) 194, 258–260\ndatetime.fromisoformat() method 260\ndb-access security group 362\ndb-credentials secret 366, 369\nDDD (domain-driven design) 52–53, 166\ndeclarative_base() function (SQLAlchemy) 153\ndecode() function (PyJWT) 287\ndecorator pattern 26\ndefault namespace, Kubernetes 347\nDEK (data encryption key) 364\ndependency injection pattern 165\ndependency inversion pattern 146\nDeployment workload, Kubernetes 345\ndeployment objects, creating 354–356\nDeprecation HTTP header 389–390\ndeserialization 259\nDias, Nuwan 333\ndirected connections (GraphQL) 197\ndiscoverability (HATEOAS, REST) 72–73\ndiscovery endpoint (OIDC) 277\ndispatch() method (Starlette middleware) 289\ndistributed monolith 11\ndistributed transaction tracing 13–14\nDocker 331–341\nbuilding image 333–336\nDocker volume 336–339\npublishing Docker builds to container \nregistry 340–341\nrunning applications with Docker Compose\n338–340\nrunning containers 336–338\nDocker Compose, running applications with\n338–340\nDocker in Action (Kuenzli) 338\ndocker stop command 337\ndocumentation-driven development 15–17\ndomain-driven design (DDD) 52–53, 166\nDomain-Driven Design (Evans) 52\ndomain objects 166\nDredd\noverview 304–305\nREST API testing with 304–315\ncustomizing test suite with hooks\n307–315\ninstalling and running default test suite\n305–307\nusing in testing strategy 315\nusing hooks to make Dredd use custom \nURLs 311\nDredd hooks 307–315\ncreating resources before test 312\ngenerating error responses 313–314\nsaving ID of created resource 309–311\nusing custom URLs 311\ndredd-hooks library 310\ndredd_hooks.after() decorator 310\ndredd_hooks.before() decorator 310\ndumb pipes 10",
      "content_length": 3186,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": null,
      "content": "INDEX\n406\nE\nECR (Elastic Container Registry) 340–341\nedge properties (GraphQL) 196\neksctl CLI 346–349\nEKS (Elastic Kubernetes Service), AWS 346–349\nElastic Container Registry (ECR), AWS 340–341\nencode() function, PyJWT 282–283\n__enter__() method, context manager class 174–176\nEnumType Ariadne 240\nenum type (GraphQL) 202–203\nenvelope encryption 364–365\nEnvelope property, SOAP 379\nenvFrom property, Kubernetes manifest 369\nenvironment keyword, Docker Compose 338\nError generic model, OpenAPI 105, 107\nEvans, Eric 52\n__exit__() method, context manager class 174–175\nEXPOSE directive, Docker 335\nexp reserved claim, JWT 281–283\nExternalName, Kubernates service 358\nF\nfacade pattern 167\nFargate, AWS 347\nFargate profile 348\nFastAPI\nimplementing API endpoints 23–28\nimplementing data validation models with \npydantic 30–34\nmarshalling and validating response payloads \nwith pydantic 38–41\noverriding dynamically generated \nspecification 118–120\nvalidating payloads with unknown fields 115–118\nvalidating request payloads with pydantic 34–38\nvalidating URL query parameters 112–115\nFault property, SOAP 379\nfield() decorator (Ariadne) 240, 256\nfield resolvers, implementing with Ariadne 262–265\nfilter_by() method (SQLAlchemy) 160\nfirst() method (SQLAlchemy) 160\nfixed_dictionaries(), hypothesis 319–320\nFlask application object 124\nflask-smorest\nimplementing endpoints 125–129\nimplementing payload validation models 129–132\ninitializing web application 123–125\noverriding dynamically generated specification\n142–143\noverview 122–123\nvalidating response payload 136–139\nvalidating URL query parameters 133–136\nFloat scalar (GraphQL) 192, 194\nFowler, Martin 4, 8, 10\nFROM directive, Docker 335\nG\ngenerate_jwt() function 283\nGET requests, response payloads for (REST) 86\ngiven() decorator, hypothesis 321\ngrant_type property, OAuth 399\nGraphQL API design 185–209\narrays 195–196\nconnecting types 196–200\nwith edge properties 196–198\nwith through types 198–200\ndefined 186–189\ndesigning and documenting queries 203–206\nenumerations 202–203\nmutations 206–209\noverview 186–189\nproducts API 189–192\nSDL (Schema Definition Language) 15, 186\ntype system 192–195\ncustom scalars 194–195\nmodeling resources 193–194\nscalars 192–193\nunions and interfaces 200–202\nGraphQL API implementation 233–266\nanalyzing requirements 234\nAriadne framework 235–241\nproducts API 241–265\nbuilding resolvers for custom scalars 258–262\ncreating entry point for GraphQL server\n242–243\nhandling query parameters 252–255\nimplementing field resolvers 262–265\nimplementing mutation resolvers 256–258\nimplementing query resolvers 243–246\nimplementing type resolvers 246–252\nlaying out project structure 241–242\nGraphQL API querying 210–232\ncalling GraphQL API with Python code 230–231\ngranular queries 382–386\nmutations\nrunning 225–226\nrunning parametrized 226–229\nnavigating the API graph 219–221\nqueries 214–217\naliasing 222–225\nover HTTP 229–230\nrunning multiple 221\nrunning parametrized 226–229\nrunning simple 214–215\nrunning with input parameters 219\nrunning with parameters 215\nunderstanding errors 215–217\nusing fragments in 217–219\nrunning mock server 211–214\nGraphQL API testing 327–329\nGraphQL class (Ariadne framework) 237, 242–243\ngraphql-faker (GraphQL mocking tool) 211–213",
      "content_length": 3245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": null,
      "content": "INDEX\n407\nGraphQL type resolvers (Ariadne library) 246–252\ngRPC 380–381\nH\nHATEOAS (Hypermedia as Engine of Application \nState) 67–70\nheader, JWT 179–180\nheader (HTTP) 105–106\nHeader property, SOAP 379\nhexagonal architecture 145–148\nHMAC encryption 280\nHMAC-SHA256 encryption 280\nHS256 encryption 280\nHTTP (Hypertext Transfer Protocol) 9\nHTTP methods 72–76\nHTTP-native APIs with REST 382\nHTTP payloads 83–86\ndefinition 83–84\ndesign patterns 84–86\nresponse payloads for GET requests 86\nresponse payloads for POST requests 84\nresponse payloads for PUT and PATCH \nrequests 85\nHTTP status codes 72, 77–83\ndefined 77–78\nreporting client errors with 78–81\nreporting errors in server with 82–83\nHypermedia as Engine of Application State \n(HATEOAS) 67–70\nhypothesis library 315–316, 318, 322–323\n- -hypothesis-database flag 323\n- -hypothesis-deadline option 328\n- -hypothesis-suppress-health-check flag 327\nproperty-based testing with 318–319\ntesting endpoints 319–322\nI\nIAM roles, AWS 350–351\niat property, JWT 281–283\nidentity, OIDC 391–402\nID tokens (OIDC) 277\nID (Unique Identifiers) scalar, GraphQL 192–194\ninfo parameter (Ariadne resolvers) 239–240, 256\ninfrastructure overhead 14–15\ningress controller, Kubernetes 351\ningress objects, Kubernetes 359–361\ningress resources 351\ninline fragment (GraphQL) 218\ninput parameters (GraphQL) 225, 252, 256\nInteger field class, marshmallow library 129\nintegers() strategy, hypothesis 318\nInterfaceType (Ariadne framework) 240\ninterface type (GraphQL) 200–202\nInt scalar (GraphQL) 192, 194\ninversion of control container 147, 165\ninversion of control principle 165\niss reserved claim, JWT 281\nitems property, OpenAPI 93\nJ\njq (JSON query CLI) 344\nJSON (JavaScript Object Notation) 377, 380\nJSONPath 94\nJSON pointer 93–94\nJSON-RPC 377–386\nJSON Schema 91–95\njsonschema library 321\njti reserved claim, JWT 281\nJWKS (JSON Web Keys) 278\njwks_uri property 394\njwt_generator.py script 284, 292\nJWTs (JSON Web Tokens) 108, 278–286, 391\ninspecting 284–285\nJWT claims 280–282\nJWT header 279–280\nproducing 282–284\nvalidating 286\nK\nKEK (key encryption key) 364\nkid header property, JWT 280\nkind property, Kubernetes manifest 357\nkitchen API\nflask-smorest 122–123\nimplementing API endpoints 125–129\nimplementing in-memory list of schedules 140–142\nimplementing payload validation models with \nmarshmallow 129–132\ninitializing web application for API 123–125\noverriding flask-smorest dynamically generated \nAPI specification 142–143\noverview 120–122\nvalidating response payloads 136–139\nvalidating URL query parameters 133–136\nkubectl CLI 344\napi-resources command 356\napply command 357\nKubernetes 342–375\ncreating clusters with EKS 346–349\ndeleting clusters 372–374\ndeploying load balancers 351–353\ndeploying microservices to cluster 353–361\ncreating deployment object 354–356\ncreating service object 357–358\nexposing services with ingress objects 359–361\nIAM roles for service accounts 350–351\nmanaging secrets in Kubernetes 364–367\noverview 344–346",
      "content_length": 2973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": null,
      "content": "INDEX\n408\nKubernetes clusters\ncreating 346–349\ndeleting 372–374\ndeploying microservices to 353–361\nKubernetes control plane 345\nKubernetes in Action (Lukša) 372\nkube-system namespace 347, 352–353\nKuenzli, Stephen 338\nL\nlabels, Kubernetes 355\nLancey, Vallery 348\nlayered system principle 66\nLewis, James 4, 8, 10\nlibcurl library 8–9\nLink HTTP header 390\nlinks, OpenAPI 323–325\nlists() strategy, hypothesis 320\nload balancer 346\nLoadBalancer, Kubernetes service 358\nloose coupling principle 48\nLove, Chris 345\nLukša, Marko 372\nM\nmachine_to_machine_test.py script 400–401\nmake_executable_schema() function (Ariadne \nframework) 237, 240, 243, 245, 251–252, 257, \n261\nmarshalling 30\nmarshmallow, implementing payload validation \nmodels 129–132\nMeta class, marshmallow 129\nmetadata property, Kubernetes manifest 357\nMethodView class, Flask 125–127\nmicroservice API deployment 342–375\ncreating clusters with EKS 346–349\ndeploying microservices to cluster 353–361\ncreating deployment object 354–356\ncreating service object 357–358\nexposing services with ingress objects\n359–361\nmicroservice APIs 3–19\nauthorization and authentication 269–301\nbasic implementation of REST API with FastAPI\n20–44\nimplementing data validation models\n30–34\nimplementing endpoints 23–28\nmarshalling and validating response payloads\n38–41\nvalidating request payloads 34–38\nbuilding REST APIs with Python\n110–143\nchallenges of microservices architecture 11–15\ndistributed transaction tracing 13–14\nmicroservices integration tests 12\noperational complexity and infrastructure \noverhead 14–15\nservice decomposition 11\nservice unavailability 12–13\nCoffeeMesh application 17\nDockerizing 331–341\ndocumentation-driven development 15–17\nGraphQL API\nconsumption 210–232\ncreation 233–266\ndesign 185–209\nREST API design principles 61–89\nREST API documentation 90–109\nservice implementation patterns 144–181\ntesting and validation 302–330\nweb APIs 8–11\nAPIs, defined 8–9\nrole of in microservices integration 9–11\nmicroservice design 45–58\ndesign principles 46–49\ndatabase-per-service principle 46–48\nloose coupling principle 48\nsingle responsibility principle 49\nservice decomposition\nby business capability 49–52, 57–58\nby subdomains 52–58\nmicroservices 4–8\ndefined 4–5\nhistory of 7–8\nmonoliths vs. 5–7\nmicroservices integration tests 12\nMicroservices Patterns (Richardson) 11\nMicroservices Security in Action (Siriwardena and \nDias) 333\nmiddleware 287\nmigration (databases) 150\nrunning with Alembic 151, 155, 339\nmock servers\nGraphQL 211\nREST API 168–169\nmodel composition, JSON Schema/OpenAPI\n103–104\nmonoliths 5\nmicroservices vs. 5–7\nMTV (model-template-view) pattern 17\nmutation resolvers, implementing, GraphQL/\nAriadne framework 256–258\nmutations, GraphQL\ndesigning 206–209\nimplementing mutation resolvers 256–258\nrunning 225–226\nrunning parametrized 226–229\nMutationType class (Ariadne framework) 256–257",
      "content_length": 2869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": null,
      "content": "INDEX\n409\nN\nnamespaces, Kubernetes 346\nnbf reserved claim, JWT 281\nNetworking and Kubernetes (Strong and Lancey) 348\nNickoloff, Jeff 338\nNodePort service, Kubernetes 358\nnonce keyword, JWT header 280\nnull type, JSON Schema/OpenAPI 92\nnumber type, JSON Schema/OpenAPI 92\nO\nOAS (OpenAPI Specification) 21, 382\nOAuth2 (Open Authorization) 108\nOAuth (Open Authorization) 271–275, 391\nauthorization code flow 273\nclient credentials flow 275\nPKCE flow 273–275\nrefresh token flow 275\nobject relational mapper (ORM) 150–151\nobject type (GraphQL) 193\nobject type, JSON Schema/OpenAPI 92\nObjectType (Ariadne) 240\nobj parameter, Ariadne resolvers 239–240, 256\nOIDC (OpenID Connect) 18, 271, 276–278, 350, \n391\none() method (SQLAlchemy) 160\none-to-many connection, GraphQL 197\none-to-one connection, GraphQL 197\non keyword (GraphQL) 218\nOpenAPI 21, 90–109, 382\nanatomy of 95–96\nendpoints 96–97\nJSON Schema 91–95\nlinks 323–325\nmodel composition 103–104\nrefactoring schema definitions 100–102\nrequest payloads 98–100\nresponses\ncreating generic 105–107\ndocumenting 102–105\nsecuritySchemes 107–109\nservers property 96, 370–372\nURL query parameters 97–98\nopenapi property, OpenAPI 119\nOpen Authorization (OAuth) 108\nOpenID Connect in Action (Siriwardena) 278\nOpenID Connect (OIDC) 18, 271, 276–278, 350, 391\noperational complexity 14–15\norders API\nhigh-level architecture 22–23\nimplementing API endpoints 23–28\nimplementing data validation models with \npydantic 30–34\nimplementing in-memory list of orders 41–43\nmarshalling and validating response payloads \nwith pydantic 38–41\noverriding FastAPI dynamically generated \nspecification 118–120\noverview 111–112\nURL query parameters for 112–115\nvalidating payloads with unknown fields 115–118\nvalidating request payloads with pydantic 34–38\nORM (object relational mapper) 150–151\noverfetching 384\nP\nPATCH (HTTP method) 74, 85, 384\npercent encoding 230\npickle serialization (Python) 380\nPipenv 25\n- -deploy flag, pipenv 335\n- -system flag 335\nPKCE (Proof of Key for Code Exchange) flow, \nOAuth 273–275, 397–398\npods, Kubernetes 345\nports, hexagonal architecture 145–146\nPostel’s law 115\nPostgreSQL 332, 365\nPOST (HTTP method) 74\nPOST requests, response payloads for 84\npreflight request 289\nprism (API mocking tool) 168–169\nprivate key 283–284\nProductInterface type, Products API 201–202, 218, \n220–221, 225, 234, 247\nproducts API (GraphQL) 241–265\nbuilding resolvers for custom scalars 258–262\ncombining types through unions and \ninterfaces 200–202\nconstraining property values with \nenumerations 202–203\ncreating entry point for GraphQL server 242–243\ncustom scalars 194–195\ndesigning queries 203–206\nGraphQL 186–189\nhandling query parameters 252–255\nimplementing field resolvers 262–265\nimplementing mutation resolvers 256–258\nimplementing query resolvers 243–246\nimplementing type resolvers 246–252\nlaying out project structure 241–242\nmutations 206–209\noverview 189–192\nProduct type 201–202, 207, 217, 225, 234, 239, \n247, 250–251, 258\nProduct type 201–202, 207, 217, 225, 234, 239, 243, \n247, 250–251, 258\nproperties, JSON Schema/OpenAPI 92\nproperty() decorator, Python built-in 166",
      "content_length": 3117,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": null,
      "content": "INDEX\n410\nproperty-based testing 315–322\ndefined 315\nusing hypothesis to test endpoints 319–322\nvs. traditional approach to testing 316–317\nwith hypothesis 318–319\nProtobuf (Protocol Buffers) 380\npsycopg, Python library 332\npublic certificate 283–284\npublic key 283–286\nPUT (HTTP method) 74, 80\nPUT vs. PATCH requests 85\nPydantic library\nimplementing data validation models 30–34\nmarshalling and validating response \npayloads 38–41\nvalidating payloads with unknown fields 115–118\nvalidating request payloads 34–38\nPyJWT library 270–271, 282\npytest 317\nPYTHONPATH environment variable 155\nQ\nqueries (GraphQL) 206, 214, 217\naliasing 222–225\nhandling query parameters 252–255\nimplementing query resolvers 243–246\nover HTTP 229–230\nquery document (GraphQL) 214\nrunning 214–215\nrunning multiple 221\nrunning parametrized 226–229\nrunning with input parameters 219\nrunning with parameters 215\nunderstanding errors 215–217\nusing fragments in 217–219\nquery aliasing, GraphQL 222–225\nquery resolvers, implementing (GraphQL) 243–246\nQueryType class (Ariadne framework) 240, 245, 256\nR\nread-only properties (REST) 84\nReal-World Cryptography (Wong) 280\nrefresh token flow, OAuth 275\nrelationship() function, SQLAlchemy 153\nreplaceability principle 11\nrepository pattern 155–162\ndefinition 155–157\nimplementing 157–162\nrequestBody, OpenAPI 99\nrequest payloads, REST APIs\ndocumenting 98–100\nimplementing validation models with \nmarshmallow 129–132\nvalidating with pydantic 34–38\nrequests library 169\nreserved claims, OIDC, OAuth, and JWT 281\nresolvers (GraphQL; Ariadne framework)\nbuilding resolvers for custom scalars 258–262\nimplementing field resolvers 262–265\nimplementing mutation resolvers 256–258\nimplementing query resolvers 243–246\nimplementing type resolvers 246–252\nresource owner, OAuth 272\nresources, REST 71–72\nresource server, OAuth 272\nresponse() decorator, flask-smorest 130\nresponse payloads (REST)\nfor GET requests 86\nfor POST requests 84\nfor PUT and PATCH requests 85\nimplementing validation models with \nmarshmallow 129–132\nmarshalling and validating with pydantic 38–41\nresponses, OpenAPI\ndesigning generic 105–107\ndocumenting 102–105\nREST API design principles 61–89\narchitectural constraints of REST applications\n63–67\ncacheability principle 65–66\nclient-server architecture principle 64\ncode-on-demand principle 66\nlayered system principle 66\nstatelessness principle 64–65\nuniform interface principle 67\nHATEOAS 67–70\nHTTP-native APIs with REST 382\nHTTP payloads 83–86\ndefined 83–84\ndesign patterns 84–86\nHTTP status codes 77–83\ndefined 77–78\nreporting client errors in request 78–81\nreporting errors in server 82–83\nREST, defined 62–63\nRichardson maturity model 70–73\nstructured resource URLs with HTTP \nmethods 73–76\nURL query parameter design 87–88\nREST API endpoints\ndocumenting 96–97\nimplementing with FastAPI 23–28\nimplementing with flask-smorest 125–129\ntesting with hypothesis 319–322\nREST API implementation 110–143\nkitchen API\nflask-smorest 122–123\nimplementing endpoints 125–129\nimplementing payload validation models\n129–132",
      "content_length": 3045,
      "extraction_method": "Direct"
    },
    {
      "page_number": 439,
      "chapter": null,
      "content": "INDEX\n411\nREST API implementation, kitchen API (continued)\ninitializing web application 123–125\noverriding dynamically generated \nspecification 142–143\noverview 120–122\nvalidating response payloads 136–139\nvalidating URL query parameters 133–136\norders API\nhigh-level architecture 22–23\nimplementing API endpoints 23–28\nimplementing data validation models with \npydantic 30–34\nmarshalling and validating response payloads \nwith pydantic 38–41\noverriding FastAPI dynamically generated \nspecification 118–120\noverview 111–112\nspecification 21–22\nURL query parameters for 112–115\nvalidating payloads with unknown fields\n115–118\nvalidating request payloads with pydantic 34–38\nREST API testing\nwith Dredd 304–315\ncustomizing test suite with hooks 307–315\ninstalling and running default test suite\n305–307\noverview 304–305\nusing in testing strategy 315\nwith Schemathesis 322–327\nrunning default test suite 322–323\nusing - -checks=all 326–327\nusing - -stateful=links 323–327\nREST (Representational State Transfer) 61, 382\nRichardson, Chris 11\nRichardson, Leonard 70\nRichardson maturity model 70–73\nRiedesel, Jamie 14\nrollback() method (SQLAlchemy) 174, 176\nroute() decorator, flask-smorest 125, 127\nRPC (remote procedure call) 71, 377–386\nJSON-RPC 377–386\nXML-RPC 377–386\nRS256 encryption 280\nRSA encryption 280\nRSA-SHA256 encryption 280\nRUN directive, Docker 335\nrun_migrations_online() function (Alembic) 296\nS\nScalarType class (Ariadne framework) 260\nscalar types (GraphQL) 192\nbuilding resolvers for custom scalars 258–262\ncreating custom 194–195\ndesigning object properties with 192–193\nScheduleOrderSchema schema, kitchen API 121, \n129\nSchema class, marshmallow 129\nSchemathesis library 315, 327\nGraphQL API testing 327–329\nREST API testing 322–327\nrunning default test suite 322–323\nusing - -checks=all 326–327\nusing - -stateful=links 323–327\nSDL (Schema Definition Language) 15, 186\nsecuritySchemes, OpenAPI 107–109\nselection set (GraphQL) 214\nserialization 259, 380\nserializer() decorator, ScalarType (Ariadne \nframework) 260\nserverless databases 361–370\ncreating 361–364\nrunning database migrations and connecting \nservice to database 367–370\nserver-side properties, REST APIs 84\nservers section, OpenAPI 96, 119, 370–372\nservice, Kubernetes 354, 357–358\nservice decomposition 11, 45\nby business capability 49–52\nanalyzing business structure 49–50\ndecomposing microservices 50–52\nvs. by subdomains 57–58\nby subdomains 52–56\napplying strategic analysis 53–56\ndomain-driven design 52–53\nvs. by business capability 57–58\nservice implementation patterns 144–181\nhexagonal architecture 145–148\nimplementing business layer 162–172\nimplementing database models 149–155\nimplementing unit of work pattern 172–177\nintegrating API layer and service layer\n177–181\nrepository pattern 155–162\ndefined 155–157\nimplementing 157–162\nservice layer, integrating with API layer 177–181\nservice-oriented architecture 7\nservice unavailability 12–13\nsessionmaker() function (SQLAlchemy) 175\nsession object, UnitOfWork 176\nSession object (SQLAlchemy) 161, 174\nsignature, JWT 279\nsingle responsibility principle (SRP) 46, 49\nsingleton resource, REST APIs 62\nSiriwardena, Prabath 278, 333\nsmart endpoints 10\nSOAP (Simple Object Access Protocol) 378–379\nSoftware Telemetry (Riedesel) 14\nsolution space, domain-driven design 53\nSPAs (single-page applications) 64, 274, 397",
      "content_length": 3348,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": null,
      "content": "INDEX\n412\nSQLAlchemy 150–155\nColumn class 153\ncommit() method 175–176\ncreate_engine() function 175\ndeclarative_base() function 153\nfilter_by() method 160\nfirst() method 160\none() method 160\nrelationship() function 153\nrollback() method 174, 176\nsessionmaker() function 175\nSession object 161, 174\nSQLite 150–151, 295–296\nSRP (single responsibility principle) 46, 49\nStarlette framework 24\nStatefulSet workload (Kubernetes) 345\nstatelessness principle (REST) 64–65\nstrategic design, domain-driven design 53\nstring data type, JSON Schema/OpenAPI 92\nString field class, marshmallow 129\nString scalar (GraphQL) 192–194\nStrong, James 348\nstubs (gRPC) 381\nsubdomain, domain-driven design 53\nsubnet, AWS 349\nsub reserved claim, JWT 281\nSunset HTTP header 390\nsupportive subdomain, domain-driven design 53\nSwagger UI\nauthorizing requests 400–402\ndefined 117\nT\ntags property, OpenAPI 107\ntarget groups, AWS ALB 352\ntest fixtures 307\nTesting Web APIs (Winteringham) 304\ntext() strategy, hypothesis 318\nthree-tier architecture 22–23\ntolerant reader pattern 115–116\ntypes, GraphQL 192–195\nbuilding resolvers for custom scalars 258–262\ncombining through unions and interfaces\n200–202\nedge properties 196–198\nscalars, creating custom 194–195\nthrough types 198–200\nU\nubiquitous language, domain-driven design 53\nunevaluatedProperties, JSON \nSchema/OpenAPI 114\nuniform interface principle, REST APIs 67\nunion type, GraphQL 200–202\nUnionType class (Ariadne) 251\nUnitOfWork class 177\nunit of work pattern 172–177\nunmarshalling 380\nUnprocessable Entity (409 HTTP status code) 79\nURI (Uniform Resource Identifier) 67\nURL encoding 229\nURL query parameters\ndesign 87–88\ndocumenting 97–98\nfor kitchen API (flask-smorest) 133–136\nfor orders API (FastAPI) 112–115\nvalidating 133–136\nURLs (Uniform Resource Locators) 9\nstructured resource URLs with HTTP \nmethods 73–76\nUUID (universally unique identifier) 27, 102\nuvicorn 24, 112, 235\nV\nvalidate() function, jsonschema 321\nValidationError exception, marshmallow 139\nvalidator() decorator, Pydantic 37\nvalue_parser() decorator (Ariadne) 260\nVPC (Virtual Private Cloud), AWS 348\nVyas, Jay 345\nW\nweb APIs 8–11, 376–386\nAPIs, defined 8–9\ndefined 9\nemergence of API standards 378–379\ngranular queries with GraphQL 382–386\nHTTP-native APIs with REST 382\nrole of in microservices integration 9–11\nRPC 377–386\ngRPC 380–381\nJSON-RPC 377–386\nXML-RPC 377–386\nSOAP 378–379\nWinteringham , Mark 304\nWong, David 280\nWORKDIR directive, Docker 335\nX\nX.509 certificates\ndefined 283\ngenerating 283\nXML-RPC 377–386\nY\nyaml library, Python 321",
      "content_length": 2545,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": null,
      "content": "Hexagonal architecture helps us build services with loosely coupled components. We distinguish a core \nbusiness layer, which is represented by the hexagon's core, and a collection of ports and adapters that allow \nus to connect the core business layer with external components, such as databases and API connections over \nHTTP. We use the repository pattern to encapsulate the complexity of accessing data sources and ensure their \nimplementation details don't leak into the business layer.\nClient\nMySQL\nMongoDB\nPostgreSQL\nGraphQL allows API clients to request granular data from the server and to retrieve details of related resources. \nIn this example, the products API has two types of resources: products and ingredients, both of which are \nconnected through products’ ingredients field. Using this connection, a client can request the name and price \nof each product, as well as the name of each product's ingredient.\nIngredient\nid: ea2ac48d-40e3\nname: coﬀee\nstock: 55.4 KG\nlastUpdated: 2022/07/14\nProduct\nid: f262441b-a3de\nname: Mocha\nprice: 9.59\navailable: true\nsize: SMALL\nlastUpdated: 2022/06/03\ningredients: [\nea2ac48d-40e3,\n3166c9a1-c7fc,\n528b3a61-14d7\n]\nDatabase\nIngredient\nid: 3166c9a1-c7fc\nname: milk\nstock: 119.2 L\nlastUpdated: 2022/07/14\nIngredient\nid: 528b3a61-14d7\nname: chocolate\nstock: 67.3 KG\nlastUpdated: 2022/07/14\nRequest\nproducts {\nname,\nprice,\ningredients {\nname\n}\n}\nproducts: [\n{\nname: Mocha,\nprice: $9.59,\ningredients: [\n{ name: coﬀee },\n{ name: milk },\n{ name: chocolate }\n}\nResponse\nClient\nGraphQL server\n]\n]",
      "content_length": 1538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": null,
      "content": "José Haro Peralta\nISBN-13: 978-1-61729-841-7\nC\nlean, clear APIs are essential to the success of microservice \napplications. Well-designed APIs enable reliable integra-\ntions between services and help simplify maintenance, \nscaling, and redesigns. Th is book teaches you the patterns, \nprotocols, and strategies you need to design, build, and deploy \neff ective REST and GraphQL microservices APIs.\nMicroservice APIs gathers proven techniques for creating and \nbuilding easy-to-consume APIs for microservices applications. \nRich with proven advice and Python-based examples, this \npractical book focuses on implementation over philosophy. \nYou’ll learn how to build robust microservice APIs, test and \nprotect them, and deploy them to the cloud following prin-\nciples and patterns that work in any language. \nWhat’s Inside\n● Service decomposition strategies for microservices\n● Best practices for designing and building REST and \n   GraphQL APIs\n● Service implementation patterns for loosely coupled \n   components\n● API authorization with OAuth and OIDC\n● Deployments with AWS and Kubernetes\nFor developers familiar with the basics of web development. \nExamples are in Python.\nJosé Haro Peralta is a consultant, author, and instructor. He’s \nalso the founder of microapis.io.\nRegister this print book to get free access to all ebook formats. \nVisit https://www.manning.com/freebook\nMicroservice APIs\nMICROSERVICES / PYTHON\nM A N N I N G\n“\nAn insightful guide for \ncreating REST and GraphQL \nAPIs, with neat examples \nusing FastAPI and Flask. \nTh e service implementation \npatterns chapter is a must-read \nfor every developer.”\n—William Jamir Silva, Adjust \n“\nA perfect introduction \nto microservice web APIs \n  in Python.”\n—Stuart Woodward\nCEO, Hanamaru  \n“\nA well-designed API makes \nall the diff erence in the \nsuccess of your next project. \nTh is book equips you with \nthe knowledge and the skills \n you need. Excellent!”\n—Alain Lompo, ISO-Gruppe\n“\nExcellent coverage with \n practical examples.”\n—Sambasiva Andaluri, IBM  \nSee first page",
      "content_length": 2040,
      "extraction_method": "Direct"
    }
  ],
  "enrichment": {
    "version": "1.0.0",
    "generated_by": "generate_chapter_metadata.py",
    "contains": [
      "keywords",
      "concepts",
      "summary"
    ]
  }
}