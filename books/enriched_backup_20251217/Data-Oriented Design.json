{
  "metadata": {
    "title": "Data-Oriented Design",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 217,
    "conversion_date": "2025-11-23T12:25:27.954680",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Data-Oriented Design.pdf"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Data-Oriented Design",
      "start_page": 6,
      "end_page": 29,
      "detection_method": "regex_chapter",
      "content": "Chapter 1\nData-Oriented Design\nData-oriented design has been around for decades in one\nform or another but was only oﬃcially given a name by\nNoel Llopis in his September 2009 article[?] of the same\nname.\nWhether it is, or is not a programming paradigm\nis seen as contentious.\nMany believe it can be used side\nby side with other programming paradigms such as object-\noriented, procedural, or functional programming.\nIn one\nrespect they are right, data-oriented design can function\nalongside the other paradigms, but that does not preclude\nit from being a way to approach programming in the large.\nOther programming paradigms are known to function along-\nside each other to some extent as well. A Lisp programmer\nknows that functional programming can coexist with object-\noriented programming and a C programmer is well aware\nthat object-oriented programming can coexist with proce-\ndural programming. We shall ignore these comments and\nclaim data-oriented design as another important tool; a tool\njust as capable of coexistence as the rest. 1\n1There are some limits, but it is not mutually exclusive with any paradigm\nother than maybe the logic programming languages such as Prolog. The ex-\ntremely declarative ”what, not how” approach does seem to exclude thinking\nabout the data and how it interacts with the machine.\n3\n\n\n4\nCHAPTER 1. DATA-ORIENTED DESIGN\nThe time was right in 2009. The hardware was ripe for\na change in how to develop. Potentially very fast comput-\ners were hindered by a hardware ignorant programming\nparadigm. The way game programmers coded at the time\nmade many engine programmers weep.\nThe times have\nchanged.\nMany mobile and desktop solutions now seem\nto need the data-oriented design approach less, not be-\ncause the machines are better at mitigating an ineﬀective\napproach, but the games being designed are less demanding\nand less complex. The trend for mobile seems to be moving\nto AAA development, which should bring the return of a\nneed for managing complexity and getting the most out of\nthe hardware.\nAs we now live in a world where multi-core machines in-\nclude the ones in our pockets, learning how to develop soft-\nware in a less serial manner is important. Moving away from\nobjects messaging and getting responses immediately is part\nof the beneﬁts available to the data-oriented programmer.\nProgramming, with a ﬁrm reliance on awareness of the data\nﬂow, sets you up to take the next step to GPGPU and other\ncompute approaches. This leads to handling the workloads\nthat bring game titles to life. The need for data-oriented de-\nsign will only grow. It will grow because abstractions and se-\nrial thinking will be the bottleneck of your competitors, and\nthose that embrace the data-oriented approach will thrive.\n1.1\nIt’s all about the data\nData is all we have. Data is what we need to transform in\norder to create a user experience. Data is what we load when\nwe open a document. Data is the graphics on the screen, the\npulses from the buttons on your gamepad, the cause of your\nspeakers producing waves in the air, the method by which\nyou level up and how the bad guy knew where you were so\nas to shoot at you. Data is how long the dynamite took to ex-\nplode and how many rings you dropped when you fell on the\n\n\n1.1. IT’S ALL ABOUT THE DATA\n5\nspikes. It is the current position and velocity of every particle\nin the beautiful scene that ended the game which was loaded\noﬀthe disc and into your life via transformations by machin-\nery driven by decoded instructions themselves ordered by\nassemblers instructed by compilers fed with source-code.\nNo application is anything without its data. Adobe Pho-\ntoshop without the images is nothing. It’s nothing without\nthe brushes, the layers, the pen pressure. Microsoft Word is\nnothing without the characters, the fonts, the page breaks.\nFL Studio is worthless without the events.\nVisual Studio\nis nothing without source.\nAll the applications that have\never been written, have been written to output data based\non some input data. The form of that data can be extremely\ncomplex, or so simple it requires no documentation at all,\nbut all applications produce and need data.\nIf they don’t\nneed recognisable data, then they are toys or tech demos at\nbest.\nInstructions are data too. Instructions take up memory,\nuse up bandwidth, and can be transformed, loaded, saved\nand constructed. It’s natural for a developer to not think of\ninstructions as being data2, but there is very little diﬀerenti-\nating them on older, less protective hardware. Even though\nmemory set aside for executables is protected from harm and\nmodiﬁcation on most contemporary hardware, this relatively\nnew invention is still merely an invention, and the modiﬁed\nHarvard architecture relies on the same memory for data\nas it does for instructions.\nInstructions are therefore still\ndata, and they are what we transform too. We take instruc-\ntions and turn them into actions.\nThe number, size, and\nfrequency of them is something that matters. The idea that\nwe have control over which instructions we use to solve prob-\nlems leads us to optimisations. Applying our knowledge of\nwhat the data is allows us to make decisions about how the\ndata can be treated. Knowing the outcome of instructions\ngives us the data to decide what instructions are necessary,\nwhich are busywork, and which can be replaced with equiv-\n2unless they are a Lisp programmer\n\n\n6\nCHAPTER 1. DATA-ORIENTED DESIGN\nalent but less costly alternatives.\nThis forms the basis of the argument for a data-oriented\napproach to development, but leaves out one major element.\nAll this data and the transforming of data, from strings, to\nimages, to instructions, they all have to run on something.\nSometimes that thing is quite abstract, such as a virtual\nmachine running on unknown hardware. Sometimes that\nthing is concrete, such as knowing which speciﬁc CPU and\nGPU you have, and the memory capacity and bandwidth you\nhave available. But in all cases, the data is not just data,\nbut data that exists on some hardware somewhere, and it\nhas to be transformed by that same hardware. In essence,\ndata-oriented design is the practice of designing software by\ndeveloping transformations for well-formed data where the\ncriteria for well-formed is guided by the target hardware and\nthe patterns and types of transforms that need to operate on\nit. Sometimes the data isn’t well deﬁned, and sometimes the\nhardware is equally evasive, but in most cases a good back-\nground of hardware appreciation can help out almost every\nsoftware project.\nIf the ultimate result of an application is data, and all\ninput can be represented by data, and it is recognised that\nall data transforms are not performed in a vacuum, then a\nsoftware development methodology can be founded on these\nprinciples; the principles of understanding the data, and\nhow to transform it given some knowledge of how a machine\nwill do what it needs to do with data of this quantity, fre-\nquency, and its statistical qualities. Given this basis, we can\nbuild up a set of founding statements about what makes a\nmethodology data-oriented.\n1.2\nData is not the problem domain\nThe ﬁrst principle: Data is not the problem domain.\n\n\n1.2. DATA IS NOT THE PROBLEM DOMAIN\n7\nFor some, it would seem that data-oriented design is the\nantithesis of most other programming paradigms because\ndata-oriented design is a technique that does not readily al-\nlow the problem domain to enter into the software as written\nin source. It does not promote the concept of an object as\na mapping to the context of the user in any way, as data is\nintentionally and consistently without meaning. Abstraction\nheavy paradigms try to pretend the computer and its data do\nnot exist at every turn, abstracting away the idea that there\nare bytes, or CPU pipelines, or other hardware features, and\ninstead bringing the model of the problem into the program.\nThey regularly bring either the model of the view into the\ncode, or the model of the world as a context for the problem.\nThat is, they either structure the code around attributes of\nthe expected solution, or they structure the code around the\ndescription of the problem domain.\nMeaning can be applied to data to create information.\nMeaning is not inherent in data. When you say 4, it means\nvery little, but say 4 miles, or 4 eggs, it means something.\nWhen you have 3 numbers, they mean very little as a tu-\nple, but when you name them x,y,z, you can put meaning on\nthem as a position. When you have a list of positions in a\ngame, they mean very little without context. Object-oriented\ndesign would likely have the positions as part of an object,\nand by the class name and neighbouring data (also named)\nyou can get an idea of what that data means. Without the\nconnected named contextualising data, the positions could\nbe interpreted in a number of diﬀerent ways, and though\nputting the numbers in context is good in some sense, it\nalso blocks thinking about the positions as just sets of three\nnumbers, which can be important for thinking of solutions\nto the real problems the programmers are trying to solve.\nFor an example of what can happen when you put data\nso deep inside an object that you forget its impact, consider\nthe numerous games released, and in production, where a\n2D or 3D grid system could have been used for the data\nlayout, but for unknown reasons the developers kept with\n\n\n8\nCHAPTER 1. DATA-ORIENTED DESIGN\nthe object paradigm for each entity on the map. This isn’t\na singular event, and real shipping games have seen this\nobject-centric approach commit crimes against the hardware\nby having hundreds of objects placed in WorldSpace at grid\ncoordinates, rather than actually being driven by a grid. It’s\npossible that programmers look at a grid, and see the num-\nber of elements required to fulﬁl the request, and are hesitant\nto the idea of allocating it in a single lump of memory. Con-\nsider a simple 256 by 256 tilemap requiring 65,536 tiles. An\nobject-oriented programmer may think about those sixty-ﬁve\nthousand objects as being quite expensive. It might make\nmore sense for them to allocate the objects for the tiles only\nwhen necessary, even to the point where there literally are\nsixty-ﬁve thousand tiles created by hand in editor, but be-\ncause they were placed by hand, their necessity has been es-\ntablished, and they are now something to be handled, rather\nthan something potentially worrying.\nNot only is this pervasive lack of an underlying form a\npoor way to handle rendering and simple element placement,\nbut it leads to much higher complexity when interpreting lo-\ncality of elements.\nGaining access to elements on a grid-\nfree representation often requires jumping through hoops\nsuch as having neighbour links (which need to be kept up to\ndate), running through the entire list of elements (inherently\ncostly), or references to an auxiliary augmented grid object\nor spatial mapping system connecting to the objects which\nare otherwise free to move, but won’t, due to the design of the\ngame. This fake form of freedom introduced by the grid-free\ndesign presents issues with understanding the data, and has\nbeen the cause of some signiﬁcant performance penalties in\nsome titles. Thus also causing a signiﬁcant waste of pro-\ngrammer mental resources in all.\nOther than not having grids where they make sense,\nmany modern games also seem to carry instances for each\nand every item in the game.\nAn instance for each rather\nthan a variable storing the number of items.\nFor some\ngames this is an optimisation, as creation and destruction\n\n\n1.2. DATA IS NOT THE PROBLEM DOMAIN\n9\nof objects is a costly activity, but the trend is worrying, as\nthese ways of storing information about the world make the\nworld impenetrable to simple interrogation.\nMany games seem to try to keep everything about the\nplayer in the player class. If the player dies in-game, they\nhave to hang around as a dead object, otherwise, they lose\naccess to their achievement data. This linking of what the\ndata is, to where it resides and what it shares lifetime with,\ncauses monolithic classes and hard to untangle relation-\nships which frequently turn out to be the cause of bugs. I\nwill not name any of the games, but it’s not just one title, nor\njust one studio, but an epidemic of poor technical design that\nseems to infect those who use oﬀthe shelf object-oriented\nengines more than those who develop their own regardless\nof paradigm.\nThe data-oriented design approach doesn’t build the real-\nworld problem into the code. This could be seen as a failing of\nthe data-oriented approach by veteran object-oriented devel-\nopers, as examples of the success of object-oriented design\ncome from being able to bring the human concepts to the ma-\nchine, then in this middle ground, a solution can be written\nthat is understandable by both human and computer. The\ndata-oriented approach gives up some of the human read-\nability by leaving the problem domain in the design docu-\nment, bringing elements of constraints and expectations into\nthe transforms, but stops the machine from having to handle\nhuman concepts at any data level by just that same action.\nLet us consider how the problem domain becomes part of\nthe software in programming paradigms that promote need-\nless abstraction. In the case of objects, we tie meanings to\ndata by associating them with their containing classes and\ntheir associated functions. In high-level abstraction, we sep-\narate actions and data by high-level concepts, which might\nnot apply at the low level, thus reducing the likelihood the\nfunctions can be implemented eﬃciently.\nWhen a class owns some data, it gives that data a context\n\n\n10\nCHAPTER 1. DATA-ORIENTED DESIGN\nwhich can sometimes limit the ability to reuse the data or un-\nderstand the impact of operations upon it. Adding functions\nto a context can bring in further data, which quickly leads\nto classes containing many diﬀerent pieces of data that are\nunrelated in themselves, but need to be in the same class\nbecause an operation required a context and the context re-\nquired more data for other reasons such as for other related\noperations. This sounds awfully familiar, and Joe Armstrong\nis quoted to have said “I think the lack of reusability comes\nin object-oriented languages, not functional languages. Be-\ncause the problem with object-oriented languages is they’ve\ngot all this implicit environment that they carry around with\nthem. You wanted a banana but what you got was a gorilla\nholding the banana and the entire jungle.”3 which certainly\nseems to resonate with the issue of contextual referencing\nthat seems to be plaguing the object-oriented languages.\nYou could be forgiven for believing that it’s possible to re-\nmove the connections between contexts by using interfaces\nor dependency injection, but the connections lie deeper than\nthat. The contexts in the objects are often connecting diﬀer-\nent classes of data about diﬀerent categories in which the\nobject ﬁts. Consider how this banana has many diﬀerent\npurposes, from being a fruit, to being a colour, to being a\nword beginning with the letter B. We have to consider the\nproblem presented by the idea of the banana as an instance,\nas well as the banana being a class of entity too. If we need\nto gain information about bananas from the point of view of\nthe law on imported goods, or about its nutritional value, it’s\ngoing to be diﬀerent from information about how many we\nare currently stocking. We were lucky to start with the ba-\nnana. If we talk about the gorilla, then we have information\nabout the individual gorilla, the gorillas in the zoo or jungle,\nand the class of gorilla too. This is three diﬀerent layers of\nabstraction about something which we might give one name.\nAt least with a banana, each individual doesn’t have much\nin the way of important data. We see this kind of contextual\nlinkage all the time in the real world, and we manage the\n3From Peter Seibel’s Coders at Work[?]\n\n\n1.2. DATA IS NOT THE PROBLEM DOMAIN\n11\ncomplexity very well in conversation, but as soon as we start\nputting these contexts down in hard terms we connect them\ntogether and make them brittle.\nAll these mixed layers of abstraction become hard to un-\ntangle as functions which operate over each context drag\nin random pieces of data from all over the classes mean-\ning many data items cannot be removed as they would then\nbe inaccessible. This can be enough to stop most program-\nmers from attempting large-scale evolving software projects,\nbut there is another issue caused by hiding the actions ap-\nplied to the data that leads to unnecessary complexity. When\nyou see lists and trees, arrays and maps, tables and rows,\nyou can reason about them and their interactions and trans-\nformations. If you attempt to do the same with homes and\noﬃces, roads and commuters, coﬀee shops and parks, you\ncan often get stuck in thinking about the problem domain\nconcepts and not see the details that would provide clues to\na better data representation or a diﬀerent algorithmic ap-\nproach.\nThere are very few computer science algorithms that\ncannot be reused on primitive data types, but when you\nintroduce new classes with their own internal layouts of\ndata, that don’t follow clearly in the patterns of existing\ndata-structures, then you won’t be able to fully utilise those\nalgorithms, and might not even be able to see how they\nwould apply. Putting data structures inside your object de-\nsigns might make sense from what they are, but they often\nmake little sense from the perspective of data manipulation.\nWhen we consider the data from the data-oriented design\npoint of view, data is mere facts that can be interpreted in\nwhatever way necessary to get the output data in the format\nit needs to be. We only care about what transforms we do,\nand where the data ends up. In practice, when you discard\nmeanings from data, you also reduce the chance of tangling\nthe facts with their contexts, and thus you also reduce the\nlikelihood of mixing unrelated data just for the sake of an\n\n\n12\nCHAPTER 1. DATA-ORIENTED DESIGN\noperation or two.\n1.3\nData and statistics\nThe second principle: Data is the type, frequency, quantity,\nshape, and probability.\nThe second statement is that data is not just the struc-\nture. A common misconception about data-oriented design\nis that it’s all about cache misses. Even if it was all about\nmaking sure you never missed the cache, and it was all about\nstructuring your classes so the hot and cold data was split\napart, it would be a generally useful addition to your pro-\ngramming toolkit, but data-oriented design is about all as-\npects of the data.\nTo write a book on how to avoid cache\nmisses, you need more than just some tips on how to organ-\nise your structures, you need a grounding in what is really\nhappening inside your computer when it is running your pro-\ngram. Teaching that in a book is also impossible as it would\nonly apply to one generation of hardware, and one genera-\ntion of programming languages, however, data-oriented de-\nsign is not rooted in just one language and just some unusual\nhardware, even though the language to best beneﬁt from it\nis C++, and the hardware to beneﬁt the approach the most\nis anything with unbalanced bottlenecks. The schema of the\ndata is important, but the values and how the data is trans-\nformed are as important, if not more so. It is not enough to\nhave some photographs of a cheetah to determine how fast it\ncan run. You need to see it in the wild and understand the\ntrue costs of being slow.\nThe data-oriented design model is centred around data.\nIt pivots on live data, real data, data that is also information.\nObject-oriented design is centred around the problem deﬁni-\ntion. Objects are not real things but abstract representations\nof the context in which the problem will be solved. The ob-\njects manipulate the data needed to represent them without\n\n\n1.3. DATA AND STATISTICS\n13\nany consideration for the hardware or the real-world data\npatterns or quantities. This is why object-oriented design\nallows you to quickly build up ﬁrst versions of applications,\nallowing you to put the ﬁrst version of the design document\nor problem deﬁnition directly into the code, and make a quick\nattempt at a solution.\nData-oriented design takes a diﬀerent approach to the\nproblem, instead of assuming we know nothing about the\nhardware, it assumes we know little about the true nature\nof our problem, and makes the schema of the data a second-\nclass citizen.\nAnyone who has written a sizeable piece of\nsoftware may recognise that the technical structure and the\ndesign for a project often changes so much that there is\nbarely any section from the ﬁrst draft remaining unchanged\nin the ﬁnal implementation.\nData-oriented design avoids\nwasting resources by never assuming the design needs to\nexist anywhere other than in a document. It makes progress\nby providing a solution to the current problem through some\nhigh-level code controlling sequences of events and specify-\ning schema in which to give temporary meaning to the data.\nData-oriented design takes its cues from the data which\nis seen or expected. Instead of planning for all eventualities,\nor planning to make things adaptable, there is a preference\nfor using the most probable input to direct the choice of al-\ngorithm. Instead of planning to be extendable, it plans to be\nsimple and replaceable, and get the job done. Extendable\ncan be added later, with the safety net of unit tests to ensure\nit remains working as it did while it was simple.\nLuckily,\nthere is a way to make your data layout extendable without\nrequiring much thought, by utilising techniques developed\nmany years ago for working with databases.\nDatabase technology took a great turn for the positive\nwhen the relational model was introduced. In the paper Out\nof the Tar Pit[?], Functional Relational Programming takes\nit a step further when it references the idea of using re-\nlational model data-structures with functional transforms.\n\n\n14\nCHAPTER 1. DATA-ORIENTED DESIGN\nThese are well deﬁned, and much literature on how to adapt\ntheir form to match your requirements is available.\n1.4\nData can change\nData-oriented design is current. It is not a representation of\nthe history of a problem or a solution that has been brought\nup to date, nor is it the future, with generic solutions made\nup to handle whatever will come along. Holding onto the past\nwill interfere with ﬂexibility, and looking to the future is gen-\nerally fruitless as programmers are not fortune tellers. It’s\nthe opinion of the author, that future-proof systems rarely\nare. Object-oriented design starts to show its weaknesses\nwhen designs change in the real-world.\nObject-oriented design is known to handle changes to un-\nderlying implementation details very well, as these are the\nexpected changes, the obvious changes, and the ones often\ncited in introductions to object-oriented design.\nHowever,\nreal world changes such as change of user’s needs, changes\nto input format, quantity, frequency, and the route by which\nthe information will travel, are not handled with grace. It was\nintroduced in On the Criteria To Be Used in Decomposing Sys-\ntems into Modules[?] that the modularisation approach used\nby many at the time was rather like that of a production\nline, where elements of the implementation are caught up\nin the stages of the proposed solution. These stages them-\nselves would be identiﬁed with a current interpretation of the\nproblem. In the original document, the solution was to intro-\nduce a data hiding approach to modularisation, and though\nit was an improvement, in the later book Software Pioneers:\nContributions to Software Engineering[?], D. L. Parnas revis-\nits the issue and reminds us that even though initial soft-\nware development can be faster when making structural de-\ncisions based on business facts, it lays a burden on main-\ntenance and evolutionary development. Object-oriented de-\nsign approaches suﬀer from this inertia inherent in keeping\n\n\n1.4. DATA CAN CHANGE\n15\nthe problem domain coupled with the implementation. As\nmentioned, the problem domain, when introduced into the\nimplementation, can help with making decisions quickly, as\nyou can immediately see the impact the implementation will\nhave on getting closer to the goal of solving or working with\nthe problem in its current form. The problem with object-\noriented design lies in the inevitability of change at a higher\nlevel.\nDesigns change for multiple reasons, occasionally includ-\ning times when they actually haven’t. A misunderstanding\nof a design, or a misinterpretation of a design, will cause as\nmuch change in the implementation as a literal request for\nchange of design. A data-oriented approach to code design\nconsiders the change in design through the lens of under-\nstanding the change in the meaning of the data. The data-\noriented approach to design also allows for change to the\ncode when the source of data changes, unlike the encap-\nsulated internal state manipulations of the object-oriented\napproach. In general, data-oriented design handles change\nbetter as pieces of data and transforms can be more sim-\nply coupled and decoupled than objects can be mutated and\nreused.\nThe reason this is so, comes from linking the intention,\nor the aspect, to the data. When lumping data and func-\ntions in with concepts of objects, you ﬁnd the objects are\nthe schema of the data.\nThe aspect of the data is linked\nto that object, which means it’s hard to think of the data\nfrom another point of view. The use case of the data, and\nthe real-world or design, are now linked to the data layout\nthrough a singular vision implied by the object deﬁnition. If\nyou link your data layout to the union of the required data\nfor your expected manipulations, and your data manipula-\ntions are linked by aspects of your data, then you make it\nhard to unlink data related by aspect. The diﬃculty comes\nwhen diﬀerent aspects need diﬀerent subsets of the data,\nand they overlap. When they overlap, they create a larger\nand larger set of values that need to travel around the sys-\n\n\n16\nCHAPTER 1. DATA-ORIENTED DESIGN\ntem as one unit.\nIt’s common to refactor a class out into\ntwo or more classes, or give ownership of data to a diﬀerent\nclass. This is what is meant by tying data to an aspect. It\nis tied to the lens through which the data has purpose, but\nwith static typed objects that purpose is predeﬁned, a union\nof multiple purposes, and sometimes carries around defunct\nrelationships. Some purposes may no longer required by the\ndesign. Unfortunately, it’s easier to see when a relationship\nneeds to exist, than when it doesn’t, and that leads to more\nconnections, not fewer, over time.\nIf you link your operations by related data, such as when\nyou put methods on a class, you make it hard to unlink your\noperations when the data changes or splits, and you make\nit hard to split data when an operation requires the data\nto be together for its own purposes. If you keep your data\nin one place, operations in another place, and keep the as-\npects and roles of data intrinsic from how the operations and\ntransforms are applied to the data, then you will ﬁnd that\nmany times when refactoring would have been large and dif-\nﬁcult in object-oriented code, the task now becomes trivial\nor non-existent. With this beneﬁt comes a cost of keeping\ntabs on what data is required for each operation, and the\npotential danger of de-synchronisation. This consideration\ncan lead to keeping some cold code in an object-oriented style\nwhere objects are responsible for maintaining internal con-\nsistency over eﬃciency and mutability. Examples of places\nwhere object-oriented design is far superior to data-oriented\ncan be that of driver layers for systems or hardware. Even\nthough Vulkan and OpenGL are object-oriented, the gran-\nularity of the objects is large and linked to stable concepts\nin their space, just like the object-oriented approach of the\nFILE type or handle, in open, close, read, and write opera-\ntions in ﬁlesystems.\nA big misunderstanding for many new to the data-oriented\ndesign paradigm, a concept brought over from abstraction\nbased development, is that we can design a static library or\nset of templates to provide generic solutions to everything\n\n\n1.4. DATA CAN CHANGE\n17\npresented in this book as a data-oriented solution. Much like\nwith domain driven design, data-oriented design is product\nand work-ﬂow speciﬁc. You learn how to do data-oriented\ndesign, not how to add it to your project. The fundamental\ntruth is that data, though it can be generic by type, is not\ngeneric in how it is used. The values are diﬀerent and often\ncontain patterns we can turn to our advantage.\nThe idea\nthat data can be generic is a false claim that data-oriented\ndesign attempts to rectify. The transforms applied to data\ncan be generic to some extent, but the order and selection of\noperations are literally the solution to the problem. Source\ncode is the recipe for conversion of data from one form into\nanother. There cannot be a library of templates for under-\nstanding and leveraging patterns in the data, and that’s\nwhat drives a successful data-oriented design. It’s true we\ncan build algorithms to ﬁnd patterns in data, otherwise,\nhow would it be possible to do compression, but the pat-\nterns we think about when it comes to data-oriented design\nare higher level, domain-speciﬁc, and not simple frequency\nmappings.\nOur run-time beneﬁts from specialisation through perfor-\nmance tricks that sometimes make the code harder to read,\nbut it is frequently discouraged as being not object-oriented,\nor being too hard-coded.\nIt can be better to hard-code a\ntransform than to pretend it’s not hard-coded by wrapping it\nin a generic container and using less direct algorithms on it.\nUsing existing templates like this provides a beneﬁt of an in-\ncrease in readability for those who already know the library,\nand potentially fewer bugs if the functionality was in some\nway generic. But, if the functionality was not well mapped to\nthe existing generic solution, writing it with a function tem-\nplate and then extending will make the code harder to under-\nstand. Hiding the fact that the technique had been changed\nsubtly will introduced false assumptions. Hard-coding a new\nalgorithm is a better choice as long as it has suﬃcient tests,\nand is objectively new. Tests will also be easier to write if\nyou constrain yourself to the facts about concrete data and\nonly test with real, but simple data for your problem, and\n\n\n18\nCHAPTER 1. DATA-ORIENTED DESIGN\nnot generic types on generic data.\n1.5\nHow is data formed?\nThe games we write have a lot of data, in a lot of diﬀerent\nformats.\nWe have textures in multiple formats for multi-\nple platforms. There are animations, usually optimised for\ndiﬀerent skeletons or types of playback. There are sounds,\nlights, and scripts. Don’t forget meshes, they consist of mul-\ntiple buﬀers of attributes. Only a very small proportion of\nmeshes are old ﬁxed function type with vertices containing\npositions, UVs, and normals. The data in game development\nis hard to box, and getting harder to pin down as more ideas\nwhich were previously considered impossible have now be-\ncome commonplace. This is why we spend a lot of time work-\ning on editors and tool-chains, so we can take the free-form\noutput from designers and artists and ﬁnd a way to put it\ninto our engines. Without our tool-chains, editors, viewers,\nand tweaking tools, there would be no way we could pro-\nduce a game with the time we have. The object-oriented ap-\nproach provides a good way to wrap our heads around all\nthese diﬀerent formats of data. It gives a centralised view\nof where each type of data belongs and classiﬁes it by what\ncan be done to it. This makes it very easy to add and use\ndata quickly, but implementing all these diﬀerent wrapper\nobjects takes time.\nAdding new functionality to these ob-\njects can sometimes require large amounts of refactoring as\noccasionally objects are classiﬁed in such a way that they\ndon’t allow for new features to exist. For example, in many\nold engines, textures were always 1,2, or 4 bytes per pixel.\nWith the advent of ﬂoating point textures, all that code re-\nquired a minor refactoring. In the past, it was not possible\nto read a texture from the vertex shader, so when texture\nbased skinning came along, many engine programmers had\nto refactor their render update. They had to allow for a vertex\nshader texture upload because it might be necessary when\nuploading transforms for rendering a skinned mesh. When\n\n\n1.5. HOW IS DATA FORMED?\n19\nthe PlayStation2 came along, or an engine ﬁrst used shaders,\nthe very idea of what made a material had to change.\nIn\nthe move from small 3D environments to large open worlds\nwith level of detail caused many engineers to start think-\ning about what it meant for something to need rendering.\nWhen newer hardware became more picky about alignment,\nother hard to inject changes had to be made. In many en-\ngines, mesh data is optimised for rendering, but when you\nhave to do mesh ray casting to see where bullets have hit, or\nfor doing IK, or physics, then you need multiple representa-\ntions of an entity. At this point, the object-oriented approach\nstarts to look cobbled together as there are fewer objects that\nrepresent real things, and more objects used as containers\nso programmers can think in larger building blocks. These\nblocks hinder though, as they become the only blocks used\nin thought, and stop potential mental connections from hap-\npening. We went from 2D sprites to 3D meshes, following the\nformat of the hardware provider, to custom data streams and\ncompute units turning the streams into rendered triangles.\nWave data, to banks, to envelope controlled grain tables and\nslews of layered sounds. Tilemaps, to portals and rooms, to\nstreamed, multiple levels of detail chunks of world, to hybrid\nmesh palette, props, and unique stitching assets. From ﬂip-\nbook to Euler angle sequences, to quaternions and spherical\ninterpolated animations, to animation trees and behaviour\nmapping/trees. Change is the only constant.\nAll these types of data are pretty common if you’ve worked\nin games at all, and many engines do provide an abstraction\nto these more fundamental types. When a new type of data\nbecomes heavily used it is promoted into engines as a core\ntype. We normally consider the trade-oﬀof new types being\nhandled as special cases until they become ubiquitous to be\none of usability vs performance. We don’t want to provide\nfree access to the lesser understood elements of game devel-\nopment. People who are not, or can not, invest time in ﬁnd-\ning out how best to use new features, are discouraged from\nusing them. The object-oriented game development way to\ndo that is to not provide objects which represent them, and\n\n\n20\nCHAPTER 1. DATA-ORIENTED DESIGN\ninstead only oﬀer the features to people who know how to\nutilise the more advanced tools.\nApart from the objects representing digital assets, there\nare also objects for internal game logic.\nFor every game,\nthere are objects which only exist to further the game-play.\nCollectable card games have a lot of textures, but they also\nhave a great deal of rules, card stats, player decks, match\nrecords, with many objects to represent the current state of\nplay. All of these objects are completely custom designed for\none game. There may be sequels, but unless it’s primarily a\nre-skin, it will use quite diﬀerent game logic in many places,\nand therefore require diﬀerent data, which would imply dif-\nferent methods on the now guaranteed to be internally dif-\nferent objects.\nGame data is complex. Any ﬁrst layout of the data is in-\nspired by the game’s initial design. Once development is un-\nderway, the layout needs to keep up with whichever way the\ngame evolves. Object-oriented techniques oﬀer a quick way\nto implement any given design, are very quick at implement-\ning each singular design in turn, but don’t oﬀer a clean or\ngraceful way to migrate from one data schema to the next.\nThere are hacks, such as those used in version based asset\nhandlers, or in frameworks backed by update systems and\nconversion scripts, but normally, game developers change\nthe tool-chain and the engine at the same time, do a full re-\nexport of all the assets, then commit to the next version all\nin one go. This can be quite a painful experience if it has to\nhappen over multiple sites at the same time, or if you have\na lot of assets, or if you are trying to provide engine sup-\nport for more than one title, and only one wants to change\nto the new revision. An example of an object-oriented ap-\nproach that handles migration of design with some grace is\nthe Django framework, but the reason it handles the migra-\ntion well is that the objects would appear to be views into\ndata models, not the data itself.\nThere have not yet been any successful eﬀorts to build a\n\n\n1.6. THE FRAMEWORK\n21\ngeneric game asset solution. This may be because all games\ndiﬀer in so many subtle ways that if you did provide a generic\nsolution, it wouldn’t be a game solution, just a new language.\nThere is no solution to be found in trying to provide all the\npossible types of object a game can use. But, there is a so-\nlution if we go back to thinking about a game as merely run-\nning a set of computations on some data. The closest we\ncan get in 2018 is the FBX format, with some dependence\non the current standard shader languages. The current so-\nlutions appear to have excess baggage which does not seem\neasy to remove. Due to the need to be generic, many details\nare lost through abstractions and strategies to present data\nin a non-confrontational way.\n1.6\nWhat can provide a computational\nframework for such complex data?\nGame developers are notorious for thinking about game\ndevelopment from either a low level all out performance\nperspective or from a very high-level gameplay and inter-\naction perspective. This may have come about because of\nthe widening gap between the amount of code that has to be\nhigh performance, and the amount of code to make the game\ncomplete. Object-oriented techniques provide good coverage\nof the high-level aspect, so the high-level programmers are\ncontent with their tools. The performance specialists have\nbeen ﬁnding ways of doing more with the hardware, so\nmuch so that a lot of the time content creators think they\ndon’t have a part in the optimisation process.\nThere has\nnever been much of a middle ground in game development,\nwhich is probably the primary reason why the structure\nand performance techniques employed by big-iron compa-\nnies didn’t seem useful.\nThe secondary reason could be\nthat game developers don’t normally develop systems and\napplications which have decade-long maintenance expecta-\n\n\n22\nCHAPTER 1. DATA-ORIENTED DESIGN\ntions4 and therefore are less likely to be concerned about\nwhy their code should be encapsulated and protected or\nat least well documented.\nWhen game development was\nﬁrst ﬂourishing into larger studios in the late 1990’s, aca-\ndemic or corporate software engineering practices were seen\nas suspicious because wherever they were employed, there\nwas a dramatic drop in game performance, and whenever\nany prospective employees came from those industries, they\nfailed to impress. As games machines became more like the\nstandard micro-computers, and standard micro-computers\ndrew closer in design to the mainframes of old, the more ap-\nparent it became that some of those standard professional\nsoftware engineering practices could be useful.\nNow the\nscale of games has grown to match the hardware, but the\ngames industry has stopped looking at where those non-\ngame development practices led. As an industry, we should\nbe looking to where others have gone before us, and the clos-\nest set of academic and professional development techniques\nseem to be grounded in simulation and high volume data\nanalysis. We still have industry-speciﬁc challenges such as\nthe problems of high frequency highly heterogeneous trans-\nformational requirements that we experience in suﬃciently\nvoluminous AI environments, and we have the issue of user\nproximity in networked environments, such as the problems\nfaced by MMOs when they have location-based events, and\nbandwidth starts to hit n2 issues as everyone is trying to\nmessage everyone else.\nWith each successive generation, the number of devel-\noper hours to create a game has grown, which is why project\nmanagement and software engineering practices have be-\ncome standardised at the larger games companies.\nThere\nwas a time when game developers were seen as cutting-edge\nprogrammers, inventing new technology as the need arises,\nbut with the advent of less adventurous hardware (most no-\ntably in the x86 based recent 8thgenerations), there has been\na shift away from ingenious coding practices, and towards a\n4people at Blizzard Entertainment, Inc.\nlikely have something to say\nabout this\n\n\n1.6. THE FRAMEWORK\n23\nstandardised process. This means game development can be\ntuned to ensure the release date will coincide with market-\ning dates. There will always be an element of randomness\nin high proﬁle game development. There will always be an\nelement of innovation that virtually guarantees you will not\nbe able to predict how long the project, or at least one part\nof the project, will take. Even if data-oriented design isn’t\nneeded to make your game go faster, it can be used to make\nyour game development schedule more regular.\nPart of the diﬃculty in adding new and innovative fea-\ntures to a game is the data layout. If you need to change the\ndata layout for a game, it will need objects to be redesigned\nor extended in order to work within the existing framework.\nIf there is no new data, then a feature might require that\npreviously separate systems suddenly be able to talk to each\nother quite intimately. This coupling can often cause system-\nwide confusion with additional temporal coupling and corner\ncases so obscure they can only be reproduced one time in\na million. These odds might sound ﬁne to some developers,\nbut if you’re expecting to sell ﬁve to ﬁfty million copies of your\ngame, at one in a million, that’s ﬁve to ﬁfty people who will\nexperience the problem, can take a video of your game be-\nhaving oddly, post it on the YouTube, and call your company\nrubbish, or your developers lazy, because they hadn’t ﬁxed\nan obvious bug. Worse, what if the one in a million issue\nwas a way to circumvent in-app-purchases, and was repro-\nducible if you knew what to do and the steps start spreading\non Twitter, or maybe created an economy-destroying inﬂux\nof resources in a live MMO universe5. In the past, if you had\nsold ﬁve to ﬁfty million copies of your game, you wouldn’t\ncare, but with the advent of free-to-play games, ﬁve million\nplayers might be considered a good start, and poor reviews\ncoming in will curb the growth. IAP circumventions will kill\nyour income, and economy destruction will end you.\n5The\nwebcomic\nand\nanecdotes\nsite\nThe-Trenches\ndid\na\nse-\nquence\nof\nstrips\nin\na\nwebcomic\non\nthis,\nand\npointed\nout\nmany\nof\nthe\nissues\nwith\ntrying\nto\nﬁx\nit\nonce\nit\nhas\ngone\nlive\nhttp://www.trenchescomic.com/comic/post/apocalypse\n\n\n24\nCHAPTER 1. DATA-ORIENTED DESIGN\nBig iron developers had these same concerns back in the\n1970’s. Their software had to be built to high standards be-\ncause their programs would frequently be working on data\nconcerned with real money transactions.\nThey needed to\nwrite business logic that operated on the data, but most im-\nportant of all, they had to make sure the data was updated\nthrough a provably careful set of operations in order to main-\ntain its integrity. Database technology grew from the need to\nprocess stored data, to do complex analysis on it, to store and\nupdate it, and be able to guarantee it was valid at all times.\nTo do this, the ACID test was used to ensure atomicity, con-\nsistency, isolation, and durability. Atomicity was the test to\nensure all transactions would either complete or do noth-\ning. It could be very bad for a database to update only one\naccount in a ﬁnancial transaction. There could be money\nlost or created if a transaction was not atomic. Consistency\nwas added to ensure all the resultant state changes which\nshould happen during a transaction do happen, that is, all\ntriggers which should ﬁre, do ﬁre, even if the triggers cause\ntriggers recursively, with no limit. This would be highly im-\nportant if an account should be blocked after it has triggered\na form of fraud detection.\nIf a trigger has not ﬁred, then\nthe company using the database could risk being liable for\neven more than if they had stopped the account when they\nﬁrst detected fraud. Isolation is concerned with ensuring all\ntransactions which occur cannot cause any other transac-\ntions to diﬀer in behaviour. Normally this means that if two\ntransactions appear to work on the same data, they have to\nqueue up and not try to operate at the same time. Although\nthis is generally good, it does cause concurrency problems.\nFinally, durability. This was the second most important el-\nement of the four, as it has always been important to en-\nsure that once a transaction has completed, it remains so.\nIn database terminology, durability meant the transaction\nwould be guaranteed to have been stored in such a way that\nit would survive server crashes or power outages. This was\nimportant for networked computers where it would be im-\nportant to know what transactions had deﬁnitely happened\nwhen a server crashed or a connection dropped.\n\n\n1.7. CONCLUSIONS AND TAKEAWAYS\n25\nModern networked games also have to worry about highly\nimportant data like this. With non-free downloadable con-\ntent, consumers care about consistency. With consumable\ndownloadable content, users care a great deal about every\ntransaction. To provide much of the functionality required of\nthe database ACID test, game developers have gone back to\nlooking at how databases were designed to cope with these\nstrict requirements and found reference to staged commits,\nidempotent functions, techniques for concurrent develop-\nment, and a vast literature base on how to design tables for\na database.\n1.7\nConclusions and takeaways\nWe’ve talked about data-oriented design being a way to think\nabout and lay out your data and to make decisions about\nyour architecture.\nWe have two principles that can drive\nmany of the decisions we need to make when doing data-\noriented design. To ﬁnish the chapter, there are some take-\naways you can use immediately to begin your journey.\nConsider how your data is being inﬂuenced by what it’s\ncalled. Consider the possibility that the proximity of other\ndata can inﬂuence the meaning of your data, and in doing\nso, trap it in a model that inhibits ﬂexibility. For the consid-\neration of the ﬁrst principle, data is not the problem domain,\nit’s worth thinking about the following items.\n• What is tying your data together, is it a concept or im-\nplied meaning?\n• Is your data layout deﬁned by a single interpretation\nfrom a single point of view?\n• Think about how the data could be reinterpreted and\ncut along those lines.\n• What is it about the data that makes it uniquely impor-\ntant?\n\n\n26\nCHAPTER 1. DATA-ORIENTED DESIGN\nYou are not targeting an unknown device with unknow-\nable characteristics. Know your data, and know your tar-\nget hardware. To some extent, understand how much each\nstream of data matters, and who is consuming it. Under-\nstand the cost and potential value of improvements. Access\npatterns matter, as you cannot hit the cache if you’re ac-\ncessing things in a burst, then not touching them again for\na whole cycle of the application. For the consideration of the\nsecond principle, data is the type, frequency, quantity, shape,\nand probability, it’s worth thinking about the following items.\n• What is the smallest unit of memory on your target plat-\nform?6\n• When you read data, how much of it are you using?\n• How often do you need the data? Is it once, or a thou-\nsand times a frame?\n• How do you access the data? At random, or in a burst?\n• Are you always modifying the data, or just reading it?\nAre you modifying all of it?\n• Who does the data matter to, and what about it mat-\nters?\n• Find out the quality constraints of your solutions, in\nterms of bandwidth and latency.\n• What information do you have that isn’t in the data per-\nse? What is implicit?\n6On most machines in 2018, the smallest unit of memory is 64 byte\naligned lump called a cache line.\n",
      "page_number": 6,
      "chapter_number": 1,
      "summary": "This chapter covers data-oriented design. Key topics include data, design, and games. Whether it is, or is not a programming paradigm\nis seen as contentious.",
      "keywords": [
        "data",
        "Data-Oriented Design",
        "Design",
        "Data-Oriented",
        "game",
        "Object-oriented design",
        "problem",
        "problem domain",
        "objects",
        "data-oriented design approach",
        "data layout",
        "object-oriented",
        "hardware",
        "game development",
        "change"
      ],
      "concepts": [
        "data",
        "design",
        "games",
        "object",
        "develop",
        "change",
        "changed",
        "functional",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 55,
          "title": "Segment 55 (pages 489-497)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 2,
          "title": "Segment 2 (pages 19-41)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 2,
          "title": "Segment 2 (pages 10-17)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 9,
          "title": "Segment 9 (pages 72-79)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 1,
          "title": "Segment 1 (pages 1-10)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 2,
      "title": "Relational Databases",
      "start_page": 30,
      "end_page": 59,
      "detection_method": "regex_chapter",
      "content": "Chapter 2\nRelational Databases\nIn order to lay your data out better, it’s useful to have an\nunderstanding of the methods available to convert your ex-\nisting structures into something linear.\nThe problems we\nface when applying data-oriented approaches to existing\ncode and data layouts usually stem from the complexity of\nstate inherent in data-hiding or encapsulating programming\nparadigms. These paradigms hide away internal state so you\ndon’t have to think about it, but they hinder when it comes\nto reconﬁguring data layouts. This is not because they don’t\nabstract enough to allow changes to the underlying structure\nwithout impacting the correctness of the code that uses it,\nbut instead because they have connected and given meaning\nto the structure of the data. That type of coupling can be\nhard to remove.\nIn this chapter, we go over some of the pertinent parts\nof the relational model, relational database technology, and\nnormalisation, as these are examples of converting highly\ncomplex data structures and relationships into very clean\ncollections of linear storable data entries.\nYou certainly don’t have to move your data to a database\nstyle to do data-oriented design, but there are many places\n27\n\n\n28\nCHAPTER 2. RELATIONAL DATABASES\nwhere you will wish you had a simple array to work with, and\nthis chapter will help you by giving you an example of how\nyou can migrate from a web of connected complex objects to\na simpler to reason about relational model of arrays.\n2.1\nComplex state\nWhen you think about the data present in most software,\nit has some qualities of complexity or interconnectedness.\nWhen it comes to game development, there are many ways\nin which the game entities interact, and many ways in which\ntheir attached resources will need to feed through diﬀerent\nstages of processes to achieve the audio, visual and some-\ntimes haptic feedback necessary to fully immerse the player.\nFor many programmers brought up on object-oriented de-\nsign, the idea of reducing the types of structure available\ndown to just simple arrays, is virtually unthinkable. It’s very\nhard to go from working with objects, classes, templates, and\nmethods on encapsulated data to a world where you only\nhave access to linear containers.\nIn A Relational Model of Data for Large Shared Data\nBanks[?], Edgar F. Codd proposed the relational model to\nhandle the current and future needs of agents interacting\nwith data. He proposed a solution to structuring data for\ninsert, update, delete, and query operations. His proposal\nclaimed to reduce the need to maintain a deep understand-\ning of how the data was laid out to use it well. His proposal\nalso claimed to reduce the likelihood of introducing internal\ninconsistencies.\nThe relational model provided a framework, and in Fur-\nther Normalization of the Data Base Relational Model.[?],\nEdgar F. Codd introduced the fundamental terms of nor-\nmalisation we use to this day in a systematic approach to\nreducing the most complex of interconnected state informa-\ntion to linear lists of unique independent tuples.\n\n\n2.2. THE FRAMEWORK\n29\n2.2\nWhat can provide a computational\nframework for complex data?\nDatabases store highly complex data in a structured way\nand provide a language for transforming and generating re-\nports based on that data. The language, SQL, invented in\nthe 1970’s by Donald D. Chamberlin and Raymond F. Boyce\nat IBM, provides a method by which it is possible to store\ncomputable data while also maintaining data relationships\nfollowing in the form of the relational model. Games don’t\nhave simple computable data, they have classes and objects.\nThey have guns, swords, cars, gems, daily events, textures,\nsounds, and achievements. It is very easy to conclude that\ndatabase technology doesn’t work for the object-oriented ap-\nproach game developers use.\nThe data relationships in games can be highly complex,\nit would seem at ﬁrst glance that it doesn’t neatly ﬁt into\ndatabase rows.\nA CD collection easily ﬁts in a database,\nwith your albums neatly arranged in a single table.\nBut,\nmany game objects won’t ﬁt into rows of columns. For the\nuninitiated, it can be hard to ﬁnd the right table columns\nto describe a level ﬁle. Trying to ﬁnd the right columns to\ndescribe a car in a racing game can be a puzzle.\nDo you\nneed a column for each wheel? Do you need a column for\neach collision primitive, or just a column for the collision\nmesh?\nAn obvious answer could be that game data doesn’t ﬁt\nneatly into the database way of thinking.\nHowever, that’s\nonly because we’ve not normalised the data. To show how\nyou can convert from a network model, or hierarchical model\nto what we need, we will work through these normalisation\nsteps. We’ll start with a level ﬁle as we ﬁnd out how these\ndecades-old techniques can provide a very useful insight into\nwhat game data is really doing.\nWe shall discover that everything we do is already in a\n\n\n30\nCHAPTER 2. RELATIONAL DATABASES\ndatabase, but it wasn’t obvious to us because of how we\nstore our data. The structure of any data is a trade-oﬀbe-\ntween performance, readability, maintenance, future proof-\ning, extendibility, and reuse. For example, the most ﬂexible\ndatabase in common use is your ﬁlesystem. It has one ta-\nble with two columns. A primary key of the ﬁle path, and a\nstring for the data. This simple database system is the per-\nfect ﬁt for a completely future proof system. There’s nothing\nthat can’t be stored in a ﬁle. The more complex the tables\nget, the less future proof, and the less maintainable, but\nthe higher the performance and readability. For example, a\nﬁle has no documentation of its own, but the schema of a\ndatabase could be all that is required to understand a suﬃ-\nciently well-designed database. That’s how games don’t even\nappear to have databases. They are so complex, for the sake\nof performance, they have forgotten they are merely a data\ntransform. This sliding scale of complexity aﬀects scalability\ntoo, which is why some people have moved towards NoSQL\ndatabases, and document store types of data storage. These\nsystems are more like a ﬁlesystem where the documents are\naccessed by name, and have fewer limits on how they are\nstructured. This has been good for horizontal scalability, as\nit’s simpler to add more hardware when you don’t have to\nkeep your data consistent across multiple tables that might\nbe on diﬀerent machines. There may come a day when mem-\nory is so tightly tied to the closest physical CPU, or when\nmemory chips themselves get more processing power, or run-\nning 100 SoCs inside your desktop rig is more eﬀective than\na single monolithic CPU, that moving to document store at\nthe high-level could be beneﬁcial inside your app, but for\nnow, there do not seem to be any beneﬁts in that processing\nmodel for tasks on local hardware.\nWe’re not going to go into the details of the lowest level\nof how we utilise large data primitives such as meshes, tex-\ntures, sounds and such. For now, think of these raw assets\n(sounds, textures, vertex buﬀers, etc.) as primitives, much\nlike the integers, ﬂoating point numbers, strings and boolean\nvalues we shall be working with. We do this because the re-\n\n\n2.2. THE FRAMEWORK\n31\nlational model calls for atomicity when working with data.\nWhat is and is not atomic has been debated without an abso-\nlute answer becoming clear, but for the intents of developing\nsoftware intended for human consumption, the granularity\ncan be rooted in considering the data from the perspective\nof human perception. There are existing APIs that present\nstrings in various ways depending on how they are used,\nfor example the diﬀerence between human-readable strings\n(usually UTF-8) and ASCII strings for debugging.\nAdding\nsounds, textures, and meshes to this seems quite natural\nonce you realise all these things are resources which if cut\ninto smaller pieces begin to lose what it is that makes them\nwhat they are. For example, half of a sentence is a lot less\nuseful than a whole one, and loses integrity by disassocia-\ntion. A slice of a sentence is clearly not reusable in any mean-\ningful way with another random slice of a diﬀerent sentence.\nEven subtitles are split along meaningful boundaries, and\nit’s this idea of meaningful boundary that gives us the our\ndeﬁnition of atomicity for software developed for humans. To\nthis end, when working with your data, when you’re normal-\nising, try to stay at the level of nouns, the nameable pieces.\nA whole song can be an atom, but so is a single tick sound\nof a clock. A whole page of text is an atom, but so is the\nplayer’s gamer-tag.\n\n\n32\nCHAPTER 2. RELATIONAL DATABASES\n2.3\nNormalising your data\nFigure 2.1: Visual representation of the setup script\nWe’re going to work with a level ﬁle for a game where you hunt\nfor keys to unlock doors in order to get to the exit room. The\nlevel ﬁle is a sequence of script calls which create and con-\nﬁgure a collection of diﬀerent game objects which represent\na playable level of the game, and the relationships between\nthose objects. First, we’ll assume it contains rooms (some\ntrapped, some not), with doors leading to other rooms which\ncan be locked. It will also contain a set of pickups, some let\nthe player unlock doors, some aﬀect the player’s stats (like\nhealth potions and armour), and all the rooms have lovely\ntextured meshes, as do all the pickups. One of the rooms is\nmarked as the exit, and one has a player start point.\n1\n//\ncreate\nrooms , pickups , and\nother\nthings .\n2\nMesh\nmsh_room = LoadMesh( \" roommesh \" );\n3\nMesh\nmsh_roomstart = LoadMesh( \" roommeshstart \" );\n4\nMesh\nmsh_roomtrapped = LoadMesh( \" roommeshtrapped \" );\n5\nMesh\nmsh_key = LoadMesh( \" keymesh \" );\n6\nMesh\nmsh_pot = LoadMesh( \" potionmesh \" );\n7\nMesh\nmsh_arm = LoadMesh( \" armourmesh \" );\n8\n// ...\n9\nTexture\ntex_room = LoadTexture ( \" roomtexture \" );\n10\nTexture\ntex_roomstart = LoadTexture ( \" roomtexturestart \" );\n11\nTexture\ntex_roomtrapped = LoadTexture( \" roomtexturetrapped \" );\n12\nTexture\ntex_key = LoadTexture( \" keytexture \" );\n13\nTexture\ntex_pot = LoadTexture( \" potiontexture \" );\n14\nTexture\ntex_arm = LoadTexture( \" armourtexture \" );\n\n\n2.3. NORMALISING YOUR DATA\n33\n15\n16\nAnim\nanim_keybob = LoadAnim( \" keybobanim \" );\n17\n// ...\n18\nPickupID\nk1 = CreatePickup ( TYPE_KEY , msh_key , tex_key ,\nTintColourCopper , anim_keybob );\n19\nPickupID\nk2 = CreatePickup ( TYPE_KEY , msh_key , tex_key ,\nTintColourSilver , anim_keybob\n);\n20\nPickupID\nk3 = CreatePickup ( TYPE_KEY , msh_key , tex_key ,\nTintColourGold , anim_keybob\n);\n21\nPickupID\np1 = CreatePickup ( TYPE_POTION , msh_pot , tex_pot ,\nTintColourGreen );\n22\nPickupID\np2 = CreatePickup ( TYPE_POTION , msh_pot , tex_pot ,\nTintColourPurple );\n23\nPickupID\na1 = CreatePickup ( TYPE_ARMOUR , msh_arm , tex_arm );\n24\n// ...\n25\nRoom r1 = CreateRoom( WorldPos (0 ,0), msh_roomstart , tex_roomstart\n);\n26\nRoom r2 = CreateRoom( WorldPos (-20,0), msh_roomtrapped ,\ntex_roomtrapped , HPDamage (10) );\n27\nRoom r3 = CreateRoom( WorldPos ( -10 ,20), msh_room , tex_room );\n28\nRoom r4 = CreateRoom( WorldPos ( -30 ,20), msh_room , tex_room );\n29\nRoom r5 = CreateRoom( WorldPos (20 ,10) , msh_roomtrapped ,\ntex_roomtrapped , HPDamage (25) );\n30\n// ...\n31\nAddDoor( r1 , r2 );\n32\nAddDoor( r1 , r3 , k1 );\n33\nSetRoomAsSpecial ( r1 , E_STARTINGROOM , WorldPos (1 ,1) );\n34\n//\n35\nAddPickup( r2 , k1 , WorldPos (-18,2));\n36\nAddDoor( r2 , r1 );\n37\nAddDoor( r2 , r4 , k2 );\n38\n// ...\n39\nAddPickup( r3 , k2 , WorldPos (-8,12));\n40\nAddPickup( r3 , p1 , WorldPos (-7,13));\n41\nAddPickup( r3 , a1 , WorldPos (-8,14));\n42\nAddDoor( r3 , r1 );\n43\nAddDoor( r3 , r2 );\n44\nAddDoor( r3 , r5 , k3 );\n45\n// ...\n46\nAddDoor( r4 , r2 );\n47\nAddPickup( r4 , k3 , WorldPos ( -28 ,14));\n48\nAddPickup( r4 , p2 , WorldPos ( -27 ,13));\n49\n// ...\n50\nSetRoomAsSpecial ( r5 , E_EXITROOM );\nListing 2.1: A setup script\nIn this setup script (Listing 2.1) we load some resources,\ncreate some pickup prototypes, build up a few rooms, add\nsome instances to the rooms, and then link things together.\nHere we also see a standard solution to the problem of things\nwhich reference each other. We create the rooms before we\nconnect them to each other because before they exist we\ncan’t. When we create entities in C++, we assume they are\nbound to memory, and the only eﬃcient way to reference\nthem is through pointers, but we cannot know where they\nexist in memory before we allocate them, and we cannot allo-\ncate them before ﬁlling them out with their data as the allo-\ncation and initialisation are bound to each other through the\n‘new’ mechanism. This means we have diﬃculty describing\nrelationships between objects before they exist and have to\n\n\n34\nCHAPTER 2. RELATIONAL DATABASES\nstagger the creation of content into phases of setting up and\nconnecting things together.\nTo bring this setup script into a usable database-like for-\nmat, or relational model, we will need to normalise it. When\nputting things in a relational model of any sort, it needs to\nbe in tables. In the ﬁrst step you take all the data and put\nit into a very messy, but hopefully complete, table design. In\nour case we take the form of the data from the object cre-\nation script and ﬁt it into a table. The asset loading can be\ndirectly translated into tables, as can be seen in table 2.1\nMeshes\nMeshID\nMeshName\nmsh rm\n\"roommesh\"\nmsh rmstart\n\"roommeshstart\"\nmsh rmtrap\n\"roommeshtrapped\"\nmsh key\n\"keymesh\"\nmsh pot\n\"potionmesh\"\nmsh arm\n\"armourmesh\"\nTextures\nTextureID\nTextureName\ntex rm\n\"roomtexture\"\ntex rmstart\n\"roomtexturestart\"\ntex rmtrapped\n\"roomtexturetrapped\"\ntex key\n\"keytexture\"\ntex pot\n\"potiontexture\"\ntex arm\n\"armourtexture\"\nAnimations\nAnimID\nAnimName\nanim keybob\n\"keybobanim\"\nTable 2.1: Initial tables created by converting asset load calls\nPrimed with this data, it’s now possible for us to create\nthe Pickups. We convert the calls to CreatePickup into the\ntables in table 2.2. Notice that there was a pickup which\ndid not specify a colour tint, and this means we need to use\na NULL to represent not giving details about that aspect of\nthe row. The same applies to animations. Only keys had\nanimations, so there needs to be NULL entries for all non-\nkey rows.\n\n\n2.3. NORMALISING YOUR DATA\n35\nPickups\nPickupID\nMeshID\nTextureID\nPickupType\nColourTint\nAnim\nk1\nmsh key\ntex key\nKEY\nCopper\nanim keybob\nk2\nmsh key\ntex key\nKEY\nSilver\nanim keybob\nk3\nmsh key\ntex key\nKEY\nGold\nanim keybob\np1\nmsh pot\ntex pot\nPOTION\nGreen\nNULL\np2\nmsh pot\ntex pot\nPOTION\nPurple\nNULL\na1\nmsh arm\ntex arm\nARMOUR\nNULL\nNULL\nTable 2.2: Initial tables created by converting CreatePickup\ncalls\nOnce we have loaded the assets and have created the\npickup prototypes, we move onto creating a table for rooms.\nWe need to invent attributes as necessary using NULL ev-\nerywhere that an instance doesn’t have that attribute. We\nconvert the calls to CreateRoom, AddDoor, SetRoomAsSpe-\ncial, and AddPickup, to columns in the Rooms table. See\ntable 2.3 for one way to build up a table that represents all\nthose setup function calls.\nRooms\nRoomID\nMeshID\nTextureID\nWorldPos\nPickups\n...\nr1\nmsh rmstart\ntex rmstart\n0, 0\nNULL\n...\nr2\nmsh rmtrap\ntex rmtrap\n-20,10\nk1\n...\nr3\nmsh rm\ntex rm\n-10,20\nk2,p1,a1\n...\nr4\nmsh rm\ntex rm\n-30,20\nk3,p2\n...\nr5\nmsh rmtrap\ntex rmtrap\n20,10\nNULL\n...\n...\nDoorsTo\nLocked\nIsStart\nIsEnd\n...\nNULL\nr2,r3\nr3 with k1\ntrue WorldPos(1,1)\nfalse\n...\n10HP\nr1,r4\nr4 with k2\nfalse\nfalse\n...\nNULL\nr1,r2,r5\nr5 with k3\nfalse\nfalse\n...\nNULL\nr2\nfalse\nfalse\n...\n25HP\nNULL\nfalse\ntrue\nTable 2.3: Initial table created by converting CreateRoom\nand other calls.\nOnce we have taken the construction script and gener-\nated these ﬁrst tables, we ﬁnd the tables contain a lot of\nNULLs. The NULLs in the rows replace the optional content\nof the objects. If an object instance doesn’t have a certain\nattribute then we replace those features with NULLs. There\nare also elements which contain more than one item of data.\nHaving multiple doors per room is tricky to handle in this\n\n\n36\nCHAPTER 2. RELATIONAL DATABASES\ntable.\nHow would you ﬁgure out what doors it had?\nThe\nsame goes for whether the door is locked, and whether there\nare any pickups. The ﬁrst stage in normalising is going to\nbe reducing the number of elements in each cell to 1, and\nincreasing it to 1 where it’s currently NULL.\n2.4\nNormalisation\nBack when SQL was ﬁrst created there were only three well-\ndeﬁned stages of data normalisation. There are many more\nnow, including six numbered normal forms. To get the most\nout of a database, it is important to know most of them, or\nat least get a feel for why they exist. They teach you about\ndata dependency and can hint at reinterpretations of your\ndata layout. For game structures, BCNF (Boyce-Codd nor-\nmal form is explained later) is probably as far as you nor-\nmally would need to take your methodical process. Beyond\nthat, you might wish to normalise your data for hot/cold ac-\ncess patterns, but that kind of normalisation is not part of\nthe standard literature on database normalisation. If you’re\ninterested in more than this book covers on the subject, a\nvery good read, and one which introduces the phrase “The\nkey, the whole key, and nothing but the key.” is the article\nA Simple Guide to Five Normal Forms in Relational Database\nTheory[?] by William Kent.\nIf a table is in ﬁrst normal form, then every cell contains\none and only one atomic value. That is, no arrays of values,\nand no NULL entries. First normal form also requires every\nrow be distinct. For those unaware of what a primary key is,\nwe shall discuss that ﬁrst.\n\n\n2.4. NORMALISATION\n37\n2.4.1\nPrimary keys\nAll tables are made up of rows and columns. In a database,\neach row must be unique.\nThis constraint has important\nconsequences. When you have normalised your data, it be-\ncomes clear why duplicate rows don’t make sense, but for\nnow, from a computer programming point of view, consider\ntables to be more like sets, where the whole row is the set\nvalue. This is very close to reality, as sets are also not or-\ndered, and a database table is not ordered either. There is\nalways some diﬀerentiation between rows, even if a database\nmanagement system (DBMS) has to rely on hidden row ID\nvalues. It is better to not rely on this as databases work more\neﬃciently when the way in which they are used matches\ntheir design. All tables need a key. The key is often used to\norder the sorting of the table in physical media, to help op-\ntimise queries. For this reason, the key needs to be unique,\nbut as small as possible. You can think of the key as the\nkey in a map or dictionary. Because of the uniqueness rule,\nevery table has an implicit key because the table can use the\ncombination of all the columns at once to identify each row\nuniquely. That is, the key, or the unique lookup, which is\nthe primary key for a table, can be deﬁned as the totality of\nthe whole row. If the row is unique, then the primary key is\nunique. Normally, we try to avoid using the whole row as the\nprimary key, but sometimes, it’s actually our only choice. We\nwill come across examples of that later.\nFor example, in the mesh table, the combination of\nmeshID and ﬁlename is guaranteed to be unique.\nHow-\never, currently it’s only guaranteed to be unique because\nwe have presumed that the meshID is unique. If it was the\nsame mesh, loaded from the same ﬁle, it could still have a\ndiﬀerent meshID. The same can be said for the textureID\nand ﬁlename in the textures table. From the table 2.2 it’s\npossible to see how we could use the type, mesh, texture,\ntint and animation to uniquely deﬁne each Pickup prototype.\nNow consider rooms.\nIf you use all the columns other\n\n\n38\nCHAPTER 2. RELATIONAL DATABASES\nthan the RoomID of the room table, you will ﬁnd the combi-\nnation can be used to uniquely deﬁne the room. If you con-\nsider an alternative, where a row had the same combination\nof values making up the room, it would in fact be describing\nthe same room. From this, it can be claimed that the Roo-\nmID is being used as an alias for the rest of the data. We have\nstuck the RoomID in the table, but where did it come from?\nTo start with, it came from the setup script. The script had\na RoomID, but we didn’t need it at that stage. We needed it\nfor the destination of the doors. In another situation, where\nnothing connected logically to the room, we would not need\na RoomID as we would not need an alias to it.\nA primary key must be unique. RoomID is an example of\na primary key because it uniquely describes the room. It is\nan alias in this sense as it contains no data in and of itself,\nbut merely acts as a handle. In some cases the primary key\nis information too, which again, we will meet later.\nAs a bit of an aside, the idea that a row in a database\nis also the key can be a core concept worth spending time\nthinking about. If a database table is a set, when you in-\nsert a record, you’re actually just asking that one particular\ncombination of data is being recorded as existing. It is as if\na database table is a very sparse set from an extremely large\ndomain of possible values. This can be useful because you\nmay notice that under some circumstances, the set of possi-\nble values isn’t very large, and your table can be more easily\ndeﬁned as a bit set. As an example, consider a table which\nlists the players in an MMO that are online right now. For\nan MMO that shards its servers, there can be limits in the\nearly thousands for the number of unique players on each\nserver. In that case, it may be easier to store the currently\nonline players as a bit set. If there are at most 10,000 players\nonline, and only 1000 players online at any one time, then\nthe bitset representation would take up 1.25kb of memory,\nwhereas storing the online players as a list of IDs, would re-\nquire at least 2kb of data if their IDs were shrunk to shorts,\nor 4kb if they had 32bit IDs to keep them unique across\n\n\n2.4. NORMALISATION\n39\nmultiple servers. The other beneﬁt in this case is the perfor-\nmance of queries into the data. To quickly access the ID in\nthe list, you need it to remain sorted. The best case then is\nO(log n). In the bitset variant, it’s O(1).\nGoing back to the asset table, an important and useful\ndetail when we talk about the meshID and mesh ﬁlename is\nthat even though there could be two diﬀerent meshIDs point-\ning at the same ﬁle, most programmers would intuitively un-\nderstand that a single meshID was unlikely to point at two\ndiﬀerent mesh ﬁles.\nBecause of this asymmetry, you can\ndeduce, the column that seems more likely to be unique will\nalso be the column you can use as the primary key. We’ll\nchoose the meshID as it is easier to manipulate and is un-\nlikely to have more than one meaning or usage, but remem-\nber, we could have chosen the ﬁlename and gone without the\nmeshID altogether.\nIf we settle on TextureID, PickupID, and RoomID as the\nprimary keys for those tables, we can then look at continuing\non to ﬁrst normal form. We’re using t1, m2, r3, etc. to show\ntypesafe ID values, but in reality, these can all be simple\nintegers.\nThe idea here is to remain readable, but it also\nshows that each type can have unique IDs for that type, but\nhave common IDs with another. For example, a room may\nhave an integer ID value of 0, but so may a texture. It can\nbe beneﬁcial to have IDs which are unique across types, as\nthat can help debugging, and using the top few bits in that\ncase can be helpful. If you’re unlikely to have more than a\nmillion entities per class of entity, then you have enough bits\nto handle over a thousand distinct classes.\n2.4.2\n1st Normal Form\nFirst normal form can be described as making sure the ta-\nbles are not sparse. We require that there be no NULL point-\ners and that there be no arrays of data in each element of\ndata. This can be performed as a process of moving the re-\n\n\n40\nCHAPTER 2. RELATIONAL DATABASES\npeats and all the optional content to other tables. Anywhere\nthere is a NULL, it implies optional content. Our ﬁrst ﬁx is\ngoing to be the Pickups table, it has optional ColourTint and\nAnimation elements. We invent a new table PickupTint, and\nuse the primary key of the Pickup as the primary key of the\nnew table. We also invent a new table PickupAnim. Table\n2.4 shows the result of the transformation, and note we no\nlonger have any NULL entries.\nPickups\nPickupID\nMeshID\nTextureID\nPickupType\nk1\nmsh key\ntex key\nKEY\nk2\nmsh key\ntex key\nKEY\nk3\nmsh key\ntex key\nKEY\np1\nmsh mpot\ntex pot\nPOTION\np2\nmsh mpot\ntex pot\nPOTION\na1\nmsh marm\ntex arm\nARMOUR\nPickupTints\nPickupID\nColourTint\nk1\nCopper\nk2\nSilver\nk3\nGold\np1\nGreen\np2\nPurple\nPickupAnims\nPickupID\nAnim\nk1\nanim keybob\nk2\nanim keybob\nk3\nanim keybob\nTable 2.4: Pickups in 1NF\nTwo things become evident at this point, ﬁrstly that nor-\nmalisation appears to create more tables and fewer columns\nin each table, secondly that there are only rows for things\nwhich matter. The former is worrisome, as it means more\nmemory usage. The latter is interesting as when using an\nobject-oriented approach, we allow objects to optionally have\nattributes. Optional attributes cause us to check they are\nnot NULL before continuing. If we store data like this, then\nwe know everything is not NULL. Moving away from having\nto do a null check at all will make your code more concise,\n\n\n2.4. NORMALISATION\n41\nand you have less state to consider when trying to reason\nabout your systems.\nLet’s move onto the Rooms table. In there we saw single\nelements that contained multiple atomic values. We need\nto remove all elements from this table that do not conform\nto the rules of ﬁrst normal form. First, we remove reference\nto the pickups, as they had various quantities of elements,\nfrom none to many. Then we must consider the traps, as\neven though there was only ever one trap, there wasn’t al-\nways a trap. Finally, we must strip out the doors, as even\nthough every room has a door, they often had more than\none. Remember that the rule is one and only one entry in\nevery meeting of row and column. In table 2.5 it shows how\nwe only keep columns that are in a one to one relationship\nwith the RoomID.\nRooms\nRoomID\nMeshID\nTextureID\nWorldPos\nIsStart\nIsExit\nr1\nmsh rmstart\ntex rmstart\n0,0\ntrue\nfalse\nr2\nmsh rmtrap\ntex rmtrap\n-20,0\nfalse\nfalse\nr3\nmsh rm\ntex rm\n-10,20\nfalse\nfalse\nr4\nmsh rm\ntex rm\n-30,20\nfalse\nfalse\nr5\nmsh rmtrap\ntex rmtrap\n20,10\nfalse\ntrue\nTable 2.5: Rooms table now in 1NF\nNow we will make new tables for Pickups, Doors, and\nTraps. In table 2.6 we see many decisions made to satisfy\nthe ﬁrst normal form. We have split out the array like ele-\nments into separate rows. Note the use of multiple rows to\nspecify the numerous pickups all in the same room. We see\nthat doors now need two tables. The ﬁrst table to identify\nwhere the doors are, and where they lead. The second ta-\nble seems to do the same, but doesn’t cover all doors, only\nthe ones that are locked. What’s actually happening here is\na need to identify doors by their primary key in the locked\ndoors table. If you look at the Doors table, you can immedi-\nately tell that neither column is a candidate for the primary\nkey, as neither contain only unique values. What is unique\nthough is the combination of values, so the primary key is\nmade up of both columns. In the table LockedDoors, From-\n\n\n42\nCHAPTER 2. RELATIONAL DATABASES\nRoom and ToRoom are being used as a lookup into the Doors\ntable. This is often called a foreign key, meaning that there\nexists a table for which these columns directly map to that\ntable’s primary key. In this case, the primary key is made up\nof two columns, so the LockedDoors table has a large foreign\nkey and a small bit of extra detail about that entry in the\nforeign table.\nPickupInstances\nRoomID\nPickupID\nr2\nk1\nr3\nk2\nr3\na1\nr3\np1\nr4\nk3\nr4\np2\nDoors\nFromRoom\nToRoom\nr1\nr2\nr1\nr3\nr2\nr1\nr2\nr4\nr3\nr1\nr3\nr2\nr3\nr5\nr4\nr2\nLockedDoors\nFromRoom\nToRoom\nLockedWith\nr1\nr3\nk1\nr2\nr4\nk2\nr3\nr5\nk3\nTraps\nRoomID\nTrapped\nr2\n10hp\nr5\n25hp\nTable 2.6: Additional tables to support 1NF rooms\nLaying out the data in this way takes less space in larger\nprojects as the number of NULL entries or arrays would have\nonly increased with increased complexity of the level ﬁle. By\nlaying out the data this way, we can add new features with-\nout having to revisit the original objects. For example, if we\n\n\n2.4. NORMALISATION\n43\nwanted to add monsters, normally we would not only have\nto add a new object for the monsters, but also add them to\nthe room objects. In this format, all we need to do is add a\nnew table such as in table 2.7.\nMonsters\nMonsterID\nAttack\nHitPoints\nStartRoom\nM1\n2\n5\nr3\nM2\n2\n5\nr4\nTable 2.7: Adding monsters\nAnd now we have information about the monster and\nwhat room it starts in without touching any of the original\nlevel data.\n2.4.3\n2nd Normal Form\nSecond normal form is about trying to pull out columns that\ndon’t depend on only a part of the primary key. This can be\ncaused by having a table that requires a compound primary\nkey, and some attributes of the row only being dependent\non part of that compound key. An example might be where\nyou have weapons deﬁned by quality and type, and the table\nlooks like that in table 2.8, what you can see is that the\nprimary key must be compound, as there are no columns\nwith unique values here.\nWeapons\nWeaponType\nWeaponQuality\nWeaponDamage\nWeaponDamageType\nSword\nRusty\n2d4\nSlashing\nSword\nAverage\n2d6\nSlashing\nSword\nMasterwork\n2d8\nSlashing\nLance\nAverage\n2d6\nPiercing\nLance\nMasterwork\n3d6\nPiercing\nHammer\nRusty\n2d4\nCrushing\nHammer\nAverage\n2d4+4\nCrushing\nTable 2.8: Weapons in 1NF\n\n\n44\nCHAPTER 2. RELATIONAL DATABASES\nIt makes sense for us looking at the table that the primary\nkey should be the compound of WeaponType and Weapon-\nQuality, as it’s a fairly obvious move for us to want to look\nup damage amount and damage type values based on what\nweapon we’re using.\nIt’s also possible to notice that the\nDamageType does not depend on the WeaponQuality, and\nin fact only depends on the WeaponType.\nThat’s what we\nmean about depending on part of the key. Even though each\nweapon is deﬁned in 1NF, the type of damage being dealt\ncurrently relies on too little of the primary key to allow this\ntable to remain in 2NF. We split the table out in table 2.9\nto remove the column that only relies on WeaponType. If we\nfound a weapon that changed DamageType based on quality,\nthen we would put the table back the way it was. An example\nmight be the badly damaged morningstar, which no longer\ndoes piercing damage, but only bludgeons.\nWeapons\nWeaponType\nWeaponQuality\nWeaponDamage\nSword\nRusty\n2d4\nSword\nAverage\n2d6\nSword\nMasterwork\n2d8\nLance\nAverage\n2d6\nLance\nMasterwork\n3d6\nHammer\nRusty\n2d4\nHammer\nAverage\n2d4+4\nWeaponDamageTypes\nWeaponType\nWeaponDamageType\nSword\nSlashing\nLance\nPiercing\nHammer\nCrushing\nTable 2.9: Weapons in 2NF\nWhen considering second normal form for our level data,\nit’s worth understanding some shortcuts we made in mov-\ning to ﬁrst normal form. Firstly, we didn’t necessarily need\nto move to having a PickupID, but instead could have refer-\nenced the pickup prototype by PickupType and TintColour,\nbut that was cumbersome, and would have introduced a\nNULL as a requirement as the armour doesn’t have a tint.\n\n\n2.4. NORMALISATION\n45\nTable 2.10 shows how this may have looked, but the compli-\ncations with making this connect to the rooms was the de-\nciding factor for introducing a PickupID. Without the pickup\nID, the only way to put the pickups in rooms was to have\ntwo tables. One table for pickups with tints, and another\nfor pickups without tints. This is not absurd, but it doesn’t\nseem clean in this particular situation. There will be cases\nwhere this would be the right approach.\nPickups\nMeshID\nTextureID\nPickupType\nColourTint\nmkey\ntkey\nKEY\nCopper\nmkey\ntkey\nKEY\nSilver\nmkey\ntkey\nKEY\nGold\nmpot\ntpot\nPOTION\nGreen\nmpot\ntpot\nPOTION\nPurple\nmarm\ntarm\nARMOUR\nNULL\nNormalising to 1NF:\nPickups 1NF\nPickupType\nMeshID\nTextureID\nKEY\nmkey\ntkey\nPOTION\nmpot\ntpot\nARMOUR\nmarm\ntarm\nTintedPickups 1NF\nPickupType\nColourTint\nKEY\nCopper\nKEY\nSilver\nKEY\nGold\nPOTION\nGreen\nPOTION\nPurple\nTable 2.10: An alternative 0NF and 1NF for Pickups\nIf we now revisit the Pickup table from before, with the\nknowledge that the PickupID is an alias for the combina-\ntion of PickupType and ColourTint, then we can apply the\nsame transform we see when moving to 1NF in the alterna-\ntive form. That is, of moving MeshID and TextureID to their\nown table, and depending only on PickupType, not the com-\npound key of PickupType and ColourTint.\nIn table 2.11, the assets elements now rely on the whole\n\n\n46\nCHAPTER 2. RELATIONAL DATABASES\nof their compound key, not just part of it.\nPickups\nPickupID\nPickupType\nk1\nKEY\nk2\nKEY\nk3\nKEY\np1\nPOTION\np2\nPOTION\na1\nARMOUR\nPickupTints\nPickupID\nColourTint\nk1\nCopper\nk2\nSilver\nk3\nGold\np1\nGreen\np2\nPurple\nPickupAssets\nPickupType\nMeshID\nTextureID\nKEY\nmsh key\ntex key\nPOTION\nmsh pot\ntex pot\nARMOUR\nmsh arm\ntex arm\nPickupAnims\nPickupType\nAnimID\nKEY\nkey bob\nTable 2.11: Pickups in 2NF\nWe can’t apply the same normalisation of table data to\nthe Room table. The Room table’s RoomID is an alias for\nthe whole row, possibly, or just the WorldPos, but in both\ncases, it’s possible to see a correlation between the MeshID,\nTextureID, and the value of IsStart. The problem is that it\nalso relies on the existence of entries in an external table.\nIf we take the table as it is, the MeshID and TextureID do\nnot directly rely on anything other than the RoomID in this\nform.\n\n\n2.4. NORMALISATION\n47\n2.4.4\n3rd Normal Form\nWhen considering further normalisation, we ﬁrst have to re-\nmove any transitive dependencies. By this we mean any de-\npendencies on the primary key only via another column in\nthe row. We can do a quick scan of the current tables and\nsee all resources references refer to pairs of MeshID and Tex-\ntureID values.\nAnything that uses a MeshID will use the\nmatching TextureID. This means we can pull out one or the\nother from all the tables that use them, and look them up\nvia a table of pairs. We shall arbitrarily choose to use the\nTextureID as the main lookup, and slim down to one table\nfor meshes and textures.\nTexturesAndMeshes\nTextureID\nTextureName\nMeshName\ntex room\n\"roomtexture\"\n\"roommesh\"\ntex roomstart\n\"roomtexturestart\"\n\"roommeshstart\"\ntex roomtrap\n\"roomtexturetrapped\"\n\"roommeshtrapped\"\ntex key\n\"keytexture\"\n\"keymesh\"\ntex pot\n\"potiontexture\"\n\"potionmesh\"\ntex arm\n\"armourtexture\"\n\"armourmesh\"\nTable 2.12: Assets in 3NF\n2.4.5\nBoyce-Codd Normal Form\nThe assets used for a room are based on whether it is\ntrapped, or it’s the starting room. This is a functional de-\npendency, not a direct one, so we have to introduce a new\ncolumn to describe that aspect, and it’s going to require\ngenerating intermediate data to drive the value query, but\nit makes real the lack of direct link between the room and\nthe assets.\nThe rooms can be trapped, and can be start-\ning rooms, and the assets connected to the room depend on\nthose attributes, not the room itself. This is why Boyce-Codd\nNormal Form, or BCNF, can be thought of as the functionally\ndependent normalisation stage.\n\n\n48\nCHAPTER 2. RELATIONAL DATABASES\nRooms\nRoomID\nWorldPos\nIsStart\nIsExit\nr1\n0,0\ntrue\nfalse\nr2\n-20,10\nfalse\nfalse\nr3\n-10,20\nfalse\nfalse\nr4\n-30,20\nfalse\nfalse\nr5\n20,10\nfalse\ntrue\nRooms\nIsStart\nHasTrap\nTextureID\ntrue\nfalse\ntex rmstart\nfalse\nfalse\ntex rm\nfalse\ntrue\ntex rmtrap\nTable 2.13: Rooms table now in BCNF\n2.4.6\nDomain Key / Knowledge\nDomain key normal form is normally thought of as the last\nnormal form, but for developing eﬃcient data structures, it’s\none of the things best studied early and often. The term do-\nmain knowledge is preferable when writing code as it makes\nmore immediate sense and encourages use outside of keys\nand tables. Domain knowledge is the idea that data depends\non other data, but only given information about the domain\nin which it resides. Domain knowledge can be as simple as\nawareness of a colloquialism for something, such as know-\ning that a certain number of degrees Celsius or Fahrenheit is\nhot, or whether some SI unit relates to a man-made concept\nsuch as 100m/s being rather quick.\nAn example of where domain knowledge can help with\ncatching issues can be with putting human interpreta-\ntions of values into asserts. Consider an assert for catching\nphysics systems blowups. What is a valid expected range of\nvalues for acceleration? Multiply it by ten, and you have a\ncheck for when everything goes a bit crazy.\nSome applications avoid the traditional inaccurate and\nerratic countdown timer, and resort to human-readable\nforms such as in a few minutes or time to grab a coﬀee,\nhowever domain knowledge isn’t just about presenting a hu-\n\n\n2.4. NORMALISATION\n49\nman interpretation of data. For example things such as the\nspeed of sound, of light, speed limits and average speed of\ntraﬃc on a given road network, psychoacoustic properties,\nthe boiling point of water, and how long it takes a human\nto react to any given visual input.\nAll these facts may be\nuseful in some way, but can only be put into an application\nif the programmer adds it speciﬁcally as procedural domain\nknowledge or as an attribute of a speciﬁc instance.\nLooking at our level data, one thing we can guess at is\nthe asset ﬁlenames based on the basic name. The textures\nand meshes share a common format, so moving away from\nstoring the full ﬁlenames could give us a Domain Knowledge\nnormalised form.\nAssetLookupTable\nAssetID\nStubbedName\nast room\n\"room%s\"\nast roomstart\n\"room%sstart\"\nast roomtrap\n\"room%strapped\"\nast key\n\"key%s\"\nast pot\n\"potion%s\"\nast arm\n\"armour%s\"\nTable 2.14: Assets in DKNF\nDomain knowledge is useful because it allows us to lose\nsome otherwise unnecessarily stored data. It is a compiler’s\njob to analyse the produced output of code (the abstract syn-\ntax tree) to then provide itself with data upon which it can\ninfer and use its domain knowledge about what operations\ncan be omitted, reordered, or transformed to produce faster\nor cheaper assembly.\nIt’s our job to do the same for ele-\nments the compiler can’t know about, such as the chance\nthat someone in the middle of a ﬁght is going to be able to\nhear a coin drop in another room.\nDomain knowledge is what leads to inventions such as\nJPEG and MP3. Thinking about what is possible, what is\npossible to perceive, and what can possibly be aﬀected by\nuser actions, can reduce the amount of work done by an\napplication, and can reduce its complexity. When you jump\n\n\n50\nCHAPTER 2. RELATIONAL DATABASES\nin a game with physics, we don’t move the world down by\nfractions of a nanometre to represent the opposite reaction\ncaused by the forces applied.\n2.4.7\nReﬂections\nWhat we see here as we normalise our data is a tendency\nto split data by dependency. Looking at many third party\nengines and APIs, you can see some parallels with the re-\nsults of these normalisations. It’s unlikely that the people\ninvolved in the design and evolution of these engines took\ntheir data and applied database normalisation techniques,\nbut sometimes the separations between object and compo-\nnents of objects can be obvious enough that you don’t need a\nformal technique in order to realise some positive structural\nchanges.\nIn some games, the entity object is not just an object that\ncan be anything, but is instead a speciﬁc subset of the types\nof entity involved in the game.\nFor example, in one game\nthere might be a class for the player character, and one for\neach major type of enemy character, and another for vehi-\ncles. The player may have diﬀerent attributes to other enti-\nties, such as lacking AI controls, or having player controls,\nor having regenerating health, or having ammo. This object-\noriented approach puts a line, invisible to the user, but in-\ntrusive to the developer, between classes of object and their\ninstances. It is intrusive because when classes touch, they\nhave to adapt to each other. When they don’t reside in the\nsame hierarchy, they have to work through abstraction lay-\ners to message each other. The amount of code required to\nbridge these gaps can be small, but they always introduce\ncomplexity.\nWhen developing software, this usually manifests as time\nspent writing out templated code that can operate on multi-\nple classes rather than refactoring the classes involved into\nmore discrete components. This could be considered wasted\n\n\n2.5. OPERATIONS\n51\ntime as the likelihood of other operations needing to operate\non all the objects is greater than zero, and the eﬀort to refac-\ntor into components is usually similar to the eﬀort to create\na working templated operation.\nWithout classes to deﬁne boundaries, the table-based ap-\nproach levels the playing ﬁeld for data to be manipulated\ntogether. In all cases on our journey through normalising\nthe level data, we have made it so changes to the design re-\nquire fewer changes to the data, and made it so data changes\nare less likely to cause the state to become inconsistent. In\nmany cases, it would seem we have added complexity when\nit wasn’t necessary, and that’s up to experimentation and\nexperience to help you decide how far to go.\n2.5\nOperations\nWhen you use objects, you call methods on them, so how\ndo you unlock a door in this table-based approach? Actions\nare always going to be insert, delete, or updates. These were\nclearly speciﬁed in Edgar F. Codd’s works, and they are all\nyou need to manipulate a relational model.\nIn a real database, ﬁnding what mesh to load, or whether\na door is locked would normally require a join between ta-\nbles. A real database would also attempt to optimise the join\nby changing the sequence of operations until it had made the\nsmallest possible expected workload. We can do better than\nthat because we can take absolute charge of how we look at\nand request data from our tables. To ﬁnd out if a door is\nlocked, we don’t need to join tables, we know we can look up\ninto the locked doors table directly. Just because the data\nis laid out like a database, doesn’t mean we have to use a\nquery language to access it.\nWhen it comes to operations that change state, it’s best\nto try to stick to the kind of operation you would normally\n\n\n52\nCHAPTER 2. RELATIONAL DATABASES\nﬁnd in a DBMS, as doing unexpected operations brings un-\nexpected state complexity. For example, imagine you have a\ntable of doors that are open, and a table of doors that are\nclosed. Moving a door from one table might be considered\nwasteful, so you may consider changing the representation\nto a single table, but with all closed doors at one end, and\nall open at the other.\nBy having both tables represented\nas a single table, and having the isClosed attribute deﬁned\nimplicitly by a cut-oﬀpoint in the array, such as in listing\n2.2, leads to the table being somewhat ordered. This type\nof memory optimisation comes at a price. Introducing order\ninto a table makes the whole table inherently less parallelis-\nable to operations, so beware the additional complexity in-\ntroduced by making changes like this, and document them\nwell.\n1\ntypedef\nstd ::pair <int ,int > Door;\n2\ntypedef\nstd :: vector <Door > DoorVector\n3\nDoorVector\ngDoors;\n4\nint\ngDoors_firstClosedDoor = 0;\n5\n6\nAddClosedDoor ( Door d ) {\n7\ngDoors.push_back ();\n8\n}\n9\nAddOpenDoor( Door d ) {\n10\ngDoors.insert( gDoors.begin () + gDoors_firstClosedDoor , d );\n11\ngDoors_firstClosedDoor\n+= 1;\n12\n}\nListing 2.2: Abusing the ordered nature of a vector\nUnlocking a door can be a delete. A door is locked because\nthere is an entry in the LockedDoors table that matches the\nDoor you are interested in. Unlocking a door is a delete if\ndoor matches, and you have the right key.\nThe player inventory would be a table with just PickupIDs.\nThis is the idea that ”the primary key is also the data” men-\ntioned much earlier. If the player enters a room and picks up\na Pickup, then the entry matching the room is deleted while\nthe inventory is updated to include the new PickupID.\nDatabases have the concept of triggers, whereupon oper-\nations on a table can cause cascades of further operations.\nIn the case of picking up a key, we would want a trigger on\ninsert into the inventory that joined the new PickupID with\n\n\n2.6. SUMMING UP\n53\nthe LockedDoors table. For each matching row there, delete\nit, and now the door is unlocked.\n2.6\nSumming up\nAt this point we can see it is perfectly reasonable to store any\nhighly complex data structures in a database format, even\ngame data with its high interconnectedness and rapid design\nchanging criteria.\nGames have lots of state, and the relational model pro-\nvides a strong structure to hold both static information, and\nmutable state.\nThe strong structure leads to similar so-\nlutions to similar problems in practise, and similar solu-\ntions have similar processing.\nYou can expect algorithms\nand techniques to be more reusable while working with ta-\nbles, as the data layout is less surprising.\nIf you’re looking for a way to convert your interconnected\ncomplicated objects into a simpler ﬂatter memory layout, you\ncould do worse than approach the conversion with normali-\nsation in mind.\nA database approach to data storage has some other use-\nful side-eﬀects. It provides an easier route to allowing old\nexecutables to run oﬀnew data, and it allows new executa-\nbles to more easily run with old data. This can be vital when\nworking with other people who might need to run an earlier\nor later version. We saw that sometimes adding new features\nrequired nothing more than adding a new table, or a new col-\numn to an existing table. That’s a non-intrusive modiﬁcation\nif you are using a database style of storage, but a signiﬁcant\nchange if you’re adding a new member to a class.\n\n\n54\nCHAPTER 2. RELATIONAL DATABASES\n2.7\nStream Processing\nNow we realise that all the game data and game runtime can\nbe implemented in a database-like approach, we can also\nsee that all game data can be implemented as streams. Our\npersistent storage is a database, our runtime data is in the\nsame format as it was on disk, what do we beneﬁt from this?\nDatabases can be thought of as collections of rows, or col-\nlections of columns, but it’s also possible to think about the\ntables as sets. The set is the set of all possible permutations\nof the attributes.\nFor most applications, using a bitset to represent a ta-\nble would be wasteful, as the set size quickly grows out of\nscope of any hardware, but it can be interesting to note what\nthis means from a processing point of view. Processing a set,\ntransforming it into another set, can be thought of as travers-\ning the set and producing the output set, but the interesting\nattribute of a set is that it is unordered. An unordered list\ncan be trivially parallel processed. There are massive ben-\neﬁts to be had by taking advantage of this trivialisation of\nparallelism wherever possible, and we normally cannot get\nnear this because of the data layout of the object-oriented\napproaches.\nComing at this from another angle, graphics cards ven-\ndors have been pushing in this direction for many years, and\nwe now need to think in this way for game logic too.\nWe\ncan process lots of data quickly as long as we utilise stream\nprocessing or set processing as much as possible and use\nrandom access processing as little as possible. Stream pro-\ncessing in this case means to process data without writing to\nvariables external to the process. This means not allowing\nthings like global accumulators, or accessing global mem-\nory not set as a source for the process. This ensures the\nprocesses or transforms are trivially parallelisable.\nWhen you prepare a primitive render for a graphics card,\nyou set up constants such as the transform matrix, the tex-\n\n\n2.8. WHY DOES DATABASE TECHNOLOGY MATTER?\n55\nture binding, any lighting values, or which shader you want\nto run. When you come to run the shader, each vertex and\npixel may have its own scratchpad of local variables, but\nthey never write to globals or refer to a global scratchpad.\nThe concept of shared memory in general purpose GPU code,\nsuch as CUDA and OpenCL, allows the use of a kind of man-\naged cache. None of the GPGPU techniques oﬀer access to\nglobal memory, and thus maintain a clear separation of do-\nmains and continue to guarantee no side-eﬀects caused by\nany kernels being run outside of their own sandboxed shared\nmemory. By enforcing this lack of side-eﬀects, we can guar-\nantee trivial parallelism because the order of operations are\nassured to be irrelevant. If a shader was allowed to write to\nglobals, there would be locking, or it would become an inher-\nently serial operation. Neither of these are good for massive\ncore count devices like graphics cards, so that has been a\nself imposed limit and an important factor in their design.\nAdding shared memory to the mix starts to inject some po-\ntential locking into the process, and hence is explicitly only\nused when writing compute shaders.\nDoing all processing this way, without globals / global\nscratchpads, gives you the rigidity of intention to highly par-\nallelise your processing and make it easier to think about\nthe system, inspect it, debug it, and extend it or interrupt it\nto hook in new features. If you know the order doesn’t mat-\nter, it’s very easy to rerun any tests or transforms that have\ncaused bad state.\n2.8\nWhy does database technology mat-\nter?\nAs mentioned at the start of the chapter, the relational model\nis currently a very good ﬁt for developing non-sparse data\nlayouts that are manipulable with very little complicated\nstate management required once the tables have been de-\n\n\n56\nCHAPTER 2. RELATIONAL DATABASES\nsigned. However, the only constant is change. That which\nis current, regularly becomes the old way, and for widely\nscaled systems, the relational model no longer provides all\nfeatures required.\nAfter the emergence of NoSQL solutions for handling even\nlarger workloads, and various large companies’ work on cre-\nating solutions to distribute computing power, there have\nbeen advances in techniques to process enormous data-sets.\nThere have been advances in how to keep databases current,\ndistributed, and consistent (within tolerance).\nDatabases\nnow regularly include NULL entries, to the point where there\nare far more NULL entries than there are values, and these\nhighly sparse databases need a diﬀerent solution for pro-\ncessing.\nMany large calculations and processes now run\nvia a technique called map-reduce, and distributing work-\nloads has become commonplace enough that people have to\nbe reminded they don’t always need a cluster to add up some\nnumbers.\nWhat’s become clear over the last decade is that most of\nthe high-level data processing techniques which are prov-\ning to be useful are a combination of hardware-aware data\nmanipulation layers being used by functional programming\nstyle high-level algorithms. As the hardware in your PC be-\ncomes more and more like the internet itself, these tech-\nniques will begin to dominate on personal hardware, whether\nit be personal computers, phones, or whatever the next gen-\neration brings. Data-oriented design was inspired by a real-\nisation that the hardware had moved on to the point where\nthe techniques we used to use to defend against latency from\nCPU to hard drive, now apply to memory. In the future, if we\nraise processing power by the utilisation of hoards of iso-\nlated unreliable computation units, then the techniques for\ndistributing computing across servers that we’re developing\nin this era, will apply to the desktops of the next.\n",
      "page_number": 30,
      "chapter_number": 2,
      "summary": "In this chapter, we go over some of the pertinent parts\nof the relational model, relational database technology, and\nnormalisation, as these are examples of converting highly\ncomplex data structures and relationships into very clean\ncollections of linear storable data entries Key topics include data, tables, and databases.",
      "keywords": [
        "key",
        "msh key tex",
        "data",
        "primary key",
        "Relational Databases",
        "tex key",
        "tex",
        "msh key",
        "room",
        "msh",
        "database",
        "tables",
        "NULL",
        "Relational",
        "normal form"
      ],
      "concepts": [
        "data",
        "tables",
        "databases",
        "room",
        "key",
        "keys",
        "normalisation",
        "normalised",
        "normalise",
        "normalisations"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "Segment 20 (pages 181-194)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 18,
          "title": "Segment 18 (pages 164-172)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 3,
          "title": "[ 77 ]",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 7,
          "title": "Segment 7 (pages 52-61)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 4,
          "title": "[ 113 ]",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    },
    {
      "number": 3,
      "title": "Existential Processing",
      "start_page": 60,
      "end_page": 85,
      "detection_method": "regex_chapter",
      "content": "Chapter 3\nExistential Processing\nIf you saw there weren’t any apples in stock, would you still\nhaggle over their price?\nExistential processing attempts to provide a way to re-\nmove unnecessary querying about whether or not to process\nyour data. In most software, there are checks for NULL and\nqueries to make sure the objects are in a valid state before\nwork is started. What if you could always guarantee your\npointers were not null? What if you were able to trust that\nyour objects were in a valid state, and should always be pro-\ncessed?\nIn this chapter, a dynamic runtime polymorphism tech-\nnique is shown that can work with the data-oriented de-\nsign methodology. It is not the only way to implement data-\noriented design friendly runtime polymorphism, but was the\nﬁrst solution discovered by the author, and ﬁts well with\nother game development technologies, such as components\nand compute shaders.\n57\n\n\n58\nCHAPTER 3. EXISTENTIAL PROCESSING\n3.1\nComplexity\nWhen studying software engineering you may ﬁnd references\nto cyclomatic complexity or conditional complexity. This is\na complexity metric providing a numeric representation of\nthe complexity of programs and is used in analysing large-\nscale software projects. Cyclomatic complexity concerns it-\nself only with ﬂow control. The formula, summarised for our\npurposes, is one (1) plus the number of conditionals present\nin the system being analysed. That means for any system\nit starts at one, and for each if, while, for, and do-while, we\nadd one. We also add one per path in a switch statement\nexcluding the default case if present.\nUnder the hood, if we consider how a virtual call works,\nthat is, a lookup in a function pointer table followed by a\nbranch into the class method, we can see that a virtual call\nis eﬀectively just as complex as a switch statement. Count-\ning the ﬂow control statements is more diﬃcult in a virtual\ncall because to know the complexity value, you have to know\nthe number of possible methods that can fulﬁl the request.\nIn the case of a virtual call, you have to count the number\nof overrides to a base virtual call. If the base is pure-virtual,\nthen you may subtract one from the complexity. However, if\nyou don’t have access to all the code that is running, which\ncan be possible in the case of dynamically loaded libraries,\nthen the number of diﬀerent potential code paths increases\nby an unknown amount. This hidden or obscured complex-\nity is necessary to allow third party libraries to interface with\nthe core process, but requires a level of trust that implies\nno single part of the process is ever going to be thoroughly\ntested.\nThis kind of complexity is commonly called control ﬂow\ncomplexity. There is another form of complexity inherent in\nsoftware, and that is the complexity of state. In the paper\nOut of the Tar Pit[?], it’s concluded that the aspect of software\nwhich causes the most complexity is state. The paper con-\ntinues and presents a solution which attempts to minimise\n\n\n3.1. COMPLEXITY\n59\nwhat it calls accidental state, that is, state which is required\nby the software to do its job, but not directly required by the\nproblem being solved. The solution also attempts to abolish\nany state introduced merely to support a programming style.\nWe use ﬂow control to change state, and state changes\nwhat is executed in our programs. In most cases ﬂow con-\ntrol is put in for one of two reasons: to solve the problem\npresented (which is equivalent to the essential state in Out\nof the Tar Pit), and to help with the implementation of the\nsolution (which is equivalent to the accidental state).\nEssential control is when we need to implement the de-\nsign, a gameplay feature which has to happen when some\nconditions are met, such as jumping when the jump but-\nton is pressed or autosaving at a save checkpoint when the\nsavedata is dirty, or a timer has run out.\nAccidental control is non-essential to the program from\nthe point of view of the person using it, but could be founda-\ntion work, making it critical for successful program creation.\nThis type of control complexity is itself generally split into two\nforms. The ﬁrst form is structural, such as to support a pro-\ngramming paradigm, to provide performance improvements,\nor to drive an algorithm. The second form is defensive pro-\ngramming or developer helpers such as reference counting\nor garbage collection. These techniques increase complexity\nwhere functions operating on the data aren’t sure the data\nexists, or is making sure bounds are observed. In practice,\nyou will ﬁnd this kind of control complexity when using con-\ntainers and other structures, control ﬂow is going to be in\nthe form of bounds checks and ensuring data has not gone\nout of scope. Garbage collection adds complexity. In many\nlanguages, there are few guarantees about how and when it\nwill happen. This also means it can be hard to reason about\nobject lifetimes.\nBecause of a tendency to ignore memory\nallocations early in development when working with these\nlanguages, it can be very hard to ﬁx memory leaks closer to\nshipping dates. Garbage collection in unmanaged languages\n\n\n60\nCHAPTER 3. EXISTENTIAL PROCESSING\nis easier to handle, as reference counts can more easily be\ninterrogated, but also due to the fact that unmanaged lan-\nguages generally allocate less often in the ﬁrst place.\n3.2\nDebugging\nWhat classes of issues do we suﬀer with high complexity\nprograms? Analysing the complexity of a system helps us\nunderstand how diﬃcult it is to test, and in turn, how hard\nit is to debug. Some issues can be classiﬁed as being in an\nunexpected state, and then having no way forward. Others\ncan be classiﬁed as having bad state, and then exhibiting\nunexpected behaviour due to reacting to this invalid data.\nYet others can be classiﬁed as performance problems, not\njust correctness, and these issues, though somewhat disre-\ngarded by a large amount of academic literature, are costly\nin practice and usually come from complex dependencies of\nstate.\nFor example, the complexity caused by performance tech-\nniques such as caching, are issues of complexity of state.\nThe CPU cache is in a state, and not being aware of it, and\nnot working with the expected state in mind, leads to issues\nof poor or inconsistent performance.\nMuch of the time, the diﬃculty we have in debugging\ncomes from not fully observing all the ﬂow control points,\nassuming one route has been taken when it hasn’t. When\nprograms do what they are told, and not what we mean, they\nwill have entered into a state we had not expected or prepared\nfor.\nWith runtime polymorphism using virtual calls, the like-\nlihood of that happening can dramatically increase as we\ncannot be sure we know all the diﬀerent ways the code can\nbranch until we either litter the code with logging, or step\nthrough in a debugger to see where it goes at run-time.\n\n\n3.3. WHY USE AN IF\n61\n3.3\nWhy use an if\nIn real-world cases of game development, the most common\nuse of an explicit ﬂow control statement would appear to be\nin the non-essential set. Where defensive programming is\nbeing practiced, many of the ﬂow control statements are just\nto stop crashes. There are fail-safes for out of bounds ac-\ncesses, protection from pointers being NULL, and defenses\nagainst other exceptional cases that would bring the pro-\ngram to a halt. It’s pleasing to note, GitHub contains plenty\nof high quality C++ source-code that bucks this trend, prefer-\nring to work with reference types, or with value types where\npossible.\nIn game development, another common form of\nﬂow control is looping. Though these are numerous, most\ncompilers can spot them, and have good optimisations for\nthese and do a very good job of removing condition checks\nthat aren’t necessary. The ﬁnal inessential but common ﬂow\ncontrol comes from polymorphic calls, which can be helpful\nin implementing some of the gameplay logic, but mostly are\nthere to entertain the do-more-with-less-code development\nmodel partially enforced in the object-oriented approach to\nwriting games.\nEssential game design originating ﬂow control doesn’t ap-\npear very often in proﬁles as causes of branching, as all the\nsupporting code is run far more frequently. This can lead to\nan underappreciation of the eﬀect each conditional has on\nthe performance of the software. Code that does use a con-\nditional to implement AI or handle character movement, or\ndecide on when to load a level, will be calling down into sys-\ntems which are full of loops and tree traversals, or bounds\nchecks on arrays they are accessing in order to return the\ndata upon which the game is going to produce the boolean\nvalue to ﬁnally drive the side of the if to which it will fall\nthrough. That is, when the rest of your code-base is slow,\nit’s hard to validate writing fast code for any one task. It’s\nhard to tell what additional costs you’re adding on.\nIf we decide the elimination of control ﬂow is a goal wor-\n\n\n62\nCHAPTER 3. EXISTENTIAL PROCESSING\nthy of consideration, then we must begin to understand what\ncontrol ﬂow operations we can eliminate. If we begin our at-\ntempt to eliminate control ﬂow by looking at defensive pro-\ngramming, we can try to keep our working set of data as a\ncollections of arrays. This way we can guarantee none of our\ndata will be NULL. That one step alone may eliminate many\nof our ﬂow control statements. It won’t get rid of loops, but\nas long as they are loops over data running a pure func-\ntional style transform, then there are no side-eﬀects to worry\nabout, and it will be easier to reason about.1\nThe inherent ﬂow control in a virtual call is avoidable,\nas it is a fact that many programs were written in a non-\nobject-oriented style. Without virtuals, we can rely on switch\nstatements. Without those, we can rely on function pointer\ntables. Without those, we can have a long sequence of ifs.\nThere are many ways to implement runtime polymorphism.\nIt is also possible to maintain that if you don’t have an explicit\ntype, you don’t need to switch on it, so if you can eradicate\nthe object-oriented approach to solving the problem, those\nﬂow control statements go away completely.\nWhen we get to the control ﬂow in gameplay logic, we ﬁnd\nthere is no simple way to eradicate it. This is not a terrible\nthing to worry about, as the gameplay logic is as close to\nessential complexity as we can get when it comes to game\ndevelopment.\nReducing the number of conditionals, and thus reducing\nthe cyclomatic complexity on such a scale is a beneﬁt which\ncannot be overlooked, but it is one that comes with a cost.\nThe reason we are able to get rid of the check for NULL is\nthat we will have our data in a format that doesn’t allow for\nNULL at all. This inﬂexibility will prove to be a beneﬁt, but\nit requires a new way of processing our entities.\nWhere once we would have an object instance for an area\n1Sean Parent’s talks on C++ seasoning are worth watching. They talk\npractically about simpliﬁcation and elimination of unnecessary loops and\nstructure.\n\n\n3.3. WHY USE AN IF\n63\nin a game, and we would interrogate it for exits that take us to\nother areas, now we look into a structure that only contains\nlinks between areas, and ﬁlter by the area we are in. This\nreversal of ownership can be a massive beneﬁt in debugging,\nbut can sometimes appear backward when all you want to\ndo is ﬁnd out what exits are available to get out of an area.\nIf you’ve ever worked with shopping lists or to-do lists,\nyou’ll know how much more eﬃcient you can be when you\nhave a deﬁnite list of things to purchase or complete. It’s\nvery easy to make a list, and adding to it is easy as well. If\nyou’re going shopping, it’s very hard to think what might be\nmissing from your house in order to get what you need. If\nyou’re the type that tries to plan meals, then a list is nigh on\nessential as you ﬁgure out ingredients and then tally up the\nnumber of tins of tomatoes, or other ingredients you need to\nlast through all the meals you have planned. If you have a\nto-do list and a calendar, you know who is coming and what\nneeds to be done to prepare for them. You know how many\nextra mouths need feeding, how much food and drink you\nneed to buy, and how much laundry you need done to make\nenough beds for the visitors.\nTo-do lists are great because you can set an end goal and\nthen add in subtasks that make a large and long distant\ngoal seem more doable. Adding in estimates can provide a\nlittle urgency that is usually missing when the deadline is so\nfar away. Many companies use software to support tracking\nof tasks, and this software often comes with features allow-\ning the producers to determine critical paths, expected de-\nveloper hours required, and sometimes even the balance of\nskills required to complete a project. Not using this kind of\nsoftware is often a sign that a company isn’t overly concerned\nwith eﬃciency, or waste. If you’re concerned about eﬃciency\nand waste in your program, lists of tasks seem like a good\nway to start analysing where the costs are coming from. If\nyou keep track of these lists by logging them, you can look\nat the data and see the general shape of the processing your\nsoftware is performing. Without this, it can be diﬃcult to\n\n\n64\nCHAPTER 3. EXISTENTIAL PROCESSING\ntell where the real bottlenecks are, as it might not be the\nprocessing that is the problem, but the requirement to pro-\ncess data itself which has gotten out of hand.\nWhen your program is running, if you don’t give it ho-\nmogeneous lists to work with, but instead let it do whatever\ncomes up next, it will be ineﬃcient and have irregular or\nlumpy frame timings.\nIneﬃciency of hardware utilisation\noften comes from unpredictable processing. In the case of\nlarge arrays of pointers to heterogeneous classes all being\ncalled with an update() function, you can hit high amounts\nof data dependency which leads to misses in both data and\ninstruction caches. See chapter 11 for more details on why.\nSlowness also comes from not being able to see how much\nwork needs to be done, and therefore not being able to priori-\ntise or scale the work to ﬁt what is possible within the given\ntime-frame. Without a to-do list, and an ability to estimate\nthe amount of time each task will take, it is diﬃcult to decide\nthe best course of action to take in order to reduce overhead\nwhile maintaining feedback to the user.\nObject-oriented programming works very well when there\nare few patterns in the way the program runs. When either\nthe program is working with only a small amount of data, or\nwhen the data is incredibly heterogeneous, to the point that\nthere are as many classes of things as there are things.\nIrregular frame timings can often be blamed on not be-\ning able to act on distant goals ahead of time. If you, as a\ndeveloper, know you have to load the assets for a new island\nwhen a player ventures into the seas around it, the stream-\ning system can be told to drag in any data necessary. This\ncould also be for a room and the rooms beyond.\nIt could\nbe for a cave or dungeon when the player is within sight of\nthe entrance. We consider this kind of preemptive streaming\nof data to be a special case and invent systems to provide\nthis level of forethought. Relying on humans, or even level-\ndesigners, to link these together is prone to error. In many\ncases, there are chains of dependencies that can be missed\n\n\n3.3. WHY USE AN IF\n65\nwithout an automated check. The reason we cannot make\nsystems self-aware enough to preload themselves is that we\ndon’t have a common language to describe temporal depen-\ndencies.\nIn many games, we stream things in with explicit triggers,\nbut there is often no such system for many of the other game\nelements. It’s virtually unheard of for an AI to pathﬁnd to\nsome goal because there might soon be a need to head that\nway. The closest would be for the developer to pre-populate\na navigation map so coarse grain pathing can be completed\nswiftly.\nThere’s also the problem of depth of preemptive work.\nConsider the problem of a small room, built as a separate\nasset, a waiting room with two doors near each other, both\nleading to large, but diﬀerent maps. When the player gets\nnear the door to the waiting room in map A, that little room\ncan be preemptively streamed in. However, in many engines,\nmap B won’t be streamed in, as the locality of map B to map\nA is hidden behind the logical layer of the waiting room.\nIt’s also not commonplace to ﬁnd a physics system doing\nlook ahead to see if a collision has happened in the future in\norder to start doing further work. It might be possible to do\na more complex breakup simulation if it were more aware.\nIf you let your game generate to-do lists, shopping lists,\ndistant goals, and allow for preventative measures by forward-\nthinking, then you can simplify your task as a coder into\nprioritising goals and eﬀects, or writing code that generates\npriorities at runtime. You can start to think about how to\nchain those dependencies to solve the waiting room problem.\nYou can begin to preempt all types of processing.\n\n\n66\nCHAPTER 3. EXISTENTIAL PROCESSING\n3.4\nTypes of processing\nExistential processing is related to to-do lists.\nWhen you\nprocess every element in a homogeneous set of data, you\nknow you are processing every element the same way. You\nare running the same instructions for every element in that\nset. There is no deﬁnite requirement for the output in this\nspeciﬁcation, however, it usually comes down to one of three\ntypes of operation: a ﬁlter, a mutation, or an emission. A\nmutation is a one to one manipulation of the data, it takes\nincoming data and some constants that are set up before\nthe transform, and produces one and only one element for\neach input element. A ﬁlter takes incoming data, again with\nsome constants set up before the transform, and produces\none element or zero elements for each input element.\nAn\nemission is a manipulation of the incoming data that can\nproduce multiple output elements. Just like the other two\ntransforms, an emission can use constants, but there is no\nguaranteed size of the output table; it can produce anywhere\nbetween zero and inﬁnity elements.\nA fourth, and ﬁnal form, is not really a manipulation of\ndata, but is often part of a transform pipeline, and that is\nthe generator. A generator takes no input data, but merely\nproduces output based on the constants set up. When work-\ning with compute shaders, you might come across this as a\nfunction that merely clears out an array to zero, one, or an\nascending sequence.\nThese categories can help you decide what data structure\nyou will use to store the elements in your arrays, and whether\nyou even need a structure, or you should instead pipe data\nfrom one stage to another without it touching down on an\nintermediate buﬀer.\nEvery CPU can eﬃciently handle running processing\nkernels over homogeneous sets of data, that is, doing the\nsame operation over and over again over contiguous data.\nWhen there is no global state, no accumulator, it is proven\n\n\n3.4. TYPES OF PROCESSING\n67\nTransforms\nMutation\nin == out\nHandles input data. Produces\none item of output for every item\nof input.\nFilter\nin >= out\nHandles input data. Produces\nup to one item of output for\nevery item of input.\nEmission\nout =\n(\n0,\nin = 0\n>= 0,\notherwise\nHandles input data. Produces\nunknown amount of items per\nitem of input. With no input,\noutput is also empty.\nGeneration\nin = 0 ∧out >= 0\nDoes not read data. Produces an\nunknown amount of items just\nby running.\nTable 3.1: Types of transform normally encountered\nto be parallelisable.\nExamples can be given from exist-\ning technologies such as map-reduce and simple compute\nshaders, as to how to go about building real work applica-\ntions within these restrictions.\nStateless transforms also\ncommit no crimes that prevent them from being used within\ndistributed processing technologies. Erlang relies on these\nguarantees of being side-eﬀect free to enable not just thread\nsafe processing or interprocess safe processing, but dis-\ntributed computing safe processing. Stateless transforms of\nstateful data are highly robust and deeply parallelisable.\nWithin the processing of each element, that is for each\ndatum operated on by the transform kernel, it is fair to use\ncontrol ﬂow. Almost all compilers should be able to reduce\nsimple local value branch instructions into a platform’s pre-\nferred branch-free representation, such as a CMOV, or select\nfunction for a SIMD operation. When considering branches\ninside transforms, it’s best to compare to existing implemen-\ntations of stream processing such as graphics card shaders\nor compute kernels.\nIn predication, ﬂow control statements are not ignored,\nbut they are used instead as an indicator of how to merge two\nresults. When the ﬂow control is not based on a constant,\na predicated if will generate code that will run both sides of\n\n\n68\nCHAPTER 3. EXISTENTIAL PROCESSING\nthe branch at the same time and discard one result based on\nthe value of the condition. It manages this by selecting one\nof the results based on the condition. As mentioned before,\nin many CPUs there is an intrinsic for this, but all CPUs can\nuse bit masking to eﬀect this trick.\nSIMD or single-instruction-multiple-data allows the par-\nallel processing of data when the instructions are the same.\nThe data is diﬀerent but local. When there are no condition-\nals, SIMD operations are simple to implement on your trans-\nforms. In MIMD, that is multiple instructions, multiple data,\nevery piece of data can be operated on by a diﬀerent set of\ninstructions. Each piece of data can take a diﬀerent path.\nThis is the simplest and most error-prone to code for because\nit’s how most parallel programming is currently done. We\nadd a thread and process some more data with a separate\nthread of execution. MIMD includes multi-core general pur-\npose CPUs. It often allows shared memory access and all\nthe synchronisation issues that come with it. It is by far the\neasiest to get up and running, but it is also the most prone\nto the kind of rare fatal error caused by complexity of state.\nBecause the order of operations become non-deterministic,\nthe number of diﬀerent possible routes taken through the\ncode explode super-exponentially.\n3.5\nDon’t use booleans\nWhen you study compression technology, one of the most\nimportant aspects you have to understand is the diﬀerence\nbetween data and information.\nThere are many ways to\nstore information in systems, from literal strings that can be\nparsed to declare something exists, right down to something\nsimple like a single bit ﬂag to show that a thing might have\nan attribute. Examples include the text that declares the\nexistence of a local variable in a scripting language, or the\nbit ﬁeld containing all the diﬀerent collision types a physics\nmesh will respond to.\nSometimes we can store even less\n\n\n3.5. DON’T USE BOOLEANS\n69\ninformation than a bit by using advanced algorithms such\nas arithmetic encoding, or by utilising domain knowledge.\nDomain knowledge normalisation applies in most game de-\nvelopment, but it is increasingly infrequently applied, as\nmany developers are falling foul to overzealous application\nof quoting premature optimisation.\nAs information is en-\ncoded in data, and the amount of information encoded can\nbe ampliﬁed by domain knowledge, it’s important that we be-\ngin to see that the advice oﬀered by compression techniques\nis: what we are really encoding is probabilities.\nIf we take an example, a game where the entities have\nhealth, regenerate after a while of not taking damage, can\ndie, can shoot each other, then let’s see what domain knowl-\nedge can do to reduce processing.\nWe assume the following domain knowledge:\n• If you have full health, then you don’t need to regener-\nate.\n• Once you have been shot, it takes some time until you\nbegin regenerating.\n• Once you are dead, you cannot regenerate.\n• Once you are dead you have zero health.\n1\nstruct\nEntity {\n2\n//\ninformation\nabout\nthe\nentity\nposition\n3\n// ...\n4\n// now\nhealth\ndata\nin the\nmiddle\nof the\nentity\n5\nfloat\ntimeoflastdamage ;\n6\nfloat\nhealth;\n7\n// ...\n8\n//\nother\nentity\ninformation\n9\n};\n10\nlist <Entity > entities;\nListing 3.1: basic entity approach\n1\nvoid\nUpdateHealth ( Entity *e ) {\n2\nTimeType\ntimeSinceLastShot = e-> timeOfLastDamage\n- currentTime ;\n3\nbool\nisHurt = e->health < MAX_HEALTH;\n4\nbool\nisDead = e->health\n<= 0;\n5\nbool\nregenCanStart = timeSinceLastShot\n>\nTIME_BEFORE_REGENERATING ;\n6\n// if alive , and hurt , and it’s been\nlong\nenough\n7\nif( !isDead && isHurt &&\nregenCanStart ) {\n8\ne->health = min(MAX_HEALTH , e->health + tickTime * regenRate)\n;\n\n\n70\nCHAPTER 3. EXISTENTIAL PROCESSING\n9\n}\n10\n}\nListing 3.2: simple health regen\nIf we have a list for the entities such as in listing 3.1, then\nwe see the normal problem of data potentially causing cache\nline utilisation issues, but aside from that, we can see how\nyou might run an update function over the list, such as in\nlisting 3.2, which will run for every entity in the game, every\nupdate.\nWe can make this better by looking at the ﬂow control\nstatement. The function won’t run if health is at max. It\nwon’t run if the entity is dead. The regenerate function only\nneeds to run if it has been long enough since the last dam-\nage dealt. All these things considered, regeneration isn’t the\ncommon case. We should try to organise the data layout for\nthe common case.\n1\nstruct\nEntity {\n2\n//\ninformation\nabout\nthe\nentity\nposition\n3\n// ...\n4\n//\nother\nentity\ninformation\n5\n};\n6\nstruct\nEntitydamage {\n7\nfloat\ntimeoflastdamage ;\n8\nfloat\nhealth;\n9\n}\n10\nlist <Entity > entities;\n11\nmap <EntityRef ,Entitydamage > entitydamages ;\nListing 3.3: Existential processing style health\nLet’s change the structures to those in listing 3.3 and then\nwe can run the update function over the health table rather\nthan the entities. This means we already know, as soon as\nwe are in this function, that the entity is not dead, and they\nare hurt.\n1\nvoid\nUpdateHealth () {\n2\nfor( edIter : entityDamages ) {\n3\nEntityDamage &ed = edIter ->second;\n4\nif( ed.health\n<= 0 ) {\n5\n// if dead , insert\nthe\nfact\nthat\nthis\nentity\nis\ndead\n6\nEntityRef\nentity = edIter ->first;\n7\ndeadEntities .insert( entity );\n8\n// if dead , discard\nbeing\ndamaged\n9\ndiscard(ed);\n10\n} else {\n11\nTimeType\ntimeSinceLastShot = currentTime\n- ed.\ntimeOfLastShot ;\n12\nbool\nregenCanStart = timeSinceLastShot\n>\nTIME_BEFORE_REGENERATING ;\n\n\n3.5. DON’T USE BOOLEANS\n71\n13\nif( regenCanStart )\n14\ned ->health =ed ->health + tickTime * regenRate;\n15\n// if at max\nhealth\nor beyond , discard\nbeing\ndamaged\n16\nif( ed ->health\n>= MAX_HEALTH )\n17\ndiscard(ed);\n18\n}\n19\n}\n20\n}\nListing 3.4: every entity health regen\nWe only add a new entityhealth element when an entity\ntakes damage. If an entity takes damage when it already has\nan entityhealth element, then it can update the health rather\nthan create a new row, also updating the time damage was\nlast dealt. If you want to ﬁnd out someone’s health, then\nyou only need to look and see if they have an entityhealth\nrow, or if they have a row in deadEntities table.\nThe rea-\nson this works is, an entity has an implicit boolean hidden\nin the row existing in the table. For the entityDamages ta-\nble, that implicit boolean is the isHurt variable from the ﬁrst\nfunction. For the deadEntities table, the boolean of isDead\nis now implicit, and also implies a health value of 0, which\ncan reduce processing for many other systems. If you don’t\nhave to load a ﬂoat and check it is less than 0, then you’re\nsaving a ﬂoating point comparison or conversion to boolean.\nThis eradication of booleans is nothing new, because ev-\nery time you have a pointer to something you introduce a\nboolean of having a non-NULL value. It’s the fact that we\ndon’t want to check for NULL which pushes us towards ﬁnd-\ning a diﬀerent representation for the lack of existence of an\nobject to process.\nOther similar cases include weapon reloading, oxygen lev-\nels when swimming, anything which has a value that runs\nout, has a maximum, or has a minimum. Even things like\ndriving speeds of cars. If they are traﬃc, then they will spend\nmost of their time driving at traﬃc speed not some speed they\nneed to calculate. If you have a group of people all heading\nin the same direction, then someone joining the group can\nbe intercepting until they manage to, at which point they can\ngive up their independence, and become controlled by the\n\n\n72\nCHAPTER 3. EXISTENTIAL PROCESSING\ngroup. There is more on this point in chapter 5.\nBy moving to keeping lists of attribute state, you can\nintroduce even more performance improvements. The ﬁrst\nthing you can do for attributes that are linked to time is to\nput them in a sorted list, sorted by time of when they should\nbe acted upon. You could put the regeneration times in a\nsorted list and pop entityDamage elements until you reach\none that can’t be moved to the active list, then run through\nall the active list in one go, knowing they have some damage,\naren’t dead, and can regen as it’s been long enough.\nAnother aspect is updating certain attributes at diﬀerent\ntime intervals. Animals and plants react to their environ-\nment through diﬀerent mechanisms. There are the very fast\nmechanisms such as reactions to protect us from danger.\nPulling your hand away from hot things, for example. There\nare the slower systems too, like the rationalising parts of the\nbrain. Some, apparently quick enough that we think of them\nas real-time, are the quick thinking and acting processes we\nconsider to be the actions taken by our brains when we don’t\nhave time to think about things in detail, such as catching\na ball or balancing a bicycle. There is an even slower part of\nthe brain, the part that isn’t so much reading this book, but\nis consuming the words, and making a model of what they\nmean so as to digest them. There is also the even slower sys-\ntems, the ones which react to stress, chemical levels spread\nthrough the body as hormones, or just the amount of sugar\nyou have available, or current level of hydration. An AI which\ncan think and react on multiple time-scales is more likely to\nwaste fewer resources, but also much less likely to act oddly,\nor ﬂip-ﬂop between their decisions. Committing to doing an\nupdate of every system every frame could land you in an im-\npossible situation. Splitting the workload into diﬀerent up-\ndate rates can still be regular, but oﬀers a chance to balance\nthe work over multiple frames.\nAnother use is in state management. If an AI hears gun-\nﬁre, then they can add a row to a table for when they last\n\n\n3.6. DON’T USE ENUMS QUITE AS MUCH\n73\nheard gunﬁre, and that can be used to determine whether\nthey are in a heightened state of awareness. If an AI has been\ninvolved in a transaction with the player, it is important they\nremember what has happened as long as the player is likely\nto remember it. If the player has just sold an AI their +5\nlongsword, it’s very important the shopkeeper AI still have it\nin stock if the player just pops out of the shop for a moment.\nSome games don’t even keep inventory between transactions,\nand that can become a sore point if they accidentally sell\nsomething they need and then save their progress.\nFrom a gameplay point of view, these extra bits of infor-\nmation are all about how the world and player interact. In\nsome games, you can leave your stuﬀlying around forever,\nand it will always remain just how you left it. It’s quite a feat\nthat all the things you have dumped in the caves of some\nopen-world role-playing games, are still hanging around pre-\ncisely where you left them hours and hours ago.\nThe general concept of tacking on data, or patching\nloaded data with dynamic additional attributes, has been\naround for quite a while. Save games often encode the state\nof a dynamic world as a delta from the base state, and one\nof the ﬁrst major uses was in fully dynamic environments,\nwhere a world is loaded, but can be destroyed or altered\nlater. Some world generators took a procedural landscape\nand allowed their content creators to add patches of ex-\ntra information, villages, forts, outposts, or even break out\nlandscaping tools to drastically adjust the generated data.\n3.6\nDon’t use enums quite as much\nEnumerations are used to deﬁne sets of states. We could\nhave had a state variable for the regenerating entity, one that\nhad infullhealth, ishurt, isdead as its three states. We could\nhave had a team index variable for the avoidance entity enu-\nmerating all the available teams. Instead, we used tables to\n\n\n74\nCHAPTER 3. EXISTENTIAL PROCESSING\nprovide all the information we needed, as there were only two\nteams. Any enum can be emulated with a variety of tables.\nAll you need is one table per enumerable value. Setting the\nenumeration is an insert into a table or a migration from one\ntable to another.\nWhen using tables to replace enums, some things become\nmore diﬃcult: ﬁnding out the value of an enum in an entity\nis diﬃcult as it requires checking all the tables which repre-\nsent that state for the entity. However, the main reason for\ngetting the value is either to do an operation based on an ex-\nternal state or to ﬁnd out if an entity is in the right state to be\nconsidered for an operation. This is disallowed and unnec-\nessary for the most part, as ﬁrstly, accessing external state\nis not valid in a pure function, and secondly, any dependent\ndata should already be part of the table element.\nIf the enum is a state or type enum previously handled\nby a switch or virtual call, then we don’t need to look up the\nvalue, instead, we change the way we think about the prob-\nlem. The solution is to run transforms taking the content of\neach of the switch cases or virtual methods as the operation\nto apply to the appropriate table, the table corresponding to\nthe original enumeration value.\nIf the enum is instead used to determine whether or not\nan entity can be operated upon, such as for reasons of com-\npatibility, then consider an auxiliary table to represent be-\ning in a compatible state. If you’re thinking about the case\nwhere you have an entity as the result of a query and need\nto know if it is in a certain state before deciding to com-\nmit some changes, consider that the compatibility you seek\ncould have been part of the criteria for generating the output\ntable in the ﬁrst place, or a second ﬁltering operation could\nbe committed to create a table in the right form.\nIn conclusion, the reason why you would put an enum in\ntable form, is to reduce control ﬂow impact. Given this, it’s\nwhen we aren’t using the enumerations to control instruc-\ntion ﬂow that it’s ﬁne to leave them alone. Another possibility\n\n\n3.7. PRELUDE TO POLYMORPHISM\n75\nis when the value of the enum changes with great frequency,\nas moving objects from table to table has a cost too.\nExamples of enumerations that make sense are keybind-\nings, enumerations of colours, or good names for small ﬁnite\nsets of values. Functions that return enums, such as colli-\nsion responses (none, penetrating, through).\nAny kind of\nenumeration which is actually a lookup into data of another\nform is good, where the enum is being used to rationalise\nthe access to those larger or harder to remember data ta-\nbles. There is also a beneﬁt to some enums in that they will\nhelp you trap unhandled cases in switches, and to some ex-\ntent, they are a self-documenting feature in most languages.\n3.7\nPrelude to polymorphism\nLet’s consider now how we implement polymorphism.\nWe\nknow we don’t have to use a virtual table pointer; we could\nuse an enum as a type variable. That variable, the member\nof the structure that deﬁnes at runtime what that structure\nshould be capable of and how it is meant to react.\nThat\nvariable will be used to direct the choice of functions called\nwhen methods are called on the object.\nWhen your type is deﬁned by a member type variable, it’s\nusual to implement virtual functions as switches based on\nthat type, or as an array of functions. If we want to allow\nfor runtime loaded libraries, then we would need a system\nto update which functions are called.\nThe humble switch\nis unable to accommodate this, but the array of functions\ncould be modiﬁed at runtime.\nWe have a solution, but it’s not elegant, or eﬃcient. The\ndata is still in charge of the instructions, and we suﬀer the\nsame instruction cache misses and branch mispredictions\nas whenever a virtual function is unexpected.\nHowever,\nwhen we don’t really use enums, but instead tables that\n\n\n76\nCHAPTER 3. EXISTENTIAL PROCESSING\nrepresent each possible value of an enum, it is still possible\nto keep compatible with dynamic library loading the same\nas with pointer based polymorphism, but we also gain the\neﬃciency of a data-ﬂow processing approach to processing\nheterogeneous types.\nFor each class, instead of a class declaration, we have\na factory that produces the correct selection of table insert\ncalls. Instead of a polymorphic method call, we utilise ex-\nistential processing. Our elements in tables allow the char-\nacteristics of the class to be implicit. Creating your classes\nwith factories can easily be extended by runtime loaded li-\nbraries. Registering a new factory should be simple as long\nas there is a data-driven factory method. The processing of\nthe tables and their update() functions would also be added\nto the main loop.\n3.8\nDynamic runtime polymorphism\nIf you create your classes by composition, and you allow the\nstate to change by inserting and removing from tables, then\nyou also allow yourself access to dynamic runtime polymor-\nphism. This is a feature normally only available when dy-\nnamically responding via a switch.\nPolymorphism is the ability for an instance in a program\nto react to a common entry point in diﬀerent ways due only\nto the nature of the instance. In C++, compile-time polymor-\nphism can be implemented through templates and overload-\ning. Runtime polymorphism is the ability for a class to pro-\nvide a diﬀerent implementation for a common base operation\nwith the class type unknown at compile-time. C++ handles\nthis through virtual tables, calling the right function at run-\ntime based on the type hidden in the virtual table pointer at\nthe start of the memory pointed to by the this pointer. Dy-\nnamic runtime polymorphism is when a class can react to a\ncommon call signature in diﬀerent ways based on its type,\n\n\n3.8. DYNAMIC RUNTIME POLYMORPHISM\n77\nbut its type can change at runtime. C++ doesn’t implement\nthis explicitly, but if a class allows the use of an internal state\nvariable or variables, it can provide diﬀering reactions based\non the state as well as the core language runtime virtual ta-\nble lookup. Other languages which deﬁne their classes more\nﬂuidly, such as Python, allow each instance to update how\nit responds to messages, but most of these languages have\nvery poor general performance as the dispatch mechanism\nhas been built on top of dynamic lookup.\n1\nclass\nshape {\n2\npublic:\n3\nshape () {}\n4\nvirtual ~shape () {}\n5\nvirtual\nfloat\ngetarea () const = 0;\n6\n};\n7\nclass\ncircle : public\nshape {\n8\npublic:\n9\ncircle( float\ndiameter ) : d(diameter ) {}\n10\n~circle () {}\n11\nfloat\ngetarea () const { return d*d*pi /4; }\n12\nfloat d;\n13\n};\n14\nclass\nsquare : public\nshape {\n15\npublic:\n16\nsquare( float\nacross ) : width( across ) {}\n17\n~square () {}\n18\nfloat\ngetarea () const { return\nwidth*width; }\n19\nfloat\nwidth;\n20\n};\n21\nvoid\ntest () {\n22\ncircle\ncircle( 2.5f );\n23\nsquare\nsquare( 5.0f );\n24\nshape *shape1 = &circle , *shape2 = &square;\n25\nprintf( \"areas\nare %f and %f\\n\", shape1 ->getarea (), shape2 ->\ngetarea () );\n26\n}\nListing 3.5: simple object-oriented shape code\nConsider the code in listing 3.5, where we expect the run-\ntime method lookup to solve the problem of not knowing the\ntype but wanting the size. Allowing the objects to change\nshape during their lifetime requires some compromise. One\nway is to keep a type variable inside the class such as in\nlisting 3.6, where the object acts as a container for the type\nvariable, rather than as an instance of a speciﬁc shape.\n1\nenum\nshapetype { circletype , squaretype\n};\n2\nclass\nmutableshape {\n3\npublic:\n4\nmutableshape ( shapetype\ntype , float\nargument )\n5\n: m_type( type ), distanceacross ( argument )\n6\n{}\n7\n~ mutableshape () {}\n8\nfloat\ngetarea () const {\n9\nswitch( m_type ) {\n10\ncase\ncircletype: return\ndistanceacross * distanceacross *pi /4;\n11\ncase\nsquaretype: return\ndistanceacross * distanceacross ;\n12\n}\n\n\n78\nCHAPTER 3. EXISTENTIAL PROCESSING\n13\n}\n14\nvoid\nsetnewtype( shapetype\ntype ) {\n15\nm_type = type;\n16\n}\n17\nshapetype\nm_type;\n18\nfloat\ndistanceacross ;\n19\n};\n20\nvoid\ntestinternaltype () {\n21\nmutableshape\nshape1( circletype , 5.0f );\n22\nmutableshape\nshape2( circletype , 5.0f );\n23\nshape2. setnewtype( squaretype );\n24\nprintf( \" areas\nare %f and %f\\n\", shape1.getarea (), shape2.\ngetarea () );\n25\n}\nListing 3.6: ugly internal type code\nA better way is to have a conversion function to handle\neach case. In listing 3.7 we see how that can be achieved.\n1\nsquare\nsquarethecircle ( const\ncircle &circle ) {\n2\nreturn\nsquare( circle.d );\n3\n}\n4\nvoid\ntestconvertintype () {\n5\ncircle\ncircle( 5.0f );\n6\nsquare\nsquare = squarethecircle ( circle );\n7\n}\nListing 3.7: convert existing class to new class\nThough this works, all the pointers to the old class are\nnow invalid.\nUsing handles would mitigate these worries,\nbut add another layer of indirection in most cases, dragging\ndown performance even further.\nIf you use existential processing techniques, your classes\ndeﬁned by the tables they belong to, then you can switch\nbetween tables at runtime. This allows you to change be-\nhaviour without any tricks, without the complexity of man-\naging a union to carry all the diﬀerent data around for all\nthe states you need. If you compose your class from diﬀer-\nent attributes and abilities then need to change them post\ncreation, you can.\nIf you’re updating tables, the fact that\nthe pointer address of an entity has changed will mean little\nto you. It’s normal for an entity to move around memory in\ntable-based processing, so there are fewer surprises. Look-\ning at it from a hardware point of view, in order to implement\nthis form of polymorphism you need a little extra space for\nthe reference to the entity in each of the class attributes or\nabilities, but you don’t need a virtual table pointer to ﬁnd\n\n\n3.9. EVENT HANDLING\n79\nwhich function to call. You can run through all entities of\nthe same type increasing cache eﬀectiveness, even though it\nprovides a safe way to change type at runtime.\nVia the nature of having classes deﬁned implicitly by the\ntables they belong to, there is an opportunity to register a\nsingle entity with more than one table. This means that not\nonly can a class be dynamically runtime polymorphic, but\nit can also be multi-faceted in the sense that it can be more\nthan one class at a time. A single entity might react in two\ndiﬀerent ways to the same trigger call because it might be\nappropriate for the current state of that class.\nThis kind of multidimensional classing doesn’t come up\nmuch in traditional gameplay code, but in rendering, there\nare usually a few diﬀerent axes of variation such as the ma-\nterial, what blend mode, what kind of skinning or other ver-\ntex adjustments are going to take place on a given instance.\nMaybe we don’t see this ﬂexibility in gameplay code because\nit’s not available through the natural tools of the language.\nIt could be that we do see it, but it’s what some people call\nentity component systems.\n3.9\nEvent handling\nWhen you wanted to listen for events in a system in the old\ndays, you’d attach yourself to an interrupt. Sometimes you\nmight get to poke at code that still does this, but it’s normally\nreserved for old or microcontroller scale hardware. The idea\nwas simple, the processor wasn’t really fast enough to poll all\nthe possible sources of information and do something about\nthe data, but it was fast enough to be told about events and\nprocess the information as and when it arrived. Event han-\ndling in games has often been like this, register yourself as\ninterested in an event, then get told about it when it hap-\npens. The publish and subscribe model has been around\nfor decades, but there’s no standard interface built for it in\n\n\n80\nCHAPTER 3. EXISTENTIAL PROCESSING\nsome languages and too many standards in others.\nAs it\noften requires some knowledge from the problem domain to\nchoose the most eﬀective implementation.\nSome systems want to be told about every event in the\nsystem and decide for themselves, such as Windows event\nhandling. Some systems subscribe to very particular events\nbut want to react to them as soon as they happen, such as\nhandlers for the BIOS events like the keyboard interrupt.\nThe events could be very important and dispatched directly\nby the action of posting the event, such as with callbacks.\nThe events could be lazy, stuck in a queue somewhere wait-\ning to be dispatched at some later point. The problem they\nare trying to solve will deﬁne the best approach.\nUsing your existence in a table as the registration tech-\nnique makes this simpler than before and lets you regis-\nter and de-register with great pace. Subscription becomes\nan insert, and unsubscribing a delete. It’s possible to have\nglobal tables for subscribing to global events. It would also\nbe possible to have named tables. Named tables would al-\nlow a subscriber to subscribe to events before the publisher\nexists.\nWhen it comes to ﬁring oﬀevents, you have a choice. You\ncan choose to ﬁre oﬀthe transform immediately, or queue up\nnew events until the whole transform is complete, then dis-\npatch them all in one go. As the model becomes simpler and\nmore usable, the opportunity for more common use leads\nus to new ways of implementing code traditionally done via\npolling.\nFor example: unless a player character is within the dis-\ntance to activate a door, the event handler for the player’s\naction button needn’t be attached to anything door related.\nWhen the character comes within range, the character\nregisters into the has pressed action event table with the\nopen door (X) event result. This reduces the amount of time\nthe CPU wastes ﬁguring out what thing the player was trying\nto activate, and also helps provide state information such as\n\n\n3.9. EVENT HANDLING\n81\non-screen displays saying pressing Green will Open the door.\nIf we allow for all tables to have triggers like those found\nin DBMSs, then it may be possible to register interest in\nchanges to input mappings, and react.\nHooking into low-\nlevel tables such as a insert into a has pressed action ta-\nble would allow user interfaces to know to change their on-\nscreen display to show the new prompt.\nThis coding style is somewhat reminiscent of aspect-\noriented programming where it is easy to allow for cross-\ncutting concerns in the code. In aspect-oriented program-\nming, the core code for any activities is kept clean, and any\nside eﬀects or vetoes of actions are handled by other con-\ncerns hooking into the activity from outside. This keeps the\ncore code clean at the expense of not knowing what is really\ngoing to be called when you write a line of code. How using\nregistration tables diﬀers is in where the reactions come\nfrom and how they are determined. Debugging can become\nsigniﬁcantly simpler as the barriers between cause and ef-\nfect normally implicit in aspect-oriented programming are\nsigniﬁcantly diminished or removed, and the hard to adjust\nnature of object-oriented decision making can be softened\nto allow your code to become more dynamic without the\nnormally associated cost of data-driven control ﬂow.\n\n\n82\nCHAPTER 3. EXISTENTIAL PROCESSING\n",
      "page_number": 60,
      "chapter_number": 3,
      "summary": "In this chapter, a dynamic runtime polymorphism tech-\nnique is shown that can work with the data-oriented de-\nsign methodology Key topics include processing, process, and processes.",
      "keywords": [
        "Existential Processing",
        "data",
        "Processing",
        "ﬂow control",
        "state",
        "n’t",
        "Entity",
        "control",
        "Complexity",
        "type",
        "ﬂow",
        "ﬂow control statements",
        "tables",
        "Existential",
        "health"
      ],
      "concepts": [
        "processing",
        "process",
        "processes",
        "data",
        "state",
        "tables",
        "complexity",
        "code",
        "coded",
        "coding"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 7",
          "chapter": 29,
          "title": "Segment 29 (pages 271-279)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "AntiPatterns",
          "chapter": 18,
          "title": "Segment 18 (pages 152-157)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 22,
          "title": "Segment 22 (pages 433-453)",
          "relevance_score": 0.43,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 18,
          "title": "Segment 18 (pages 164-172)",
          "relevance_score": 0.43,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 9,
          "title": "Segment 9 (pages 68-77)",
          "relevance_score": 0.43,
          "method": "api"
        }
      ]
    },
    {
      "number": 4,
      "title": "Component Based",
      "start_page": 86,
      "end_page": 99,
      "detection_method": "regex_chapter",
      "content": "Chapter 4\nComponent Based\nObjects\nA component-oriented design is a good start for high-level\ndata-oriented design. Developing with components can put\nyou in the right frame of mind to avoid linking together con-\ncepts needlessly. Objects built this way can more easily be\nprocessed by type, instead of by instance, which can lead\nto them being easier to proﬁle. Entity systems built around\nthem are often found in game development as a way to pro-\nvide data-driven functionality packs for entities, allowing for\ndesigner control over what would normally be in the realm\nof a programmer.\nNot only are component based entities\nbetter for rapid design changes, but they also stymie the\nchances of getting bogged down into monolithic objects, as\nmost game designers would demand more components with\nnew features over extending the scope of existing compo-\nnents. This is because most new designs need iterating on,\nand extending an existing component by code to introduce\ndesign changes wouldn’t allow game designers to switch back\nand forth trying out diﬀerent things as easily. It’s usually\nmore ﬂexible to add another component as an extension or\nas an alternative.\n83\n\n\n84\nCHAPTER 4. COMPONENT BASED OBJECTS\nA problem that comes up with talking about component-\noriented development is how many diﬀerent types of entity\ncomponent systems there are. To help clear the ambiguity,\nwe shall describe some diﬀerent ways in which component-\noriented designs work.\nThe ﬁrst kind of component-oriented approach most peo-\nple use is a compound object. There are a few engines that\nuse them this way, and most of them use the power of their\nscripting language to help them achieve a ﬂexible, and de-\nsigner friendly way to edit and create objects out of compo-\nnents. For example, Unity’s GameObject is a base entity type\nwhich can include components by adding them to that par-\nticular instance’s list of components. They are all built onto\nthe core entity object, and they refer to each other through\nit. This approach means every entity still tends to update via\niteration over root instances, not iteration over systems.\nCommon dialogue around creating compound objects\nfrequently refers to using components to make up an ob-\nject directly by including them as members of the object.\nThough this is better than a monolithic class, it is not yet a\nfully component based approach. This technique uses com-\nponents to make the object more readable, and potentially\nmore reusable and robust to change. These systems are ex-\ntensible enough to support large ecosystems of components\nshareable between projects.\nThe Unity Asset Store proves\nthe worth of components from the point of view of rapid\ndevelopment.\nWhen you introduce component based entities, you have\nan opportunity to turn the idea of how you deﬁne an ob-\nject on its head. The normal approach to deﬁning an ob-\nject in object-oriented design is to name it, then ﬁll out the\ndetails as and when they become necessary. For example,\nyour car object is deﬁned as a Car, if not extending Vehicle,\nthen at least including some data about what physics and\nmeshes are needed, with construction arguments for wheels\nand body shell model assets etc, possibly changing class de-\n\n\n4.1. COMPONENTS IN THE WILD\n85\npendent on whether it’s an AI or player car. In component-\noriented design, objects aren’t so rigidly deﬁned, and don’t\nso much become deﬁned after they are named, as much as\na deﬁnition is selected or compiled, and then tagged with a\nname if necessary. For example, instancing a physics com-\nponent with four-wheel physics, instancing a renderable for\neach part (wheels, shell, suspension) adding an AI or player\ncomponent to control the inputs for the physics component,\nall adds up to something which we can tag as a Car, or leave\nas is and it becomes something implicit rather than explicit\nand immutable.\nA truly component based object is nothing more than the\nsum of its parts. This means the deﬁnition of a component\nbased object is also nothing more than an inventory with\nsome construction arguments. This object or deﬁnition ag-\nnostic approach makes refactoring and redesigning a much\nsimpler exercise. Unity’s ECS provides such a solution. In\nthe ECS, entities are intangible and implicit, and the com-\nponents are ﬁrst class citizens.\n4.1\nComponents in the wild\nComponent based approaches to development have been\ntried and tested. Many high-proﬁle studios have used com-\nponent driven entity systems to great success1, and this\nwas in part due to their developer’s unspoken understand-\ning that objects aren’t a good place to store all your data\nand traits.\nFor some, it was the opportunity to present\nthe complexity of what makes up an entity through simpler\npieces, so designers and modders would be able to reason\nabout how their changes ﬁt within the game framework. For\nsome, it was about giving power over to performance, where\ncomponents are more easily moved to a structure-of-arrays\napproach to processing.\n1Gas Powered Games, Looking Glass Studios, Insomniac, Neversoft all\nused component based objects.\n\n\n86\nCHAPTER 4. COMPONENT BASED OBJECTS\nGas Powered Games’ Dungeon Siege Architecture is prob-\nably the earliest published document about a game company\nusing a component based approach. If you get a chance, you\nshould read the article[?] to see where things really kicked\noﬀ. The article explains that using components means the\nentity type2 doesn’t need to have the ability to do anything.\nInstead, all the attributes and functionality come from the\ncomponents of which the entity is made.\nThe list of reasons to move to a manager driven, compo-\nnent based approach are numerous, and we shall attempt\nto cover at least a few.\nWe will talk about the beneﬁts of\nclear update sequences. We will mention how components\ncan make it easier to debug. We will talk about the prob-\nlem of objects applying meaning to data, causing coupling,\nand therefore with the dissolution of the object as the central\nentity, how the tyranny of the instance is mitigated.\nIn this section, we’ll show how we can take an existing\nclass and rewrite it in a component based fashion.\nWe’re\ngoing to tackle a fairly typical complex object, the Player\nclass.\nNormally these classes get messy and out of hand\nquite quickly. We’re going to assume it’s a Player class de-\nsigned for a generic 3rd person action game, and take a typ-\nically messy class as our starting point. We shall use listing\n4.1 as a reference example of one such class.\n1\nclass\nPlayer {\n2\npublic:\n3\nPlayer ();\n4\n~Player ();\n5\nVec\nGetPos (); // the\nroot\nnode\nposition\n6\nvoid\nSetPos( Vec ); // for\nspawning\n7\nVec\nGetSpeed (); //\ncurrent\nvelocity\n8\nfloat\nGetHealth ();\n9\nbool\nIsDead ();\n10\nint\nGetPadIndex (); // the\nplayer\npad\ncontrolling\nme\n11\nfloat\nGetAngle (); // the\ndirection\nthe\nplayer\nis\npointing\n12\nvoid\nSetAnimGoal ( ... ); //\npush\nstate\nto anim -tree\n13\nvoid\nShoot( Vec\ntarget ); //\nfire\nthe\nplayer ’s weapon\n14\nvoid\nTakeDamage( ... ); //\ntake\nsome\nhealth\noff , maybe\nanimate\nfor\nthe\ndamage\nreaction\n15\nvoid\nSpeak( ... ); //\ncause\nthe\nplayer\nto\nstart\naudio /anim\n16\nvoid\nSetControllable ( bool ); // no\ncontrol\nin cut - scene\n17\nvoid\nSetVisible( bool ); //\nhide\nwhen\nloading / streaming\n18\nvoid\nSetModel( ... ); //\ninit\nstreaming\nthe\nmeshes\netc\n19\nbool\nIsReadyForRender ();\n20\nvoid\nRender (); // put\nthis\nin the\nrender\nqueue\n2GPG:DG uses GO or Game-Objects, but we stick with the term entity\nbecause it has become the standard term.\n\n\n4.1. COMPONENTS IN THE WILD\n87\n21\nbool\nIsControllable (); //\nplayer\ncan\nmove\nabout ?\n22\nbool\nIsAiming (); // in\nnormal\nmove -mode , or aim -mode\n23\nbool\nIsClimbing ();\n24\nbool\nInWater (); // if the\nroot\nbone\nis\nunderwater\n25\nbool\nIsFalling ();\n26\nvoid\nSetBulletCount ( int ); //\nreload\nis\n-1\n27\nvoid\nAddItem( ... ); //\ninventory\nitems\n28\nvoid\nUseItem( ... );\n29\nbool\nHaveItem( ... );\n30\nvoid\nAddXP( int ); // not\nreally XP , but\nused\nto\nindicate\nwhen\nwe let\nthe\nplayer\npower -up\n31\nint\nGetLevel (); // not\nreally\nlevel , power -up\ncount\n32\nint\nGetNumPowerups (); // how\nmany\nwe’ve\nused\n33\nfloat\nGetPlayerSpeed (); // how\nfast\nthe\nplayer\ncan go\n34\nfloat\nGetJumpHeight ();\n35\nfloat\nGetStrength (); // for\nmelee\nattacks\nand\nclimb\nspeed\n36\nfloat\nGetDodge (); //\navoiding\nbullets\n37\nbool\nIsInBounds( Bound ); // in\ntrigger\nzone?\n38\nvoid\nSetGodMode( bool ); //\ncheater\n39\nprivate:\n40\nVec pos;\n41\nVec up , forward , right;\n42\nVec\nvelocity;\n43\nArray <ItemType > inventory;\n44\nfloat\nhealth;\n45\nint\ncontroller;\n46\nAnimID\nidleAnim;\n47\nAnimID\nshootAnim;\n48\nAnimID\nreloadAnim;\n49\nAnimID\nmovementAnim ;\n50\nAnimID\ncurrentAnimGoal ;\n51\nAnimID\ncurrentAnim;\n52\nint\nbulletCount;\n53\nfloat\nshotsPerSecond ;\n54\nfloat\ntimeSinceLastShot ;\n55\nSoundHandle\nplayingSoundHandle ; //\nnull\nmost\nof the\ntime\n56\nbool\ncontrollable ;\n57\nbool\nvisible;\n58\nAssetID\nplayerModel ;\n59\nLocomotionType\ncurrentLocomotiveModel ;\n60\nint xp;\n61\nint\nusedPowerups ;\n62\nint SPEED , JUMP , STRENGTH , DODGE;\n63\nbool\ncheating;\n64\n};\nListing 4.1: Monolithic Player class\nThis example class includes many of the types of things\nfound in games, where the codebase has grown organically.\nIt’s common for the Player class to have lots of helper func-\ntions to make writing game code easier.\nHelper functions\ntypically consider the Player as an instance in itself, from\ndata in save through to rendering on screen.\nIt’s not un-\nusual for the Player class to touch nearly every aspect of a\ngame, as the human player is the target of the code in the\nﬁrst place, the Player class is going to reference nearly every-\nthing too.\nAI characters will have similarly gnarly looking classes if\nthey are generalised rather than specialised. Specialising AI\nwas more commonplace when games needed to ﬁt in smaller\n\n\n88\nCHAPTER 4. COMPONENT BASED OBJECTS\nmachines, but now, because the Player class has to interact\nwith many of them over the course of the game, they tend to\nbe uniﬁed into one type just like the player, if not the same\nas the player, to help simplify the code that allows them to\ninteract. As of writing, the way in which AI is diﬀerentiated\nis mostly by data, with behaviour trees taking the main stage\nfor driving how AI thinks about its world. Behaviour trees are\nanother concept subject to various interpretations, so some\nforms are data-oriented design friendly, and others are not.\n4.2\nAway from the hierarchy\nA recurring theme in articles and post-mortems from people\nmoving from object-oriented hierarchies of gameplay classes\nto a component based approach is the transitional states of\nturning their classes into containers of smaller objects, an\napproach often called composition.\nThis transitional form\ntakes an existing class and ﬁnds the boundaries between\nconcepts internal to the class and attempts to refactor them\nout into new classes which can be owned or pointed to by\nthe original class. From our monolithic player class, we can\nsee there are lots of things that are not directly related, but\nthat does not mean they are not linked together.\nObject-oriented hierarchies are is-a relationships, and\ncomponents and composition oriented designs are tradition-\nally thought of as has-a relationships. Moving from one to\nthe other can be thought of as delegating responsibility or\nmoving away from being locked into what you are, but hav-\ning a looser role and keeping the specialisation until further\ndown the tree. Composition clears up most of the common\ncases of diamond inheritance issues, as capabilities of the\nclasses are added by accretion as much as they are added\nby overriding.\nThe ﬁrst move we need to make will be to take related\npieces of our monolithic class and move them into their own\n\n\n4.2. AWAY FROM THE HIERARCHY\n89\nclasses, along the lines of composing, changing the class\nfrom owning all the data and the actions that modify the data\ninto having instances which contain data and delegating ac-\ntions down into those specialised structures where possible.\nWe move the data out into separate structures so they can\nbe more easily combined into new classes later. We will ini-\ntially only separate by categories we perceive as being the\nboundaries between systems. For example, we separate ren-\ndering from controller input, from gameplay details such as\ninventory, and we split out animation from all.\nTaking a look at the results of splitting the player class\nup, such as in listing 4.2, it’s possible to make some initial\nassessments of how this may turn out. We can see how a\nﬁrst pass of building a class out of smaller classes can help\norganise the data into distinct, purpose oriented collections,\nbut we can also see the reason why a class ends up being a\ntangled mess. When you think about the needs of each of\nthe pieces, what their data requirements are, the coupling\ncan become evident. The rendering functions need access\nto the player’s position as well as the model, and the game-\nplay functions such as Shoot(Vec target) need access to the\ninventory as well as setting animations and dealing dam-\nage. Taking damage will need access to the animations and\nhealth. Things are already seeming more diﬃcult to han-\ndle than expected, but what’s really happening here is that\nit’s becoming clear that code needs to cut across diﬀerent\npieces of data. With just this ﬁrst pass, we can start to see\nthat functionality and data don’t belong together.\n1\nstruct\nPlayerPhysical {\n2\nVec pos;\n3\nVec up , forward , right;\n4\nVec\nvelocity;\n5\n};\n6\nstruct\nPlayerGameplay {\n7\nfloat\nhealth;\n8\nint xp;\n9\nint\nusedPowerups ;\n10\nint SPEED , JUMP , STRENGTH , DODGE;\n11\nbool\ncheating;\n12\nfloat\nshotsPerSecond ;\n13\nfloat\ntimeSinceLastShot ;\n14\n};\n15\nstruct\nEntityAnim {\n16\nAnimID\nidleAnim;\n17\nAnimID\nshootAnim;\n18\nAnimID\nreloadAnim;\n19\nAnimID\nmovementAnim ;\n20\nAnimID\ncurrentAnimGoal ;\n21\nAnimID\ncurrentAnim;\n\n\n90\nCHAPTER 4. COMPONENT BASED OBJECTS\n22\nSoundHandle\nplayingSoundHandle ; //\nnull\nmost\nof the\ntime\n23\n};\n24\nstruct\nPlayerControl {\n25\nint\ncontroller;\n26\nbool\ncontrollable ;\n27\n};\n28\nstruct\nEntityRender {\n29\nbool\nvisible;\n30\nAssetID\nplayerModel ;\n31\n};\n32\nstruct\nEntityInWorld {\n33\nLocomotionType\ncurrentLocomotiveModel ;\n34\n};\n35\nstruct\nInventory {\n36\nArray <ItemType > inventory;\n37\nint\nbulletCount ;\n38\n};\n39\n40\nclass\nPlayer {\n41\npublic:\n42\nPlayer ();\n43\n~Player ();\n44\n// ...\n45\n// ...\nthe\nmember\nfunctions\n46\n// ...\n47\nprivate:\n48\nPlayerPhysical\nphsyical;\n49\nPlayerGameplay\ngameplay;\n50\nEntityAnim\nanim;\n51\nPlayerControl\ncontrol;\n52\nEntityRender\nrender;\n53\nEntityInWorld\ninWorld;\n54\nInventory\ninventory;\n55\n};\nListing 4.2: Composite Player class\nIn this ﬁrst step, we made the player class a container for\nthe components. Currently, the player has the components,\nand the player class has to be instantiated to make a player\nexist. To allow for the cleanest separation into components\nin the most reusable way, it’s worth attempting to move com-\nponents into being managed by managers, and not handled\nor updated by their entities. In doing this, there will also be\na beneﬁt of cache locality when we’re iterating over multiple\nentities doing related tasks when we move them away from\ntheir owners.\nThis is where it gets a bit philosophical. Each system has\nan idea of the data it needs in order to function, and even\nthough they will overlap, they will not share all data. Con-\nsider what it is that a serialisation system needs to know\nabout a character. It is unlikely to care about the current\nstate of the animation system, but it will care about inven-\ntory. The rendering system will care about position and an-\nimation, but won’t care about the current amount of ammo.\nThe UI rendering code won’t even care about where the player\n\n\n4.3. TOWARDS MANAGERS\n91\nis, but will care about inventory and their health and dam-\nage. This diﬀerence of interest is at the heart of why putting\nall the data in one class isn’t a good long-term solution.\nThe functionality of a class, or an object, comes from\nhow the internal state is interpreted, and how the changes\nto state over time are interpreted too. The relationship be-\ntween facts is part of the problem domain and could be called\nmeaning, but the facts are only raw data. This separation of\nfact from meaning is not possible with an object-oriented\napproach, which is why every time a fact acquires a new\nmeaning, the meaning has to be implemented as part of the\nclass containing the fact. Dissolving the class, extracting the\nfacts and keeping them as separate components, has given\nus the chance to move away from classes that instill perma-\nnent meaning at the expense of occasionally having to look\nup facts via less direct methods. Rather than store all the\npossibly associated data by meaning, we choose to only add\nmeaning when necessary. We add meaning when it is part\nof the immediate problem we are trying to solve.\n4.3\nTowards managers\n1\nclass\nRenderable {\n2\nvoid\nRenderUpdate () {\n3\nauto\npos = gPositionArray [index ];\n4\ngRenderer.AddModel( playerModel , pos );\n5\n}\n6\n};\n7\nclass\nRenderManager {\n8\nvoid\nUpdate () {\n9\ngRenderer.BeginFrame ();\n10\nfor( auto &renderable : renderArray ) {\n11\nrenderable. RenderUpdate ();\n12\n}\n13\ngRenderer. SubmitFrame ();\n14\n}\n15\n};\n16\nclass\nPhysicsManager {\n17\nvoid\nUpdate () {\n18\nfor( auto & physicsRequest : physicsRequestArray ) {\n19\nphysicalArray [ physicsRequest .index ]. UpdateValues (\nphysicsRequest .updateData );\n20\n}\n21\n// Run\nphysics\nsimulation\n22\nfor( auto &physical : physicalArray ) {\n23\npositionArray [physical.index ]. pos = physical.pos;\n24\n}\n25\n}\n26\n};\n27\nclass\nController {\n\n\n92\nCHAPTER 4. COMPONENT BASED OBJECTS\n28\nvoid\nUpdate () {\n29\nPad pad = GetPad( controller );\n30\nif( pad.IsPressed( SHOOT ) ) {\n31\nif( inventoryArray [index ]. bulletCount\n> 0 )\n32\nanimRequest.Add( SHOOT_ONCE );\n33\n}\n34\n}\n35\n}\n36\n};\n37\nclass\nPlayerInventory {\n38\nvoid\nUpdate () {\n39\nif( inv. bulletCount\n== 0 ) {\n40\nif( animArray.contains( inv.index ) {\n41\nanim = animArray[ index ];\n42\nanim. currentAnim = RELOAD;\n43\ninventoryArray [index ]. bulletCount = 6;\n44\nanim. playingSoundHandle = PlaySound( GUNFIRE );\n45\n}\n46\n}\n47\n}\n48\n};\n49\nclass\nPlayerControl {\n50\nvoid\nUpdate () {\n51\nfor( auto &control : controlArray ) {\n52\ncontrol.Update ();\n53\n}\n54\nfor( auto &inv : inventoryArray )\n{\n55\ninv.Update ();\n56\n}\n57\n}\n58\n}\nListing 4.3: Manager ticked components\nAfter splitting your classes up into components, you\nmight ﬁnd your classes look more awkward now they are\naccessing variables hidden away in new structures. But it’s\nnot your classes that should be looking up variables, but in-\nstead transforms on the classes. A common operation such\nas rendering requires the position and the model informa-\ntion, but it also requires access to the renderer. Such object\nboundary crossing access is seen as a compromise during\ngame development, but here it can be seen as the method\nby which we move away from a class-centric approach to a\ndata-oriented approach.\nWe will aim at transforming our\ndata into render requests which aﬀect the graphics pipeline\nwithout referring to data unimportant to the renderer.\nReferring to listing 4.3, we move to no longer having a\nplayer update, but instead an update for each component\nthat makes up the player. This way, everyone entity’s physics\nis updated before it is rendered, or could be updated while\nthe rendering is happening on another thread. All entity’s\ncontrols (whether they be player or AI) can be updated be-\n\n\n4.3. TOWARDS MANAGERS\n93\nfore they are animated. Having the managers control when\nthe code is executed is a large part of the leap towards fully\nparallelisable code. This is where performance can be gained\nwith more conﬁdence that it’s not negatively impacting other\nareas.\nAnalysing which components need updating every\nframe, and which can be updated less frequently leads to\noptimisations that unlock components from each other.\nIn many component systems that allow scripting lan-\nguages to deﬁne the actions taken by components or their\nentities, performance can fall foul of the same ineﬃciencies\npresent in an object-oriented program design. Notably, the\ndependency inversion practice of calling Tick or Update func-\ntions will often have to be sandboxed in some way which will\nlead to error checking and other safety measures wrapping\nthe internal call. There is a good example of this being an\nissue with the older versions of Unity, where their compo-\nnent based approach allowed every instance to have its own\nscript which would have its own call from the core of Unity\non every frame. The main cost appeared to be transitioning\nin and out of the scripting language, crossing the boundary\nbetween the C++ at the core, and the script that described\nthe behaviour of the component. In his article 10,000 Up-\ndate() calls[?], Valentin Simonov provided information on\nwhy the move to managers makes so much sense, giving\ndetails on what is costing the most when utilising depen-\ndency inversion to drive your general code update strategies.\nThe main cost was in moving between the diﬀerent areas\nof code, but even without having to straddle the language\nbarrier, managers make sense as they ensure updates to\ncomponents happen in sync.\nWhat happens when we let more than just the player use\nthese arrays?\nNormally we’d have some separate logic for\nhandling player ﬁre until we refactored the weapons to be\ngeneric weapons with NPCs using the same code for weapons\nprobably by having a new weapon class that can be pointed\nto by the player or an NPC, but instead what we have here\nis a way to split oﬀthe weapon ﬁring code in such a way as\n\n\n94\nCHAPTER 4. COMPONENT BASED OBJECTS\nto allow the player and the NPC to share ﬁring code without\ninventing a new class to hold the ﬁring. In fact, what we’ve\ndone is split the ﬁring up into the diﬀerent tasks it really\ncontains.\nTasks are good for parallel processing, and with compo-\nnent based objects, we open up the opportunity to move most\nof our previously class oriented processes out, and into more\ngeneric tasks that can be dished out to whatever CPU or co-\nprocessor can handle them.\n4.4\nThere is no entity\nWhat happens when we completely remove the Player class?\nIf an entity can be represented by its collection of compo-\nnents, does it need any further identity than those self same\ncomponents? Like the values in the rows of a table, the com-\nponents describe a single instance, but also like the rows in\na table, the table is also a set. In the universe of possibilities\nof component combinations, the components which make up\nthe entity are not facts about the entity, but are the entity,\nand are the only identity the entity needs. As an entity is\nits current conﬁguration of components, then there is the\npossibility of removing the core Player class completely. Re-\nmoving this class can mean we no longer think of the player\nas being the centre of the game, but because the class no\nlonger exists, it means the code is no longer tied to a speciﬁc\nsingular entity. Listing 4.4 shows a rough example of how\nyou might develop this kind of setup.\n1\nstruct\nOrientation { Vec pos , up , forward , right; };\n2\nSparseArray <Orientation > orientationArray ;\n3\nSparseArray <Vec > velocityArray ;\n4\nSparseArray <float > healthArray;\n5\nSparseArray <int > xpArray , usedPowerupsArray , controllerID ,\nbulletCount ;\n6\nstruct\nAttributes { int SPEED , JUMP , STRENGTH , DODGE; };\n7\nSparseArray <Attributes > attributeArray ;\n8\nSparseArray <bool > godmodeArray , controllable , isVisible;\n9\nSparseArray <AnimID > currentAnim , animGoal;\n10\nSparseArray <SoundHandle > playingSound ;\n11\nSparseArray <AssetID > modelArray;\n12\nSparseArray <LocomotionType > locoModelArray ;\n13\nSparseArray <Array <ItemType > > inventoryArray ;\n14\n15\nint\nNewPlayer( int padID , Vec\nstartPoint ) {\n\n\n4.4. THERE IS NO ENTITY\n95\n16\nint ID = newID ();\n17\ncontrollerID [ ID ] padID;\n18\nGetAsset( \" PlayerModel \", ID ); //\nadds a request\nto put\nthe\nplayer\nmodel\ninto\nmodelArray [ID]\n19\norientationArray [ ID ] = Orientation( startPoint );\n20\nvelocityArray [ ID ] = VecZero ();\n21\nreturn\nID;\n22\n}\nListing 4.4: Sparse arrays for components\nMoving away from compile-time deﬁned classes means\nmany other classes can be invented without adding much\ncode. Allowing scripts to generate new classes of entity by\ncomposition or prototyping increases their power dramati-\ncally, and cleanly increase the apparent complexity of the\ngame without adding more actual complexity. Finally, all the\ndiﬀerent entities in the game will now run the same code at\nthe same time, which simpliﬁes and centralises your pro-\ncessing code, leading to more opportunity to share optimi-\nsations, and fewer places for bugs to hide.\n\n\n96\nCHAPTER 4. COMPONENT BASED OBJECTS\n",
      "page_number": 86,
      "chapter_number": 4,
      "summary": "This chapter covers component based. Key topics include classes, void, and component. Objects built this way can more easily be\nprocessed by type, instead of by instance, which can lead\nto them being easier to proﬁle.",
      "keywords": [
        "COMPONENT BASED OBJECTS",
        "Component Based",
        "Player class",
        "player",
        "BASED OBJECTS",
        "Component",
        "component based approach",
        "Based",
        "Objects",
        "component based entities",
        "Entity",
        "void",
        "Monolithic Player class",
        "int",
        "bool"
      ],
      "concepts": [
        "classes",
        "void",
        "component",
        "objects",
        "entity",
        "entities",
        "player",
        "oriented",
        "float",
        "controlling"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 16,
          "title": "Segment 16 (pages 148-155)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 12,
          "title": "Segment 12 (pages 96-104)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 1,
          "title": "Segment 1 (pages 1-9)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 15,
          "title": "Segment 15 (pages 126-147)",
          "relevance_score": 0.46,
          "method": "api"
        }
      ]
    },
    {
      "number": 5,
      "title": "Hierarchical Level of",
      "start_page": 100,
      "end_page": 115,
      "detection_method": "regex_chapter",
      "content": "Chapter 5\nHierarchical Level of\nDetail and\nImplicit-state\nConsoles and graphics cards are not generally bottlenecked\nat the polygon rendering stage in the pipeline. Usually, they\nare bandwidth bound.\nIf there is a lot of alpha blending,\nit’s often ﬁll-rate issues. For the most part, graphics chips\nspend a lot of their time reading textures, and texture band-\nwidth often becomes the bottleneck. Because of this, the old\nway of doing level of detail with multiple meshes with de-\ncreasing numbers of polygons is never going to be as good\nas a technique which takes into account the actual data re-\nquired of the level of detail used in each renderable. The vast\nmajority of stalls when rendering come from driver side pro-\ncessing, or from processing too much for what you want to\nactually render. Hierarchical level of detail can ﬁx the prob-\nlem of high primitive count which causes more driver calls\nthan necessary.\nThe basic approach for art is to make optimisations by\ngrouping and merging many low level of detail meshes into\n97\n\n\n98\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\none single low level of detail mesh. This reduces the time\nspent in the setup of render calls which is beneﬁcial in sit-\nuations where driver calls are costly. In a typical very large\nscale environment, a hierarchical level of detail approach to\ngame content can reduce the workload on a game engine by\nan order of magnitude as the number of entities in the scene\nconsidered for processing and rendering drops signiﬁcantly.\nEven though the number of polygons rendered may be\nexactly the same, or maybe even higher, the fact that the\nengine usually is only handling roughly the same number of\nentities at once on average increases stability and allows for\nmore accurately targeted optimisations of both art and code.\n5.1\nExistence from Null to Inﬁnity\nIf we consider that entities can be implicit based on their at-\ntributes, we can utilise the technique of hierarchical level of\ndetail to oﬀer up some optimisations for our code. In tra-\nditional level of detail techniques, as we move further away\nfrom the object or entity of interest, we lose details and ﬁ-\ndelity.\nWe might reduce polygon count, or texture sizes,\nor even the number of bones in a skeleton that drives the\nskinned mesh. Game logic can also degrade. Moving away\nfrom an entity, it might switch to a much coarser grain time\nstep. It’s not unheard of for behaviours of AI to migrate from\na 50hz update to a 1hz update. In a hierarchical level of de-\ntail implementation, as the entity becomes closer, or more\napparent to the player, it might be that only at that point\ndoes it even begin to exist.\nConsider a shooter game where you are defending a base\nfrom incoming attacks. You are manning an anti-air turret,\nand the attackers come in squadrons of aircraft, you can see\nthem all coming at once, over ten thousand aircraft in all,\nand up to a hundred at once in each squadron. You have to\nshoot them down or be inundated with gunﬁre and bombs,\n\n\n5.1. EXISTENCE\n99\ntaking out both you and the base you are defending.\nRunning full AI, with swarming for motion and avoidance\nfor your slower moving ordnance might be too much if it was\nrun on all ten thousand ships every tick, but you don’t need\nto. The basic assumption made by most AI programmers is\nthat unless they are within attacking range, then they don’t\nneed to be running AI. This is true and oﬀers an immediate\nspeedup compared to the na¨ıve approach. Hierarchical LOD\nprovides another way to think about this, by changing the\nnumber of entities based on how they are perceived by the\nplayer. For want of a better term, collective lodding is a name\nthat describes what is happening behind the scenes a little\nbetter. Sometimes there is no hierarchy, and yet, there can\nstill be a change in the manner in which the elements are\nreferenced between the levels of detail. The term collective\nlodding is inspired by the concept of a collective term.\nA\nmurder of crows is a computational element, but each crow\nis a lower level of detail sub-element of the collective.\nMurder\ny\n\u000f\n%\nCrow\nCrow\nCrow\nIn the collective lodding version of the base defender\ngame, there are a few wave entities which project squadron\nblips on the radar. The squadrons don’t exist as their own\nentities until they get close enough. Once a wave’s squadron\nis within range, the wave will decrease its squadron count\nand pop out a new squadron entity.\nThe newly created\nsquadron entity shows blips on the radar for each of its\ncomponent aircraft. The aircraft don’t exist yet, but they are\nimplicit in the squadron in the same way the squadron was\nimplicit in the wave. The wave continues to pop Squadrons\nas they come into range, and once its internal count has\ndropped to zero, it can delete itself as it now represents no\nentities.\nAs a squadron comes into even closer range, it\npops out its aircraft into their own entities and eventually\n\n\n100\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\ndeletes itself. As the aircraft get closer, traditional level of\ndetail techniques kick in and their renderables are allowed\nto switch to higher resolution and their AI is allowed to run\nat a higher intelligence setting.\nBlip\nw\n\u000f\n'\nSquadron\nSquadron\nw\n\u000f\n'\nSquadron\nAircraft\nAircraft\nw\n\u000f\n'\nAircraft\nEjectingPilot\nFuselage\nWing\nWhen the aircraft are shot at, they switch to a taken dam-\nage type. They are full health enemy aircraft unless they take\ndamage. If an AI reacts to damage with fear, they may eject,\nadding another entity to the world. If the wing of the plane\nis shot oﬀ, then that also becomes a new entity in the world.\nOnce a plane has crashed, it can delete its entity and replace\nit with a smoking wreck entity that will be much simpler to\nprocess than an aerodynamic simulation, faked or not.\nIf things get out of hand and the player can’t keep the\naircraft at bay and their numbers increase in size so much\nthat any normal level of detail system can’t kick in to miti-\ngate it, collective lodding can still help by returning aircraft\nto squadrons and ﬂying them around the base attacking as a\ngroup, rather than as individual aircraft. In the board game\nWarhammer Fantasy Battle, there were often so many troops\nﬁring arrows at each other, that players would often think\nof attacks by squads as being collections of attacks, and not\nactually roll for each individual soldier, rat, orc or whatever it\nwas, but instead counted up how many troops they had, and\nrolled that many dice to see how many attacks got through.\nThis is what is meant by attacking as a squadron. The air-\n\n\n5.2. MEMENTOS\n101\ncraft no longer attack, instead, the likelihood an attack will\nsucceed is calculated, dice are rolled, and that many attacks\nget through.\nThe level of detail heuristic can be tuned so\nthe nearest and front-most squadron are always the high-\nest level of detail, eﬀectively making them roll individually,\nand the ones behind the player maintain a very simplistic\nrepresentation.\nThis is game development smoke and mirrors as a ba-\nsic game engine element. In the past we have reduced the\nnumber of concurrent attacking AI1, reduced the number of\ncars on screen by staggering the lineup over the whole race\ntrack2, and we’ve literally combined people together into one\nperson instead of having loads of people on screen at once3.\nThis kind of reduction of processing is commonplace. Now\nconsider using it everywhere appropriate, not just when a\nplayer is not looking.\n5.2\nMementos\nReducing detail introduces an old problem, though. Chang-\ning level of detail in game logic systems, AI and such, brings\nwith it the loss of high detail history. In this case, we need\na way to store what is needed to maintain a highly cohesive\nplayer experience. If a high detail squadron in front of the\nplayer goes out of sight and another squadron takes their\nplace, we still want any damage done to the ﬁrst group to\nreappear when they come into sight again. Imagine if you\nhad shot out the glass on all the aircraft and when they came\nround again, it was all back the way it was when they ﬁrst\narrived. A cosmetic eﬀect, but one that is jarring and makes\nit harder to suspend disbelief.\nWhen a high detail entity drops to a lower level of de-\n1I believe this was Half-Life\n2Ridge Racer was known for this\n3Populous did this\n\n\n102\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\ntail, it should store a memento, a small, well-compressed\nnugget of data that contains all the necessary information\nin order to rebuild the higher detail entity from the lower de-\ntail one.\nWhen the squadron drops out of sight, it stores\na memento containing compressed information about the\namount of damage, where it was damaged, and rough posi-\ntions of all the aircraft in the squadron. When the squadron\ncomes into view once more, it can read this data and gener-\nate the high detail entities back in the state they were before.\nLossy compression is ﬁne for most things, it doesn’t matter\nprecisely which windows, or how they were cracked, maybe\njust that about 2/3 of the windows were broken.\nHighDetail\nstore\n'\nHighDetail\nMemento\nextract\n7\nAnother example is in a city-based free-roaming game. If\nAIs are allowed to enter vehicles and get out of them, then\nthere is a good possibility you can reduce processing time\nby removing the AIs from the world when they enter a ve-\nhicle. If they are a passenger, then they only need enough\ninformation to rebuild them and nothing else.\nIf they are\nthe driver, then you might want to create a new driver type\nbased on some attributes of the pedestrian before making\nthe memento for when they exit the vehicle.\nIf a vehicle reaches a certain distance away from the\nplayer, then you can delete it.\nTo keep performance high,\nyou can change the priorities of vehicles that have memen-\ntos so they try to lose sight of the player thus allowing for\nearlier removal from the game. Optimisations like this are\nhard to coordinate in object-oriented systems as internal in-\nspection of types isn’t encouraged. Some games get around\nit by designing in ways to reset memento data as a gameplay\nelement. The game Zelda: Breath of the Wild resets mon-\nsters during a Blood Moon, and by doing so, you as a player,\n\n\n5.3. JIT MEMENTOS\n103\nare not surprised when you return to camps to ﬁnd all the\nmonsters are just as you left them.\n5.3\nJIT mementos\nIf a vehicle that has been created as part of the ambient pop-\nulation is suddenly required to take on a more important\nrole, such as the car being involved in a ﬁreﬁght, it needs\nto gain detail. This detail must come from somewhere and\nmust be convincing. It is important to generate new entities\nwhich don’t seem overly generic or unlikely, given what the\nplayer knows about the game so far. Generating that data\ncan be thought of as providing a memento to read from just\nin time.\nJust in time mementos, or JIT mementos, oﬀers\na way to create fake mementos that can provide continuity\nby utilising pseudo-random generators or hash functions to\ncreate suitable information on demand without relying on\nstoring data anywhere. Instead, they rely only on informa-\ntion provided implicitly by the entity in need of it.\nInstead of generating new characters from a global ran-\ndom number generator, it is possible to seed the generator\nwith details about the thing that needs generation. For ex-\nample, you want to generate a driver and some passengers,\nas you’re about to get close enough to a car to need to render\nthe people inside it. Just creating random characters from\na set of lookup tables is good, but if you drive past them far\nenough for them to get out of rendering range, and then re-\nturn, the people in the car might not look the same anymore\nas they had to be regenerated. Instead, generate the driver\nand passengers using some other unique attribute, such as\nthe license plate, as a seed. This way, while you have not\naﬀected the result of generating the memento, you have no\nmemory overhead to store it, and no object lifetime to worry\nabout either, as it can always be reproduced from nothing\nagain.\n\n\n104\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\nV ehicle\nseed\nv\nPassengerStub\nseed\n(\n+3 Character\nMemento\nextract\n7\nThis technique is used all the time in landscape genera-\ntors, where the landscape is seeded from the x,y location in\nthe map, so why not use it when generating the weather for\nday 107 of the game? When generating Perlin noise, many\nalgorithms call upon a noise function, but to have a repro-\nducible landscape, the noise function must be a repeatable\nfunction, so it can create the same results over and over\nagain. If you’re generating a landscape, it’s preferred for the\nnoise function to be coherent, that is, for small variances in\nthe input function, only small changes should be observed\nin the output. We don’t need such qualities when generating\nJIT mementos, and a hash function which varies wildly with\neven the smallest change in the input will suﬃce.\nAn example of using this to create a JIT memento might\nbe to generate a house for a given landscape.\nFirst, take\nany normal random number generator and seed it with the\nlocation of the building. Given the landscape the house is on,\nselect from a building template and start generating random\nnumbers to answer questions about the house the same way\nloading a ﬁle oﬀdisk answers questions about the object.\nHow large is the house? Is it small, medium, large? Generate\na random number and select one answer. How many rooms\ndoes it have based on the size? 2 or 3 for small, or (int)(7\n+ rand * 10) for large. The point is, once you have seeded\nthe random number generator, you’re going to get the same\nresults back every time you run through the same process.\nEvery time you visit the house at {223.17,-100.5}, you’re\ngoing to see the same 4 (or more) walls, and it will have the\nsame paint job, broken windows, or perfect idyllic little frog\n\n\n5.3. JIT MEMENTOS\n105\npond in the back garden.\nJIT mementos can be the basis of a highly textured en-\nvironment with memento style sheets or style guides which\ncan direct a feel bias for any mementos generated in those\nvirtual spaces. Imagine a city style guide that speciﬁes rules\nfor occupants of cars. The style guide might claim that busi-\nnessmen might share, but are much less likely to, that fami-\nlies have children in the back seats with an older adult driv-\ning. It might declare that young adults tend to drive around\nin pairs. Style guides help add believability to any generated\ndata. Add in local changes such as having types of car linked\nto types of drivers. Have convertibles driven by well-dressed\ntypes or kids, low riders driven almost exclusively by their\nstereotypical owner, and imports and modded cars driven by\nyoung adults. In a space game, dirty hairy pilots of cargo\nships, well turned out oﬃcers commanding yachts, rough\nand ready mercenaries in everything from a single seater to\na dreadnought. Then, once you have the ﬂavour in place,\nallow for a little surprise to bring it to life fully.\nJIT mementos are a good way to keep the variety up, and\nstyle guides bias that so it comes without the impression that\neveryone is diﬀerent so everyone is the same. When these\nbiases are played out without being strictly adhered to, you\ncan build a more textured environment. If your environment\nis heavily populated with completely diﬀerent people all the\ntime, there is nothing to hold onto, no patterns to recognise.\nWhen there are no patterns, the mind tends to see noise\nor consider it to be a samey soup.\nEven the most varied\nvirtual worlds look bland when there is too much content\nall in the same place. Walk along the street and see if you\ncan spot any identical paving slabs. You probably can, but\nalso see the little bits of damage, decay, dirt, mistakes, and\nblemishes. To make an environment believable, you have to\nmake it look like someone took a lot of eﬀort trying to make\nit all conform.\n\n\n106\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\n5.4\nAlternative axes\nAs with all things, take away an assumption and you can\nﬁnd other uses for a tool. Whenever you read about, or work\nwith a level of detail system, you will be aware that the con-\nstraint on what level of detail is shown has always been some\ndistance function in space. It’s now time to take the assump-\ntion, discard it, and analyse what is really happening.\nFirst, we ﬁnd that if we take away the assumption of dis-\ntance, we can infer the conditional as some kind of linear\nmeasure. This value normally comes from a function which\ntakes the camera position and ﬁnds the relative distance to\nthe entity under consideration. What we may also realise\nwhen discarding the distance assumption is a more funda-\nmental understanding of what we are trying to do. We are\nusing a single runtime variable to control the presentation\nstate of the entities of our game. We use runtime variables\nto control the state of many parts of our game already, but\nin this case, there is a passive presentation response to the\nvariable, or axis being monitored. The presentation is usu-\nally some graphical, or logical level of detail, but it could be\nsomething as important to the entity as its own existence.\n5.4.1\nThe true measure\nDistance is the measure we normally use to identify what\nlevel of detail something should be at, but it’s not the met-\nric we really need, it’s just very closely related. In fact, it’s\ninversely related. The true metric of level of detail should\nbe how much of our perception an entity is taking up.\nIf\nan entity is very large, and far away, it takes up as much of\nour perception as something small and nearby. All this time\nwe have talked about hierarchical level of detail the elephant\nin the room has been the language used. We had waves on\nour radar. They took up as much perception attention as\na single squadron, and a single squadron took up as much\n\n\n5.4. ALTERNATIVE AXES\n107\nperceptual space as a single aircraft when it was in ﬁring\nrange.\nUnderstand this concept: level of detail should be deﬁned\nby how the player perceives a thing, at the range it is at.\nIf you internalise this, you will be on your way to making\ngood decisions about where the boundaries are between your\nlevels of detail.\n5.4.2\nBeyond space\nLet’s now consider what other variables we can calculate\nthat present an opportunity to remove details from the\ngame’s representation. We should consider anything which\npresents an opportunity to no longer process data unneces-\nsarily. If some element of a game is not the player’s current\nconcern, or will fade from memory soon enough, we can\ndissolve it away. If we consider the probability of the player\ncaring about a thing as a metric, then we begin to think\nabout recollection and attention as measurable quantities\nwe can use to drive how we end up representing it.\nAn entity that you know has the player’s attention, but is\nhidden, maintains a large stake on the player’s perception.\nThat stake allows the entity to maintain a higher priority on\nlevel of detail than it would otherwise deserve. For example,\na character the player is chasing in an assassination game,\nmay be spotted only once at the beginning of the mission,\nbut will have to remain at a high consistency of attribute\nthroughout the mission, as they are the object the player\ncares about the most, coming second only to primitive needs\nsuch as survival. Even if the character slips into the crowd,\nand is not seen again until much later, they must look just\nlike they did when you ﬁrst caught sight of them.\nAsk the question, how long until a player forgets about\nsomething that might otherwise be important? This infor-\nmation will help reduce memory usage as much as distance.\n\n\n108\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\nIf you have ever played Grand Theft Auto IV, you might have\nnoticed that the cars can disappear just by not looking at\nthem. As you turn around a few times you might notice the\ncars seem to be diﬀerent each time you face their way. This\nis a stunning use of temporal level of detail. Cars which have\nbeen bumped into or driven and parked by the player remain\nwhere they were, because, in essence, the player put them\nthere. Because the player has interacted with them, they\nare likely to remember they are there. However, ambient ve-\nhicles, whether they are police cruisers or civilian vehicles,\nare less important and don’t normally get to keep any special\nstatus so can vanish when the player looks away.\nAt the opposite end of the scale, some games remem-\nber everything you have done. Kill enemies in the ﬁrst few\nminutes of your game, loot their corpses, and chuck items\naround, then come back a hundred hours later and the items\nare still wherever you left them. Games like this store vast\namounts of tiny details, and these details need careful stor-\nage otherwise they would cause continual and crushing per-\nformance degradation.\nUsing spatially mapped mementos\nis one approach that can attempt to rationalise this kind of\nlevel of attention to player game interaction.\nIn addition to time-since-seen, some elements may base\ntheir level of detail on how far a player has progressed in\nthe game, or how many of something a player has, or how\nmany times they have done it. For example, a typical barter-\ning animation might be cut shorter and shorter as the game\nuses the axis of how many recent barters to draw back the\nlength of any non-interactive sections which could be caused\nby the event. This can be done simply, and the player will be\nthankful. Consider allowing multi-item transactions only af-\nter a certain number of single transactions have happened.\nIn eﬀect, you could set up gameplay elements, reactions to\nsituations, triggers for tutorials, reminders, or extensions to\ngameplay options all through these abstracted level of de-\ntail style axes. Handling the idea of player expertise through\naxes of level of detail of gameplay mechanic depth or com-\n\n\n5.4. ALTERNATIVE AXES\n109\nplexity.\nThis way of manipulating the present state of the game is\nsafer from transition errors. These are errors that happen\nbecause going from one state to another may have set some-\nthing to true when transitioning one direction, but might not\nset it back to false when transitioning the other way. You can\nthink of the states as being implicit on the axis. When state\nis modiﬁed, it’s prone to being modiﬁed incorrectly, or not\nmodiﬁed at the right time. If state is tied to other variables,\nthat is, if state is a function of other state, then it’s less prone\nto inconsistency.\nAn example of where transition errors occur is in menu\nsystems where all transitions should be reversible, some-\ntimes you may ﬁnd that going down two levels of menu, but\nback only one level, takes you back to where you started. For\nexample, entering the options menu, then entering an adjust\nvolume slider, but backing out of the slider might take you\nout of the options menu altogether. These bugs are common\nin UI code as there are large numbers of diﬀerent layers of\ninteraction. Player input is often captured in obscure ways\ncompared to gameplay input response. A common problem\nwith menus is one of ownership of the input for a partic-\nular frame. For example, if a player hits both the forward\nand backward button at the same time, a state machine UI\nmight choose to enter whichever transition response comes\nﬁrst.\nAnother might manage to accept the forward event,\nonly to have the next menu accept the back event, but worst\nof all might be the unlikely, but seen in the wild, menu tran-\nsitioning to two diﬀerent menus at the same time. Some-\ntimes the menu may transition due to external forces, and if\nthere is player input captured in a diﬀerent thread of execu-\ntion, the game state can become disjoint and unresponsive.\nConsider a network game’s lobby, where if everyone is ready\nto play, but the host of the game disconnects while you are\nentering into the options screen prior to game launch, in\na traditional state-machine like approach to menus, where\nshould the player return to once they exit the options screen?\n\n\n110\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\nThe lobby would normally have dropped you back to a server\nsearch screen, but in this case, the lobby has gone away to\nbe replaced with nothing. This is where having simple axes\ninstead of state machines can prove to be simpler to the point\nof being less buggy and more responsive.\n5.5\nCollective lodding - or how to re-\nduce your instance count.\nIt’s an ugly term, and I hope one day someone comes up with\na better one, but it’s a technique that didn’t need a name un-\ntil people stopped doing it. Over the time it has taken to write\nthis book, games have started to have too many instances.\nWe’re not talking about games that have hundreds of en-\nemy spacecraft, battling each other in a desperate ﬁght for\nsuperiority, ﬁring oﬀmissile after missile, generating visual\neﬀects which spawn multiple GPU particles. We’re talking\nabout simple seeming games. We’re talking about your aver-\nage gardening simulator, where for some reason, every leaf\non your plants is modeled as an instance, and every insect\ngoing around pollinating is an instance, and every plot of\nland in which your plants can grow is an instance, and ev-\nery seed you sew is an instance, and each have their own\nlifetimes, components, animations, and their own internal\nstate adding to the ever-growing complexity of the system as\na whole.\nI have a ﬁctional farming game, where I harvest wheat. I\nhave a ﬁeld which is 100 by 100 tiles, each with wheat grow-\ning. In some games, those wheat tiles would be instances,\nand the wheat on the tiles would be instances too. There’s\nlittle reason for this, as we can reduce the ﬁeld down to some\nvery small data. What do we actually need to know about the\nﬁeld and the wheat? Do we need to know the position of the\nwheat? We don’t, because it’s in a tiled grid. Do we need to\nknow if the tile has wheat or not? Yes, but it doesn’t need\n\n\n5.5. COLLECTIVE LOD\n111\nan object instance to tell us that. Do we need an object to\nrender the wheat?\nIt needs to blow in the wind, so don’t\nwe need to have it keep track of where it is to blow around\nand maintain momentum? No, because in almost all cases,\ncheating at this kind of thing is cheap and believable. Grass\nrendering works ﬁne without an instance per blade of grass.\nThe right data format for a ﬁeld full of wheat could be as\nsimple as 10,000 unsigned chars, with zero being no wheat,\nand values from 1 to 100 being how grown it is. The wheat\ndoesn’t have positions. The positions have wheat.\nIf you have a stack of blocks in Minecraft, you don’t have\n64 instances in your inventory slot, you just have a type, and\na multiple. You have a stack. If you have a stack of plates\nin a restaurant sim, you don’t have 10 plate instances, you\nhave a stack of plates object with an int saying how many\nplates there currently are.\nThe underlying principle of this is making sure you have\nslots in the world, whether hand placed, or generated in a\npattern, and keeping track of what’s in them, rather than\nplacing things in the world directly. Refer to things by how a\nstranger would name them. When you ask someone what is\nin a room, they won’t say a sofa, a bookshelf, an armchair,\nanother armchair, a coﬀee table, a TV stand, more book-\nshelves. No, they will say furniture. Look at your game from\nthe outside. Use how the players describe what is on screen.\nLook at how they describe their inventory. Look at how they\ndescribe the game, understand their mental model, match\nthat, and you will ﬁnd a strong correlation to what is taking\nup the players perception space.\nWhen normalising your data, look at how your rows are\naligned to some kind of container. If you have any form of\ngrid, from 1D to 4D, it’s worth looking at how you can utilise\nit. Don’t ignore other tesselations, such as triangle grids,\nor hexagon grids. Hexagon grids, in particular, get a bad\nname, but they can be represented by a square grid with\ndiﬀerent traversal functions. Don’t give up just because the\n\n\n112\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\nliteral grid is irregular either, in some grid-based games, the\ncentres of the cells are perturbed to give a more natural look,\nbut the game code can be strict grid-based, leading to better\nsolution space, and more likely easier for the player to reason\nabout what they can and can’t do.\n",
      "page_number": 100,
      "chapter_number": 5,
      "summary": "This chapter covers hierarchical level of. Key topics include game, player, and mementos. 5.1\nExistence from Null to Inﬁnity\nIf we consider that entities can be implicit based on their at-\ntributes, we can utilise the technique of hierarchical level of\ndetail to oﬀer up some optimisations for our code.",
      "keywords": [
        "level of detail",
        "Hierarchical Level",
        "Detail",
        "Level",
        "game",
        "player",
        "JIT MEMENTOS",
        "squadron",
        "n’t",
        "Hierarchical",
        "entity",
        "time",
        "aircraft",
        "MEMENTOS",
        "high detail"
      ],
      "concepts": [
        "game",
        "player",
        "mementos",
        "level",
        "generate",
        "generating",
        "generators",
        "generation",
        "looking",
        "squadrons"
      ],
      "similar_chapters": [
        {
          "book": "Game_Engine_Architecture",
          "chapter": 37,
          "title": "Segment 37 (pages 741-763)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 51,
          "title": "Segment 51 (pages 488-496)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 1,
          "title": "Segment 1 (pages 1-18)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 36,
          "title": "Segment 36 (pages 719-740)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 61-81)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 6,
      "title": "Searching",
      "start_page": 116,
      "end_page": 127,
      "detection_method": "regex_chapter",
      "content": "Chapter 6\nSearching\nWhen looking for speciﬁc data, it’s very important to remem-\nber why you’re doing it. If the search is not necessary, then\nthat’s your biggest possible saving. Finding if a row exists in\na table will be slow if approached na¨ıvely. You can manually\nadd searching helpers such as binary trees, hash tables, or\njust keep your table sorted by using ordered insertion when-\never you add to the table. If you’re looking to do the latter,\nthis could slow things down, as ordered inserts aren’t nor-\nmally concurrent, and adding extra helpers is normally a\nmanual task.\nIn this chapter, we ﬁnd ways to combat all\nthese problems.\n6.1\nIndexes\nDatabase management systems have long held the concept\nof an index.\nTraditionally, they were automatically added\nwhen a DBMS noticed a particular query had been run a\nlarge number of times. We can use this idea and implement\na just-in-time indexing system in our games to provide the\nsame kinds of performance improvement.\n113\n\n\n114\nCHAPTER 6. SEARCHING\nIn SQL, every time you want to ﬁnd out if an element\nexists, or even just generate a subset like when you need to\nﬁnd all the entities in a certain range, you will have to build\nit as a query. The query exists as an entity of a kind, and\nhelps build intuition into the DBMS.\nThe query that creates the row or table generation can be\nthought of as an object which can hang around in case it’s\nused again, and can transform itself depending on how it’s\nused over time. Starting out as a simple linear search query\n(if the data is not already sorted), the process can ﬁnd out\nthat it’s being used quite often through internal telemetry,\nand be able to discover that it generally returns a simply\ntunable set of results, such as the ﬁrst N items in a sorted\nlist. After some predeﬁned threshold number of operations,\nlifetime, or other metric, it would be valuable for the query\nobject to hook itself into the tables it references. Hooking\ninto the insertion, modiﬁcation, and deletion would allow the\nquery to update its answers, rather than run the full query\nagain each time it’s asked.\nThis kind of smart object is what object-oriented pro-\ngramming can bring to data-oriented design.\nIt can be a\nsigniﬁcant saving in some cases, but it can also be safe, due\nto its optionality.\nIf we build generalised backends to handle building\nqueries into these tables, they can provide multiple ben-\neﬁts. Not only can we expect garbage collection of indexes\nwhich aren’t in use, but they can also make the programs\nin some way self-documenting and self-proﬁling. If we study\nthe logs of what tables had pushed for building indexes for\ntheir queries, then we can see data hotspots and where there\nis room for improvement. It may even be possible to have\nthe code self-document what optimisation steps should be\ntaken.\n\n\n6.2. DATA-ORIENTED LOOKUP\n115\n6.2\nData-oriented Lookup\nThe ﬁrst step in any data-oriented approach to searching\nis to understand the diﬀerence between the search criteria,\nand the data dependencies of the search criteria. Object-\noriented solutions to searching often ask the object whether\nor not it satisﬁes some criteria. Because the object is asked\na question, there can be a lot of code required, memory in-\ndirectly accessed, and cache lines ﬁlled but hardly utilised.\nEven outside of object-oriented code-bases, there’s still a lot\nof poor utilisation of memory bandwidth. In listing 6.1, there\nis an example of simple binary search for a key in a na¨ıve im-\nplementation of an animation container. This kind of data\naccess pattern is common in animation libraries, but also in\nmany hand-rolled structures which look up entries that are\ntrivially sorted along an axis.\n1\nstruct\nFullAnimKey {\n2\nfloat\ntime;\n3\nVec3\ntranslation ;\n4\nVec3\nscale;\n5\nVec4\nrotation; //\nsijk\nquaternion\n6\n};\n7\nstruct\nFullAnim {\n8\nint\nnumKeys;\n9\nFullAnimKey *keys;\n10\nFullAnimKey\nGetKeyAtTimeBinary ( float t ) {\n11\nint l = 0, h = numKeys -1;\n12\nint m = (l+h) / 2;\n13\nwhile( l < h ) {\n14\nif( t < keys[m]. time ) {\n15\nh = m-1;\n16\n} else {\n17\nl = m;\n18\n}\n19\nm = (l+h+1) / 2;\n20\n}\n21\nreturn\nkeys[m];\n22\n}\n23\n};\nListing 6.1: Binary search through objects\nWe can improve on this very quickly by understanding\nthe dependence on the producer and the consumer of the\nprocess. Listing 6.2, is a quick rewrite that saves us a lot\nof memory requests by moving out to a partial structure-of-\narrays approach.\nThe data layout stems from recognising\nwhat data is needed to satisfy the requirements of the pro-\ngram.\n\n\n116\nCHAPTER 6. SEARCHING\nFirst, we consider what we have to work with as inputs,\nand then what we need to provide as outputs. The only input\nwe have is a time value in the form of a ﬂoat, and the only\nvalue we need to return in this instance is an animation key.\nThe animation key we need to return is dependent on data\ninternal to our system, and we are allowing ourselves the\nopportunity to rearrange the data any way we like. As we\nknow the input will be compared to the key times, but not\nany of the rest of the key data, we can extract the key times\nto a separate array. We don’t need to access just one part of\nthe animation key when we ﬁnd the one we want to return,\nbut instead, we want to return the whole key. Given that,\nit makes sense to keep the animation key data as an array\nof structures so we access fewer cache lines when returning\nthe ﬁnal value.\n1\nstruct\nDataOnlyAnimKey {\n2\nVec3\ntranslation ;\n3\nVec3\nscale;\n4\nVec4\nrotation; //\nsijk\nquaternion\n5\n};\n6\nstruct\nDataOnlyAnim {\n7\nint\nnumKeys;\n8\nfloat *keyTime;\n9\nDataOnlyAnimKey *keys;\n10\nDataOnlyAnimKey\nGetKeyAtTimeBinary ( float t ) {\n11\nint l = 0, h = numKeys -1;\n12\nint m = (l+h) / 2;\n13\nwhile( l < h ) {\n14\nif( t < keyTime[m] ) {\n15\nh = m-1;\n16\n} else {\n17\nl = m;\n18\n}\n19\nm = (l+h+1) / 2;\n20\n}\n21\nreturn\nkeys[m];\n22\n}\n23\n};\nListing 6.2: Binary search through values\nIt is faster on most hardware, but why is it faster? The\nﬁrst impression most people get is that we’ve moved the keys\nfrom nearby the returned data, ensuring we have another\nfetch before we have the chance to return. Sometimes it pays\nto think a bit further than what looks right at ﬁrst glance.\nLet’s look at the data layout of the AnimKeys.\n\n\n6.2. DATA-ORIENTED LOOKUP\n117\nt\ntx\nty\ntz\nsx\nsy\nsz\nrs\nri\nrj\nrk\nt\ntx\nty\ntz\nsx\ncacheline\nsy\nsz\nrs\nri\nrj\nrk\nt\ntx\nty\ntz\nsx\nsy\nsz\nrs\nri\nrj\ncacheline\nrk\nt\ntx\nty\ntz\nsx\nsy\nsz\nrs\nri\nrj\nrk\nt\n.\n.\n.\ncacheline\nPrimarily, the processing we want to be doing is all about\nﬁnding the index of the key by hunting for through values in\na list of times. In the extracted times code, we’re no longer\nlooking for a whole struct by one of its members in an array\nof structs. This is faster because the cache will be ﬁlled with\nmostly pertinent data during the hunt phase. In the orig-\ninal layout, we one or two key times per cache line. In the\nupdated code, we see 16 key times per cache line.\nt0\nt1\nt2\nt3\nt4\nt5\nt6\nt7\nt8\nt9\nt10\nt11\nt12\nt13\nt14\nt15\ncacheline\nThere are ways to organise the data better still, but any\nmore optimisation requires a complexity or space time trade\noﬀ. A basic binary search will home in on the correct data\nquite quickly, but each of the ﬁrst steps will cause a new\ncache line to be read in. If you know how big your cache\nline is, then you can check all the values that have been\nloaded for free while you wait for the next cache line to load\nin. Once you have got near the destination, most of the data\nyou need is in the cache and all you’re doing from then on\nis making sure you have found the right key. In a cache line\naware engine, all this can be done behind the scenes with\na well-optimised search algorithm usable all over the game\ncode. It is worth mentioning again, every time you break out\ninto larger data structures, you deny your proven code the\nchance to be reused.\nA binary search is one of the best search algorithms for\nusing the smallest number of instructions to ﬁnd a key value.\nBut if you want the fastest algorithm, you must look at what\ntakes time, and often, it’s not the instructions. Loading a\nwhole cache line of information and doing as much as you\n\n\n118\nCHAPTER 6. SEARCHING\ncan with that would be a lot more helpful than using the\nsmallest number of instructions. It is worth considering that\ntwo diﬀerent data layouts for an algorithm could have more\nimpact than the algorithm used.\nAs a comparison to the previous animation key ﬁnding\ncode, a third solution was developed which attempted to\nutilise the remaining cache line space in the structure. The\nstructure that contained the number of keys, and the two\npointers to the times and the key data, had quite a bit of\nspace left on the cache line. One of the biggest costs on the\nPS3 and Xbox360 was poor cache line utilisation, or CLU.\nIn modern CPUs, it’s not quite as bad, partially because the\ncache lines are smaller, but it’s still worth thinking about\nwhat you get to read for free with each request.\nIn this\nparticular case, there was enough cache line left to store\nanother 11 ﬂoating point values, which are used as a place\nto store something akin to skip-list.\ntimes\nkeys\nn\ns0\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns9\ns10\ncacheline\n1\nstruct\nClumpedAnim {\n2\nfloat *keyTime;\n3\nDataOnlyAnimKey *keys;\n4\nint\nnumKeys;\n5\nstatic\nconst\nint\nnumPrefetchedKeyTimes = (64- sizeof(int)-sizeof\n(float *)-sizeof( DataOnlyAnimKey *))/sizeof(float);\n6\nstatic\nconst\nint\nkeysPerLump = 64/ sizeof(float);\n7\nfloat\nfirstStage[ numPrefetchedKeyTimes ];\n8\nDataOnlyAnimKey\nGetKeyAtTimeLinear ( float t ) {\n9\nfor( int\nstart = 0; start < numPrefetchedKeyTimes ; ++ start )\n{\n10\nif( firstStage[start] > t ) {\n11\nint l = start* keysPerLump ;\n12\nint h = l + keysPerLump ;\n13\nh = h > numKeys ? numKeys : h;\n14\nreturn\nGetKeyAtTimeLinear ( t, l );\n15\n}\n16\n}\n17\nreturn\nGetKeyAtTimeLinear ( t, numPrefetchedKeyTimes *\nkeysPerLump );\n18\n}\n19\nDataOnlyAnimKey\nGetKeyAtTimeLinear ( float t, int\nstartIndex ) {\n20\nint i = startIndex;\n21\nwhile( i < numKeys ) {\n22\nif( keyTime[i] > t ) {\n23\n--i;\n24\nbreak;\n25\n}\n26\n++i;\n27\n}\n28\nif( i < 0 )\n29\nreturn\nkeys [0];\n30\nreturn\nkeys[i];\n31\n}\n32\n};\n\n\n6.2. DATA-ORIENTED LOOKUP\n119\nListing 6.3: Better cache line utilisation\nUsing the fact that these keys would be loaded into mem-\nory, we give ourselves the opportunity to interrogate some\ndata for free. In listing 6.3 you can see it uses a linear search\ninstead of a binary search, and yet it still manages to make\nthe original binary search look slow by comparison, and we\nmust assume, as with most things on modern machines, it\nis because the path the code is taking is using the resources\nbetter, rather than being better in a theoretical way, or using\nfewer instructions.\ni5-4430 @ 3.00GHz\nAverage 13.71ms [Full anim key - linear search]\nAverage 11.13ms [Full anim key - binary search]\nAverage\n8.23ms [Data only key - linear search]\nAverage\n7.79ms [Data only key - binary search]\nAverage\n1.63ms [Pre-indexed - binary search]\nAverage\n1.45ms [Pre-indexed - linear search]\nIf the reason for your search is simpler, such as checking\nfor existence, then there are even faster alternatives. Bloom\nﬁlters oﬀer a constant time lookup. Even though it produces\nsome false positives, it can be tweaked to generate a reason-\nable answer hit rate for very large sets. In particular, if you\nare checking for which table a row exists in, then bloom ﬁl-\nters work very well, by providing data about which tables to\nlook in, usually only returning the correct table, but some-\ntimes more than one.\nThe engineers at Google have used\nbloom ﬁlters to help mitigate the costs of something of a\nwrite-ahead approach with their BigTable technology[?], and\nuse bloom ﬁlters to quickly ﬁnd out if data requests should\nlookup their values in recent change tables, or should go\nstraight to the backing store.\nIn relational databases, indexes are added to tables at\nruntime when there are multiple queries that could beneﬁt\n\n\n120\nCHAPTER 6. SEARCHING\nfrom their presence. For our data-oriented approach, there\nwill always be some way to speed up a search but only by\nlooking at the data. If the data is not already sorted, then\nan index is a simple way to ﬁnd the speciﬁc item we need. If\nthe data is already sorted, but needs even faster access, then\na search tree optimised for the cache line size would help.\nMost data isn’t this simple to optimise. But importantly,\nwhen there is a lot of data, it usually is simple to learn pat-\nterns from it. A lot of the time, we have to work with spatial\ndata, but because we use objects, it’s hard to strap on an ef-\nﬁcient spatial reference container after the fact. It’s virtually\nimpossible to add one at runtime to an externally deﬁned\nclass of objects.\nAdding spatial partitioning when your data is in a simple\ndata format like rows allows us to generate spatial contain-\ners or lookup systems that will be easy to maintain and op-\ntimise for each particular situation. Because of the inherent\nreusability in data-oriented transforms, we can write some\nvery highly optimised routines for the high-level program-\nmers.\n6.3\nFinding lowest or highest is a sort-\ning problem\nIn some circumstances, you don’t even really need to search.\nIf the reason for searching is to ﬁnd something within a\nrange, such as ﬁnding the closest food, or shelter, or cover,\nthen the problem isn’t really one of searching, but one of\nsorting. In the ﬁrst few runs of a query, the search might\nliterally do a real search to ﬁnd the results, but if it’s run\noften enough, there is no reason not to promote the query\nto a runtime-updating sorted-subset of some other tables’\ndata. If you need the nearest three elements, then you keep\na sorted list of the nearest three elements, and when an el-\nement has an update, insertion or deletion, you can update\n\n\n6.4. FINDING RANDOM\n121\nthe sorted three with that information.\nFor insertions or\nmodiﬁcations which bring elements that are not in the set\ncloser, you can check whether the element is closer and pop\nthe lowest before adding the new element to the sorted best.\nIf there is a deletion or a modiﬁcation that makes one in the\nsorted set a contender for elimination, a quick check of the\nrest of the elements to ﬁnd a new best set might be neces-\nsary. If you keep a larger than necessary set of best values,\nhowever, then you might ﬁnd this never happens.\n1\nArray <int > bigArray;\n2\nArray <int > bestValue;\n3\nconst\nint\nLIMIT = 3;\n4\n5\nvoid\nAddValue( int\nnewValue ) {\n6\nbigArray.push( newValue );\n7\nbestValue. sortedinsert ( newValue );\n8\nif( bestValue.size () > LIMIT )\n9\nbestValue.erase(bestValue.begin ());\n10\n}\n11\nvoid\nRemoveValue ( int\ndeletedValue ) {\n12\nbigArray.remove( deletedValue );\n13\nbestValue.remove( deletedValue );\n14\n}\n15\nint\nGetBestValue () {\n16\nif( bestValue.size () ) {\n17\nreturn\nbestValue.top();\n18\n} else {\n19\nint\nbest = bigArray.findbest ();\n20\nbestvalue.push( best );\n21\nreturn\nbest;\n22\n}\n23\n}\nListing 6.4: keeping more than you need\nThe trick is to ﬁnd, at runtime, the best value to use that\ncovers the solution requirement. The only way to do that is\nto check the data at runtime. For this, either keep logs or\nrun the tests with dynamic resizing based on feedback from\nthe table’s query optimiser.\n6.4\nFinding random is a hash/tree is-\nsue\nFor some tables, the values change very often. For a tree\nrepresentation to be high performance, it’s best not to have\na high number of modiﬁcations as each one could trigger the\n\n\n122\nCHAPTER 6. SEARCHING\nneed for a rebalance. Of course, if you do all your modiﬁca-\ntions in one stage of processing, then rebalance, and then\nall your reads in another, then you’re probably going to be\nokay still using a tree.\nThe C++ standard template library implementation of\nmap for your compiler might not work well even when com-\nmitting all modiﬁcations in one go, but a more cache line\naware implementation of a tree, such as a B-tree, may help\nyou. A B-tree has much wider nodes, and therefore is much\nshallower. It also has a much lower chance of making mul-\ntiple changes at once under insert and delete operations, as\neach node has a much higher capacity. Typically, you will\nsee some form of balancing going on in a red-black tree every\nother insert or delete, but in most B-tree implementations,\nyou will have tree rotations occur relative to the width of\nthe node, and nodes can be very wide. For example, it’s not\nunusual to have nodes with 8 child nodes.\nIf you have many diﬀerent queries on some data, you can\nend up with multiple diﬀerent indexes. How frequently the\nentries are changed should inﬂuence how you store your in-\ndex data. Keeping a tree around for each query could become\nexpensive, but would be cheaper than a hash table in many\nimplementations. Hash tables become cheaper where there\nare many modiﬁcations interspersed with lookups, trees are\ncheaper where the data is mostly static, or at least hangs\naround in one form for a while over multiple reads.\nWhen the data becomes constant, a perfect hash can\ntrump a tree. Perfect hash tables use pre-calculated hash\nfunctions to generate an index and don’t require any space\nother than what is used to store the constants and the array\nof pointers or oﬀsets into the original table. If you have the\ntime, then you might ﬁnd a perfect hash that returns the\nactual indexes. It’s not often you have that long though.\nFor example, what if we need to ﬁnd the position of some-\none given their name? The players won’t normally be sorted\nby name, so we need a name to player lookup. This data is\n\n\n6.4. FINDING RANDOM\n123\nmostly constant during the game so would be better to ﬁnd a\nway to directly access it. A single lookup will almost always\ntrump following a pointer chain, so a hash to ﬁnd an array\nentry is likely to be the best ﬁt. Consider a normal hash ta-\nble, where each slot contains either the element you’re look-\ning for, or a diﬀerent element, and a way of calculating the\nnext slot you should check. If you know you want to do one\nand only one lookup, you can make each of your hash buck-\nets as large as a cache line. That way you can beneﬁt from\nfree memory lookups.\n\n\n124\nCHAPTER 6. SEARCHING\n",
      "page_number": 116,
      "chapter_number": 6,
      "summary": "In this chapter, we ﬁnd ways to combat all\nthese problems Key topics include data, searching, and times. 6.1\nIndexes\nDatabase management systems have long held the concept\nof an index.",
      "keywords": [
        "data",
        "cache line",
        "search",
        "key",
        "binary search",
        "int",
        "cache",
        "ﬁnd",
        "keys",
        "line",
        "key data",
        "key times",
        "time",
        "return keys",
        "query"
      ],
      "concepts": [
        "data",
        "searching",
        "times",
        "key",
        "keys",
        "lookup",
        "indexes",
        "index",
        "code",
        "oriented"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 11,
          "title": "Segment 11 (pages 89-108)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 34,
          "title": "Segment 34 (pages 357-368)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 18,
          "title": "Segment 18 (pages 149-156)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 12,
          "title": "Segment 12 (pages 109-118)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 24,
          "title": "Segment 24 (pages 228-238)",
          "relevance_score": 0.5,
          "method": "api"
        }
      ]
    },
    {
      "number": 7,
      "title": "Sorting",
      "start_page": 128,
      "end_page": 137,
      "detection_method": "regex_chapter",
      "content": "Chapter 7\nSorting\nFor some subsystems, sorting is a highly important func-\ntion. Sorting the primitive render calls so they render from\nfront to back for opaque objects can have a massive impact\non GPU performance, so it’s worth doing. Sorting the prim-\nitive render calls so they render from back to front for alpha\nblended objects is usually a necessity. Sorting sound chan-\nnels by their amplitude over their sample position is a good\nindicator of priority.\nWhatever you need to sort for, make sure you need to sort\nﬁrst, as usually, sorting is a highly memory intense busi-\nness.\n7.1\nDo you need to?\nThere are some algorithms which seem to require sorted\ndata, but don’t, and some which require sorted data but\ndon’t seem to. Be sure you know whether you need to before\nyou make any false moves.\nA common use of sorting in games is in the render pass\n125\n\n\n126\nCHAPTER 7. SORTING\nwhere some engine programmers recommend having all your\nrender calls sorted by a high bit count key generated from\na combination of depth, mesh, material, shader, and other\nﬂags such as whether the call is alpha blended. This then\nallows the renderer to adjust the sort at runtime to get the\nmost out of the bandwidth available. In the case of the ren-\ndering list sort, you could run the whole list through a gen-\neral sorting algorithm, but in reality, there’s no reason to\nsort the alpha blended objects with the opaque objects, so\nin many cases you can take a ﬁrst step of putting the list\ninto two separate buckets, and save some work overall. Also,\nchoose your sorting algorithm wisely. With opaque objects,\nthe most important part is usually sorting by textures then\nby depth, but that can change with how much your ﬁll rate is\nbeing trashed by overwriting the same pixel multiple times.\nIf your overdraw doesn’t matter too much but your texture\nuploads do, then you probably want to radix sort your calls.\nWith alpha blended calls, you just have to sort by depth, so\nchoose an algorithm which handles your case best. Be aware\nof how accurately you need your data to be sorted. Some\nsorts are stable, others unstable. Unstable sorts are usually\na little quicker. For analogue ranges, a quick sort or a merge\nsort usually oﬀer slow but guaranteed accurate sorting. For\ndiscrete ranges of large n, a radix sort is very hard to beat. If\nyou know your range of values, then a counting sort is a very\nfast two pass sort, for example, sorting by material, shader,\nor other input buﬀer index.\nWhen sorting, it’s also very important to be aware of algo-\nrithms that can sort a range only partially. If you only need\nthe lowest or highest n items of an m long array, you can use\na diﬀerent type of algorithm to ﬁnd the nth item, then sort all\nthe items greater or less than the returned pivot. In some se-\nlection algorithms you will end with some guarantees about\nthe data. Notably, quickselect will result in the nth item by\nsorting criteria residing in the nth position. Once complete,\nall items either side remain unsorted in their sub-ranges,\nbut are guaranteed to be less than or more than the pivot,\ndepending on the side of the pivot they fall.\n\n\n7.1. DO YOU NEED TO?\n127\nIf you have a general range of items which need to be\nsorted in two diﬀerent ways, you can either sort with a spe-\ncialised comparison function in a one-hit sort, or you can\nsort hierarchically. This can be beneﬁcial when the order of\nitems is less important for a subset of the whole range. The\nrender queue is still a good example. If you split your sort\ninto diﬀerent sub-sorts, it makes it possible to proﬁle each\npart of the sort, which can lead to beneﬁcial discoveries.\nYou don’t need to write your own algorithms to do this\neither. Most of the ideas presented here can be implemented\nusing the STL, using the functions in algorithms.\nYou\ncan use std::partial sort to ﬁnd and sort the ﬁrst n el-\nements, you can use std::nth element to ﬁnd the nth value\nas if the container was sorted. Using std::partition and\nstd::stable partition allow you to split a range by a criteria,\neﬀectively sorting a range into two sub-ranges.\nIt’s important to be aware of the contracts of these algo-\nrithms, as something as simple as the erase/remove process\ncan be very expensive if you use it without being aware that\nremove will shuﬄe all your data down, as it is required to\nmaintain order. If there was one algorithm you should add to\nyour collection, it would be your own version of remove which\ndoes not guarantee maintaining order. Listing 7.1 shows one\nsuch implementation.\n1\ntemplate\n<class It , class T>\n2\nIt\nunstable_remove ( It begin , It end , const T& value )\n3\n{\n4\nbegin = find(begin , end , value);\n5\nif (begin != end) {\n6\n--end;\n7\n*begin = move( *end );\n8\n}\n9\nreturn\nend;\n10\n}\nListing 7.1: A basic implementation of unstable remove\n\n\n128\nCHAPTER 7. SORTING\n7.2\nMaintain by insertion sort or par-\nallel merge sort\nDepending on what you need the list sorted for, you could\nsort while modifying. If the sort is for some AI function that\ncares about priority, then you may as well insertion sort as\nthe base heuristic commonly has completely orthogonal in-\nputs. If the inputs are related, then a post insertion table\nwide sort might be in order, but there’s little call for a full-\nscale sort.\nIf you really do need a full sort, then use an algorithm\nwhich likes being parallel.\nMerge sort and quick sort are\nsomewhat serial in that they end or start with a single\nthread doing all the work, but there are variants which\nwork well with multiple processing threads, and for small\ndatasets there are special sorting network techniques which\ncan be faster than better algorithms just because they ﬁt the\nhardware so well1.\n7.3\nSorting for your platform\nAlways remember that in data-oriented development you\nmust look to the data for information before deciding which\nway you’re going to write the code. What does the data look\nlike? For rendering, there is a large amount of data with dif-\nferent axes for sorting. If your renderer is sorting by mesh\nand material, to reduce vertex and texture uploads, then the\ndata will show that there are a number of render calls which\nshare texture data, and a number of render calls which\nshare vertex data. Finding out which way to sort ﬁrst could\nbe ﬁgured out by calculating the time it takes to upload a\ntexture, how long it takes to upload a mesh, how many ex-\n1Tony Albrecht proves this point in his article on sorting networks\nhttp://seven-degrees-of-freedom.blogspot.co.uk/2010/07/question-of-\nsorts.html\n\n\n7.3. SORTING FOR YOUR PLATFORM\n129\ntra uploads are required for each, then calculating the total\nscene time, but mostly, proﬁling is the only way to be sure.\nIf you want to be able to proﬁle and get feedback quickly\nor allow for runtime changes in case your game has such\nvarying asset proﬁles that there is no one solution to ﬁt all,\nhaving some ﬂexibility of sorting criteria is extremely useful\nand sometimes necessary. Fortunately, it can be made just\nas quick as any inﬂexible sorting technique, bar a small\nsetup cost.\nRadix sort is the fastest serial sort. If you can do it, radix\nsort is very fast because it generates a list of starting points\nfor data of diﬀerent values in a ﬁrst pass, then operates using\nthat data in a second pass. This allows the sorter to drop\ntheir contents into containers based on a translation table, a\ntable that returns an oﬀset for a given data value. If you build\na list from a known small value space, then radix sort can\noperate very fast to give a coarse ﬁrst pass. The reason radix\nsort is serial, is that it has to modify the table it is reading\nfrom in order to update the oﬀsets for the next element that\nwill be put in the same bucket. If you ran multiple threads\ngiving them part of the work each, then you would ﬁnd they\nwere non-linearly increasing in throughput as they would\nbe contending to write and read from the same memory, and\nyou don’t want to have to use atomic updates in your sorting\nalgorithm.\nIt is possible to make this last stage of the process parallel\nby having each sorter ignore any values it reads which are\noutside its working set, meaning each worker reads through\nthe entire set of values gathering for their bucket, but there is\nstill a small chance of non-linear performance due to having\nto write to nearby memory on diﬀerent threads. During the\ntime the worker collects the elements for its bucket, it could\nbe generating the counts for the next radix in the sequence,\nonly requiring a summing before use in the next pass of the\ndata, mitigating the cost of iterating over the whole set with\nevery worker.\n\n\n130\nCHAPTER 7. SORTING\nIf your data is not simple enough to radix sort, you might\nbe better oﬀusing a merge sort or a quicksort, but there\nare other sorts that work very well if you know the length\nof your sortable buﬀer at compile time, such as sorting net-\nworks. Through merge-sort is not itself a concurrent algo-\nrithm, the many early merges can be run in parallel, only\nthe ﬁnal merge is serial, and with a quick pre-parse of the\nto-be-merged data, you can ﬁnalise with two threads rather\nthan one by starting from both ends (you need to make sure\nthe mergers don’t run out of data). Though quick sort is not\na concurrent algorithm each of the substages can be run in\nparallel. These algorithms are inherently serial, but can be\nturned into partially parallelisable algorithms with O(log n)\nlatency.\nWhen your n is small enough, a traditionally good tech-\nnique is to write an in-place bubble sort. The algorithm is\nso simple, it is hard to write wrong, and because of the small\nnumber of swaps required, the time taken to set up a bet-\nter sort could be better spent elsewhere. Another argument\nfor rewriting such trivial code is that inline implementations\ncan be small enough for the whole of the data and the algo-\nrithm to ﬁt in cache2. As the negative impact of the ineﬃ-\nciency of the bubble sort is negligible over such a small n, it\nis hardly ever frowned upon to do this. In some cases, the\nfact that there are fewer instructions can be more important\nthan the operational eﬃciency, as instruction eviction could\ncost more than the time saved by the better algorithm. As\nalways, measure so you can be certain.\nIf you’ve been developing data-oriented, you’ll have a\ntransform which takes a table of n and produces the sorted\nversion of it. The algorithm doesn’t have to be great to be\nbetter than bubble sort, but notice it doesn’t cost any devel-\nopment time to use a better algorithm as the data is in the\nright shape already.\nData-oriented development naturally\n2It might be wise to have some inline sort function templates in your own\nutility header so you can utilise the beneﬁts of miniaturisation, but don’t\ndrop in a bloated std::sort\n\n\n7.3. SORTING FOR YOUR PLATFORM\n131\nleads us to reuse of good algorithms.\nWhen looking for the right algorithm, it’s worth knowing\nabout more than you are presented during any coursework,\nand look into the more esoteric forms. For sorting, some-\ntimes you want an algorithm that always sorts in the same\namount of time, and when you do, you can’t use any of the\nstandard quick sorts, radix sorts, bubble or other. Merge\nsort tends to have good performance, but to get truly sta-\nble times when sorting, you may need to resort to sorting\nnetworks.\nSorting networks work by implementing the sort in a\nstatic manner. They have input data and run swap if nec-\nessary functions on pairs of values of the input data before\noutputting the ﬁnal.\nThe simplest sorting network is two\ninputs.\nA\n/\n/\n\u001c\n/ A′\nB\n/\n/\nB\n/ B′\nIf the values entering are in order, the sorting crossover\ndoes nothing. If the values are out of order, then the sorting\ncrossover causes the values to swap.\nThis can be imple-\nmented as branch-free writes:\na’ <= MAX(a,b)\nb’ <= MIN(a,b)\nThis is fast on any hardware. The MAX and MIN functions\nwill need diﬀerent implementations for each platform and\ndata type, but in general, branch-free code executes a little\nfaster than code that includes branches. In most current\ncompilers, the MIN and MAX functions will be promoted to\nintrinsics if they can be, but you might need to ﬁnesse the\n\n\n132\nCHAPTER 7. SORTING\ndata so the value is part of the key, so it is sorted along with\nthe keys.\nIntroducing more elements:\nA\n/\n1\n/\n\u0015\n2\n/\n\u001c\n3\n/\n/ A′\nB\n/\n/\n\u0015\n/\nB\n\u001c\n/\n/ B′\nC\n/\n/\nI\n/\n\u001c\nB\n/\n/ C′\nD\n/\n/\nI\n/\nB\n/\n/ D′\nWhat you may notice here is that the critical path is not\nlong (just three stages in total), the ﬁrst stage is two con-\ncurrent sortings of A/C, and B/D pairs. The second stage,\nsorting A/B, and C/D pairs. The ﬁnal cleanup sorts the B/C\npair. As these are all branch-free functions, the performance\nis regular over all data permutations. With such a regular\nperformance proﬁle, we can use the sort in ways where the\nvariability of sorting time length gets in the way, such as\njust-in-time sorting for subsections of rendering. If we had\nradix sorted our renderables, we can network sort any ﬁnal\nrequired ordering as we can guarantee a consistent timing.\na’ <= MAX(a,c)\nb’ <= MIN(b,d)\nc’ <= MAX(a,c)\nd’ <= MIN(b,d)\na’’ <= MAX(a’,b’)\nb’’ <= MIN(a’,b’)\nc’’ <= MAX(c’,d’)\nd’’ <= MIN(c’,d’)\nb’’’ <= MIN(b’’,c’’)\nc’’’ <= MAX(b’’,c’’)\n\n\n7.3. SORTING FOR YOUR PLATFORM\n133\nSorting networks are somewhat like predication, the\nbranch-free way of handling conditional calculations.\nBe-\ncause sorting networks use a min/max function, rather\nthan a conditional swap, they gain the same beneﬁts when\nit comes to the actual sorting of individual elements. Given\nthat sorting networks can be faster than radix sort for cer-\ntain implementations, it goes without saying that for some\ntypes of calculation, predication, even long chains of it, will\nbe faster than code that branches to save processing time.\nJust such an example exists in the Pitfalls of Object Oriented\nProgramming[?] presentation, concluding that lazy evalua-\ntion costs more than the job it tried to avoid. I have no hard\nevidence for it yet, but I believe a lot of AI code could beneﬁt\nthe same, in that it would be wise to gather information even\nwhen you are not sure you need it, as gathering it might be\nquicker than deciding not to. For example, seeing if someone\nis in your ﬁeld of vision, and is close enough, might be small\nenough that it can be done for all AI rather than just the\nones requiring it, or those that require it occasionally.\n\n\n134\nCHAPTER 7. SORTING\n",
      "page_number": 128,
      "chapter_number": 7,
      "summary": "This chapter covers sorting. Key topics include sorting, data, and times. Sorting the primitive render calls so they render from\nfront to back for opaque objects can have a massive impact\non GPU performance, so it’s worth doing.",
      "keywords": [
        "sort",
        "data",
        "radix sort",
        "sorting networks",
        "algorithm",
        "sorting algorithm",
        "merge sort",
        "MAX",
        "MIN",
        "radix",
        "render",
        "time",
        "n’t",
        "render calls",
        "small"
      ],
      "concepts": [
        "sorting",
        "data",
        "times",
        "timing",
        "algorithms",
        "values",
        "small",
        "good",
        "function",
        "functions"
      ],
      "similar_chapters": [
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 2,
          "title": "Segment 2 (pages 36-70)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 42,
          "title": "Segment 42 (pages 416-425)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 10,
          "title": "Segment 10 (pages 95-103)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 11,
          "title": "Segment 11 (pages 104-112)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 39,
          "title": "Segment 39 (pages 789-807)",
          "relevance_score": 0.46,
          "method": "api"
        }
      ]
    },
    {
      "number": 8,
      "title": "Optimisations and",
      "start_page": 138,
      "end_page": 165,
      "detection_method": "regex_chapter",
      "content": "Chapter 8\nOptimisations and\nImplementations\nWhen optimising software, you have to know what is causing\nthe software to run slower than you need it to run. We ﬁnd in\nmost cases, data movement is what really costs us the most.\nData movement is where most of the energy goes when pro-\ncessing data. Calculating solutions to functions, or running\nan algorithm on the data uses less energy. It is the fulﬁll-\nment of the request for data in the ﬁrst place that appears\nto be the largest cost. As this is most deﬁnitely true about\nour current architectures, we ﬁnd implicit or calculable in-\nformation is often much more useful than cached values or\nexplicit state data.\nIf we start our game development by organising our data\ninto arrays, we open ourselves up to many opportunities for\noptimisation. Starting with such a problem agnostic layout,\nwe can pick and choose from tools we’ve created for other\ntasks, at worst elevating the solution to a template or a strat-\negy, before applying it to both the old and new use cases.\nIn Out of the Tar Pit[?], it’s considered poor form to add\n135\n\n\n136\nCHAPTER 8. OPTIMISATIONS\nstate and complexity for the sake of performance until late\nin the development of the solution. By using arrays to solve\nthe problem, and side-eﬀect free transforms on those tables,\nperformance improvements can be made across systems in\ngeneral. The improvements can be applied at many sites in\nthe program with little fear of incompatibility, and a convic-\ntion that we’re not adding state, but augmenting the lan-\nguage in which we work.\nThe bane of many projects, and the cause of their late-\nness, has been the insistence on not doing optimisation pre-\nmaturely. The reason optimisation at late stages is so dif-\nﬁcult is that many pieces of software are built up with in-\nstances of objects everywhere, even when not needed. Many\nissues with object-oriented design are caused by the idea\nthat an instance is the unit of processing. Object-oriented\ndevelopment practices tend to assume the instance is the\nunit on which code will work, and techniques and standards\nof practice treat collections of objects as collections of indi-\nviduals.\nWhen the basic assumption is that an object is a unique\nand special thing with its own purpose, then the instructions\nto carry out what it needs to do, will necessarily be selected\nin some way dependent on the object. Accessing instructions\nvia the vtable pointer is the usual method by which opera-\ntions are selected. The greater threat is when ﬁve, ten, or\na hundred individual instances, which could have been rep-\nresented as a group, a swarm, or merely an increment on a\nvalue, are processed as a sequence of individuals. There are\nmany cases where an object exists just because it seemed\nto match the real world concept it was trying to represent at\nthe scale of the developer implementing it, rather than be-\ncause it needed to function as a unique individual element\nof which the user would be aware. It’s easy to get caught up\nimplementing features from the perspective of what they are,\nrather than how they are perceived.\n\n\n8.1. WHEN SHOULD WE OPTIMISE?\n137\n8.1\nWhen should we optimise?\nWhen should optimisation be done? When is it truly prema-\nture? The answer lies in data of a diﬀerent sort. Premature\noptimisation is when you optimise something without know-\ning whether it will make a diﬀerence. If you attempt to op-\ntimise something because in your mind it will “speed things\nup a bit”, then it can be considered premature, as it’s not\napparent there is anything to optimise.\nLet’s be clear here, without the data to show that a game\nis running slow, or running out of memory, then all optimisa-\ntions are premature. If an application has not been proﬁled,\nbut feels slow, sluggish, or erratic, then anything you do can-\nnot be objectively deﬁned as improving it, and any improve-\nments you attempt to do cannot be anything but premature\noptimisations. The only way to stop premature optimisation\nis to start with real data. If your application seems slow, and\nhas been proﬁled, and what is considered unacceptable is a\nclearly deﬁned statement based on data, then anything you\ndo to improve the solution will not be premature, because it\nhas been measured, and can be evaluated in terms of failure,\nsuccess, or progress.\nGiven that we think we will need to optimise at some\npoint, and we know optimising without proﬁling is not ac-\ntually optimising, the next question becomes clear. When\nshould you start proﬁling?\nWhen should you start work\non your proﬁling framework?\nHow much game content is\nenough to warrant testing performance? How much of the\ngame’s mechanics should be in before you start testing them\nfor performance spikes?\nConsider a diﬀerent question. Is the performance of your\nﬁnal product optional?\nWould you be able to release the\ngame if you knew it had sections which ran at 5fps on cer-\ntain hardware? If you answer that it’s probably okay for your\ngame to run at around 30fps, then that’s a metric, even if it’s\nquite imprecise. How do you know your game already isn’t\n\n\n138\nCHAPTER 8. OPTIMISATIONS\nrunning at 5fps on one of your target audience’s hardware\nconﬁgurations? If you believe there are lower limits to frame-\nrate, and upper limits to your memory usage, if there is an\nexpected maximum time for a level to load before it’s just\nassumed to be stuck, or a strong belief the game should at\nleast not kill the battery on a phone when it’s running, then\nyou have, in at least some respect, agreed that performance\nis not optional.\nIf performance is not optional, and it requires real work to\noptimise, then start asking yourself a diﬀerent set of ques-\ntions. How long can you delay proﬁling? How much art or\nother content can you aﬀord to redo? How many features\nare you willing to work on without knowing if they can be\nincluded in the ﬁnal game? How long can you work with-\nout feedback on whether any of what you have done, can be\nincluded in the ﬁnal product?\n8.2\nFeedback\nNot knowing you are writing poor performance code doesn’t\njust hurt your application. By not having feedback on their\nwork, developers cannot get better, and myths and tech-\nniques which do not work are reinforced and perpetuated.\nDaniel Kahneman, in his book Thinking, Fast and Slow[?],\nprovides some evidence that you can learn well from imme-\ndiate reactions, but cannot easily pick up skills when the\nfeedback is longer in arriving.\nIn one part, he puts it in\nterms of psychotherapists being able to acquire strong intu-\nitive skills in patient interaction, as they are able to observe\nthe patient’s immediate reactions, but they are less likely to\nbe able to build strong intuitions for identifying the appro-\npriate treatment for a patient, as the feedback is not always\navailable, not always complete, and often delayed. Choosing\nto work without feedback would make no sense, but there\nis little option for many game developers, as third party en-\ngines oﬀer very little in the way of feedback mechanisms for\n\n\n8.2. FEEDBACK\n139\nthose learning or starting out on their projects. They do not\nprovide mechanisms to apply budgets to separate aspects\nof their engines, other than the coarse grain of CPU, GPU,\nPhysics, render, etc. They provide lots of tools to help ﬁx per-\nformance when it has been identiﬁed as an issue, but can\noften provide feedback which is incomplete, or inaccurate\nto the ﬁnal form of the product, as built-in proﬁling tools\nare not always available in fully optimised publishing ready\nbuilds.\nYou must get feedback on what is going on, as otherwise\nthere is a risk the optimisations you will need to do will con-\nsume any polish time you have. Make sure your feedback\nis complete and immediate where possible. Adding metrics\non the status of the performance of your game will help with\nthis. Instant feedback on success or failure of optimisations\nhelps mitigate the sunk cost fallacy that can intrude on ra-\ntional discourse about a direction taken. If a developer has\na belief in a way of doing things, but it’s not helping, then it’s\nbetter to know sooner rather than later. Even the most en-\ntrenched in their ways are more approachable with raw data,\nas curiosity is a good tonic for a developer with a wounded\nego. If you haven’t invested a lot of time and eﬀort into an\napproach, then the feedback is even easier to integrate, as\nyou’re going to be more willing to throw the work away and\nﬁgure out how to do it diﬀerently.\nYou also need to get the feedback about the right thing. If\nyou ﬁnd you’ve been optimising your game for a silky smooth\nframe rate and you think you have an average frame rate of\n60fps, and yet your customers and testers keep coming back\nwith comments about nasty frame spikes and dropout, then\nit could be that you’re not proﬁling the right thing, or not\nproﬁling the right way. Sometimes it can be that you have\nto proﬁle a game while it is being played. Sometimes it can\nbe as simple as remembering to proﬁle frame times on a per\nframe basis, not just an average.\nProﬁling doesn’t have to be about frame rate.\nA frame\n\n\n140\nCHAPTER 8. OPTIMISATIONS\nisn’t a slow thing, something in that frame was slow.\nAn\nold-fashioned, but powerful way to develop software, is to\nprovide budgets to systems and departments. We’re not talk-\ning about ﬁnancial budgets here, but instead time, memory,\nbandwidth, disk space, or other limits which aﬀect the ﬁnal\nproduct directly. If you give your frame a budget of 16ms, and\nyou don’t go over, you have a 60fps game, no ifs, no buts. If\nyou decide you want to maintain good level load times, and\nset yourself a budget of 4 seconds to load level data, then as\nlong as you don’t go over, no one is going to complain about\nyour load times.\nBeyond games, if you have a web-based retail site, you\nmight want to be aware of latency, as it has an eﬀect on your\nusers.\nIt was revealed in a presentation in 2008 by Greg\nLinden that for every additional 100ms of latency, Amazon\nwould experience a loss of 1% in sales. It was also revealed\nthat Google had statistics showing a 20% drop in site traf-\nﬁc was experienced when they added just half a second of\nlatency to page generation. Most scarily of all was a com-\nment from TABB group in 2008, where they mention com-\npany wrecking levels of costs.\nTABB Group estimates that if a broker’s electronic\ntrading platform is 5 milliseconds behind the com-\npetition, it could lose at least 1% of its ﬂow; that’s\n$4 million in revenues per millisecond. Up to 10\nmilliseconds of latency could result in a 10% drop\nin revenues. From there it gets worse. If a broker\nis 100 milliseconds slower than the fastest broker,\nit may as well shut down its FIX engine and be-\ncome a ﬂoor broker.\n1\nIf latency, throughput, frame times, memory usage, or\nanother resource is your limit, then budget for it.\nWhat\n1From THE VALUE OF A MILLISECOND: FINDING THE OPTIMAL SPEED\nOF A TRADING INFRASTRUCTURE by Viraf (Willy) Reporter\n\n\n8.2. FEEDBACK\n141\nwould cripple your business? Are you measuring it? How\nlong can you go without checking that you’re not already out\nof business?\n8.2.1\nKnow your limits\nBuilding budgets into how you work means, you can set re-\nalistic budgets for systems early and have them work at a\ncertain level throughout development knowing they will not\ncause grief later in development. On a project without bud-\ngets, frame spikes may only become apparent near release\ndates as it is only then that all systems are coming together\nto create the ﬁnal product. A system which was assumed to\nbe quite cheap, could cause frame spikes in the ﬁnal prod-\nuct, without any evidence being previously apparent. When\nyou ﬁnally ﬁnd out which system causes the spikes, it may\nbe that it was caused by a change from a very long time ago,\nbut as resources were plentiful in the early times of develop-\nment on the project, the spikes caused by the system would\nhave gone completely unnoticed, ﬂying under the radar. If\nyou give your systems budgets, violations can be recorded\nand raised as issues immediately. If you do this, then prob-\nlems can be caught at the moment they are created, and the\ncause is usually within easy reach.\nBuild or get yourself a proﬁler that runs all the time. En-\nsure your proﬁler can report the overall state of the game\nwhen the frame time goes over budget.\nIt’s highly beneﬁ-\ncial to make it respond to any single system going over bud-\nget. Sometimes you need the data from a number of frames\naround when a violation occurred to really ﬁgure out what is\ngoing on. If you have AI in your game, consider running con-\ntinuous testing to capture performance issues as fast as your\nbuild machine churns out testable builds. In all cases, un-\nless you’re letting real testers run your proﬁler, you’re never\ngoing to get real world proﬁling data. If real testers are going\nto be using your proﬁling system, it’s worth considering how\nyou gather data from it. If it’s possible for you, see if you can\n\n\n142\nCHAPTER 8. OPTIMISATIONS\nget automatically generated proﬁle data sent back to an an-\nalytics or metrics server, to capture issues without requiring\nuser intervention.\n8.3\nA strategy for optimisation\nYou can’t just open up an editor and start optimising. You\nneed a strategy. In this section, we walk through just one\nsuch strategy. The steps have parallels in industries outside\ngame development, where large companies such as Toyota\noptimise as part of their business model.\nToyota has re-\nﬁned their techniques for ensuring maximum performance\nand growth, and the Toyota Production System has been the\ndriving idea behind the Lean manufacturing method for the\nreduction of waste. There are other techniques available, but\nthis subset of steps shares much with many of them.\n8.3.1\nDeﬁne the problem\nDeﬁne your problem. Find out what it is you think is bad.\nDeﬁne it in terms of what is factual, and what is assumed to\nbe a ﬁnal good solution. This can be as simple as saying the\nproblem is that the game is running at 25fps, and you need\nit to be at 30fps. Stick to clear, objective language.\nIt’s important to not include any guesses in this step, so\nstatements which include ideas on what or how to optimise\nshould be prohibited. Consider writing it from the point of\nview of someone using the application, not from the perspec-\ntive of the developer. This is sometimes called quality crite-\nria, or customer requirements.\n\n\n8.3. A STRATEGY\n143\n8.3.2\nMeasure\nMeasure what you need to measure. Unlike measuring ran-\ndomly, targeted measuring is better for ﬁguring out what is\nactually going on, as you are less likely to ﬁnd a pattern in\nirrelevant data. P-hacking or data dredging can lead you to\nfalse convictions about causes of problems.\nAt this stage, you also need to get an idea of the quality\nof your measurements. Run your tests, but then run them\nagain to make sure they’re reproducible. If you can’t repro-\nduce the same results before you have made changes, then\nhow are you going to be sure the changes you have made,\nhave had any eﬀect?\n8.3.3\nAnalyse\nThe ﬁrst step in most informal optimisation strategies: the\nguessing phase. This is when you come up with ideas about\nwhat could be the problem and suggest diﬀerent ways to\ntackle the problem.\nIn the informal optimisation process,\nyou pick the idea which seems best, or at least the most fun\nto implement.\nIn this more formal strategy, we analyse what we have\nmeasured. Sometimes it’s apparent from this step that the\nmeasurements didn’t provide enough direction to come up\nwith a good optimisation plan. If your analysis proves you\ndon’t have good data, the next step should be to rectify your\nability to capture useful data.\nDon’t tackle optimisation\nwithout understanding the cost associated with failing to\nunderstand the problem.\nThis is also the stage to make predictions. Estimate the\nexpected impact of an improvement you plan to make. Don’t\njust lightly guess, have a really good go at guessing with some\nnumber crunching. You won’t be able to do it after the im-\nplementation, as you will have too much knowledge to make\n\n\n144\nCHAPTER 8. OPTIMISATIONS\nan honest guess. You will be suﬀering what some call the\ncurse of knowledge. By doing this, you can learn about how\ngood you are at estimating the impact of your optimisations,\nbut also, you can get an idea of the relative impact of your\nchange before you begin work.\n8.3.4\nImplement\nThe second step in most informal optimisation strategies; the\nimplementation phase. This is when you make the changes\nyou think will ﬁx the problem.\nIf possible, do an experimental implementation of the op-\ntimisation to your solution.\nA program is a solution to a\nproblem, it is a strategy to solve a data transform, and you\nshould remember that when designing your experiment.\nBefore you consider the local version to be working, and\nindeed, worth working on, you must prove it’s useful. Check\nthe measurements you get from the localised experiment are\nin line with your expectations as measured from the inte-\ngrated version.\nIf your optimisation is going to be perfect ﬁrst time, then\nthe experimental implementation will only be used as a proof\nthat the process can be repeated and can be applicable in\nother circumstances. It will only really be useful as a teach-\ning tool for others, in helping them understand the costs of\nthe original process and the expected improvement under\nsimilar constraints.\nIf you are not sure the optimisation will work out ﬁrst\ntime, then the time saved by not doing a full implementation\ncan be beneﬁcial, as a localised experiment can be worked\non faster. It can also be a good place to start when trying to\nbuild an example for third parties to provide support, as a\nsmaller example of the problem will be easier to communi-\ncate through.\n\n\n8.3. A STRATEGY\n145\n8.3.5\nConﬁrm\nThis step is critical in more ways than expected. Some may\nthink it an optional step, but it is essential for retaining the\nvaluable information you will have generated while doing the\noptimisation.\nCreate a report of what you have done, and what you have\nfound. The beneﬁts of doing this are twofold. First, you have\nthe beneﬁt of sharing knowledge of a technique for optimi-\nsation, which clearly can help others hitting the same kind\nof issue. The second is that creating the report can identify\nany errors of measurement, or any steps which can be tested\nto ensure they were actually pertinent to the ﬁnal changes\ncommitted.\nIn a report, others can point out any illogical leaps of rea-\nsoning, which can lead to even better understanding and can\nalso help deny any false assumptions from building up in\nyour understanding of how the machine really works. Writ-\ning a report can be a powerful experience that will give you\nvaluable mental building blocks and the ability to better ex-\nplain what happens under certain conditions.\n8.3.6\nSummary\nAbove all things, keep track. If you can, do your optimisa-\ntion work in isolation of a working test bed. Make sure your\ntimings are reproducible even if you have to get up to date\nwith the rest of the project due to having to work on a bug\nor feature. Making sure you keep track of what you are do-\ning with notes can help you understand what was in your\nhead when you made earlier changes, and what you might\nnot have thought about.\nIt is important to keep trying to improve your ability to\nsee; to observe.\nYou cannot make measurable progress if\nyou cannot measure, and you cannot tell you have made an\n\n\n146\nCHAPTER 8. OPTIMISATIONS\nimprovement without tools for identifying the improvement.\nImprove your tools for measuring when you can. Look for\nways to look. Whenever you ﬁnd that there was no way to\nknow with the tools you had available, either ﬁnd the tools\nyou need or if you can’t ﬁnd them, attempt to make them\nyourself. If you cannot make them yourself, petition others,\nor commission someone else to create them. Don’t give in\nto hopeful optimisations, because they will teach you bad\nhabits and you will learn false facts from random chance\nproving you right.\n8.4\nTables\nTo keep things simple, advice from multiple sources indi-\ncate that keeping your data as vectors has a lot of positive\nbeneﬁts.\nThere are some reasons to use something other\nthan the STL, but learn its quirks, and you can avoid a lot\nof the issues. Whether you use std::vector, or roll your own\ndynamically sized array, it is a good starting place for any\nfuture optimisations. Most of the processing you will do will\nbe reading an array, transforming one array into another, or\nmodifying a table in place. In all these cases, a simple array\nwill suﬃce for most tasks.\nMoving to arrays is good, moving to structure-of-arrays\ncan be better. Not always. It’s very much worth considering\nthe access patterns for your data. If you can’t consider the\naccess patterns, and change is costly, choose based on some\nother criteria, such as readability.\nAnother reason to move away from arrays of objects, or\narrays of structures, is to keep the memory accesses spe-\nciﬁc to their tasks. When thinking about how to structure\nyour data, it’s important to think about what data will be\nloaded and what data will be stored. CPUs are optimised for\ncertain patterns of memory activity. Many CPUs have a cost\nassociated with changing from read operations to write oper-\n\n\n8.4. TABLES\n147\nations. To help the CPU not have to transition between read\nand write, it can be beneﬁcial to arrange writing to mem-\nory in a very predictable and serial manner. An example of\nhot cold separation that doesn’t take into account the impor-\ntance of writing can be seen in the example code in listing\n8.1 that attempts to update values which are used both for\nread and write, but are close neighbours of data which is\nonly used for reading.\n1\nstruct\nPosInfo\n2\n{\n3\nvec3\npos;\n4\nvec3\nvelocity;\n5\nPosInfo ():\n6\npos (1.0f, 2.0f, 3.0f),\n7\nvelocity (4.0f, 5.0f, 6.0f)\n8\n{}\n9\n};\n10\n11\nstruct\nnodes\n12\n{\n13\nstd :: vector <PosInfo > posInfos;\n14\nstd :: vector <vec3 > colors;\n15\nstd :: vector <LifetimeInfo > lifetimeInfos ;\n16\n} nodesystem;\n17\n18\n// ...\n19\n20\nfor (size_t\ntimes = 0; times < trialCount ; times ++)\n21\n{\n22\nstd :: vector <PosInfo >&\nposInfos = nodesystem .posInfos;\n23\nfor (size_t i = 0; i < node_count; ++i)\n24\n{\n25\nposInfos[i]. pos +=\nposInfos[i]. velocity * deltaTime;\n26\n}\n27\n}\nListing 8.1: Mixing hot reads with hot and cold writes\nThe code in listing 8.2 shows a signiﬁcant performance\nimprovement.\n1\nstruct\nnodes\n2\n{\n3\nstd ::vector <vec3 > positions;\n4\nstd ::vector <vec3 > velocities;\n5\nstd ::vector <vec3 > colors;\n6\nstd ::vector <LifetimeInfo > lifetimeInfos ;\n7\n};\n8\n// ...\n9\nnodes\nnodesystem;\n10\n// ...\n11\nfor (size_t\ntimes = 0; times < trialCount ; times ++)\n12\n{\n13\nfor (size_t i = 0; i < node_count; ++i)\n14\n{\n15\nnodesystem.positions[i] +=\nnodesystem. velocities [i] *\ndeltaTime;\n16\n}\n17\n}\nListing 8.2: Ensuring each stream is continuous\nFor the beneﬁt of your cache, structs of arrays can be\n\n\n148\nCHAPTER 8. OPTIMISATIONS\nmore cache-friendly if the data is not strongly related both\nfor reading and writing. It’s important to remember this is\nonly true when the data is not always accessed as a unit, as\none advocate of the data-oriented design movement assumed\nthat structures of arrays were intrinsically cache-friendly,\nthen put the x,y, and z coordinates in separate arrays of\nﬂoats. It is possible to beneﬁt from having each element in\nits own array when you utilise SIMD operations on larger\nlists. However, if you need to access the x,y, or z of an ele-\nment in an array, then you more than likely need to access\nthe other two axes as well. This means that for every ele-\nment you will be loading three cache lines of ﬂoat data, not\none. If the operation involves a lot of other values, then this\nmay overﬁll the cache. This is why it is important to think\nabout where the data is coming from, how it is related, and\nhow it will be used. Data-oriented design is not just a set\nof simple rules to convert from one style to another. Learn\nto see the connections between data. In this case, we see\nthat in some circumstances, it’s better to keep your vector\nas three or four ﬂoats if it’s not commonly used as a value in\nan operation that will be optimised with SIMD instructions.\nThere are other reasons why you might prefer to not store\ndata in trivial SoA format, such as if the data is commonly\nsubject to insertions and deletions. Keeping free lists around\nto stop deletions from mutating the arrays can help alleviate\nthe pressure, but being unable to guarantee every element\nrequires processing moves away from simple homogeneous\ntransformations which are often the point of such data lay-\nout changes.\nIf you use dynamic arrays, and you need to delete el-\nements from them, and these tables refer to each other\nthrough some IDs, then you may need a way to splice the\ntables together in order to process them as you may want\nto keep them sorted to assist with zipping operations. If the\ntables are sorted by the same value, then it can be written\nout as a simple merge operation, such as in listing 8.3.\n1\nProcessJoin( Func\nfunctionToCall ) {\n2\nTableIterator A = t1Table.begin ();\n\n\n8.4. TABLES\n149\n3\nTableIterator B = t2Table.begin ();\n4\nTableIterator C = t3Table.begin ();\n5\nwhile( !A.finished\n&& !B.finished\n&& !C.finished ) {\n6\nif( A == B && B == C ) {\n7\nfunctionToCall ( A, B, C );\n8\n++A; ++B; ++C;\n9\n} else {\n10\nif( A < B || A < C ) ++A;\n11\nif( B < A || B < C ) ++B;\n12\nif( C < A || C < B ) ++C;\n13\n}\n14\n}\n15\n}\nListing 8.3: Zipping together multiple tables by merging\nThis works as long as the == operator knows about the ta-\nble types and can ﬁnd the speciﬁc column to check against,\nand as long as the tables are sorted based on this same col-\numn. But what about the case where the tables are zipped\ntogether without being the sorted by the same columns? For\nexample, if you have a lot of entities which refer to a mod-\nelID, and you have a lot of mesh-texture combinations which\nrefer to the same modelID, then you will likely need to zip to-\ngether the matching rows for the orientation of the entity,\nthe modelID in the entity render data, and the mesh and\ntexture combinations in the models.\nThe simplest way to\nprogram a solution to this is to loop through each table in\nturn looking for matches such as in Listing 8.4.\nThis so-\nlution, though simple to write, is incredibly ineﬃcient, and\nshould be avoided where possible. But as with all things,\nthere are exceptions. In some situations, very small tables\nmight be more eﬃcient this way, as they will remain resident,\nand sorting them could cost more time.\n1\nProcessJoin( Func\nfunctionToCall ) {\n2\nfor( auto A : orientationTable ) {\n3\nfor( auto B : entityRenderableTable ) {\n4\nif( A == B ) {\n5\nfor( auto C : meshAndTextureTable ) {\n6\nif( A == C ) {\n7\nfunctionToCall ( A, B, C );\n8\n}\n9\n}\n10\n}\n11\n}\n12\n}\n13\n}\nListing 8.4: Join by looping through all tables\nAnother thing you have to learn about when working with\ndata which is joined on diﬀerent columns is the use of join\n\n\n150\nCHAPTER 8. OPTIMISATIONS\nstrategies. In databases, a join strategy is used to reduce the\ntotal number of operations when querying across multiple\ntables. When joining tables on a column (or key made up of\nmultiple columns), you have a number of choices about how\nyou approach the problem. In our trivial coded attempt, you\ncan see we simply iterate over the whole table for each table\ninvolved in the join, which ends up being O(nmo) or O(n3)for\nroughly same size tables. This is no good for large tables, but\nfor small ones it’s ﬁne. You have to know your data to decide\nwhether your tables are big2 or not. If your tables are too big\nto use such a trivial join, then you will need an alternative\nstrategy.\nYou can join by iteration, or you can join by lookup3, or\nyou can even join once and keep a join cache around. Keep-\ning the join cache around makes it appear as if you can op-\nerate on the tables as if they are sorted in multiple ways at\nthe same time.\nIt’s perfectly feasible to add auxiliary data which will allow\nfor traversal of a table in a diﬀerent order. We add join caches\nin the same way databases allow for any number of indexes\ninto a table. Each index is created and kept up to date as the\ntable is modiﬁed. In our case, we implement each index the\nway we need to. Maybe some tables are written to in bursts,\nand an insertion sort would be slow, it might be better to\nsort on ﬁrst read, or trash the whole index on modify. In\nother cases, the sorting might be better done on write, as\nthe writes are infrequent, or always interleaved with many\nreads.\n2dependent on the target hardware, how many rows and columns, and\nwhether you want the process to run without trashing too much cache\n3often a lookup join is called a join by hash, but as we know our data, we\ncan use better row search algorithms than a hash when they are available\n\n\n8.5. TRANSFORMS\n151\n8.5\nTransforms\nTaking the concept of schemas a step further, a static\nschema deﬁnition can allow for a diﬀerent approach to iter-\nators. Instead of iterating over a container, giving access to\nan element, a schema iterator can become an accessor for a\nset of tables, meaning the merging work can be done during\niteration, generating a context upon which the transform\noperates. This would beneﬁt large, complex merges which\ndo little with the data, as there would be less memory us-\nage creating temporary tables. It would not beneﬁt complex\ntransforms as it would reduce the likelihood that the next\nset of data is in cache ready for the next cycle.\nAnother aspect of transforms is the separation of what\nfrom how.\nThat is, separating the gathering or loading of\ndata we will transform from the code which ultimately per-\nforms the operations on the data. In some languages, in-\ntroducing map and reduce is part of the basic syllabus, in\nC++, not so much. This is probably because lists aren’t part\nof the base language, and without that, it’s hard to intro-\nduce powerful tools which require them. These tools, map\nand reduce, can be the basis of a purely transform and ﬂow\ndriven program. Turning a large set of data into a single re-\nsult sounds eminently serial, however, as long as one of the\nsteps, the reduce step, is associative, then you can reduce\nin parallel for a signiﬁcant portion of the reduction.\nA simple reduce, one made to create a ﬁnal total from a\nmapping which produces values of zero or one for all match-\ning elements, can be processed as a less and less parallel\ntree of reductions. In the ﬁrst step, all reductions produce\nthe total of all odd-even pairs of elements and produce a new\nlist which goes through the same process. This list reduc-\ntion continues until there is only one item left remaining. Of\ncourse, this particular reduction is of very little use, as each\nreduction is so trivial, you’d be better oﬀassigning an nthof\nthe workload to each of the n cores and doing one ﬁnal sum-\nming. A more complex, but equally useful reduction would\n\n\n152\nCHAPTER 8. OPTIMISATIONS\nbe the concatenation of a chain of matrices. Matrices are\nassociative even if they are not commutative, and as such,\nthe chain can be reduced in parallel the same way building\nthe total worked. By maintaining the order during reduc-\ntion you can apply parallel processing to many things which\nwould normally seem serial, so long as they are associative\nin the reduce step. Not only matrix concatenation, but also\nproducts of ﬂoating point values such as colour modulation\nby multiple causes such as light, diﬀuse, or gameplay related\ntinting. Building text strings can be associative, as can be\nbuilding lists.\n8.6\nSpatial sets for collisions\nIn collision detection, there is often a broad-phase step which\ncan massively reduce the number of potential collisions we\ncheck against.\nWhen ray casting, it’s often useful to ﬁnd\nthe potential intersection via an octree, BSP, or other spatial\nquery accelerator.\nWhen running pathﬁnding, sometimes\nit’s useful to look up local nodes to help choose a starting\nnode for your journey.\nAll spatial data-stores accelerate queries by letting them\ndo less. They are based on some spatial criteria and return\na reduced set which is shorter and thus less expensive to\ntransform into new data.\nExisting libraries which support spatial partitioning have\nto try to work with arbitrary structures, but because all our\ndata is already organised by table, writing adaptors for any\npossible table layout is made simpler. Writing generic algo-\nrithms becomes easier without any of the side eﬀects nor-\nmally associated with writing code that is used in multiple\nplaces. Using the table-based approach, because of its in-\ntention agnosticism (that is, the spatial system has no idea\nit’s being used on data which doesn’t technically belong in\nspace), we can use spatial partitioning algorithms in unex-\n\n\n8.7. LAZY EVALUATION\n153\npected places, such as assigning audio channels by not only\ntheir distance from the listener, but also their volume and\nimportance.\nMaking a 5 dimensional spatial partitioning\nsystem, or even an n dimensional one, would be an invest-\nment. It would only have to be written once and have unit\ntests written once, before it could be used and trusted to do\nsome very strange things. Spatially partitioning by the quest\nprogress for tasks to do seems a little overkill, but getting the\nset of all nearby interesting entities by their location, threat,\nand reward, seems like something an AI might consider use-\nful.\n8.7\nLazy evaluation for the masses\nWhen optimising object-oriented code, it’s quite common to\nﬁnd local caches of completed calculations hidden in mu-\ntable member variables. One trick found in most updating\nhierarchies is the dirty bit, the ﬂag that says whether the\nchild or parent members of a tree have decided this object\nneeds updating. When traversing the hierarchy, this dirty bit\ncauses branching based on data which has only just loaded,\nusually meaning there is no chance to guess the outcome\nand thus in most cases, causes memory to be read in prepa-\nration, when it’s not required.\nIf your calculation is expensive, then you might not want\nto go the route that render engines now use. In render en-\ngines, it’s often cheaper to do every scene matrix concate-\nnation every frame than it is only doing the ones necessary\nand ﬁguring out if they are.\nFor example, in the Pitfalls of Object-Oriented Program-\nming [?] presentation by Tony Albrecht, in the early slides\nhe declares that checking a dirty ﬂag is less useful than not\nchecking it, as when it does fail (the case where the object\nis not dirty) the calculation that would have taken 12 cycles\nis dwarfed by the cost of a branch misprediction (23-24 cy-\n\n\n154\nCHAPTER 8. OPTIMISATIONS\ncles). Things always move on, and in the later talk Pitfalls\nrevisited[?], he notes that the previous improvement gained\nthrough manual devirtualization no longer provides any ben-\neﬁt. Whether it was the improvements in the compiler or the\nchange in hardware, reality will always trump experience.\nIf your calculation is expensive, you don’t want to bog\ndown the game with a large number of checks to see if the\nvalue needs updating. This is the point at which existence-\nbased-processing comes into its own again as existence in\nthe dirty table implies it needs updating, and as a dirty ele-\nment is updated it can be pushing new dirty elements onto\nthe end of the table, even prefetching if it can improve band-\nwidth.\n8.8\nNecessity, or not getting what you\ndidn’t ask for\nWhen you normalise your data you reduce the chance of an-\nother multifaceted problem of object-oriented development.\nC++’s implementation of objects forces unrelated data to\nshare cache lines.\nObjects collect their data by the class, but many objects,\nby design, contain more than one role’s worth of data. This is\nbecause object-oriented development doesn’t naturally allow\nfor objects to be recomposed based on their role in a trans-\naction, and also because C++ needed to provide a method\nby which you could have object-oriented programming while\nkeeping the system level memory allocations overloadable in\na simple way. Most classes contain more than just the bare\nminimum, either because of inheritance or because of the\nmany contexts in which an object can play a part. Unless\nyou have very carefully laid out a class, many operations\nwhich require only a small amount of information from the\nclass will load a lot of unnecessary data into the cache in or-\nder to do so. Only using a very small amount of the loaded\n\n\n8.9. VARYING LENGTH SETS\n155\ndata is one of the most common sins of the object-oriented\nprogrammer.\nEvery virtual call loads in the cache line that contains the\nvirtual-table pointer of the instance. If the function doesn’t\nuse any of the class’s early data, then that will be cache\nline utilisation in the region of only 4%. That’s a memory\nthroughput waste, and cannot be recovered without rethink-\ning how you dispatch your functions. Adding a ﬁnal keyword\nto your class can help when your class calls into its own vir-\ntual functions, but cannot help when they are called via a\nbase type.\nIn practice, only after the function has loaded, can the\nCPU load the data it wants to work on, which can be scat-\ntered across the memory allocated for the class too. It won’t\nknow what data it needs until it has decoded the instruc-\ntions from the function pointed to by the virtual table entry.\n8.9\nVarying length sets\nThroughout the techniques so far, there’s been an implied\ntable structure to the data. Each row is a struct, or each\ntable is a row of columns of data, depending on the need\nof the transforms. When working with stream processing,\nfor example, with shaders, we would normally use ﬁxed size\nbuﬀers.\nMost work done with stream processing has this\nsame limitation, we tend to have a ﬁxed number of elements\nfor both sides.\nFor ﬁltering where the input is known to be a superset of\nthe output, there can be a strong case for an annealing struc-\nture. Outputting to multiple separate vectors, and concate-\nnating them in a ﬁnal reduce. Each transform thread has\nits own output vector, the reduce step would ﬁrst generate\na total and a start position for each reduce entry and then\nprocesses the list of reduces onto the ﬁnal contiguous mem-\n\n\n156\nCHAPTER 8. OPTIMISATIONS\nory. A parallel preﬁx sum would work well here, but simple\nlinear passes would suﬃce.\nIf the ﬁltering was a stage in a radix sort, counting sort,\nor something which uses a similar histogram for generating\noﬀsets, then a parallel preﬁx sum would reduce the latency\nto generate the oﬀsets. A preﬁx sum is the running total of a\nlist of values. The radix sort output histogram is a great ex-\nample because the bucket counts indicate the starting points\nthrough the sum of all histogram buckets that come prior.\non = Pn−1\ni=0 bi.\nThis is easy to generate in serial form, but\nin parallel, we have to consider the minimum required op-\nerations to produce the ﬁnal result.\nIn this case, we can\nremember that the longest chain will be the value of the last\noﬀset, which is a sum of all the elements. This is normally\noptimised by summing in a binary tree fashion. Dividing and\nconquering: ﬁrst summing all odd numbered slots with all\neven numbered slots, then doing the same, but for only the\noutputs of the previous stage.\nA\n\u000f\n \nB\n\u000f\nC\n\u000f\n\"\nD\n\u000f\na\n\u000f\nab\n\u000f\n(\nc\n\u000f\ncd\n\u000f\na\nab\nabc\nabcd\nThen once you have the last element, backﬁll all the other\nelements you didn’t ﬁnish on your way to making the last\nelement. When you come to write this in code, you will ﬁnd\nthese backﬁlled values can be done in parallel while making\nthe longest chain.\nThey have no dependency on the ﬁnal\nvalue so can be given over to another process, or managed\nby some clever use of SIMD.\n\n\n8.9. VARYING LENGTH SETS\n157\na\n\u000f\nab\n\u000f\n!\nc\n\u000f\nabcd\n\u000f\na\nab\nabc\nabcd\nParallel preﬁx sums provide a way to reduce latency, but\nare not a general solution which is better than doing a lin-\near preﬁx sum. A linear preﬁx sum uses far fewer machine\nresources to do the same thing, so if you can handle the\nlatency, then simplify your code and do the sum linearly.\nAlso, for cases where the entity count can rise and fall,\nyou need a way of adding and deleting without causing any\nhiccups. For this, if you intend to transform your data in\nplace, you need to handle the case where one thread can be\nreading and using the data you’re deleting. To do this in a\nsystem where objects’ existence was based on their memory\nbeing allocated, it would be very hard to delete objects that\nwere being referenced by other transforms. You could use\nsmart pointers, but in a multi-threaded environment, smart\npointers cost a mutex to be thread safe for every reference\nand dereference. This is a high cost to pay, so how do we\navoid it? There are at least two ways.\nDon’t have a mutex. One way to avoid the mutex is to\nuse a smart pointer type which is bound to a single thread.\nIn some game engines, there are smart pointer types that\ninstead of keeping a mutex, store an identiﬁer for the thread\nthey belong to. This is so they can assert every access is\nmade by the same thread. For performance considerations,\nthis data doesn’t need to be present in release builds, as\nthe checks are done to protect against misuse at runtime\ncaused by decisions made at compile time. For example, if\nyou know the data should not be used outside of the audio\nsubsystem, and the audio subsystem is running on a single\nthread of its own, lock it down and tie the memory allocation\nto the audio thread. Any time the audio system memory is\naccessed outside of the audio thread, it’s either because the\naudio system is exposing memory to the outside systems or\n\n\n158\nCHAPTER 8. OPTIMISATIONS\nit’s doing more work than it should in any callback functions.\nIn either case, the assert will catch the bad behaviour, and\nﬁxes can be made to the code to counter the general issue,\nnot the speciﬁc case.\nDon’t delete. If you are deleting in a system that is con-\nstantly changing, then you would normally use pools any-\nway.\nBy explicitly not deleting, by doing something else\ninstead, you change the way all code accesses data.\nYou\nchange what the data represents. If you need an entity to\nexist, such as a CarDriverAI, then it can stack up on your\ntable of CarDriverAIs while it’s in use, but the moment it’s\nnot in use, it won’t get deleted, but instead marked as not\nused. This is not the same as deleting, because you’re say-\ning the entity is still valid, won’t crash your transform, but\nit can be skipped as if it were not there until you get around\nto overwriting it with the latest request for a CarDriverAI.\nKeeping dead entities around can be as cheap as keeping\npools for your components, as long as there are only a few\ndead entities in your tables.\n8.10\nJoins as intersections\nSometimes, normalisation can mean you need to join tables\ntogether to create the right situation for a query.\nUnlike\nRDBMS queries, we can organise our queries much more\ncarefully and use the algorithm from merge sort to help us\nzip together two tables. As an alternative, we don’t have to\noutput to a table, it could be a pass-through transform which\ntakes more than one table and generates a new stream into\nanother transform. For example, per entityRenderable, join\nwith entityPosition by entityID, to transform with AddRen-\nderCall( Renderable, Position ).\n\n\n8.11. DATA-DRIVEN TECHNIQUES\n159\n8.11\nData-driven techniques\nApart from ﬁnite state machines, there are some other com-\nmon forms of data-driven coding practices.\nSome are not\nvery obvious, such as callbacks.\nSome are very obvious,\nsuch as scripting.\nIn both these cases, data causing the\nﬂow of code to change will cause the same kind of cache and\npipeline problems as seen in virtual calls and ﬁnite state ma-\nchines.\nCallbacks can be made safer by using triggers from event\nsubscription tables. Rather than have a callback which ﬁres\noﬀwhen a job is done, have an event table for done jobs so\ncallbacks can be called once the whole run is ﬁnished. For\nexample, if a scoring system has a callback from “badGuy-\nDies”, then in an object-oriented message watcher you would\nhave the scorer increment its internal score whenever it re-\nceived the message that a badGuyDies. Instead, run each\nof the callbacks in the callback table once the whole set of\nbadGuys has been checked for death. If you do that and ex-\necute every time all the badGuys have had their tick, then\nyou can add points once for all badGuys killed. That means\none read for the internal state, and one write. Much better\nthan multiple reads and writes accumulating a ﬁnal score.\nFor scripting, if you have scripts which run over multi-\nple entities, consider how the graphics kernels operate with\nbranches, sometimes using predication and doing both sides\nof a branch before selecting a solution. This would allow you\nto reduce the number of branches caused merely by inter-\npreting the script on demand. If you go one step further an\nactually build SIMD into the scripting core, then you might\nﬁnd you can use scripts for a very large number of entities\ncompared to traditional per entity serial scripting. If your\nSIMD operations operate over the whole collection of entities,\nthen you will pay almost no price for script interpretation4.\n4Take a look at the section headed The Massively Vectorized Virtual\nMachine on the BitSquid blog http://bitsquid.blogspot.co.uk/2012/10/a-\ndata-oriented-data-driven-system-for.html\n\n\n160\nCHAPTER 8. OPTIMISATIONS\n8.11.1\nSIMD\nSIMD operations can be very beneﬁcial as long as you have\na decent chunk of work to do, such as making an operation\nthat handles updating positions of particles (see listing 8.5).\nThis example of SIMDifying some code is straightforward,\nand in tests ran about four times faster than both the array\nof structs code and the na¨ıve struct of arrays code.\n1\nvoid\nSimpleUpdateParticles ( particle_buffer *pb , float\ndelta_time\n) {\n2\nfloat g = pb ->gravity;\n3\nfloat\ngd2 = g * delta_time * delta_time * 0.5f;\n4\nfloat gd = g * delta_time;\n5\nfor( int i = 0; i < NUM_PARTICLES ; ++i ) {\n6\npb ->posx[i] += pb ->vx[i] * delta_time;\n7\npb ->posy[i] += pb ->vy[i] * delta_time + gd2;\n8\npb ->posz[i] += pb ->vz[i] * delta_time;\n9\npb ->vy[i] += gd;\n10\n}\n11\n}\n12\n13\nvoid\nSIMD_SSE_UpdateParticles ( particle_buffer *pb , float\ndelta_time ) {\n14\nfloat g = pb ->gravity;\n15\nfloat\nf_gd = g * delta_time;\n16\nfloat\nf_gd2 = pb ->gravity * delta_time * delta_time * 0.5f;\n17\n18\n__m128\nmmd = _mm_setr_ps( delta_time , delta_time , delta_time ,\ndelta_time );\n19\n__m128\nmmgd = _mm_load1_ps ( &f_gd );\n20\n__m128\nmmgd2 = _mm_load1_ps ( &f_gd2 );\n21\n22\n__m128 *px = (__m128 *)pb ->posx;\n23\n__m128 *py = (__m128 *)pb ->posx;\n24\n__m128 *pz = (__m128 *)pb ->posz;\n25\n__m128 *vx = (__m128 *)pb ->vx;\n26\n__m128 *vy = (__m128 *)pb ->vy;\n27\n__m128 *vz = (__m128 *)pb ->vz;\n28\n29\nint\niterationCount = NUM_PARTICLES / 4;\n30\nfor( int i = 0; i < iterationCount ; ++i ) {\n31\n__m128\ndx = _mm_mul_ps(vx[i], mmd );\n32\n__m128\ndy = _mm_add_ps( _mm_mul_ps(vy[i], mmd ), mmgd2 );\n33\n__m128\ndz = _mm_mul_ps(vz[i], mmd );\n34\n__m128\nnewx = _mm_add_ps(px[i], dx);\n35\n__m128\nnewy = _mm_add_ps(py[i], dy);\n36\n__m128\nnewz = _mm_add_ps(pz[i], dz);\n37\n__m128\nnewvy = _mm_add_ps(vy[i], mmgd);\n38\n_mm_store_ps (( float *)(px+i), newx);\n39\n_mm_store_ps (( float *)(py+i), newy);\n40\n_mm_store_ps (( float *)(pz+i), newz);\n41\n_mm_store_ps (( float *)(vy+i), newvy);\n42\n}\n43\n}\nListing 8.5: Simple particle update with SIMD\nIn many optimising compilers, simple vectorisation is car-\nried out by default, but only as far as the compiler can ﬁgure\nthings out. It’s not often very easy to ﬁgure these things out.\nSIMD operations on machines which support SSE, allow\n\n\n8.12. STRUCTS OF ARRAYS\n161\nyou to get more data into the CPU in one go. Many people\nstarted out by putting their 3D vectors into SIMD units, but\nthat doesn’t allow full utilisation of the SIMD pipeline. The\nexample loads in four diﬀerent particles at the same time,\nand updates them all at the same time too. This very simple\ntechnique also means you don’t have to do anything clever\nwith the data layout, as you can just use a na¨ıve struct of ar-\nrays to prepare for SIMDiﬁcation once you ﬁnd it has become\na bottleneck.\n8.12\nStructs of arrays\nIn addition to all the other beneﬁts of keeping your runtime\ndata in a database style format, there is the opportunity to\ntake advantage of structures of arrays rather than arrays of\nstructures. SoA has been coined as a term to describe an\naccess pattern for object data. It is okay to keep hot and\ncold data side by side in an SoA object as data is pulled into\nthe cache by necessity rather than by accidental physical\nlocation.\nIf your animation timekey/value class resembles this:\n1\nstruct\nKeyframe\n2\n{\n3\nfloat time , x,y,z;\n4\n};\n5\nstruct\nStream\n6\n{\n7\nKeyframe *keyframes;\n8\nint\nnumKeys;\n9\n};\nListing 8.6: animation timekey/value class\nthen when you iterate over a large collection of them, all\nthe data has to be pulled into the cache at once. If we assume\nthat a cache line is 64 bytes, and the size of ﬂoats is 4 bytes,\nthe Keyframe struct is 16 bytes. This means that every time\nyou look up a key time, you accidentally pull in four keys and\nall the associated keyframe data. If you are doing a binary\n\n\n162\nCHAPTER 8. OPTIMISATIONS\nsearch of a 128 key stream, it could mean you end up loading\n64 bytes of data and only using 4 bytes of it in up to 5 of the\nsteps. If you change the data layout so the searching takes\nplace in one array, and the data is stored separately, then\nyou get structures that look like this:\n1\nstruct\nKeyData\n2\n{\n3\nfloat x,y,z;\n4\n//\nconsider\npadding\nout to 16\nbytes\nlong\n5\n};\n6\nstruct\nstream\n7\n{\n8\nfloat *times;\n9\nKeyData *values;\n10\nnumKeys;\n11\n};\nListing 8.7: struct of arrays\nDoing this means that for a 128 key stream, the key times\nonly take up 8 cache lines in total, and a binary search is go-\ning to pull in at most three of them, and the data lookup is\nguaranteed to only require one, or two at most if your data\nstraddles two cache lines due to choosing memory space ef-\nﬁciency over performance.\nDatabase technology was here ﬁrst. In DBMS terms, it’s\ncalled column-oriented databases and they provide better\nthroughput for data processing over traditional row-oriented\nrelational databases simply because irrelevant data is not\nloaded when doing column aggregations or ﬁltering. There\nare other features that make column-store databases more\neﬃcient, such as allowing them to collect many keys under\none value instead of having a key value 1:1 mapping, but\ndatabase advances are always being made, and it’s worth\nhunting down current literature to see what else might be\nworth migrating to your codebase.\n",
      "page_number": 138,
      "chapter_number": 8,
      "summary": "This chapter covers optimisations and. Key topics include data, optimisations, and optimising. We ﬁnd in\nmost cases, data movement is what really costs us the most.",
      "keywords": [
        "data",
        "time",
        "n’t",
        "Optimisations",
        "tables",
        "work",
        "game",
        "arrays",
        "SIMD",
        "feedback",
        "cache",
        "ﬁnal",
        "system",
        "reduce",
        "frame"
      ],
      "concepts": [
        "data",
        "optimisations",
        "optimising",
        "optimise",
        "optimised",
        "tables",
        "uses",
        "useful",
        "time",
        "timings"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 10,
          "title": "Segment 10 (pages 86-98)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 18,
          "title": "Segment 18 (pages 348-366)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 22,
          "title": "Segment 22 (pages 184-192)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 1,
          "title": "Segment 1 (pages 1-15)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 17,
          "title": "Segment 17 (pages 326-347)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 9,
      "title": "Helping the compiler",
      "start_page": 166,
      "end_page": 181,
      "detection_method": "regex_chapter",
      "content": "Chapter 9\nHelping the compiler\nCompilers are rather good at optimising code, but there\nare ways in which we code that make things harder. There\nare tricks we use that break assumptions the compiler can\nmake. In this section, we will look at some of the things we\ndo that we should try not to, and we look at how to introduce\nsome habits that will make it easier for the compiler to do\nwhat we mean, not what we say.\n9.1\nReducing order dependence\nIf the compiler is unable to deduce that the order of oper-\nations is not important to you, then it won’t be able to do\nwork ahead of schedule.\nWhen composing the translated\ncode into intermediate representation form, there’s a quality\nsome compilers use called static single assignment form, or\nSSA. The idea is that you never modify variables once they\nare initially assigned, and instead create new ones when a\nmodiﬁcation becomes required. Although you cannot actu-\nally use this in loops, as any operations which carry through\nwould require the assigned value to change, you can get close\n163\n\n\n164\nCHAPTER 9. HELPING THE COMPILER\nto it, and in doing so, you can help the compiler understand\nwhat you mean when you are modifying and assigning val-\nues. Skimming the available features and tutorials in lan-\nguages such as Haskell, Erlang, and Single-Assignment C\ncan give you the necessary hints to write your code in a sin-\ngle assignment manner.\nWriting code like this means you will see where the com-\npiler has to branch more easily, but also, you can make\nyour writes more explicit, which means that where a com-\npiler might have had to break away from writing to memory,\nyou can force it to write in all cases, making your process-\ning more homogeneous, and therefore more likely to stream\nbetter.\n9.2\nReducing memory dependency\nLinked lists are expensive due to dependencies, but depen-\ndencies of a diﬀerent sort. Memory being slow, you want to\nbe able to load it in time for your operations, but when the\naddress you need to load is itself still being loaded, you can’t\ncheat anymore. Pointer driven tree algorithms are slow, not\nbecause of the memory lookups, but because the memory\nlookups are chained together.\nIf you want to make your map or set implementation run\nfaster, move to a wide node algorithm such as a B-tree, or\nB*-tree. Hopefully, at some point soon, the STL will allow\nyou to chose the method by which std::map and std::set\nare implemented.\nWhen you have an entity component system using the\ncompositional style, and you have a pointer based composi-\ntion, then the two layers of pointers to get to the component\nis slowing you down. If you have pointers inside those com-\nponents, you’re just compounding the problem.\nAttempt where possible to reduce the number of hops to\n\n\n9.3. WRITE BUFFER AWARENESS\n165\nget to the data you need. Each hop that depends on previous\ndata is potentially a stall waiting for main memory.\n9.3\nWrite buﬀer awareness\nWhen writing, the same issues need to be considered as\nwhen reading. Try to keep things contiguous where possible.\nTry to keep modiﬁed values separated from read-only values,\nand also from write-only values.\nIn short, write contiguously, in large amounts at a time,\nand use all the bytes, not a small part of them. We need to\ntry to do this, as not only does it help with activation and\ndeactivation of diﬀerent memory pages, but also opens up\nopportunities for the compiler to optimise.\nWhen you have a cache, sometimes it’s important to ﬁnd\nways to bypass it. If you know that you won’t be using the\ndata you’re loading more than once or at least not soon\nenough to beneﬁt from caching, then it can be useful to\nﬁnd ways to avoid polluting the cache. When you write your\ntransforms in simple ways, it can help the compiler promote\nyour operations from ones which pollute the cache, to in-\nstructions that bypass the cache completely. These stream-\ning operations beneﬁt the caches by not evicting randomly\naccessed memory.\nIn the article What every programmer should know about\nmemory[?], Ulrich Drepper talks about many aspects of\nmemory which are interesting to get the most out of your\ncomputer hardware. In the article, he used the term non-\ntemporality to describe the kinds of operations we call\nstreaming.\nThese non-temporal memory operations help\nbecause they bypass the cache completely, which na¨ıvely\nwould seem to be a poor choice, but as the name suggests,\nstreaming data is not likely to be recalled into registers any\ntime soon, so having it available in the cache is pointless, and\n\n\n166\nCHAPTER 9. HELPING THE COMPILER\nmerely evicts potentially useful data. Streaming operations,\ntherefore, allow you some control over what you consider\nimportant to be in cache, and what is almost certainly not.\n9.4\nAliasing\nAliasing is when it’s possible for pointers to reference the\nsame memory, and therefore require reloading between reads\nif the other pointer has been written to.\nA simple exam-\nple could be where the value we’re looking for is speciﬁed\nby reference, rather than by value, so if any functions that\ncould potentially aﬀect the memory being referred to by that\nlookup reference, then the reference must be re-read before\ndoing a comparison. The very fact it is a pointer, rather than\na value, is what causes the issue.\nA reason to work with data in an immutable way comes\nin the form of preparations for optimisation. C++, as a lan-\nguage, provides a lot of ways for the programmer to shoot\nthemselves in the foot, and one of the best is that pointers to\nmemory can cause unexpected side eﬀects when used with-\nout caution. Consider this piece of code:\n1\nchar\nbuffer[ 100 ];\n2\nbuffer [0] = ’X’;\n3\nmemcpy( buffer +1, buffer , 98 );\n4\nbuffer[ 99 ] = ’\\0’;\nListing 9.1: byte copying\nThis is perfectly correct code if you just want to get a\nstring ﬁlled with 99 ’X’s. However, because this is possible,\nmemcpy has to copy one byte at a time. To speed up copying,\nyou normally load in a lot of memory locations at once, then\nsave them out once they are all in the cache. If your input\ndata can be modiﬁed by writes to your output buﬀer, then\nyou have to tread very carefully. Now consider this:\n\n\n9.5. RETURN VALUE OPTIMISATION\n167\n1\nint q=10;\n2\nint p[10];\n3\nfor( int i = 0; i < q; ++i )\n4\np[i] = i;\nListing 9.2: trivially parallelisable code\nThe compiler can ﬁgure out that q is unaﬀected, and can\nhappily unroll this loop or replace the check against q with\na register value. However, looking at this code instead:\n1\nvoid\nfoo( int* p, const\nint &q )\n2\n{\n3\nfor( int i = 0; i < q; ++i)\n4\np[i] = i;\n5\n}\n6\n7\nint q=10;\n8\nint p[10];\n9\nfoo( p, q );\nListing 9.3: potentially aliased int\nThe compiler cannot tell that q is unaﬀected by operations\non p, so it has to store p and reload q every time it checks the\nend of the loop. This is called aliasing, where the address of\ntwo variables that are in use are not known to be diﬀerent,\nso to ensure functionally correct code, the variables have to\nbe handled as if they might be at the same address.\n9.5\nReturn value optimisation\nIf you want to return multiple values, the normal way is to\nreturn via reference arguments, or by ﬁlling out an object\npassed by reference. In many cases, return by value can be\nvery cheap as many compilers can turn it into a non-copy\noperation.\nWhen a function attempts to return a structure by con-\nstructing the value in place during the return, it is allowed\nto move the construction straight into the value that will re-\n\n\n168\nCHAPTER 9. HELPING THE COMPILER\nceive it, without doing a copy at all.\nUtilising std::pair or other small temporary structs can\nhelp by making more of your code run on value types, which\nare not only inherently easier to reason about, but also easier\nto optimise by a compiler.\n9.6\nCache line utilisation\nIt is a truth universally acknowledged that a single memory\nrequest will always read in at least one complete cache line.\nThat complete cache line will contain multiple bytes of data.\nAt the time of writing this book, the most common cache\nline size seems to have stabilized at 64 bytes. With this in-\nformation, we can speculate about what data will be cheap\nto access purely by their location relative to other data.\nIn Searching (Chapter 6), we utilise this information to\ndecide the location and quantity of data that is available for\ncreating the rapid lookup table included in the example that\nuses a two-layer linear search that turns out to be faster than\na binary search.\nWhen you have an object you will be loading into memory,\ncalculate the diﬀerence between a cache line and the size of\nthe object. That diﬀerence is how much memory you have\nleft to place data you can read for free. Use this space to\nanswer the common questions you have about the class, and\nyou will often see speedups as there will be no extra memory\naccesses.\nFor example, consider a codebase that partially migrated\nto components, and still has an entity class which points\nto optional rows in component arrays. In this case, we can\ncache the fact the entity has elements in those arrays in the\nlatter part of the entity class as a bitset. This would mean the\nentity on entity interactions could save doing a lookup into\nthe arrays if there was no matching row. It can also improve\n\n\n9.7. FALSE SHARING\n169\nrender performance as the renderer can immediately tell that\nthere is no damage done, so will just show a full health icon\nor nothing at all.\nIn the example code in listing ?? in Chapter ??, an at-\ntempt is made to use more of an object’s initial cache line to\nanswer questions about the rest of the object, and you can\nsee various levels of success in the results. In the case of\nfully caching the result, a massive improvement was gained.\nIf the result cannot be quickly calculated and needs to be cal-\nculated on demand, caching that there was something to do\nwas a factor of four improvement. Caching the result when\nyou can, had diﬀering levels of performance improvement,\nbased on the likelihood of hitting a cached response. In all,\nusing the extra data you have on your cache line is always\nan improvement over simple checking.\ni5-4430 @ 3.00GHz\nAverage 11.31ms [Simple, check the map]\nAverage\n9.62ms [Partially cached query (25%)]\nAverage\n8.77ms [Partially cached presence (50%)]\nAverage\n3.71ms [Simple, cache presence]\nAverage\n1.51ms [Partially cached query (95%)]\nAverage\n0.30ms [Fully cached query]\nSo, in summary, keep in mind, every time you load any\nmemory at all, you are loading in a full cache line of bytes.\nCurrently, with 64-byte cache lines, that’s a 4x4 matrix of\nﬂoats, 8 doubles, 16 ints, a 64 character ASCII string, or\n512 bits.\n9.7\nFalse sharing\nWhen a CPU core shares no resources with another, it can al-\nways operate at full speed independently, right? Well, some-\ntimes no. Even if the CPU core is working on independent\ndata, there are times it can get choked up on the cache.\n\n\n170\nCHAPTER 9. HELPING THE COMPILER\nOn the opposite side of the same issue as writing linearly,\nwhen you are writing out data to the same cache line, it can\ninterfere with threading. Due to the advancement of compil-\ners, it seems this happens far less frequently than it should,\nand when attempting to reproduce the issue to give ideas on\nthe eﬀect it can have, only by turning oﬀoptimisations is it\npossible to witness the eﬀect with trivial examples.\nThe idea is that multiple threads will want to read from\nand write to the same cache line, but not necessarily the\nsame memory addresses in the cache line. It’s relatively easy\nto avoid this by ensuring any rapidly updated variables are\nkept local to the thread, whether on the stack or in thread lo-\ncal storage. Other data, as long as it’s not updated regularly,\nis highly unlikely to cause a collision.\nThere has been a lot of talk about this particular problem,\nbut the real-world is diﬀerent from the real-world problems\nsupposed. Always check your problems are real after optimi-\nsation, as well as before, as even the high and mighty have\nfallen for this as a cause of massive grief.\n1\nvoid\nFalseSharing () {\n2\nint sum =0;\n3\nint\naligned_sum_store [ NUM_THREADS ] __attribute__ (( aligned (64)))\n;\n4\n#pragma\nomp\nparallel\nnum_threads (NUM_THREADS )\n5\n{\n6\nint me = omp_get_thread_num ();\n7\naligned_sum_store [me] = 0;\n8\nfor (int i = me; i < ELEMENT_COUNT ; i +=\nNUM_THREADS ) {\n9\naligned_sum_store [me] +=\nCalcValue( i );\n10\n}\n11\n#pragma\nomp\natomic\n12\nsum +=\naligned_sum_store [me];\n13\n}\n14\n}\n15\n16\nvoid\nLocalAccumulator () {\n17\nint sum =0;\n18\n#pragma\nomp\nparallel\nnum_threads (NUM_THREADS )\n19\n{\n20\nint me = omp_get_thread_num ();\n21\nint\nlocal_accumulator = 0;\n22\nfor (int i = me; i < ELEMENT_COUNT ; i +=\nNUM_THREADS ) {\n23\nlocal_accumulator\n+=\nCalcValue( i );\n24\n}\n25\n#pragma\nomp\natomic\n26\nsum +=\nlocal_accumulator ;\n27\n}\n28\n}\nListing 9.4: False sharing\nSo, how can you tell if this problem is real or not? If your\n\n\n9.8. SPECULATIVE EXECUTION AWARENESS\n171\nmulti-threaded code is not growing at a linear rate of pro-\ncessing as you add cores, then you might be suﬀering from\nfalse sharing, look at the where your threads are writing, and\ntry to remove the writes from shared memory where possible\nuntil the last step. The common example given is of adding\nup some arrays and updating the sum value in some global\nshared location, such as in listing 9.4.\nIn the FalseSharing function, the sums are written to as\na shared resource, and each thread will cause the cache to\nclean up and handle that line being dirty for each of the other\ncores before they can update their elements in the cache line.\nIn the second function, LocalAccumulator, each thread sums\nup their series before writing out the result.\n9.8\nSpeculative execution awareness\nSpeculative execution helps as it executes instructions and\nprepares data before we arrive at where we might need them,\neﬀectively allowing us to do work before we know we need it,\nbut sometimes it could have a detrimental eﬀect. For exam-\nple, consider the codebase mentioned previously, that had\npartially migrated to components. The bit arrays of which\noptional tables it was currently resident could lead, through\nspeculation, to loading in details about those arrays. With\nspeculative execution, you will need to watch out for the code\naccidentally prefetching data because it was waiting to ﬁnd\nout the result of a comparison. These speculative operations\nhave been in the news with SPECTRE and MELTDOWN vul-\nnerabilities.\nThese branch prediction caused reads can be reduced by\npre-calculating predicates where possible, storing the result\nof doing a common query in your rows is a big win for most\nmachines and a massive one for machines with poor mem-\nory latency or high CPU bandwidth to memory bandwidth\nratios. Moving to techniques where branch mispredictions\n\n\n172\nCHAPTER 9. HELPING THE COMPILER\ncause the smallest side-eﬀects to the data is a generally good\nidea. Even caching only when you can, storing the result\nback in the initial section, can save bandwidth over time.\nIn the cache line utilisation section, the numbers showed\nthat the possibility of getting data seemed to aﬀect how fast\nthe process went, much more than it would be expected,\nwhich leads to a belief that speculative loads of unnecessary\ndata were potentially harming overall throughput.\nEven if all you are able to cache is whether a query will\nreturn a result, it can be beneﬁcial. Avoiding lookups into\ncomplex data structures by keeping data on whether or not\nthere are entries matching that description can give speed\nboosts with very few detrimental side-eﬀects.\n9.9\nBranch prediction\nOne of the main causes of stalling in CPUs comes down to not\nhaving any work to do, or having to unravel what they have\nalready done because they predicted badly. If code is specu-\nlatively executed, and requests memory that is not needed,\nthen the load has become a wasteful use of memory band-\nwidth. Any work done will be rejected and the correct work\nhas to be started or continued.\nTo get around this issue,\nthere are ways to make code branch free, but another way is\nto understand the branch prediction mechanism of the CPU\nand help it out.\nIf you make prediction trivial, then the predictor will get\nit right most of the time. If you ensure the conditions are\nconsistently true or false in large chunks, the predictor will\nmake fewer mistakes. A trivial example such as in listing 9.5\nwill predict to either do or not do the accumulation, based\non the incoming data.\nThe work being done here can be\noptimised away by most compilers using a conditional move\ninstruction if the CPU supports it.\nIf you make the work\n\n\n9.9. BRANCH PREDICTION\n173\ndone a little more realistic, then even with full optimisations\nturned on, you can see a very large diﬀerence1 if you can\nsort the data so the branches are much more predictable.\nThe other thing to remember is that if the compiler can help\nyou, let it. The optimised trivial example is only trivial in\ncomparison to other common workloads, but if your actual\nwork is trivially optimised into a conditional execution, then\nsorting your data will be a waste of eﬀort.\n1\nint\nSumBasedOnData () {\n2\nint sum =0;\n3\nfor (int i = 0; i < ELEMENT_COUNT ; i++) {\n4\nif( a[i] > 128 ) {\n5\nsum += b[i];\n6\n}\n7\n}\n8\nreturn\nsum;\n9\n}\nListing 9.5: Doing work based on data\ni5-4430 @ 3.00GHz\nAverage\n4.40ms [Random branching]\nAverage\n1.15ms [Sorted branching]\nAverage\n0.80ms [Trivial Random branching]\nAverage\n0.76ms [Trivial Sorted branching]\nBranching happens because of data, and remember the\nreason why branching is bad is not that jumps are expen-\nsive, but the work being done because of a misprediction will\nhave to be undone. Because of this, it’s valuable to remem-\nber that a vtable pointer is data too. When you don’t batch\nupdate, you won’t be getting the most out of your branch\npredictor, but even if you don’t hit the branch predictor at\nall, you may still be committing to sequences of instructions\nbased on data.\n1On an i5-4430 the unsorted sum ran in 4.2ms vs the sorted sum run-\nning in 0.8ms. The trivial version, which was likely mostly compiled into\nCMOVs, ran in 0.4ms both sorted and unsorted\n\n\n174\nCHAPTER 9. HELPING THE COMPILER\n9.10\nDon’t get evicted\nIf you’re working with others, as many are, then perhaps\nthe simplest solution to a lot of issues with poor cache per-\nformance has to take into account other areas of the code.\nIf you’re working on a multi-core machine (you are, unless\nwe went back in time), then there’s a good chance that all\nprocesses are sharing and contending for the caches on the\nmachine. Your code will be evicted from the cache, there is\nno doubt. So will your data. To reduce the chance or fre-\nquency of your code and data being evicted, keep both code\nand data small and process in bursts when you can.\nIt’s very simple advice. Not only is small code less likely to\nbe evicted, but if it’s done in bursts it will have had a chance\nto get a reasonable amount of work before being overwritten.\nSome cache architectures don’t have any way to tell if the\nelements in the cache have been used recently, so they rely\non when they were added as a metric for what should be\nevicted ﬁrst. In particular, some Intel CPUs can have their\nL1 and L2 cache lines evicted because of L3 needing to evict,\nbut L3 doesn’t have full access to LRU information. The Intel\nCPUs in question have some other magic that reduces the\nlikelihood of this happening, but it does happen.\nTo that end, try to ﬁnd ways to guarantee to the compiler\nthat you are working with aligned data, in arrays that are\nmultiples of 4 or 8, or 16, so the compiler doesn’t need to\nadd preambles and postamble code to handle unaligned, or\nirregularly sized arrays. It can be better to have 3 more dead\nelements in an array and handle it as an array of length N ∗4.\n9.11\nAuto vectorisation\nAuto vectorisation will help your applications run faster just\nby enabling it and forming your code in such a way that it\n\n\n9.11. AUTO VECTORISATION\n175\nis possible for the compiler to make safe assumptions, and\nchange the instructions from scalar to vector.\n1\nvoid\nAmplify( float *a, float mult , int\ncount )\n2\n{\n3\nfor( int i = 0; i < count; ++i ) {\n4\na[i] *= mult;\n5\n}\n6\n}\nListing 9.6: Trivial ampliﬁcation function\nThere are many trivial examples of things which can be\ncleanly vectorised. The ﬁrst example is found in listing 9.6,\nwhich is simple enough to be vectorised by most compilers\nwhen optimisations are turned on. The issue is that there\nare few guarantees with the code, so even though it can be\nquite fast to process the data, this code will take up a lot\nmore space than is necessary in the instruction cache.\n1\ntypedef\nfloat\nf16\n__attribute__ (( aligned (16)));\n2\n3\nvoid\nAmplify( f16 *a, float mult , int\ncount )\n4\n{\n5\ncount &=\n-4;\n6\nfor( int i = 0; i < count; ++i ) {\n7\na[i] *= mult;\n8\n}\n9\n}\nListing 9.7: Ampliﬁcation function with alignment hints\nIf you can add some simple guarantees, such as by us-\ning aligned pointers, and by giving the compiler some guar-\nantees about the number of elements, then you can cut the\nsize of the emitted assembly, which on a per case basis won’t\nhelp, but over a large codebase, it will increase the eﬀective-\nness of your instruction cache as the number of instructions\nto be decoded is slashed. Listing 9.7 isn’t faster in isolated\ntest beds, but the size of the ﬁnal executable will be smaller,\nas the generated code is less than half the size.\nThis is\na problem with micro-benchmarks, they can’t always show\nhow systems work together or ﬁght against each other. In\nreal-world tests, ﬁxing up the alignment of pointers can im-\nprove performance dramatically. In small test beds, memory\nthroughput is normally the only bottleneck.\n1\ntypedef\nfloat\nf16\n__attribute__ (( aligned (16)));\n2\n3\nvoid\nAmplify( f16 *a, float mult , int\ncount )\n\n\n176\nCHAPTER 9. HELPING THE COMPILER\n4\n{\n5\ncount &=\n-4;\n6\nfor( int i = 0; i < count; ++i ) {\n7\nif( a[i] < 0 )\n8\nbreak;\n9\na[i] *= mult;\n10\n}\n11\n}\nListing 9.8: Breaking out, breaks vectorisation\nA thing to watch out for is making sure the loops are triv-\nial and always run their course. If a loop has to break based\non data, then it won’t be able to commit to doing all elements\nof the processing, and that means it has to do each element\nat a time. In listing 9.8 the introduction of a break based on\nthe data turns the function from a fast parallel SIMD opera-\ntion auto-vectorisable loop, into a single stepping loop. Note\nthat branching in and of itself does not cause a breakdown in\nvectorisation, but the fact the loop is exited based on data.\nFor example, in listing 9.9, the branch can be turned into\nother operations. It’s also the case that calling out to a func-\ntion can often break the vectorisation, as side eﬀects cannot\nnormally be guaranteed. If the function is a constexpr, then\nthere’s a much better chance it can be consumed into the\nbody of the loop, and won’t break vectorisation. On some\ncompilers, there are certain mathematical functions which\nare available in a vectorised form, such as min, abs, sqrt,\ntan, pow, etc. Find out what your compiler can vectorise. It\ncan often help to write your series of operations out longhand\nto some extent, as trying to shorten the C++ code, can lead\nto slight ambiguities with what the compiler is allowed to do.\nOne thing to watch out for in particular is making sure you\nalways write out. If you only write part of the output stream,\nthen it won’t be able to write out whole SIMD data types, so\nwrite out to your output variable, even if it means reading it\nin, just to write it out again.\n1\ntypedef\nfloat\nf16\n__attribute__ (( aligned (16)));\n2\n3\nvoid\nAmplify( f16 *a, float mult , int\ncount )\n4\n{\n5\ncount &=\n-4;\n6\nfor( int i = 0; i < count; ++i ) {\n7\nf16 val = a[i] * mult;\n8\nif( val > 0 )\n9\na[i] = val;\n10\nelse\n11\na[i] = 0;\n\n\n9.11. AUTO VECTORISATION\n177\n12\n}\n13\n}\nListing 9.9: Vectorising an if\nAliasing can also aﬀect auto vectorisation, as when point-\ners can overlap, there could be dependencies between diﬀer-\nent members of the same SIMD register. Consider the listing\n9.10, where the ﬁrst version of the function increments each\nmember by its direct neighbour. This function is pointless\nbut serves us as an example. The function will create a pair-\nwise sum all the way to the end ﬂoat by ﬂoat. As such, it\ncannot be trivially vectorised. The second function, though\nequally pointless, makes large enough steps that auto vec-\ntorisation can ﬁnd a way to calculate multiple values per\nstep.\n1\nvoid\nCombineNext ( float *a, int\ncount )\n2\n{\n3\nfor( int i = 0; i < count - 1; ++i ) {\n4\na[i] += a[i+1]\n5\n}\n6\n}\n7\n8\nvoid\nCombineFours ( float *a, int\ncount )\n9\n{\n10\nfor( int i = 0; i < count - 4; ++i ) {\n11\na[i] += a[i+4]\n12\n}\n13\n}\nListing 9.10: Aliasing aﬀecting vectorisation\nDiﬀerent compilers will manage diﬀerent amounts of vec-\ntorisation based on the way you write your code, but in gen-\neral, the simpler you write your code, the more likely the\ncompiler will be able to optimise your source.\nOver the next decade, compilers will get better and better.\nClang already attempts to unroll loops far more than GCC\ndoes, and many new ways to detect and optimise simple code\nwill likely appear. At the time of writing, the online Compiler\nExplorer provided by Matt Godbolt2, provides a good way to\nsee how your code will be compiled into assembly, so you\ncan see what can and will be vectorised, optimised out, re-\narranged, or otherwise mutated into the machine-readable\n2https://godbolt.org/\n\n\n178\nCHAPTER 9. HELPING THE COMPILER\nform. Remember that the number of assembly instructions\nis not a good metric for fast code, that SIMD operations are\nnot inherently faster in all cases, and measuring the code\nrunning cannot be replaced by stroking your chin3 while\nthinking about whether the instructions look cool, and you\nshould be okay.\n3or even stroking a beard, or biting a pencil (while making a really serious\nface), as one reviewer pleaded\n",
      "page_number": 166,
      "chapter_number": 9,
      "summary": "In this section, we will look at some of the things we\ndo that we should try not to, and we look at how to introduce\nsome habits that will make it easier for the compiler to do\nwhat we mean, not what we say Key topics include memory, data, and cache.",
      "keywords": [
        "data",
        "cache",
        "compiler",
        "Cache line",
        "code",
        "int",
        "Helping the compiler",
        "memory",
        "int count",
        "COUNT",
        "Listing",
        "write",
        "n’t",
        "sum",
        "line"
      ],
      "concepts": [
        "memory",
        "data",
        "cache",
        "caching",
        "code",
        "compiler",
        "lists",
        "sum",
        "sums",
        "ways"
      ],
      "similar_chapters": [
        {
          "book": "Python Architecture Patterns",
          "chapter": 14,
          "title": "[ 465 ]",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 30,
          "title": "Segment 30 (pages 601-619)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 34,
          "title": "Segment 34 (pages 683-702)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 10,
      "title": "Maintenance and",
      "start_page": 182,
      "end_page": 191,
      "detection_method": "regex_chapter",
      "content": "Chapter 10\nMaintenance and\nreuse\nWhen object-oriented design was ﬁrst promoted, it was said\nto be easier to modify and extend existing code bases than\nthe more traditional procedural approach. Though it is not\ntrue in practice, it is often cited by object-oriented developers\nwhen reading about other programming paradigms. Regard-\nless of their level of expertise, an object-oriented program-\nmer will very likely cite the extensible, encapsulating nature\nof object-oriented development as a boon when it comes to\nworking on larger projects.\nHighly experienced but more objective developers have\nadmitted or even written about how object-oriented C++ is\nnot highly suited to big projects with lots of dependencies,\nbut can be used as long as you follow strict guidelines such\nas those found in the Large-scale C++ book[?].\nFor those\nwho cannot immediately see the beneﬁt of the data-oriented\ndevelopment paradigm with respect to maintenance and evo-\nlutionary development, this chapter covers why it is easier\nthan working with objects.\n179\n\n\n180\nCHAPTER 10. MAINTENANCE AND REUSE\n10.1\nCosmic hierarchies\nWhatever you call them, be it Cosmic Base Class, Root of all\nEvil, Gotcha #97, or CObject, having a base class that every-\nthing derives from has pretty much been a universal failure\npoint in large C++ projects. The language does not naturally\nsupport introspection or duck typing, so it has diﬃculty util-\nising CObjects eﬀectively. If we have a database driven ap-\nproach, the idea of a cosmic base class might make a subtle\nentrance right at the beginning by appearing as the entity to\nwhich all other components are adjectives about, thus not\nletting anything be anything other than an entity. Although\ncomponent–based engines can often be found sporting an\nEntityID as their owner, not all require owners. Not all have\nonly one owner. When you normalise databases, you ﬁnd\nyou have a collection of diﬀerent entity types. In our level\nﬁle example, we saw how the objects we started with turned\ninto a MeshID, TextureID, RoomID, and a PickupID. We even\nsaw the emergence through necessity of a DoorID. If we pile\nall these Ids into a central EntityID, the system should work\nﬁne, but it’s not a necessary step. A lot of entity systems do\ntake this approach, but as is the case with most movements,\nthe ﬁrst swing away from danger often swings too far. The\nbalance is to be found in practical examples of data normal-\nisation provided by the database industry.\n10.2\nDebugging\nThe prime causes of bugs are the unexpected side eﬀects of a\ntransform or an unexpected corner case where a conditional\ndidn’t return the correct value. In object-oriented program-\nming, this can manifest in many ways, from an exception\ncaused by de-referencing a null, to ignoring the interactions\nof the player because the game logic hadn’t noticed it was\nmeant to be interactive.\n\n\n10.2. DEBUGGING\n181\nHolding the state of a system in your head, and playing\ncomputer to ﬁgure out what is going on, is where we get the\nidea that programmers absolutely need to be in the zone\nto get any real work done. The reality is probably far less\nthrilling. The reality is closer to the fear that programmers\nonly need to be in the zone if the code is nearing deadly levels\nof complexity.\n10.2.1\nLifetimes\nOne of the most common causes of the null dereference is\nwhen an object’s lifetime is handled by a separate object\nto the one manipulating it.\nFor example, if you are play-\ning a game where the badguys can die, you have to be care-\nful to update all the objects that are using them whenever\nthe badguy gets deleted, otherwise, you can end up derefer-\nencing invalid memory which can lead to dereferencing null\npointers because the class has destructed.\nData-oriented\ndevelopment tends towards this being impossible as the ex-\nistence of an entity in an array implies its processability, and\nif you leave part of an entity around in a table, you haven’t\ndeleted the entity fully. This is a diﬀerent kind of bug, but\nit’s not a crash bug, and it’s easier to ﬁnd and kill as it’s just\nmaking sure that when an entity is destroyed, all the tables\nit can be part of also destroy their elements too.\n10.2.2\nAvoiding pointers\nWhen looking for data-oriented solutions to programming\nproblems, we often ﬁnd pointers aren’t required, and often\nmake the solution harder to scale. Using pointers where null\nvalues are possible implies each pointer doesn’t only have\nthe value of the object being pointed at, but also implies a\nboolean value for whether or not the instance exists. Remov-\ning this unnecessary extra feature can remove bugs, save\ntime, and reduce complexity.\n\n\n182\nCHAPTER 10. MAINTENANCE AND REUSE\n10.2.3\nBad State\nBugs have a lot to do with not being in the right state. De-\nbugging, therefore, becomes a case of ﬁnding out how the\ngame got into its current, broken state.\n1\nbool\nSingleReturn ( int\nnumDucks ) {\n2\nbool\nvalid = true;\n3\n//\nmust\nbe 10 or\nfewer\nducks.\n4\nif( numDucks\n> 10 ) valid = false;\n5\n//\nnumber\nof\nducks\nshould\nbe\neven.\n6\nvalid = ( numDucks & 1 ) == 0;\n7\n// can ’t have\nnegative\nducks.\n8\nif( numDucks\n< 0 ) valid = false;\n9\nreturn\nvalid;\n10\n}\n11\nbool\nRecursiveCheck ( Node * node\n) {\n12\nbool\nvalid = true;\n13\nif( node ) {\n14\nvalid = node ->Valid ();\n15\nvalid &=\nRecursiveCheck ( node ->sibling );\n16\nvalid &=\nRecursiveCheck ( node ->child );\n17\n}\n18\nreturn\nvalid;\n19\n}\nListing 10.1: Modifying state can shadow history\nWhenever you assign a value to a variable, you are de-\nstroying history. Take the example in listing 10.1. The ideal\nof having only one return statement in a function can cause\nthis kind of error with greater frequency than expected. Hav-\ning more than one return point has its own problems. What’s\nimportant is once you have got to the end of the function, it’s\nhard to ﬁgure out what it was that caused it to fail validation.\nYou can’t even breakpoint the bail points. The recursive ex-\nample is even more dangerous, as there’s a whole tree of ob-\njects and it will recurse through all of them before returning,\nregardless of value, and again, is impossible to breakpoint.\nWhen you encapsulate your state, you hide internal\nchanges.\nThis quickly leads to adding lots of debugging\nlogs. Instead of hiding, data-oriented suggests keeping data\nin simple forms. Potentially, leaving it around longer than\nrequired can lead to highly simpliﬁed transform inspection.\nIf you have a transform that appears to work, but for one\nodd case it doesn’t, the simplicity of adding an assert and\nnot deleting the input data can reduce the amount of guess-\nwork and toil required to generate the reproduction required\n\n\n10.3. REUSABILITY\n183\nto understand the bug and make a clean ﬁx.\nIf you keep\nmost of your transforms as one-way, that is to say, they take\nfrom one source, and produce or update another, then even\nif you run the code multiple times it will still produce the\nsame results as it would have the ﬁrst time. The transform\nis idempotent. This useful property allows you to ﬁnd a bug\nsymptom, then rewind and trace through the causes without\nhaving to attempt to rebuild the initial state.\nOne way of keeping your code idempotent is to write your\ntransforms in a single assignment style. If you operate with\nmultiple transforms but all leading to predicated join points,\nyou can guarantee yourself some timings, and you can look\nback at what caused the ﬁnal state to turn out like it did\nwithout even rewinding. If your conditions are condition ta-\nbles, just leave the inputs around until validity checks have\nbeen completed then you have the ability to go into any live\nsystem and check how it arrived at that state. This alone\nshould reduce any investigation time to a minimum.\n10.3\nReusability\nA feature commonly cited by the object-oriented developers\nwhich seems to be missing from data-oriented development\nis reusability. The idea that you won’t be able to take already\nwritten libraries of code and use them again, or on multiple\nprojects, because the design is partially within the imple-\nmentation. To be sure, once you start optimising your code\nto the particular features of a software project, you do end up\nwith code which cannot be reused. While developing data-\noriented projects, the assumed inability to reuse source code\nwould be signiﬁcant, but it is also highly unlikely. The truth\nis found when considering the true meaning of reusability.\nReusability is not fundamentally concerned with reusing\nsource ﬁles or libraries. Reusability is the ability to main-\ntain an investment in information, or the invention of more\n\n\n184\nCHAPTER 10. MAINTENANCE AND REUSE\nvocabulary with which to communicate intention, such as\nwith the STL, or with other libraries of structural code. In\nthe primary example of reuse as sequences of actions, this\nis a wealth of knowledge for the entity that owns the develop-\nment IP and is very nearly what patents are built on. In the\nlatter, the vocabulary is often stumbled upon, rather than\ntruly invented.\nCopyright law has made it hard to see what resources\nhave value in reuse, as it maintains the source as the ob-\nject of its discussion rather than the intellectual property\nrepresented by the source. The reason for this is that ideas\ncannot be copyrighted, so by maintaining this stance, the\ncopyrighter keeps hold of this tenuous link to a right to with-\nhold information. Reusability comes from being aware of the\ninformation contained within the medium it is stored. In our\ncase, it is normally stored as source code, but the informa-\ntion is not the source code. With object-oriented develop-\nment, the source can be adapted (adapter pattern) to any\nproject we wish to venture. However, the source is not the\ninformation. The information is the order and existence of\ntasks that can and will be performed on the data.\nView-\ning the information this way leads to an understanding that\nany reusability a programming technique can provide comes\ndown to its mutability of inputs and outputs. Its willingness\nto adapt a set of temporally coupled tasks into a new us-\nage framework is how you can ﬁnd out how well it functions\nreusably.\nIn object-oriented development, you apply the informa-\ntion inherent in the code by adapting a class that does the\njob, or wrapper it, or use an agent. In data-oriented devel-\nopment, you copy the functions and schema and transform\ninto and out of the input and output data structures around\nthe time you apply the information contained in the data-\noriented transform.\nEven though, at ﬁrst sight, data-oriented code doesn’t ap-\npear as reusable on the outside, the fact is, it maintains the\n\n\n10.3. REUSABILITY\n185\nsame amount of information in a simpler form, so it’s more\nreusable as it doesn’t carry the baggage of related data or\nfunctions like object-oriented programming, and doesn’t re-\nquire complex transforms to generate the input and extract\nfrom the output like procedural programming tends to gen-\nerate due to the normalising.\nDuck typing, not normally available in object-oriented\nprogramming due to a stricter set of rules on how to in-\nterface between data, can be implemented with templates\nto great eﬀect, turning code which might not be obviously\nreusable into a simple strategy, or a sequence of transforms\nwhich can be applied to data or structures of any type, as\nlong as they maintain a naming convention.\nThe object-oriented C++ idea of reusability is a mix-\nture of information and architecture.\nDeveloping from a\ndata-oriented transform centric viewpoint, architecture just\nseems like a lot of ﬂuﬀcode.\nThe only good architecture\nthat’s worth saving is the actualisation of data-ﬂow and\ntransform.\nThere are situations where an object-oriented\nmodule can be used again, but they are few and far between\nbecause of the inherent diﬃculty interfacing object-oriented\nprojects with each other.\nThe most reusable object-oriented code appears as in-\nterfaces to agents into a much more complex system. The\nbest example of an object-oriented approach that made ev-\nerything easier to handle, that was highly reusable, and was\nfully encapsulated was the FILE type from stdio.h which is\nused as an agent into whatever the platform and OS would\nneed to open, access, write, and read to and from a ﬁle on\nthe system.\n\n\n186\nCHAPTER 10. MAINTENANCE AND REUSE\n10.4\nReusable functions\nApart from the freedom of extension when it comes to keep-\ning all your data in simple linear arrangements, there is also\nan implicit tendency to turn out accidentally reusable solu-\ntions to problems. This is caused by the data being formatted\nmuch more rigidly, and therefore when it ﬁts, can almost be\nseen as a type of duck-typing. If the data can ﬁt a trans-\nform, a transform should be able to act on it. Some would\nargue, just because the types match, it doesn’t mean the\nfunction will create the expected outcome, but in addition\nto this being avoidable by not reusing code you don’t under-\nstand, in some cases, all you need is to know the signature\nto understand the transform. As an extreme example, it’s\npossible to understand a fair number of Haskell functions\npurely based on their arguments. Finally, because the code\nbecomes much more penetrable, it takes less time to look at\nwhat a transform is doing before committing to reusing it in\nyour own code.\nBecause the data is built in the same way each time,\nhandled with transforms and always being held in the same\ntypes of container, there is a very good chance there are\nmultiple design agnostic optimisations which can be ap-\nplied to many parts of the code. General purpose sorting,\ncounting, searches and spatial awareness systems can be\nattached to new data without calling for OOP adapters or\nimplementing interfaces so Strategies can run over them.\nThis is why it’s possible to have generalised query optimisa-\ntions in databases, and if you start to develop your code this\nway, you can carry your optimisations with you across more\nprojects.\n\n\n10.5. UNIT TESTING\n187\n10.5\nUnit testing\nUnit testing can be very helpful when developing games, but\nbecause of the object-oriented paradigm making program-\nmers think about code as representations of objects, and\nnot as data transforms, it’s hard to see what can be tested.\nLinking together unrelated concepts into the same object\nand requiring complex setup state before a test can be car-\nried out, has given unit testing a stunted start in games as\nobject-oriented programming caused simple tests to be hard\nto write. Making tests is further complicated by the addition\nof the non-obvious nature of how objects are transformed\nwhen they represent entities in a game world. It can be very\nhard to write unit tests unless you’ve been working with them\nfor a while, and the main point of unit tests is that someone\nwho doesn’t fully grok the system can make changes without\nfalling foul of making things worse.\nUnit testing is mostly useful during refactorings, taking a\ngame or engine from one code and data layout into another\none, ready for future changes. Usually, this is done because\nthe data is in the wrong shape, which in itself is harder to\ndo if you normalise your data as you’re more likely to have\nleft the data in an unconﬁgured form. There will obviously\nbe times when even normalised data is not suﬃcient, such\nas when the design of the game changes suﬃcient to ren-\nder the original data-analysis incorrect, or at the very least,\nineﬀective or ineﬃcient.\nUnit testing is simple with data-oriented technique be-\ncause you are already concentrating on the transform. Gen-\nerating tables of test data would be part of your development,\nso leaving some in as unit tests would be simple, if not part of\nthe process of developing the game. Using unit tests to help\nguide the code could be considered to be partial following\nthe test-driven development technique, a proven good way to\ngenerate eﬃcient and clear code.\nRemember, when you’re doing data-oriented development\n\n\n188\nCHAPTER 10. MAINTENANCE AND REUSE\nyour game is entirely driven by stateful data and stateless\ntransforms. It is very simple to produce unit tests for your\ntransforms. You don’t even need a framework, just an input\nand output table and then a comparison function to check\nthe transform produced the right data.\n10.6\nRefactoring\nDuring refactoring, it’s always important to know you’ve not\nbroken anything by changing the code.\nAllowing for such\nsimple unit testing gets you halfway there. Another advan-\ntage of data-oriented development is that, at every turn, it\npeels away the unnecessary elements. You might ﬁnd refac-\ntoring is more a case of switching out the order of transforms\nthan changing how things are represented. Refactoring nor-\nmally involves some new data representation, but as long\nas you build your structures with normalisation in mind,\nthere’s going to be little need of that.\nWhen it is needed,\ntools for converting from one schema to another could be\nwritten once and used many times.\nIt might come to pass, as you work with normalised data,\nthat you realise the reason you were refactoring so much in\nthe ﬁrst place, was that you had embedded meaning in the\ncode by putting the data in objects with names, and methods\nthat did things to the objects, rather than transformed the\ndata.\n",
      "page_number": 182,
      "chapter_number": 10,
      "summary": "For those\nwho cannot immediately see the beneﬁt of the data-oriented\ndevelopment paradigm with respect to maintenance and evo-\nlutionary development, this chapter covers why it is easier\nthan working with objects Key topics include data, oriented, and reusable.",
      "keywords": [
        "data",
        "code",
        "object-oriented",
        "transform",
        "n’t",
        "Maintenance and reuse",
        "UNIT TESTING",
        "valid",
        "UNIT",
        "reuse",
        "REUSABILITY",
        "data-oriented",
        "state",
        "Maintenance",
        "development"
      ],
      "concepts": [
        "data",
        "oriented",
        "reusable",
        "developers",
        "state",
        "transform",
        "programming",
        "program",
        "valid",
        "unit"
      ],
      "similar_chapters": [
        {
          "book": "Software Architecture",
          "chapter": 10,
          "title": "Segment 10 (pages 80-87)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 53,
          "title": "Segment 53 (pages 484-491)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 44-51)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 52,
          "title": "Segment 52 (pages 518-525)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 6,
          "title": "Segment 6 (pages 45-52)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 11,
      "title": "What’s wrong?",
      "start_page": 192,
      "end_page": 217,
      "detection_method": "regex_chapter",
      "content": "Chapter 11\nWhat’s wrong?\nWhat’s wrong with object-oriented design?\nWhere’s the\nharm in it?\nOver the years, game developers have fallen into a style\nof C++ that is so unappealing to hardware that the managed\nlanguages don’t seem all that much slower in comparison.\nThe pattern of usage of C++ in game development was so ap-\npallingly mismatched to the hardware of the PlayStation 3\nand Xbox 360 generation, it is no wonder an interpreted lan-\nguage is only in the region of 50% slower under normal use\nand sometimes faster1 in their specialist areas. What is this\nstrange language that has embedded itself into the minds of\nC++ game developers? What is it that makes the fashionable\nway of coding games one of the worst ways of making use\nof the machines we’re targeting? Where, in essence, is the\nharm in game-development style object-oriented C++?\nSome of this comes from the initial interpretation of what\nobject-oriented means, as game developers tended to believe\nthat object-oriented meant you had to map instances of ev-\n1http://keithlea.com/javabench/ tells the tale of the server JVM being\nfaster than C++. There are some arguments against the results, but there\nare others backing it up. Read, make up your own mind.\n189\n\n\n190\nCHAPTER 11. WHAT’S WRONG?\nerything you cared about into the code as instances of ob-\njects.\nThis form of object-oriented development could be\ninterpreted as instance-oriented development, and it puts\nthe singular unique entity ahead of the program as a whole.\nWhen put this way, it is easier to see some of the problems\nthat can arise. Performance of an individual is very hard to\ndecry as poor, as object methods are hard to time accurately,\nand unlikely to be timed at all. When your development prac-\ntices promote individual elements above the program as a\nwhole, you will also pay the mental capacity penalty, as you\nhave to consider all operations from the point of view of the\nactors, with their hidden state, not from a point of view of\nvalue semantics.\nAnother issue is it appears that performance has not been\nignored by the language designers, but potentially instead it\nhas been tested for quality in isolation. This could be be-\ncause the real world uses of C++ are quite diﬀerent from the\nexpectation of the library providers, or it could be the library\nproviders are working to internal metrics instead of making\nsure they understand their customer. It’s the opinion of the\nauthor, when developing a library, or a set of templates for\nuse in C++, it shouldn’t just be possible to tune performance\nout of the code you are using, it should come as default. If\nyou make it possible to tune performance, you trade features\nfor understanding and performance. This is a poor trade for\ngame developers, but has been accepted, as the beneﬁt of a\ncommon language is a very tempting oﬀer.\n11.1\nThe harm\nClaim: Virtuals don’t cost much, but if you call them a lot it\ncan add up.\naka - death by a thousand paper cuts\nThe overhead of a virtual call is negligible under simple\ninspection. Compared to what you do inside a virtual call,\n\n\n11.1. THE HARM\n191\nthe extra dereference required seems petty and very likely\nnot to have any noticeable side eﬀects other than the cost\nof a dereference and the extra space taken up by the virtual\ntable pointer. The extra dereference before getting the pointer\nto the function we want to call on this particular instance\nseems to be a trivial addition, but let’s have a closer look at\nwhat is going on.\nA class that has derived from a base class with virtual\nmethods has a certain structure to it. Adding any virtual\nmethods to a class instantly adds a virtual table to the exe-\ncutable, and a virtual table pointer as the implicit ﬁrst data\nmember of the class. There is very little way around this. It’s\nallowed in the language speciﬁcation for the data layout of\nclasses to be up to the compiler to the point where they can\nimplement such things as virtual methods by adding hidden\nmembers and generating new arrays of function pointers be-\nhind the scenes. It is possible to do this diﬀerently, but it\nappears most compilers implement virtual tables to store vir-\ntual method function pointers. It’s important to remember\nvirtual calls are not an operating system level concept, and\nthey don’t exist as far as the CPU is concerned, they are just\nan implementation detail of C++.\nWhen we call a virtual method on a class we have to know\nwhat code to run. Normally we need to know which entry in\nthe virtual table to access, and to do that we read the ﬁrst\ndata member in order to access the right virtual table for\ncalling. This requires loading from the address of the class\ninto a register and adding an oﬀset to the loaded value. Every\nnon-trivial virtual method call is a lookup into a table, so in\nthe compiled code, all virtual calls are really function pointer\narray dereferences, which is where the oﬀset comes in. It’s\nthe oﬀset into the array of function pointers. Once the ad-\ndress of the real function pointer is generated, only then can\ninstruction decoding begin. There are ways to not call into\nthe virtual table, notably with C++11, there has been some\nprogress with the final keyword that can help as classes\nthat cannot be overridden can now know that if they call\n\n\n192\nCHAPTER 11. WHAT’S WRONG?\ninto themselves, then they can call functions directly. This\ndoesn’t help for polymorphic calls, or call sites that access\nthe methods from the interface without knowing the concrete\ntype (see listing 11.1), but it can occasionally help with some\nidioms such as private implementation (pImpl), and the cu-\nriously recurring template pattern.\n1\n#include\n<stdio.h>\n2\n3\nclass B {\n4\npublic:\n5\nB() {}\n6\nvirtual ~B() {}\n7\nvirtual\nvoid\nCall () { printf( \"Base\\n\" ); }\n8\nvoid\nLocalCall () {\n9\nCall ();\n10\n}\n11\n};\n12\n13\nclass D final : public B {\n14\npublic:\n15\nD() {}\n16\n~D() {}\n17\nvirtual\nvoid\nCall () { printf( \" Derived \\n\" ); }\n18\nvoid\nLocalCall () {\n19\nCall ();\n20\n}\n21\n};\n22\n23\nB *pb;\n24\nD *pd;\n25\n26\nint\nmain () {\n27\nD *d = new D;\n28\npb = pd = d;\n29\n30\npb ->LocalCall ();\n31\n//\nprints \" Derived \" via\nvirtual\ncall\n32\npd ->LocalCall ();\n33\n//\nprints \" Derived \" via\ndirect\ncall\n34\n}\nListing 11.1: A simple derived class\nFor multiple inheritance it is slightly more convoluted,\nbut basically, it’s still virtual tables, but now each function\nwill deﬁne which class of vtable it will be referencing.\nSo let’s count up the actual operations involved in this\nmethod call: ﬁrst we have a load, then an add, then another\nload, then a branch. To almost all programmers this doesn’t\nseem like a heavy cost to pay for runtime polymorphism.\nFour operations per call so you can throw all your game en-\ntities into one array and loop through them updating, ren-\ndering, gathering collision state, spawning oﬀsound eﬀects.\nThis seems to be a good trade-oﬀ, but it was only a good\ntrade-oﬀwhen these particular instructions were cheap.\n\n\n11.1. THE HARM\n193\nTwo out of the four instructions are loads, which don’t\nseem like they should cost much, but unless you hit a nearby\ncache, a load takes a long time and instructions take time to\ndecode. The add is very cheap2, to modify the register value\nto address the correct function pointer, but the branch is not\nalways cheap as it doesn’t know where it’s going until the sec-\nond load completes. This could cause an instruction cache\nmiss. All in all, it’s common to see a chunk of time wasted\ndue to a single virtual call in any signiﬁcantly large scale\ngame. In that chunk of time, the ﬂoating point unit alone\ncould have ﬁnished na¨ıvely calculating lots of dot products,\nor a decent pile of square roots. In the best case, the virtual\ntable pointer will already be in memory, the object type the\nsame as last time, so the function pointer address will be the\nsame, and therefore the function pointer will be in cache too,\nand in that circumstance it’s likely the branch won’t stall as\nthe instructions are probably still in the cache too. But this\nbest case is not always the common case for all types of data.\nConsider the alternative, where your function ends, and\nyou are returning some value, then calling into another func-\ntion. The order of instructions is fairly well known, and to\nthe CPU looks very similar to a straight line. There are no\ndeviations from getting instructions based on just following\nthe program counter along each function in turn. It’s possi-\nble to guess quite far ahead the address of any new functions\nthat will be called, as none of them are dependent on data.\nEven with lots of function calls, the fact they are deducible at\ncompile time makes them easy to prefetch, and pretranslate.\nThe implementation of C++ doesn’t like how we iterate\nover objects. The standard way of iterating over a set of het-\nerogeneous objects is to literally do that, grab an iterator and\ncall the virtual function on each object in turn. In normal\ngame code, this will involve loading the virtual table pointer\nfor each and every object. This causes a wait while loading\nthe cache line, and cannot easily be avoided. Once the vir-\ntual table pointer is loaded, it can be used, with the constant\n2Adding to a register before accessing memory is free on most platforms\n\n\n194\nCHAPTER 11. WHAT’S WRONG?\noﬀset (the index of the virtual method), to ﬁnd the function\npointer to call, however, due to the size of virtual functions\ncommonly found in game development, the table won’t be in\nthe cache. Naturally, this will cause another wait for load,\nand once this load has ﬁnished, we can only hope the object\nis actually the same type as the previous element, otherwise,\nwe will have to wait some more for the instructions to load.\nEven without loads, not knowing which function will be\ncalled until the data is loaded means you rely on a cache line\nof information before you can be conﬁdent you are decoding\nthe right instructions.\nThe reason virtual functions in games are large is that\ngame developers have had it drilled into them that virtual\nfunctions are okay, as long as you don’t use them in tight\nloops, which invariably leads to them being used for more\narchitectural considerations such as hierarchies of object\ntypes, or classes of solution helpers in tree-like problem-\nsolving systems (such as pathﬁnding, or behaviour trees).\nLet’s go over this again: many developers now believe the\nbest way to use virtuals is to put large workloads into the\nbody of the virtual methods, so as to mitigate the overhead\nof the virtual call mechanism.\n3 However, doing this, you\ncan virtually guarantee not only will a large portion of the in-\nstruction and data cache be evicted by each call to update(),\nbut most branch predictor slots may become dirty too, and\nfail to oﬀer any beneﬁt when the next update() runs. As-\nsuming virtual calls don’t add up because they are called\non high-level code is ﬁne until they become the general pro-\ngramming style, leading to developers failing to think about\nhow they aﬀect the application, ultimately leading to millions\nof virtual calls per second. All those ineﬃcient calls are go-\ning to add up and impact the hardware, but they hardly ever\nappear on any proﬁles. The issue isn’t that it’s not there, it’s\nthat it’s spread thinly over the whole of the processing of the\n3There are parallels with task systems, where you want to mitigate the\ncost of setup and tear down of tasks.\n\n\n11.1. THE HARM\n195\nmachine. They always appear somewhere in the code being\ncalled.\nCarlos Bueno’s book Mature Optimization Handbook[?],\ntalks about how it’s very easy to miss the real cause of slow-\nness by blindly following the low hanging fruit approach.\nThis is where the idea of creating a hypothesis can prove\nuseful, as when it turns out to not reap the expected re-\nwards, you can retrace and regroup faster. For Facebook,\nthey traced what was causing evictions and optimised those\nfunctions, not for speed, but to remove as much as possible\nthe chance that they evicted other data from the cache.\nIn C++, classes’ virtual tables store function pointers by\ntheir class. The alternative is to have a virtual table for each\nfunction and switch function pointer on the type of the call-\ning class. This works ﬁne in practice and does save some of\nthe overhead as the virtual table would be the same for all\nthe calls in a single iteration of a group of objects. However,\nC++ was designed to allow for runtime linking to other li-\nbraries, libraries with new classes that may inherit from the\nexisting codebase. The design had to allow a runtime linked\nclass to add new virtual methods, and have them callable\nfrom the original running code. If C++ had gone with func-\ntion oriented virtual tables, the language would have had to\nruntime patch the virtual tables whenever a new library was\nlinked, whether at link-time for statically compiled additions,\nor at runtime for dynamically linked libraries. As it is, us-\ning a virtual table per class oﬀers the same functionality but\ndoesn’t require any link-time or runtime modiﬁcation to the\nvirtual tables as the tables are oriented by the classes, which\nby the language design are immutable during link-time.\nCombining the organisation of virtual tables and the or-\nder in which games tend to call methods, even when running\nthrough lists in a highly predictable manner, cache misses\nare commonplace. It’s not just the implementation of classes\nthat causes these cache misses, it’s any time data is the de-\nciding factor in which instructions are run.\nGames com-\n\n\n196\nCHAPTER 11. WHAT’S WRONG?\nmonly implement scripting languages, and these languages\nare often interpreted and run on a virtual machine. However\nthe virtual machine or JIT compiler is implemented, there is\nalways an aspect of data controlling which instructions will\nbe called next, and this causes branch misprediction. This\nis why, in general, interpreted languages are slower, they ei-\nther run code based on loaded data in the case of bytecode\ninterpreters or they compile code just in time, which though\nit creates faster code, causes issues of its own.\nWhen a developer implements an object-oriented frame-\nwork without using the built-in virtual functions, virtual ta-\nbles and this pointers present in the C++ language, it doesn’t\nreduce the chance of cache miss unless they use virtual ta-\nbles by function rather than by class. But even when the\ndeveloper has been especially careful, the very fact they are\ndoing object-oriented programming with game developer ac-\ncess patterns, that of calling singular virtual functions on\narrays of heterogeneous objects, they are still going to have\nsome of the same instruction decode and cache misses as\nfound with built-in virtuals. That is, the best they can hope\nfor is one less data dependent CPU state change per virtual\ncall. That still leaves the opportunity for two mispredictions.\nSo, with all this apparent ineﬃciency, what makes game\ndevelopers stick with object-oriented coding practices? As\ngame developers are frequently cited as a source of how the\nbleeding edge of computer software development is progress-\ning, why have they not moved away wholesale from the prob-\nlem and stopped using object-oriented development prac-\ntices all together?\n11.2\nMapping the problem\nClaim: Objects provide a better mapping from the real world\ndescription of the problem to the ﬁnal code solution.\n\n\n11.2. MAPPING THE PROBLEM\n197\nObject-oriented design when programming in games\nstarts with thinking about the game design in terms of\nentities.\nEach entity in the game design is given a class,\nsuch as ship, player, bullet, or score. Each object maintains\nits own state, communicates with other objects through\nmethods, and provides encapsulation so when the imple-\nmentation of a particular entity changes, the other objects\nthat use it or provide it with utility do not need to change.\nGame developers like abstraction, because historically they\nhave had to write games for not just one target platform, but\nusually at least two.\nIn the past, it was between console\nmanufacturers, but now game developers have to manage\nbetween Windows™and console platforms, plus the mo-\nbile targets too. The abstractions in the past were mostly\nhardware access abstractions, and naturally some gameplay\nabstractions as well, but as the game development indus-\ntry matured, we found common forms of abstractions for\nareas such as physics, AI, and even player control. Finding\nthese common abstractions allowed for third party libraries,\nand many of these use object-oriented design as well. It’s\nquite common for libraries to interact with the game through\nagents. These agent objects contain their own state data,\nwhether hidden or publicly accessible, and provide functions\nby which they can be manipulated inside the constraints of\nthe system that provided them.\nThe game design inspired objects (such as ship, player,\nlevel) keep hold of agents and use them to ﬁnd out what’s\ngoing on in their world. A player interacts with physics, in-\nput, animation, other entities, and doing this through an\nobject-oriented API hides much of the details about what’s\nactually required to do all these diﬀerent tasks.\nThe entities in object-oriented design are containers for\ndata and a place to keep all the functions that manipulate\nthat data.\nDon’t confuse these entities with those of en-\ntity systems, as the entities in object-oriented design are\nimmutable of class over their lifetime.\nAn object-oriented\nentity does not change class during its lifetime in C++ be-\n\n\n198\nCHAPTER 11. WHAT’S WRONG?\ncause there is no process by which to reconstruct a class in\nplace in the language. As can be expected, if you don’t have\nthe right tools for the job, a good workman works around\nit. Game developers don’t change the type of their objects\nat runtime, instead, they create new and destroy old in the\ncase of a game entity that needs this functionality. But as\nis often the case, because the feature is not present in the\nlanguage, it is underutilised even when it would make sense.\nFor example, in a ﬁrst-person shooter, an object will be\ndeclared to represent the animating player mesh, but when\nthe player dies, a clone would be made to represent the dead\nbody as a rag doll. The animating player object may be made\ninvisible and moved to their next spawn point while the dead\nbody object with its diﬀerent set of virtual functions, and\ndiﬀerent data, remains where the player died so as to let the\nplayer watch their dead body. To achieve this sleight of hand,\nwhere the dead body object sits in as a replacement for the\nplayer once they are dead, copy constructors need to be de-\nﬁned. When the player is spawned back into the game, the\nplayer model will be made visible again, and if they wish to,\nthe player can go and visit their dead clone. This works re-\nmarkably well, but it is a trick that would be unnecessary if\nthe player could become a dead rag doll rather than spawn\na clone of a diﬀerent type. There is an inherent danger in\nthis too, the cloning could have bugs, and cause other is-\nsues, and also if the player dies but somehow is allowed to\nresurrect, then they have to ﬁnd a way to convert the rag doll\nback into the animating player, and that is no simple feat.\nAnother example is in AI. The ﬁnite state machines and\nbehaviour trees that run most game AI maintain all the data\nnecessary for all their potential states. If an AI has three\nstates, { Idle, Making-a-stand, Fleeing-in-terror } then it has\nthe data for all three states.\nIf the Making-a-stand has a\nscared-points accumulator for accounting their fear, so they\ncan ﬁght, but only up until they are too scared to continue,\nand the Fleeing-in-terror has a timer so they will ﬂee, but\nonly for a certain time, then Idle will have these two unnec-\n\n\n11.2. MAPPING THE PROBLEM\n199\nessary attributes as well. In this trivial example, the AI class\nhas three data entries, { state, how-scared, ﬂee-time }, and\nonly one of these data entries is used by all three states. If\nthe AI could change type when it transitioned from state to\nstate, then it wouldn’t even need the state member, as that\nfunctionality would be covered by the virtual table pointer.\nThe AI would only allocate space for each of the state track-\ning members when in the appropriate state. The best we can\ndo in C++ is to fake it by changing the virtual table pointer\nby hand, dangerous but possible, or setting up a copy con-\nstructor for each possible transition.\nApart from immutable type, object-oriented development\nalso has a philosophical problem.\nConsider how humans\nperceive objects in real life. There is always a context to every\nobservation. The humble table, when you look at it, you may\nsee it to be a table with four legs, made of wood and modestly\npolished. If so, you will see it as being a brown colour, but\nyou will also see the reﬂection of the light. You will see the\ngrain, but when you think about what colour it is, you will\nthink of it as being one colour.\nHowever, if you have the\ntraining of an artist, you will know what you see is not what is\nactually there. There is no solid colour, and if you are looking\nat the table, you cannot see its precise shape, but only infer\nit. If you are inferring it is brown by the average light colour\nentering your eye, then does it cease to be brown if you turn\noﬀthe light? What about if there is too much light and all\nyou can see is the reﬂection oﬀthe polished surface? If you\nclose one eye and look at its rectangular form from one of the\nlong sides, you will not see right angle corners, but instead,\na trapezium. We automatically adjust for this and classify\nobjects when we see them. We apply our prejudices to them\nand lock them down to make reasoning about them easier.\nThis is why object-oriented development is so appealing to\nus.\nHowever, what we ﬁnd easy to consume as humans,\nis not optimal for a computer. When we think about game\nentities being objects, we think about them as wholes. But\na computer has no concept of objects, and only sees objects\nas being badly organised data and functions randomly called\n\n\n200\nCHAPTER 11. WHAT’S WRONG?\non it.\nIf you take another example from the table, consider the\ntable to have legs about three feet long. That’s someone’s\nstandard table. If the legs are only one foot long, it could be\nconsidered to be a coﬀee table. Short, but still usable as a\nplace to throw the magazines and leave your cups. But when\nyou get down to one inch long legs, it’s no longer a table, but\ninstead, just a large piece of wood with some stubs stuck on\nit. We can happily classify the same item but with diﬀerent\ndimensions into three distinct classes of object. Table, coﬀee\ntable, a lump of wood with some little bits of wood on it. But,\nat what point does the lump of wood become a coﬀee table?\nIs it somewhere between 4 and 8 inch long legs? This is the\nsame problem as presented about sand, when does it transi-\ntion from grains of sand to a pile of sand? How many grains\nare a pile, are a dune? The answer must be that there is no\nanswer. The answer is also helpful in understanding how a\ncomputer thinks. It doesn’t know the speciﬁc diﬀerence be-\ntween our human classiﬁcations because to a certain degree\neven humans don’t.\nThe class of an object is poorly deﬁned by what it is, but\nbetter by what it does. This is why duck typing is a strong\napproach. We also realise, if a type is better deﬁned by what\nit can do, then when we get to the root of what a polymor-\nphic type is, we ﬁnd it is only polymorphic in terms of what\nit can do. In C++, it’s clear a class with virtual functions can\nbe called as a runtime polymorphic instance, but it might\nnot have been clear that if it didn’t have those functions, it\nwould not need to be classiﬁed in the ﬁrst place. The reason\nmultiple inheritance is useful stems from this. Multiple in-\nheritance just means an object can behave, that is react, to\ncertain impulses. It has declared that it can fulﬁl some con-\ntract of polymorphic function response. If polymorphism is\njust the ability for an object to fulﬁl a functionality contract,\nthen we don’t need virtual calls to handle that every time, as\nthere are other ways to make code behave diﬀerently based\non the object.\n\n\n11.2. MAPPING THE PROBLEM\n201\nIn most games engines, the object-oriented approach\nleads to a lot of objects in very deep hierarchies. A common\nancestor chain for an entity might be: PlayerEntity →Char-\nacterEntity →MovingEntity →PhysicalEntity →Entity →\nSerialisable →ReferenceCounted →Base.\nThese deep hierarchies virtually guarantee multiple indi-\nrect calls when calling virtual methods, but they also cause\na lot of pain when it comes to cross-cutting code, that is\ncode that aﬀects or is aﬀected by unrelated concerns, or\nconcerns incongruous to the hierarchy. Consider a normal\ngame with characters moving around a scene. In the scene\nyou will have characters, the world, possibly some particle\neﬀects, lights, some static and some dynamic. In this scene,\nall these things need to be rendered, or used for rendering.\nThe traditional approach is to use multiple inheritance or to\nmake sure there is a Renderable base class somewhere in ev-\nery entity’s inheritance chain. But what about entities that\nmake noises? Do you add an audio emitter class as well?\nWhat about entities that are serialised vs those that are ex-\nplicitly managed by the level?\nWhat about those that are\nso common they need a diﬀerent memory manager (such as\nthe particles), or those that only optionally have to be ren-\ndered (like trash, ﬂowers, or grass in the distance). This has\nbeen solved numerous times by putting all the most com-\nmon functionality into the core base class for everything in\nthe game, with special exceptions for special circumstances,\nsuch as when the level is animated, when a player character\nis in an intro or death screen, or is a boss character (who is\nspecial and deserves a little more code). These hacks are only\nnecessary if you don’t use multiple inheritance, but when\nyou use multiple inheritance you then start to weave a web\nthat could ultimately end up with virtual inheritance and the\ncomplexity of state that brings with it. The compromise al-\nmost always turns out to be some form of cosmic base class\nanti-pattern.\nObject-oriented development is good at providing a hu-\nman oriented representation of the problem in the source\n\n\n202\nCHAPTER 11. WHAT’S WRONG?\ncode, but bad at providing a machine representation of the\nsolution. It is bad at providing a framework for creating an\noptimal solution, so the question remains: why are game\ndevelopers still using object-oriented techniques to develop\ngames? It’s possible it’s not about better design, but instead,\nmaking it easier to change the code.\nIt’s common knowl-\nedge that game developers are constantly changing code to\nmatch the natural evolution of the design of the game, right\nup until launch. Does object-oriented development provide\na good way of making maintenance and modiﬁcation simpler\nor safer?\n11.3\nInternalised state\nClaim: Encapsulation makes code more reusable.\nIt’s eas-\nier to modify the implementation without aﬀecting the usage.\nMaintenance and refactoring become easy, quick, and safe.\nThe idea behind encapsulation is to provide a contract to\nthe person using the code rather than providing a raw imple-\nmentation. In theory, well written object-oriented code that\nuses encapsulation is immune to damage caused by chang-\ning how an object manipulates its data. If all the code us-\ning the object complies with the contract and never directly\nuses any of the data members without going through acces-\nsor functions, then no matter what you change about how\nthe class fulﬁls that contract, there won’t be any new bugs\nintroduced by any change. In theory, the object implemen-\ntation can change in any way as long as the contract is not\nmodiﬁed, but only extended. This is the open closed prin-\nciple. A class should be open for extension, but closed for\nmodiﬁcation.\nA contract is meant to provide some guarantees about\nhow a complex system works. In practice, only unit testing\ncan provide these guarantees.\n\n\n11.3. INTERNALISED STATE\n203\nSometimes, programmers unwittingly rely on hidden fea-\ntures of objects’ implementations. Sometimes the object they\nrely on has a bug that just so happens to ﬁt their use case.\nIf that bug is ﬁxed, then the code using the object no longer\nworks as expected. The use of the contract, though it was\nkept intact, has not helped the other piece of code to main-\ntain working status across revisions.\nInstead, it provided\nfalse hope that the returned values would not change.\nIt\ndoesn’t even have to be a bug. Temporal couplings inside ob-\njects or accidental or undocumented features that go away in\nlater revisions can also damage the code using the contract\nwithout breaking it.\nConsider an implementation that maintained an internal\nlist in sorted order, and a use case that accidentally relied\non it (an unforeseen bug in the user’s use case, not an inten-\ntional dependency), but when the maintainer pushes out a\nperformance enhancing update, the only thing the users are\ngoing to see is a pile of new bugs, and they will likely assume\nthe performance update is suspect, not their own code.\nA concrete example could be an item manager that kept\na list of items sorted by name. If the function returns all the\nitem types that match a ﬁlter, then the caller could iterate\nthe returned list until it found the item it wanted. To speed\nthings up, it could early-out if it found an item with a name\nlater than the item it was looking for, or it could do a binary\nsearch of the returned list. In both those cases, if the inter-\nnal representation changed to something that wasn’t ordered\nby name, then the code would no longer work. If the internal\nrepresentation was changed so it was ordered by hash, then\nthe early-out and binary search would be completely broken.\nIn many linked list implementations, there is a decision\nmade about whether to store the length of the list or not. The\nchoice to store a count member will make multi-threaded ac-\ncess slower, but the choice not to store it will make ﬁnding\nthe length of the list an O(n)operation. For situations where\nyou only want to ﬁnd out whether the list is empty, if the ob-\n\n\n204\nCHAPTER 11. WHAT’S WRONG?\nject contract only supplies a get count() function, you can-\nnot know for sure whether it would be cheaper to check if\nthe count was greater than zero, or check if the begin() and\nend() are the same. This is another example of the contract\nbeing too little information.\nEncapsulation only seems to provide a way to hide bugs\nand cause assumptions in programmers.\nThere is an old\nsaying about assumptions, and encapsulation doesn’t let you\nconﬁrm or deny them unless you have access to the source\ncode.\nIf you have, and you need to look at it to ﬁnd out\nwhat went wrong, then all the encapsulation has done is\nadd another layer to work around rather than add any useful\nfunctionality of its own.\n11.4\nInstance oriented development\nClaim: Making every object an instance makes it very easy\nto think about what an object’s responsibilities are, what its\nlifetime looks like, and where it belongs in the world of objects.\nThe ﬁrst problem with instance thinking is that every-\nthing is centred around the idea of one item doing a thing,\nand that is a sure way to lead to poor performance.\nThe second, and more pervasive issue with instance\nthinking is it leads to thinking in the abstract about in-\nstances, and using full objects as building blocks for thought\ncan lead to very ineﬃcient algorithms. When you hide the\ninternal representation of an item even from the programmer\nusing it, you often introduce issues of translation from one\nway of thinking about an object to another, and back again.\nSometimes you may have an item that needs to change an-\nother object, but cannot reach it in the world it ﬁnds itself, so\nhas to send a message to its container to help it achieve the\ngoal of answering a question about another entity. Unfortu-\nnately, it’s not uncommon for programs to lose sight of the\n\n\n11.4. INSTANCE ORIENTED DEVELOPMENT\n205\ndata requirement along these routes, and send more than\nnecessary in the query, or in the response, carrying around\nnot only unnecessary permissions, but also unnecessary\nlimitations due to related system state.\nAs an example of how things can go wrong, imagine a\ncity building game where the population has happiness rat-\nings. If each individual citizen has a happiness rating, then\nthey will need to calculate that happiness rating. Let’s as-\nsume the number of citizens isn’t grossly overwhelming, with\nmaybe a maximum of a thousand buildings and up to ten\ncitizens per building. If we only calculate the happiness of\nthe citizens when necessary, it will speed things up, and\nin at least one game where these numbers are similar, lazy\nevaluation of the citizen happiness was the way things were\ndone. How the happiness is calculated can be an issue if it\nis worked out from the perspective of the individual, rather\nthan the perspective of the city. If a citizen is happy when\nthey are close to work, close to local amenities, far from in-\ndustrial locations, and able to get to recreational areas eas-\nily, then a lot of the happiness rating comes from a kind of\npathﬁnding. If the result of pathﬁnding is cached, then at\nleast the citizens in the same building can beneﬁt, but ev-\nery building will have small diﬀerences in distances to each\nof the diﬀerent types of building. Running pathﬁnding over\nthat many instances is very expensive.\nIf instead, the city calculates happiness, it can build a\nmap of distances from each of the types of building under\nconsideration as a ﬂood ﬁll pass and create a general dis-\ntance map of the whole city using a Floyd-Warshall algorithm\nto help citizens decide on how close their places of work are.\nNormally, substituting an O(n3)algorithm for an O(n2)could\nbe seen as silly, but the pathﬁnding is being done for each cit-\nizen, so becomes O(n2m) and is not in fact algorithmically su-\nperior. Finally, this is the real world, and doing the pathing\nitself has other overheads, and running the Floyd-Warshall\nalgorithm to generate a lookup before calculating happiness\nmeans the work to calculate happiness can be simpler (in\n\n\n206\nCHAPTER 11. WHAT’S WRONG?\ndata storage terms), and require fewer branches oﬀinto sup-\nporting code. The Floyd-Warshall algorithm can also have a\npartial update run upon it, using the existing map to indi-\ncate which items need to be updated. If running from the\ninstance point of view, knowing a change to the topology or\nthe type of buildings nearby would require doing some form\nof distance check per instance.\nIn conclusion, abstractions form the basis of solving diﬃ-\ncult problems, but in games, we’re often not solving diﬃcult\nalgorithmic problems at a gameplay level. To the contrary, we\nhave a tendency to abstract too early, and object-oriented de-\nsign often gives us an easy and recognisable way to commit\nto abstractions without rendering the costs apparent until\nmuch later, when we have become too dependent upon them\nto clear them away without impacting other code.\n11.5\nHierarchical design vs change\nClaim: Inheritance allows reuse of code by extension. Adding\nnew features is simple.\nInheritance was seen as a major reason to use classes in\nC++ by game programmers. The obvious beneﬁt was being\nable to inherit from multiple interfaces to gain attributes or\nagency in system objects such as physics, animation, and\nrendering. In the early days of C++ adoption, the hierarchies\nwere shallow, not usually going much more than three layers\ndeep, but later it became commonplace to ﬁnd more than\nnine levels of ancestors in central classes such as that of\nthe player, their vehicles, or the AI players. For example, in\nUnreal Tournament, the minigun ammo object had this:\nMiniammo →TournamentAmmo →Ammo →Pickup →\nInventory →Actor →Object\nGame developers use inheritance to provide a robust way\nto implement polymorphism in games, where many game en-\n\n\n11.5. HIERARCHICAL DESIGN VS CHANGE\n207\ntities can be updated, rendered, or queried en-mass, without\nany hand coded checking of type. They also appreciate the\nreduced copy-pasting, because inheriting from a class also\nadds functionality to a class. This early form of mix-ins was\nseen to reduce errors in coding as there were often times\nwhere bugs only existed because a programmer had ﬁxed\na bug in one place, but not all of them. Gradually, multiple\ninheritance faded into interfaces only, the practice of only in-\nheriting from one real class, and any others had to be pure\nvirtual interface classes as per the Java deﬁnition.\nAlthough it seems like inheriting from class to extend its\nfunctionality is safe, there are many circumstances where\nclasses don’t quite behave as expected when methods are\noverridden. To extend a class, it is often necessary to read\nthe source, not just of the class you’re inheriting, but also\nthe classes it inherits too. If a base class creates a pure vir-\ntual method, then it forces the child class to implement that\nmethod. If this was for a good reason, then that should be\nenforced, but you cannot enforce that every inheriting class\nimplements this method, only the ﬁrst instantiable class in-\nheriting it. This can lead to obscure bugs where a new class\nsometimes acts or is treated like the class it is inheriting\nfrom.\nA feature missing from C++ also is the idea of being non-\nvirtual. You cannot declare a function as not being virtual.\nThat is, you can deﬁne that a function is an override, but\nyou cannot declare that it is not an override. This can cause\nissues when common words are used, and a new virtual\nmethod is brought into existence. If it overlaps extant func-\ntions with the same signature, then you likely have a bug.\n1\nclass A {\n2\nvirtual\nvoid\nfoo( int bar = 5 ) { cout\n<< bar; }\n3\n};\n4\nclass B : public A {\n5\nvoid\nfoo( int bar = 7 ) { cout\n<< bar * 2; }\n6\n};\n7\nint\nmain( int argc , char *argv [] ) {\n8\nA *a = new B;\n9\na->foo ();\n10\nreturn\n0;\n11\n}\nListing 11.2: Runtime, compile-time, or link-time?\n\n\n208\nCHAPTER 11. WHAT’S WRONG?\nAnother pitfall of inheritance in C++ comes in the form\nof runtime versus compile time linking. A good example is\ndefault arguments on method calls and badly understood\noverriding rules. What would you expect the output of the\nprogram in listing 11.2 to be?\nWould you be surprised to ﬁnd out it reported a value of\n10? Some code relies on the compiled state, some on run-\ntime. Adding new functionality to a class by extending it can\nquickly become a dangerous game as classes from two lay-\ners down can cause coupling side eﬀects, throw exceptions\n(or worse, not throw an exception and quietly fail), circum-\nvent your changes, or possibly just make it impossible to\nimplement your feature as they might already be taking up\nthe namespace or have some other incompatibility with your\nplans, such as requiring a certain alignment or need to be\nin a certain bank of ram.\nInheritance does provide a clean way of implementing\nruntime polymorphism, but it’s not the only way as we saw\nearlier. Adding a new feature by inheritance requires revis-\niting the base class, providing a default implementation, or\na pure virtual, then providing implementations for all the\nclasses that need to handle the new feature. This requires\nmodiﬁcation to the base class, and possible touching all of\nthe child classes if the pure virtual route is taken. So even\nthough the compiler can help you ﬁnd all the places where\nthe code needs to change, it has not made it signiﬁcantly\neasier to change the code.\nUsing a type member instead of a virtual table pointer can\ngive you the same runtime code linking, could be better for\ncache misses, and could be easier to add new features and\nreason about because it has less baggage when it comes to\nimplementing those new features, provides a very simple way\nto mix and match capabilities compared to inheritance, and\nkeeps the polymorphic code in one place. For example, in\nthe fake virtual function go-forward, the class Car will step\non the gas. In the class Person, it will set the direction vec-\n\n\n11.6. DIVISIONS OF LABOUR\n209\ntor. In the class UFO, it will also just set the direction vector.\nThis sounds like a job for a switch statement fall through.\nIn the fake virtual function re-fuel, the class Car and UFO\nwill start a re-fuel timer and remain stationary while their\nfuelling-up animations play, whereas the Person class could\njust reduce their stamina-potion count and be instantly re-\nfuelled. Again, a switch statement with fall through provides\nall the runtime polymorphism you need, but you don’t need\nto multiple inherit in order to provide diﬀerent functionality\non a per class per function level. Being able to pick what\neach method does in a class is not something inheritance is\ngood at, but it is something desirable, and non inheritance\nbased polymorphism does allow it.\nThe original reason for using inheritance was that you\nwould not need to revisit the base class, or change any of\nthe existing code in order to extend and add functionality\nto the codebase, however, it is highly likely you will at least\nneed to view the base class implementation, and with chang-\ning speciﬁcations in games, it’s also quite common to need\nchanges at the base class level.\nInheritance also inhibits\ncertain types of analysis by locking people into thinking of\nobjects as having IS-A relationships with the other object\ntypes in the game. A lot of ﬂexibility is lost when a program-\nmer is locked out of conceptualising objects as being com-\nbinations of features. Reducing multiple inheritance to in-\nterfaces, though helping to reduce the code complexity, has\ndrawn a veil over the one good way of building up classes\nas compound objects. Although not a good solution in itself\nas it still abuses the cache, a switch on type seems to of-\nfer similar functionality to virtual tables without some of the\nassociated baggage. So why put things in classes?\n11.6\nDivisions of labour\nClaim: Modular architecture for reduced coupling and better\ntesting\n\n\n210\nCHAPTER 11. WHAT’S WRONG?\nThe object-oriented paradigm is seen as another tool in\nthe kit when it comes to ensuring quality of code. Strictly ad-\nhering to the open closed principle, always using accessors,\nmethods, and inheritance to use or extend objects, program-\nmers write signiﬁcantly more modular code than they do if\nprogramming from a purely procedural perspective.\nThis\nmodularity separates each object’s code into units. These\nunits are collections of all the data and methods that act\nupon the data. It has been written about many times that\ntesting objects is simpler because each object can be tested\nin isolation.\nHowever, we know it to be untrue, due to data being linked\ntogether by purpose, and purposes being linked together by\ndata in a long chain of accidental relationships.\nObject-oriented design suﬀers from the problem of errors\nin communication.\nObjects are not systems, and systems\nneed to be tested, and systems comprise of not only objects,\nbut their inherent communication. The communication of\nobjects is diﬃcult to test because in practice it is hard to iso-\nlate the interactions between classes. Object-oriented devel-\nopment leads to an object-oriented view of the system which\nmakes it hard to isolate non-objects such as data trans-\nforms, communication, and temporal coupling.\nModular architecture is good because it limits the poten-\ntial damage caused by changes, but just like encapsulation\nbefore, the contract to any module has to be unambiguous so\nas to reduce the chance of external reliance on unintended\nside eﬀects of the implementation.\nThe reason object-oriented modular approach doesn’t\nwork as well is that the modules are deﬁned by object\nboundary, not by a higher level concept.\nGood examples\nof modularity include stdio’s FILE, the CRT’s malloc/free,\nThe NvTriStrip library’s GenerateStrips. Each of these pro-\nvides a solid, documented, narrow set of functions to access\nfunctionality that could otherwise be overwhelming and dif-\nﬁcult to reason about.\n\n\n11.7. REUSABLE GENERIC CODE\n211\nModularity in object-oriented development can oﬀer pro-\ntection from other programmers who don’t understand the\ncode. But why is a programmer that doesn’t understand the\ncode going to be safe even using a trivialised and simpli-\nﬁed interface?\nAn object’s methods are often the instruc-\ntion manual for an object in the eyes of someone new to\nthe code, so writing all the important manipulation methods\nin one block can give clues to anyone using the class. The\nmodularity is important here because game development ob-\njects are regularly large, oﬀering a lot of functionality spread\nacross their many diﬀerent aspects. Rather than ﬁnd a way\nto address cross-cutting concerns, game objects tend to ful-\nﬁl all requirements rather than restrict themselves to their\noriginal design. Because of this bloating, the modular ap-\nproach, that is, collecting methods by their concern in the\nsource, can be beneﬁcial to programmers coming at the ob-\nject fresh.\nThe obvious way to ﬁx this would be to use a\nparadigm that supports cross-cutting concerns at a more\nfundamental level, but object-oriented development in C++\nseems to be ineﬃcient at representing this in code.\nIf object-oriented development doesn’t increase modular-\nity in such a way as it provides better results than explicitly\nmodularising code, then what does it oﬀer?\n11.7\nReusable generic code\nClaim: Faster development time through reuse of generic code\nIt is regarded as one of the holy grails of development to be\nable to consistently reduce development overhead by reusing\nold code. In order to stop wasting any of the investment in\ntime and eﬀort, it’s been assumed it will be possible to put\ntogether an application from existing code and only have to\nwrite some minor new features.\nThe unfortunate truth is\nany interesting new features you want to add will probably\nbe incompatible with your old code and old way of laying out\n\n\n212\nCHAPTER 11. WHAT’S WRONG?\nyour data, and you will need to either rewrite the old code\nto allow for the new feature, or rewrite the old code to allow\nfor the new data layout. If a software project can be built\nfrom existing solutions, from objects invented to provide fea-\ntures for an old project, then it’s probably not very complex.\nAny project of signiﬁcant complexity includes hundreds if\nnot thousands of special case objects that provide all par-\nticular needs of that project. For example, the vast major-\nity of games will have a player class, but almost none share\na common core set of attributes. Is there a world position\nmember in a game of poker? Is there a hit point count mem-\nber in the player of a racing game? Does the player have a\ngamer tag in a purely oﬄine game? Having a generic class\nthat can be reused doesn’t make the game easier to create,\nall it does is move the specialisation into somewhere else.\nSome game toolkits do this by allowing script to extend the\nbasic classes. Some game engines limit the gameplay to a\ncertain genre and allow extension away from that through\ndata-driven means. No one has so far created a game API,\nbecause to do so, it would have to be so generic it wouldn’t\nprovide anything more than what we already have with our\nlanguages we use for development.\nReuse, being hankered after by production, and thought\nof so highly by anyone without much experience in making\ngames, has become an end in itself for many game devel-\nopers. The pitfall of generics is a focus on keeping a class\ngeneric enough to be reused or re-purposed without thought\nas to why, or how. The ﬁrst, the why, is a major stumbling\nblock and needs to be taught out of developers as quickly as\npossible. Making something generic, for the sake of general-\nity, is not a valid goal. Making something generic in the ﬁrst\ninstance adds time to development without adding value.\nSome developers would cite this as short-sighted, however,\nit is the how that deﬂates this argument. How do you gener-\nalise a class if you only use it in one place? The implementa-\ntion of a class is testable only so far as it can be tested, and\nif you only use a class in one place, you can only test that\nit works in one situation. The quality of a class’s reusability\n\n\n11.7. REUSABLE GENERIC CODE\n213\nis inherently untestable until there is something to reuse it,\nand the general rule of thumb is that it’s not reusable unless\nthere are at least three things using it. If you then generalise\nthe class, yet don’t have any other test cases than the ﬁrst\nsituation, then all you can test is that you didn’t break the\nclass when generalising it. So, if you cannot guarantee that\nthe class works for other types or situations, all you have\ndone by generalising the class is added more code for bugs\nto hide in. The resultant bugs are now hidden in code that\nworks, possibly even tested in its isolation, which means any\nbugs introduced during this generalising have been stamped\nand approved, and are now trusted.\nTest-driven development implicitly denies generic coding\nuntil the point where it is a good choice to do so. The only\ntime when it is a good choice to move code to a more generic\nstate, is when it reduces redundancy through reuse of com-\nmon functionality.\nGeneric code has to fulﬁl more than just a basic set of\nfeatures if it is to be used in many situations. If you write a\ntemplated array container, access to the array through the\nsquare bracket operators would be considered a basic fea-\nture, but you will also want to write iterators for it and pos-\nsibly add an insert routine to take the headache out of shuf-\nﬂing the array up in memory.\nLittle bugs can creep in if\nyou rewrite these functions whenever you need them, and\nlinked lists are notorious for having bugs in quick and dirty\nimplementations. To be ﬁt for use by all users, any generic\ncontainer should provide a full set of methods for manipu-\nlation, and the STL does that. There are hundreds of dif-\nferent functions to understand before you can be considered\nan STL-expert, and you have to be an STL-expert before you\ncan be sure you’re writing eﬃcient code with the STL. There\nis a large amount of documentation available for the various\nimplementations of the STL. Most of the implementations of\nthe STL are very similar if not functionally the same. Even\nso, it can take some time for a programmer to become a valu-\nable STL programmer due to this need to learn another lan-\n\n\n214\nCHAPTER 11. WHAT’S WRONG?\nguage. The programmer has to learn a new language, the\nlanguage of the STL, with its own nouns verbs and adjec-\ntives.\nTo limit this, many games companies have a much\nreduced feature set reinterpretation of the STL that option-\nally provides better memory handling (because of the awk-\nward hardware), more choice for the containers (so you may\nchoose a hash-map, trie, or b-tree directly, rather than just a\nmap), or explicit implementations of simpler containers such\nas stack or singly linked lists and their intrusive brethren.\nThese libraries are normally smaller in scope and are there-\nfore easier to learn and hack than the STL variants, but they\nstill need to be learnt and that takes some time. In the past\nthis was a good compromise, but now the STL has extensive\nonline documentation, there is no excuse not to use the STL\nexcept where memory overhead is very intrusive, such as\nin the embedded space where main memory is measured in\nkilobytes, or where compilation time is of massive concern4.\nThe takeaway from this, however, is that generic code still\nneeds to be learnt in order for the coder to be eﬃcient, or not\ncause accidental performance bottlenecks.\nIf you go with\nthe STL, then at least you have a lot of documentation on\nyour side. If your game company implements an amazingly\ncomplex template library, don’t expect any coders to use it\nuntil they’ve had enough time to learn it, and that means,\nif you write generic code, expect people to not use it unless\nthey come across it accidentally, or have been explicitly told\nto, as they won’t know it’s there, or won’t trust it. In other\nwords, starting out by writing generic code is a good way to\nwrite a lot of code quickly without adding any value to your\ndevelopment.\n4The STL is large, but not as large as some OS headers, so ﬁght the right\nbattle ﬁrst\n",
      "page_number": 192,
      "chapter_number": 11,
      "summary": "This chapter covers what’s wrong?. Key topics include object, classes, and code. Where, in essence, is the\nharm in game-development style object-oriented C++.",
      "keywords": [
        "virtual",
        "virtual table pointer",
        "code",
        "virtual table",
        "game",
        "n’t",
        "virtual functions",
        "object",
        "game developers",
        "virtual call",
        "function",
        "data",
        "virtual methods",
        "call",
        "object-oriented"
      ],
      "concepts": [
        "object",
        "classes",
        "code",
        "coded",
        "function",
        "functions",
        "functionality",
        "virtual",
        "virtually",
        "development"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 29,
          "title": "Segment 29 (pages 293-301)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 14,
          "title": "Segment 14 (pages 133-140)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 42,
          "title": "Segment 42 (pages 849-853)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "Data-Oriented Design\nRichard Fabian\n8th October 2018\n",
      "content_length": 53,
      "extraction_method": "Direct"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "Contents\n1\nData-Oriented Design\n3\n1.1\nIt’s all about the data . . . . . . . . . . . . . .\n4\n1.2\nData is not the problem domain\n. . . . . . .\n6\n1.3\nData and statistics . . . . . . . . . . . . . . .\n12\n1.4\nData can change\n. . . . . . . . . . . . . . . .\n14\n1.5\nHow is data formed?\n. . . . . . . . . . . . . .\n18\n1.6\nThe framework . . . . . . . . . . . . . . . . . .\n21\n1.7\nConclusions and takeaways . . . . . . . . . .\n25\n2\nRelational Databases\n27\n2.1\nComplex state . . . . . . . . . . . . . . . . . .\n28\n2.2\nThe framework . . . . . . . . . . . . . . . . . .\n29\n2.3\nNormalising your data . . . . . . . . . . . . .\n32\n2.4\nNormalisation . . . . . . . . . . . . . . . . . .\n36\n2.5\nOperations . . . . . . . . . . . . . . . . . . . .\n51\n2.6\nSumming up . . . . . . . . . . . . . . . . . . .\n53\n2.7\nStream Processing\n. . . . . . . . . . . . . . .\n54\n2.8\nWhy does database technology matter?\n. . .\n55\n3\nExistential Processing\n57\n3.1\nComplexity . . . . . . . . . . . . . . . . . . . .\n58\n3.2\nDebugging . . . . . . . . . . . . . . . . . . . .\n60\n3.3\nWhy use an if\n. . . . . . . . . . . . . . . . . .\n61\n3.4\nTypes of processing . . . . . . . . . . . . . . .\n66\n3.5\nDon’t use booleans . . . . . . . . . . . . . . .\n68\n3.6\nDon’t use enums quite as much\n. . . . . . .\n73\n3.7\nPrelude to polymorphism\n. . . . . . . . . . .\n75\n3.8\nDynamic runtime polymorphism . . . . . . .\n76\n3.9\nEvent handling\n. . . . . . . . . . . . . . . . .\n79\ni\n",
      "content_length": 1416,
      "extraction_method": "Direct"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "ii\nCONTENTS\n4\nComponent Based Objects\n83\n4.1\nComponents in the wild\n. . . . . . . . . . . .\n85\n4.2\nAway from the hierarchy . . . . . . . . . . . .\n88\n4.3\nTowards managers\n. . . . . . . . . . . . . . .\n91\n4.4\nThere is no entity . . . . . . . . . . . . . . . .\n94\n5\nHierarchical Level of Detail\n97\n5.1\nExistence . . . . . . . . . . . . . . . . . . . . .\n98\n5.2\nMementos\n. . . . . . . . . . . . . . . . . . . . 101\n5.3\nJIT mementos . . . . . . . . . . . . . . . . . . 103\n5.4\nAlternative axes . . . . . . . . . . . . . . . . . 106\n5.5\nCollective LOD . . . . . . . . . . . . . . . . . . 110\n6\nSearching\n113\n6.1\nIndexes . . . . . . . . . . . . . . . . . . . . . . 113\n6.2\nData-oriented Lookup\n. . . . . . . . . . . . . 115\n6.3\nFinding low and high . . . . . . . . . . . . . . 120\n6.4\nFinding random . . . . . . . . . . . . . . . . . 121\n7\nSorting\n125\n7.1\nDo you need to? . . . . . . . . . . . . . . . . . 125\n7.2\nMaintaining\n. . . . . . . . . . . . . . . . . . . 128\n7.3\nSorting for your platform . . . . . . . . . . . . 128\n8\nOptimisations\n135\n8.1\nWhen should we optimise?\n. . . . . . . . . . 137\n8.2\nFeedback . . . . . . . . . . . . . . . . . . . . . 138\n8.3\nA strategy\n. . . . . . . . . . . . . . . . . . . . 142\n8.4\nTables . . . . . . . . . . . . . . . . . . . . . . . 146\n8.5\nTransforms . . . . . . . . . . . . . . . . . . . . 151\n8.6\nSpatial sets . . . . . . . . . . . . . . . . . . . . 152\n8.7\nLazy evaluation . . . . . . . . . . . . . . . . . 153\n8.8\nNecessity . . . . . . . . . . . . . . . . . . . . . 154\n8.9\nVarying length sets . . . . . . . . . . . . . . . 155\n8.10\nJoins as intersections\n. . . . . . . . . . . . . 158\n8.11\nData-driven techniques\n. . . . . . . . . . . . 159\n8.12\nStructs of arrays\n. . . . . . . . . . . . . . . . 161\n9\nHelping the compiler\n163\n9.1\nReducing order dependence . . . . . . . . . . 163\n",
      "content_length": 1822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "CONTENTS\n1\n9.2\nReducing memory dependency . . . . . . . . 164\n9.3\nWrite buﬀer awareness . . . . . . . . . . . . . 165\n9.4\nAliasing . . . . . . . . . . . . . . . . . . . . . . 166\n9.5\nReturn value optimisation . . . . . . . . . . . 167\n9.6\nCache line utilisation . . . . . . . . . . . . . . 168\n9.7\nFalse sharing\n. . . . . . . . . . . . . . . . . . 169\n9.8\nSpeculative execution awareness . . . . . . . 171\n9.9\nBranch prediction . . . . . . . . . . . . . . . . 172\n9.10\nDon’t get evicted . . . . . . . . . . . . . . . . . 174\n9.11\nAuto vectorisation . . . . . . . . . . . . . . . . 174\n10\nMaintenance and reuse\n179\n10.1\nCosmic hierarchies . . . . . . . . . . . . . . . 180\n10.2\nDebugging . . . . . . . . . . . . . . . . . . . . 180\n10.3\nReusability . . . . . . . . . . . . . . . . . . . . 183\n10.4\nReusable functions . . . . . . . . . . . . . . . 186\n10.5\nUnit testing\n. . . . . . . . . . . . . . . . . . . 187\n10.6\nRefactoring . . . . . . . . . . . . . . . . . . . . 188\n11\nWhat’s wrong?\n189\n11.1\nThe harm . . . . . . . . . . . . . . . . . . . . . 190\n11.2\nMapping the problem . . . . . . . . . . . . . . 196\n11.3\nInternalised state . . . . . . . . . . . . . . . . 202\n11.4\nInstance oriented development . . . . . . . . 204\n11.5\nHierarchical design vs change . . . . . . . . . 206\n11.6\nDivisions of labour\n. . . . . . . . . . . . . . . 209\n11.7\nReusable generic code\n. . . . . . . . . . . . . 211\n",
      "content_length": 1397,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "2\nCONTENTS\n",
      "content_length": 11,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "Chapter 1\nData-Oriented Design\nData-oriented design has been around for decades in one\nform or another but was only oﬃcially given a name by\nNoel Llopis in his September 2009 article[?] of the same\nname.\nWhether it is, or is not a programming paradigm\nis seen as contentious.\nMany believe it can be used side\nby side with other programming paradigms such as object-\noriented, procedural, or functional programming.\nIn one\nrespect they are right, data-oriented design can function\nalongside the other paradigms, but that does not preclude\nit from being a way to approach programming in the large.\nOther programming paradigms are known to function along-\nside each other to some extent as well. A Lisp programmer\nknows that functional programming can coexist with object-\noriented programming and a C programmer is well aware\nthat object-oriented programming can coexist with proce-\ndural programming. We shall ignore these comments and\nclaim data-oriented design as another important tool; a tool\njust as capable of coexistence as the rest. 1\n1There are some limits, but it is not mutually exclusive with any paradigm\nother than maybe the logic programming languages such as Prolog. The ex-\ntremely declarative ”what, not how” approach does seem to exclude thinking\nabout the data and how it interacts with the machine.\n3\n",
      "content_length": 1321,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "4\nCHAPTER 1. DATA-ORIENTED DESIGN\nThe time was right in 2009. The hardware was ripe for\na change in how to develop. Potentially very fast comput-\ners were hindered by a hardware ignorant programming\nparadigm. The way game programmers coded at the time\nmade many engine programmers weep.\nThe times have\nchanged.\nMany mobile and desktop solutions now seem\nto need the data-oriented design approach less, not be-\ncause the machines are better at mitigating an ineﬀective\napproach, but the games being designed are less demanding\nand less complex. The trend for mobile seems to be moving\nto AAA development, which should bring the return of a\nneed for managing complexity and getting the most out of\nthe hardware.\nAs we now live in a world where multi-core machines in-\nclude the ones in our pockets, learning how to develop soft-\nware in a less serial manner is important. Moving away from\nobjects messaging and getting responses immediately is part\nof the beneﬁts available to the data-oriented programmer.\nProgramming, with a ﬁrm reliance on awareness of the data\nﬂow, sets you up to take the next step to GPGPU and other\ncompute approaches. This leads to handling the workloads\nthat bring game titles to life. The need for data-oriented de-\nsign will only grow. It will grow because abstractions and se-\nrial thinking will be the bottleneck of your competitors, and\nthose that embrace the data-oriented approach will thrive.\n1.1\nIt’s all about the data\nData is all we have. Data is what we need to transform in\norder to create a user experience. Data is what we load when\nwe open a document. Data is the graphics on the screen, the\npulses from the buttons on your gamepad, the cause of your\nspeakers producing waves in the air, the method by which\nyou level up and how the bad guy knew where you were so\nas to shoot at you. Data is how long the dynamite took to ex-\nplode and how many rings you dropped when you fell on the\n",
      "content_length": 1924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "1.1. IT’S ALL ABOUT THE DATA\n5\nspikes. It is the current position and velocity of every particle\nin the beautiful scene that ended the game which was loaded\noﬀthe disc and into your life via transformations by machin-\nery driven by decoded instructions themselves ordered by\nassemblers instructed by compilers fed with source-code.\nNo application is anything without its data. Adobe Pho-\ntoshop without the images is nothing. It’s nothing without\nthe brushes, the layers, the pen pressure. Microsoft Word is\nnothing without the characters, the fonts, the page breaks.\nFL Studio is worthless without the events.\nVisual Studio\nis nothing without source.\nAll the applications that have\never been written, have been written to output data based\non some input data. The form of that data can be extremely\ncomplex, or so simple it requires no documentation at all,\nbut all applications produce and need data.\nIf they don’t\nneed recognisable data, then they are toys or tech demos at\nbest.\nInstructions are data too. Instructions take up memory,\nuse up bandwidth, and can be transformed, loaded, saved\nand constructed. It’s natural for a developer to not think of\ninstructions as being data2, but there is very little diﬀerenti-\nating them on older, less protective hardware. Even though\nmemory set aside for executables is protected from harm and\nmodiﬁcation on most contemporary hardware, this relatively\nnew invention is still merely an invention, and the modiﬁed\nHarvard architecture relies on the same memory for data\nas it does for instructions.\nInstructions are therefore still\ndata, and they are what we transform too. We take instruc-\ntions and turn them into actions.\nThe number, size, and\nfrequency of them is something that matters. The idea that\nwe have control over which instructions we use to solve prob-\nlems leads us to optimisations. Applying our knowledge of\nwhat the data is allows us to make decisions about how the\ndata can be treated. Knowing the outcome of instructions\ngives us the data to decide what instructions are necessary,\nwhich are busywork, and which can be replaced with equiv-\n2unless they are a Lisp programmer\n",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "6\nCHAPTER 1. DATA-ORIENTED DESIGN\nalent but less costly alternatives.\nThis forms the basis of the argument for a data-oriented\napproach to development, but leaves out one major element.\nAll this data and the transforming of data, from strings, to\nimages, to instructions, they all have to run on something.\nSometimes that thing is quite abstract, such as a virtual\nmachine running on unknown hardware. Sometimes that\nthing is concrete, such as knowing which speciﬁc CPU and\nGPU you have, and the memory capacity and bandwidth you\nhave available. But in all cases, the data is not just data,\nbut data that exists on some hardware somewhere, and it\nhas to be transformed by that same hardware. In essence,\ndata-oriented design is the practice of designing software by\ndeveloping transformations for well-formed data where the\ncriteria for well-formed is guided by the target hardware and\nthe patterns and types of transforms that need to operate on\nit. Sometimes the data isn’t well deﬁned, and sometimes the\nhardware is equally evasive, but in most cases a good back-\nground of hardware appreciation can help out almost every\nsoftware project.\nIf the ultimate result of an application is data, and all\ninput can be represented by data, and it is recognised that\nall data transforms are not performed in a vacuum, then a\nsoftware development methodology can be founded on these\nprinciples; the principles of understanding the data, and\nhow to transform it given some knowledge of how a machine\nwill do what it needs to do with data of this quantity, fre-\nquency, and its statistical qualities. Given this basis, we can\nbuild up a set of founding statements about what makes a\nmethodology data-oriented.\n1.2\nData is not the problem domain\nThe ﬁrst principle: Data is not the problem domain.\n",
      "content_length": 1788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "1.2. DATA IS NOT THE PROBLEM DOMAIN\n7\nFor some, it would seem that data-oriented design is the\nantithesis of most other programming paradigms because\ndata-oriented design is a technique that does not readily al-\nlow the problem domain to enter into the software as written\nin source. It does not promote the concept of an object as\na mapping to the context of the user in any way, as data is\nintentionally and consistently without meaning. Abstraction\nheavy paradigms try to pretend the computer and its data do\nnot exist at every turn, abstracting away the idea that there\nare bytes, or CPU pipelines, or other hardware features, and\ninstead bringing the model of the problem into the program.\nThey regularly bring either the model of the view into the\ncode, or the model of the world as a context for the problem.\nThat is, they either structure the code around attributes of\nthe expected solution, or they structure the code around the\ndescription of the problem domain.\nMeaning can be applied to data to create information.\nMeaning is not inherent in data. When you say 4, it means\nvery little, but say 4 miles, or 4 eggs, it means something.\nWhen you have 3 numbers, they mean very little as a tu-\nple, but when you name them x,y,z, you can put meaning on\nthem as a position. When you have a list of positions in a\ngame, they mean very little without context. Object-oriented\ndesign would likely have the positions as part of an object,\nand by the class name and neighbouring data (also named)\nyou can get an idea of what that data means. Without the\nconnected named contextualising data, the positions could\nbe interpreted in a number of diﬀerent ways, and though\nputting the numbers in context is good in some sense, it\nalso blocks thinking about the positions as just sets of three\nnumbers, which can be important for thinking of solutions\nto the real problems the programmers are trying to solve.\nFor an example of what can happen when you put data\nso deep inside an object that you forget its impact, consider\nthe numerous games released, and in production, where a\n2D or 3D grid system could have been used for the data\nlayout, but for unknown reasons the developers kept with\n",
      "content_length": 2187,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "8\nCHAPTER 1. DATA-ORIENTED DESIGN\nthe object paradigm for each entity on the map. This isn’t\na singular event, and real shipping games have seen this\nobject-centric approach commit crimes against the hardware\nby having hundreds of objects placed in WorldSpace at grid\ncoordinates, rather than actually being driven by a grid. It’s\npossible that programmers look at a grid, and see the num-\nber of elements required to fulﬁl the request, and are hesitant\nto the idea of allocating it in a single lump of memory. Con-\nsider a simple 256 by 256 tilemap requiring 65,536 tiles. An\nobject-oriented programmer may think about those sixty-ﬁve\nthousand objects as being quite expensive. It might make\nmore sense for them to allocate the objects for the tiles only\nwhen necessary, even to the point where there literally are\nsixty-ﬁve thousand tiles created by hand in editor, but be-\ncause they were placed by hand, their necessity has been es-\ntablished, and they are now something to be handled, rather\nthan something potentially worrying.\nNot only is this pervasive lack of an underlying form a\npoor way to handle rendering and simple element placement,\nbut it leads to much higher complexity when interpreting lo-\ncality of elements.\nGaining access to elements on a grid-\nfree representation often requires jumping through hoops\nsuch as having neighbour links (which need to be kept up to\ndate), running through the entire list of elements (inherently\ncostly), or references to an auxiliary augmented grid object\nor spatial mapping system connecting to the objects which\nare otherwise free to move, but won’t, due to the design of the\ngame. This fake form of freedom introduced by the grid-free\ndesign presents issues with understanding the data, and has\nbeen the cause of some signiﬁcant performance penalties in\nsome titles. Thus also causing a signiﬁcant waste of pro-\ngrammer mental resources in all.\nOther than not having grids where they make sense,\nmany modern games also seem to carry instances for each\nand every item in the game.\nAn instance for each rather\nthan a variable storing the number of items.\nFor some\ngames this is an optimisation, as creation and destruction\n",
      "content_length": 2177,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "1.2. DATA IS NOT THE PROBLEM DOMAIN\n9\nof objects is a costly activity, but the trend is worrying, as\nthese ways of storing information about the world make the\nworld impenetrable to simple interrogation.\nMany games seem to try to keep everything about the\nplayer in the player class. If the player dies in-game, they\nhave to hang around as a dead object, otherwise, they lose\naccess to their achievement data. This linking of what the\ndata is, to where it resides and what it shares lifetime with,\ncauses monolithic classes and hard to untangle relation-\nships which frequently turn out to be the cause of bugs. I\nwill not name any of the games, but it’s not just one title, nor\njust one studio, but an epidemic of poor technical design that\nseems to infect those who use oﬀthe shelf object-oriented\nengines more than those who develop their own regardless\nof paradigm.\nThe data-oriented design approach doesn’t build the real-\nworld problem into the code. This could be seen as a failing of\nthe data-oriented approach by veteran object-oriented devel-\nopers, as examples of the success of object-oriented design\ncome from being able to bring the human concepts to the ma-\nchine, then in this middle ground, a solution can be written\nthat is understandable by both human and computer. The\ndata-oriented approach gives up some of the human read-\nability by leaving the problem domain in the design docu-\nment, bringing elements of constraints and expectations into\nthe transforms, but stops the machine from having to handle\nhuman concepts at any data level by just that same action.\nLet us consider how the problem domain becomes part of\nthe software in programming paradigms that promote need-\nless abstraction. In the case of objects, we tie meanings to\ndata by associating them with their containing classes and\ntheir associated functions. In high-level abstraction, we sep-\narate actions and data by high-level concepts, which might\nnot apply at the low level, thus reducing the likelihood the\nfunctions can be implemented eﬃciently.\nWhen a class owns some data, it gives that data a context\n",
      "content_length": 2096,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "10\nCHAPTER 1. DATA-ORIENTED DESIGN\nwhich can sometimes limit the ability to reuse the data or un-\nderstand the impact of operations upon it. Adding functions\nto a context can bring in further data, which quickly leads\nto classes containing many diﬀerent pieces of data that are\nunrelated in themselves, but need to be in the same class\nbecause an operation required a context and the context re-\nquired more data for other reasons such as for other related\noperations. This sounds awfully familiar, and Joe Armstrong\nis quoted to have said “I think the lack of reusability comes\nin object-oriented languages, not functional languages. Be-\ncause the problem with object-oriented languages is they’ve\ngot all this implicit environment that they carry around with\nthem. You wanted a banana but what you got was a gorilla\nholding the banana and the entire jungle.”3 which certainly\nseems to resonate with the issue of contextual referencing\nthat seems to be plaguing the object-oriented languages.\nYou could be forgiven for believing that it’s possible to re-\nmove the connections between contexts by using interfaces\nor dependency injection, but the connections lie deeper than\nthat. The contexts in the objects are often connecting diﬀer-\nent classes of data about diﬀerent categories in which the\nobject ﬁts. Consider how this banana has many diﬀerent\npurposes, from being a fruit, to being a colour, to being a\nword beginning with the letter B. We have to consider the\nproblem presented by the idea of the banana as an instance,\nas well as the banana being a class of entity too. If we need\nto gain information about bananas from the point of view of\nthe law on imported goods, or about its nutritional value, it’s\ngoing to be diﬀerent from information about how many we\nare currently stocking. We were lucky to start with the ba-\nnana. If we talk about the gorilla, then we have information\nabout the individual gorilla, the gorillas in the zoo or jungle,\nand the class of gorilla too. This is three diﬀerent layers of\nabstraction about something which we might give one name.\nAt least with a banana, each individual doesn’t have much\nin the way of important data. We see this kind of contextual\nlinkage all the time in the real world, and we manage the\n3From Peter Seibel’s Coders at Work[?]\n",
      "content_length": 2294,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "1.2. DATA IS NOT THE PROBLEM DOMAIN\n11\ncomplexity very well in conversation, but as soon as we start\nputting these contexts down in hard terms we connect them\ntogether and make them brittle.\nAll these mixed layers of abstraction become hard to un-\ntangle as functions which operate over each context drag\nin random pieces of data from all over the classes mean-\ning many data items cannot be removed as they would then\nbe inaccessible. This can be enough to stop most program-\nmers from attempting large-scale evolving software projects,\nbut there is another issue caused by hiding the actions ap-\nplied to the data that leads to unnecessary complexity. When\nyou see lists and trees, arrays and maps, tables and rows,\nyou can reason about them and their interactions and trans-\nformations. If you attempt to do the same with homes and\noﬃces, roads and commuters, coﬀee shops and parks, you\ncan often get stuck in thinking about the problem domain\nconcepts and not see the details that would provide clues to\na better data representation or a diﬀerent algorithmic ap-\nproach.\nThere are very few computer science algorithms that\ncannot be reused on primitive data types, but when you\nintroduce new classes with their own internal layouts of\ndata, that don’t follow clearly in the patterns of existing\ndata-structures, then you won’t be able to fully utilise those\nalgorithms, and might not even be able to see how they\nwould apply. Putting data structures inside your object de-\nsigns might make sense from what they are, but they often\nmake little sense from the perspective of data manipulation.\nWhen we consider the data from the data-oriented design\npoint of view, data is mere facts that can be interpreted in\nwhatever way necessary to get the output data in the format\nit needs to be. We only care about what transforms we do,\nand where the data ends up. In practice, when you discard\nmeanings from data, you also reduce the chance of tangling\nthe facts with their contexts, and thus you also reduce the\nlikelihood of mixing unrelated data just for the sake of an\n",
      "content_length": 2068,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "12\nCHAPTER 1. DATA-ORIENTED DESIGN\noperation or two.\n1.3\nData and statistics\nThe second principle: Data is the type, frequency, quantity,\nshape, and probability.\nThe second statement is that data is not just the struc-\nture. A common misconception about data-oriented design\nis that it’s all about cache misses. Even if it was all about\nmaking sure you never missed the cache, and it was all about\nstructuring your classes so the hot and cold data was split\napart, it would be a generally useful addition to your pro-\ngramming toolkit, but data-oriented design is about all as-\npects of the data.\nTo write a book on how to avoid cache\nmisses, you need more than just some tips on how to organ-\nise your structures, you need a grounding in what is really\nhappening inside your computer when it is running your pro-\ngram. Teaching that in a book is also impossible as it would\nonly apply to one generation of hardware, and one genera-\ntion of programming languages, however, data-oriented de-\nsign is not rooted in just one language and just some unusual\nhardware, even though the language to best beneﬁt from it\nis C++, and the hardware to beneﬁt the approach the most\nis anything with unbalanced bottlenecks. The schema of the\ndata is important, but the values and how the data is trans-\nformed are as important, if not more so. It is not enough to\nhave some photographs of a cheetah to determine how fast it\ncan run. You need to see it in the wild and understand the\ntrue costs of being slow.\nThe data-oriented design model is centred around data.\nIt pivots on live data, real data, data that is also information.\nObject-oriented design is centred around the problem deﬁni-\ntion. Objects are not real things but abstract representations\nof the context in which the problem will be solved. The ob-\njects manipulate the data needed to represent them without\n",
      "content_length": 1857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "1.3. DATA AND STATISTICS\n13\nany consideration for the hardware or the real-world data\npatterns or quantities. This is why object-oriented design\nallows you to quickly build up ﬁrst versions of applications,\nallowing you to put the ﬁrst version of the design document\nor problem deﬁnition directly into the code, and make a quick\nattempt at a solution.\nData-oriented design takes a diﬀerent approach to the\nproblem, instead of assuming we know nothing about the\nhardware, it assumes we know little about the true nature\nof our problem, and makes the schema of the data a second-\nclass citizen.\nAnyone who has written a sizeable piece of\nsoftware may recognise that the technical structure and the\ndesign for a project often changes so much that there is\nbarely any section from the ﬁrst draft remaining unchanged\nin the ﬁnal implementation.\nData-oriented design avoids\nwasting resources by never assuming the design needs to\nexist anywhere other than in a document. It makes progress\nby providing a solution to the current problem through some\nhigh-level code controlling sequences of events and specify-\ning schema in which to give temporary meaning to the data.\nData-oriented design takes its cues from the data which\nis seen or expected. Instead of planning for all eventualities,\nor planning to make things adaptable, there is a preference\nfor using the most probable input to direct the choice of al-\ngorithm. Instead of planning to be extendable, it plans to be\nsimple and replaceable, and get the job done. Extendable\ncan be added later, with the safety net of unit tests to ensure\nit remains working as it did while it was simple.\nLuckily,\nthere is a way to make your data layout extendable without\nrequiring much thought, by utilising techniques developed\nmany years ago for working with databases.\nDatabase technology took a great turn for the positive\nwhen the relational model was introduced. In the paper Out\nof the Tar Pit[?], Functional Relational Programming takes\nit a step further when it references the idea of using re-\nlational model data-structures with functional transforms.\n",
      "content_length": 2098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "14\nCHAPTER 1. DATA-ORIENTED DESIGN\nThese are well deﬁned, and much literature on how to adapt\ntheir form to match your requirements is available.\n1.4\nData can change\nData-oriented design is current. It is not a representation of\nthe history of a problem or a solution that has been brought\nup to date, nor is it the future, with generic solutions made\nup to handle whatever will come along. Holding onto the past\nwill interfere with ﬂexibility, and looking to the future is gen-\nerally fruitless as programmers are not fortune tellers. It’s\nthe opinion of the author, that future-proof systems rarely\nare. Object-oriented design starts to show its weaknesses\nwhen designs change in the real-world.\nObject-oriented design is known to handle changes to un-\nderlying implementation details very well, as these are the\nexpected changes, the obvious changes, and the ones often\ncited in introductions to object-oriented design.\nHowever,\nreal world changes such as change of user’s needs, changes\nto input format, quantity, frequency, and the route by which\nthe information will travel, are not handled with grace. It was\nintroduced in On the Criteria To Be Used in Decomposing Sys-\ntems into Modules[?] that the modularisation approach used\nby many at the time was rather like that of a production\nline, where elements of the implementation are caught up\nin the stages of the proposed solution. These stages them-\nselves would be identiﬁed with a current interpretation of the\nproblem. In the original document, the solution was to intro-\nduce a data hiding approach to modularisation, and though\nit was an improvement, in the later book Software Pioneers:\nContributions to Software Engineering[?], D. L. Parnas revis-\nits the issue and reminds us that even though initial soft-\nware development can be faster when making structural de-\ncisions based on business facts, it lays a burden on main-\ntenance and evolutionary development. Object-oriented de-\nsign approaches suﬀer from this inertia inherent in keeping\n",
      "content_length": 2009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "1.4. DATA CAN CHANGE\n15\nthe problem domain coupled with the implementation. As\nmentioned, the problem domain, when introduced into the\nimplementation, can help with making decisions quickly, as\nyou can immediately see the impact the implementation will\nhave on getting closer to the goal of solving or working with\nthe problem in its current form. The problem with object-\noriented design lies in the inevitability of change at a higher\nlevel.\nDesigns change for multiple reasons, occasionally includ-\ning times when they actually haven’t. A misunderstanding\nof a design, or a misinterpretation of a design, will cause as\nmuch change in the implementation as a literal request for\nchange of design. A data-oriented approach to code design\nconsiders the change in design through the lens of under-\nstanding the change in the meaning of the data. The data-\noriented approach to design also allows for change to the\ncode when the source of data changes, unlike the encap-\nsulated internal state manipulations of the object-oriented\napproach. In general, data-oriented design handles change\nbetter as pieces of data and transforms can be more sim-\nply coupled and decoupled than objects can be mutated and\nreused.\nThe reason this is so, comes from linking the intention,\nor the aspect, to the data. When lumping data and func-\ntions in with concepts of objects, you ﬁnd the objects are\nthe schema of the data.\nThe aspect of the data is linked\nto that object, which means it’s hard to think of the data\nfrom another point of view. The use case of the data, and\nthe real-world or design, are now linked to the data layout\nthrough a singular vision implied by the object deﬁnition. If\nyou link your data layout to the union of the required data\nfor your expected manipulations, and your data manipula-\ntions are linked by aspects of your data, then you make it\nhard to unlink data related by aspect. The diﬃculty comes\nwhen diﬀerent aspects need diﬀerent subsets of the data,\nand they overlap. When they overlap, they create a larger\nand larger set of values that need to travel around the sys-\n",
      "content_length": 2088,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "16\nCHAPTER 1. DATA-ORIENTED DESIGN\ntem as one unit.\nIt’s common to refactor a class out into\ntwo or more classes, or give ownership of data to a diﬀerent\nclass. This is what is meant by tying data to an aspect. It\nis tied to the lens through which the data has purpose, but\nwith static typed objects that purpose is predeﬁned, a union\nof multiple purposes, and sometimes carries around defunct\nrelationships. Some purposes may no longer required by the\ndesign. Unfortunately, it’s easier to see when a relationship\nneeds to exist, than when it doesn’t, and that leads to more\nconnections, not fewer, over time.\nIf you link your operations by related data, such as when\nyou put methods on a class, you make it hard to unlink your\noperations when the data changes or splits, and you make\nit hard to split data when an operation requires the data\nto be together for its own purposes. If you keep your data\nin one place, operations in another place, and keep the as-\npects and roles of data intrinsic from how the operations and\ntransforms are applied to the data, then you will ﬁnd that\nmany times when refactoring would have been large and dif-\nﬁcult in object-oriented code, the task now becomes trivial\nor non-existent. With this beneﬁt comes a cost of keeping\ntabs on what data is required for each operation, and the\npotential danger of de-synchronisation. This consideration\ncan lead to keeping some cold code in an object-oriented style\nwhere objects are responsible for maintaining internal con-\nsistency over eﬃciency and mutability. Examples of places\nwhere object-oriented design is far superior to data-oriented\ncan be that of driver layers for systems or hardware. Even\nthough Vulkan and OpenGL are object-oriented, the gran-\nularity of the objects is large and linked to stable concepts\nin their space, just like the object-oriented approach of the\nFILE type or handle, in open, close, read, and write opera-\ntions in ﬁlesystems.\nA big misunderstanding for many new to the data-oriented\ndesign paradigm, a concept brought over from abstraction\nbased development, is that we can design a static library or\nset of templates to provide generic solutions to everything\n",
      "content_length": 2176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "1.4. DATA CAN CHANGE\n17\npresented in this book as a data-oriented solution. Much like\nwith domain driven design, data-oriented design is product\nand work-ﬂow speciﬁc. You learn how to do data-oriented\ndesign, not how to add it to your project. The fundamental\ntruth is that data, though it can be generic by type, is not\ngeneric in how it is used. The values are diﬀerent and often\ncontain patterns we can turn to our advantage.\nThe idea\nthat data can be generic is a false claim that data-oriented\ndesign attempts to rectify. The transforms applied to data\ncan be generic to some extent, but the order and selection of\noperations are literally the solution to the problem. Source\ncode is the recipe for conversion of data from one form into\nanother. There cannot be a library of templates for under-\nstanding and leveraging patterns in the data, and that’s\nwhat drives a successful data-oriented design. It’s true we\ncan build algorithms to ﬁnd patterns in data, otherwise,\nhow would it be possible to do compression, but the pat-\nterns we think about when it comes to data-oriented design\nare higher level, domain-speciﬁc, and not simple frequency\nmappings.\nOur run-time beneﬁts from specialisation through perfor-\nmance tricks that sometimes make the code harder to read,\nbut it is frequently discouraged as being not object-oriented,\nor being too hard-coded.\nIt can be better to hard-code a\ntransform than to pretend it’s not hard-coded by wrapping it\nin a generic container and using less direct algorithms on it.\nUsing existing templates like this provides a beneﬁt of an in-\ncrease in readability for those who already know the library,\nand potentially fewer bugs if the functionality was in some\nway generic. But, if the functionality was not well mapped to\nthe existing generic solution, writing it with a function tem-\nplate and then extending will make the code harder to under-\nstand. Hiding the fact that the technique had been changed\nsubtly will introduced false assumptions. Hard-coding a new\nalgorithm is a better choice as long as it has suﬃcient tests,\nand is objectively new. Tests will also be easier to write if\nyou constrain yourself to the facts about concrete data and\nonly test with real, but simple data for your problem, and\n",
      "content_length": 2253,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "18\nCHAPTER 1. DATA-ORIENTED DESIGN\nnot generic types on generic data.\n1.5\nHow is data formed?\nThe games we write have a lot of data, in a lot of diﬀerent\nformats.\nWe have textures in multiple formats for multi-\nple platforms. There are animations, usually optimised for\ndiﬀerent skeletons or types of playback. There are sounds,\nlights, and scripts. Don’t forget meshes, they consist of mul-\ntiple buﬀers of attributes. Only a very small proportion of\nmeshes are old ﬁxed function type with vertices containing\npositions, UVs, and normals. The data in game development\nis hard to box, and getting harder to pin down as more ideas\nwhich were previously considered impossible have now be-\ncome commonplace. This is why we spend a lot of time work-\ning on editors and tool-chains, so we can take the free-form\noutput from designers and artists and ﬁnd a way to put it\ninto our engines. Without our tool-chains, editors, viewers,\nand tweaking tools, there would be no way we could pro-\nduce a game with the time we have. The object-oriented ap-\nproach provides a good way to wrap our heads around all\nthese diﬀerent formats of data. It gives a centralised view\nof where each type of data belongs and classiﬁes it by what\ncan be done to it. This makes it very easy to add and use\ndata quickly, but implementing all these diﬀerent wrapper\nobjects takes time.\nAdding new functionality to these ob-\njects can sometimes require large amounts of refactoring as\noccasionally objects are classiﬁed in such a way that they\ndon’t allow for new features to exist. For example, in many\nold engines, textures were always 1,2, or 4 bytes per pixel.\nWith the advent of ﬂoating point textures, all that code re-\nquired a minor refactoring. In the past, it was not possible\nto read a texture from the vertex shader, so when texture\nbased skinning came along, many engine programmers had\nto refactor their render update. They had to allow for a vertex\nshader texture upload because it might be necessary when\nuploading transforms for rendering a skinned mesh. When\n",
      "content_length": 2043,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "1.5. HOW IS DATA FORMED?\n19\nthe PlayStation2 came along, or an engine ﬁrst used shaders,\nthe very idea of what made a material had to change.\nIn\nthe move from small 3D environments to large open worlds\nwith level of detail caused many engineers to start think-\ning about what it meant for something to need rendering.\nWhen newer hardware became more picky about alignment,\nother hard to inject changes had to be made. In many en-\ngines, mesh data is optimised for rendering, but when you\nhave to do mesh ray casting to see where bullets have hit, or\nfor doing IK, or physics, then you need multiple representa-\ntions of an entity. At this point, the object-oriented approach\nstarts to look cobbled together as there are fewer objects that\nrepresent real things, and more objects used as containers\nso programmers can think in larger building blocks. These\nblocks hinder though, as they become the only blocks used\nin thought, and stop potential mental connections from hap-\npening. We went from 2D sprites to 3D meshes, following the\nformat of the hardware provider, to custom data streams and\ncompute units turning the streams into rendered triangles.\nWave data, to banks, to envelope controlled grain tables and\nslews of layered sounds. Tilemaps, to portals and rooms, to\nstreamed, multiple levels of detail chunks of world, to hybrid\nmesh palette, props, and unique stitching assets. From ﬂip-\nbook to Euler angle sequences, to quaternions and spherical\ninterpolated animations, to animation trees and behaviour\nmapping/trees. Change is the only constant.\nAll these types of data are pretty common if you’ve worked\nin games at all, and many engines do provide an abstraction\nto these more fundamental types. When a new type of data\nbecomes heavily used it is promoted into engines as a core\ntype. We normally consider the trade-oﬀof new types being\nhandled as special cases until they become ubiquitous to be\none of usability vs performance. We don’t want to provide\nfree access to the lesser understood elements of game devel-\nopment. People who are not, or can not, invest time in ﬁnd-\ning out how best to use new features, are discouraged from\nusing them. The object-oriented game development way to\ndo that is to not provide objects which represent them, and\n",
      "content_length": 2266,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "20\nCHAPTER 1. DATA-ORIENTED DESIGN\ninstead only oﬀer the features to people who know how to\nutilise the more advanced tools.\nApart from the objects representing digital assets, there\nare also objects for internal game logic.\nFor every game,\nthere are objects which only exist to further the game-play.\nCollectable card games have a lot of textures, but they also\nhave a great deal of rules, card stats, player decks, match\nrecords, with many objects to represent the current state of\nplay. All of these objects are completely custom designed for\none game. There may be sequels, but unless it’s primarily a\nre-skin, it will use quite diﬀerent game logic in many places,\nand therefore require diﬀerent data, which would imply dif-\nferent methods on the now guaranteed to be internally dif-\nferent objects.\nGame data is complex. Any ﬁrst layout of the data is in-\nspired by the game’s initial design. Once development is un-\nderway, the layout needs to keep up with whichever way the\ngame evolves. Object-oriented techniques oﬀer a quick way\nto implement any given design, are very quick at implement-\ning each singular design in turn, but don’t oﬀer a clean or\ngraceful way to migrate from one data schema to the next.\nThere are hacks, such as those used in version based asset\nhandlers, or in frameworks backed by update systems and\nconversion scripts, but normally, game developers change\nthe tool-chain and the engine at the same time, do a full re-\nexport of all the assets, then commit to the next version all\nin one go. This can be quite a painful experience if it has to\nhappen over multiple sites at the same time, or if you have\na lot of assets, or if you are trying to provide engine sup-\nport for more than one title, and only one wants to change\nto the new revision. An example of an object-oriented ap-\nproach that handles migration of design with some grace is\nthe Django framework, but the reason it handles the migra-\ntion well is that the objects would appear to be views into\ndata models, not the data itself.\nThere have not yet been any successful eﬀorts to build a\n",
      "content_length": 2083,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "1.6. THE FRAMEWORK\n21\ngeneric game asset solution. This may be because all games\ndiﬀer in so many subtle ways that if you did provide a generic\nsolution, it wouldn’t be a game solution, just a new language.\nThere is no solution to be found in trying to provide all the\npossible types of object a game can use. But, there is a so-\nlution if we go back to thinking about a game as merely run-\nning a set of computations on some data. The closest we\ncan get in 2018 is the FBX format, with some dependence\non the current standard shader languages. The current so-\nlutions appear to have excess baggage which does not seem\neasy to remove. Due to the need to be generic, many details\nare lost through abstractions and strategies to present data\nin a non-confrontational way.\n1.6\nWhat can provide a computational\nframework for such complex data?\nGame developers are notorious for thinking about game\ndevelopment from either a low level all out performance\nperspective or from a very high-level gameplay and inter-\naction perspective. This may have come about because of\nthe widening gap between the amount of code that has to be\nhigh performance, and the amount of code to make the game\ncomplete. Object-oriented techniques provide good coverage\nof the high-level aspect, so the high-level programmers are\ncontent with their tools. The performance specialists have\nbeen ﬁnding ways of doing more with the hardware, so\nmuch so that a lot of the time content creators think they\ndon’t have a part in the optimisation process.\nThere has\nnever been much of a middle ground in game development,\nwhich is probably the primary reason why the structure\nand performance techniques employed by big-iron compa-\nnies didn’t seem useful.\nThe secondary reason could be\nthat game developers don’t normally develop systems and\napplications which have decade-long maintenance expecta-\n",
      "content_length": 1862,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "22\nCHAPTER 1. DATA-ORIENTED DESIGN\ntions4 and therefore are less likely to be concerned about\nwhy their code should be encapsulated and protected or\nat least well documented.\nWhen game development was\nﬁrst ﬂourishing into larger studios in the late 1990’s, aca-\ndemic or corporate software engineering practices were seen\nas suspicious because wherever they were employed, there\nwas a dramatic drop in game performance, and whenever\nany prospective employees came from those industries, they\nfailed to impress. As games machines became more like the\nstandard micro-computers, and standard micro-computers\ndrew closer in design to the mainframes of old, the more ap-\nparent it became that some of those standard professional\nsoftware engineering practices could be useful.\nNow the\nscale of games has grown to match the hardware, but the\ngames industry has stopped looking at where those non-\ngame development practices led. As an industry, we should\nbe looking to where others have gone before us, and the clos-\nest set of academic and professional development techniques\nseem to be grounded in simulation and high volume data\nanalysis. We still have industry-speciﬁc challenges such as\nthe problems of high frequency highly heterogeneous trans-\nformational requirements that we experience in suﬃciently\nvoluminous AI environments, and we have the issue of user\nproximity in networked environments, such as the problems\nfaced by MMOs when they have location-based events, and\nbandwidth starts to hit n2 issues as everyone is trying to\nmessage everyone else.\nWith each successive generation, the number of devel-\noper hours to create a game has grown, which is why project\nmanagement and software engineering practices have be-\ncome standardised at the larger games companies.\nThere\nwas a time when game developers were seen as cutting-edge\nprogrammers, inventing new technology as the need arises,\nbut with the advent of less adventurous hardware (most no-\ntably in the x86 based recent 8thgenerations), there has been\na shift away from ingenious coding practices, and towards a\n4people at Blizzard Entertainment, Inc.\nlikely have something to say\nabout this\n",
      "content_length": 2158,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "1.6. THE FRAMEWORK\n23\nstandardised process. This means game development can be\ntuned to ensure the release date will coincide with market-\ning dates. There will always be an element of randomness\nin high proﬁle game development. There will always be an\nelement of innovation that virtually guarantees you will not\nbe able to predict how long the project, or at least one part\nof the project, will take. Even if data-oriented design isn’t\nneeded to make your game go faster, it can be used to make\nyour game development schedule more regular.\nPart of the diﬃculty in adding new and innovative fea-\ntures to a game is the data layout. If you need to change the\ndata layout for a game, it will need objects to be redesigned\nor extended in order to work within the existing framework.\nIf there is no new data, then a feature might require that\npreviously separate systems suddenly be able to talk to each\nother quite intimately. This coupling can often cause system-\nwide confusion with additional temporal coupling and corner\ncases so obscure they can only be reproduced one time in\na million. These odds might sound ﬁne to some developers,\nbut if you’re expecting to sell ﬁve to ﬁfty million copies of your\ngame, at one in a million, that’s ﬁve to ﬁfty people who will\nexperience the problem, can take a video of your game be-\nhaving oddly, post it on the YouTube, and call your company\nrubbish, or your developers lazy, because they hadn’t ﬁxed\nan obvious bug. Worse, what if the one in a million issue\nwas a way to circumvent in-app-purchases, and was repro-\nducible if you knew what to do and the steps start spreading\non Twitter, or maybe created an economy-destroying inﬂux\nof resources in a live MMO universe5. In the past, if you had\nsold ﬁve to ﬁfty million copies of your game, you wouldn’t\ncare, but with the advent of free-to-play games, ﬁve million\nplayers might be considered a good start, and poor reviews\ncoming in will curb the growth. IAP circumventions will kill\nyour income, and economy destruction will end you.\n5The\nwebcomic\nand\nanecdotes\nsite\nThe-Trenches\ndid\na\nse-\nquence\nof\nstrips\nin\na\nwebcomic\non\nthis,\nand\npointed\nout\nmany\nof\nthe\nissues\nwith\ntrying\nto\nﬁx\nit\nonce\nit\nhas\ngone\nlive\nhttp://www.trenchescomic.com/comic/post/apocalypse\n",
      "content_length": 2255,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "24\nCHAPTER 1. DATA-ORIENTED DESIGN\nBig iron developers had these same concerns back in the\n1970’s. Their software had to be built to high standards be-\ncause their programs would frequently be working on data\nconcerned with real money transactions.\nThey needed to\nwrite business logic that operated on the data, but most im-\nportant of all, they had to make sure the data was updated\nthrough a provably careful set of operations in order to main-\ntain its integrity. Database technology grew from the need to\nprocess stored data, to do complex analysis on it, to store and\nupdate it, and be able to guarantee it was valid at all times.\nTo do this, the ACID test was used to ensure atomicity, con-\nsistency, isolation, and durability. Atomicity was the test to\nensure all transactions would either complete or do noth-\ning. It could be very bad for a database to update only one\naccount in a ﬁnancial transaction. There could be money\nlost or created if a transaction was not atomic. Consistency\nwas added to ensure all the resultant state changes which\nshould happen during a transaction do happen, that is, all\ntriggers which should ﬁre, do ﬁre, even if the triggers cause\ntriggers recursively, with no limit. This would be highly im-\nportant if an account should be blocked after it has triggered\na form of fraud detection.\nIf a trigger has not ﬁred, then\nthe company using the database could risk being liable for\neven more than if they had stopped the account when they\nﬁrst detected fraud. Isolation is concerned with ensuring all\ntransactions which occur cannot cause any other transac-\ntions to diﬀer in behaviour. Normally this means that if two\ntransactions appear to work on the same data, they have to\nqueue up and not try to operate at the same time. Although\nthis is generally good, it does cause concurrency problems.\nFinally, durability. This was the second most important el-\nement of the four, as it has always been important to en-\nsure that once a transaction has completed, it remains so.\nIn database terminology, durability meant the transaction\nwould be guaranteed to have been stored in such a way that\nit would survive server crashes or power outages. This was\nimportant for networked computers where it would be im-\nportant to know what transactions had deﬁnitely happened\nwhen a server crashed or a connection dropped.\n",
      "content_length": 2345,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "1.7. CONCLUSIONS AND TAKEAWAYS\n25\nModern networked games also have to worry about highly\nimportant data like this. With non-free downloadable con-\ntent, consumers care about consistency. With consumable\ndownloadable content, users care a great deal about every\ntransaction. To provide much of the functionality required of\nthe database ACID test, game developers have gone back to\nlooking at how databases were designed to cope with these\nstrict requirements and found reference to staged commits,\nidempotent functions, techniques for concurrent develop-\nment, and a vast literature base on how to design tables for\na database.\n1.7\nConclusions and takeaways\nWe’ve talked about data-oriented design being a way to think\nabout and lay out your data and to make decisions about\nyour architecture.\nWe have two principles that can drive\nmany of the decisions we need to make when doing data-\noriented design. To ﬁnish the chapter, there are some take-\naways you can use immediately to begin your journey.\nConsider how your data is being inﬂuenced by what it’s\ncalled. Consider the possibility that the proximity of other\ndata can inﬂuence the meaning of your data, and in doing\nso, trap it in a model that inhibits ﬂexibility. For the consid-\neration of the ﬁrst principle, data is not the problem domain,\nit’s worth thinking about the following items.\n• What is tying your data together, is it a concept or im-\nplied meaning?\n• Is your data layout deﬁned by a single interpretation\nfrom a single point of view?\n• Think about how the data could be reinterpreted and\ncut along those lines.\n• What is it about the data that makes it uniquely impor-\ntant?\n",
      "content_length": 1648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "26\nCHAPTER 1. DATA-ORIENTED DESIGN\nYou are not targeting an unknown device with unknow-\nable characteristics. Know your data, and know your tar-\nget hardware. To some extent, understand how much each\nstream of data matters, and who is consuming it. Under-\nstand the cost and potential value of improvements. Access\npatterns matter, as you cannot hit the cache if you’re ac-\ncessing things in a burst, then not touching them again for\na whole cycle of the application. For the consideration of the\nsecond principle, data is the type, frequency, quantity, shape,\nand probability, it’s worth thinking about the following items.\n• What is the smallest unit of memory on your target plat-\nform?6\n• When you read data, how much of it are you using?\n• How often do you need the data? Is it once, or a thou-\nsand times a frame?\n• How do you access the data? At random, or in a burst?\n• Are you always modifying the data, or just reading it?\nAre you modifying all of it?\n• Who does the data matter to, and what about it mat-\nters?\n• Find out the quality constraints of your solutions, in\nterms of bandwidth and latency.\n• What information do you have that isn’t in the data per-\nse? What is implicit?\n6On most machines in 2018, the smallest unit of memory is 64 byte\naligned lump called a cache line.\n",
      "content_length": 1292,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "Chapter 2\nRelational Databases\nIn order to lay your data out better, it’s useful to have an\nunderstanding of the methods available to convert your ex-\nisting structures into something linear.\nThe problems we\nface when applying data-oriented approaches to existing\ncode and data layouts usually stem from the complexity of\nstate inherent in data-hiding or encapsulating programming\nparadigms. These paradigms hide away internal state so you\ndon’t have to think about it, but they hinder when it comes\nto reconﬁguring data layouts. This is not because they don’t\nabstract enough to allow changes to the underlying structure\nwithout impacting the correctness of the code that uses it,\nbut instead because they have connected and given meaning\nto the structure of the data. That type of coupling can be\nhard to remove.\nIn this chapter, we go over some of the pertinent parts\nof the relational model, relational database technology, and\nnormalisation, as these are examples of converting highly\ncomplex data structures and relationships into very clean\ncollections of linear storable data entries.\nYou certainly don’t have to move your data to a database\nstyle to do data-oriented design, but there are many places\n27\n",
      "content_length": 1213,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "28\nCHAPTER 2. RELATIONAL DATABASES\nwhere you will wish you had a simple array to work with, and\nthis chapter will help you by giving you an example of how\nyou can migrate from a web of connected complex objects to\na simpler to reason about relational model of arrays.\n2.1\nComplex state\nWhen you think about the data present in most software,\nit has some qualities of complexity or interconnectedness.\nWhen it comes to game development, there are many ways\nin which the game entities interact, and many ways in which\ntheir attached resources will need to feed through diﬀerent\nstages of processes to achieve the audio, visual and some-\ntimes haptic feedback necessary to fully immerse the player.\nFor many programmers brought up on object-oriented de-\nsign, the idea of reducing the types of structure available\ndown to just simple arrays, is virtually unthinkable. It’s very\nhard to go from working with objects, classes, templates, and\nmethods on encapsulated data to a world where you only\nhave access to linear containers.\nIn A Relational Model of Data for Large Shared Data\nBanks[?], Edgar F. Codd proposed the relational model to\nhandle the current and future needs of agents interacting\nwith data. He proposed a solution to structuring data for\ninsert, update, delete, and query operations. His proposal\nclaimed to reduce the need to maintain a deep understand-\ning of how the data was laid out to use it well. His proposal\nalso claimed to reduce the likelihood of introducing internal\ninconsistencies.\nThe relational model provided a framework, and in Fur-\nther Normalization of the Data Base Relational Model.[?],\nEdgar F. Codd introduced the fundamental terms of nor-\nmalisation we use to this day in a systematic approach to\nreducing the most complex of interconnected state informa-\ntion to linear lists of unique independent tuples.\n",
      "content_length": 1845,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "2.2. THE FRAMEWORK\n29\n2.2\nWhat can provide a computational\nframework for complex data?\nDatabases store highly complex data in a structured way\nand provide a language for transforming and generating re-\nports based on that data. The language, SQL, invented in\nthe 1970’s by Donald D. Chamberlin and Raymond F. Boyce\nat IBM, provides a method by which it is possible to store\ncomputable data while also maintaining data relationships\nfollowing in the form of the relational model. Games don’t\nhave simple computable data, they have classes and objects.\nThey have guns, swords, cars, gems, daily events, textures,\nsounds, and achievements. It is very easy to conclude that\ndatabase technology doesn’t work for the object-oriented ap-\nproach game developers use.\nThe data relationships in games can be highly complex,\nit would seem at ﬁrst glance that it doesn’t neatly ﬁt into\ndatabase rows.\nA CD collection easily ﬁts in a database,\nwith your albums neatly arranged in a single table.\nBut,\nmany game objects won’t ﬁt into rows of columns. For the\nuninitiated, it can be hard to ﬁnd the right table columns\nto describe a level ﬁle. Trying to ﬁnd the right columns to\ndescribe a car in a racing game can be a puzzle.\nDo you\nneed a column for each wheel? Do you need a column for\neach collision primitive, or just a column for the collision\nmesh?\nAn obvious answer could be that game data doesn’t ﬁt\nneatly into the database way of thinking.\nHowever, that’s\nonly because we’ve not normalised the data. To show how\nyou can convert from a network model, or hierarchical model\nto what we need, we will work through these normalisation\nsteps. We’ll start with a level ﬁle as we ﬁnd out how these\ndecades-old techniques can provide a very useful insight into\nwhat game data is really doing.\nWe shall discover that everything we do is already in a\n",
      "content_length": 1837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "30\nCHAPTER 2. RELATIONAL DATABASES\ndatabase, but it wasn’t obvious to us because of how we\nstore our data. The structure of any data is a trade-oﬀbe-\ntween performance, readability, maintenance, future proof-\ning, extendibility, and reuse. For example, the most ﬂexible\ndatabase in common use is your ﬁlesystem. It has one ta-\nble with two columns. A primary key of the ﬁle path, and a\nstring for the data. This simple database system is the per-\nfect ﬁt for a completely future proof system. There’s nothing\nthat can’t be stored in a ﬁle. The more complex the tables\nget, the less future proof, and the less maintainable, but\nthe higher the performance and readability. For example, a\nﬁle has no documentation of its own, but the schema of a\ndatabase could be all that is required to understand a suﬃ-\nciently well-designed database. That’s how games don’t even\nappear to have databases. They are so complex, for the sake\nof performance, they have forgotten they are merely a data\ntransform. This sliding scale of complexity aﬀects scalability\ntoo, which is why some people have moved towards NoSQL\ndatabases, and document store types of data storage. These\nsystems are more like a ﬁlesystem where the documents are\naccessed by name, and have fewer limits on how they are\nstructured. This has been good for horizontal scalability, as\nit’s simpler to add more hardware when you don’t have to\nkeep your data consistent across multiple tables that might\nbe on diﬀerent machines. There may come a day when mem-\nory is so tightly tied to the closest physical CPU, or when\nmemory chips themselves get more processing power, or run-\nning 100 SoCs inside your desktop rig is more eﬀective than\na single monolithic CPU, that moving to document store at\nthe high-level could be beneﬁcial inside your app, but for\nnow, there do not seem to be any beneﬁts in that processing\nmodel for tasks on local hardware.\nWe’re not going to go into the details of the lowest level\nof how we utilise large data primitives such as meshes, tex-\ntures, sounds and such. For now, think of these raw assets\n(sounds, textures, vertex buﬀers, etc.) as primitives, much\nlike the integers, ﬂoating point numbers, strings and boolean\nvalues we shall be working with. We do this because the re-\n",
      "content_length": 2260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "2.2. THE FRAMEWORK\n31\nlational model calls for atomicity when working with data.\nWhat is and is not atomic has been debated without an abso-\nlute answer becoming clear, but for the intents of developing\nsoftware intended for human consumption, the granularity\ncan be rooted in considering the data from the perspective\nof human perception. There are existing APIs that present\nstrings in various ways depending on how they are used,\nfor example the diﬀerence between human-readable strings\n(usually UTF-8) and ASCII strings for debugging.\nAdding\nsounds, textures, and meshes to this seems quite natural\nonce you realise all these things are resources which if cut\ninto smaller pieces begin to lose what it is that makes them\nwhat they are. For example, half of a sentence is a lot less\nuseful than a whole one, and loses integrity by disassocia-\ntion. A slice of a sentence is clearly not reusable in any mean-\ningful way with another random slice of a diﬀerent sentence.\nEven subtitles are split along meaningful boundaries, and\nit’s this idea of meaningful boundary that gives us the our\ndeﬁnition of atomicity for software developed for humans. To\nthis end, when working with your data, when you’re normal-\nising, try to stay at the level of nouns, the nameable pieces.\nA whole song can be an atom, but so is a single tick sound\nof a clock. A whole page of text is an atom, but so is the\nplayer’s gamer-tag.\n",
      "content_length": 1411,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "32\nCHAPTER 2. RELATIONAL DATABASES\n2.3\nNormalising your data\nFigure 2.1: Visual representation of the setup script\nWe’re going to work with a level ﬁle for a game where you hunt\nfor keys to unlock doors in order to get to the exit room. The\nlevel ﬁle is a sequence of script calls which create and con-\nﬁgure a collection of diﬀerent game objects which represent\na playable level of the game, and the relationships between\nthose objects. First, we’ll assume it contains rooms (some\ntrapped, some not), with doors leading to other rooms which\ncan be locked. It will also contain a set of pickups, some let\nthe player unlock doors, some aﬀect the player’s stats (like\nhealth potions and armour), and all the rooms have lovely\ntextured meshes, as do all the pickups. One of the rooms is\nmarked as the exit, and one has a player start point.\n1\n//\ncreate\nrooms , pickups , and\nother\nthings .\n2\nMesh\nmsh_room = LoadMesh( \" roommesh \" );\n3\nMesh\nmsh_roomstart = LoadMesh( \" roommeshstart \" );\n4\nMesh\nmsh_roomtrapped = LoadMesh( \" roommeshtrapped \" );\n5\nMesh\nmsh_key = LoadMesh( \" keymesh \" );\n6\nMesh\nmsh_pot = LoadMesh( \" potionmesh \" );\n7\nMesh\nmsh_arm = LoadMesh( \" armourmesh \" );\n8\n// ...\n9\nTexture\ntex_room = LoadTexture ( \" roomtexture \" );\n10\nTexture\ntex_roomstart = LoadTexture ( \" roomtexturestart \" );\n11\nTexture\ntex_roomtrapped = LoadTexture( \" roomtexturetrapped \" );\n12\nTexture\ntex_key = LoadTexture( \" keytexture \" );\n13\nTexture\ntex_pot = LoadTexture( \" potiontexture \" );\n14\nTexture\ntex_arm = LoadTexture( \" armourtexture \" );\n",
      "content_length": 1533,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "2.3. NORMALISING YOUR DATA\n33\n15\n16\nAnim\nanim_keybob = LoadAnim( \" keybobanim \" );\n17\n// ...\n18\nPickupID\nk1 = CreatePickup ( TYPE_KEY , msh_key , tex_key ,\nTintColourCopper , anim_keybob );\n19\nPickupID\nk2 = CreatePickup ( TYPE_KEY , msh_key , tex_key ,\nTintColourSilver , anim_keybob\n);\n20\nPickupID\nk3 = CreatePickup ( TYPE_KEY , msh_key , tex_key ,\nTintColourGold , anim_keybob\n);\n21\nPickupID\np1 = CreatePickup ( TYPE_POTION , msh_pot , tex_pot ,\nTintColourGreen );\n22\nPickupID\np2 = CreatePickup ( TYPE_POTION , msh_pot , tex_pot ,\nTintColourPurple );\n23\nPickupID\na1 = CreatePickup ( TYPE_ARMOUR , msh_arm , tex_arm );\n24\n// ...\n25\nRoom r1 = CreateRoom( WorldPos (0 ,0), msh_roomstart , tex_roomstart\n);\n26\nRoom r2 = CreateRoom( WorldPos (-20,0), msh_roomtrapped ,\ntex_roomtrapped , HPDamage (10) );\n27\nRoom r3 = CreateRoom( WorldPos ( -10 ,20), msh_room , tex_room );\n28\nRoom r4 = CreateRoom( WorldPos ( -30 ,20), msh_room , tex_room );\n29\nRoom r5 = CreateRoom( WorldPos (20 ,10) , msh_roomtrapped ,\ntex_roomtrapped , HPDamage (25) );\n30\n// ...\n31\nAddDoor( r1 , r2 );\n32\nAddDoor( r1 , r3 , k1 );\n33\nSetRoomAsSpecial ( r1 , E_STARTINGROOM , WorldPos (1 ,1) );\n34\n//\n35\nAddPickup( r2 , k1 , WorldPos (-18,2));\n36\nAddDoor( r2 , r1 );\n37\nAddDoor( r2 , r4 , k2 );\n38\n// ...\n39\nAddPickup( r3 , k2 , WorldPos (-8,12));\n40\nAddPickup( r3 , p1 , WorldPos (-7,13));\n41\nAddPickup( r3 , a1 , WorldPos (-8,14));\n42\nAddDoor( r3 , r1 );\n43\nAddDoor( r3 , r2 );\n44\nAddDoor( r3 , r5 , k3 );\n45\n// ...\n46\nAddDoor( r4 , r2 );\n47\nAddPickup( r4 , k3 , WorldPos ( -28 ,14));\n48\nAddPickup( r4 , p2 , WorldPos ( -27 ,13));\n49\n// ...\n50\nSetRoomAsSpecial ( r5 , E_EXITROOM );\nListing 2.1: A setup script\nIn this setup script (Listing 2.1) we load some resources,\ncreate some pickup prototypes, build up a few rooms, add\nsome instances to the rooms, and then link things together.\nHere we also see a standard solution to the problem of things\nwhich reference each other. We create the rooms before we\nconnect them to each other because before they exist we\ncan’t. When we create entities in C++, we assume they are\nbound to memory, and the only eﬃcient way to reference\nthem is through pointers, but we cannot know where they\nexist in memory before we allocate them, and we cannot allo-\ncate them before ﬁlling them out with their data as the allo-\ncation and initialisation are bound to each other through the\n‘new’ mechanism. This means we have diﬃculty describing\nrelationships between objects before they exist and have to\n",
      "content_length": 2500,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "34\nCHAPTER 2. RELATIONAL DATABASES\nstagger the creation of content into phases of setting up and\nconnecting things together.\nTo bring this setup script into a usable database-like for-\nmat, or relational model, we will need to normalise it. When\nputting things in a relational model of any sort, it needs to\nbe in tables. In the ﬁrst step you take all the data and put\nit into a very messy, but hopefully complete, table design. In\nour case we take the form of the data from the object cre-\nation script and ﬁt it into a table. The asset loading can be\ndirectly translated into tables, as can be seen in table 2.1\nMeshes\nMeshID\nMeshName\nmsh rm\n\"roommesh\"\nmsh rmstart\n\"roommeshstart\"\nmsh rmtrap\n\"roommeshtrapped\"\nmsh key\n\"keymesh\"\nmsh pot\n\"potionmesh\"\nmsh arm\n\"armourmesh\"\nTextures\nTextureID\nTextureName\ntex rm\n\"roomtexture\"\ntex rmstart\n\"roomtexturestart\"\ntex rmtrapped\n\"roomtexturetrapped\"\ntex key\n\"keytexture\"\ntex pot\n\"potiontexture\"\ntex arm\n\"armourtexture\"\nAnimations\nAnimID\nAnimName\nanim keybob\n\"keybobanim\"\nTable 2.1: Initial tables created by converting asset load calls\nPrimed with this data, it’s now possible for us to create\nthe Pickups. We convert the calls to CreatePickup into the\ntables in table 2.2. Notice that there was a pickup which\ndid not specify a colour tint, and this means we need to use\na NULL to represent not giving details about that aspect of\nthe row. The same applies to animations. Only keys had\nanimations, so there needs to be NULL entries for all non-\nkey rows.\n",
      "content_length": 1496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "2.3. NORMALISING YOUR DATA\n35\nPickups\nPickupID\nMeshID\nTextureID\nPickupType\nColourTint\nAnim\nk1\nmsh key\ntex key\nKEY\nCopper\nanim keybob\nk2\nmsh key\ntex key\nKEY\nSilver\nanim keybob\nk3\nmsh key\ntex key\nKEY\nGold\nanim keybob\np1\nmsh pot\ntex pot\nPOTION\nGreen\nNULL\np2\nmsh pot\ntex pot\nPOTION\nPurple\nNULL\na1\nmsh arm\ntex arm\nARMOUR\nNULL\nNULL\nTable 2.2: Initial tables created by converting CreatePickup\ncalls\nOnce we have loaded the assets and have created the\npickup prototypes, we move onto creating a table for rooms.\nWe need to invent attributes as necessary using NULL ev-\nerywhere that an instance doesn’t have that attribute. We\nconvert the calls to CreateRoom, AddDoor, SetRoomAsSpe-\ncial, and AddPickup, to columns in the Rooms table. See\ntable 2.3 for one way to build up a table that represents all\nthose setup function calls.\nRooms\nRoomID\nMeshID\nTextureID\nWorldPos\nPickups\n...\nr1\nmsh rmstart\ntex rmstart\n0, 0\nNULL\n...\nr2\nmsh rmtrap\ntex rmtrap\n-20,10\nk1\n...\nr3\nmsh rm\ntex rm\n-10,20\nk2,p1,a1\n...\nr4\nmsh rm\ntex rm\n-30,20\nk3,p2\n...\nr5\nmsh rmtrap\ntex rmtrap\n20,10\nNULL\n...\n...\nDoorsTo\nLocked\nIsStart\nIsEnd\n...\nNULL\nr2,r3\nr3 with k1\ntrue WorldPos(1,1)\nfalse\n...\n10HP\nr1,r4\nr4 with k2\nfalse\nfalse\n...\nNULL\nr1,r2,r5\nr5 with k3\nfalse\nfalse\n...\nNULL\nr2\nfalse\nfalse\n...\n25HP\nNULL\nfalse\ntrue\nTable 2.3: Initial table created by converting CreateRoom\nand other calls.\nOnce we have taken the construction script and gener-\nated these ﬁrst tables, we ﬁnd the tables contain a lot of\nNULLs. The NULLs in the rows replace the optional content\nof the objects. If an object instance doesn’t have a certain\nattribute then we replace those features with NULLs. There\nare also elements which contain more than one item of data.\nHaving multiple doors per room is tricky to handle in this\n",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "36\nCHAPTER 2. RELATIONAL DATABASES\ntable.\nHow would you ﬁgure out what doors it had?\nThe\nsame goes for whether the door is locked, and whether there\nare any pickups. The ﬁrst stage in normalising is going to\nbe reducing the number of elements in each cell to 1, and\nincreasing it to 1 where it’s currently NULL.\n2.4\nNormalisation\nBack when SQL was ﬁrst created there were only three well-\ndeﬁned stages of data normalisation. There are many more\nnow, including six numbered normal forms. To get the most\nout of a database, it is important to know most of them, or\nat least get a feel for why they exist. They teach you about\ndata dependency and can hint at reinterpretations of your\ndata layout. For game structures, BCNF (Boyce-Codd nor-\nmal form is explained later) is probably as far as you nor-\nmally would need to take your methodical process. Beyond\nthat, you might wish to normalise your data for hot/cold ac-\ncess patterns, but that kind of normalisation is not part of\nthe standard literature on database normalisation. If you’re\ninterested in more than this book covers on the subject, a\nvery good read, and one which introduces the phrase “The\nkey, the whole key, and nothing but the key.” is the article\nA Simple Guide to Five Normal Forms in Relational Database\nTheory[?] by William Kent.\nIf a table is in ﬁrst normal form, then every cell contains\none and only one atomic value. That is, no arrays of values,\nand no NULL entries. First normal form also requires every\nrow be distinct. For those unaware of what a primary key is,\nwe shall discuss that ﬁrst.\n",
      "content_length": 1571,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "2.4. NORMALISATION\n37\n2.4.1\nPrimary keys\nAll tables are made up of rows and columns. In a database,\neach row must be unique.\nThis constraint has important\nconsequences. When you have normalised your data, it be-\ncomes clear why duplicate rows don’t make sense, but for\nnow, from a computer programming point of view, consider\ntables to be more like sets, where the whole row is the set\nvalue. This is very close to reality, as sets are also not or-\ndered, and a database table is not ordered either. There is\nalways some diﬀerentiation between rows, even if a database\nmanagement system (DBMS) has to rely on hidden row ID\nvalues. It is better to not rely on this as databases work more\neﬃciently when the way in which they are used matches\ntheir design. All tables need a key. The key is often used to\norder the sorting of the table in physical media, to help op-\ntimise queries. For this reason, the key needs to be unique,\nbut as small as possible. You can think of the key as the\nkey in a map or dictionary. Because of the uniqueness rule,\nevery table has an implicit key because the table can use the\ncombination of all the columns at once to identify each row\nuniquely. That is, the key, or the unique lookup, which is\nthe primary key for a table, can be deﬁned as the totality of\nthe whole row. If the row is unique, then the primary key is\nunique. Normally, we try to avoid using the whole row as the\nprimary key, but sometimes, it’s actually our only choice. We\nwill come across examples of that later.\nFor example, in the mesh table, the combination of\nmeshID and ﬁlename is guaranteed to be unique.\nHow-\never, currently it’s only guaranteed to be unique because\nwe have presumed that the meshID is unique. If it was the\nsame mesh, loaded from the same ﬁle, it could still have a\ndiﬀerent meshID. The same can be said for the textureID\nand ﬁlename in the textures table. From the table 2.2 it’s\npossible to see how we could use the type, mesh, texture,\ntint and animation to uniquely deﬁne each Pickup prototype.\nNow consider rooms.\nIf you use all the columns other\n",
      "content_length": 2076,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "38\nCHAPTER 2. RELATIONAL DATABASES\nthan the RoomID of the room table, you will ﬁnd the combi-\nnation can be used to uniquely deﬁne the room. If you con-\nsider an alternative, where a row had the same combination\nof values making up the room, it would in fact be describing\nthe same room. From this, it can be claimed that the Roo-\nmID is being used as an alias for the rest of the data. We have\nstuck the RoomID in the table, but where did it come from?\nTo start with, it came from the setup script. The script had\na RoomID, but we didn’t need it at that stage. We needed it\nfor the destination of the doors. In another situation, where\nnothing connected logically to the room, we would not need\na RoomID as we would not need an alias to it.\nA primary key must be unique. RoomID is an example of\na primary key because it uniquely describes the room. It is\nan alias in this sense as it contains no data in and of itself,\nbut merely acts as a handle. In some cases the primary key\nis information too, which again, we will meet later.\nAs a bit of an aside, the idea that a row in a database\nis also the key can be a core concept worth spending time\nthinking about. If a database table is a set, when you in-\nsert a record, you’re actually just asking that one particular\ncombination of data is being recorded as existing. It is as if\na database table is a very sparse set from an extremely large\ndomain of possible values. This can be useful because you\nmay notice that under some circumstances, the set of possi-\nble values isn’t very large, and your table can be more easily\ndeﬁned as a bit set. As an example, consider a table which\nlists the players in an MMO that are online right now. For\nan MMO that shards its servers, there can be limits in the\nearly thousands for the number of unique players on each\nserver. In that case, it may be easier to store the currently\nonline players as a bit set. If there are at most 10,000 players\nonline, and only 1000 players online at any one time, then\nthe bitset representation would take up 1.25kb of memory,\nwhereas storing the online players as a list of IDs, would re-\nquire at least 2kb of data if their IDs were shrunk to shorts,\nor 4kb if they had 32bit IDs to keep them unique across\n",
      "content_length": 2234,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "2.4. NORMALISATION\n39\nmultiple servers. The other beneﬁt in this case is the perfor-\nmance of queries into the data. To quickly access the ID in\nthe list, you need it to remain sorted. The best case then is\nO(log n). In the bitset variant, it’s O(1).\nGoing back to the asset table, an important and useful\ndetail when we talk about the meshID and mesh ﬁlename is\nthat even though there could be two diﬀerent meshIDs point-\ning at the same ﬁle, most programmers would intuitively un-\nderstand that a single meshID was unlikely to point at two\ndiﬀerent mesh ﬁles.\nBecause of this asymmetry, you can\ndeduce, the column that seems more likely to be unique will\nalso be the column you can use as the primary key. We’ll\nchoose the meshID as it is easier to manipulate and is un-\nlikely to have more than one meaning or usage, but remem-\nber, we could have chosen the ﬁlename and gone without the\nmeshID altogether.\nIf we settle on TextureID, PickupID, and RoomID as the\nprimary keys for those tables, we can then look at continuing\non to ﬁrst normal form. We’re using t1, m2, r3, etc. to show\ntypesafe ID values, but in reality, these can all be simple\nintegers.\nThe idea here is to remain readable, but it also\nshows that each type can have unique IDs for that type, but\nhave common IDs with another. For example, a room may\nhave an integer ID value of 0, but so may a texture. It can\nbe beneﬁcial to have IDs which are unique across types, as\nthat can help debugging, and using the top few bits in that\ncase can be helpful. If you’re unlikely to have more than a\nmillion entities per class of entity, then you have enough bits\nto handle over a thousand distinct classes.\n2.4.2\n1st Normal Form\nFirst normal form can be described as making sure the ta-\nbles are not sparse. We require that there be no NULL point-\ners and that there be no arrays of data in each element of\ndata. This can be performed as a process of moving the re-\n",
      "content_length": 1926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "40\nCHAPTER 2. RELATIONAL DATABASES\npeats and all the optional content to other tables. Anywhere\nthere is a NULL, it implies optional content. Our ﬁrst ﬁx is\ngoing to be the Pickups table, it has optional ColourTint and\nAnimation elements. We invent a new table PickupTint, and\nuse the primary key of the Pickup as the primary key of the\nnew table. We also invent a new table PickupAnim. Table\n2.4 shows the result of the transformation, and note we no\nlonger have any NULL entries.\nPickups\nPickupID\nMeshID\nTextureID\nPickupType\nk1\nmsh key\ntex key\nKEY\nk2\nmsh key\ntex key\nKEY\nk3\nmsh key\ntex key\nKEY\np1\nmsh mpot\ntex pot\nPOTION\np2\nmsh mpot\ntex pot\nPOTION\na1\nmsh marm\ntex arm\nARMOUR\nPickupTints\nPickupID\nColourTint\nk1\nCopper\nk2\nSilver\nk3\nGold\np1\nGreen\np2\nPurple\nPickupAnims\nPickupID\nAnim\nk1\nanim keybob\nk2\nanim keybob\nk3\nanim keybob\nTable 2.4: Pickups in 1NF\nTwo things become evident at this point, ﬁrstly that nor-\nmalisation appears to create more tables and fewer columns\nin each table, secondly that there are only rows for things\nwhich matter. The former is worrisome, as it means more\nmemory usage. The latter is interesting as when using an\nobject-oriented approach, we allow objects to optionally have\nattributes. Optional attributes cause us to check they are\nnot NULL before continuing. If we store data like this, then\nwe know everything is not NULL. Moving away from having\nto do a null check at all will make your code more concise,\n",
      "content_length": 1441,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "2.4. NORMALISATION\n41\nand you have less state to consider when trying to reason\nabout your systems.\nLet’s move onto the Rooms table. In there we saw single\nelements that contained multiple atomic values. We need\nto remove all elements from this table that do not conform\nto the rules of ﬁrst normal form. First, we remove reference\nto the pickups, as they had various quantities of elements,\nfrom none to many. Then we must consider the traps, as\neven though there was only ever one trap, there wasn’t al-\nways a trap. Finally, we must strip out the doors, as even\nthough every room has a door, they often had more than\none. Remember that the rule is one and only one entry in\nevery meeting of row and column. In table 2.5 it shows how\nwe only keep columns that are in a one to one relationship\nwith the RoomID.\nRooms\nRoomID\nMeshID\nTextureID\nWorldPos\nIsStart\nIsExit\nr1\nmsh rmstart\ntex rmstart\n0,0\ntrue\nfalse\nr2\nmsh rmtrap\ntex rmtrap\n-20,0\nfalse\nfalse\nr3\nmsh rm\ntex rm\n-10,20\nfalse\nfalse\nr4\nmsh rm\ntex rm\n-30,20\nfalse\nfalse\nr5\nmsh rmtrap\ntex rmtrap\n20,10\nfalse\ntrue\nTable 2.5: Rooms table now in 1NF\nNow we will make new tables for Pickups, Doors, and\nTraps. In table 2.6 we see many decisions made to satisfy\nthe ﬁrst normal form. We have split out the array like ele-\nments into separate rows. Note the use of multiple rows to\nspecify the numerous pickups all in the same room. We see\nthat doors now need two tables. The ﬁrst table to identify\nwhere the doors are, and where they lead. The second ta-\nble seems to do the same, but doesn’t cover all doors, only\nthe ones that are locked. What’s actually happening here is\na need to identify doors by their primary key in the locked\ndoors table. If you look at the Doors table, you can immedi-\nately tell that neither column is a candidate for the primary\nkey, as neither contain only unique values. What is unique\nthough is the combination of values, so the primary key is\nmade up of both columns. In the table LockedDoors, From-\n",
      "content_length": 1980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "42\nCHAPTER 2. RELATIONAL DATABASES\nRoom and ToRoom are being used as a lookup into the Doors\ntable. This is often called a foreign key, meaning that there\nexists a table for which these columns directly map to that\ntable’s primary key. In this case, the primary key is made up\nof two columns, so the LockedDoors table has a large foreign\nkey and a small bit of extra detail about that entry in the\nforeign table.\nPickupInstances\nRoomID\nPickupID\nr2\nk1\nr3\nk2\nr3\na1\nr3\np1\nr4\nk3\nr4\np2\nDoors\nFromRoom\nToRoom\nr1\nr2\nr1\nr3\nr2\nr1\nr2\nr4\nr3\nr1\nr3\nr2\nr3\nr5\nr4\nr2\nLockedDoors\nFromRoom\nToRoom\nLockedWith\nr1\nr3\nk1\nr2\nr4\nk2\nr3\nr5\nk3\nTraps\nRoomID\nTrapped\nr2\n10hp\nr5\n25hp\nTable 2.6: Additional tables to support 1NF rooms\nLaying out the data in this way takes less space in larger\nprojects as the number of NULL entries or arrays would have\nonly increased with increased complexity of the level ﬁle. By\nlaying out the data this way, we can add new features with-\nout having to revisit the original objects. For example, if we\n",
      "content_length": 1008,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "2.4. NORMALISATION\n43\nwanted to add monsters, normally we would not only have\nto add a new object for the monsters, but also add them to\nthe room objects. In this format, all we need to do is add a\nnew table such as in table 2.7.\nMonsters\nMonsterID\nAttack\nHitPoints\nStartRoom\nM1\n2\n5\nr3\nM2\n2\n5\nr4\nTable 2.7: Adding monsters\nAnd now we have information about the monster and\nwhat room it starts in without touching any of the original\nlevel data.\n2.4.3\n2nd Normal Form\nSecond normal form is about trying to pull out columns that\ndon’t depend on only a part of the primary key. This can be\ncaused by having a table that requires a compound primary\nkey, and some attributes of the row only being dependent\non part of that compound key. An example might be where\nyou have weapons deﬁned by quality and type, and the table\nlooks like that in table 2.8, what you can see is that the\nprimary key must be compound, as there are no columns\nwith unique values here.\nWeapons\nWeaponType\nWeaponQuality\nWeaponDamage\nWeaponDamageType\nSword\nRusty\n2d4\nSlashing\nSword\nAverage\n2d6\nSlashing\nSword\nMasterwork\n2d8\nSlashing\nLance\nAverage\n2d6\nPiercing\nLance\nMasterwork\n3d6\nPiercing\nHammer\nRusty\n2d4\nCrushing\nHammer\nAverage\n2d4+4\nCrushing\nTable 2.8: Weapons in 1NF\n",
      "content_length": 1239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "44\nCHAPTER 2. RELATIONAL DATABASES\nIt makes sense for us looking at the table that the primary\nkey should be the compound of WeaponType and Weapon-\nQuality, as it’s a fairly obvious move for us to want to look\nup damage amount and damage type values based on what\nweapon we’re using.\nIt’s also possible to notice that the\nDamageType does not depend on the WeaponQuality, and\nin fact only depends on the WeaponType.\nThat’s what we\nmean about depending on part of the key. Even though each\nweapon is deﬁned in 1NF, the type of damage being dealt\ncurrently relies on too little of the primary key to allow this\ntable to remain in 2NF. We split the table out in table 2.9\nto remove the column that only relies on WeaponType. If we\nfound a weapon that changed DamageType based on quality,\nthen we would put the table back the way it was. An example\nmight be the badly damaged morningstar, which no longer\ndoes piercing damage, but only bludgeons.\nWeapons\nWeaponType\nWeaponQuality\nWeaponDamage\nSword\nRusty\n2d4\nSword\nAverage\n2d6\nSword\nMasterwork\n2d8\nLance\nAverage\n2d6\nLance\nMasterwork\n3d6\nHammer\nRusty\n2d4\nHammer\nAverage\n2d4+4\nWeaponDamageTypes\nWeaponType\nWeaponDamageType\nSword\nSlashing\nLance\nPiercing\nHammer\nCrushing\nTable 2.9: Weapons in 2NF\nWhen considering second normal form for our level data,\nit’s worth understanding some shortcuts we made in mov-\ning to ﬁrst normal form. Firstly, we didn’t necessarily need\nto move to having a PickupID, but instead could have refer-\nenced the pickup prototype by PickupType and TintColour,\nbut that was cumbersome, and would have introduced a\nNULL as a requirement as the armour doesn’t have a tint.\n",
      "content_length": 1638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "2.4. NORMALISATION\n45\nTable 2.10 shows how this may have looked, but the compli-\ncations with making this connect to the rooms was the de-\nciding factor for introducing a PickupID. Without the pickup\nID, the only way to put the pickups in rooms was to have\ntwo tables. One table for pickups with tints, and another\nfor pickups without tints. This is not absurd, but it doesn’t\nseem clean in this particular situation. There will be cases\nwhere this would be the right approach.\nPickups\nMeshID\nTextureID\nPickupType\nColourTint\nmkey\ntkey\nKEY\nCopper\nmkey\ntkey\nKEY\nSilver\nmkey\ntkey\nKEY\nGold\nmpot\ntpot\nPOTION\nGreen\nmpot\ntpot\nPOTION\nPurple\nmarm\ntarm\nARMOUR\nNULL\nNormalising to 1NF:\nPickups 1NF\nPickupType\nMeshID\nTextureID\nKEY\nmkey\ntkey\nPOTION\nmpot\ntpot\nARMOUR\nmarm\ntarm\nTintedPickups 1NF\nPickupType\nColourTint\nKEY\nCopper\nKEY\nSilver\nKEY\nGold\nPOTION\nGreen\nPOTION\nPurple\nTable 2.10: An alternative 0NF and 1NF for Pickups\nIf we now revisit the Pickup table from before, with the\nknowledge that the PickupID is an alias for the combina-\ntion of PickupType and ColourTint, then we can apply the\nsame transform we see when moving to 1NF in the alterna-\ntive form. That is, of moving MeshID and TextureID to their\nown table, and depending only on PickupType, not the com-\npound key of PickupType and ColourTint.\nIn table 2.11, the assets elements now rely on the whole\n",
      "content_length": 1355,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "46\nCHAPTER 2. RELATIONAL DATABASES\nof their compound key, not just part of it.\nPickups\nPickupID\nPickupType\nk1\nKEY\nk2\nKEY\nk3\nKEY\np1\nPOTION\np2\nPOTION\na1\nARMOUR\nPickupTints\nPickupID\nColourTint\nk1\nCopper\nk2\nSilver\nk3\nGold\np1\nGreen\np2\nPurple\nPickupAssets\nPickupType\nMeshID\nTextureID\nKEY\nmsh key\ntex key\nPOTION\nmsh pot\ntex pot\nARMOUR\nmsh arm\ntex arm\nPickupAnims\nPickupType\nAnimID\nKEY\nkey bob\nTable 2.11: Pickups in 2NF\nWe can’t apply the same normalisation of table data to\nthe Room table. The Room table’s RoomID is an alias for\nthe whole row, possibly, or just the WorldPos, but in both\ncases, it’s possible to see a correlation between the MeshID,\nTextureID, and the value of IsStart. The problem is that it\nalso relies on the existence of entries in an external table.\nIf we take the table as it is, the MeshID and TextureID do\nnot directly rely on anything other than the RoomID in this\nform.\n",
      "content_length": 892,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "2.4. NORMALISATION\n47\n2.4.4\n3rd Normal Form\nWhen considering further normalisation, we ﬁrst have to re-\nmove any transitive dependencies. By this we mean any de-\npendencies on the primary key only via another column in\nthe row. We can do a quick scan of the current tables and\nsee all resources references refer to pairs of MeshID and Tex-\ntureID values.\nAnything that uses a MeshID will use the\nmatching TextureID. This means we can pull out one or the\nother from all the tables that use them, and look them up\nvia a table of pairs. We shall arbitrarily choose to use the\nTextureID as the main lookup, and slim down to one table\nfor meshes and textures.\nTexturesAndMeshes\nTextureID\nTextureName\nMeshName\ntex room\n\"roomtexture\"\n\"roommesh\"\ntex roomstart\n\"roomtexturestart\"\n\"roommeshstart\"\ntex roomtrap\n\"roomtexturetrapped\"\n\"roommeshtrapped\"\ntex key\n\"keytexture\"\n\"keymesh\"\ntex pot\n\"potiontexture\"\n\"potionmesh\"\ntex arm\n\"armourtexture\"\n\"armourmesh\"\nTable 2.12: Assets in 3NF\n2.4.5\nBoyce-Codd Normal Form\nThe assets used for a room are based on whether it is\ntrapped, or it’s the starting room. This is a functional de-\npendency, not a direct one, so we have to introduce a new\ncolumn to describe that aspect, and it’s going to require\ngenerating intermediate data to drive the value query, but\nit makes real the lack of direct link between the room and\nthe assets.\nThe rooms can be trapped, and can be start-\ning rooms, and the assets connected to the room depend on\nthose attributes, not the room itself. This is why Boyce-Codd\nNormal Form, or BCNF, can be thought of as the functionally\ndependent normalisation stage.\n",
      "content_length": 1615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "48\nCHAPTER 2. RELATIONAL DATABASES\nRooms\nRoomID\nWorldPos\nIsStart\nIsExit\nr1\n0,0\ntrue\nfalse\nr2\n-20,10\nfalse\nfalse\nr3\n-10,20\nfalse\nfalse\nr4\n-30,20\nfalse\nfalse\nr5\n20,10\nfalse\ntrue\nRooms\nIsStart\nHasTrap\nTextureID\ntrue\nfalse\ntex rmstart\nfalse\nfalse\ntex rm\nfalse\ntrue\ntex rmtrap\nTable 2.13: Rooms table now in BCNF\n2.4.6\nDomain Key / Knowledge\nDomain key normal form is normally thought of as the last\nnormal form, but for developing eﬃcient data structures, it’s\none of the things best studied early and often. The term do-\nmain knowledge is preferable when writing code as it makes\nmore immediate sense and encourages use outside of keys\nand tables. Domain knowledge is the idea that data depends\non other data, but only given information about the domain\nin which it resides. Domain knowledge can be as simple as\nawareness of a colloquialism for something, such as know-\ning that a certain number of degrees Celsius or Fahrenheit is\nhot, or whether some SI unit relates to a man-made concept\nsuch as 100m/s being rather quick.\nAn example of where domain knowledge can help with\ncatching issues can be with putting human interpreta-\ntions of values into asserts. Consider an assert for catching\nphysics systems blowups. What is a valid expected range of\nvalues for acceleration? Multiply it by ten, and you have a\ncheck for when everything goes a bit crazy.\nSome applications avoid the traditional inaccurate and\nerratic countdown timer, and resort to human-readable\nforms such as in a few minutes or time to grab a coﬀee,\nhowever domain knowledge isn’t just about presenting a hu-\n",
      "content_length": 1577,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "2.4. NORMALISATION\n49\nman interpretation of data. For example things such as the\nspeed of sound, of light, speed limits and average speed of\ntraﬃc on a given road network, psychoacoustic properties,\nthe boiling point of water, and how long it takes a human\nto react to any given visual input.\nAll these facts may be\nuseful in some way, but can only be put into an application\nif the programmer adds it speciﬁcally as procedural domain\nknowledge or as an attribute of a speciﬁc instance.\nLooking at our level data, one thing we can guess at is\nthe asset ﬁlenames based on the basic name. The textures\nand meshes share a common format, so moving away from\nstoring the full ﬁlenames could give us a Domain Knowledge\nnormalised form.\nAssetLookupTable\nAssetID\nStubbedName\nast room\n\"room%s\"\nast roomstart\n\"room%sstart\"\nast roomtrap\n\"room%strapped\"\nast key\n\"key%s\"\nast pot\n\"potion%s\"\nast arm\n\"armour%s\"\nTable 2.14: Assets in DKNF\nDomain knowledge is useful because it allows us to lose\nsome otherwise unnecessarily stored data. It is a compiler’s\njob to analyse the produced output of code (the abstract syn-\ntax tree) to then provide itself with data upon which it can\ninfer and use its domain knowledge about what operations\ncan be omitted, reordered, or transformed to produce faster\nor cheaper assembly.\nIt’s our job to do the same for ele-\nments the compiler can’t know about, such as the chance\nthat someone in the middle of a ﬁght is going to be able to\nhear a coin drop in another room.\nDomain knowledge is what leads to inventions such as\nJPEG and MP3. Thinking about what is possible, what is\npossible to perceive, and what can possibly be aﬀected by\nuser actions, can reduce the amount of work done by an\napplication, and can reduce its complexity. When you jump\n",
      "content_length": 1767,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "50\nCHAPTER 2. RELATIONAL DATABASES\nin a game with physics, we don’t move the world down by\nfractions of a nanometre to represent the opposite reaction\ncaused by the forces applied.\n2.4.7\nReﬂections\nWhat we see here as we normalise our data is a tendency\nto split data by dependency. Looking at many third party\nengines and APIs, you can see some parallels with the re-\nsults of these normalisations. It’s unlikely that the people\ninvolved in the design and evolution of these engines took\ntheir data and applied database normalisation techniques,\nbut sometimes the separations between object and compo-\nnents of objects can be obvious enough that you don’t need a\nformal technique in order to realise some positive structural\nchanges.\nIn some games, the entity object is not just an object that\ncan be anything, but is instead a speciﬁc subset of the types\nof entity involved in the game.\nFor example, in one game\nthere might be a class for the player character, and one for\neach major type of enemy character, and another for vehi-\ncles. The player may have diﬀerent attributes to other enti-\nties, such as lacking AI controls, or having player controls,\nor having regenerating health, or having ammo. This object-\noriented approach puts a line, invisible to the user, but in-\ntrusive to the developer, between classes of object and their\ninstances. It is intrusive because when classes touch, they\nhave to adapt to each other. When they don’t reside in the\nsame hierarchy, they have to work through abstraction lay-\ners to message each other. The amount of code required to\nbridge these gaps can be small, but they always introduce\ncomplexity.\nWhen developing software, this usually manifests as time\nspent writing out templated code that can operate on multi-\nple classes rather than refactoring the classes involved into\nmore discrete components. This could be considered wasted\n",
      "content_length": 1883,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "2.5. OPERATIONS\n51\ntime as the likelihood of other operations needing to operate\non all the objects is greater than zero, and the eﬀort to refac-\ntor into components is usually similar to the eﬀort to create\na working templated operation.\nWithout classes to deﬁne boundaries, the table-based ap-\nproach levels the playing ﬁeld for data to be manipulated\ntogether. In all cases on our journey through normalising\nthe level data, we have made it so changes to the design re-\nquire fewer changes to the data, and made it so data changes\nare less likely to cause the state to become inconsistent. In\nmany cases, it would seem we have added complexity when\nit wasn’t necessary, and that’s up to experimentation and\nexperience to help you decide how far to go.\n2.5\nOperations\nWhen you use objects, you call methods on them, so how\ndo you unlock a door in this table-based approach? Actions\nare always going to be insert, delete, or updates. These were\nclearly speciﬁed in Edgar F. Codd’s works, and they are all\nyou need to manipulate a relational model.\nIn a real database, ﬁnding what mesh to load, or whether\na door is locked would normally require a join between ta-\nbles. A real database would also attempt to optimise the join\nby changing the sequence of operations until it had made the\nsmallest possible expected workload. We can do better than\nthat because we can take absolute charge of how we look at\nand request data from our tables. To ﬁnd out if a door is\nlocked, we don’t need to join tables, we know we can look up\ninto the locked doors table directly. Just because the data\nis laid out like a database, doesn’t mean we have to use a\nquery language to access it.\nWhen it comes to operations that change state, it’s best\nto try to stick to the kind of operation you would normally\n",
      "content_length": 1790,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "52\nCHAPTER 2. RELATIONAL DATABASES\nﬁnd in a DBMS, as doing unexpected operations brings un-\nexpected state complexity. For example, imagine you have a\ntable of doors that are open, and a table of doors that are\nclosed. Moving a door from one table might be considered\nwasteful, so you may consider changing the representation\nto a single table, but with all closed doors at one end, and\nall open at the other.\nBy having both tables represented\nas a single table, and having the isClosed attribute deﬁned\nimplicitly by a cut-oﬀpoint in the array, such as in listing\n2.2, leads to the table being somewhat ordered. This type\nof memory optimisation comes at a price. Introducing order\ninto a table makes the whole table inherently less parallelis-\nable to operations, so beware the additional complexity in-\ntroduced by making changes like this, and document them\nwell.\n1\ntypedef\nstd ::pair <int ,int > Door;\n2\ntypedef\nstd :: vector <Door > DoorVector\n3\nDoorVector\ngDoors;\n4\nint\ngDoors_firstClosedDoor = 0;\n5\n6\nAddClosedDoor ( Door d ) {\n7\ngDoors.push_back ();\n8\n}\n9\nAddOpenDoor( Door d ) {\n10\ngDoors.insert( gDoors.begin () + gDoors_firstClosedDoor , d );\n11\ngDoors_firstClosedDoor\n+= 1;\n12\n}\nListing 2.2: Abusing the ordered nature of a vector\nUnlocking a door can be a delete. A door is locked because\nthere is an entry in the LockedDoors table that matches the\nDoor you are interested in. Unlocking a door is a delete if\ndoor matches, and you have the right key.\nThe player inventory would be a table with just PickupIDs.\nThis is the idea that ”the primary key is also the data” men-\ntioned much earlier. If the player enters a room and picks up\na Pickup, then the entry matching the room is deleted while\nthe inventory is updated to include the new PickupID.\nDatabases have the concept of triggers, whereupon oper-\nations on a table can cause cascades of further operations.\nIn the case of picking up a key, we would want a trigger on\ninsert into the inventory that joined the new PickupID with\n",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "2.6. SUMMING UP\n53\nthe LockedDoors table. For each matching row there, delete\nit, and now the door is unlocked.\n2.6\nSumming up\nAt this point we can see it is perfectly reasonable to store any\nhighly complex data structures in a database format, even\ngame data with its high interconnectedness and rapid design\nchanging criteria.\nGames have lots of state, and the relational model pro-\nvides a strong structure to hold both static information, and\nmutable state.\nThe strong structure leads to similar so-\nlutions to similar problems in practise, and similar solu-\ntions have similar processing.\nYou can expect algorithms\nand techniques to be more reusable while working with ta-\nbles, as the data layout is less surprising.\nIf you’re looking for a way to convert your interconnected\ncomplicated objects into a simpler ﬂatter memory layout, you\ncould do worse than approach the conversion with normali-\nsation in mind.\nA database approach to data storage has some other use-\nful side-eﬀects. It provides an easier route to allowing old\nexecutables to run oﬀnew data, and it allows new executa-\nbles to more easily run with old data. This can be vital when\nworking with other people who might need to run an earlier\nor later version. We saw that sometimes adding new features\nrequired nothing more than adding a new table, or a new col-\numn to an existing table. That’s a non-intrusive modiﬁcation\nif you are using a database style of storage, but a signiﬁcant\nchange if you’re adding a new member to a class.\n",
      "content_length": 1507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "54\nCHAPTER 2. RELATIONAL DATABASES\n2.7\nStream Processing\nNow we realise that all the game data and game runtime can\nbe implemented in a database-like approach, we can also\nsee that all game data can be implemented as streams. Our\npersistent storage is a database, our runtime data is in the\nsame format as it was on disk, what do we beneﬁt from this?\nDatabases can be thought of as collections of rows, or col-\nlections of columns, but it’s also possible to think about the\ntables as sets. The set is the set of all possible permutations\nof the attributes.\nFor most applications, using a bitset to represent a ta-\nble would be wasteful, as the set size quickly grows out of\nscope of any hardware, but it can be interesting to note what\nthis means from a processing point of view. Processing a set,\ntransforming it into another set, can be thought of as travers-\ning the set and producing the output set, but the interesting\nattribute of a set is that it is unordered. An unordered list\ncan be trivially parallel processed. There are massive ben-\neﬁts to be had by taking advantage of this trivialisation of\nparallelism wherever possible, and we normally cannot get\nnear this because of the data layout of the object-oriented\napproaches.\nComing at this from another angle, graphics cards ven-\ndors have been pushing in this direction for many years, and\nwe now need to think in this way for game logic too.\nWe\ncan process lots of data quickly as long as we utilise stream\nprocessing or set processing as much as possible and use\nrandom access processing as little as possible. Stream pro-\ncessing in this case means to process data without writing to\nvariables external to the process. This means not allowing\nthings like global accumulators, or accessing global mem-\nory not set as a source for the process. This ensures the\nprocesses or transforms are trivially parallelisable.\nWhen you prepare a primitive render for a graphics card,\nyou set up constants such as the transform matrix, the tex-\n",
      "content_length": 1996,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "2.8. WHY DOES DATABASE TECHNOLOGY MATTER?\n55\nture binding, any lighting values, or which shader you want\nto run. When you come to run the shader, each vertex and\npixel may have its own scratchpad of local variables, but\nthey never write to globals or refer to a global scratchpad.\nThe concept of shared memory in general purpose GPU code,\nsuch as CUDA and OpenCL, allows the use of a kind of man-\naged cache. None of the GPGPU techniques oﬀer access to\nglobal memory, and thus maintain a clear separation of do-\nmains and continue to guarantee no side-eﬀects caused by\nany kernels being run outside of their own sandboxed shared\nmemory. By enforcing this lack of side-eﬀects, we can guar-\nantee trivial parallelism because the order of operations are\nassured to be irrelevant. If a shader was allowed to write to\nglobals, there would be locking, or it would become an inher-\nently serial operation. Neither of these are good for massive\ncore count devices like graphics cards, so that has been a\nself imposed limit and an important factor in their design.\nAdding shared memory to the mix starts to inject some po-\ntential locking into the process, and hence is explicitly only\nused when writing compute shaders.\nDoing all processing this way, without globals / global\nscratchpads, gives you the rigidity of intention to highly par-\nallelise your processing and make it easier to think about\nthe system, inspect it, debug it, and extend it or interrupt it\nto hook in new features. If you know the order doesn’t mat-\nter, it’s very easy to rerun any tests or transforms that have\ncaused bad state.\n2.8\nWhy does database technology mat-\nter?\nAs mentioned at the start of the chapter, the relational model\nis currently a very good ﬁt for developing non-sparse data\nlayouts that are manipulable with very little complicated\nstate management required once the tables have been de-\n",
      "content_length": 1875,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "56\nCHAPTER 2. RELATIONAL DATABASES\nsigned. However, the only constant is change. That which\nis current, regularly becomes the old way, and for widely\nscaled systems, the relational model no longer provides all\nfeatures required.\nAfter the emergence of NoSQL solutions for handling even\nlarger workloads, and various large companies’ work on cre-\nating solutions to distribute computing power, there have\nbeen advances in techniques to process enormous data-sets.\nThere have been advances in how to keep databases current,\ndistributed, and consistent (within tolerance).\nDatabases\nnow regularly include NULL entries, to the point where there\nare far more NULL entries than there are values, and these\nhighly sparse databases need a diﬀerent solution for pro-\ncessing.\nMany large calculations and processes now run\nvia a technique called map-reduce, and distributing work-\nloads has become commonplace enough that people have to\nbe reminded they don’t always need a cluster to add up some\nnumbers.\nWhat’s become clear over the last decade is that most of\nthe high-level data processing techniques which are prov-\ning to be useful are a combination of hardware-aware data\nmanipulation layers being used by functional programming\nstyle high-level algorithms. As the hardware in your PC be-\ncomes more and more like the internet itself, these tech-\nniques will begin to dominate on personal hardware, whether\nit be personal computers, phones, or whatever the next gen-\neration brings. Data-oriented design was inspired by a real-\nisation that the hardware had moved on to the point where\nthe techniques we used to use to defend against latency from\nCPU to hard drive, now apply to memory. In the future, if we\nraise processing power by the utilisation of hoards of iso-\nlated unreliable computation units, then the techniques for\ndistributing computing across servers that we’re developing\nin this era, will apply to the desktops of the next.\n",
      "content_length": 1938,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "Chapter 3\nExistential Processing\nIf you saw there weren’t any apples in stock, would you still\nhaggle over their price?\nExistential processing attempts to provide a way to re-\nmove unnecessary querying about whether or not to process\nyour data. In most software, there are checks for NULL and\nqueries to make sure the objects are in a valid state before\nwork is started. What if you could always guarantee your\npointers were not null? What if you were able to trust that\nyour objects were in a valid state, and should always be pro-\ncessed?\nIn this chapter, a dynamic runtime polymorphism tech-\nnique is shown that can work with the data-oriented de-\nsign methodology. It is not the only way to implement data-\noriented design friendly runtime polymorphism, but was the\nﬁrst solution discovered by the author, and ﬁts well with\nother game development technologies, such as components\nand compute shaders.\n57\n",
      "content_length": 908,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "58\nCHAPTER 3. EXISTENTIAL PROCESSING\n3.1\nComplexity\nWhen studying software engineering you may ﬁnd references\nto cyclomatic complexity or conditional complexity. This is\na complexity metric providing a numeric representation of\nthe complexity of programs and is used in analysing large-\nscale software projects. Cyclomatic complexity concerns it-\nself only with ﬂow control. The formula, summarised for our\npurposes, is one (1) plus the number of conditionals present\nin the system being analysed. That means for any system\nit starts at one, and for each if, while, for, and do-while, we\nadd one. We also add one per path in a switch statement\nexcluding the default case if present.\nUnder the hood, if we consider how a virtual call works,\nthat is, a lookup in a function pointer table followed by a\nbranch into the class method, we can see that a virtual call\nis eﬀectively just as complex as a switch statement. Count-\ning the ﬂow control statements is more diﬃcult in a virtual\ncall because to know the complexity value, you have to know\nthe number of possible methods that can fulﬁl the request.\nIn the case of a virtual call, you have to count the number\nof overrides to a base virtual call. If the base is pure-virtual,\nthen you may subtract one from the complexity. However, if\nyou don’t have access to all the code that is running, which\ncan be possible in the case of dynamically loaded libraries,\nthen the number of diﬀerent potential code paths increases\nby an unknown amount. This hidden or obscured complex-\nity is necessary to allow third party libraries to interface with\nthe core process, but requires a level of trust that implies\nno single part of the process is ever going to be thoroughly\ntested.\nThis kind of complexity is commonly called control ﬂow\ncomplexity. There is another form of complexity inherent in\nsoftware, and that is the complexity of state. In the paper\nOut of the Tar Pit[?], it’s concluded that the aspect of software\nwhich causes the most complexity is state. The paper con-\ntinues and presents a solution which attempts to minimise\n",
      "content_length": 2074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "3.1. COMPLEXITY\n59\nwhat it calls accidental state, that is, state which is required\nby the software to do its job, but not directly required by the\nproblem being solved. The solution also attempts to abolish\nany state introduced merely to support a programming style.\nWe use ﬂow control to change state, and state changes\nwhat is executed in our programs. In most cases ﬂow con-\ntrol is put in for one of two reasons: to solve the problem\npresented (which is equivalent to the essential state in Out\nof the Tar Pit), and to help with the implementation of the\nsolution (which is equivalent to the accidental state).\nEssential control is when we need to implement the de-\nsign, a gameplay feature which has to happen when some\nconditions are met, such as jumping when the jump but-\nton is pressed or autosaving at a save checkpoint when the\nsavedata is dirty, or a timer has run out.\nAccidental control is non-essential to the program from\nthe point of view of the person using it, but could be founda-\ntion work, making it critical for successful program creation.\nThis type of control complexity is itself generally split into two\nforms. The ﬁrst form is structural, such as to support a pro-\ngramming paradigm, to provide performance improvements,\nor to drive an algorithm. The second form is defensive pro-\ngramming or developer helpers such as reference counting\nor garbage collection. These techniques increase complexity\nwhere functions operating on the data aren’t sure the data\nexists, or is making sure bounds are observed. In practice,\nyou will ﬁnd this kind of control complexity when using con-\ntainers and other structures, control ﬂow is going to be in\nthe form of bounds checks and ensuring data has not gone\nout of scope. Garbage collection adds complexity. In many\nlanguages, there are few guarantees about how and when it\nwill happen. This also means it can be hard to reason about\nobject lifetimes.\nBecause of a tendency to ignore memory\nallocations early in development when working with these\nlanguages, it can be very hard to ﬁx memory leaks closer to\nshipping dates. Garbage collection in unmanaged languages\n",
      "content_length": 2132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "60\nCHAPTER 3. EXISTENTIAL PROCESSING\nis easier to handle, as reference counts can more easily be\ninterrogated, but also due to the fact that unmanaged lan-\nguages generally allocate less often in the ﬁrst place.\n3.2\nDebugging\nWhat classes of issues do we suﬀer with high complexity\nprograms? Analysing the complexity of a system helps us\nunderstand how diﬃcult it is to test, and in turn, how hard\nit is to debug. Some issues can be classiﬁed as being in an\nunexpected state, and then having no way forward. Others\ncan be classiﬁed as having bad state, and then exhibiting\nunexpected behaviour due to reacting to this invalid data.\nYet others can be classiﬁed as performance problems, not\njust correctness, and these issues, though somewhat disre-\ngarded by a large amount of academic literature, are costly\nin practice and usually come from complex dependencies of\nstate.\nFor example, the complexity caused by performance tech-\nniques such as caching, are issues of complexity of state.\nThe CPU cache is in a state, and not being aware of it, and\nnot working with the expected state in mind, leads to issues\nof poor or inconsistent performance.\nMuch of the time, the diﬃculty we have in debugging\ncomes from not fully observing all the ﬂow control points,\nassuming one route has been taken when it hasn’t. When\nprograms do what they are told, and not what we mean, they\nwill have entered into a state we had not expected or prepared\nfor.\nWith runtime polymorphism using virtual calls, the like-\nlihood of that happening can dramatically increase as we\ncannot be sure we know all the diﬀerent ways the code can\nbranch until we either litter the code with logging, or step\nthrough in a debugger to see where it goes at run-time.\n",
      "content_length": 1728,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "3.3. WHY USE AN IF\n61\n3.3\nWhy use an if\nIn real-world cases of game development, the most common\nuse of an explicit ﬂow control statement would appear to be\nin the non-essential set. Where defensive programming is\nbeing practiced, many of the ﬂow control statements are just\nto stop crashes. There are fail-safes for out of bounds ac-\ncesses, protection from pointers being NULL, and defenses\nagainst other exceptional cases that would bring the pro-\ngram to a halt. It’s pleasing to note, GitHub contains plenty\nof high quality C++ source-code that bucks this trend, prefer-\nring to work with reference types, or with value types where\npossible.\nIn game development, another common form of\nﬂow control is looping. Though these are numerous, most\ncompilers can spot them, and have good optimisations for\nthese and do a very good job of removing condition checks\nthat aren’t necessary. The ﬁnal inessential but common ﬂow\ncontrol comes from polymorphic calls, which can be helpful\nin implementing some of the gameplay logic, but mostly are\nthere to entertain the do-more-with-less-code development\nmodel partially enforced in the object-oriented approach to\nwriting games.\nEssential game design originating ﬂow control doesn’t ap-\npear very often in proﬁles as causes of branching, as all the\nsupporting code is run far more frequently. This can lead to\nan underappreciation of the eﬀect each conditional has on\nthe performance of the software. Code that does use a con-\nditional to implement AI or handle character movement, or\ndecide on when to load a level, will be calling down into sys-\ntems which are full of loops and tree traversals, or bounds\nchecks on arrays they are accessing in order to return the\ndata upon which the game is going to produce the boolean\nvalue to ﬁnally drive the side of the if to which it will fall\nthrough. That is, when the rest of your code-base is slow,\nit’s hard to validate writing fast code for any one task. It’s\nhard to tell what additional costs you’re adding on.\nIf we decide the elimination of control ﬂow is a goal wor-\n",
      "content_length": 2064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "62\nCHAPTER 3. EXISTENTIAL PROCESSING\nthy of consideration, then we must begin to understand what\ncontrol ﬂow operations we can eliminate. If we begin our at-\ntempt to eliminate control ﬂow by looking at defensive pro-\ngramming, we can try to keep our working set of data as a\ncollections of arrays. This way we can guarantee none of our\ndata will be NULL. That one step alone may eliminate many\nof our ﬂow control statements. It won’t get rid of loops, but\nas long as they are loops over data running a pure func-\ntional style transform, then there are no side-eﬀects to worry\nabout, and it will be easier to reason about.1\nThe inherent ﬂow control in a virtual call is avoidable,\nas it is a fact that many programs were written in a non-\nobject-oriented style. Without virtuals, we can rely on switch\nstatements. Without those, we can rely on function pointer\ntables. Without those, we can have a long sequence of ifs.\nThere are many ways to implement runtime polymorphism.\nIt is also possible to maintain that if you don’t have an explicit\ntype, you don’t need to switch on it, so if you can eradicate\nthe object-oriented approach to solving the problem, those\nﬂow control statements go away completely.\nWhen we get to the control ﬂow in gameplay logic, we ﬁnd\nthere is no simple way to eradicate it. This is not a terrible\nthing to worry about, as the gameplay logic is as close to\nessential complexity as we can get when it comes to game\ndevelopment.\nReducing the number of conditionals, and thus reducing\nthe cyclomatic complexity on such a scale is a beneﬁt which\ncannot be overlooked, but it is one that comes with a cost.\nThe reason we are able to get rid of the check for NULL is\nthat we will have our data in a format that doesn’t allow for\nNULL at all. This inﬂexibility will prove to be a beneﬁt, but\nit requires a new way of processing our entities.\nWhere once we would have an object instance for an area\n1Sean Parent’s talks on C++ seasoning are worth watching. They talk\npractically about simpliﬁcation and elimination of unnecessary loops and\nstructure.\n",
      "content_length": 2071,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "3.3. WHY USE AN IF\n63\nin a game, and we would interrogate it for exits that take us to\nother areas, now we look into a structure that only contains\nlinks between areas, and ﬁlter by the area we are in. This\nreversal of ownership can be a massive beneﬁt in debugging,\nbut can sometimes appear backward when all you want to\ndo is ﬁnd out what exits are available to get out of an area.\nIf you’ve ever worked with shopping lists or to-do lists,\nyou’ll know how much more eﬃcient you can be when you\nhave a deﬁnite list of things to purchase or complete. It’s\nvery easy to make a list, and adding to it is easy as well. If\nyou’re going shopping, it’s very hard to think what might be\nmissing from your house in order to get what you need. If\nyou’re the type that tries to plan meals, then a list is nigh on\nessential as you ﬁgure out ingredients and then tally up the\nnumber of tins of tomatoes, or other ingredients you need to\nlast through all the meals you have planned. If you have a\nto-do list and a calendar, you know who is coming and what\nneeds to be done to prepare for them. You know how many\nextra mouths need feeding, how much food and drink you\nneed to buy, and how much laundry you need done to make\nenough beds for the visitors.\nTo-do lists are great because you can set an end goal and\nthen add in subtasks that make a large and long distant\ngoal seem more doable. Adding in estimates can provide a\nlittle urgency that is usually missing when the deadline is so\nfar away. Many companies use software to support tracking\nof tasks, and this software often comes with features allow-\ning the producers to determine critical paths, expected de-\nveloper hours required, and sometimes even the balance of\nskills required to complete a project. Not using this kind of\nsoftware is often a sign that a company isn’t overly concerned\nwith eﬃciency, or waste. If you’re concerned about eﬃciency\nand waste in your program, lists of tasks seem like a good\nway to start analysing where the costs are coming from. If\nyou keep track of these lists by logging them, you can look\nat the data and see the general shape of the processing your\nsoftware is performing. Without this, it can be diﬃcult to\n",
      "content_length": 2194,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "64\nCHAPTER 3. EXISTENTIAL PROCESSING\ntell where the real bottlenecks are, as it might not be the\nprocessing that is the problem, but the requirement to pro-\ncess data itself which has gotten out of hand.\nWhen your program is running, if you don’t give it ho-\nmogeneous lists to work with, but instead let it do whatever\ncomes up next, it will be ineﬃcient and have irregular or\nlumpy frame timings.\nIneﬃciency of hardware utilisation\noften comes from unpredictable processing. In the case of\nlarge arrays of pointers to heterogeneous classes all being\ncalled with an update() function, you can hit high amounts\nof data dependency which leads to misses in both data and\ninstruction caches. See chapter 11 for more details on why.\nSlowness also comes from not being able to see how much\nwork needs to be done, and therefore not being able to priori-\ntise or scale the work to ﬁt what is possible within the given\ntime-frame. Without a to-do list, and an ability to estimate\nthe amount of time each task will take, it is diﬃcult to decide\nthe best course of action to take in order to reduce overhead\nwhile maintaining feedback to the user.\nObject-oriented programming works very well when there\nare few patterns in the way the program runs. When either\nthe program is working with only a small amount of data, or\nwhen the data is incredibly heterogeneous, to the point that\nthere are as many classes of things as there are things.\nIrregular frame timings can often be blamed on not be-\ning able to act on distant goals ahead of time. If you, as a\ndeveloper, know you have to load the assets for a new island\nwhen a player ventures into the seas around it, the stream-\ning system can be told to drag in any data necessary. This\ncould also be for a room and the rooms beyond.\nIt could\nbe for a cave or dungeon when the player is within sight of\nthe entrance. We consider this kind of preemptive streaming\nof data to be a special case and invent systems to provide\nthis level of forethought. Relying on humans, or even level-\ndesigners, to link these together is prone to error. In many\ncases, there are chains of dependencies that can be missed\n",
      "content_length": 2141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "3.3. WHY USE AN IF\n65\nwithout an automated check. The reason we cannot make\nsystems self-aware enough to preload themselves is that we\ndon’t have a common language to describe temporal depen-\ndencies.\nIn many games, we stream things in with explicit triggers,\nbut there is often no such system for many of the other game\nelements. It’s virtually unheard of for an AI to pathﬁnd to\nsome goal because there might soon be a need to head that\nway. The closest would be for the developer to pre-populate\na navigation map so coarse grain pathing can be completed\nswiftly.\nThere’s also the problem of depth of preemptive work.\nConsider the problem of a small room, built as a separate\nasset, a waiting room with two doors near each other, both\nleading to large, but diﬀerent maps. When the player gets\nnear the door to the waiting room in map A, that little room\ncan be preemptively streamed in. However, in many engines,\nmap B won’t be streamed in, as the locality of map B to map\nA is hidden behind the logical layer of the waiting room.\nIt’s also not commonplace to ﬁnd a physics system doing\nlook ahead to see if a collision has happened in the future in\norder to start doing further work. It might be possible to do\na more complex breakup simulation if it were more aware.\nIf you let your game generate to-do lists, shopping lists,\ndistant goals, and allow for preventative measures by forward-\nthinking, then you can simplify your task as a coder into\nprioritising goals and eﬀects, or writing code that generates\npriorities at runtime. You can start to think about how to\nchain those dependencies to solve the waiting room problem.\nYou can begin to preempt all types of processing.\n",
      "content_length": 1682,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "66\nCHAPTER 3. EXISTENTIAL PROCESSING\n3.4\nTypes of processing\nExistential processing is related to to-do lists.\nWhen you\nprocess every element in a homogeneous set of data, you\nknow you are processing every element the same way. You\nare running the same instructions for every element in that\nset. There is no deﬁnite requirement for the output in this\nspeciﬁcation, however, it usually comes down to one of three\ntypes of operation: a ﬁlter, a mutation, or an emission. A\nmutation is a one to one manipulation of the data, it takes\nincoming data and some constants that are set up before\nthe transform, and produces one and only one element for\neach input element. A ﬁlter takes incoming data, again with\nsome constants set up before the transform, and produces\none element or zero elements for each input element.\nAn\nemission is a manipulation of the incoming data that can\nproduce multiple output elements. Just like the other two\ntransforms, an emission can use constants, but there is no\nguaranteed size of the output table; it can produce anywhere\nbetween zero and inﬁnity elements.\nA fourth, and ﬁnal form, is not really a manipulation of\ndata, but is often part of a transform pipeline, and that is\nthe generator. A generator takes no input data, but merely\nproduces output based on the constants set up. When work-\ning with compute shaders, you might come across this as a\nfunction that merely clears out an array to zero, one, or an\nascending sequence.\nThese categories can help you decide what data structure\nyou will use to store the elements in your arrays, and whether\nyou even need a structure, or you should instead pipe data\nfrom one stage to another without it touching down on an\nintermediate buﬀer.\nEvery CPU can eﬃciently handle running processing\nkernels over homogeneous sets of data, that is, doing the\nsame operation over and over again over contiguous data.\nWhen there is no global state, no accumulator, it is proven\n",
      "content_length": 1943,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "3.4. TYPES OF PROCESSING\n67\nTransforms\nMutation\nin == out\nHandles input data. Produces\none item of output for every item\nof input.\nFilter\nin >= out\nHandles input data. Produces\nup to one item of output for\nevery item of input.\nEmission\nout =\n(\n0,\nin = 0\n>= 0,\notherwise\nHandles input data. Produces\nunknown amount of items per\nitem of input. With no input,\noutput is also empty.\nGeneration\nin = 0 ∧out >= 0\nDoes not read data. Produces an\nunknown amount of items just\nby running.\nTable 3.1: Types of transform normally encountered\nto be parallelisable.\nExamples can be given from exist-\ning technologies such as map-reduce and simple compute\nshaders, as to how to go about building real work applica-\ntions within these restrictions.\nStateless transforms also\ncommit no crimes that prevent them from being used within\ndistributed processing technologies. Erlang relies on these\nguarantees of being side-eﬀect free to enable not just thread\nsafe processing or interprocess safe processing, but dis-\ntributed computing safe processing. Stateless transforms of\nstateful data are highly robust and deeply parallelisable.\nWithin the processing of each element, that is for each\ndatum operated on by the transform kernel, it is fair to use\ncontrol ﬂow. Almost all compilers should be able to reduce\nsimple local value branch instructions into a platform’s pre-\nferred branch-free representation, such as a CMOV, or select\nfunction for a SIMD operation. When considering branches\ninside transforms, it’s best to compare to existing implemen-\ntations of stream processing such as graphics card shaders\nor compute kernels.\nIn predication, ﬂow control statements are not ignored,\nbut they are used instead as an indicator of how to merge two\nresults. When the ﬂow control is not based on a constant,\na predicated if will generate code that will run both sides of\n",
      "content_length": 1853,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "68\nCHAPTER 3. EXISTENTIAL PROCESSING\nthe branch at the same time and discard one result based on\nthe value of the condition. It manages this by selecting one\nof the results based on the condition. As mentioned before,\nin many CPUs there is an intrinsic for this, but all CPUs can\nuse bit masking to eﬀect this trick.\nSIMD or single-instruction-multiple-data allows the par-\nallel processing of data when the instructions are the same.\nThe data is diﬀerent but local. When there are no condition-\nals, SIMD operations are simple to implement on your trans-\nforms. In MIMD, that is multiple instructions, multiple data,\nevery piece of data can be operated on by a diﬀerent set of\ninstructions. Each piece of data can take a diﬀerent path.\nThis is the simplest and most error-prone to code for because\nit’s how most parallel programming is currently done. We\nadd a thread and process some more data with a separate\nthread of execution. MIMD includes multi-core general pur-\npose CPUs. It often allows shared memory access and all\nthe synchronisation issues that come with it. It is by far the\neasiest to get up and running, but it is also the most prone\nto the kind of rare fatal error caused by complexity of state.\nBecause the order of operations become non-deterministic,\nthe number of diﬀerent possible routes taken through the\ncode explode super-exponentially.\n3.5\nDon’t use booleans\nWhen you study compression technology, one of the most\nimportant aspects you have to understand is the diﬀerence\nbetween data and information.\nThere are many ways to\nstore information in systems, from literal strings that can be\nparsed to declare something exists, right down to something\nsimple like a single bit ﬂag to show that a thing might have\nan attribute. Examples include the text that declares the\nexistence of a local variable in a scripting language, or the\nbit ﬁeld containing all the diﬀerent collision types a physics\nmesh will respond to.\nSometimes we can store even less\n",
      "content_length": 1974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "3.5. DON’T USE BOOLEANS\n69\ninformation than a bit by using advanced algorithms such\nas arithmetic encoding, or by utilising domain knowledge.\nDomain knowledge normalisation applies in most game de-\nvelopment, but it is increasingly infrequently applied, as\nmany developers are falling foul to overzealous application\nof quoting premature optimisation.\nAs information is en-\ncoded in data, and the amount of information encoded can\nbe ampliﬁed by domain knowledge, it’s important that we be-\ngin to see that the advice oﬀered by compression techniques\nis: what we are really encoding is probabilities.\nIf we take an example, a game where the entities have\nhealth, regenerate after a while of not taking damage, can\ndie, can shoot each other, then let’s see what domain knowl-\nedge can do to reduce processing.\nWe assume the following domain knowledge:\n• If you have full health, then you don’t need to regener-\nate.\n• Once you have been shot, it takes some time until you\nbegin regenerating.\n• Once you are dead, you cannot regenerate.\n• Once you are dead you have zero health.\n1\nstruct\nEntity {\n2\n//\ninformation\nabout\nthe\nentity\nposition\n3\n// ...\n4\n// now\nhealth\ndata\nin the\nmiddle\nof the\nentity\n5\nfloat\ntimeoflastdamage ;\n6\nfloat\nhealth;\n7\n// ...\n8\n//\nother\nentity\ninformation\n9\n};\n10\nlist <Entity > entities;\nListing 3.1: basic entity approach\n1\nvoid\nUpdateHealth ( Entity *e ) {\n2\nTimeType\ntimeSinceLastShot = e-> timeOfLastDamage\n- currentTime ;\n3\nbool\nisHurt = e->health < MAX_HEALTH;\n4\nbool\nisDead = e->health\n<= 0;\n5\nbool\nregenCanStart = timeSinceLastShot\n>\nTIME_BEFORE_REGENERATING ;\n6\n// if alive , and hurt , and it’s been\nlong\nenough\n7\nif( !isDead && isHurt &&\nregenCanStart ) {\n8\ne->health = min(MAX_HEALTH , e->health + tickTime * regenRate)\n;\n",
      "content_length": 1757,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "70\nCHAPTER 3. EXISTENTIAL PROCESSING\n9\n}\n10\n}\nListing 3.2: simple health regen\nIf we have a list for the entities such as in listing 3.1, then\nwe see the normal problem of data potentially causing cache\nline utilisation issues, but aside from that, we can see how\nyou might run an update function over the list, such as in\nlisting 3.2, which will run for every entity in the game, every\nupdate.\nWe can make this better by looking at the ﬂow control\nstatement. The function won’t run if health is at max. It\nwon’t run if the entity is dead. The regenerate function only\nneeds to run if it has been long enough since the last dam-\nage dealt. All these things considered, regeneration isn’t the\ncommon case. We should try to organise the data layout for\nthe common case.\n1\nstruct\nEntity {\n2\n//\ninformation\nabout\nthe\nentity\nposition\n3\n// ...\n4\n//\nother\nentity\ninformation\n5\n};\n6\nstruct\nEntitydamage {\n7\nfloat\ntimeoflastdamage ;\n8\nfloat\nhealth;\n9\n}\n10\nlist <Entity > entities;\n11\nmap <EntityRef ,Entitydamage > entitydamages ;\nListing 3.3: Existential processing style health\nLet’s change the structures to those in listing 3.3 and then\nwe can run the update function over the health table rather\nthan the entities. This means we already know, as soon as\nwe are in this function, that the entity is not dead, and they\nare hurt.\n1\nvoid\nUpdateHealth () {\n2\nfor( edIter : entityDamages ) {\n3\nEntityDamage &ed = edIter ->second;\n4\nif( ed.health\n<= 0 ) {\n5\n// if dead , insert\nthe\nfact\nthat\nthis\nentity\nis\ndead\n6\nEntityRef\nentity = edIter ->first;\n7\ndeadEntities .insert( entity );\n8\n// if dead , discard\nbeing\ndamaged\n9\ndiscard(ed);\n10\n} else {\n11\nTimeType\ntimeSinceLastShot = currentTime\n- ed.\ntimeOfLastShot ;\n12\nbool\nregenCanStart = timeSinceLastShot\n>\nTIME_BEFORE_REGENERATING ;\n",
      "content_length": 1774,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "3.5. DON’T USE BOOLEANS\n71\n13\nif( regenCanStart )\n14\ned ->health =ed ->health + tickTime * regenRate;\n15\n// if at max\nhealth\nor beyond , discard\nbeing\ndamaged\n16\nif( ed ->health\n>= MAX_HEALTH )\n17\ndiscard(ed);\n18\n}\n19\n}\n20\n}\nListing 3.4: every entity health regen\nWe only add a new entityhealth element when an entity\ntakes damage. If an entity takes damage when it already has\nan entityhealth element, then it can update the health rather\nthan create a new row, also updating the time damage was\nlast dealt. If you want to ﬁnd out someone’s health, then\nyou only need to look and see if they have an entityhealth\nrow, or if they have a row in deadEntities table.\nThe rea-\nson this works is, an entity has an implicit boolean hidden\nin the row existing in the table. For the entityDamages ta-\nble, that implicit boolean is the isHurt variable from the ﬁrst\nfunction. For the deadEntities table, the boolean of isDead\nis now implicit, and also implies a health value of 0, which\ncan reduce processing for many other systems. If you don’t\nhave to load a ﬂoat and check it is less than 0, then you’re\nsaving a ﬂoating point comparison or conversion to boolean.\nThis eradication of booleans is nothing new, because ev-\nery time you have a pointer to something you introduce a\nboolean of having a non-NULL value. It’s the fact that we\ndon’t want to check for NULL which pushes us towards ﬁnd-\ning a diﬀerent representation for the lack of existence of an\nobject to process.\nOther similar cases include weapon reloading, oxygen lev-\nels when swimming, anything which has a value that runs\nout, has a maximum, or has a minimum. Even things like\ndriving speeds of cars. If they are traﬃc, then they will spend\nmost of their time driving at traﬃc speed not some speed they\nneed to calculate. If you have a group of people all heading\nin the same direction, then someone joining the group can\nbe intercepting until they manage to, at which point they can\ngive up their independence, and become controlled by the\n",
      "content_length": 2002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "72\nCHAPTER 3. EXISTENTIAL PROCESSING\ngroup. There is more on this point in chapter 5.\nBy moving to keeping lists of attribute state, you can\nintroduce even more performance improvements. The ﬁrst\nthing you can do for attributes that are linked to time is to\nput them in a sorted list, sorted by time of when they should\nbe acted upon. You could put the regeneration times in a\nsorted list and pop entityDamage elements until you reach\none that can’t be moved to the active list, then run through\nall the active list in one go, knowing they have some damage,\naren’t dead, and can regen as it’s been long enough.\nAnother aspect is updating certain attributes at diﬀerent\ntime intervals. Animals and plants react to their environ-\nment through diﬀerent mechanisms. There are the very fast\nmechanisms such as reactions to protect us from danger.\nPulling your hand away from hot things, for example. There\nare the slower systems too, like the rationalising parts of the\nbrain. Some, apparently quick enough that we think of them\nas real-time, are the quick thinking and acting processes we\nconsider to be the actions taken by our brains when we don’t\nhave time to think about things in detail, such as catching\na ball or balancing a bicycle. There is an even slower part of\nthe brain, the part that isn’t so much reading this book, but\nis consuming the words, and making a model of what they\nmean so as to digest them. There is also the even slower sys-\ntems, the ones which react to stress, chemical levels spread\nthrough the body as hormones, or just the amount of sugar\nyou have available, or current level of hydration. An AI which\ncan think and react on multiple time-scales is more likely to\nwaste fewer resources, but also much less likely to act oddly,\nor ﬂip-ﬂop between their decisions. Committing to doing an\nupdate of every system every frame could land you in an im-\npossible situation. Splitting the workload into diﬀerent up-\ndate rates can still be regular, but oﬀers a chance to balance\nthe work over multiple frames.\nAnother use is in state management. If an AI hears gun-\nﬁre, then they can add a row to a table for when they last\n",
      "content_length": 2145,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "3.6. DON’T USE ENUMS QUITE AS MUCH\n73\nheard gunﬁre, and that can be used to determine whether\nthey are in a heightened state of awareness. If an AI has been\ninvolved in a transaction with the player, it is important they\nremember what has happened as long as the player is likely\nto remember it. If the player has just sold an AI their +5\nlongsword, it’s very important the shopkeeper AI still have it\nin stock if the player just pops out of the shop for a moment.\nSome games don’t even keep inventory between transactions,\nand that can become a sore point if they accidentally sell\nsomething they need and then save their progress.\nFrom a gameplay point of view, these extra bits of infor-\nmation are all about how the world and player interact. In\nsome games, you can leave your stuﬀlying around forever,\nand it will always remain just how you left it. It’s quite a feat\nthat all the things you have dumped in the caves of some\nopen-world role-playing games, are still hanging around pre-\ncisely where you left them hours and hours ago.\nThe general concept of tacking on data, or patching\nloaded data with dynamic additional attributes, has been\naround for quite a while. Save games often encode the state\nof a dynamic world as a delta from the base state, and one\nof the ﬁrst major uses was in fully dynamic environments,\nwhere a world is loaded, but can be destroyed or altered\nlater. Some world generators took a procedural landscape\nand allowed their content creators to add patches of ex-\ntra information, villages, forts, outposts, or even break out\nlandscaping tools to drastically adjust the generated data.\n3.6\nDon’t use enums quite as much\nEnumerations are used to deﬁne sets of states. We could\nhave had a state variable for the regenerating entity, one that\nhad infullhealth, ishurt, isdead as its three states. We could\nhave had a team index variable for the avoidance entity enu-\nmerating all the available teams. Instead, we used tables to\n",
      "content_length": 1957,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "74\nCHAPTER 3. EXISTENTIAL PROCESSING\nprovide all the information we needed, as there were only two\nteams. Any enum can be emulated with a variety of tables.\nAll you need is one table per enumerable value. Setting the\nenumeration is an insert into a table or a migration from one\ntable to another.\nWhen using tables to replace enums, some things become\nmore diﬃcult: ﬁnding out the value of an enum in an entity\nis diﬃcult as it requires checking all the tables which repre-\nsent that state for the entity. However, the main reason for\ngetting the value is either to do an operation based on an ex-\nternal state or to ﬁnd out if an entity is in the right state to be\nconsidered for an operation. This is disallowed and unnec-\nessary for the most part, as ﬁrstly, accessing external state\nis not valid in a pure function, and secondly, any dependent\ndata should already be part of the table element.\nIf the enum is a state or type enum previously handled\nby a switch or virtual call, then we don’t need to look up the\nvalue, instead, we change the way we think about the prob-\nlem. The solution is to run transforms taking the content of\neach of the switch cases or virtual methods as the operation\nto apply to the appropriate table, the table corresponding to\nthe original enumeration value.\nIf the enum is instead used to determine whether or not\nan entity can be operated upon, such as for reasons of com-\npatibility, then consider an auxiliary table to represent be-\ning in a compatible state. If you’re thinking about the case\nwhere you have an entity as the result of a query and need\nto know if it is in a certain state before deciding to com-\nmit some changes, consider that the compatibility you seek\ncould have been part of the criteria for generating the output\ntable in the ﬁrst place, or a second ﬁltering operation could\nbe committed to create a table in the right form.\nIn conclusion, the reason why you would put an enum in\ntable form, is to reduce control ﬂow impact. Given this, it’s\nwhen we aren’t using the enumerations to control instruc-\ntion ﬂow that it’s ﬁne to leave them alone. Another possibility\n",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "3.7. PRELUDE TO POLYMORPHISM\n75\nis when the value of the enum changes with great frequency,\nas moving objects from table to table has a cost too.\nExamples of enumerations that make sense are keybind-\nings, enumerations of colours, or good names for small ﬁnite\nsets of values. Functions that return enums, such as colli-\nsion responses (none, penetrating, through).\nAny kind of\nenumeration which is actually a lookup into data of another\nform is good, where the enum is being used to rationalise\nthe access to those larger or harder to remember data ta-\nbles. There is also a beneﬁt to some enums in that they will\nhelp you trap unhandled cases in switches, and to some ex-\ntent, they are a self-documenting feature in most languages.\n3.7\nPrelude to polymorphism\nLet’s consider now how we implement polymorphism.\nWe\nknow we don’t have to use a virtual table pointer; we could\nuse an enum as a type variable. That variable, the member\nof the structure that deﬁnes at runtime what that structure\nshould be capable of and how it is meant to react.\nThat\nvariable will be used to direct the choice of functions called\nwhen methods are called on the object.\nWhen your type is deﬁned by a member type variable, it’s\nusual to implement virtual functions as switches based on\nthat type, or as an array of functions. If we want to allow\nfor runtime loaded libraries, then we would need a system\nto update which functions are called.\nThe humble switch\nis unable to accommodate this, but the array of functions\ncould be modiﬁed at runtime.\nWe have a solution, but it’s not elegant, or eﬃcient. The\ndata is still in charge of the instructions, and we suﬀer the\nsame instruction cache misses and branch mispredictions\nas whenever a virtual function is unexpected.\nHowever,\nwhen we don’t really use enums, but instead tables that\n",
      "content_length": 1815,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "76\nCHAPTER 3. EXISTENTIAL PROCESSING\nrepresent each possible value of an enum, it is still possible\nto keep compatible with dynamic library loading the same\nas with pointer based polymorphism, but we also gain the\neﬃciency of a data-ﬂow processing approach to processing\nheterogeneous types.\nFor each class, instead of a class declaration, we have\na factory that produces the correct selection of table insert\ncalls. Instead of a polymorphic method call, we utilise ex-\nistential processing. Our elements in tables allow the char-\nacteristics of the class to be implicit. Creating your classes\nwith factories can easily be extended by runtime loaded li-\nbraries. Registering a new factory should be simple as long\nas there is a data-driven factory method. The processing of\nthe tables and their update() functions would also be added\nto the main loop.\n3.8\nDynamic runtime polymorphism\nIf you create your classes by composition, and you allow the\nstate to change by inserting and removing from tables, then\nyou also allow yourself access to dynamic runtime polymor-\nphism. This is a feature normally only available when dy-\nnamically responding via a switch.\nPolymorphism is the ability for an instance in a program\nto react to a common entry point in diﬀerent ways due only\nto the nature of the instance. In C++, compile-time polymor-\nphism can be implemented through templates and overload-\ning. Runtime polymorphism is the ability for a class to pro-\nvide a diﬀerent implementation for a common base operation\nwith the class type unknown at compile-time. C++ handles\nthis through virtual tables, calling the right function at run-\ntime based on the type hidden in the virtual table pointer at\nthe start of the memory pointed to by the this pointer. Dy-\nnamic runtime polymorphism is when a class can react to a\ncommon call signature in diﬀerent ways based on its type,\n",
      "content_length": 1871,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "3.8. DYNAMIC RUNTIME POLYMORPHISM\n77\nbut its type can change at runtime. C++ doesn’t implement\nthis explicitly, but if a class allows the use of an internal state\nvariable or variables, it can provide diﬀering reactions based\non the state as well as the core language runtime virtual ta-\nble lookup. Other languages which deﬁne their classes more\nﬂuidly, such as Python, allow each instance to update how\nit responds to messages, but most of these languages have\nvery poor general performance as the dispatch mechanism\nhas been built on top of dynamic lookup.\n1\nclass\nshape {\n2\npublic:\n3\nshape () {}\n4\nvirtual ~shape () {}\n5\nvirtual\nfloat\ngetarea () const = 0;\n6\n};\n7\nclass\ncircle : public\nshape {\n8\npublic:\n9\ncircle( float\ndiameter ) : d(diameter ) {}\n10\n~circle () {}\n11\nfloat\ngetarea () const { return d*d*pi /4; }\n12\nfloat d;\n13\n};\n14\nclass\nsquare : public\nshape {\n15\npublic:\n16\nsquare( float\nacross ) : width( across ) {}\n17\n~square () {}\n18\nfloat\ngetarea () const { return\nwidth*width; }\n19\nfloat\nwidth;\n20\n};\n21\nvoid\ntest () {\n22\ncircle\ncircle( 2.5f );\n23\nsquare\nsquare( 5.0f );\n24\nshape *shape1 = &circle , *shape2 = &square;\n25\nprintf( \"areas\nare %f and %f\\n\", shape1 ->getarea (), shape2 ->\ngetarea () );\n26\n}\nListing 3.5: simple object-oriented shape code\nConsider the code in listing 3.5, where we expect the run-\ntime method lookup to solve the problem of not knowing the\ntype but wanting the size. Allowing the objects to change\nshape during their lifetime requires some compromise. One\nway is to keep a type variable inside the class such as in\nlisting 3.6, where the object acts as a container for the type\nvariable, rather than as an instance of a speciﬁc shape.\n1\nenum\nshapetype { circletype , squaretype\n};\n2\nclass\nmutableshape {\n3\npublic:\n4\nmutableshape ( shapetype\ntype , float\nargument )\n5\n: m_type( type ), distanceacross ( argument )\n6\n{}\n7\n~ mutableshape () {}\n8\nfloat\ngetarea () const {\n9\nswitch( m_type ) {\n10\ncase\ncircletype: return\ndistanceacross * distanceacross *pi /4;\n11\ncase\nsquaretype: return\ndistanceacross * distanceacross ;\n12\n}\n",
      "content_length": 2067,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "78\nCHAPTER 3. EXISTENTIAL PROCESSING\n13\n}\n14\nvoid\nsetnewtype( shapetype\ntype ) {\n15\nm_type = type;\n16\n}\n17\nshapetype\nm_type;\n18\nfloat\ndistanceacross ;\n19\n};\n20\nvoid\ntestinternaltype () {\n21\nmutableshape\nshape1( circletype , 5.0f );\n22\nmutableshape\nshape2( circletype , 5.0f );\n23\nshape2. setnewtype( squaretype );\n24\nprintf( \" areas\nare %f and %f\\n\", shape1.getarea (), shape2.\ngetarea () );\n25\n}\nListing 3.6: ugly internal type code\nA better way is to have a conversion function to handle\neach case. In listing 3.7 we see how that can be achieved.\n1\nsquare\nsquarethecircle ( const\ncircle &circle ) {\n2\nreturn\nsquare( circle.d );\n3\n}\n4\nvoid\ntestconvertintype () {\n5\ncircle\ncircle( 5.0f );\n6\nsquare\nsquare = squarethecircle ( circle );\n7\n}\nListing 3.7: convert existing class to new class\nThough this works, all the pointers to the old class are\nnow invalid.\nUsing handles would mitigate these worries,\nbut add another layer of indirection in most cases, dragging\ndown performance even further.\nIf you use existential processing techniques, your classes\ndeﬁned by the tables they belong to, then you can switch\nbetween tables at runtime. This allows you to change be-\nhaviour without any tricks, without the complexity of man-\naging a union to carry all the diﬀerent data around for all\nthe states you need. If you compose your class from diﬀer-\nent attributes and abilities then need to change them post\ncreation, you can.\nIf you’re updating tables, the fact that\nthe pointer address of an entity has changed will mean little\nto you. It’s normal for an entity to move around memory in\ntable-based processing, so there are fewer surprises. Look-\ning at it from a hardware point of view, in order to implement\nthis form of polymorphism you need a little extra space for\nthe reference to the entity in each of the class attributes or\nabilities, but you don’t need a virtual table pointer to ﬁnd\n",
      "content_length": 1892,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "3.9. EVENT HANDLING\n79\nwhich function to call. You can run through all entities of\nthe same type increasing cache eﬀectiveness, even though it\nprovides a safe way to change type at runtime.\nVia the nature of having classes deﬁned implicitly by the\ntables they belong to, there is an opportunity to register a\nsingle entity with more than one table. This means that not\nonly can a class be dynamically runtime polymorphic, but\nit can also be multi-faceted in the sense that it can be more\nthan one class at a time. A single entity might react in two\ndiﬀerent ways to the same trigger call because it might be\nappropriate for the current state of that class.\nThis kind of multidimensional classing doesn’t come up\nmuch in traditional gameplay code, but in rendering, there\nare usually a few diﬀerent axes of variation such as the ma-\nterial, what blend mode, what kind of skinning or other ver-\ntex adjustments are going to take place on a given instance.\nMaybe we don’t see this ﬂexibility in gameplay code because\nit’s not available through the natural tools of the language.\nIt could be that we do see it, but it’s what some people call\nentity component systems.\n3.9\nEvent handling\nWhen you wanted to listen for events in a system in the old\ndays, you’d attach yourself to an interrupt. Sometimes you\nmight get to poke at code that still does this, but it’s normally\nreserved for old or microcontroller scale hardware. The idea\nwas simple, the processor wasn’t really fast enough to poll all\nthe possible sources of information and do something about\nthe data, but it was fast enough to be told about events and\nprocess the information as and when it arrived. Event han-\ndling in games has often been like this, register yourself as\ninterested in an event, then get told about it when it hap-\npens. The publish and subscribe model has been around\nfor decades, but there’s no standard interface built for it in\n",
      "content_length": 1911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "80\nCHAPTER 3. EXISTENTIAL PROCESSING\nsome languages and too many standards in others.\nAs it\noften requires some knowledge from the problem domain to\nchoose the most eﬀective implementation.\nSome systems want to be told about every event in the\nsystem and decide for themselves, such as Windows event\nhandling. Some systems subscribe to very particular events\nbut want to react to them as soon as they happen, such as\nhandlers for the BIOS events like the keyboard interrupt.\nThe events could be very important and dispatched directly\nby the action of posting the event, such as with callbacks.\nThe events could be lazy, stuck in a queue somewhere wait-\ning to be dispatched at some later point. The problem they\nare trying to solve will deﬁne the best approach.\nUsing your existence in a table as the registration tech-\nnique makes this simpler than before and lets you regis-\nter and de-register with great pace. Subscription becomes\nan insert, and unsubscribing a delete. It’s possible to have\nglobal tables for subscribing to global events. It would also\nbe possible to have named tables. Named tables would al-\nlow a subscriber to subscribe to events before the publisher\nexists.\nWhen it comes to ﬁring oﬀevents, you have a choice. You\ncan choose to ﬁre oﬀthe transform immediately, or queue up\nnew events until the whole transform is complete, then dis-\npatch them all in one go. As the model becomes simpler and\nmore usable, the opportunity for more common use leads\nus to new ways of implementing code traditionally done via\npolling.\nFor example: unless a player character is within the dis-\ntance to activate a door, the event handler for the player’s\naction button needn’t be attached to anything door related.\nWhen the character comes within range, the character\nregisters into the has pressed action event table with the\nopen door (X) event result. This reduces the amount of time\nthe CPU wastes ﬁguring out what thing the player was trying\nto activate, and also helps provide state information such as\n",
      "content_length": 2014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "3.9. EVENT HANDLING\n81\non-screen displays saying pressing Green will Open the door.\nIf we allow for all tables to have triggers like those found\nin DBMSs, then it may be possible to register interest in\nchanges to input mappings, and react.\nHooking into low-\nlevel tables such as a insert into a has pressed action ta-\nble would allow user interfaces to know to change their on-\nscreen display to show the new prompt.\nThis coding style is somewhat reminiscent of aspect-\noriented programming where it is easy to allow for cross-\ncutting concerns in the code. In aspect-oriented program-\nming, the core code for any activities is kept clean, and any\nside eﬀects or vetoes of actions are handled by other con-\ncerns hooking into the activity from outside. This keeps the\ncore code clean at the expense of not knowing what is really\ngoing to be called when you write a line of code. How using\nregistration tables diﬀers is in where the reactions come\nfrom and how they are determined. Debugging can become\nsigniﬁcantly simpler as the barriers between cause and ef-\nfect normally implicit in aspect-oriented programming are\nsigniﬁcantly diminished or removed, and the hard to adjust\nnature of object-oriented decision making can be softened\nto allow your code to become more dynamic without the\nnormally associated cost of data-driven control ﬂow.\n",
      "content_length": 1344,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "82\nCHAPTER 3. EXISTENTIAL PROCESSING\n",
      "content_length": 37,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "Chapter 4\nComponent Based\nObjects\nA component-oriented design is a good start for high-level\ndata-oriented design. Developing with components can put\nyou in the right frame of mind to avoid linking together con-\ncepts needlessly. Objects built this way can more easily be\nprocessed by type, instead of by instance, which can lead\nto them being easier to proﬁle. Entity systems built around\nthem are often found in game development as a way to pro-\nvide data-driven functionality packs for entities, allowing for\ndesigner control over what would normally be in the realm\nof a programmer.\nNot only are component based entities\nbetter for rapid design changes, but they also stymie the\nchances of getting bogged down into monolithic objects, as\nmost game designers would demand more components with\nnew features over extending the scope of existing compo-\nnents. This is because most new designs need iterating on,\nand extending an existing component by code to introduce\ndesign changes wouldn’t allow game designers to switch back\nand forth trying out diﬀerent things as easily. It’s usually\nmore ﬂexible to add another component as an extension or\nas an alternative.\n83\n",
      "content_length": 1169,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "84\nCHAPTER 4. COMPONENT BASED OBJECTS\nA problem that comes up with talking about component-\noriented development is how many diﬀerent types of entity\ncomponent systems there are. To help clear the ambiguity,\nwe shall describe some diﬀerent ways in which component-\noriented designs work.\nThe ﬁrst kind of component-oriented approach most peo-\nple use is a compound object. There are a few engines that\nuse them this way, and most of them use the power of their\nscripting language to help them achieve a ﬂexible, and de-\nsigner friendly way to edit and create objects out of compo-\nnents. For example, Unity’s GameObject is a base entity type\nwhich can include components by adding them to that par-\nticular instance’s list of components. They are all built onto\nthe core entity object, and they refer to each other through\nit. This approach means every entity still tends to update via\niteration over root instances, not iteration over systems.\nCommon dialogue around creating compound objects\nfrequently refers to using components to make up an ob-\nject directly by including them as members of the object.\nThough this is better than a monolithic class, it is not yet a\nfully component based approach. This technique uses com-\nponents to make the object more readable, and potentially\nmore reusable and robust to change. These systems are ex-\ntensible enough to support large ecosystems of components\nshareable between projects.\nThe Unity Asset Store proves\nthe worth of components from the point of view of rapid\ndevelopment.\nWhen you introduce component based entities, you have\nan opportunity to turn the idea of how you deﬁne an ob-\nject on its head. The normal approach to deﬁning an ob-\nject in object-oriented design is to name it, then ﬁll out the\ndetails as and when they become necessary. For example,\nyour car object is deﬁned as a Car, if not extending Vehicle,\nthen at least including some data about what physics and\nmeshes are needed, with construction arguments for wheels\nand body shell model assets etc, possibly changing class de-\n",
      "content_length": 2051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "4.1. COMPONENTS IN THE WILD\n85\npendent on whether it’s an AI or player car. In component-\noriented design, objects aren’t so rigidly deﬁned, and don’t\nso much become deﬁned after they are named, as much as\na deﬁnition is selected or compiled, and then tagged with a\nname if necessary. For example, instancing a physics com-\nponent with four-wheel physics, instancing a renderable for\neach part (wheels, shell, suspension) adding an AI or player\ncomponent to control the inputs for the physics component,\nall adds up to something which we can tag as a Car, or leave\nas is and it becomes something implicit rather than explicit\nand immutable.\nA truly component based object is nothing more than the\nsum of its parts. This means the deﬁnition of a component\nbased object is also nothing more than an inventory with\nsome construction arguments. This object or deﬁnition ag-\nnostic approach makes refactoring and redesigning a much\nsimpler exercise. Unity’s ECS provides such a solution. In\nthe ECS, entities are intangible and implicit, and the com-\nponents are ﬁrst class citizens.\n4.1\nComponents in the wild\nComponent based approaches to development have been\ntried and tested. Many high-proﬁle studios have used com-\nponent driven entity systems to great success1, and this\nwas in part due to their developer’s unspoken understand-\ning that objects aren’t a good place to store all your data\nand traits.\nFor some, it was the opportunity to present\nthe complexity of what makes up an entity through simpler\npieces, so designers and modders would be able to reason\nabout how their changes ﬁt within the game framework. For\nsome, it was about giving power over to performance, where\ncomponents are more easily moved to a structure-of-arrays\napproach to processing.\n1Gas Powered Games, Looking Glass Studios, Insomniac, Neversoft all\nused component based objects.\n",
      "content_length": 1859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "86\nCHAPTER 4. COMPONENT BASED OBJECTS\nGas Powered Games’ Dungeon Siege Architecture is prob-\nably the earliest published document about a game company\nusing a component based approach. If you get a chance, you\nshould read the article[?] to see where things really kicked\noﬀ. The article explains that using components means the\nentity type2 doesn’t need to have the ability to do anything.\nInstead, all the attributes and functionality come from the\ncomponents of which the entity is made.\nThe list of reasons to move to a manager driven, compo-\nnent based approach are numerous, and we shall attempt\nto cover at least a few.\nWe will talk about the beneﬁts of\nclear update sequences. We will mention how components\ncan make it easier to debug. We will talk about the prob-\nlem of objects applying meaning to data, causing coupling,\nand therefore with the dissolution of the object as the central\nentity, how the tyranny of the instance is mitigated.\nIn this section, we’ll show how we can take an existing\nclass and rewrite it in a component based fashion.\nWe’re\ngoing to tackle a fairly typical complex object, the Player\nclass.\nNormally these classes get messy and out of hand\nquite quickly. We’re going to assume it’s a Player class de-\nsigned for a generic 3rd person action game, and take a typ-\nically messy class as our starting point. We shall use listing\n4.1 as a reference example of one such class.\n1\nclass\nPlayer {\n2\npublic:\n3\nPlayer ();\n4\n~Player ();\n5\nVec\nGetPos (); // the\nroot\nnode\nposition\n6\nvoid\nSetPos( Vec ); // for\nspawning\n7\nVec\nGetSpeed (); //\ncurrent\nvelocity\n8\nfloat\nGetHealth ();\n9\nbool\nIsDead ();\n10\nint\nGetPadIndex (); // the\nplayer\npad\ncontrolling\nme\n11\nfloat\nGetAngle (); // the\ndirection\nthe\nplayer\nis\npointing\n12\nvoid\nSetAnimGoal ( ... ); //\npush\nstate\nto anim -tree\n13\nvoid\nShoot( Vec\ntarget ); //\nfire\nthe\nplayer ’s weapon\n14\nvoid\nTakeDamage( ... ); //\ntake\nsome\nhealth\noff , maybe\nanimate\nfor\nthe\ndamage\nreaction\n15\nvoid\nSpeak( ... ); //\ncause\nthe\nplayer\nto\nstart\naudio /anim\n16\nvoid\nSetControllable ( bool ); // no\ncontrol\nin cut - scene\n17\nvoid\nSetVisible( bool ); //\nhide\nwhen\nloading / streaming\n18\nvoid\nSetModel( ... ); //\ninit\nstreaming\nthe\nmeshes\netc\n19\nbool\nIsReadyForRender ();\n20\nvoid\nRender (); // put\nthis\nin the\nrender\nqueue\n2GPG:DG uses GO or Game-Objects, but we stick with the term entity\nbecause it has become the standard term.\n",
      "content_length": 2381,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "4.1. COMPONENTS IN THE WILD\n87\n21\nbool\nIsControllable (); //\nplayer\ncan\nmove\nabout ?\n22\nbool\nIsAiming (); // in\nnormal\nmove -mode , or aim -mode\n23\nbool\nIsClimbing ();\n24\nbool\nInWater (); // if the\nroot\nbone\nis\nunderwater\n25\nbool\nIsFalling ();\n26\nvoid\nSetBulletCount ( int ); //\nreload\nis\n-1\n27\nvoid\nAddItem( ... ); //\ninventory\nitems\n28\nvoid\nUseItem( ... );\n29\nbool\nHaveItem( ... );\n30\nvoid\nAddXP( int ); // not\nreally XP , but\nused\nto\nindicate\nwhen\nwe let\nthe\nplayer\npower -up\n31\nint\nGetLevel (); // not\nreally\nlevel , power -up\ncount\n32\nint\nGetNumPowerups (); // how\nmany\nwe’ve\nused\n33\nfloat\nGetPlayerSpeed (); // how\nfast\nthe\nplayer\ncan go\n34\nfloat\nGetJumpHeight ();\n35\nfloat\nGetStrength (); // for\nmelee\nattacks\nand\nclimb\nspeed\n36\nfloat\nGetDodge (); //\navoiding\nbullets\n37\nbool\nIsInBounds( Bound ); // in\ntrigger\nzone?\n38\nvoid\nSetGodMode( bool ); //\ncheater\n39\nprivate:\n40\nVec pos;\n41\nVec up , forward , right;\n42\nVec\nvelocity;\n43\nArray <ItemType > inventory;\n44\nfloat\nhealth;\n45\nint\ncontroller;\n46\nAnimID\nidleAnim;\n47\nAnimID\nshootAnim;\n48\nAnimID\nreloadAnim;\n49\nAnimID\nmovementAnim ;\n50\nAnimID\ncurrentAnimGoal ;\n51\nAnimID\ncurrentAnim;\n52\nint\nbulletCount;\n53\nfloat\nshotsPerSecond ;\n54\nfloat\ntimeSinceLastShot ;\n55\nSoundHandle\nplayingSoundHandle ; //\nnull\nmost\nof the\ntime\n56\nbool\ncontrollable ;\n57\nbool\nvisible;\n58\nAssetID\nplayerModel ;\n59\nLocomotionType\ncurrentLocomotiveModel ;\n60\nint xp;\n61\nint\nusedPowerups ;\n62\nint SPEED , JUMP , STRENGTH , DODGE;\n63\nbool\ncheating;\n64\n};\nListing 4.1: Monolithic Player class\nThis example class includes many of the types of things\nfound in games, where the codebase has grown organically.\nIt’s common for the Player class to have lots of helper func-\ntions to make writing game code easier.\nHelper functions\ntypically consider the Player as an instance in itself, from\ndata in save through to rendering on screen.\nIt’s not un-\nusual for the Player class to touch nearly every aspect of a\ngame, as the human player is the target of the code in the\nﬁrst place, the Player class is going to reference nearly every-\nthing too.\nAI characters will have similarly gnarly looking classes if\nthey are generalised rather than specialised. Specialising AI\nwas more commonplace when games needed to ﬁt in smaller\n",
      "content_length": 2244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "88\nCHAPTER 4. COMPONENT BASED OBJECTS\nmachines, but now, because the Player class has to interact\nwith many of them over the course of the game, they tend to\nbe uniﬁed into one type just like the player, if not the same\nas the player, to help simplify the code that allows them to\ninteract. As of writing, the way in which AI is diﬀerentiated\nis mostly by data, with behaviour trees taking the main stage\nfor driving how AI thinks about its world. Behaviour trees are\nanother concept subject to various interpretations, so some\nforms are data-oriented design friendly, and others are not.\n4.2\nAway from the hierarchy\nA recurring theme in articles and post-mortems from people\nmoving from object-oriented hierarchies of gameplay classes\nto a component based approach is the transitional states of\nturning their classes into containers of smaller objects, an\napproach often called composition.\nThis transitional form\ntakes an existing class and ﬁnds the boundaries between\nconcepts internal to the class and attempts to refactor them\nout into new classes which can be owned or pointed to by\nthe original class. From our monolithic player class, we can\nsee there are lots of things that are not directly related, but\nthat does not mean they are not linked together.\nObject-oriented hierarchies are is-a relationships, and\ncomponents and composition oriented designs are tradition-\nally thought of as has-a relationships. Moving from one to\nthe other can be thought of as delegating responsibility or\nmoving away from being locked into what you are, but hav-\ning a looser role and keeping the specialisation until further\ndown the tree. Composition clears up most of the common\ncases of diamond inheritance issues, as capabilities of the\nclasses are added by accretion as much as they are added\nby overriding.\nThe ﬁrst move we need to make will be to take related\npieces of our monolithic class and move them into their own\n",
      "content_length": 1920,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "4.2. AWAY FROM THE HIERARCHY\n89\nclasses, along the lines of composing, changing the class\nfrom owning all the data and the actions that modify the data\ninto having instances which contain data and delegating ac-\ntions down into those specialised structures where possible.\nWe move the data out into separate structures so they can\nbe more easily combined into new classes later. We will ini-\ntially only separate by categories we perceive as being the\nboundaries between systems. For example, we separate ren-\ndering from controller input, from gameplay details such as\ninventory, and we split out animation from all.\nTaking a look at the results of splitting the player class\nup, such as in listing 4.2, it’s possible to make some initial\nassessments of how this may turn out. We can see how a\nﬁrst pass of building a class out of smaller classes can help\norganise the data into distinct, purpose oriented collections,\nbut we can also see the reason why a class ends up being a\ntangled mess. When you think about the needs of each of\nthe pieces, what their data requirements are, the coupling\ncan become evident. The rendering functions need access\nto the player’s position as well as the model, and the game-\nplay functions such as Shoot(Vec target) need access to the\ninventory as well as setting animations and dealing dam-\nage. Taking damage will need access to the animations and\nhealth. Things are already seeming more diﬃcult to han-\ndle than expected, but what’s really happening here is that\nit’s becoming clear that code needs to cut across diﬀerent\npieces of data. With just this ﬁrst pass, we can start to see\nthat functionality and data don’t belong together.\n1\nstruct\nPlayerPhysical {\n2\nVec pos;\n3\nVec up , forward , right;\n4\nVec\nvelocity;\n5\n};\n6\nstruct\nPlayerGameplay {\n7\nfloat\nhealth;\n8\nint xp;\n9\nint\nusedPowerups ;\n10\nint SPEED , JUMP , STRENGTH , DODGE;\n11\nbool\ncheating;\n12\nfloat\nshotsPerSecond ;\n13\nfloat\ntimeSinceLastShot ;\n14\n};\n15\nstruct\nEntityAnim {\n16\nAnimID\nidleAnim;\n17\nAnimID\nshootAnim;\n18\nAnimID\nreloadAnim;\n19\nAnimID\nmovementAnim ;\n20\nAnimID\ncurrentAnimGoal ;\n21\nAnimID\ncurrentAnim;\n",
      "content_length": 2114,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "90\nCHAPTER 4. COMPONENT BASED OBJECTS\n22\nSoundHandle\nplayingSoundHandle ; //\nnull\nmost\nof the\ntime\n23\n};\n24\nstruct\nPlayerControl {\n25\nint\ncontroller;\n26\nbool\ncontrollable ;\n27\n};\n28\nstruct\nEntityRender {\n29\nbool\nvisible;\n30\nAssetID\nplayerModel ;\n31\n};\n32\nstruct\nEntityInWorld {\n33\nLocomotionType\ncurrentLocomotiveModel ;\n34\n};\n35\nstruct\nInventory {\n36\nArray <ItemType > inventory;\n37\nint\nbulletCount ;\n38\n};\n39\n40\nclass\nPlayer {\n41\npublic:\n42\nPlayer ();\n43\n~Player ();\n44\n// ...\n45\n// ...\nthe\nmember\nfunctions\n46\n// ...\n47\nprivate:\n48\nPlayerPhysical\nphsyical;\n49\nPlayerGameplay\ngameplay;\n50\nEntityAnim\nanim;\n51\nPlayerControl\ncontrol;\n52\nEntityRender\nrender;\n53\nEntityInWorld\ninWorld;\n54\nInventory\ninventory;\n55\n};\nListing 4.2: Composite Player class\nIn this ﬁrst step, we made the player class a container for\nthe components. Currently, the player has the components,\nand the player class has to be instantiated to make a player\nexist. To allow for the cleanest separation into components\nin the most reusable way, it’s worth attempting to move com-\nponents into being managed by managers, and not handled\nor updated by their entities. In doing this, there will also be\na beneﬁt of cache locality when we’re iterating over multiple\nentities doing related tasks when we move them away from\ntheir owners.\nThis is where it gets a bit philosophical. Each system has\nan idea of the data it needs in order to function, and even\nthough they will overlap, they will not share all data. Con-\nsider what it is that a serialisation system needs to know\nabout a character. It is unlikely to care about the current\nstate of the animation system, but it will care about inven-\ntory. The rendering system will care about position and an-\nimation, but won’t care about the current amount of ammo.\nThe UI rendering code won’t even care about where the player\n",
      "content_length": 1842,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "4.3. TOWARDS MANAGERS\n91\nis, but will care about inventory and their health and dam-\nage. This diﬀerence of interest is at the heart of why putting\nall the data in one class isn’t a good long-term solution.\nThe functionality of a class, or an object, comes from\nhow the internal state is interpreted, and how the changes\nto state over time are interpreted too. The relationship be-\ntween facts is part of the problem domain and could be called\nmeaning, but the facts are only raw data. This separation of\nfact from meaning is not possible with an object-oriented\napproach, which is why every time a fact acquires a new\nmeaning, the meaning has to be implemented as part of the\nclass containing the fact. Dissolving the class, extracting the\nfacts and keeping them as separate components, has given\nus the chance to move away from classes that instill perma-\nnent meaning at the expense of occasionally having to look\nup facts via less direct methods. Rather than store all the\npossibly associated data by meaning, we choose to only add\nmeaning when necessary. We add meaning when it is part\nof the immediate problem we are trying to solve.\n4.3\nTowards managers\n1\nclass\nRenderable {\n2\nvoid\nRenderUpdate () {\n3\nauto\npos = gPositionArray [index ];\n4\ngRenderer.AddModel( playerModel , pos );\n5\n}\n6\n};\n7\nclass\nRenderManager {\n8\nvoid\nUpdate () {\n9\ngRenderer.BeginFrame ();\n10\nfor( auto &renderable : renderArray ) {\n11\nrenderable. RenderUpdate ();\n12\n}\n13\ngRenderer. SubmitFrame ();\n14\n}\n15\n};\n16\nclass\nPhysicsManager {\n17\nvoid\nUpdate () {\n18\nfor( auto & physicsRequest : physicsRequestArray ) {\n19\nphysicalArray [ physicsRequest .index ]. UpdateValues (\nphysicsRequest .updateData );\n20\n}\n21\n// Run\nphysics\nsimulation\n22\nfor( auto &physical : physicalArray ) {\n23\npositionArray [physical.index ]. pos = physical.pos;\n24\n}\n25\n}\n26\n};\n27\nclass\nController {\n",
      "content_length": 1850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "92\nCHAPTER 4. COMPONENT BASED OBJECTS\n28\nvoid\nUpdate () {\n29\nPad pad = GetPad( controller );\n30\nif( pad.IsPressed( SHOOT ) ) {\n31\nif( inventoryArray [index ]. bulletCount\n> 0 )\n32\nanimRequest.Add( SHOOT_ONCE );\n33\n}\n34\n}\n35\n}\n36\n};\n37\nclass\nPlayerInventory {\n38\nvoid\nUpdate () {\n39\nif( inv. bulletCount\n== 0 ) {\n40\nif( animArray.contains( inv.index ) {\n41\nanim = animArray[ index ];\n42\nanim. currentAnim = RELOAD;\n43\ninventoryArray [index ]. bulletCount = 6;\n44\nanim. playingSoundHandle = PlaySound( GUNFIRE );\n45\n}\n46\n}\n47\n}\n48\n};\n49\nclass\nPlayerControl {\n50\nvoid\nUpdate () {\n51\nfor( auto &control : controlArray ) {\n52\ncontrol.Update ();\n53\n}\n54\nfor( auto &inv : inventoryArray )\n{\n55\ninv.Update ();\n56\n}\n57\n}\n58\n}\nListing 4.3: Manager ticked components\nAfter splitting your classes up into components, you\nmight ﬁnd your classes look more awkward now they are\naccessing variables hidden away in new structures. But it’s\nnot your classes that should be looking up variables, but in-\nstead transforms on the classes. A common operation such\nas rendering requires the position and the model informa-\ntion, but it also requires access to the renderer. Such object\nboundary crossing access is seen as a compromise during\ngame development, but here it can be seen as the method\nby which we move away from a class-centric approach to a\ndata-oriented approach.\nWe will aim at transforming our\ndata into render requests which aﬀect the graphics pipeline\nwithout referring to data unimportant to the renderer.\nReferring to listing 4.3, we move to no longer having a\nplayer update, but instead an update for each component\nthat makes up the player. This way, everyone entity’s physics\nis updated before it is rendered, or could be updated while\nthe rendering is happening on another thread. All entity’s\ncontrols (whether they be player or AI) can be updated be-\n",
      "content_length": 1855,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "4.3. TOWARDS MANAGERS\n93\nfore they are animated. Having the managers control when\nthe code is executed is a large part of the leap towards fully\nparallelisable code. This is where performance can be gained\nwith more conﬁdence that it’s not negatively impacting other\nareas.\nAnalysing which components need updating every\nframe, and which can be updated less frequently leads to\noptimisations that unlock components from each other.\nIn many component systems that allow scripting lan-\nguages to deﬁne the actions taken by components or their\nentities, performance can fall foul of the same ineﬃciencies\npresent in an object-oriented program design. Notably, the\ndependency inversion practice of calling Tick or Update func-\ntions will often have to be sandboxed in some way which will\nlead to error checking and other safety measures wrapping\nthe internal call. There is a good example of this being an\nissue with the older versions of Unity, where their compo-\nnent based approach allowed every instance to have its own\nscript which would have its own call from the core of Unity\non every frame. The main cost appeared to be transitioning\nin and out of the scripting language, crossing the boundary\nbetween the C++ at the core, and the script that described\nthe behaviour of the component. In his article 10,000 Up-\ndate() calls[?], Valentin Simonov provided information on\nwhy the move to managers makes so much sense, giving\ndetails on what is costing the most when utilising depen-\ndency inversion to drive your general code update strategies.\nThe main cost was in moving between the diﬀerent areas\nof code, but even without having to straddle the language\nbarrier, managers make sense as they ensure updates to\ncomponents happen in sync.\nWhat happens when we let more than just the player use\nthese arrays?\nNormally we’d have some separate logic for\nhandling player ﬁre until we refactored the weapons to be\ngeneric weapons with NPCs using the same code for weapons\nprobably by having a new weapon class that can be pointed\nto by the player or an NPC, but instead what we have here\nis a way to split oﬀthe weapon ﬁring code in such a way as\n",
      "content_length": 2145,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "94\nCHAPTER 4. COMPONENT BASED OBJECTS\nto allow the player and the NPC to share ﬁring code without\ninventing a new class to hold the ﬁring. In fact, what we’ve\ndone is split the ﬁring up into the diﬀerent tasks it really\ncontains.\nTasks are good for parallel processing, and with compo-\nnent based objects, we open up the opportunity to move most\nof our previously class oriented processes out, and into more\ngeneric tasks that can be dished out to whatever CPU or co-\nprocessor can handle them.\n4.4\nThere is no entity\nWhat happens when we completely remove the Player class?\nIf an entity can be represented by its collection of compo-\nnents, does it need any further identity than those self same\ncomponents? Like the values in the rows of a table, the com-\nponents describe a single instance, but also like the rows in\na table, the table is also a set. In the universe of possibilities\nof component combinations, the components which make up\nthe entity are not facts about the entity, but are the entity,\nand are the only identity the entity needs. As an entity is\nits current conﬁguration of components, then there is the\npossibility of removing the core Player class completely. Re-\nmoving this class can mean we no longer think of the player\nas being the centre of the game, but because the class no\nlonger exists, it means the code is no longer tied to a speciﬁc\nsingular entity. Listing 4.4 shows a rough example of how\nyou might develop this kind of setup.\n1\nstruct\nOrientation { Vec pos , up , forward , right; };\n2\nSparseArray <Orientation > orientationArray ;\n3\nSparseArray <Vec > velocityArray ;\n4\nSparseArray <float > healthArray;\n5\nSparseArray <int > xpArray , usedPowerupsArray , controllerID ,\nbulletCount ;\n6\nstruct\nAttributes { int SPEED , JUMP , STRENGTH , DODGE; };\n7\nSparseArray <Attributes > attributeArray ;\n8\nSparseArray <bool > godmodeArray , controllable , isVisible;\n9\nSparseArray <AnimID > currentAnim , animGoal;\n10\nSparseArray <SoundHandle > playingSound ;\n11\nSparseArray <AssetID > modelArray;\n12\nSparseArray <LocomotionType > locoModelArray ;\n13\nSparseArray <Array <ItemType > > inventoryArray ;\n14\n15\nint\nNewPlayer( int padID , Vec\nstartPoint ) {\n",
      "content_length": 2179,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "4.4. THERE IS NO ENTITY\n95\n16\nint ID = newID ();\n17\ncontrollerID [ ID ] padID;\n18\nGetAsset( \" PlayerModel \", ID ); //\nadds a request\nto put\nthe\nplayer\nmodel\ninto\nmodelArray [ID]\n19\norientationArray [ ID ] = Orientation( startPoint );\n20\nvelocityArray [ ID ] = VecZero ();\n21\nreturn\nID;\n22\n}\nListing 4.4: Sparse arrays for components\nMoving away from compile-time deﬁned classes means\nmany other classes can be invented without adding much\ncode. Allowing scripts to generate new classes of entity by\ncomposition or prototyping increases their power dramati-\ncally, and cleanly increase the apparent complexity of the\ngame without adding more actual complexity. Finally, all the\ndiﬀerent entities in the game will now run the same code at\nthe same time, which simpliﬁes and centralises your pro-\ncessing code, leading to more opportunity to share optimi-\nsations, and fewer places for bugs to hide.\n",
      "content_length": 897,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "96\nCHAPTER 4. COMPONENT BASED OBJECTS\n",
      "content_length": 38,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "Chapter 5\nHierarchical Level of\nDetail and\nImplicit-state\nConsoles and graphics cards are not generally bottlenecked\nat the polygon rendering stage in the pipeline. Usually, they\nare bandwidth bound.\nIf there is a lot of alpha blending,\nit’s often ﬁll-rate issues. For the most part, graphics chips\nspend a lot of their time reading textures, and texture band-\nwidth often becomes the bottleneck. Because of this, the old\nway of doing level of detail with multiple meshes with de-\ncreasing numbers of polygons is never going to be as good\nas a technique which takes into account the actual data re-\nquired of the level of detail used in each renderable. The vast\nmajority of stalls when rendering come from driver side pro-\ncessing, or from processing too much for what you want to\nactually render. Hierarchical level of detail can ﬁx the prob-\nlem of high primitive count which causes more driver calls\nthan necessary.\nThe basic approach for art is to make optimisations by\ngrouping and merging many low level of detail meshes into\n97\n",
      "content_length": 1036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "98\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\none single low level of detail mesh. This reduces the time\nspent in the setup of render calls which is beneﬁcial in sit-\nuations where driver calls are costly. In a typical very large\nscale environment, a hierarchical level of detail approach to\ngame content can reduce the workload on a game engine by\nan order of magnitude as the number of entities in the scene\nconsidered for processing and rendering drops signiﬁcantly.\nEven though the number of polygons rendered may be\nexactly the same, or maybe even higher, the fact that the\nengine usually is only handling roughly the same number of\nentities at once on average increases stability and allows for\nmore accurately targeted optimisations of both art and code.\n5.1\nExistence from Null to Inﬁnity\nIf we consider that entities can be implicit based on their at-\ntributes, we can utilise the technique of hierarchical level of\ndetail to oﬀer up some optimisations for our code. In tra-\nditional level of detail techniques, as we move further away\nfrom the object or entity of interest, we lose details and ﬁ-\ndelity.\nWe might reduce polygon count, or texture sizes,\nor even the number of bones in a skeleton that drives the\nskinned mesh. Game logic can also degrade. Moving away\nfrom an entity, it might switch to a much coarser grain time\nstep. It’s not unheard of for behaviours of AI to migrate from\na 50hz update to a 1hz update. In a hierarchical level of de-\ntail implementation, as the entity becomes closer, or more\napparent to the player, it might be that only at that point\ndoes it even begin to exist.\nConsider a shooter game where you are defending a base\nfrom incoming attacks. You are manning an anti-air turret,\nand the attackers come in squadrons of aircraft, you can see\nthem all coming at once, over ten thousand aircraft in all,\nand up to a hundred at once in each squadron. You have to\nshoot them down or be inundated with gunﬁre and bombs,\n",
      "content_length": 1956,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "5.1. EXISTENCE\n99\ntaking out both you and the base you are defending.\nRunning full AI, with swarming for motion and avoidance\nfor your slower moving ordnance might be too much if it was\nrun on all ten thousand ships every tick, but you don’t need\nto. The basic assumption made by most AI programmers is\nthat unless they are within attacking range, then they don’t\nneed to be running AI. This is true and oﬀers an immediate\nspeedup compared to the na¨ıve approach. Hierarchical LOD\nprovides another way to think about this, by changing the\nnumber of entities based on how they are perceived by the\nplayer. For want of a better term, collective lodding is a name\nthat describes what is happening behind the scenes a little\nbetter. Sometimes there is no hierarchy, and yet, there can\nstill be a change in the manner in which the elements are\nreferenced between the levels of detail. The term collective\nlodding is inspired by the concept of a collective term.\nA\nmurder of crows is a computational element, but each crow\nis a lower level of detail sub-element of the collective.\nMurder\ny\n\u000f\n%\nCrow\nCrow\nCrow\nIn the collective lodding version of the base defender\ngame, there are a few wave entities which project squadron\nblips on the radar. The squadrons don’t exist as their own\nentities until they get close enough. Once a wave’s squadron\nis within range, the wave will decrease its squadron count\nand pop out a new squadron entity.\nThe newly created\nsquadron entity shows blips on the radar for each of its\ncomponent aircraft. The aircraft don’t exist yet, but they are\nimplicit in the squadron in the same way the squadron was\nimplicit in the wave. The wave continues to pop Squadrons\nas they come into range, and once its internal count has\ndropped to zero, it can delete itself as it now represents no\nentities.\nAs a squadron comes into even closer range, it\npops out its aircraft into their own entities and eventually\n",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "100\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\ndeletes itself. As the aircraft get closer, traditional level of\ndetail techniques kick in and their renderables are allowed\nto switch to higher resolution and their AI is allowed to run\nat a higher intelligence setting.\nBlip\nw\n\u000f\n'\nSquadron\nSquadron\nw\n\u000f\n'\nSquadron\nAircraft\nAircraft\nw\n\u000f\n'\nAircraft\nEjectingPilot\nFuselage\nWing\nWhen the aircraft are shot at, they switch to a taken dam-\nage type. They are full health enemy aircraft unless they take\ndamage. If an AI reacts to damage with fear, they may eject,\nadding another entity to the world. If the wing of the plane\nis shot oﬀ, then that also becomes a new entity in the world.\nOnce a plane has crashed, it can delete its entity and replace\nit with a smoking wreck entity that will be much simpler to\nprocess than an aerodynamic simulation, faked or not.\nIf things get out of hand and the player can’t keep the\naircraft at bay and their numbers increase in size so much\nthat any normal level of detail system can’t kick in to miti-\ngate it, collective lodding can still help by returning aircraft\nto squadrons and ﬂying them around the base attacking as a\ngroup, rather than as individual aircraft. In the board game\nWarhammer Fantasy Battle, there were often so many troops\nﬁring arrows at each other, that players would often think\nof attacks by squads as being collections of attacks, and not\nactually roll for each individual soldier, rat, orc or whatever it\nwas, but instead counted up how many troops they had, and\nrolled that many dice to see how many attacks got through.\nThis is what is meant by attacking as a squadron. The air-\n",
      "content_length": 1637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "5.2. MEMENTOS\n101\ncraft no longer attack, instead, the likelihood an attack will\nsucceed is calculated, dice are rolled, and that many attacks\nget through.\nThe level of detail heuristic can be tuned so\nthe nearest and front-most squadron are always the high-\nest level of detail, eﬀectively making them roll individually,\nand the ones behind the player maintain a very simplistic\nrepresentation.\nThis is game development smoke and mirrors as a ba-\nsic game engine element. In the past we have reduced the\nnumber of concurrent attacking AI1, reduced the number of\ncars on screen by staggering the lineup over the whole race\ntrack2, and we’ve literally combined people together into one\nperson instead of having loads of people on screen at once3.\nThis kind of reduction of processing is commonplace. Now\nconsider using it everywhere appropriate, not just when a\nplayer is not looking.\n5.2\nMementos\nReducing detail introduces an old problem, though. Chang-\ning level of detail in game logic systems, AI and such, brings\nwith it the loss of high detail history. In this case, we need\na way to store what is needed to maintain a highly cohesive\nplayer experience. If a high detail squadron in front of the\nplayer goes out of sight and another squadron takes their\nplace, we still want any damage done to the ﬁrst group to\nreappear when they come into sight again. Imagine if you\nhad shot out the glass on all the aircraft and when they came\nround again, it was all back the way it was when they ﬁrst\narrived. A cosmetic eﬀect, but one that is jarring and makes\nit harder to suspend disbelief.\nWhen a high detail entity drops to a lower level of de-\n1I believe this was Half-Life\n2Ridge Racer was known for this\n3Populous did this\n",
      "content_length": 1726,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "102\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\ntail, it should store a memento, a small, well-compressed\nnugget of data that contains all the necessary information\nin order to rebuild the higher detail entity from the lower de-\ntail one.\nWhen the squadron drops out of sight, it stores\na memento containing compressed information about the\namount of damage, where it was damaged, and rough posi-\ntions of all the aircraft in the squadron. When the squadron\ncomes into view once more, it can read this data and gener-\nate the high detail entities back in the state they were before.\nLossy compression is ﬁne for most things, it doesn’t matter\nprecisely which windows, or how they were cracked, maybe\njust that about 2/3 of the windows were broken.\nHighDetail\nstore\n'\nHighDetail\nMemento\nextract\n7\nAnother example is in a city-based free-roaming game. If\nAIs are allowed to enter vehicles and get out of them, then\nthere is a good possibility you can reduce processing time\nby removing the AIs from the world when they enter a ve-\nhicle. If they are a passenger, then they only need enough\ninformation to rebuild them and nothing else.\nIf they are\nthe driver, then you might want to create a new driver type\nbased on some attributes of the pedestrian before making\nthe memento for when they exit the vehicle.\nIf a vehicle reaches a certain distance away from the\nplayer, then you can delete it.\nTo keep performance high,\nyou can change the priorities of vehicles that have memen-\ntos so they try to lose sight of the player thus allowing for\nearlier removal from the game. Optimisations like this are\nhard to coordinate in object-oriented systems as internal in-\nspection of types isn’t encouraged. Some games get around\nit by designing in ways to reset memento data as a gameplay\nelement. The game Zelda: Breath of the Wild resets mon-\nsters during a Blood Moon, and by doing so, you as a player,\n",
      "content_length": 1892,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "5.3. JIT MEMENTOS\n103\nare not surprised when you return to camps to ﬁnd all the\nmonsters are just as you left them.\n5.3\nJIT mementos\nIf a vehicle that has been created as part of the ambient pop-\nulation is suddenly required to take on a more important\nrole, such as the car being involved in a ﬁreﬁght, it needs\nto gain detail. This detail must come from somewhere and\nmust be convincing. It is important to generate new entities\nwhich don’t seem overly generic or unlikely, given what the\nplayer knows about the game so far. Generating that data\ncan be thought of as providing a memento to read from just\nin time.\nJust in time mementos, or JIT mementos, oﬀers\na way to create fake mementos that can provide continuity\nby utilising pseudo-random generators or hash functions to\ncreate suitable information on demand without relying on\nstoring data anywhere. Instead, they rely only on informa-\ntion provided implicitly by the entity in need of it.\nInstead of generating new characters from a global ran-\ndom number generator, it is possible to seed the generator\nwith details about the thing that needs generation. For ex-\nample, you want to generate a driver and some passengers,\nas you’re about to get close enough to a car to need to render\nthe people inside it. Just creating random characters from\na set of lookup tables is good, but if you drive past them far\nenough for them to get out of rendering range, and then re-\nturn, the people in the car might not look the same anymore\nas they had to be regenerated. Instead, generate the driver\nand passengers using some other unique attribute, such as\nthe license plate, as a seed. This way, while you have not\naﬀected the result of generating the memento, you have no\nmemory overhead to store it, and no object lifetime to worry\nabout either, as it can always be reproduced from nothing\nagain.\n",
      "content_length": 1848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "104\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\nV ehicle\nseed\nv\nPassengerStub\nseed\n(\n+3 Character\nMemento\nextract\n7\nThis technique is used all the time in landscape genera-\ntors, where the landscape is seeded from the x,y location in\nthe map, so why not use it when generating the weather for\nday 107 of the game? When generating Perlin noise, many\nalgorithms call upon a noise function, but to have a repro-\nducible landscape, the noise function must be a repeatable\nfunction, so it can create the same results over and over\nagain. If you’re generating a landscape, it’s preferred for the\nnoise function to be coherent, that is, for small variances in\nthe input function, only small changes should be observed\nin the output. We don’t need such qualities when generating\nJIT mementos, and a hash function which varies wildly with\neven the smallest change in the input will suﬃce.\nAn example of using this to create a JIT memento might\nbe to generate a house for a given landscape.\nFirst, take\nany normal random number generator and seed it with the\nlocation of the building. Given the landscape the house is on,\nselect from a building template and start generating random\nnumbers to answer questions about the house the same way\nloading a ﬁle oﬀdisk answers questions about the object.\nHow large is the house? Is it small, medium, large? Generate\na random number and select one answer. How many rooms\ndoes it have based on the size? 2 or 3 for small, or (int)(7\n+ rand * 10) for large. The point is, once you have seeded\nthe random number generator, you’re going to get the same\nresults back every time you run through the same process.\nEvery time you visit the house at {223.17,-100.5}, you’re\ngoing to see the same 4 (or more) walls, and it will have the\nsame paint job, broken windows, or perfect idyllic little frog\n",
      "content_length": 1816,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "5.3. JIT MEMENTOS\n105\npond in the back garden.\nJIT mementos can be the basis of a highly textured en-\nvironment with memento style sheets or style guides which\ncan direct a feel bias for any mementos generated in those\nvirtual spaces. Imagine a city style guide that speciﬁes rules\nfor occupants of cars. The style guide might claim that busi-\nnessmen might share, but are much less likely to, that fami-\nlies have children in the back seats with an older adult driv-\ning. It might declare that young adults tend to drive around\nin pairs. Style guides help add believability to any generated\ndata. Add in local changes such as having types of car linked\nto types of drivers. Have convertibles driven by well-dressed\ntypes or kids, low riders driven almost exclusively by their\nstereotypical owner, and imports and modded cars driven by\nyoung adults. In a space game, dirty hairy pilots of cargo\nships, well turned out oﬃcers commanding yachts, rough\nand ready mercenaries in everything from a single seater to\na dreadnought. Then, once you have the ﬂavour in place,\nallow for a little surprise to bring it to life fully.\nJIT mementos are a good way to keep the variety up, and\nstyle guides bias that so it comes without the impression that\neveryone is diﬀerent so everyone is the same. When these\nbiases are played out without being strictly adhered to, you\ncan build a more textured environment. If your environment\nis heavily populated with completely diﬀerent people all the\ntime, there is nothing to hold onto, no patterns to recognise.\nWhen there are no patterns, the mind tends to see noise\nor consider it to be a samey soup.\nEven the most varied\nvirtual worlds look bland when there is too much content\nall in the same place. Walk along the street and see if you\ncan spot any identical paving slabs. You probably can, but\nalso see the little bits of damage, decay, dirt, mistakes, and\nblemishes. To make an environment believable, you have to\nmake it look like someone took a lot of eﬀort trying to make\nit all conform.\n",
      "content_length": 2027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "106\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\n5.4\nAlternative axes\nAs with all things, take away an assumption and you can\nﬁnd other uses for a tool. Whenever you read about, or work\nwith a level of detail system, you will be aware that the con-\nstraint on what level of detail is shown has always been some\ndistance function in space. It’s now time to take the assump-\ntion, discard it, and analyse what is really happening.\nFirst, we ﬁnd that if we take away the assumption of dis-\ntance, we can infer the conditional as some kind of linear\nmeasure. This value normally comes from a function which\ntakes the camera position and ﬁnds the relative distance to\nthe entity under consideration. What we may also realise\nwhen discarding the distance assumption is a more funda-\nmental understanding of what we are trying to do. We are\nusing a single runtime variable to control the presentation\nstate of the entities of our game. We use runtime variables\nto control the state of many parts of our game already, but\nin this case, there is a passive presentation response to the\nvariable, or axis being monitored. The presentation is usu-\nally some graphical, or logical level of detail, but it could be\nsomething as important to the entity as its own existence.\n5.4.1\nThe true measure\nDistance is the measure we normally use to identify what\nlevel of detail something should be at, but it’s not the met-\nric we really need, it’s just very closely related. In fact, it’s\ninversely related. The true metric of level of detail should\nbe how much of our perception an entity is taking up.\nIf\nan entity is very large, and far away, it takes up as much of\nour perception as something small and nearby. All this time\nwe have talked about hierarchical level of detail the elephant\nin the room has been the language used. We had waves on\nour radar. They took up as much perception attention as\na single squadron, and a single squadron took up as much\n",
      "content_length": 1935,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "5.4. ALTERNATIVE AXES\n107\nperceptual space as a single aircraft when it was in ﬁring\nrange.\nUnderstand this concept: level of detail should be deﬁned\nby how the player perceives a thing, at the range it is at.\nIf you internalise this, you will be on your way to making\ngood decisions about where the boundaries are between your\nlevels of detail.\n5.4.2\nBeyond space\nLet’s now consider what other variables we can calculate\nthat present an opportunity to remove details from the\ngame’s representation. We should consider anything which\npresents an opportunity to no longer process data unneces-\nsarily. If some element of a game is not the player’s current\nconcern, or will fade from memory soon enough, we can\ndissolve it away. If we consider the probability of the player\ncaring about a thing as a metric, then we begin to think\nabout recollection and attention as measurable quantities\nwe can use to drive how we end up representing it.\nAn entity that you know has the player’s attention, but is\nhidden, maintains a large stake on the player’s perception.\nThat stake allows the entity to maintain a higher priority on\nlevel of detail than it would otherwise deserve. For example,\na character the player is chasing in an assassination game,\nmay be spotted only once at the beginning of the mission,\nbut will have to remain at a high consistency of attribute\nthroughout the mission, as they are the object the player\ncares about the most, coming second only to primitive needs\nsuch as survival. Even if the character slips into the crowd,\nand is not seen again until much later, they must look just\nlike they did when you ﬁrst caught sight of them.\nAsk the question, how long until a player forgets about\nsomething that might otherwise be important? This infor-\nmation will help reduce memory usage as much as distance.\n",
      "content_length": 1819,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "108\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\nIf you have ever played Grand Theft Auto IV, you might have\nnoticed that the cars can disappear just by not looking at\nthem. As you turn around a few times you might notice the\ncars seem to be diﬀerent each time you face their way. This\nis a stunning use of temporal level of detail. Cars which have\nbeen bumped into or driven and parked by the player remain\nwhere they were, because, in essence, the player put them\nthere. Because the player has interacted with them, they\nare likely to remember they are there. However, ambient ve-\nhicles, whether they are police cruisers or civilian vehicles,\nare less important and don’t normally get to keep any special\nstatus so can vanish when the player looks away.\nAt the opposite end of the scale, some games remem-\nber everything you have done. Kill enemies in the ﬁrst few\nminutes of your game, loot their corpses, and chuck items\naround, then come back a hundred hours later and the items\nare still wherever you left them. Games like this store vast\namounts of tiny details, and these details need careful stor-\nage otherwise they would cause continual and crushing per-\nformance degradation.\nUsing spatially mapped mementos\nis one approach that can attempt to rationalise this kind of\nlevel of attention to player game interaction.\nIn addition to time-since-seen, some elements may base\ntheir level of detail on how far a player has progressed in\nthe game, or how many of something a player has, or how\nmany times they have done it. For example, a typical barter-\ning animation might be cut shorter and shorter as the game\nuses the axis of how many recent barters to draw back the\nlength of any non-interactive sections which could be caused\nby the event. This can be done simply, and the player will be\nthankful. Consider allowing multi-item transactions only af-\nter a certain number of single transactions have happened.\nIn eﬀect, you could set up gameplay elements, reactions to\nsituations, triggers for tutorials, reminders, or extensions to\ngameplay options all through these abstracted level of de-\ntail style axes. Handling the idea of player expertise through\naxes of level of detail of gameplay mechanic depth or com-\n",
      "content_length": 2220,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "5.4. ALTERNATIVE AXES\n109\nplexity.\nThis way of manipulating the present state of the game is\nsafer from transition errors. These are errors that happen\nbecause going from one state to another may have set some-\nthing to true when transitioning one direction, but might not\nset it back to false when transitioning the other way. You can\nthink of the states as being implicit on the axis. When state\nis modiﬁed, it’s prone to being modiﬁed incorrectly, or not\nmodiﬁed at the right time. If state is tied to other variables,\nthat is, if state is a function of other state, then it’s less prone\nto inconsistency.\nAn example of where transition errors occur is in menu\nsystems where all transitions should be reversible, some-\ntimes you may ﬁnd that going down two levels of menu, but\nback only one level, takes you back to where you started. For\nexample, entering the options menu, then entering an adjust\nvolume slider, but backing out of the slider might take you\nout of the options menu altogether. These bugs are common\nin UI code as there are large numbers of diﬀerent layers of\ninteraction. Player input is often captured in obscure ways\ncompared to gameplay input response. A common problem\nwith menus is one of ownership of the input for a partic-\nular frame. For example, if a player hits both the forward\nand backward button at the same time, a state machine UI\nmight choose to enter whichever transition response comes\nﬁrst.\nAnother might manage to accept the forward event,\nonly to have the next menu accept the back event, but worst\nof all might be the unlikely, but seen in the wild, menu tran-\nsitioning to two diﬀerent menus at the same time. Some-\ntimes the menu may transition due to external forces, and if\nthere is player input captured in a diﬀerent thread of execu-\ntion, the game state can become disjoint and unresponsive.\nConsider a network game’s lobby, where if everyone is ready\nto play, but the host of the game disconnects while you are\nentering into the options screen prior to game launch, in\na traditional state-machine like approach to menus, where\nshould the player return to once they exit the options screen?\n",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "110\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\nThe lobby would normally have dropped you back to a server\nsearch screen, but in this case, the lobby has gone away to\nbe replaced with nothing. This is where having simple axes\ninstead of state machines can prove to be simpler to the point\nof being less buggy and more responsive.\n5.5\nCollective lodding - or how to re-\nduce your instance count.\nIt’s an ugly term, and I hope one day someone comes up with\na better one, but it’s a technique that didn’t need a name un-\ntil people stopped doing it. Over the time it has taken to write\nthis book, games have started to have too many instances.\nWe’re not talking about games that have hundreds of en-\nemy spacecraft, battling each other in a desperate ﬁght for\nsuperiority, ﬁring oﬀmissile after missile, generating visual\neﬀects which spawn multiple GPU particles. We’re talking\nabout simple seeming games. We’re talking about your aver-\nage gardening simulator, where for some reason, every leaf\non your plants is modeled as an instance, and every insect\ngoing around pollinating is an instance, and every plot of\nland in which your plants can grow is an instance, and ev-\nery seed you sew is an instance, and each have their own\nlifetimes, components, animations, and their own internal\nstate adding to the ever-growing complexity of the system as\na whole.\nI have a ﬁctional farming game, where I harvest wheat. I\nhave a ﬁeld which is 100 by 100 tiles, each with wheat grow-\ning. In some games, those wheat tiles would be instances,\nand the wheat on the tiles would be instances too. There’s\nlittle reason for this, as we can reduce the ﬁeld down to some\nvery small data. What do we actually need to know about the\nﬁeld and the wheat? Do we need to know the position of the\nwheat? We don’t, because it’s in a tiled grid. Do we need to\nknow if the tile has wheat or not? Yes, but it doesn’t need\n",
      "content_length": 1890,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "5.5. COLLECTIVE LOD\n111\nan object instance to tell us that. Do we need an object to\nrender the wheat?\nIt needs to blow in the wind, so don’t\nwe need to have it keep track of where it is to blow around\nand maintain momentum? No, because in almost all cases,\ncheating at this kind of thing is cheap and believable. Grass\nrendering works ﬁne without an instance per blade of grass.\nThe right data format for a ﬁeld full of wheat could be as\nsimple as 10,000 unsigned chars, with zero being no wheat,\nand values from 1 to 100 being how grown it is. The wheat\ndoesn’t have positions. The positions have wheat.\nIf you have a stack of blocks in Minecraft, you don’t have\n64 instances in your inventory slot, you just have a type, and\na multiple. You have a stack. If you have a stack of plates\nin a restaurant sim, you don’t have 10 plate instances, you\nhave a stack of plates object with an int saying how many\nplates there currently are.\nThe underlying principle of this is making sure you have\nslots in the world, whether hand placed, or generated in a\npattern, and keeping track of what’s in them, rather than\nplacing things in the world directly. Refer to things by how a\nstranger would name them. When you ask someone what is\nin a room, they won’t say a sofa, a bookshelf, an armchair,\nanother armchair, a coﬀee table, a TV stand, more book-\nshelves. No, they will say furniture. Look at your game from\nthe outside. Use how the players describe what is on screen.\nLook at how they describe their inventory. Look at how they\ndescribe the game, understand their mental model, match\nthat, and you will ﬁnd a strong correlation to what is taking\nup the players perception space.\nWhen normalising your data, look at how your rows are\naligned to some kind of container. If you have any form of\ngrid, from 1D to 4D, it’s worth looking at how you can utilise\nit. Don’t ignore other tesselations, such as triangle grids,\nor hexagon grids. Hexagon grids, in particular, get a bad\nname, but they can be represented by a square grid with\ndiﬀerent traversal functions. Don’t give up just because the\n",
      "content_length": 2086,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "112\nCHAPTER 5. HIERARCHICAL LEVEL OF DETAIL\nliteral grid is irregular either, in some grid-based games, the\ncentres of the cells are perturbed to give a more natural look,\nbut the game code can be strict grid-based, leading to better\nsolution space, and more likely easier for the player to reason\nabout what they can and can’t do.\n",
      "content_length": 332,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "Chapter 6\nSearching\nWhen looking for speciﬁc data, it’s very important to remem-\nber why you’re doing it. If the search is not necessary, then\nthat’s your biggest possible saving. Finding if a row exists in\na table will be slow if approached na¨ıvely. You can manually\nadd searching helpers such as binary trees, hash tables, or\njust keep your table sorted by using ordered insertion when-\never you add to the table. If you’re looking to do the latter,\nthis could slow things down, as ordered inserts aren’t nor-\nmally concurrent, and adding extra helpers is normally a\nmanual task.\nIn this chapter, we ﬁnd ways to combat all\nthese problems.\n6.1\nIndexes\nDatabase management systems have long held the concept\nof an index.\nTraditionally, they were automatically added\nwhen a DBMS noticed a particular query had been run a\nlarge number of times. We can use this idea and implement\na just-in-time indexing system in our games to provide the\nsame kinds of performance improvement.\n113\n",
      "content_length": 981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "114\nCHAPTER 6. SEARCHING\nIn SQL, every time you want to ﬁnd out if an element\nexists, or even just generate a subset like when you need to\nﬁnd all the entities in a certain range, you will have to build\nit as a query. The query exists as an entity of a kind, and\nhelps build intuition into the DBMS.\nThe query that creates the row or table generation can be\nthought of as an object which can hang around in case it’s\nused again, and can transform itself depending on how it’s\nused over time. Starting out as a simple linear search query\n(if the data is not already sorted), the process can ﬁnd out\nthat it’s being used quite often through internal telemetry,\nand be able to discover that it generally returns a simply\ntunable set of results, such as the ﬁrst N items in a sorted\nlist. After some predeﬁned threshold number of operations,\nlifetime, or other metric, it would be valuable for the query\nobject to hook itself into the tables it references. Hooking\ninto the insertion, modiﬁcation, and deletion would allow the\nquery to update its answers, rather than run the full query\nagain each time it’s asked.\nThis kind of smart object is what object-oriented pro-\ngramming can bring to data-oriented design.\nIt can be a\nsigniﬁcant saving in some cases, but it can also be safe, due\nto its optionality.\nIf we build generalised backends to handle building\nqueries into these tables, they can provide multiple ben-\neﬁts. Not only can we expect garbage collection of indexes\nwhich aren’t in use, but they can also make the programs\nin some way self-documenting and self-proﬁling. If we study\nthe logs of what tables had pushed for building indexes for\ntheir queries, then we can see data hotspots and where there\nis room for improvement. It may even be possible to have\nthe code self-document what optimisation steps should be\ntaken.\n",
      "content_length": 1832,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "6.2. DATA-ORIENTED LOOKUP\n115\n6.2\nData-oriented Lookup\nThe ﬁrst step in any data-oriented approach to searching\nis to understand the diﬀerence between the search criteria,\nand the data dependencies of the search criteria. Object-\noriented solutions to searching often ask the object whether\nor not it satisﬁes some criteria. Because the object is asked\na question, there can be a lot of code required, memory in-\ndirectly accessed, and cache lines ﬁlled but hardly utilised.\nEven outside of object-oriented code-bases, there’s still a lot\nof poor utilisation of memory bandwidth. In listing 6.1, there\nis an example of simple binary search for a key in a na¨ıve im-\nplementation of an animation container. This kind of data\naccess pattern is common in animation libraries, but also in\nmany hand-rolled structures which look up entries that are\ntrivially sorted along an axis.\n1\nstruct\nFullAnimKey {\n2\nfloat\ntime;\n3\nVec3\ntranslation ;\n4\nVec3\nscale;\n5\nVec4\nrotation; //\nsijk\nquaternion\n6\n};\n7\nstruct\nFullAnim {\n8\nint\nnumKeys;\n9\nFullAnimKey *keys;\n10\nFullAnimKey\nGetKeyAtTimeBinary ( float t ) {\n11\nint l = 0, h = numKeys -1;\n12\nint m = (l+h) / 2;\n13\nwhile( l < h ) {\n14\nif( t < keys[m]. time ) {\n15\nh = m-1;\n16\n} else {\n17\nl = m;\n18\n}\n19\nm = (l+h+1) / 2;\n20\n}\n21\nreturn\nkeys[m];\n22\n}\n23\n};\nListing 6.1: Binary search through objects\nWe can improve on this very quickly by understanding\nthe dependence on the producer and the consumer of the\nprocess. Listing 6.2, is a quick rewrite that saves us a lot\nof memory requests by moving out to a partial structure-of-\narrays approach.\nThe data layout stems from recognising\nwhat data is needed to satisfy the requirements of the pro-\ngram.\n",
      "content_length": 1682,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "116\nCHAPTER 6. SEARCHING\nFirst, we consider what we have to work with as inputs,\nand then what we need to provide as outputs. The only input\nwe have is a time value in the form of a ﬂoat, and the only\nvalue we need to return in this instance is an animation key.\nThe animation key we need to return is dependent on data\ninternal to our system, and we are allowing ourselves the\nopportunity to rearrange the data any way we like. As we\nknow the input will be compared to the key times, but not\nany of the rest of the key data, we can extract the key times\nto a separate array. We don’t need to access just one part of\nthe animation key when we ﬁnd the one we want to return,\nbut instead, we want to return the whole key. Given that,\nit makes sense to keep the animation key data as an array\nof structures so we access fewer cache lines when returning\nthe ﬁnal value.\n1\nstruct\nDataOnlyAnimKey {\n2\nVec3\ntranslation ;\n3\nVec3\nscale;\n4\nVec4\nrotation; //\nsijk\nquaternion\n5\n};\n6\nstruct\nDataOnlyAnim {\n7\nint\nnumKeys;\n8\nfloat *keyTime;\n9\nDataOnlyAnimKey *keys;\n10\nDataOnlyAnimKey\nGetKeyAtTimeBinary ( float t ) {\n11\nint l = 0, h = numKeys -1;\n12\nint m = (l+h) / 2;\n13\nwhile( l < h ) {\n14\nif( t < keyTime[m] ) {\n15\nh = m-1;\n16\n} else {\n17\nl = m;\n18\n}\n19\nm = (l+h+1) / 2;\n20\n}\n21\nreturn\nkeys[m];\n22\n}\n23\n};\nListing 6.2: Binary search through values\nIt is faster on most hardware, but why is it faster? The\nﬁrst impression most people get is that we’ve moved the keys\nfrom nearby the returned data, ensuring we have another\nfetch before we have the chance to return. Sometimes it pays\nto think a bit further than what looks right at ﬁrst glance.\nLet’s look at the data layout of the AnimKeys.\n",
      "content_length": 1680,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "6.2. DATA-ORIENTED LOOKUP\n117\nt\ntx\nty\ntz\nsx\nsy\nsz\nrs\nri\nrj\nrk\nt\ntx\nty\ntz\nsx\ncacheline\nsy\nsz\nrs\nri\nrj\nrk\nt\ntx\nty\ntz\nsx\nsy\nsz\nrs\nri\nrj\ncacheline\nrk\nt\ntx\nty\ntz\nsx\nsy\nsz\nrs\nri\nrj\nrk\nt\n.\n.\n.\ncacheline\nPrimarily, the processing we want to be doing is all about\nﬁnding the index of the key by hunting for through values in\na list of times. In the extracted times code, we’re no longer\nlooking for a whole struct by one of its members in an array\nof structs. This is faster because the cache will be ﬁlled with\nmostly pertinent data during the hunt phase. In the orig-\ninal layout, we one or two key times per cache line. In the\nupdated code, we see 16 key times per cache line.\nt0\nt1\nt2\nt3\nt4\nt5\nt6\nt7\nt8\nt9\nt10\nt11\nt12\nt13\nt14\nt15\ncacheline\nThere are ways to organise the data better still, but any\nmore optimisation requires a complexity or space time trade\noﬀ. A basic binary search will home in on the correct data\nquite quickly, but each of the ﬁrst steps will cause a new\ncache line to be read in. If you know how big your cache\nline is, then you can check all the values that have been\nloaded for free while you wait for the next cache line to load\nin. Once you have got near the destination, most of the data\nyou need is in the cache and all you’re doing from then on\nis making sure you have found the right key. In a cache line\naware engine, all this can be done behind the scenes with\na well-optimised search algorithm usable all over the game\ncode. It is worth mentioning again, every time you break out\ninto larger data structures, you deny your proven code the\nchance to be reused.\nA binary search is one of the best search algorithms for\nusing the smallest number of instructions to ﬁnd a key value.\nBut if you want the fastest algorithm, you must look at what\ntakes time, and often, it’s not the instructions. Loading a\nwhole cache line of information and doing as much as you\n",
      "content_length": 1885,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "118\nCHAPTER 6. SEARCHING\ncan with that would be a lot more helpful than using the\nsmallest number of instructions. It is worth considering that\ntwo diﬀerent data layouts for an algorithm could have more\nimpact than the algorithm used.\nAs a comparison to the previous animation key ﬁnding\ncode, a third solution was developed which attempted to\nutilise the remaining cache line space in the structure. The\nstructure that contained the number of keys, and the two\npointers to the times and the key data, had quite a bit of\nspace left on the cache line. One of the biggest costs on the\nPS3 and Xbox360 was poor cache line utilisation, or CLU.\nIn modern CPUs, it’s not quite as bad, partially because the\ncache lines are smaller, but it’s still worth thinking about\nwhat you get to read for free with each request.\nIn this\nparticular case, there was enough cache line left to store\nanother 11 ﬂoating point values, which are used as a place\nto store something akin to skip-list.\ntimes\nkeys\nn\ns0\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns9\ns10\ncacheline\n1\nstruct\nClumpedAnim {\n2\nfloat *keyTime;\n3\nDataOnlyAnimKey *keys;\n4\nint\nnumKeys;\n5\nstatic\nconst\nint\nnumPrefetchedKeyTimes = (64- sizeof(int)-sizeof\n(float *)-sizeof( DataOnlyAnimKey *))/sizeof(float);\n6\nstatic\nconst\nint\nkeysPerLump = 64/ sizeof(float);\n7\nfloat\nfirstStage[ numPrefetchedKeyTimes ];\n8\nDataOnlyAnimKey\nGetKeyAtTimeLinear ( float t ) {\n9\nfor( int\nstart = 0; start < numPrefetchedKeyTimes ; ++ start )\n{\n10\nif( firstStage[start] > t ) {\n11\nint l = start* keysPerLump ;\n12\nint h = l + keysPerLump ;\n13\nh = h > numKeys ? numKeys : h;\n14\nreturn\nGetKeyAtTimeLinear ( t, l );\n15\n}\n16\n}\n17\nreturn\nGetKeyAtTimeLinear ( t, numPrefetchedKeyTimes *\nkeysPerLump );\n18\n}\n19\nDataOnlyAnimKey\nGetKeyAtTimeLinear ( float t, int\nstartIndex ) {\n20\nint i = startIndex;\n21\nwhile( i < numKeys ) {\n22\nif( keyTime[i] > t ) {\n23\n--i;\n24\nbreak;\n25\n}\n26\n++i;\n27\n}\n28\nif( i < 0 )\n29\nreturn\nkeys [0];\n30\nreturn\nkeys[i];\n31\n}\n32\n};\n",
      "content_length": 1947,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "6.2. DATA-ORIENTED LOOKUP\n119\nListing 6.3: Better cache line utilisation\nUsing the fact that these keys would be loaded into mem-\nory, we give ourselves the opportunity to interrogate some\ndata for free. In listing 6.3 you can see it uses a linear search\ninstead of a binary search, and yet it still manages to make\nthe original binary search look slow by comparison, and we\nmust assume, as with most things on modern machines, it\nis because the path the code is taking is using the resources\nbetter, rather than being better in a theoretical way, or using\nfewer instructions.\ni5-4430 @ 3.00GHz\nAverage 13.71ms [Full anim key - linear search]\nAverage 11.13ms [Full anim key - binary search]\nAverage\n8.23ms [Data only key - linear search]\nAverage\n7.79ms [Data only key - binary search]\nAverage\n1.63ms [Pre-indexed - binary search]\nAverage\n1.45ms [Pre-indexed - linear search]\nIf the reason for your search is simpler, such as checking\nfor existence, then there are even faster alternatives. Bloom\nﬁlters oﬀer a constant time lookup. Even though it produces\nsome false positives, it can be tweaked to generate a reason-\nable answer hit rate for very large sets. In particular, if you\nare checking for which table a row exists in, then bloom ﬁl-\nters work very well, by providing data about which tables to\nlook in, usually only returning the correct table, but some-\ntimes more than one.\nThe engineers at Google have used\nbloom ﬁlters to help mitigate the costs of something of a\nwrite-ahead approach with their BigTable technology[?], and\nuse bloom ﬁlters to quickly ﬁnd out if data requests should\nlookup their values in recent change tables, or should go\nstraight to the backing store.\nIn relational databases, indexes are added to tables at\nruntime when there are multiple queries that could beneﬁt\n",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "120\nCHAPTER 6. SEARCHING\nfrom their presence. For our data-oriented approach, there\nwill always be some way to speed up a search but only by\nlooking at the data. If the data is not already sorted, then\nan index is a simple way to ﬁnd the speciﬁc item we need. If\nthe data is already sorted, but needs even faster access, then\na search tree optimised for the cache line size would help.\nMost data isn’t this simple to optimise. But importantly,\nwhen there is a lot of data, it usually is simple to learn pat-\nterns from it. A lot of the time, we have to work with spatial\ndata, but because we use objects, it’s hard to strap on an ef-\nﬁcient spatial reference container after the fact. It’s virtually\nimpossible to add one at runtime to an externally deﬁned\nclass of objects.\nAdding spatial partitioning when your data is in a simple\ndata format like rows allows us to generate spatial contain-\ners or lookup systems that will be easy to maintain and op-\ntimise for each particular situation. Because of the inherent\nreusability in data-oriented transforms, we can write some\nvery highly optimised routines for the high-level program-\nmers.\n6.3\nFinding lowest or highest is a sort-\ning problem\nIn some circumstances, you don’t even really need to search.\nIf the reason for searching is to ﬁnd something within a\nrange, such as ﬁnding the closest food, or shelter, or cover,\nthen the problem isn’t really one of searching, but one of\nsorting. In the ﬁrst few runs of a query, the search might\nliterally do a real search to ﬁnd the results, but if it’s run\noften enough, there is no reason not to promote the query\nto a runtime-updating sorted-subset of some other tables’\ndata. If you need the nearest three elements, then you keep\na sorted list of the nearest three elements, and when an el-\nement has an update, insertion or deletion, you can update\n",
      "content_length": 1850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "6.4. FINDING RANDOM\n121\nthe sorted three with that information.\nFor insertions or\nmodiﬁcations which bring elements that are not in the set\ncloser, you can check whether the element is closer and pop\nthe lowest before adding the new element to the sorted best.\nIf there is a deletion or a modiﬁcation that makes one in the\nsorted set a contender for elimination, a quick check of the\nrest of the elements to ﬁnd a new best set might be neces-\nsary. If you keep a larger than necessary set of best values,\nhowever, then you might ﬁnd this never happens.\n1\nArray <int > bigArray;\n2\nArray <int > bestValue;\n3\nconst\nint\nLIMIT = 3;\n4\n5\nvoid\nAddValue( int\nnewValue ) {\n6\nbigArray.push( newValue );\n7\nbestValue. sortedinsert ( newValue );\n8\nif( bestValue.size () > LIMIT )\n9\nbestValue.erase(bestValue.begin ());\n10\n}\n11\nvoid\nRemoveValue ( int\ndeletedValue ) {\n12\nbigArray.remove( deletedValue );\n13\nbestValue.remove( deletedValue );\n14\n}\n15\nint\nGetBestValue () {\n16\nif( bestValue.size () ) {\n17\nreturn\nbestValue.top();\n18\n} else {\n19\nint\nbest = bigArray.findbest ();\n20\nbestvalue.push( best );\n21\nreturn\nbest;\n22\n}\n23\n}\nListing 6.4: keeping more than you need\nThe trick is to ﬁnd, at runtime, the best value to use that\ncovers the solution requirement. The only way to do that is\nto check the data at runtime. For this, either keep logs or\nrun the tests with dynamic resizing based on feedback from\nthe table’s query optimiser.\n6.4\nFinding random is a hash/tree is-\nsue\nFor some tables, the values change very often. For a tree\nrepresentation to be high performance, it’s best not to have\na high number of modiﬁcations as each one could trigger the\n",
      "content_length": 1642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "122\nCHAPTER 6. SEARCHING\nneed for a rebalance. Of course, if you do all your modiﬁca-\ntions in one stage of processing, then rebalance, and then\nall your reads in another, then you’re probably going to be\nokay still using a tree.\nThe C++ standard template library implementation of\nmap for your compiler might not work well even when com-\nmitting all modiﬁcations in one go, but a more cache line\naware implementation of a tree, such as a B-tree, may help\nyou. A B-tree has much wider nodes, and therefore is much\nshallower. It also has a much lower chance of making mul-\ntiple changes at once under insert and delete operations, as\neach node has a much higher capacity. Typically, you will\nsee some form of balancing going on in a red-black tree every\nother insert or delete, but in most B-tree implementations,\nyou will have tree rotations occur relative to the width of\nthe node, and nodes can be very wide. For example, it’s not\nunusual to have nodes with 8 child nodes.\nIf you have many diﬀerent queries on some data, you can\nend up with multiple diﬀerent indexes. How frequently the\nentries are changed should inﬂuence how you store your in-\ndex data. Keeping a tree around for each query could become\nexpensive, but would be cheaper than a hash table in many\nimplementations. Hash tables become cheaper where there\nare many modiﬁcations interspersed with lookups, trees are\ncheaper where the data is mostly static, or at least hangs\naround in one form for a while over multiple reads.\nWhen the data becomes constant, a perfect hash can\ntrump a tree. Perfect hash tables use pre-calculated hash\nfunctions to generate an index and don’t require any space\nother than what is used to store the constants and the array\nof pointers or oﬀsets into the original table. If you have the\ntime, then you might ﬁnd a perfect hash that returns the\nactual indexes. It’s not often you have that long though.\nFor example, what if we need to ﬁnd the position of some-\none given their name? The players won’t normally be sorted\nby name, so we need a name to player lookup. This data is\n",
      "content_length": 2074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "6.4. FINDING RANDOM\n123\nmostly constant during the game so would be better to ﬁnd a\nway to directly access it. A single lookup will almost always\ntrump following a pointer chain, so a hash to ﬁnd an array\nentry is likely to be the best ﬁt. Consider a normal hash ta-\nble, where each slot contains either the element you’re look-\ning for, or a diﬀerent element, and a way of calculating the\nnext slot you should check. If you know you want to do one\nand only one lookup, you can make each of your hash buck-\nets as large as a cache line. That way you can beneﬁt from\nfree memory lookups.\n",
      "content_length": 587,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "124\nCHAPTER 6. SEARCHING\n",
      "content_length": 25,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "Chapter 7\nSorting\nFor some subsystems, sorting is a highly important func-\ntion. Sorting the primitive render calls so they render from\nfront to back for opaque objects can have a massive impact\non GPU performance, so it’s worth doing. Sorting the prim-\nitive render calls so they render from back to front for alpha\nblended objects is usually a necessity. Sorting sound chan-\nnels by their amplitude over their sample position is a good\nindicator of priority.\nWhatever you need to sort for, make sure you need to sort\nﬁrst, as usually, sorting is a highly memory intense busi-\nness.\n7.1\nDo you need to?\nThere are some algorithms which seem to require sorted\ndata, but don’t, and some which require sorted data but\ndon’t seem to. Be sure you know whether you need to before\nyou make any false moves.\nA common use of sorting in games is in the render pass\n125\n",
      "content_length": 859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "126\nCHAPTER 7. SORTING\nwhere some engine programmers recommend having all your\nrender calls sorted by a high bit count key generated from\na combination of depth, mesh, material, shader, and other\nﬂags such as whether the call is alpha blended. This then\nallows the renderer to adjust the sort at runtime to get the\nmost out of the bandwidth available. In the case of the ren-\ndering list sort, you could run the whole list through a gen-\neral sorting algorithm, but in reality, there’s no reason to\nsort the alpha blended objects with the opaque objects, so\nin many cases you can take a ﬁrst step of putting the list\ninto two separate buckets, and save some work overall. Also,\nchoose your sorting algorithm wisely. With opaque objects,\nthe most important part is usually sorting by textures then\nby depth, but that can change with how much your ﬁll rate is\nbeing trashed by overwriting the same pixel multiple times.\nIf your overdraw doesn’t matter too much but your texture\nuploads do, then you probably want to radix sort your calls.\nWith alpha blended calls, you just have to sort by depth, so\nchoose an algorithm which handles your case best. Be aware\nof how accurately you need your data to be sorted. Some\nsorts are stable, others unstable. Unstable sorts are usually\na little quicker. For analogue ranges, a quick sort or a merge\nsort usually oﬀer slow but guaranteed accurate sorting. For\ndiscrete ranges of large n, a radix sort is very hard to beat. If\nyou know your range of values, then a counting sort is a very\nfast two pass sort, for example, sorting by material, shader,\nor other input buﬀer index.\nWhen sorting, it’s also very important to be aware of algo-\nrithms that can sort a range only partially. If you only need\nthe lowest or highest n items of an m long array, you can use\na diﬀerent type of algorithm to ﬁnd the nth item, then sort all\nthe items greater or less than the returned pivot. In some se-\nlection algorithms you will end with some guarantees about\nthe data. Notably, quickselect will result in the nth item by\nsorting criteria residing in the nth position. Once complete,\nall items either side remain unsorted in their sub-ranges,\nbut are guaranteed to be less than or more than the pivot,\ndepending on the side of the pivot they fall.\n",
      "content_length": 2274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "7.1. DO YOU NEED TO?\n127\nIf you have a general range of items which need to be\nsorted in two diﬀerent ways, you can either sort with a spe-\ncialised comparison function in a one-hit sort, or you can\nsort hierarchically. This can be beneﬁcial when the order of\nitems is less important for a subset of the whole range. The\nrender queue is still a good example. If you split your sort\ninto diﬀerent sub-sorts, it makes it possible to proﬁle each\npart of the sort, which can lead to beneﬁcial discoveries.\nYou don’t need to write your own algorithms to do this\neither. Most of the ideas presented here can be implemented\nusing the STL, using the functions in algorithms.\nYou\ncan use std::partial sort to ﬁnd and sort the ﬁrst n el-\nements, you can use std::nth element to ﬁnd the nth value\nas if the container was sorted. Using std::partition and\nstd::stable partition allow you to split a range by a criteria,\neﬀectively sorting a range into two sub-ranges.\nIt’s important to be aware of the contracts of these algo-\nrithms, as something as simple as the erase/remove process\ncan be very expensive if you use it without being aware that\nremove will shuﬄe all your data down, as it is required to\nmaintain order. If there was one algorithm you should add to\nyour collection, it would be your own version of remove which\ndoes not guarantee maintaining order. Listing 7.1 shows one\nsuch implementation.\n1\ntemplate\n<class It , class T>\n2\nIt\nunstable_remove ( It begin , It end , const T& value )\n3\n{\n4\nbegin = find(begin , end , value);\n5\nif (begin != end) {\n6\n--end;\n7\n*begin = move( *end );\n8\n}\n9\nreturn\nend;\n10\n}\nListing 7.1: A basic implementation of unstable remove\n",
      "content_length": 1664,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "128\nCHAPTER 7. SORTING\n7.2\nMaintain by insertion sort or par-\nallel merge sort\nDepending on what you need the list sorted for, you could\nsort while modifying. If the sort is for some AI function that\ncares about priority, then you may as well insertion sort as\nthe base heuristic commonly has completely orthogonal in-\nputs. If the inputs are related, then a post insertion table\nwide sort might be in order, but there’s little call for a full-\nscale sort.\nIf you really do need a full sort, then use an algorithm\nwhich likes being parallel.\nMerge sort and quick sort are\nsomewhat serial in that they end or start with a single\nthread doing all the work, but there are variants which\nwork well with multiple processing threads, and for small\ndatasets there are special sorting network techniques which\ncan be faster than better algorithms just because they ﬁt the\nhardware so well1.\n7.3\nSorting for your platform\nAlways remember that in data-oriented development you\nmust look to the data for information before deciding which\nway you’re going to write the code. What does the data look\nlike? For rendering, there is a large amount of data with dif-\nferent axes for sorting. If your renderer is sorting by mesh\nand material, to reduce vertex and texture uploads, then the\ndata will show that there are a number of render calls which\nshare texture data, and a number of render calls which\nshare vertex data. Finding out which way to sort ﬁrst could\nbe ﬁgured out by calculating the time it takes to upload a\ntexture, how long it takes to upload a mesh, how many ex-\n1Tony Albrecht proves this point in his article on sorting networks\nhttp://seven-degrees-of-freedom.blogspot.co.uk/2010/07/question-of-\nsorts.html\n",
      "content_length": 1712,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "7.3. SORTING FOR YOUR PLATFORM\n129\ntra uploads are required for each, then calculating the total\nscene time, but mostly, proﬁling is the only way to be sure.\nIf you want to be able to proﬁle and get feedback quickly\nor allow for runtime changes in case your game has such\nvarying asset proﬁles that there is no one solution to ﬁt all,\nhaving some ﬂexibility of sorting criteria is extremely useful\nand sometimes necessary. Fortunately, it can be made just\nas quick as any inﬂexible sorting technique, bar a small\nsetup cost.\nRadix sort is the fastest serial sort. If you can do it, radix\nsort is very fast because it generates a list of starting points\nfor data of diﬀerent values in a ﬁrst pass, then operates using\nthat data in a second pass. This allows the sorter to drop\ntheir contents into containers based on a translation table, a\ntable that returns an oﬀset for a given data value. If you build\na list from a known small value space, then radix sort can\noperate very fast to give a coarse ﬁrst pass. The reason radix\nsort is serial, is that it has to modify the table it is reading\nfrom in order to update the oﬀsets for the next element that\nwill be put in the same bucket. If you ran multiple threads\ngiving them part of the work each, then you would ﬁnd they\nwere non-linearly increasing in throughput as they would\nbe contending to write and read from the same memory, and\nyou don’t want to have to use atomic updates in your sorting\nalgorithm.\nIt is possible to make this last stage of the process parallel\nby having each sorter ignore any values it reads which are\noutside its working set, meaning each worker reads through\nthe entire set of values gathering for their bucket, but there is\nstill a small chance of non-linear performance due to having\nto write to nearby memory on diﬀerent threads. During the\ntime the worker collects the elements for its bucket, it could\nbe generating the counts for the next radix in the sequence,\nonly requiring a summing before use in the next pass of the\ndata, mitigating the cost of iterating over the whole set with\nevery worker.\n",
      "content_length": 2085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "130\nCHAPTER 7. SORTING\nIf your data is not simple enough to radix sort, you might\nbe better oﬀusing a merge sort or a quicksort, but there\nare other sorts that work very well if you know the length\nof your sortable buﬀer at compile time, such as sorting net-\nworks. Through merge-sort is not itself a concurrent algo-\nrithm, the many early merges can be run in parallel, only\nthe ﬁnal merge is serial, and with a quick pre-parse of the\nto-be-merged data, you can ﬁnalise with two threads rather\nthan one by starting from both ends (you need to make sure\nthe mergers don’t run out of data). Though quick sort is not\na concurrent algorithm each of the substages can be run in\nparallel. These algorithms are inherently serial, but can be\nturned into partially parallelisable algorithms with O(log n)\nlatency.\nWhen your n is small enough, a traditionally good tech-\nnique is to write an in-place bubble sort. The algorithm is\nso simple, it is hard to write wrong, and because of the small\nnumber of swaps required, the time taken to set up a bet-\nter sort could be better spent elsewhere. Another argument\nfor rewriting such trivial code is that inline implementations\ncan be small enough for the whole of the data and the algo-\nrithm to ﬁt in cache2. As the negative impact of the ineﬃ-\nciency of the bubble sort is negligible over such a small n, it\nis hardly ever frowned upon to do this. In some cases, the\nfact that there are fewer instructions can be more important\nthan the operational eﬃciency, as instruction eviction could\ncost more than the time saved by the better algorithm. As\nalways, measure so you can be certain.\nIf you’ve been developing data-oriented, you’ll have a\ntransform which takes a table of n and produces the sorted\nversion of it. The algorithm doesn’t have to be great to be\nbetter than bubble sort, but notice it doesn’t cost any devel-\nopment time to use a better algorithm as the data is in the\nright shape already.\nData-oriented development naturally\n2It might be wise to have some inline sort function templates in your own\nutility header so you can utilise the beneﬁts of miniaturisation, but don’t\ndrop in a bloated std::sort\n",
      "content_length": 2158,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "7.3. SORTING FOR YOUR PLATFORM\n131\nleads us to reuse of good algorithms.\nWhen looking for the right algorithm, it’s worth knowing\nabout more than you are presented during any coursework,\nand look into the more esoteric forms. For sorting, some-\ntimes you want an algorithm that always sorts in the same\namount of time, and when you do, you can’t use any of the\nstandard quick sorts, radix sorts, bubble or other. Merge\nsort tends to have good performance, but to get truly sta-\nble times when sorting, you may need to resort to sorting\nnetworks.\nSorting networks work by implementing the sort in a\nstatic manner. They have input data and run swap if nec-\nessary functions on pairs of values of the input data before\noutputting the ﬁnal.\nThe simplest sorting network is two\ninputs.\nA\n/\n/\n\u001c\n/ A′\nB\n/\n/\nB\n/ B′\nIf the values entering are in order, the sorting crossover\ndoes nothing. If the values are out of order, then the sorting\ncrossover causes the values to swap.\nThis can be imple-\nmented as branch-free writes:\na’ <= MAX(a,b)\nb’ <= MIN(a,b)\nThis is fast on any hardware. The MAX and MIN functions\nwill need diﬀerent implementations for each platform and\ndata type, but in general, branch-free code executes a little\nfaster than code that includes branches. In most current\ncompilers, the MIN and MAX functions will be promoted to\nintrinsics if they can be, but you might need to ﬁnesse the\n",
      "content_length": 1394,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "132\nCHAPTER 7. SORTING\ndata so the value is part of the key, so it is sorted along with\nthe keys.\nIntroducing more elements:\nA\n/\n1\n/\n\u0015\n2\n/\n\u001c\n3\n/\n/ A′\nB\n/\n/\n\u0015\n/\nB\n\u001c\n/\n/ B′\nC\n/\n/\nI\n/\n\u001c\nB\n/\n/ C′\nD\n/\n/\nI\n/\nB\n/\n/ D′\nWhat you may notice here is that the critical path is not\nlong (just three stages in total), the ﬁrst stage is two con-\ncurrent sortings of A/C, and B/D pairs. The second stage,\nsorting A/B, and C/D pairs. The ﬁnal cleanup sorts the B/C\npair. As these are all branch-free functions, the performance\nis regular over all data permutations. With such a regular\nperformance proﬁle, we can use the sort in ways where the\nvariability of sorting time length gets in the way, such as\njust-in-time sorting for subsections of rendering. If we had\nradix sorted our renderables, we can network sort any ﬁnal\nrequired ordering as we can guarantee a consistent timing.\na’ <= MAX(a,c)\nb’ <= MIN(b,d)\nc’ <= MAX(a,c)\nd’ <= MIN(b,d)\na’’ <= MAX(a’,b’)\nb’’ <= MIN(a’,b’)\nc’’ <= MAX(c’,d’)\nd’’ <= MIN(c’,d’)\nb’’’ <= MIN(b’’,c’’)\nc’’’ <= MAX(b’’,c’’)\n",
      "content_length": 1040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "7.3. SORTING FOR YOUR PLATFORM\n133\nSorting networks are somewhat like predication, the\nbranch-free way of handling conditional calculations.\nBe-\ncause sorting networks use a min/max function, rather\nthan a conditional swap, they gain the same beneﬁts when\nit comes to the actual sorting of individual elements. Given\nthat sorting networks can be faster than radix sort for cer-\ntain implementations, it goes without saying that for some\ntypes of calculation, predication, even long chains of it, will\nbe faster than code that branches to save processing time.\nJust such an example exists in the Pitfalls of Object Oriented\nProgramming[?] presentation, concluding that lazy evalua-\ntion costs more than the job it tried to avoid. I have no hard\nevidence for it yet, but I believe a lot of AI code could beneﬁt\nthe same, in that it would be wise to gather information even\nwhen you are not sure you need it, as gathering it might be\nquicker than deciding not to. For example, seeing if someone\nis in your ﬁeld of vision, and is close enough, might be small\nenough that it can be done for all AI rather than just the\nones requiring it, or those that require it occasionally.\n",
      "content_length": 1172,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "134\nCHAPTER 7. SORTING\n",
      "content_length": 23,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "Chapter 8\nOptimisations and\nImplementations\nWhen optimising software, you have to know what is causing\nthe software to run slower than you need it to run. We ﬁnd in\nmost cases, data movement is what really costs us the most.\nData movement is where most of the energy goes when pro-\ncessing data. Calculating solutions to functions, or running\nan algorithm on the data uses less energy. It is the fulﬁll-\nment of the request for data in the ﬁrst place that appears\nto be the largest cost. As this is most deﬁnitely true about\nour current architectures, we ﬁnd implicit or calculable in-\nformation is often much more useful than cached values or\nexplicit state data.\nIf we start our game development by organising our data\ninto arrays, we open ourselves up to many opportunities for\noptimisation. Starting with such a problem agnostic layout,\nwe can pick and choose from tools we’ve created for other\ntasks, at worst elevating the solution to a template or a strat-\negy, before applying it to both the old and new use cases.\nIn Out of the Tar Pit[?], it’s considered poor form to add\n135\n",
      "content_length": 1086,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "136\nCHAPTER 8. OPTIMISATIONS\nstate and complexity for the sake of performance until late\nin the development of the solution. By using arrays to solve\nthe problem, and side-eﬀect free transforms on those tables,\nperformance improvements can be made across systems in\ngeneral. The improvements can be applied at many sites in\nthe program with little fear of incompatibility, and a convic-\ntion that we’re not adding state, but augmenting the lan-\nguage in which we work.\nThe bane of many projects, and the cause of their late-\nness, has been the insistence on not doing optimisation pre-\nmaturely. The reason optimisation at late stages is so dif-\nﬁcult is that many pieces of software are built up with in-\nstances of objects everywhere, even when not needed. Many\nissues with object-oriented design are caused by the idea\nthat an instance is the unit of processing. Object-oriented\ndevelopment practices tend to assume the instance is the\nunit on which code will work, and techniques and standards\nof practice treat collections of objects as collections of indi-\nviduals.\nWhen the basic assumption is that an object is a unique\nand special thing with its own purpose, then the instructions\nto carry out what it needs to do, will necessarily be selected\nin some way dependent on the object. Accessing instructions\nvia the vtable pointer is the usual method by which opera-\ntions are selected. The greater threat is when ﬁve, ten, or\na hundred individual instances, which could have been rep-\nresented as a group, a swarm, or merely an increment on a\nvalue, are processed as a sequence of individuals. There are\nmany cases where an object exists just because it seemed\nto match the real world concept it was trying to represent at\nthe scale of the developer implementing it, rather than be-\ncause it needed to function as a unique individual element\nof which the user would be aware. It’s easy to get caught up\nimplementing features from the perspective of what they are,\nrather than how they are perceived.\n",
      "content_length": 2006,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "8.1. WHEN SHOULD WE OPTIMISE?\n137\n8.1\nWhen should we optimise?\nWhen should optimisation be done? When is it truly prema-\nture? The answer lies in data of a diﬀerent sort. Premature\noptimisation is when you optimise something without know-\ning whether it will make a diﬀerence. If you attempt to op-\ntimise something because in your mind it will “speed things\nup a bit”, then it can be considered premature, as it’s not\napparent there is anything to optimise.\nLet’s be clear here, without the data to show that a game\nis running slow, or running out of memory, then all optimisa-\ntions are premature. If an application has not been proﬁled,\nbut feels slow, sluggish, or erratic, then anything you do can-\nnot be objectively deﬁned as improving it, and any improve-\nments you attempt to do cannot be anything but premature\noptimisations. The only way to stop premature optimisation\nis to start with real data. If your application seems slow, and\nhas been proﬁled, and what is considered unacceptable is a\nclearly deﬁned statement based on data, then anything you\ndo to improve the solution will not be premature, because it\nhas been measured, and can be evaluated in terms of failure,\nsuccess, or progress.\nGiven that we think we will need to optimise at some\npoint, and we know optimising without proﬁling is not ac-\ntually optimising, the next question becomes clear. When\nshould you start proﬁling?\nWhen should you start work\non your proﬁling framework?\nHow much game content is\nenough to warrant testing performance? How much of the\ngame’s mechanics should be in before you start testing them\nfor performance spikes?\nConsider a diﬀerent question. Is the performance of your\nﬁnal product optional?\nWould you be able to release the\ngame if you knew it had sections which ran at 5fps on cer-\ntain hardware? If you answer that it’s probably okay for your\ngame to run at around 30fps, then that’s a metric, even if it’s\nquite imprecise. How do you know your game already isn’t\n",
      "content_length": 1974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "138\nCHAPTER 8. OPTIMISATIONS\nrunning at 5fps on one of your target audience’s hardware\nconﬁgurations? If you believe there are lower limits to frame-\nrate, and upper limits to your memory usage, if there is an\nexpected maximum time for a level to load before it’s just\nassumed to be stuck, or a strong belief the game should at\nleast not kill the battery on a phone when it’s running, then\nyou have, in at least some respect, agreed that performance\nis not optional.\nIf performance is not optional, and it requires real work to\noptimise, then start asking yourself a diﬀerent set of ques-\ntions. How long can you delay proﬁling? How much art or\nother content can you aﬀord to redo? How many features\nare you willing to work on without knowing if they can be\nincluded in the ﬁnal game? How long can you work with-\nout feedback on whether any of what you have done, can be\nincluded in the ﬁnal product?\n8.2\nFeedback\nNot knowing you are writing poor performance code doesn’t\njust hurt your application. By not having feedback on their\nwork, developers cannot get better, and myths and tech-\nniques which do not work are reinforced and perpetuated.\nDaniel Kahneman, in his book Thinking, Fast and Slow[?],\nprovides some evidence that you can learn well from imme-\ndiate reactions, but cannot easily pick up skills when the\nfeedback is longer in arriving.\nIn one part, he puts it in\nterms of psychotherapists being able to acquire strong intu-\nitive skills in patient interaction, as they are able to observe\nthe patient’s immediate reactions, but they are less likely to\nbe able to build strong intuitions for identifying the appro-\npriate treatment for a patient, as the feedback is not always\navailable, not always complete, and often delayed. Choosing\nto work without feedback would make no sense, but there\nis little option for many game developers, as third party en-\ngines oﬀer very little in the way of feedback mechanisms for\n",
      "content_length": 1930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "8.2. FEEDBACK\n139\nthose learning or starting out on their projects. They do not\nprovide mechanisms to apply budgets to separate aspects\nof their engines, other than the coarse grain of CPU, GPU,\nPhysics, render, etc. They provide lots of tools to help ﬁx per-\nformance when it has been identiﬁed as an issue, but can\noften provide feedback which is incomplete, or inaccurate\nto the ﬁnal form of the product, as built-in proﬁling tools\nare not always available in fully optimised publishing ready\nbuilds.\nYou must get feedback on what is going on, as otherwise\nthere is a risk the optimisations you will need to do will con-\nsume any polish time you have. Make sure your feedback\nis complete and immediate where possible. Adding metrics\non the status of the performance of your game will help with\nthis. Instant feedback on success or failure of optimisations\nhelps mitigate the sunk cost fallacy that can intrude on ra-\ntional discourse about a direction taken. If a developer has\na belief in a way of doing things, but it’s not helping, then it’s\nbetter to know sooner rather than later. Even the most en-\ntrenched in their ways are more approachable with raw data,\nas curiosity is a good tonic for a developer with a wounded\nego. If you haven’t invested a lot of time and eﬀort into an\napproach, then the feedback is even easier to integrate, as\nyou’re going to be more willing to throw the work away and\nﬁgure out how to do it diﬀerently.\nYou also need to get the feedback about the right thing. If\nyou ﬁnd you’ve been optimising your game for a silky smooth\nframe rate and you think you have an average frame rate of\n60fps, and yet your customers and testers keep coming back\nwith comments about nasty frame spikes and dropout, then\nit could be that you’re not proﬁling the right thing, or not\nproﬁling the right way. Sometimes it can be that you have\nto proﬁle a game while it is being played. Sometimes it can\nbe as simple as remembering to proﬁle frame times on a per\nframe basis, not just an average.\nProﬁling doesn’t have to be about frame rate.\nA frame\n",
      "content_length": 2063,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "140\nCHAPTER 8. OPTIMISATIONS\nisn’t a slow thing, something in that frame was slow.\nAn\nold-fashioned, but powerful way to develop software, is to\nprovide budgets to systems and departments. We’re not talk-\ning about ﬁnancial budgets here, but instead time, memory,\nbandwidth, disk space, or other limits which aﬀect the ﬁnal\nproduct directly. If you give your frame a budget of 16ms, and\nyou don’t go over, you have a 60fps game, no ifs, no buts. If\nyou decide you want to maintain good level load times, and\nset yourself a budget of 4 seconds to load level data, then as\nlong as you don’t go over, no one is going to complain about\nyour load times.\nBeyond games, if you have a web-based retail site, you\nmight want to be aware of latency, as it has an eﬀect on your\nusers.\nIt was revealed in a presentation in 2008 by Greg\nLinden that for every additional 100ms of latency, Amazon\nwould experience a loss of 1% in sales. It was also revealed\nthat Google had statistics showing a 20% drop in site traf-\nﬁc was experienced when they added just half a second of\nlatency to page generation. Most scarily of all was a com-\nment from TABB group in 2008, where they mention com-\npany wrecking levels of costs.\nTABB Group estimates that if a broker’s electronic\ntrading platform is 5 milliseconds behind the com-\npetition, it could lose at least 1% of its ﬂow; that’s\n$4 million in revenues per millisecond. Up to 10\nmilliseconds of latency could result in a 10% drop\nin revenues. From there it gets worse. If a broker\nis 100 milliseconds slower than the fastest broker,\nit may as well shut down its FIX engine and be-\ncome a ﬂoor broker.\n1\nIf latency, throughput, frame times, memory usage, or\nanother resource is your limit, then budget for it.\nWhat\n1From THE VALUE OF A MILLISECOND: FINDING THE OPTIMAL SPEED\nOF A TRADING INFRASTRUCTURE by Viraf (Willy) Reporter\n",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "8.2. FEEDBACK\n141\nwould cripple your business? Are you measuring it? How\nlong can you go without checking that you’re not already out\nof business?\n8.2.1\nKnow your limits\nBuilding budgets into how you work means, you can set re-\nalistic budgets for systems early and have them work at a\ncertain level throughout development knowing they will not\ncause grief later in development. On a project without bud-\ngets, frame spikes may only become apparent near release\ndates as it is only then that all systems are coming together\nto create the ﬁnal product. A system which was assumed to\nbe quite cheap, could cause frame spikes in the ﬁnal prod-\nuct, without any evidence being previously apparent. When\nyou ﬁnally ﬁnd out which system causes the spikes, it may\nbe that it was caused by a change from a very long time ago,\nbut as resources were plentiful in the early times of develop-\nment on the project, the spikes caused by the system would\nhave gone completely unnoticed, ﬂying under the radar. If\nyou give your systems budgets, violations can be recorded\nand raised as issues immediately. If you do this, then prob-\nlems can be caught at the moment they are created, and the\ncause is usually within easy reach.\nBuild or get yourself a proﬁler that runs all the time. En-\nsure your proﬁler can report the overall state of the game\nwhen the frame time goes over budget.\nIt’s highly beneﬁ-\ncial to make it respond to any single system going over bud-\nget. Sometimes you need the data from a number of frames\naround when a violation occurred to really ﬁgure out what is\ngoing on. If you have AI in your game, consider running con-\ntinuous testing to capture performance issues as fast as your\nbuild machine churns out testable builds. In all cases, un-\nless you’re letting real testers run your proﬁler, you’re never\ngoing to get real world proﬁling data. If real testers are going\nto be using your proﬁling system, it’s worth considering how\nyou gather data from it. If it’s possible for you, see if you can\n",
      "content_length": 2006,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "142\nCHAPTER 8. OPTIMISATIONS\nget automatically generated proﬁle data sent back to an an-\nalytics or metrics server, to capture issues without requiring\nuser intervention.\n8.3\nA strategy for optimisation\nYou can’t just open up an editor and start optimising. You\nneed a strategy. In this section, we walk through just one\nsuch strategy. The steps have parallels in industries outside\ngame development, where large companies such as Toyota\noptimise as part of their business model.\nToyota has re-\nﬁned their techniques for ensuring maximum performance\nand growth, and the Toyota Production System has been the\ndriving idea behind the Lean manufacturing method for the\nreduction of waste. There are other techniques available, but\nthis subset of steps shares much with many of them.\n8.3.1\nDeﬁne the problem\nDeﬁne your problem. Find out what it is you think is bad.\nDeﬁne it in terms of what is factual, and what is assumed to\nbe a ﬁnal good solution. This can be as simple as saying the\nproblem is that the game is running at 25fps, and you need\nit to be at 30fps. Stick to clear, objective language.\nIt’s important to not include any guesses in this step, so\nstatements which include ideas on what or how to optimise\nshould be prohibited. Consider writing it from the point of\nview of someone using the application, not from the perspec-\ntive of the developer. This is sometimes called quality crite-\nria, or customer requirements.\n",
      "content_length": 1430,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "8.3. A STRATEGY\n143\n8.3.2\nMeasure\nMeasure what you need to measure. Unlike measuring ran-\ndomly, targeted measuring is better for ﬁguring out what is\nactually going on, as you are less likely to ﬁnd a pattern in\nirrelevant data. P-hacking or data dredging can lead you to\nfalse convictions about causes of problems.\nAt this stage, you also need to get an idea of the quality\nof your measurements. Run your tests, but then run them\nagain to make sure they’re reproducible. If you can’t repro-\nduce the same results before you have made changes, then\nhow are you going to be sure the changes you have made,\nhave had any eﬀect?\n8.3.3\nAnalyse\nThe ﬁrst step in most informal optimisation strategies: the\nguessing phase. This is when you come up with ideas about\nwhat could be the problem and suggest diﬀerent ways to\ntackle the problem.\nIn the informal optimisation process,\nyou pick the idea which seems best, or at least the most fun\nto implement.\nIn this more formal strategy, we analyse what we have\nmeasured. Sometimes it’s apparent from this step that the\nmeasurements didn’t provide enough direction to come up\nwith a good optimisation plan. If your analysis proves you\ndon’t have good data, the next step should be to rectify your\nability to capture useful data.\nDon’t tackle optimisation\nwithout understanding the cost associated with failing to\nunderstand the problem.\nThis is also the stage to make predictions. Estimate the\nexpected impact of an improvement you plan to make. Don’t\njust lightly guess, have a really good go at guessing with some\nnumber crunching. You won’t be able to do it after the im-\nplementation, as you will have too much knowledge to make\n",
      "content_length": 1670,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "144\nCHAPTER 8. OPTIMISATIONS\nan honest guess. You will be suﬀering what some call the\ncurse of knowledge. By doing this, you can learn about how\ngood you are at estimating the impact of your optimisations,\nbut also, you can get an idea of the relative impact of your\nchange before you begin work.\n8.3.4\nImplement\nThe second step in most informal optimisation strategies; the\nimplementation phase. This is when you make the changes\nyou think will ﬁx the problem.\nIf possible, do an experimental implementation of the op-\ntimisation to your solution.\nA program is a solution to a\nproblem, it is a strategy to solve a data transform, and you\nshould remember that when designing your experiment.\nBefore you consider the local version to be working, and\nindeed, worth working on, you must prove it’s useful. Check\nthe measurements you get from the localised experiment are\nin line with your expectations as measured from the inte-\ngrated version.\nIf your optimisation is going to be perfect ﬁrst time, then\nthe experimental implementation will only be used as a proof\nthat the process can be repeated and can be applicable in\nother circumstances. It will only really be useful as a teach-\ning tool for others, in helping them understand the costs of\nthe original process and the expected improvement under\nsimilar constraints.\nIf you are not sure the optimisation will work out ﬁrst\ntime, then the time saved by not doing a full implementation\ncan be beneﬁcial, as a localised experiment can be worked\non faster. It can also be a good place to start when trying to\nbuild an example for third parties to provide support, as a\nsmaller example of the problem will be easier to communi-\ncate through.\n",
      "content_length": 1692,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "8.3. A STRATEGY\n145\n8.3.5\nConﬁrm\nThis step is critical in more ways than expected. Some may\nthink it an optional step, but it is essential for retaining the\nvaluable information you will have generated while doing the\noptimisation.\nCreate a report of what you have done, and what you have\nfound. The beneﬁts of doing this are twofold. First, you have\nthe beneﬁt of sharing knowledge of a technique for optimi-\nsation, which clearly can help others hitting the same kind\nof issue. The second is that creating the report can identify\nany errors of measurement, or any steps which can be tested\nto ensure they were actually pertinent to the ﬁnal changes\ncommitted.\nIn a report, others can point out any illogical leaps of rea-\nsoning, which can lead to even better understanding and can\nalso help deny any false assumptions from building up in\nyour understanding of how the machine really works. Writ-\ning a report can be a powerful experience that will give you\nvaluable mental building blocks and the ability to better ex-\nplain what happens under certain conditions.\n8.3.6\nSummary\nAbove all things, keep track. If you can, do your optimisa-\ntion work in isolation of a working test bed. Make sure your\ntimings are reproducible even if you have to get up to date\nwith the rest of the project due to having to work on a bug\nor feature. Making sure you keep track of what you are do-\ning with notes can help you understand what was in your\nhead when you made earlier changes, and what you might\nnot have thought about.\nIt is important to keep trying to improve your ability to\nsee; to observe.\nYou cannot make measurable progress if\nyou cannot measure, and you cannot tell you have made an\n",
      "content_length": 1687,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "146\nCHAPTER 8. OPTIMISATIONS\nimprovement without tools for identifying the improvement.\nImprove your tools for measuring when you can. Look for\nways to look. Whenever you ﬁnd that there was no way to\nknow with the tools you had available, either ﬁnd the tools\nyou need or if you can’t ﬁnd them, attempt to make them\nyourself. If you cannot make them yourself, petition others,\nor commission someone else to create them. Don’t give in\nto hopeful optimisations, because they will teach you bad\nhabits and you will learn false facts from random chance\nproving you right.\n8.4\nTables\nTo keep things simple, advice from multiple sources indi-\ncate that keeping your data as vectors has a lot of positive\nbeneﬁts.\nThere are some reasons to use something other\nthan the STL, but learn its quirks, and you can avoid a lot\nof the issues. Whether you use std::vector, or roll your own\ndynamically sized array, it is a good starting place for any\nfuture optimisations. Most of the processing you will do will\nbe reading an array, transforming one array into another, or\nmodifying a table in place. In all these cases, a simple array\nwill suﬃce for most tasks.\nMoving to arrays is good, moving to structure-of-arrays\ncan be better. Not always. It’s very much worth considering\nthe access patterns for your data. If you can’t consider the\naccess patterns, and change is costly, choose based on some\nother criteria, such as readability.\nAnother reason to move away from arrays of objects, or\narrays of structures, is to keep the memory accesses spe-\nciﬁc to their tasks. When thinking about how to structure\nyour data, it’s important to think about what data will be\nloaded and what data will be stored. CPUs are optimised for\ncertain patterns of memory activity. Many CPUs have a cost\nassociated with changing from read operations to write oper-\n",
      "content_length": 1832,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "8.4. TABLES\n147\nations. To help the CPU not have to transition between read\nand write, it can be beneﬁcial to arrange writing to mem-\nory in a very predictable and serial manner. An example of\nhot cold separation that doesn’t take into account the impor-\ntance of writing can be seen in the example code in listing\n8.1 that attempts to update values which are used both for\nread and write, but are close neighbours of data which is\nonly used for reading.\n1\nstruct\nPosInfo\n2\n{\n3\nvec3\npos;\n4\nvec3\nvelocity;\n5\nPosInfo ():\n6\npos (1.0f, 2.0f, 3.0f),\n7\nvelocity (4.0f, 5.0f, 6.0f)\n8\n{}\n9\n};\n10\n11\nstruct\nnodes\n12\n{\n13\nstd :: vector <PosInfo > posInfos;\n14\nstd :: vector <vec3 > colors;\n15\nstd :: vector <LifetimeInfo > lifetimeInfos ;\n16\n} nodesystem;\n17\n18\n// ...\n19\n20\nfor (size_t\ntimes = 0; times < trialCount ; times ++)\n21\n{\n22\nstd :: vector <PosInfo >&\nposInfos = nodesystem .posInfos;\n23\nfor (size_t i = 0; i < node_count; ++i)\n24\n{\n25\nposInfos[i]. pos +=\nposInfos[i]. velocity * deltaTime;\n26\n}\n27\n}\nListing 8.1: Mixing hot reads with hot and cold writes\nThe code in listing 8.2 shows a signiﬁcant performance\nimprovement.\n1\nstruct\nnodes\n2\n{\n3\nstd ::vector <vec3 > positions;\n4\nstd ::vector <vec3 > velocities;\n5\nstd ::vector <vec3 > colors;\n6\nstd ::vector <LifetimeInfo > lifetimeInfos ;\n7\n};\n8\n// ...\n9\nnodes\nnodesystem;\n10\n// ...\n11\nfor (size_t\ntimes = 0; times < trialCount ; times ++)\n12\n{\n13\nfor (size_t i = 0; i < node_count; ++i)\n14\n{\n15\nnodesystem.positions[i] +=\nnodesystem. velocities [i] *\ndeltaTime;\n16\n}\n17\n}\nListing 8.2: Ensuring each stream is continuous\nFor the beneﬁt of your cache, structs of arrays can be\n",
      "content_length": 1628,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "148\nCHAPTER 8. OPTIMISATIONS\nmore cache-friendly if the data is not strongly related both\nfor reading and writing. It’s important to remember this is\nonly true when the data is not always accessed as a unit, as\none advocate of the data-oriented design movement assumed\nthat structures of arrays were intrinsically cache-friendly,\nthen put the x,y, and z coordinates in separate arrays of\nﬂoats. It is possible to beneﬁt from having each element in\nits own array when you utilise SIMD operations on larger\nlists. However, if you need to access the x,y, or z of an ele-\nment in an array, then you more than likely need to access\nthe other two axes as well. This means that for every ele-\nment you will be loading three cache lines of ﬂoat data, not\none. If the operation involves a lot of other values, then this\nmay overﬁll the cache. This is why it is important to think\nabout where the data is coming from, how it is related, and\nhow it will be used. Data-oriented design is not just a set\nof simple rules to convert from one style to another. Learn\nto see the connections between data. In this case, we see\nthat in some circumstances, it’s better to keep your vector\nas three or four ﬂoats if it’s not commonly used as a value in\nan operation that will be optimised with SIMD instructions.\nThere are other reasons why you might prefer to not store\ndata in trivial SoA format, such as if the data is commonly\nsubject to insertions and deletions. Keeping free lists around\nto stop deletions from mutating the arrays can help alleviate\nthe pressure, but being unable to guarantee every element\nrequires processing moves away from simple homogeneous\ntransformations which are often the point of such data lay-\nout changes.\nIf you use dynamic arrays, and you need to delete el-\nements from them, and these tables refer to each other\nthrough some IDs, then you may need a way to splice the\ntables together in order to process them as you may want\nto keep them sorted to assist with zipping operations. If the\ntables are sorted by the same value, then it can be written\nout as a simple merge operation, such as in listing 8.3.\n1\nProcessJoin( Func\nfunctionToCall ) {\n2\nTableIterator A = t1Table.begin ();\n",
      "content_length": 2199,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "8.4. TABLES\n149\n3\nTableIterator B = t2Table.begin ();\n4\nTableIterator C = t3Table.begin ();\n5\nwhile( !A.finished\n&& !B.finished\n&& !C.finished ) {\n6\nif( A == B && B == C ) {\n7\nfunctionToCall ( A, B, C );\n8\n++A; ++B; ++C;\n9\n} else {\n10\nif( A < B || A < C ) ++A;\n11\nif( B < A || B < C ) ++B;\n12\nif( C < A || C < B ) ++C;\n13\n}\n14\n}\n15\n}\nListing 8.3: Zipping together multiple tables by merging\nThis works as long as the == operator knows about the ta-\nble types and can ﬁnd the speciﬁc column to check against,\nand as long as the tables are sorted based on this same col-\numn. But what about the case where the tables are zipped\ntogether without being the sorted by the same columns? For\nexample, if you have a lot of entities which refer to a mod-\nelID, and you have a lot of mesh-texture combinations which\nrefer to the same modelID, then you will likely need to zip to-\ngether the matching rows for the orientation of the entity,\nthe modelID in the entity render data, and the mesh and\ntexture combinations in the models.\nThe simplest way to\nprogram a solution to this is to loop through each table in\nturn looking for matches such as in Listing 8.4.\nThis so-\nlution, though simple to write, is incredibly ineﬃcient, and\nshould be avoided where possible. But as with all things,\nthere are exceptions. In some situations, very small tables\nmight be more eﬃcient this way, as they will remain resident,\nand sorting them could cost more time.\n1\nProcessJoin( Func\nfunctionToCall ) {\n2\nfor( auto A : orientationTable ) {\n3\nfor( auto B : entityRenderableTable ) {\n4\nif( A == B ) {\n5\nfor( auto C : meshAndTextureTable ) {\n6\nif( A == C ) {\n7\nfunctionToCall ( A, B, C );\n8\n}\n9\n}\n10\n}\n11\n}\n12\n}\n13\n}\nListing 8.4: Join by looping through all tables\nAnother thing you have to learn about when working with\ndata which is joined on diﬀerent columns is the use of join\n",
      "content_length": 1854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "150\nCHAPTER 8. OPTIMISATIONS\nstrategies. In databases, a join strategy is used to reduce the\ntotal number of operations when querying across multiple\ntables. When joining tables on a column (or key made up of\nmultiple columns), you have a number of choices about how\nyou approach the problem. In our trivial coded attempt, you\ncan see we simply iterate over the whole table for each table\ninvolved in the join, which ends up being O(nmo) or O(n3)for\nroughly same size tables. This is no good for large tables, but\nfor small ones it’s ﬁne. You have to know your data to decide\nwhether your tables are big2 or not. If your tables are too big\nto use such a trivial join, then you will need an alternative\nstrategy.\nYou can join by iteration, or you can join by lookup3, or\nyou can even join once and keep a join cache around. Keep-\ning the join cache around makes it appear as if you can op-\nerate on the tables as if they are sorted in multiple ways at\nthe same time.\nIt’s perfectly feasible to add auxiliary data which will allow\nfor traversal of a table in a diﬀerent order. We add join caches\nin the same way databases allow for any number of indexes\ninto a table. Each index is created and kept up to date as the\ntable is modiﬁed. In our case, we implement each index the\nway we need to. Maybe some tables are written to in bursts,\nand an insertion sort would be slow, it might be better to\nsort on ﬁrst read, or trash the whole index on modify. In\nother cases, the sorting might be better done on write, as\nthe writes are infrequent, or always interleaved with many\nreads.\n2dependent on the target hardware, how many rows and columns, and\nwhether you want the process to run without trashing too much cache\n3often a lookup join is called a join by hash, but as we know our data, we\ncan use better row search algorithms than a hash when they are available\n",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "8.5. TRANSFORMS\n151\n8.5\nTransforms\nTaking the concept of schemas a step further, a static\nschema deﬁnition can allow for a diﬀerent approach to iter-\nators. Instead of iterating over a container, giving access to\nan element, a schema iterator can become an accessor for a\nset of tables, meaning the merging work can be done during\niteration, generating a context upon which the transform\noperates. This would beneﬁt large, complex merges which\ndo little with the data, as there would be less memory us-\nage creating temporary tables. It would not beneﬁt complex\ntransforms as it would reduce the likelihood that the next\nset of data is in cache ready for the next cycle.\nAnother aspect of transforms is the separation of what\nfrom how.\nThat is, separating the gathering or loading of\ndata we will transform from the code which ultimately per-\nforms the operations on the data. In some languages, in-\ntroducing map and reduce is part of the basic syllabus, in\nC++, not so much. This is probably because lists aren’t part\nof the base language, and without that, it’s hard to intro-\nduce powerful tools which require them. These tools, map\nand reduce, can be the basis of a purely transform and ﬂow\ndriven program. Turning a large set of data into a single re-\nsult sounds eminently serial, however, as long as one of the\nsteps, the reduce step, is associative, then you can reduce\nin parallel for a signiﬁcant portion of the reduction.\nA simple reduce, one made to create a ﬁnal total from a\nmapping which produces values of zero or one for all match-\ning elements, can be processed as a less and less parallel\ntree of reductions. In the ﬁrst step, all reductions produce\nthe total of all odd-even pairs of elements and produce a new\nlist which goes through the same process. This list reduc-\ntion continues until there is only one item left remaining. Of\ncourse, this particular reduction is of very little use, as each\nreduction is so trivial, you’d be better oﬀassigning an nthof\nthe workload to each of the n cores and doing one ﬁnal sum-\nming. A more complex, but equally useful reduction would\n",
      "content_length": 2098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "152\nCHAPTER 8. OPTIMISATIONS\nbe the concatenation of a chain of matrices. Matrices are\nassociative even if they are not commutative, and as such,\nthe chain can be reduced in parallel the same way building\nthe total worked. By maintaining the order during reduc-\ntion you can apply parallel processing to many things which\nwould normally seem serial, so long as they are associative\nin the reduce step. Not only matrix concatenation, but also\nproducts of ﬂoating point values such as colour modulation\nby multiple causes such as light, diﬀuse, or gameplay related\ntinting. Building text strings can be associative, as can be\nbuilding lists.\n8.6\nSpatial sets for collisions\nIn collision detection, there is often a broad-phase step which\ncan massively reduce the number of potential collisions we\ncheck against.\nWhen ray casting, it’s often useful to ﬁnd\nthe potential intersection via an octree, BSP, or other spatial\nquery accelerator.\nWhen running pathﬁnding, sometimes\nit’s useful to look up local nodes to help choose a starting\nnode for your journey.\nAll spatial data-stores accelerate queries by letting them\ndo less. They are based on some spatial criteria and return\na reduced set which is shorter and thus less expensive to\ntransform into new data.\nExisting libraries which support spatial partitioning have\nto try to work with arbitrary structures, but because all our\ndata is already organised by table, writing adaptors for any\npossible table layout is made simpler. Writing generic algo-\nrithms becomes easier without any of the side eﬀects nor-\nmally associated with writing code that is used in multiple\nplaces. Using the table-based approach, because of its in-\ntention agnosticism (that is, the spatial system has no idea\nit’s being used on data which doesn’t technically belong in\nspace), we can use spatial partitioning algorithms in unex-\n",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "8.7. LAZY EVALUATION\n153\npected places, such as assigning audio channels by not only\ntheir distance from the listener, but also their volume and\nimportance.\nMaking a 5 dimensional spatial partitioning\nsystem, or even an n dimensional one, would be an invest-\nment. It would only have to be written once and have unit\ntests written once, before it could be used and trusted to do\nsome very strange things. Spatially partitioning by the quest\nprogress for tasks to do seems a little overkill, but getting the\nset of all nearby interesting entities by their location, threat,\nand reward, seems like something an AI might consider use-\nful.\n8.7\nLazy evaluation for the masses\nWhen optimising object-oriented code, it’s quite common to\nﬁnd local caches of completed calculations hidden in mu-\ntable member variables. One trick found in most updating\nhierarchies is the dirty bit, the ﬂag that says whether the\nchild or parent members of a tree have decided this object\nneeds updating. When traversing the hierarchy, this dirty bit\ncauses branching based on data which has only just loaded,\nusually meaning there is no chance to guess the outcome\nand thus in most cases, causes memory to be read in prepa-\nration, when it’s not required.\nIf your calculation is expensive, then you might not want\nto go the route that render engines now use. In render en-\ngines, it’s often cheaper to do every scene matrix concate-\nnation every frame than it is only doing the ones necessary\nand ﬁguring out if they are.\nFor example, in the Pitfalls of Object-Oriented Program-\nming [?] presentation by Tony Albrecht, in the early slides\nhe declares that checking a dirty ﬂag is less useful than not\nchecking it, as when it does fail (the case where the object\nis not dirty) the calculation that would have taken 12 cycles\nis dwarfed by the cost of a branch misprediction (23-24 cy-\n",
      "content_length": 1860,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "154\nCHAPTER 8. OPTIMISATIONS\ncles). Things always move on, and in the later talk Pitfalls\nrevisited[?], he notes that the previous improvement gained\nthrough manual devirtualization no longer provides any ben-\neﬁt. Whether it was the improvements in the compiler or the\nchange in hardware, reality will always trump experience.\nIf your calculation is expensive, you don’t want to bog\ndown the game with a large number of checks to see if the\nvalue needs updating. This is the point at which existence-\nbased-processing comes into its own again as existence in\nthe dirty table implies it needs updating, and as a dirty ele-\nment is updated it can be pushing new dirty elements onto\nthe end of the table, even prefetching if it can improve band-\nwidth.\n8.8\nNecessity, or not getting what you\ndidn’t ask for\nWhen you normalise your data you reduce the chance of an-\nother multifaceted problem of object-oriented development.\nC++’s implementation of objects forces unrelated data to\nshare cache lines.\nObjects collect their data by the class, but many objects,\nby design, contain more than one role’s worth of data. This is\nbecause object-oriented development doesn’t naturally allow\nfor objects to be recomposed based on their role in a trans-\naction, and also because C++ needed to provide a method\nby which you could have object-oriented programming while\nkeeping the system level memory allocations overloadable in\na simple way. Most classes contain more than just the bare\nminimum, either because of inheritance or because of the\nmany contexts in which an object can play a part. Unless\nyou have very carefully laid out a class, many operations\nwhich require only a small amount of information from the\nclass will load a lot of unnecessary data into the cache in or-\nder to do so. Only using a very small amount of the loaded\n",
      "content_length": 1827,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "8.9. VARYING LENGTH SETS\n155\ndata is one of the most common sins of the object-oriented\nprogrammer.\nEvery virtual call loads in the cache line that contains the\nvirtual-table pointer of the instance. If the function doesn’t\nuse any of the class’s early data, then that will be cache\nline utilisation in the region of only 4%. That’s a memory\nthroughput waste, and cannot be recovered without rethink-\ning how you dispatch your functions. Adding a ﬁnal keyword\nto your class can help when your class calls into its own vir-\ntual functions, but cannot help when they are called via a\nbase type.\nIn practice, only after the function has loaded, can the\nCPU load the data it wants to work on, which can be scat-\ntered across the memory allocated for the class too. It won’t\nknow what data it needs until it has decoded the instruc-\ntions from the function pointed to by the virtual table entry.\n8.9\nVarying length sets\nThroughout the techniques so far, there’s been an implied\ntable structure to the data. Each row is a struct, or each\ntable is a row of columns of data, depending on the need\nof the transforms. When working with stream processing,\nfor example, with shaders, we would normally use ﬁxed size\nbuﬀers.\nMost work done with stream processing has this\nsame limitation, we tend to have a ﬁxed number of elements\nfor both sides.\nFor ﬁltering where the input is known to be a superset of\nthe output, there can be a strong case for an annealing struc-\nture. Outputting to multiple separate vectors, and concate-\nnating them in a ﬁnal reduce. Each transform thread has\nits own output vector, the reduce step would ﬁrst generate\na total and a start position for each reduce entry and then\nprocesses the list of reduces onto the ﬁnal contiguous mem-\n",
      "content_length": 1750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "156\nCHAPTER 8. OPTIMISATIONS\nory. A parallel preﬁx sum would work well here, but simple\nlinear passes would suﬃce.\nIf the ﬁltering was a stage in a radix sort, counting sort,\nor something which uses a similar histogram for generating\noﬀsets, then a parallel preﬁx sum would reduce the latency\nto generate the oﬀsets. A preﬁx sum is the running total of a\nlist of values. The radix sort output histogram is a great ex-\nample because the bucket counts indicate the starting points\nthrough the sum of all histogram buckets that come prior.\non = Pn−1\ni=0 bi.\nThis is easy to generate in serial form, but\nin parallel, we have to consider the minimum required op-\nerations to produce the ﬁnal result.\nIn this case, we can\nremember that the longest chain will be the value of the last\noﬀset, which is a sum of all the elements. This is normally\noptimised by summing in a binary tree fashion. Dividing and\nconquering: ﬁrst summing all odd numbered slots with all\neven numbered slots, then doing the same, but for only the\noutputs of the previous stage.\nA\n\u000f\n \nB\n\u000f\nC\n\u000f\n\"\nD\n\u000f\na\n\u000f\nab\n\u000f\n(\nc\n\u000f\ncd\n\u000f\na\nab\nabc\nabcd\nThen once you have the last element, backﬁll all the other\nelements you didn’t ﬁnish on your way to making the last\nelement. When you come to write this in code, you will ﬁnd\nthese backﬁlled values can be done in parallel while making\nthe longest chain.\nThey have no dependency on the ﬁnal\nvalue so can be given over to another process, or managed\nby some clever use of SIMD.\n",
      "content_length": 1475,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "8.9. VARYING LENGTH SETS\n157\na\n\u000f\nab\n\u000f\n!\nc\n\u000f\nabcd\n\u000f\na\nab\nabc\nabcd\nParallel preﬁx sums provide a way to reduce latency, but\nare not a general solution which is better than doing a lin-\near preﬁx sum. A linear preﬁx sum uses far fewer machine\nresources to do the same thing, so if you can handle the\nlatency, then simplify your code and do the sum linearly.\nAlso, for cases where the entity count can rise and fall,\nyou need a way of adding and deleting without causing any\nhiccups. For this, if you intend to transform your data in\nplace, you need to handle the case where one thread can be\nreading and using the data you’re deleting. To do this in a\nsystem where objects’ existence was based on their memory\nbeing allocated, it would be very hard to delete objects that\nwere being referenced by other transforms. You could use\nsmart pointers, but in a multi-threaded environment, smart\npointers cost a mutex to be thread safe for every reference\nand dereference. This is a high cost to pay, so how do we\navoid it? There are at least two ways.\nDon’t have a mutex. One way to avoid the mutex is to\nuse a smart pointer type which is bound to a single thread.\nIn some game engines, there are smart pointer types that\ninstead of keeping a mutex, store an identiﬁer for the thread\nthey belong to. This is so they can assert every access is\nmade by the same thread. For performance considerations,\nthis data doesn’t need to be present in release builds, as\nthe checks are done to protect against misuse at runtime\ncaused by decisions made at compile time. For example, if\nyou know the data should not be used outside of the audio\nsubsystem, and the audio subsystem is running on a single\nthread of its own, lock it down and tie the memory allocation\nto the audio thread. Any time the audio system memory is\naccessed outside of the audio thread, it’s either because the\naudio system is exposing memory to the outside systems or\n",
      "content_length": 1919,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "158\nCHAPTER 8. OPTIMISATIONS\nit’s doing more work than it should in any callback functions.\nIn either case, the assert will catch the bad behaviour, and\nﬁxes can be made to the code to counter the general issue,\nnot the speciﬁc case.\nDon’t delete. If you are deleting in a system that is con-\nstantly changing, then you would normally use pools any-\nway.\nBy explicitly not deleting, by doing something else\ninstead, you change the way all code accesses data.\nYou\nchange what the data represents. If you need an entity to\nexist, such as a CarDriverAI, then it can stack up on your\ntable of CarDriverAIs while it’s in use, but the moment it’s\nnot in use, it won’t get deleted, but instead marked as not\nused. This is not the same as deleting, because you’re say-\ning the entity is still valid, won’t crash your transform, but\nit can be skipped as if it were not there until you get around\nto overwriting it with the latest request for a CarDriverAI.\nKeeping dead entities around can be as cheap as keeping\npools for your components, as long as there are only a few\ndead entities in your tables.\n8.10\nJoins as intersections\nSometimes, normalisation can mean you need to join tables\ntogether to create the right situation for a query.\nUnlike\nRDBMS queries, we can organise our queries much more\ncarefully and use the algorithm from merge sort to help us\nzip together two tables. As an alternative, we don’t have to\noutput to a table, it could be a pass-through transform which\ntakes more than one table and generates a new stream into\nanother transform. For example, per entityRenderable, join\nwith entityPosition by entityID, to transform with AddRen-\nderCall( Renderable, Position ).\n",
      "content_length": 1682,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "8.11. DATA-DRIVEN TECHNIQUES\n159\n8.11\nData-driven techniques\nApart from ﬁnite state machines, there are some other com-\nmon forms of data-driven coding practices.\nSome are not\nvery obvious, such as callbacks.\nSome are very obvious,\nsuch as scripting.\nIn both these cases, data causing the\nﬂow of code to change will cause the same kind of cache and\npipeline problems as seen in virtual calls and ﬁnite state ma-\nchines.\nCallbacks can be made safer by using triggers from event\nsubscription tables. Rather than have a callback which ﬁres\noﬀwhen a job is done, have an event table for done jobs so\ncallbacks can be called once the whole run is ﬁnished. For\nexample, if a scoring system has a callback from “badGuy-\nDies”, then in an object-oriented message watcher you would\nhave the scorer increment its internal score whenever it re-\nceived the message that a badGuyDies. Instead, run each\nof the callbacks in the callback table once the whole set of\nbadGuys has been checked for death. If you do that and ex-\necute every time all the badGuys have had their tick, then\nyou can add points once for all badGuys killed. That means\none read for the internal state, and one write. Much better\nthan multiple reads and writes accumulating a ﬁnal score.\nFor scripting, if you have scripts which run over multi-\nple entities, consider how the graphics kernels operate with\nbranches, sometimes using predication and doing both sides\nof a branch before selecting a solution. This would allow you\nto reduce the number of branches caused merely by inter-\npreting the script on demand. If you go one step further an\nactually build SIMD into the scripting core, then you might\nﬁnd you can use scripts for a very large number of entities\ncompared to traditional per entity serial scripting. If your\nSIMD operations operate over the whole collection of entities,\nthen you will pay almost no price for script interpretation4.\n4Take a look at the section headed The Massively Vectorized Virtual\nMachine on the BitSquid blog http://bitsquid.blogspot.co.uk/2012/10/a-\ndata-oriented-data-driven-system-for.html\n",
      "content_length": 2089,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "160\nCHAPTER 8. OPTIMISATIONS\n8.11.1\nSIMD\nSIMD operations can be very beneﬁcial as long as you have\na decent chunk of work to do, such as making an operation\nthat handles updating positions of particles (see listing 8.5).\nThis example of SIMDifying some code is straightforward,\nand in tests ran about four times faster than both the array\nof structs code and the na¨ıve struct of arrays code.\n1\nvoid\nSimpleUpdateParticles ( particle_buffer *pb , float\ndelta_time\n) {\n2\nfloat g = pb ->gravity;\n3\nfloat\ngd2 = g * delta_time * delta_time * 0.5f;\n4\nfloat gd = g * delta_time;\n5\nfor( int i = 0; i < NUM_PARTICLES ; ++i ) {\n6\npb ->posx[i] += pb ->vx[i] * delta_time;\n7\npb ->posy[i] += pb ->vy[i] * delta_time + gd2;\n8\npb ->posz[i] += pb ->vz[i] * delta_time;\n9\npb ->vy[i] += gd;\n10\n}\n11\n}\n12\n13\nvoid\nSIMD_SSE_UpdateParticles ( particle_buffer *pb , float\ndelta_time ) {\n14\nfloat g = pb ->gravity;\n15\nfloat\nf_gd = g * delta_time;\n16\nfloat\nf_gd2 = pb ->gravity * delta_time * delta_time * 0.5f;\n17\n18\n__m128\nmmd = _mm_setr_ps( delta_time , delta_time , delta_time ,\ndelta_time );\n19\n__m128\nmmgd = _mm_load1_ps ( &f_gd );\n20\n__m128\nmmgd2 = _mm_load1_ps ( &f_gd2 );\n21\n22\n__m128 *px = (__m128 *)pb ->posx;\n23\n__m128 *py = (__m128 *)pb ->posx;\n24\n__m128 *pz = (__m128 *)pb ->posz;\n25\n__m128 *vx = (__m128 *)pb ->vx;\n26\n__m128 *vy = (__m128 *)pb ->vy;\n27\n__m128 *vz = (__m128 *)pb ->vz;\n28\n29\nint\niterationCount = NUM_PARTICLES / 4;\n30\nfor( int i = 0; i < iterationCount ; ++i ) {\n31\n__m128\ndx = _mm_mul_ps(vx[i], mmd );\n32\n__m128\ndy = _mm_add_ps( _mm_mul_ps(vy[i], mmd ), mmgd2 );\n33\n__m128\ndz = _mm_mul_ps(vz[i], mmd );\n34\n__m128\nnewx = _mm_add_ps(px[i], dx);\n35\n__m128\nnewy = _mm_add_ps(py[i], dy);\n36\n__m128\nnewz = _mm_add_ps(pz[i], dz);\n37\n__m128\nnewvy = _mm_add_ps(vy[i], mmgd);\n38\n_mm_store_ps (( float *)(px+i), newx);\n39\n_mm_store_ps (( float *)(py+i), newy);\n40\n_mm_store_ps (( float *)(pz+i), newz);\n41\n_mm_store_ps (( float *)(vy+i), newvy);\n42\n}\n43\n}\nListing 8.5: Simple particle update with SIMD\nIn many optimising compilers, simple vectorisation is car-\nried out by default, but only as far as the compiler can ﬁgure\nthings out. It’s not often very easy to ﬁgure these things out.\nSIMD operations on machines which support SSE, allow\n",
      "content_length": 2237,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "8.12. STRUCTS OF ARRAYS\n161\nyou to get more data into the CPU in one go. Many people\nstarted out by putting their 3D vectors into SIMD units, but\nthat doesn’t allow full utilisation of the SIMD pipeline. The\nexample loads in four diﬀerent particles at the same time,\nand updates them all at the same time too. This very simple\ntechnique also means you don’t have to do anything clever\nwith the data layout, as you can just use a na¨ıve struct of ar-\nrays to prepare for SIMDiﬁcation once you ﬁnd it has become\na bottleneck.\n8.12\nStructs of arrays\nIn addition to all the other beneﬁts of keeping your runtime\ndata in a database style format, there is the opportunity to\ntake advantage of structures of arrays rather than arrays of\nstructures. SoA has been coined as a term to describe an\naccess pattern for object data. It is okay to keep hot and\ncold data side by side in an SoA object as data is pulled into\nthe cache by necessity rather than by accidental physical\nlocation.\nIf your animation timekey/value class resembles this:\n1\nstruct\nKeyframe\n2\n{\n3\nfloat time , x,y,z;\n4\n};\n5\nstruct\nStream\n6\n{\n7\nKeyframe *keyframes;\n8\nint\nnumKeys;\n9\n};\nListing 8.6: animation timekey/value class\nthen when you iterate over a large collection of them, all\nthe data has to be pulled into the cache at once. If we assume\nthat a cache line is 64 bytes, and the size of ﬂoats is 4 bytes,\nthe Keyframe struct is 16 bytes. This means that every time\nyou look up a key time, you accidentally pull in four keys and\nall the associated keyframe data. If you are doing a binary\n",
      "content_length": 1556,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "162\nCHAPTER 8. OPTIMISATIONS\nsearch of a 128 key stream, it could mean you end up loading\n64 bytes of data and only using 4 bytes of it in up to 5 of the\nsteps. If you change the data layout so the searching takes\nplace in one array, and the data is stored separately, then\nyou get structures that look like this:\n1\nstruct\nKeyData\n2\n{\n3\nfloat x,y,z;\n4\n//\nconsider\npadding\nout to 16\nbytes\nlong\n5\n};\n6\nstruct\nstream\n7\n{\n8\nfloat *times;\n9\nKeyData *values;\n10\nnumKeys;\n11\n};\nListing 8.7: struct of arrays\nDoing this means that for a 128 key stream, the key times\nonly take up 8 cache lines in total, and a binary search is go-\ning to pull in at most three of them, and the data lookup is\nguaranteed to only require one, or two at most if your data\nstraddles two cache lines due to choosing memory space ef-\nﬁciency over performance.\nDatabase technology was here ﬁrst. In DBMS terms, it’s\ncalled column-oriented databases and they provide better\nthroughput for data processing over traditional row-oriented\nrelational databases simply because irrelevant data is not\nloaded when doing column aggregations or ﬁltering. There\nare other features that make column-store databases more\neﬃcient, such as allowing them to collect many keys under\none value instead of having a key value 1:1 mapping, but\ndatabase advances are always being made, and it’s worth\nhunting down current literature to see what else might be\nworth migrating to your codebase.\n",
      "content_length": 1438,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "Chapter 9\nHelping the compiler\nCompilers are rather good at optimising code, but there\nare ways in which we code that make things harder. There\nare tricks we use that break assumptions the compiler can\nmake. In this section, we will look at some of the things we\ndo that we should try not to, and we look at how to introduce\nsome habits that will make it easier for the compiler to do\nwhat we mean, not what we say.\n9.1\nReducing order dependence\nIf the compiler is unable to deduce that the order of oper-\nations is not important to you, then it won’t be able to do\nwork ahead of schedule.\nWhen composing the translated\ncode into intermediate representation form, there’s a quality\nsome compilers use called static single assignment form, or\nSSA. The idea is that you never modify variables once they\nare initially assigned, and instead create new ones when a\nmodiﬁcation becomes required. Although you cannot actu-\nally use this in loops, as any operations which carry through\nwould require the assigned value to change, you can get close\n163\n",
      "content_length": 1044,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "164\nCHAPTER 9. HELPING THE COMPILER\nto it, and in doing so, you can help the compiler understand\nwhat you mean when you are modifying and assigning val-\nues. Skimming the available features and tutorials in lan-\nguages such as Haskell, Erlang, and Single-Assignment C\ncan give you the necessary hints to write your code in a sin-\ngle assignment manner.\nWriting code like this means you will see where the com-\npiler has to branch more easily, but also, you can make\nyour writes more explicit, which means that where a com-\npiler might have had to break away from writing to memory,\nyou can force it to write in all cases, making your process-\ning more homogeneous, and therefore more likely to stream\nbetter.\n9.2\nReducing memory dependency\nLinked lists are expensive due to dependencies, but depen-\ndencies of a diﬀerent sort. Memory being slow, you want to\nbe able to load it in time for your operations, but when the\naddress you need to load is itself still being loaded, you can’t\ncheat anymore. Pointer driven tree algorithms are slow, not\nbecause of the memory lookups, but because the memory\nlookups are chained together.\nIf you want to make your map or set implementation run\nfaster, move to a wide node algorithm such as a B-tree, or\nB*-tree. Hopefully, at some point soon, the STL will allow\nyou to chose the method by which std::map and std::set\nare implemented.\nWhen you have an entity component system using the\ncompositional style, and you have a pointer based composi-\ntion, then the two layers of pointers to get to the component\nis slowing you down. If you have pointers inside those com-\nponents, you’re just compounding the problem.\nAttempt where possible to reduce the number of hops to\n",
      "content_length": 1706,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "9.3. WRITE BUFFER AWARENESS\n165\nget to the data you need. Each hop that depends on previous\ndata is potentially a stall waiting for main memory.\n9.3\nWrite buﬀer awareness\nWhen writing, the same issues need to be considered as\nwhen reading. Try to keep things contiguous where possible.\nTry to keep modiﬁed values separated from read-only values,\nand also from write-only values.\nIn short, write contiguously, in large amounts at a time,\nand use all the bytes, not a small part of them. We need to\ntry to do this, as not only does it help with activation and\ndeactivation of diﬀerent memory pages, but also opens up\nopportunities for the compiler to optimise.\nWhen you have a cache, sometimes it’s important to ﬁnd\nways to bypass it. If you know that you won’t be using the\ndata you’re loading more than once or at least not soon\nenough to beneﬁt from caching, then it can be useful to\nﬁnd ways to avoid polluting the cache. When you write your\ntransforms in simple ways, it can help the compiler promote\nyour operations from ones which pollute the cache, to in-\nstructions that bypass the cache completely. These stream-\ning operations beneﬁt the caches by not evicting randomly\naccessed memory.\nIn the article What every programmer should know about\nmemory[?], Ulrich Drepper talks about many aspects of\nmemory which are interesting to get the most out of your\ncomputer hardware. In the article, he used the term non-\ntemporality to describe the kinds of operations we call\nstreaming.\nThese non-temporal memory operations help\nbecause they bypass the cache completely, which na¨ıvely\nwould seem to be a poor choice, but as the name suggests,\nstreaming data is not likely to be recalled into registers any\ntime soon, so having it available in the cache is pointless, and\n",
      "content_length": 1771,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "166\nCHAPTER 9. HELPING THE COMPILER\nmerely evicts potentially useful data. Streaming operations,\ntherefore, allow you some control over what you consider\nimportant to be in cache, and what is almost certainly not.\n9.4\nAliasing\nAliasing is when it’s possible for pointers to reference the\nsame memory, and therefore require reloading between reads\nif the other pointer has been written to.\nA simple exam-\nple could be where the value we’re looking for is speciﬁed\nby reference, rather than by value, so if any functions that\ncould potentially aﬀect the memory being referred to by that\nlookup reference, then the reference must be re-read before\ndoing a comparison. The very fact it is a pointer, rather than\na value, is what causes the issue.\nA reason to work with data in an immutable way comes\nin the form of preparations for optimisation. C++, as a lan-\nguage, provides a lot of ways for the programmer to shoot\nthemselves in the foot, and one of the best is that pointers to\nmemory can cause unexpected side eﬀects when used with-\nout caution. Consider this piece of code:\n1\nchar\nbuffer[ 100 ];\n2\nbuffer [0] = ’X’;\n3\nmemcpy( buffer +1, buffer , 98 );\n4\nbuffer[ 99 ] = ’\\0’;\nListing 9.1: byte copying\nThis is perfectly correct code if you just want to get a\nstring ﬁlled with 99 ’X’s. However, because this is possible,\nmemcpy has to copy one byte at a time. To speed up copying,\nyou normally load in a lot of memory locations at once, then\nsave them out once they are all in the cache. If your input\ndata can be modiﬁed by writes to your output buﬀer, then\nyou have to tread very carefully. Now consider this:\n",
      "content_length": 1614,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "9.5. RETURN VALUE OPTIMISATION\n167\n1\nint q=10;\n2\nint p[10];\n3\nfor( int i = 0; i < q; ++i )\n4\np[i] = i;\nListing 9.2: trivially parallelisable code\nThe compiler can ﬁgure out that q is unaﬀected, and can\nhappily unroll this loop or replace the check against q with\na register value. However, looking at this code instead:\n1\nvoid\nfoo( int* p, const\nint &q )\n2\n{\n3\nfor( int i = 0; i < q; ++i)\n4\np[i] = i;\n5\n}\n6\n7\nint q=10;\n8\nint p[10];\n9\nfoo( p, q );\nListing 9.3: potentially aliased int\nThe compiler cannot tell that q is unaﬀected by operations\non p, so it has to store p and reload q every time it checks the\nend of the loop. This is called aliasing, where the address of\ntwo variables that are in use are not known to be diﬀerent,\nso to ensure functionally correct code, the variables have to\nbe handled as if they might be at the same address.\n9.5\nReturn value optimisation\nIf you want to return multiple values, the normal way is to\nreturn via reference arguments, or by ﬁlling out an object\npassed by reference. In many cases, return by value can be\nvery cheap as many compilers can turn it into a non-copy\noperation.\nWhen a function attempts to return a structure by con-\nstructing the value in place during the return, it is allowed\nto move the construction straight into the value that will re-\n",
      "content_length": 1301,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "168\nCHAPTER 9. HELPING THE COMPILER\nceive it, without doing a copy at all.\nUtilising std::pair or other small temporary structs can\nhelp by making more of your code run on value types, which\nare not only inherently easier to reason about, but also easier\nto optimise by a compiler.\n9.6\nCache line utilisation\nIt is a truth universally acknowledged that a single memory\nrequest will always read in at least one complete cache line.\nThat complete cache line will contain multiple bytes of data.\nAt the time of writing this book, the most common cache\nline size seems to have stabilized at 64 bytes. With this in-\nformation, we can speculate about what data will be cheap\nto access purely by their location relative to other data.\nIn Searching (Chapter 6), we utilise this information to\ndecide the location and quantity of data that is available for\ncreating the rapid lookup table included in the example that\nuses a two-layer linear search that turns out to be faster than\na binary search.\nWhen you have an object you will be loading into memory,\ncalculate the diﬀerence between a cache line and the size of\nthe object. That diﬀerence is how much memory you have\nleft to place data you can read for free. Use this space to\nanswer the common questions you have about the class, and\nyou will often see speedups as there will be no extra memory\naccesses.\nFor example, consider a codebase that partially migrated\nto components, and still has an entity class which points\nto optional rows in component arrays. In this case, we can\ncache the fact the entity has elements in those arrays in the\nlatter part of the entity class as a bitset. This would mean the\nentity on entity interactions could save doing a lookup into\nthe arrays if there was no matching row. It can also improve\n",
      "content_length": 1775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "9.7. FALSE SHARING\n169\nrender performance as the renderer can immediately tell that\nthere is no damage done, so will just show a full health icon\nor nothing at all.\nIn the example code in listing ?? in Chapter ??, an at-\ntempt is made to use more of an object’s initial cache line to\nanswer questions about the rest of the object, and you can\nsee various levels of success in the results. In the case of\nfully caching the result, a massive improvement was gained.\nIf the result cannot be quickly calculated and needs to be cal-\nculated on demand, caching that there was something to do\nwas a factor of four improvement. Caching the result when\nyou can, had diﬀering levels of performance improvement,\nbased on the likelihood of hitting a cached response. In all,\nusing the extra data you have on your cache line is always\nan improvement over simple checking.\ni5-4430 @ 3.00GHz\nAverage 11.31ms [Simple, check the map]\nAverage\n9.62ms [Partially cached query (25%)]\nAverage\n8.77ms [Partially cached presence (50%)]\nAverage\n3.71ms [Simple, cache presence]\nAverage\n1.51ms [Partially cached query (95%)]\nAverage\n0.30ms [Fully cached query]\nSo, in summary, keep in mind, every time you load any\nmemory at all, you are loading in a full cache line of bytes.\nCurrently, with 64-byte cache lines, that’s a 4x4 matrix of\nﬂoats, 8 doubles, 16 ints, a 64 character ASCII string, or\n512 bits.\n9.7\nFalse sharing\nWhen a CPU core shares no resources with another, it can al-\nways operate at full speed independently, right? Well, some-\ntimes no. Even if the CPU core is working on independent\ndata, there are times it can get choked up on the cache.\n",
      "content_length": 1633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "170\nCHAPTER 9. HELPING THE COMPILER\nOn the opposite side of the same issue as writing linearly,\nwhen you are writing out data to the same cache line, it can\ninterfere with threading. Due to the advancement of compil-\ners, it seems this happens far less frequently than it should,\nand when attempting to reproduce the issue to give ideas on\nthe eﬀect it can have, only by turning oﬀoptimisations is it\npossible to witness the eﬀect with trivial examples.\nThe idea is that multiple threads will want to read from\nand write to the same cache line, but not necessarily the\nsame memory addresses in the cache line. It’s relatively easy\nto avoid this by ensuring any rapidly updated variables are\nkept local to the thread, whether on the stack or in thread lo-\ncal storage. Other data, as long as it’s not updated regularly,\nis highly unlikely to cause a collision.\nThere has been a lot of talk about this particular problem,\nbut the real-world is diﬀerent from the real-world problems\nsupposed. Always check your problems are real after optimi-\nsation, as well as before, as even the high and mighty have\nfallen for this as a cause of massive grief.\n1\nvoid\nFalseSharing () {\n2\nint sum =0;\n3\nint\naligned_sum_store [ NUM_THREADS ] __attribute__ (( aligned (64)))\n;\n4\n#pragma\nomp\nparallel\nnum_threads (NUM_THREADS )\n5\n{\n6\nint me = omp_get_thread_num ();\n7\naligned_sum_store [me] = 0;\n8\nfor (int i = me; i < ELEMENT_COUNT ; i +=\nNUM_THREADS ) {\n9\naligned_sum_store [me] +=\nCalcValue( i );\n10\n}\n11\n#pragma\nomp\natomic\n12\nsum +=\naligned_sum_store [me];\n13\n}\n14\n}\n15\n16\nvoid\nLocalAccumulator () {\n17\nint sum =0;\n18\n#pragma\nomp\nparallel\nnum_threads (NUM_THREADS )\n19\n{\n20\nint me = omp_get_thread_num ();\n21\nint\nlocal_accumulator = 0;\n22\nfor (int i = me; i < ELEMENT_COUNT ; i +=\nNUM_THREADS ) {\n23\nlocal_accumulator\n+=\nCalcValue( i );\n24\n}\n25\n#pragma\nomp\natomic\n26\nsum +=\nlocal_accumulator ;\n27\n}\n28\n}\nListing 9.4: False sharing\nSo, how can you tell if this problem is real or not? If your\n",
      "content_length": 1976,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "9.8. SPECULATIVE EXECUTION AWARENESS\n171\nmulti-threaded code is not growing at a linear rate of pro-\ncessing as you add cores, then you might be suﬀering from\nfalse sharing, look at the where your threads are writing, and\ntry to remove the writes from shared memory where possible\nuntil the last step. The common example given is of adding\nup some arrays and updating the sum value in some global\nshared location, such as in listing 9.4.\nIn the FalseSharing function, the sums are written to as\na shared resource, and each thread will cause the cache to\nclean up and handle that line being dirty for each of the other\ncores before they can update their elements in the cache line.\nIn the second function, LocalAccumulator, each thread sums\nup their series before writing out the result.\n9.8\nSpeculative execution awareness\nSpeculative execution helps as it executes instructions and\nprepares data before we arrive at where we might need them,\neﬀectively allowing us to do work before we know we need it,\nbut sometimes it could have a detrimental eﬀect. For exam-\nple, consider the codebase mentioned previously, that had\npartially migrated to components. The bit arrays of which\noptional tables it was currently resident could lead, through\nspeculation, to loading in details about those arrays. With\nspeculative execution, you will need to watch out for the code\naccidentally prefetching data because it was waiting to ﬁnd\nout the result of a comparison. These speculative operations\nhave been in the news with SPECTRE and MELTDOWN vul-\nnerabilities.\nThese branch prediction caused reads can be reduced by\npre-calculating predicates where possible, storing the result\nof doing a common query in your rows is a big win for most\nmachines and a massive one for machines with poor mem-\nory latency or high CPU bandwidth to memory bandwidth\nratios. Moving to techniques where branch mispredictions\n",
      "content_length": 1894,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "172\nCHAPTER 9. HELPING THE COMPILER\ncause the smallest side-eﬀects to the data is a generally good\nidea. Even caching only when you can, storing the result\nback in the initial section, can save bandwidth over time.\nIn the cache line utilisation section, the numbers showed\nthat the possibility of getting data seemed to aﬀect how fast\nthe process went, much more than it would be expected,\nwhich leads to a belief that speculative loads of unnecessary\ndata were potentially harming overall throughput.\nEven if all you are able to cache is whether a query will\nreturn a result, it can be beneﬁcial. Avoiding lookups into\ncomplex data structures by keeping data on whether or not\nthere are entries matching that description can give speed\nboosts with very few detrimental side-eﬀects.\n9.9\nBranch prediction\nOne of the main causes of stalling in CPUs comes down to not\nhaving any work to do, or having to unravel what they have\nalready done because they predicted badly. If code is specu-\nlatively executed, and requests memory that is not needed,\nthen the load has become a wasteful use of memory band-\nwidth. Any work done will be rejected and the correct work\nhas to be started or continued.\nTo get around this issue,\nthere are ways to make code branch free, but another way is\nto understand the branch prediction mechanism of the CPU\nand help it out.\nIf you make prediction trivial, then the predictor will get\nit right most of the time. If you ensure the conditions are\nconsistently true or false in large chunks, the predictor will\nmake fewer mistakes. A trivial example such as in listing 9.5\nwill predict to either do or not do the accumulation, based\non the incoming data.\nThe work being done here can be\noptimised away by most compilers using a conditional move\ninstruction if the CPU supports it.\nIf you make the work\n",
      "content_length": 1826,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "9.9. BRANCH PREDICTION\n173\ndone a little more realistic, then even with full optimisations\nturned on, you can see a very large diﬀerence1 if you can\nsort the data so the branches are much more predictable.\nThe other thing to remember is that if the compiler can help\nyou, let it. The optimised trivial example is only trivial in\ncomparison to other common workloads, but if your actual\nwork is trivially optimised into a conditional execution, then\nsorting your data will be a waste of eﬀort.\n1\nint\nSumBasedOnData () {\n2\nint sum =0;\n3\nfor (int i = 0; i < ELEMENT_COUNT ; i++) {\n4\nif( a[i] > 128 ) {\n5\nsum += b[i];\n6\n}\n7\n}\n8\nreturn\nsum;\n9\n}\nListing 9.5: Doing work based on data\ni5-4430 @ 3.00GHz\nAverage\n4.40ms [Random branching]\nAverage\n1.15ms [Sorted branching]\nAverage\n0.80ms [Trivial Random branching]\nAverage\n0.76ms [Trivial Sorted branching]\nBranching happens because of data, and remember the\nreason why branching is bad is not that jumps are expen-\nsive, but the work being done because of a misprediction will\nhave to be undone. Because of this, it’s valuable to remem-\nber that a vtable pointer is data too. When you don’t batch\nupdate, you won’t be getting the most out of your branch\npredictor, but even if you don’t hit the branch predictor at\nall, you may still be committing to sequences of instructions\nbased on data.\n1On an i5-4430 the unsorted sum ran in 4.2ms vs the sorted sum run-\nning in 0.8ms. The trivial version, which was likely mostly compiled into\nCMOVs, ran in 0.4ms both sorted and unsorted\n",
      "content_length": 1521,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "174\nCHAPTER 9. HELPING THE COMPILER\n9.10\nDon’t get evicted\nIf you’re working with others, as many are, then perhaps\nthe simplest solution to a lot of issues with poor cache per-\nformance has to take into account other areas of the code.\nIf you’re working on a multi-core machine (you are, unless\nwe went back in time), then there’s a good chance that all\nprocesses are sharing and contending for the caches on the\nmachine. Your code will be evicted from the cache, there is\nno doubt. So will your data. To reduce the chance or fre-\nquency of your code and data being evicted, keep both code\nand data small and process in bursts when you can.\nIt’s very simple advice. Not only is small code less likely to\nbe evicted, but if it’s done in bursts it will have had a chance\nto get a reasonable amount of work before being overwritten.\nSome cache architectures don’t have any way to tell if the\nelements in the cache have been used recently, so they rely\non when they were added as a metric for what should be\nevicted ﬁrst. In particular, some Intel CPUs can have their\nL1 and L2 cache lines evicted because of L3 needing to evict,\nbut L3 doesn’t have full access to LRU information. The Intel\nCPUs in question have some other magic that reduces the\nlikelihood of this happening, but it does happen.\nTo that end, try to ﬁnd ways to guarantee to the compiler\nthat you are working with aligned data, in arrays that are\nmultiples of 4 or 8, or 16, so the compiler doesn’t need to\nadd preambles and postamble code to handle unaligned, or\nirregularly sized arrays. It can be better to have 3 more dead\nelements in an array and handle it as an array of length N ∗4.\n9.11\nAuto vectorisation\nAuto vectorisation will help your applications run faster just\nby enabling it and forming your code in such a way that it\n",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "9.11. AUTO VECTORISATION\n175\nis possible for the compiler to make safe assumptions, and\nchange the instructions from scalar to vector.\n1\nvoid\nAmplify( float *a, float mult , int\ncount )\n2\n{\n3\nfor( int i = 0; i < count; ++i ) {\n4\na[i] *= mult;\n5\n}\n6\n}\nListing 9.6: Trivial ampliﬁcation function\nThere are many trivial examples of things which can be\ncleanly vectorised. The ﬁrst example is found in listing 9.6,\nwhich is simple enough to be vectorised by most compilers\nwhen optimisations are turned on. The issue is that there\nare few guarantees with the code, so even though it can be\nquite fast to process the data, this code will take up a lot\nmore space than is necessary in the instruction cache.\n1\ntypedef\nfloat\nf16\n__attribute__ (( aligned (16)));\n2\n3\nvoid\nAmplify( f16 *a, float mult , int\ncount )\n4\n{\n5\ncount &=\n-4;\n6\nfor( int i = 0; i < count; ++i ) {\n7\na[i] *= mult;\n8\n}\n9\n}\nListing 9.7: Ampliﬁcation function with alignment hints\nIf you can add some simple guarantees, such as by us-\ning aligned pointers, and by giving the compiler some guar-\nantees about the number of elements, then you can cut the\nsize of the emitted assembly, which on a per case basis won’t\nhelp, but over a large codebase, it will increase the eﬀective-\nness of your instruction cache as the number of instructions\nto be decoded is slashed. Listing 9.7 isn’t faster in isolated\ntest beds, but the size of the ﬁnal executable will be smaller,\nas the generated code is less than half the size.\nThis is\na problem with micro-benchmarks, they can’t always show\nhow systems work together or ﬁght against each other. In\nreal-world tests, ﬁxing up the alignment of pointers can im-\nprove performance dramatically. In small test beds, memory\nthroughput is normally the only bottleneck.\n1\ntypedef\nfloat\nf16\n__attribute__ (( aligned (16)));\n2\n3\nvoid\nAmplify( f16 *a, float mult , int\ncount )\n",
      "content_length": 1867,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "176\nCHAPTER 9. HELPING THE COMPILER\n4\n{\n5\ncount &=\n-4;\n6\nfor( int i = 0; i < count; ++i ) {\n7\nif( a[i] < 0 )\n8\nbreak;\n9\na[i] *= mult;\n10\n}\n11\n}\nListing 9.8: Breaking out, breaks vectorisation\nA thing to watch out for is making sure the loops are triv-\nial and always run their course. If a loop has to break based\non data, then it won’t be able to commit to doing all elements\nof the processing, and that means it has to do each element\nat a time. In listing 9.8 the introduction of a break based on\nthe data turns the function from a fast parallel SIMD opera-\ntion auto-vectorisable loop, into a single stepping loop. Note\nthat branching in and of itself does not cause a breakdown in\nvectorisation, but the fact the loop is exited based on data.\nFor example, in listing 9.9, the branch can be turned into\nother operations. It’s also the case that calling out to a func-\ntion can often break the vectorisation, as side eﬀects cannot\nnormally be guaranteed. If the function is a constexpr, then\nthere’s a much better chance it can be consumed into the\nbody of the loop, and won’t break vectorisation. On some\ncompilers, there are certain mathematical functions which\nare available in a vectorised form, such as min, abs, sqrt,\ntan, pow, etc. Find out what your compiler can vectorise. It\ncan often help to write your series of operations out longhand\nto some extent, as trying to shorten the C++ code, can lead\nto slight ambiguities with what the compiler is allowed to do.\nOne thing to watch out for in particular is making sure you\nalways write out. If you only write part of the output stream,\nthen it won’t be able to write out whole SIMD data types, so\nwrite out to your output variable, even if it means reading it\nin, just to write it out again.\n1\ntypedef\nfloat\nf16\n__attribute__ (( aligned (16)));\n2\n3\nvoid\nAmplify( f16 *a, float mult , int\ncount )\n4\n{\n5\ncount &=\n-4;\n6\nfor( int i = 0; i < count; ++i ) {\n7\nf16 val = a[i] * mult;\n8\nif( val > 0 )\n9\na[i] = val;\n10\nelse\n11\na[i] = 0;\n",
      "content_length": 1989,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "9.11. AUTO VECTORISATION\n177\n12\n}\n13\n}\nListing 9.9: Vectorising an if\nAliasing can also aﬀect auto vectorisation, as when point-\ners can overlap, there could be dependencies between diﬀer-\nent members of the same SIMD register. Consider the listing\n9.10, where the ﬁrst version of the function increments each\nmember by its direct neighbour. This function is pointless\nbut serves us as an example. The function will create a pair-\nwise sum all the way to the end ﬂoat by ﬂoat. As such, it\ncannot be trivially vectorised. The second function, though\nequally pointless, makes large enough steps that auto vec-\ntorisation can ﬁnd a way to calculate multiple values per\nstep.\n1\nvoid\nCombineNext ( float *a, int\ncount )\n2\n{\n3\nfor( int i = 0; i < count - 1; ++i ) {\n4\na[i] += a[i+1]\n5\n}\n6\n}\n7\n8\nvoid\nCombineFours ( float *a, int\ncount )\n9\n{\n10\nfor( int i = 0; i < count - 4; ++i ) {\n11\na[i] += a[i+4]\n12\n}\n13\n}\nListing 9.10: Aliasing aﬀecting vectorisation\nDiﬀerent compilers will manage diﬀerent amounts of vec-\ntorisation based on the way you write your code, but in gen-\neral, the simpler you write your code, the more likely the\ncompiler will be able to optimise your source.\nOver the next decade, compilers will get better and better.\nClang already attempts to unroll loops far more than GCC\ndoes, and many new ways to detect and optimise simple code\nwill likely appear. At the time of writing, the online Compiler\nExplorer provided by Matt Godbolt2, provides a good way to\nsee how your code will be compiled into assembly, so you\ncan see what can and will be vectorised, optimised out, re-\narranged, or otherwise mutated into the machine-readable\n2https://godbolt.org/\n",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "178\nCHAPTER 9. HELPING THE COMPILER\nform. Remember that the number of assembly instructions\nis not a good metric for fast code, that SIMD operations are\nnot inherently faster in all cases, and measuring the code\nrunning cannot be replaced by stroking your chin3 while\nthinking about whether the instructions look cool, and you\nshould be okay.\n3or even stroking a beard, or biting a pencil (while making a really serious\nface), as one reviewer pleaded\n",
      "content_length": 451,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "Chapter 10\nMaintenance and\nreuse\nWhen object-oriented design was ﬁrst promoted, it was said\nto be easier to modify and extend existing code bases than\nthe more traditional procedural approach. Though it is not\ntrue in practice, it is often cited by object-oriented developers\nwhen reading about other programming paradigms. Regard-\nless of their level of expertise, an object-oriented program-\nmer will very likely cite the extensible, encapsulating nature\nof object-oriented development as a boon when it comes to\nworking on larger projects.\nHighly experienced but more objective developers have\nadmitted or even written about how object-oriented C++ is\nnot highly suited to big projects with lots of dependencies,\nbut can be used as long as you follow strict guidelines such\nas those found in the Large-scale C++ book[?].\nFor those\nwho cannot immediately see the beneﬁt of the data-oriented\ndevelopment paradigm with respect to maintenance and evo-\nlutionary development, this chapter covers why it is easier\nthan working with objects.\n179\n",
      "content_length": 1042,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "180\nCHAPTER 10. MAINTENANCE AND REUSE\n10.1\nCosmic hierarchies\nWhatever you call them, be it Cosmic Base Class, Root of all\nEvil, Gotcha #97, or CObject, having a base class that every-\nthing derives from has pretty much been a universal failure\npoint in large C++ projects. The language does not naturally\nsupport introspection or duck typing, so it has diﬃculty util-\nising CObjects eﬀectively. If we have a database driven ap-\nproach, the idea of a cosmic base class might make a subtle\nentrance right at the beginning by appearing as the entity to\nwhich all other components are adjectives about, thus not\nletting anything be anything other than an entity. Although\ncomponent–based engines can often be found sporting an\nEntityID as their owner, not all require owners. Not all have\nonly one owner. When you normalise databases, you ﬁnd\nyou have a collection of diﬀerent entity types. In our level\nﬁle example, we saw how the objects we started with turned\ninto a MeshID, TextureID, RoomID, and a PickupID. We even\nsaw the emergence through necessity of a DoorID. If we pile\nall these Ids into a central EntityID, the system should work\nﬁne, but it’s not a necessary step. A lot of entity systems do\ntake this approach, but as is the case with most movements,\nthe ﬁrst swing away from danger often swings too far. The\nbalance is to be found in practical examples of data normal-\nisation provided by the database industry.\n10.2\nDebugging\nThe prime causes of bugs are the unexpected side eﬀects of a\ntransform or an unexpected corner case where a conditional\ndidn’t return the correct value. In object-oriented program-\nming, this can manifest in many ways, from an exception\ncaused by de-referencing a null, to ignoring the interactions\nof the player because the game logic hadn’t noticed it was\nmeant to be interactive.\n",
      "content_length": 1823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "10.2. DEBUGGING\n181\nHolding the state of a system in your head, and playing\ncomputer to ﬁgure out what is going on, is where we get the\nidea that programmers absolutely need to be in the zone\nto get any real work done. The reality is probably far less\nthrilling. The reality is closer to the fear that programmers\nonly need to be in the zone if the code is nearing deadly levels\nof complexity.\n10.2.1\nLifetimes\nOne of the most common causes of the null dereference is\nwhen an object’s lifetime is handled by a separate object\nto the one manipulating it.\nFor example, if you are play-\ning a game where the badguys can die, you have to be care-\nful to update all the objects that are using them whenever\nthe badguy gets deleted, otherwise, you can end up derefer-\nencing invalid memory which can lead to dereferencing null\npointers because the class has destructed.\nData-oriented\ndevelopment tends towards this being impossible as the ex-\nistence of an entity in an array implies its processability, and\nif you leave part of an entity around in a table, you haven’t\ndeleted the entity fully. This is a diﬀerent kind of bug, but\nit’s not a crash bug, and it’s easier to ﬁnd and kill as it’s just\nmaking sure that when an entity is destroyed, all the tables\nit can be part of also destroy their elements too.\n10.2.2\nAvoiding pointers\nWhen looking for data-oriented solutions to programming\nproblems, we often ﬁnd pointers aren’t required, and often\nmake the solution harder to scale. Using pointers where null\nvalues are possible implies each pointer doesn’t only have\nthe value of the object being pointed at, but also implies a\nboolean value for whether or not the instance exists. Remov-\ning this unnecessary extra feature can remove bugs, save\ntime, and reduce complexity.\n",
      "content_length": 1773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "182\nCHAPTER 10. MAINTENANCE AND REUSE\n10.2.3\nBad State\nBugs have a lot to do with not being in the right state. De-\nbugging, therefore, becomes a case of ﬁnding out how the\ngame got into its current, broken state.\n1\nbool\nSingleReturn ( int\nnumDucks ) {\n2\nbool\nvalid = true;\n3\n//\nmust\nbe 10 or\nfewer\nducks.\n4\nif( numDucks\n> 10 ) valid = false;\n5\n//\nnumber\nof\nducks\nshould\nbe\neven.\n6\nvalid = ( numDucks & 1 ) == 0;\n7\n// can ’t have\nnegative\nducks.\n8\nif( numDucks\n< 0 ) valid = false;\n9\nreturn\nvalid;\n10\n}\n11\nbool\nRecursiveCheck ( Node * node\n) {\n12\nbool\nvalid = true;\n13\nif( node ) {\n14\nvalid = node ->Valid ();\n15\nvalid &=\nRecursiveCheck ( node ->sibling );\n16\nvalid &=\nRecursiveCheck ( node ->child );\n17\n}\n18\nreturn\nvalid;\n19\n}\nListing 10.1: Modifying state can shadow history\nWhenever you assign a value to a variable, you are de-\nstroying history. Take the example in listing 10.1. The ideal\nof having only one return statement in a function can cause\nthis kind of error with greater frequency than expected. Hav-\ning more than one return point has its own problems. What’s\nimportant is once you have got to the end of the function, it’s\nhard to ﬁgure out what it was that caused it to fail validation.\nYou can’t even breakpoint the bail points. The recursive ex-\nample is even more dangerous, as there’s a whole tree of ob-\njects and it will recurse through all of them before returning,\nregardless of value, and again, is impossible to breakpoint.\nWhen you encapsulate your state, you hide internal\nchanges.\nThis quickly leads to adding lots of debugging\nlogs. Instead of hiding, data-oriented suggests keeping data\nin simple forms. Potentially, leaving it around longer than\nrequired can lead to highly simpliﬁed transform inspection.\nIf you have a transform that appears to work, but for one\nodd case it doesn’t, the simplicity of adding an assert and\nnot deleting the input data can reduce the amount of guess-\nwork and toil required to generate the reproduction required\n",
      "content_length": 1980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "10.3. REUSABILITY\n183\nto understand the bug and make a clean ﬁx.\nIf you keep\nmost of your transforms as one-way, that is to say, they take\nfrom one source, and produce or update another, then even\nif you run the code multiple times it will still produce the\nsame results as it would have the ﬁrst time. The transform\nis idempotent. This useful property allows you to ﬁnd a bug\nsymptom, then rewind and trace through the causes without\nhaving to attempt to rebuild the initial state.\nOne way of keeping your code idempotent is to write your\ntransforms in a single assignment style. If you operate with\nmultiple transforms but all leading to predicated join points,\nyou can guarantee yourself some timings, and you can look\nback at what caused the ﬁnal state to turn out like it did\nwithout even rewinding. If your conditions are condition ta-\nbles, just leave the inputs around until validity checks have\nbeen completed then you have the ability to go into any live\nsystem and check how it arrived at that state. This alone\nshould reduce any investigation time to a minimum.\n10.3\nReusability\nA feature commonly cited by the object-oriented developers\nwhich seems to be missing from data-oriented development\nis reusability. The idea that you won’t be able to take already\nwritten libraries of code and use them again, or on multiple\nprojects, because the design is partially within the imple-\nmentation. To be sure, once you start optimising your code\nto the particular features of a software project, you do end up\nwith code which cannot be reused. While developing data-\noriented projects, the assumed inability to reuse source code\nwould be signiﬁcant, but it is also highly unlikely. The truth\nis found when considering the true meaning of reusability.\nReusability is not fundamentally concerned with reusing\nsource ﬁles or libraries. Reusability is the ability to main-\ntain an investment in information, or the invention of more\n",
      "content_length": 1934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "184\nCHAPTER 10. MAINTENANCE AND REUSE\nvocabulary with which to communicate intention, such as\nwith the STL, or with other libraries of structural code. In\nthe primary example of reuse as sequences of actions, this\nis a wealth of knowledge for the entity that owns the develop-\nment IP and is very nearly what patents are built on. In the\nlatter, the vocabulary is often stumbled upon, rather than\ntruly invented.\nCopyright law has made it hard to see what resources\nhave value in reuse, as it maintains the source as the ob-\nject of its discussion rather than the intellectual property\nrepresented by the source. The reason for this is that ideas\ncannot be copyrighted, so by maintaining this stance, the\ncopyrighter keeps hold of this tenuous link to a right to with-\nhold information. Reusability comes from being aware of the\ninformation contained within the medium it is stored. In our\ncase, it is normally stored as source code, but the informa-\ntion is not the source code. With object-oriented develop-\nment, the source can be adapted (adapter pattern) to any\nproject we wish to venture. However, the source is not the\ninformation. The information is the order and existence of\ntasks that can and will be performed on the data.\nView-\ning the information this way leads to an understanding that\nany reusability a programming technique can provide comes\ndown to its mutability of inputs and outputs. Its willingness\nto adapt a set of temporally coupled tasks into a new us-\nage framework is how you can ﬁnd out how well it functions\nreusably.\nIn object-oriented development, you apply the informa-\ntion inherent in the code by adapting a class that does the\njob, or wrapper it, or use an agent. In data-oriented devel-\nopment, you copy the functions and schema and transform\ninto and out of the input and output data structures around\nthe time you apply the information contained in the data-\noriented transform.\nEven though, at ﬁrst sight, data-oriented code doesn’t ap-\npear as reusable on the outside, the fact is, it maintains the\n",
      "content_length": 2040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "10.3. REUSABILITY\n185\nsame amount of information in a simpler form, so it’s more\nreusable as it doesn’t carry the baggage of related data or\nfunctions like object-oriented programming, and doesn’t re-\nquire complex transforms to generate the input and extract\nfrom the output like procedural programming tends to gen-\nerate due to the normalising.\nDuck typing, not normally available in object-oriented\nprogramming due to a stricter set of rules on how to in-\nterface between data, can be implemented with templates\nto great eﬀect, turning code which might not be obviously\nreusable into a simple strategy, or a sequence of transforms\nwhich can be applied to data or structures of any type, as\nlong as they maintain a naming convention.\nThe object-oriented C++ idea of reusability is a mix-\nture of information and architecture.\nDeveloping from a\ndata-oriented transform centric viewpoint, architecture just\nseems like a lot of ﬂuﬀcode.\nThe only good architecture\nthat’s worth saving is the actualisation of data-ﬂow and\ntransform.\nThere are situations where an object-oriented\nmodule can be used again, but they are few and far between\nbecause of the inherent diﬃculty interfacing object-oriented\nprojects with each other.\nThe most reusable object-oriented code appears as in-\nterfaces to agents into a much more complex system. The\nbest example of an object-oriented approach that made ev-\nerything easier to handle, that was highly reusable, and was\nfully encapsulated was the FILE type from stdio.h which is\nused as an agent into whatever the platform and OS would\nneed to open, access, write, and read to and from a ﬁle on\nthe system.\n",
      "content_length": 1640,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "186\nCHAPTER 10. MAINTENANCE AND REUSE\n10.4\nReusable functions\nApart from the freedom of extension when it comes to keep-\ning all your data in simple linear arrangements, there is also\nan implicit tendency to turn out accidentally reusable solu-\ntions to problems. This is caused by the data being formatted\nmuch more rigidly, and therefore when it ﬁts, can almost be\nseen as a type of duck-typing. If the data can ﬁt a trans-\nform, a transform should be able to act on it. Some would\nargue, just because the types match, it doesn’t mean the\nfunction will create the expected outcome, but in addition\nto this being avoidable by not reusing code you don’t under-\nstand, in some cases, all you need is to know the signature\nto understand the transform. As an extreme example, it’s\npossible to understand a fair number of Haskell functions\npurely based on their arguments. Finally, because the code\nbecomes much more penetrable, it takes less time to look at\nwhat a transform is doing before committing to reusing it in\nyour own code.\nBecause the data is built in the same way each time,\nhandled with transforms and always being held in the same\ntypes of container, there is a very good chance there are\nmultiple design agnostic optimisations which can be ap-\nplied to many parts of the code. General purpose sorting,\ncounting, searches and spatial awareness systems can be\nattached to new data without calling for OOP adapters or\nimplementing interfaces so Strategies can run over them.\nThis is why it’s possible to have generalised query optimisa-\ntions in databases, and if you start to develop your code this\nway, you can carry your optimisations with you across more\nprojects.\n",
      "content_length": 1678,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "10.5. UNIT TESTING\n187\n10.5\nUnit testing\nUnit testing can be very helpful when developing games, but\nbecause of the object-oriented paradigm making program-\nmers think about code as representations of objects, and\nnot as data transforms, it’s hard to see what can be tested.\nLinking together unrelated concepts into the same object\nand requiring complex setup state before a test can be car-\nried out, has given unit testing a stunted start in games as\nobject-oriented programming caused simple tests to be hard\nto write. Making tests is further complicated by the addition\nof the non-obvious nature of how objects are transformed\nwhen they represent entities in a game world. It can be very\nhard to write unit tests unless you’ve been working with them\nfor a while, and the main point of unit tests is that someone\nwho doesn’t fully grok the system can make changes without\nfalling foul of making things worse.\nUnit testing is mostly useful during refactorings, taking a\ngame or engine from one code and data layout into another\none, ready for future changes. Usually, this is done because\nthe data is in the wrong shape, which in itself is harder to\ndo if you normalise your data as you’re more likely to have\nleft the data in an unconﬁgured form. There will obviously\nbe times when even normalised data is not suﬃcient, such\nas when the design of the game changes suﬃcient to ren-\nder the original data-analysis incorrect, or at the very least,\nineﬀective or ineﬃcient.\nUnit testing is simple with data-oriented technique be-\ncause you are already concentrating on the transform. Gen-\nerating tables of test data would be part of your development,\nso leaving some in as unit tests would be simple, if not part of\nthe process of developing the game. Using unit tests to help\nguide the code could be considered to be partial following\nthe test-driven development technique, a proven good way to\ngenerate eﬃcient and clear code.\nRemember, when you’re doing data-oriented development\n",
      "content_length": 1983,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "188\nCHAPTER 10. MAINTENANCE AND REUSE\nyour game is entirely driven by stateful data and stateless\ntransforms. It is very simple to produce unit tests for your\ntransforms. You don’t even need a framework, just an input\nand output table and then a comparison function to check\nthe transform produced the right data.\n10.6\nRefactoring\nDuring refactoring, it’s always important to know you’ve not\nbroken anything by changing the code.\nAllowing for such\nsimple unit testing gets you halfway there. Another advan-\ntage of data-oriented development is that, at every turn, it\npeels away the unnecessary elements. You might ﬁnd refac-\ntoring is more a case of switching out the order of transforms\nthan changing how things are represented. Refactoring nor-\nmally involves some new data representation, but as long\nas you build your structures with normalisation in mind,\nthere’s going to be little need of that.\nWhen it is needed,\ntools for converting from one schema to another could be\nwritten once and used many times.\nIt might come to pass, as you work with normalised data,\nthat you realise the reason you were refactoring so much in\nthe ﬁrst place, was that you had embedded meaning in the\ncode by putting the data in objects with names, and methods\nthat did things to the objects, rather than transformed the\ndata.\n",
      "content_length": 1313,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "Chapter 11\nWhat’s wrong?\nWhat’s wrong with object-oriented design?\nWhere’s the\nharm in it?\nOver the years, game developers have fallen into a style\nof C++ that is so unappealing to hardware that the managed\nlanguages don’t seem all that much slower in comparison.\nThe pattern of usage of C++ in game development was so ap-\npallingly mismatched to the hardware of the PlayStation 3\nand Xbox 360 generation, it is no wonder an interpreted lan-\nguage is only in the region of 50% slower under normal use\nand sometimes faster1 in their specialist areas. What is this\nstrange language that has embedded itself into the minds of\nC++ game developers? What is it that makes the fashionable\nway of coding games one of the worst ways of making use\nof the machines we’re targeting? Where, in essence, is the\nharm in game-development style object-oriented C++?\nSome of this comes from the initial interpretation of what\nobject-oriented means, as game developers tended to believe\nthat object-oriented meant you had to map instances of ev-\n1http://keithlea.com/javabench/ tells the tale of the server JVM being\nfaster than C++. There are some arguments against the results, but there\nare others backing it up. Read, make up your own mind.\n189\n",
      "content_length": 1230,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "190\nCHAPTER 11. WHAT’S WRONG?\nerything you cared about into the code as instances of ob-\njects.\nThis form of object-oriented development could be\ninterpreted as instance-oriented development, and it puts\nthe singular unique entity ahead of the program as a whole.\nWhen put this way, it is easier to see some of the problems\nthat can arise. Performance of an individual is very hard to\ndecry as poor, as object methods are hard to time accurately,\nand unlikely to be timed at all. When your development prac-\ntices promote individual elements above the program as a\nwhole, you will also pay the mental capacity penalty, as you\nhave to consider all operations from the point of view of the\nactors, with their hidden state, not from a point of view of\nvalue semantics.\nAnother issue is it appears that performance has not been\nignored by the language designers, but potentially instead it\nhas been tested for quality in isolation. This could be be-\ncause the real world uses of C++ are quite diﬀerent from the\nexpectation of the library providers, or it could be the library\nproviders are working to internal metrics instead of making\nsure they understand their customer. It’s the opinion of the\nauthor, when developing a library, or a set of templates for\nuse in C++, it shouldn’t just be possible to tune performance\nout of the code you are using, it should come as default. If\nyou make it possible to tune performance, you trade features\nfor understanding and performance. This is a poor trade for\ngame developers, but has been accepted, as the beneﬁt of a\ncommon language is a very tempting oﬀer.\n11.1\nThe harm\nClaim: Virtuals don’t cost much, but if you call them a lot it\ncan add up.\naka - death by a thousand paper cuts\nThe overhead of a virtual call is negligible under simple\ninspection. Compared to what you do inside a virtual call,\n",
      "content_length": 1841,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "11.1. THE HARM\n191\nthe extra dereference required seems petty and very likely\nnot to have any noticeable side eﬀects other than the cost\nof a dereference and the extra space taken up by the virtual\ntable pointer. The extra dereference before getting the pointer\nto the function we want to call on this particular instance\nseems to be a trivial addition, but let’s have a closer look at\nwhat is going on.\nA class that has derived from a base class with virtual\nmethods has a certain structure to it. Adding any virtual\nmethods to a class instantly adds a virtual table to the exe-\ncutable, and a virtual table pointer as the implicit ﬁrst data\nmember of the class. There is very little way around this. It’s\nallowed in the language speciﬁcation for the data layout of\nclasses to be up to the compiler to the point where they can\nimplement such things as virtual methods by adding hidden\nmembers and generating new arrays of function pointers be-\nhind the scenes. It is possible to do this diﬀerently, but it\nappears most compilers implement virtual tables to store vir-\ntual method function pointers. It’s important to remember\nvirtual calls are not an operating system level concept, and\nthey don’t exist as far as the CPU is concerned, they are just\nan implementation detail of C++.\nWhen we call a virtual method on a class we have to know\nwhat code to run. Normally we need to know which entry in\nthe virtual table to access, and to do that we read the ﬁrst\ndata member in order to access the right virtual table for\ncalling. This requires loading from the address of the class\ninto a register and adding an oﬀset to the loaded value. Every\nnon-trivial virtual method call is a lookup into a table, so in\nthe compiled code, all virtual calls are really function pointer\narray dereferences, which is where the oﬀset comes in. It’s\nthe oﬀset into the array of function pointers. Once the ad-\ndress of the real function pointer is generated, only then can\ninstruction decoding begin. There are ways to not call into\nthe virtual table, notably with C++11, there has been some\nprogress with the final keyword that can help as classes\nthat cannot be overridden can now know that if they call\n",
      "content_length": 2188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "192\nCHAPTER 11. WHAT’S WRONG?\ninto themselves, then they can call functions directly. This\ndoesn’t help for polymorphic calls, or call sites that access\nthe methods from the interface without knowing the concrete\ntype (see listing 11.1), but it can occasionally help with some\nidioms such as private implementation (pImpl), and the cu-\nriously recurring template pattern.\n1\n#include\n<stdio.h>\n2\n3\nclass B {\n4\npublic:\n5\nB() {}\n6\nvirtual ~B() {}\n7\nvirtual\nvoid\nCall () { printf( \"Base\\n\" ); }\n8\nvoid\nLocalCall () {\n9\nCall ();\n10\n}\n11\n};\n12\n13\nclass D final : public B {\n14\npublic:\n15\nD() {}\n16\n~D() {}\n17\nvirtual\nvoid\nCall () { printf( \" Derived \\n\" ); }\n18\nvoid\nLocalCall () {\n19\nCall ();\n20\n}\n21\n};\n22\n23\nB *pb;\n24\nD *pd;\n25\n26\nint\nmain () {\n27\nD *d = new D;\n28\npb = pd = d;\n29\n30\npb ->LocalCall ();\n31\n//\nprints \" Derived \" via\nvirtual\ncall\n32\npd ->LocalCall ();\n33\n//\nprints \" Derived \" via\ndirect\ncall\n34\n}\nListing 11.1: A simple derived class\nFor multiple inheritance it is slightly more convoluted,\nbut basically, it’s still virtual tables, but now each function\nwill deﬁne which class of vtable it will be referencing.\nSo let’s count up the actual operations involved in this\nmethod call: ﬁrst we have a load, then an add, then another\nload, then a branch. To almost all programmers this doesn’t\nseem like a heavy cost to pay for runtime polymorphism.\nFour operations per call so you can throw all your game en-\ntities into one array and loop through them updating, ren-\ndering, gathering collision state, spawning oﬀsound eﬀects.\nThis seems to be a good trade-oﬀ, but it was only a good\ntrade-oﬀwhen these particular instructions were cheap.\n",
      "content_length": 1649,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "11.1. THE HARM\n193\nTwo out of the four instructions are loads, which don’t\nseem like they should cost much, but unless you hit a nearby\ncache, a load takes a long time and instructions take time to\ndecode. The add is very cheap2, to modify the register value\nto address the correct function pointer, but the branch is not\nalways cheap as it doesn’t know where it’s going until the sec-\nond load completes. This could cause an instruction cache\nmiss. All in all, it’s common to see a chunk of time wasted\ndue to a single virtual call in any signiﬁcantly large scale\ngame. In that chunk of time, the ﬂoating point unit alone\ncould have ﬁnished na¨ıvely calculating lots of dot products,\nor a decent pile of square roots. In the best case, the virtual\ntable pointer will already be in memory, the object type the\nsame as last time, so the function pointer address will be the\nsame, and therefore the function pointer will be in cache too,\nand in that circumstance it’s likely the branch won’t stall as\nthe instructions are probably still in the cache too. But this\nbest case is not always the common case for all types of data.\nConsider the alternative, where your function ends, and\nyou are returning some value, then calling into another func-\ntion. The order of instructions is fairly well known, and to\nthe CPU looks very similar to a straight line. There are no\ndeviations from getting instructions based on just following\nthe program counter along each function in turn. It’s possi-\nble to guess quite far ahead the address of any new functions\nthat will be called, as none of them are dependent on data.\nEven with lots of function calls, the fact they are deducible at\ncompile time makes them easy to prefetch, and pretranslate.\nThe implementation of C++ doesn’t like how we iterate\nover objects. The standard way of iterating over a set of het-\nerogeneous objects is to literally do that, grab an iterator and\ncall the virtual function on each object in turn. In normal\ngame code, this will involve loading the virtual table pointer\nfor each and every object. This causes a wait while loading\nthe cache line, and cannot easily be avoided. Once the vir-\ntual table pointer is loaded, it can be used, with the constant\n2Adding to a register before accessing memory is free on most platforms\n",
      "content_length": 2294,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "194\nCHAPTER 11. WHAT’S WRONG?\noﬀset (the index of the virtual method), to ﬁnd the function\npointer to call, however, due to the size of virtual functions\ncommonly found in game development, the table won’t be in\nthe cache. Naturally, this will cause another wait for load,\nand once this load has ﬁnished, we can only hope the object\nis actually the same type as the previous element, otherwise,\nwe will have to wait some more for the instructions to load.\nEven without loads, not knowing which function will be\ncalled until the data is loaded means you rely on a cache line\nof information before you can be conﬁdent you are decoding\nthe right instructions.\nThe reason virtual functions in games are large is that\ngame developers have had it drilled into them that virtual\nfunctions are okay, as long as you don’t use them in tight\nloops, which invariably leads to them being used for more\narchitectural considerations such as hierarchies of object\ntypes, or classes of solution helpers in tree-like problem-\nsolving systems (such as pathﬁnding, or behaviour trees).\nLet’s go over this again: many developers now believe the\nbest way to use virtuals is to put large workloads into the\nbody of the virtual methods, so as to mitigate the overhead\nof the virtual call mechanism.\n3 However, doing this, you\ncan virtually guarantee not only will a large portion of the in-\nstruction and data cache be evicted by each call to update(),\nbut most branch predictor slots may become dirty too, and\nfail to oﬀer any beneﬁt when the next update() runs. As-\nsuming virtual calls don’t add up because they are called\non high-level code is ﬁne until they become the general pro-\ngramming style, leading to developers failing to think about\nhow they aﬀect the application, ultimately leading to millions\nof virtual calls per second. All those ineﬃcient calls are go-\ning to add up and impact the hardware, but they hardly ever\nappear on any proﬁles. The issue isn’t that it’s not there, it’s\nthat it’s spread thinly over the whole of the processing of the\n3There are parallels with task systems, where you want to mitigate the\ncost of setup and tear down of tasks.\n",
      "content_length": 2148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "11.1. THE HARM\n195\nmachine. They always appear somewhere in the code being\ncalled.\nCarlos Bueno’s book Mature Optimization Handbook[?],\ntalks about how it’s very easy to miss the real cause of slow-\nness by blindly following the low hanging fruit approach.\nThis is where the idea of creating a hypothesis can prove\nuseful, as when it turns out to not reap the expected re-\nwards, you can retrace and regroup faster. For Facebook,\nthey traced what was causing evictions and optimised those\nfunctions, not for speed, but to remove as much as possible\nthe chance that they evicted other data from the cache.\nIn C++, classes’ virtual tables store function pointers by\ntheir class. The alternative is to have a virtual table for each\nfunction and switch function pointer on the type of the call-\ning class. This works ﬁne in practice and does save some of\nthe overhead as the virtual table would be the same for all\nthe calls in a single iteration of a group of objects. However,\nC++ was designed to allow for runtime linking to other li-\nbraries, libraries with new classes that may inherit from the\nexisting codebase. The design had to allow a runtime linked\nclass to add new virtual methods, and have them callable\nfrom the original running code. If C++ had gone with func-\ntion oriented virtual tables, the language would have had to\nruntime patch the virtual tables whenever a new library was\nlinked, whether at link-time for statically compiled additions,\nor at runtime for dynamically linked libraries. As it is, us-\ning a virtual table per class oﬀers the same functionality but\ndoesn’t require any link-time or runtime modiﬁcation to the\nvirtual tables as the tables are oriented by the classes, which\nby the language design are immutable during link-time.\nCombining the organisation of virtual tables and the or-\nder in which games tend to call methods, even when running\nthrough lists in a highly predictable manner, cache misses\nare commonplace. It’s not just the implementation of classes\nthat causes these cache misses, it’s any time data is the de-\nciding factor in which instructions are run.\nGames com-\n",
      "content_length": 2115,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "196\nCHAPTER 11. WHAT’S WRONG?\nmonly implement scripting languages, and these languages\nare often interpreted and run on a virtual machine. However\nthe virtual machine or JIT compiler is implemented, there is\nalways an aspect of data controlling which instructions will\nbe called next, and this causes branch misprediction. This\nis why, in general, interpreted languages are slower, they ei-\nther run code based on loaded data in the case of bytecode\ninterpreters or they compile code just in time, which though\nit creates faster code, causes issues of its own.\nWhen a developer implements an object-oriented frame-\nwork without using the built-in virtual functions, virtual ta-\nbles and this pointers present in the C++ language, it doesn’t\nreduce the chance of cache miss unless they use virtual ta-\nbles by function rather than by class. But even when the\ndeveloper has been especially careful, the very fact they are\ndoing object-oriented programming with game developer ac-\ncess patterns, that of calling singular virtual functions on\narrays of heterogeneous objects, they are still going to have\nsome of the same instruction decode and cache misses as\nfound with built-in virtuals. That is, the best they can hope\nfor is one less data dependent CPU state change per virtual\ncall. That still leaves the opportunity for two mispredictions.\nSo, with all this apparent ineﬃciency, what makes game\ndevelopers stick with object-oriented coding practices? As\ngame developers are frequently cited as a source of how the\nbleeding edge of computer software development is progress-\ning, why have they not moved away wholesale from the prob-\nlem and stopped using object-oriented development prac-\ntices all together?\n11.2\nMapping the problem\nClaim: Objects provide a better mapping from the real world\ndescription of the problem to the ﬁnal code solution.\n",
      "content_length": 1851,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "11.2. MAPPING THE PROBLEM\n197\nObject-oriented design when programming in games\nstarts with thinking about the game design in terms of\nentities.\nEach entity in the game design is given a class,\nsuch as ship, player, bullet, or score. Each object maintains\nits own state, communicates with other objects through\nmethods, and provides encapsulation so when the imple-\nmentation of a particular entity changes, the other objects\nthat use it or provide it with utility do not need to change.\nGame developers like abstraction, because historically they\nhave had to write games for not just one target platform, but\nusually at least two.\nIn the past, it was between console\nmanufacturers, but now game developers have to manage\nbetween Windows™and console platforms, plus the mo-\nbile targets too. The abstractions in the past were mostly\nhardware access abstractions, and naturally some gameplay\nabstractions as well, but as the game development indus-\ntry matured, we found common forms of abstractions for\nareas such as physics, AI, and even player control. Finding\nthese common abstractions allowed for third party libraries,\nand many of these use object-oriented design as well. It’s\nquite common for libraries to interact with the game through\nagents. These agent objects contain their own state data,\nwhether hidden or publicly accessible, and provide functions\nby which they can be manipulated inside the constraints of\nthe system that provided them.\nThe game design inspired objects (such as ship, player,\nlevel) keep hold of agents and use them to ﬁnd out what’s\ngoing on in their world. A player interacts with physics, in-\nput, animation, other entities, and doing this through an\nobject-oriented API hides much of the details about what’s\nactually required to do all these diﬀerent tasks.\nThe entities in object-oriented design are containers for\ndata and a place to keep all the functions that manipulate\nthat data.\nDon’t confuse these entities with those of en-\ntity systems, as the entities in object-oriented design are\nimmutable of class over their lifetime.\nAn object-oriented\nentity does not change class during its lifetime in C++ be-\n",
      "content_length": 2149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "198\nCHAPTER 11. WHAT’S WRONG?\ncause there is no process by which to reconstruct a class in\nplace in the language. As can be expected, if you don’t have\nthe right tools for the job, a good workman works around\nit. Game developers don’t change the type of their objects\nat runtime, instead, they create new and destroy old in the\ncase of a game entity that needs this functionality. But as\nis often the case, because the feature is not present in the\nlanguage, it is underutilised even when it would make sense.\nFor example, in a ﬁrst-person shooter, an object will be\ndeclared to represent the animating player mesh, but when\nthe player dies, a clone would be made to represent the dead\nbody as a rag doll. The animating player object may be made\ninvisible and moved to their next spawn point while the dead\nbody object with its diﬀerent set of virtual functions, and\ndiﬀerent data, remains where the player died so as to let the\nplayer watch their dead body. To achieve this sleight of hand,\nwhere the dead body object sits in as a replacement for the\nplayer once they are dead, copy constructors need to be de-\nﬁned. When the player is spawned back into the game, the\nplayer model will be made visible again, and if they wish to,\nthe player can go and visit their dead clone. This works re-\nmarkably well, but it is a trick that would be unnecessary if\nthe player could become a dead rag doll rather than spawn\na clone of a diﬀerent type. There is an inherent danger in\nthis too, the cloning could have bugs, and cause other is-\nsues, and also if the player dies but somehow is allowed to\nresurrect, then they have to ﬁnd a way to convert the rag doll\nback into the animating player, and that is no simple feat.\nAnother example is in AI. The ﬁnite state machines and\nbehaviour trees that run most game AI maintain all the data\nnecessary for all their potential states. If an AI has three\nstates, { Idle, Making-a-stand, Fleeing-in-terror } then it has\nthe data for all three states.\nIf the Making-a-stand has a\nscared-points accumulator for accounting their fear, so they\ncan ﬁght, but only up until they are too scared to continue,\nand the Fleeing-in-terror has a timer so they will ﬂee, but\nonly for a certain time, then Idle will have these two unnec-\n",
      "content_length": 2256,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "11.2. MAPPING THE PROBLEM\n199\nessary attributes as well. In this trivial example, the AI class\nhas three data entries, { state, how-scared, ﬂee-time }, and\nonly one of these data entries is used by all three states. If\nthe AI could change type when it transitioned from state to\nstate, then it wouldn’t even need the state member, as that\nfunctionality would be covered by the virtual table pointer.\nThe AI would only allocate space for each of the state track-\ning members when in the appropriate state. The best we can\ndo in C++ is to fake it by changing the virtual table pointer\nby hand, dangerous but possible, or setting up a copy con-\nstructor for each possible transition.\nApart from immutable type, object-oriented development\nalso has a philosophical problem.\nConsider how humans\nperceive objects in real life. There is always a context to every\nobservation. The humble table, when you look at it, you may\nsee it to be a table with four legs, made of wood and modestly\npolished. If so, you will see it as being a brown colour, but\nyou will also see the reﬂection of the light. You will see the\ngrain, but when you think about what colour it is, you will\nthink of it as being one colour.\nHowever, if you have the\ntraining of an artist, you will know what you see is not what is\nactually there. There is no solid colour, and if you are looking\nat the table, you cannot see its precise shape, but only infer\nit. If you are inferring it is brown by the average light colour\nentering your eye, then does it cease to be brown if you turn\noﬀthe light? What about if there is too much light and all\nyou can see is the reﬂection oﬀthe polished surface? If you\nclose one eye and look at its rectangular form from one of the\nlong sides, you will not see right angle corners, but instead,\na trapezium. We automatically adjust for this and classify\nobjects when we see them. We apply our prejudices to them\nand lock them down to make reasoning about them easier.\nThis is why object-oriented development is so appealing to\nus.\nHowever, what we ﬁnd easy to consume as humans,\nis not optimal for a computer. When we think about game\nentities being objects, we think about them as wholes. But\na computer has no concept of objects, and only sees objects\nas being badly organised data and functions randomly called\n",
      "content_length": 2306,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "200\nCHAPTER 11. WHAT’S WRONG?\non it.\nIf you take another example from the table, consider the\ntable to have legs about three feet long. That’s someone’s\nstandard table. If the legs are only one foot long, it could be\nconsidered to be a coﬀee table. Short, but still usable as a\nplace to throw the magazines and leave your cups. But when\nyou get down to one inch long legs, it’s no longer a table, but\ninstead, just a large piece of wood with some stubs stuck on\nit. We can happily classify the same item but with diﬀerent\ndimensions into three distinct classes of object. Table, coﬀee\ntable, a lump of wood with some little bits of wood on it. But,\nat what point does the lump of wood become a coﬀee table?\nIs it somewhere between 4 and 8 inch long legs? This is the\nsame problem as presented about sand, when does it transi-\ntion from grains of sand to a pile of sand? How many grains\nare a pile, are a dune? The answer must be that there is no\nanswer. The answer is also helpful in understanding how a\ncomputer thinks. It doesn’t know the speciﬁc diﬀerence be-\ntween our human classiﬁcations because to a certain degree\neven humans don’t.\nThe class of an object is poorly deﬁned by what it is, but\nbetter by what it does. This is why duck typing is a strong\napproach. We also realise, if a type is better deﬁned by what\nit can do, then when we get to the root of what a polymor-\nphic type is, we ﬁnd it is only polymorphic in terms of what\nit can do. In C++, it’s clear a class with virtual functions can\nbe called as a runtime polymorphic instance, but it might\nnot have been clear that if it didn’t have those functions, it\nwould not need to be classiﬁed in the ﬁrst place. The reason\nmultiple inheritance is useful stems from this. Multiple in-\nheritance just means an object can behave, that is react, to\ncertain impulses. It has declared that it can fulﬁl some con-\ntract of polymorphic function response. If polymorphism is\njust the ability for an object to fulﬁl a functionality contract,\nthen we don’t need virtual calls to handle that every time, as\nthere are other ways to make code behave diﬀerently based\non the object.\n",
      "content_length": 2134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "11.2. MAPPING THE PROBLEM\n201\nIn most games engines, the object-oriented approach\nleads to a lot of objects in very deep hierarchies. A common\nancestor chain for an entity might be: PlayerEntity →Char-\nacterEntity →MovingEntity →PhysicalEntity →Entity →\nSerialisable →ReferenceCounted →Base.\nThese deep hierarchies virtually guarantee multiple indi-\nrect calls when calling virtual methods, but they also cause\na lot of pain when it comes to cross-cutting code, that is\ncode that aﬀects or is aﬀected by unrelated concerns, or\nconcerns incongruous to the hierarchy. Consider a normal\ngame with characters moving around a scene. In the scene\nyou will have characters, the world, possibly some particle\neﬀects, lights, some static and some dynamic. In this scene,\nall these things need to be rendered, or used for rendering.\nThe traditional approach is to use multiple inheritance or to\nmake sure there is a Renderable base class somewhere in ev-\nery entity’s inheritance chain. But what about entities that\nmake noises? Do you add an audio emitter class as well?\nWhat about entities that are serialised vs those that are ex-\nplicitly managed by the level?\nWhat about those that are\nso common they need a diﬀerent memory manager (such as\nthe particles), or those that only optionally have to be ren-\ndered (like trash, ﬂowers, or grass in the distance). This has\nbeen solved numerous times by putting all the most com-\nmon functionality into the core base class for everything in\nthe game, with special exceptions for special circumstances,\nsuch as when the level is animated, when a player character\nis in an intro or death screen, or is a boss character (who is\nspecial and deserves a little more code). These hacks are only\nnecessary if you don’t use multiple inheritance, but when\nyou use multiple inheritance you then start to weave a web\nthat could ultimately end up with virtual inheritance and the\ncomplexity of state that brings with it. The compromise al-\nmost always turns out to be some form of cosmic base class\nanti-pattern.\nObject-oriented development is good at providing a hu-\nman oriented representation of the problem in the source\n",
      "content_length": 2149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "202\nCHAPTER 11. WHAT’S WRONG?\ncode, but bad at providing a machine representation of the\nsolution. It is bad at providing a framework for creating an\noptimal solution, so the question remains: why are game\ndevelopers still using object-oriented techniques to develop\ngames? It’s possible it’s not about better design, but instead,\nmaking it easier to change the code.\nIt’s common knowl-\nedge that game developers are constantly changing code to\nmatch the natural evolution of the design of the game, right\nup until launch. Does object-oriented development provide\na good way of making maintenance and modiﬁcation simpler\nor safer?\n11.3\nInternalised state\nClaim: Encapsulation makes code more reusable.\nIt’s eas-\nier to modify the implementation without aﬀecting the usage.\nMaintenance and refactoring become easy, quick, and safe.\nThe idea behind encapsulation is to provide a contract to\nthe person using the code rather than providing a raw imple-\nmentation. In theory, well written object-oriented code that\nuses encapsulation is immune to damage caused by chang-\ning how an object manipulates its data. If all the code us-\ning the object complies with the contract and never directly\nuses any of the data members without going through acces-\nsor functions, then no matter what you change about how\nthe class fulﬁls that contract, there won’t be any new bugs\nintroduced by any change. In theory, the object implemen-\ntation can change in any way as long as the contract is not\nmodiﬁed, but only extended. This is the open closed prin-\nciple. A class should be open for extension, but closed for\nmodiﬁcation.\nA contract is meant to provide some guarantees about\nhow a complex system works. In practice, only unit testing\ncan provide these guarantees.\n",
      "content_length": 1753,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "11.3. INTERNALISED STATE\n203\nSometimes, programmers unwittingly rely on hidden fea-\ntures of objects’ implementations. Sometimes the object they\nrely on has a bug that just so happens to ﬁt their use case.\nIf that bug is ﬁxed, then the code using the object no longer\nworks as expected. The use of the contract, though it was\nkept intact, has not helped the other piece of code to main-\ntain working status across revisions.\nInstead, it provided\nfalse hope that the returned values would not change.\nIt\ndoesn’t even have to be a bug. Temporal couplings inside ob-\njects or accidental or undocumented features that go away in\nlater revisions can also damage the code using the contract\nwithout breaking it.\nConsider an implementation that maintained an internal\nlist in sorted order, and a use case that accidentally relied\non it (an unforeseen bug in the user’s use case, not an inten-\ntional dependency), but when the maintainer pushes out a\nperformance enhancing update, the only thing the users are\ngoing to see is a pile of new bugs, and they will likely assume\nthe performance update is suspect, not their own code.\nA concrete example could be an item manager that kept\na list of items sorted by name. If the function returns all the\nitem types that match a ﬁlter, then the caller could iterate\nthe returned list until it found the item it wanted. To speed\nthings up, it could early-out if it found an item with a name\nlater than the item it was looking for, or it could do a binary\nsearch of the returned list. In both those cases, if the inter-\nnal representation changed to something that wasn’t ordered\nby name, then the code would no longer work. If the internal\nrepresentation was changed so it was ordered by hash, then\nthe early-out and binary search would be completely broken.\nIn many linked list implementations, there is a decision\nmade about whether to store the length of the list or not. The\nchoice to store a count member will make multi-threaded ac-\ncess slower, but the choice not to store it will make ﬁnding\nthe length of the list an O(n)operation. For situations where\nyou only want to ﬁnd out whether the list is empty, if the ob-\n",
      "content_length": 2158,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "204\nCHAPTER 11. WHAT’S WRONG?\nject contract only supplies a get count() function, you can-\nnot know for sure whether it would be cheaper to check if\nthe count was greater than zero, or check if the begin() and\nend() are the same. This is another example of the contract\nbeing too little information.\nEncapsulation only seems to provide a way to hide bugs\nand cause assumptions in programmers.\nThere is an old\nsaying about assumptions, and encapsulation doesn’t let you\nconﬁrm or deny them unless you have access to the source\ncode.\nIf you have, and you need to look at it to ﬁnd out\nwhat went wrong, then all the encapsulation has done is\nadd another layer to work around rather than add any useful\nfunctionality of its own.\n11.4\nInstance oriented development\nClaim: Making every object an instance makes it very easy\nto think about what an object’s responsibilities are, what its\nlifetime looks like, and where it belongs in the world of objects.\nThe ﬁrst problem with instance thinking is that every-\nthing is centred around the idea of one item doing a thing,\nand that is a sure way to lead to poor performance.\nThe second, and more pervasive issue with instance\nthinking is it leads to thinking in the abstract about in-\nstances, and using full objects as building blocks for thought\ncan lead to very ineﬃcient algorithms. When you hide the\ninternal representation of an item even from the programmer\nusing it, you often introduce issues of translation from one\nway of thinking about an object to another, and back again.\nSometimes you may have an item that needs to change an-\nother object, but cannot reach it in the world it ﬁnds itself, so\nhas to send a message to its container to help it achieve the\ngoal of answering a question about another entity. Unfortu-\nnately, it’s not uncommon for programs to lose sight of the\n",
      "content_length": 1830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "11.4. INSTANCE ORIENTED DEVELOPMENT\n205\ndata requirement along these routes, and send more than\nnecessary in the query, or in the response, carrying around\nnot only unnecessary permissions, but also unnecessary\nlimitations due to related system state.\nAs an example of how things can go wrong, imagine a\ncity building game where the population has happiness rat-\nings. If each individual citizen has a happiness rating, then\nthey will need to calculate that happiness rating. Let’s as-\nsume the number of citizens isn’t grossly overwhelming, with\nmaybe a maximum of a thousand buildings and up to ten\ncitizens per building. If we only calculate the happiness of\nthe citizens when necessary, it will speed things up, and\nin at least one game where these numbers are similar, lazy\nevaluation of the citizen happiness was the way things were\ndone. How the happiness is calculated can be an issue if it\nis worked out from the perspective of the individual, rather\nthan the perspective of the city. If a citizen is happy when\nthey are close to work, close to local amenities, far from in-\ndustrial locations, and able to get to recreational areas eas-\nily, then a lot of the happiness rating comes from a kind of\npathﬁnding. If the result of pathﬁnding is cached, then at\nleast the citizens in the same building can beneﬁt, but ev-\nery building will have small diﬀerences in distances to each\nof the diﬀerent types of building. Running pathﬁnding over\nthat many instances is very expensive.\nIf instead, the city calculates happiness, it can build a\nmap of distances from each of the types of building under\nconsideration as a ﬂood ﬁll pass and create a general dis-\ntance map of the whole city using a Floyd-Warshall algorithm\nto help citizens decide on how close their places of work are.\nNormally, substituting an O(n3)algorithm for an O(n2)could\nbe seen as silly, but the pathﬁnding is being done for each cit-\nizen, so becomes O(n2m) and is not in fact algorithmically su-\nperior. Finally, this is the real world, and doing the pathing\nitself has other overheads, and running the Floyd-Warshall\nalgorithm to generate a lookup before calculating happiness\nmeans the work to calculate happiness can be simpler (in\n",
      "content_length": 2211,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "206\nCHAPTER 11. WHAT’S WRONG?\ndata storage terms), and require fewer branches oﬀinto sup-\nporting code. The Floyd-Warshall algorithm can also have a\npartial update run upon it, using the existing map to indi-\ncate which items need to be updated. If running from the\ninstance point of view, knowing a change to the topology or\nthe type of buildings nearby would require doing some form\nof distance check per instance.\nIn conclusion, abstractions form the basis of solving diﬃ-\ncult problems, but in games, we’re often not solving diﬃcult\nalgorithmic problems at a gameplay level. To the contrary, we\nhave a tendency to abstract too early, and object-oriented de-\nsign often gives us an easy and recognisable way to commit\nto abstractions without rendering the costs apparent until\nmuch later, when we have become too dependent upon them\nto clear them away without impacting other code.\n11.5\nHierarchical design vs change\nClaim: Inheritance allows reuse of code by extension. Adding\nnew features is simple.\nInheritance was seen as a major reason to use classes in\nC++ by game programmers. The obvious beneﬁt was being\nable to inherit from multiple interfaces to gain attributes or\nagency in system objects such as physics, animation, and\nrendering. In the early days of C++ adoption, the hierarchies\nwere shallow, not usually going much more than three layers\ndeep, but later it became commonplace to ﬁnd more than\nnine levels of ancestors in central classes such as that of\nthe player, their vehicles, or the AI players. For example, in\nUnreal Tournament, the minigun ammo object had this:\nMiniammo →TournamentAmmo →Ammo →Pickup →\nInventory →Actor →Object\nGame developers use inheritance to provide a robust way\nto implement polymorphism in games, where many game en-\n",
      "content_length": 1767,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "11.5. HIERARCHICAL DESIGN VS CHANGE\n207\ntities can be updated, rendered, or queried en-mass, without\nany hand coded checking of type. They also appreciate the\nreduced copy-pasting, because inheriting from a class also\nadds functionality to a class. This early form of mix-ins was\nseen to reduce errors in coding as there were often times\nwhere bugs only existed because a programmer had ﬁxed\na bug in one place, but not all of them. Gradually, multiple\ninheritance faded into interfaces only, the practice of only in-\nheriting from one real class, and any others had to be pure\nvirtual interface classes as per the Java deﬁnition.\nAlthough it seems like inheriting from class to extend its\nfunctionality is safe, there are many circumstances where\nclasses don’t quite behave as expected when methods are\noverridden. To extend a class, it is often necessary to read\nthe source, not just of the class you’re inheriting, but also\nthe classes it inherits too. If a base class creates a pure vir-\ntual method, then it forces the child class to implement that\nmethod. If this was for a good reason, then that should be\nenforced, but you cannot enforce that every inheriting class\nimplements this method, only the ﬁrst instantiable class in-\nheriting it. This can lead to obscure bugs where a new class\nsometimes acts or is treated like the class it is inheriting\nfrom.\nA feature missing from C++ also is the idea of being non-\nvirtual. You cannot declare a function as not being virtual.\nThat is, you can deﬁne that a function is an override, but\nyou cannot declare that it is not an override. This can cause\nissues when common words are used, and a new virtual\nmethod is brought into existence. If it overlaps extant func-\ntions with the same signature, then you likely have a bug.\n1\nclass A {\n2\nvirtual\nvoid\nfoo( int bar = 5 ) { cout\n<< bar; }\n3\n};\n4\nclass B : public A {\n5\nvoid\nfoo( int bar = 7 ) { cout\n<< bar * 2; }\n6\n};\n7\nint\nmain( int argc , char *argv [] ) {\n8\nA *a = new B;\n9\na->foo ();\n10\nreturn\n0;\n11\n}\nListing 11.2: Runtime, compile-time, or link-time?\n",
      "content_length": 2059,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "208\nCHAPTER 11. WHAT’S WRONG?\nAnother pitfall of inheritance in C++ comes in the form\nof runtime versus compile time linking. A good example is\ndefault arguments on method calls and badly understood\noverriding rules. What would you expect the output of the\nprogram in listing 11.2 to be?\nWould you be surprised to ﬁnd out it reported a value of\n10? Some code relies on the compiled state, some on run-\ntime. Adding new functionality to a class by extending it can\nquickly become a dangerous game as classes from two lay-\ners down can cause coupling side eﬀects, throw exceptions\n(or worse, not throw an exception and quietly fail), circum-\nvent your changes, or possibly just make it impossible to\nimplement your feature as they might already be taking up\nthe namespace or have some other incompatibility with your\nplans, such as requiring a certain alignment or need to be\nin a certain bank of ram.\nInheritance does provide a clean way of implementing\nruntime polymorphism, but it’s not the only way as we saw\nearlier. Adding a new feature by inheritance requires revis-\niting the base class, providing a default implementation, or\na pure virtual, then providing implementations for all the\nclasses that need to handle the new feature. This requires\nmodiﬁcation to the base class, and possible touching all of\nthe child classes if the pure virtual route is taken. So even\nthough the compiler can help you ﬁnd all the places where\nthe code needs to change, it has not made it signiﬁcantly\neasier to change the code.\nUsing a type member instead of a virtual table pointer can\ngive you the same runtime code linking, could be better for\ncache misses, and could be easier to add new features and\nreason about because it has less baggage when it comes to\nimplementing those new features, provides a very simple way\nto mix and match capabilities compared to inheritance, and\nkeeps the polymorphic code in one place. For example, in\nthe fake virtual function go-forward, the class Car will step\non the gas. In the class Person, it will set the direction vec-\n",
      "content_length": 2053,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "11.6. DIVISIONS OF LABOUR\n209\ntor. In the class UFO, it will also just set the direction vector.\nThis sounds like a job for a switch statement fall through.\nIn the fake virtual function re-fuel, the class Car and UFO\nwill start a re-fuel timer and remain stationary while their\nfuelling-up animations play, whereas the Person class could\njust reduce their stamina-potion count and be instantly re-\nfuelled. Again, a switch statement with fall through provides\nall the runtime polymorphism you need, but you don’t need\nto multiple inherit in order to provide diﬀerent functionality\non a per class per function level. Being able to pick what\neach method does in a class is not something inheritance is\ngood at, but it is something desirable, and non inheritance\nbased polymorphism does allow it.\nThe original reason for using inheritance was that you\nwould not need to revisit the base class, or change any of\nthe existing code in order to extend and add functionality\nto the codebase, however, it is highly likely you will at least\nneed to view the base class implementation, and with chang-\ning speciﬁcations in games, it’s also quite common to need\nchanges at the base class level.\nInheritance also inhibits\ncertain types of analysis by locking people into thinking of\nobjects as having IS-A relationships with the other object\ntypes in the game. A lot of ﬂexibility is lost when a program-\nmer is locked out of conceptualising objects as being com-\nbinations of features. Reducing multiple inheritance to in-\nterfaces, though helping to reduce the code complexity, has\ndrawn a veil over the one good way of building up classes\nas compound objects. Although not a good solution in itself\nas it still abuses the cache, a switch on type seems to of-\nfer similar functionality to virtual tables without some of the\nassociated baggage. So why put things in classes?\n11.6\nDivisions of labour\nClaim: Modular architecture for reduced coupling and better\ntesting\n",
      "content_length": 1956,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "210\nCHAPTER 11. WHAT’S WRONG?\nThe object-oriented paradigm is seen as another tool in\nthe kit when it comes to ensuring quality of code. Strictly ad-\nhering to the open closed principle, always using accessors,\nmethods, and inheritance to use or extend objects, program-\nmers write signiﬁcantly more modular code than they do if\nprogramming from a purely procedural perspective.\nThis\nmodularity separates each object’s code into units. These\nunits are collections of all the data and methods that act\nupon the data. It has been written about many times that\ntesting objects is simpler because each object can be tested\nin isolation.\nHowever, we know it to be untrue, due to data being linked\ntogether by purpose, and purposes being linked together by\ndata in a long chain of accidental relationships.\nObject-oriented design suﬀers from the problem of errors\nin communication.\nObjects are not systems, and systems\nneed to be tested, and systems comprise of not only objects,\nbut their inherent communication. The communication of\nobjects is diﬃcult to test because in practice it is hard to iso-\nlate the interactions between classes. Object-oriented devel-\nopment leads to an object-oriented view of the system which\nmakes it hard to isolate non-objects such as data trans-\nforms, communication, and temporal coupling.\nModular architecture is good because it limits the poten-\ntial damage caused by changes, but just like encapsulation\nbefore, the contract to any module has to be unambiguous so\nas to reduce the chance of external reliance on unintended\nside eﬀects of the implementation.\nThe reason object-oriented modular approach doesn’t\nwork as well is that the modules are deﬁned by object\nboundary, not by a higher level concept.\nGood examples\nof modularity include stdio’s FILE, the CRT’s malloc/free,\nThe NvTriStrip library’s GenerateStrips. Each of these pro-\nvides a solid, documented, narrow set of functions to access\nfunctionality that could otherwise be overwhelming and dif-\nﬁcult to reason about.\n",
      "content_length": 2014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "11.7. REUSABLE GENERIC CODE\n211\nModularity in object-oriented development can oﬀer pro-\ntection from other programmers who don’t understand the\ncode. But why is a programmer that doesn’t understand the\ncode going to be safe even using a trivialised and simpli-\nﬁed interface?\nAn object’s methods are often the instruc-\ntion manual for an object in the eyes of someone new to\nthe code, so writing all the important manipulation methods\nin one block can give clues to anyone using the class. The\nmodularity is important here because game development ob-\njects are regularly large, oﬀering a lot of functionality spread\nacross their many diﬀerent aspects. Rather than ﬁnd a way\nto address cross-cutting concerns, game objects tend to ful-\nﬁl all requirements rather than restrict themselves to their\noriginal design. Because of this bloating, the modular ap-\nproach, that is, collecting methods by their concern in the\nsource, can be beneﬁcial to programmers coming at the ob-\nject fresh.\nThe obvious way to ﬁx this would be to use a\nparadigm that supports cross-cutting concerns at a more\nfundamental level, but object-oriented development in C++\nseems to be ineﬃcient at representing this in code.\nIf object-oriented development doesn’t increase modular-\nity in such a way as it provides better results than explicitly\nmodularising code, then what does it oﬀer?\n11.7\nReusable generic code\nClaim: Faster development time through reuse of generic code\nIt is regarded as one of the holy grails of development to be\nable to consistently reduce development overhead by reusing\nold code. In order to stop wasting any of the investment in\ntime and eﬀort, it’s been assumed it will be possible to put\ntogether an application from existing code and only have to\nwrite some minor new features.\nThe unfortunate truth is\nany interesting new features you want to add will probably\nbe incompatible with your old code and old way of laying out\n",
      "content_length": 1928,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "212\nCHAPTER 11. WHAT’S WRONG?\nyour data, and you will need to either rewrite the old code\nto allow for the new feature, or rewrite the old code to allow\nfor the new data layout. If a software project can be built\nfrom existing solutions, from objects invented to provide fea-\ntures for an old project, then it’s probably not very complex.\nAny project of signiﬁcant complexity includes hundreds if\nnot thousands of special case objects that provide all par-\nticular needs of that project. For example, the vast major-\nity of games will have a player class, but almost none share\na common core set of attributes. Is there a world position\nmember in a game of poker? Is there a hit point count mem-\nber in the player of a racing game? Does the player have a\ngamer tag in a purely oﬄine game? Having a generic class\nthat can be reused doesn’t make the game easier to create,\nall it does is move the specialisation into somewhere else.\nSome game toolkits do this by allowing script to extend the\nbasic classes. Some game engines limit the gameplay to a\ncertain genre and allow extension away from that through\ndata-driven means. No one has so far created a game API,\nbecause to do so, it would have to be so generic it wouldn’t\nprovide anything more than what we already have with our\nlanguages we use for development.\nReuse, being hankered after by production, and thought\nof so highly by anyone without much experience in making\ngames, has become an end in itself for many game devel-\nopers. The pitfall of generics is a focus on keeping a class\ngeneric enough to be reused or re-purposed without thought\nas to why, or how. The ﬁrst, the why, is a major stumbling\nblock and needs to be taught out of developers as quickly as\npossible. Making something generic, for the sake of general-\nity, is not a valid goal. Making something generic in the ﬁrst\ninstance adds time to development without adding value.\nSome developers would cite this as short-sighted, however,\nit is the how that deﬂates this argument. How do you gener-\nalise a class if you only use it in one place? The implementa-\ntion of a class is testable only so far as it can be tested, and\nif you only use a class in one place, you can only test that\nit works in one situation. The quality of a class’s reusability\n",
      "content_length": 2274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "11.7. REUSABLE GENERIC CODE\n213\nis inherently untestable until there is something to reuse it,\nand the general rule of thumb is that it’s not reusable unless\nthere are at least three things using it. If you then generalise\nthe class, yet don’t have any other test cases than the ﬁrst\nsituation, then all you can test is that you didn’t break the\nclass when generalising it. So, if you cannot guarantee that\nthe class works for other types or situations, all you have\ndone by generalising the class is added more code for bugs\nto hide in. The resultant bugs are now hidden in code that\nworks, possibly even tested in its isolation, which means any\nbugs introduced during this generalising have been stamped\nand approved, and are now trusted.\nTest-driven development implicitly denies generic coding\nuntil the point where it is a good choice to do so. The only\ntime when it is a good choice to move code to a more generic\nstate, is when it reduces redundancy through reuse of com-\nmon functionality.\nGeneric code has to fulﬁl more than just a basic set of\nfeatures if it is to be used in many situations. If you write a\ntemplated array container, access to the array through the\nsquare bracket operators would be considered a basic fea-\nture, but you will also want to write iterators for it and pos-\nsibly add an insert routine to take the headache out of shuf-\nﬂing the array up in memory.\nLittle bugs can creep in if\nyou rewrite these functions whenever you need them, and\nlinked lists are notorious for having bugs in quick and dirty\nimplementations. To be ﬁt for use by all users, any generic\ncontainer should provide a full set of methods for manipu-\nlation, and the STL does that. There are hundreds of dif-\nferent functions to understand before you can be considered\nan STL-expert, and you have to be an STL-expert before you\ncan be sure you’re writing eﬃcient code with the STL. There\nis a large amount of documentation available for the various\nimplementations of the STL. Most of the implementations of\nthe STL are very similar if not functionally the same. Even\nso, it can take some time for a programmer to become a valu-\nable STL programmer due to this need to learn another lan-\n",
      "content_length": 2192,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "214\nCHAPTER 11. WHAT’S WRONG?\nguage. The programmer has to learn a new language, the\nlanguage of the STL, with its own nouns verbs and adjec-\ntives.\nTo limit this, many games companies have a much\nreduced feature set reinterpretation of the STL that option-\nally provides better memory handling (because of the awk-\nward hardware), more choice for the containers (so you may\nchoose a hash-map, trie, or b-tree directly, rather than just a\nmap), or explicit implementations of simpler containers such\nas stack or singly linked lists and their intrusive brethren.\nThese libraries are normally smaller in scope and are there-\nfore easier to learn and hack than the STL variants, but they\nstill need to be learnt and that takes some time. In the past\nthis was a good compromise, but now the STL has extensive\nonline documentation, there is no excuse not to use the STL\nexcept where memory overhead is very intrusive, such as\nin the embedded space where main memory is measured in\nkilobytes, or where compilation time is of massive concern4.\nThe takeaway from this, however, is that generic code still\nneeds to be learnt in order for the coder to be eﬃcient, or not\ncause accidental performance bottlenecks.\nIf you go with\nthe STL, then at least you have a lot of documentation on\nyour side. If your game company implements an amazingly\ncomplex template library, don’t expect any coders to use it\nuntil they’ve had enough time to learn it, and that means,\nif you write generic code, expect people to not use it unless\nthey come across it accidentally, or have been explicitly told\nto, as they won’t know it’s there, or won’t trust it. In other\nwords, starting out by writing generic code is a good way to\nwrite a lot of code quickly without adding any value to your\ndevelopment.\n4The STL is large, but not as large as some OS headers, so ﬁght the right\nbattle ﬁrst\n",
      "content_length": 1860,
      "extraction_method": "Direct"
    }
  ],
  "enrichment": {
    "version": "1.0.0",
    "generated_by": "generate_chapter_metadata.py",
    "contains": [
      "keywords",
      "concepts",
      "summary"
    ]
  }
}