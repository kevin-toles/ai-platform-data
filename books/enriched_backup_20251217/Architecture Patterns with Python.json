{
  "metadata": {
    "title": "Architecture Patterns with Python",
    "author": "Harry Percival, Bob Gregory",
    "publisher": "O'Reilly Media",
    "edition": "1st Edition",
    "isbn": "978-1492052203",
    "total_pages": 497,
    "conversion_date": "2025-11-08T12:42:53.064057",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Architecture-Patterns-with-Python.pdf"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Domain Modeling",
      "start_page": 46,
      "end_page": 73,
      "detection_method": "regex_chapter_title",
      "chapter_number": 1,
      "summary": "Domain Modeling\nThis chapter looks into how we can model business processes with\ncode, in a way that’s highly compatible with TDD Key topics include batches, batch, and allocating.",
      "keywords": [
        "Domain Model",
        "batch",
        "Domain",
        "line",
        "model",
        "allocate",
        "quantity",
        "SKU",
        "Domain Service",
        "def test",
        "Object",
        "order line",
        "stock",
        "business",
        "order"
      ],
      "concepts": [
        "batches",
        "batch",
        "allocating",
        "allocation",
        "allocate",
        "allocations",
        "lines",
        "modeling",
        "domain",
        "businesses"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 14,
          "title": "Segment 14 (pages 117-124)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Building Microservices",
          "chapter": 3,
          "title": "How to Model Services",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 45,
          "title": "Segment 45 (pages 456-462)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 53,
          "title": "Segment 53 (pages 484-491)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.48,
          "method": "api"
        }
      ]
    },
    {
      "number": 2,
      "title": "Repository Pattern",
      "start_page": 74,
      "end_page": 102,
      "detection_method": "regex_chapter_title",
      "chapter_number": 2,
      "summary": "Before and after the Repository pattern TIP\nThe code for this chapter is in the chapter_02_repository branch on GitHub Key topics include modeling, batches, and batch.",
      "keywords": [
        "domain model",
        "Repository Pattern",
        "Repository",
        "model",
        "ORM",
        "domain",
        "batch",
        "Pattern",
        "’ll",
        "database",
        "order",
        "Python",
        "n’t",
        "domain model classes",
        "’re"
      ],
      "concepts": [
        "modeling",
        "batches",
        "batch",
        "session",
        "sessions",
        "tested",
        "repository",
        "repositories",
        "sqlalchemy",
        "orm"
      ],
      "similar_chapters": [
        {
          "book": "Microservice APIs Using Python Flask FastAPI",
          "chapter": 21,
          "title": "Segment 21 (pages 201-209)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Microservice APIs Using Python Flask FastAPI",
          "chapter": 19,
          "title": "Segment 19 (pages 178-191)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 19,
          "title": "Segment 19 (pages 167-175)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 16,
          "title": "Segment 16 (pages 133-143)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 23,
          "title": "Segment 23 (pages 203-210)",
          "relevance_score": 0.44,
          "method": "api"
        }
      ]
    },
    {
      "number": 3,
      "title": "A Brief Interlude:",
      "start_page": 103,
      "end_page": 121,
      "detection_method": "regex_chapter_title",
      "chapter_number": 3,
      "summary": "TIP\nThe code for this chapter is in the chapter_03_abstractions branch on GitHub:\ngit clone https://github.com/cosmicpython/code.git \ngit checkout chapter_03_abstractions\nA key theme in this book, hidden among the fancy patterns, is that we\ncan use simple abstractions to hide messy details Key topics include tested, abstractions, and abstraction.",
      "keywords": [
        "dest",
        "source",
        "path",
        "hashes",
        "code",
        "file",
        "’re",
        "dst",
        "src",
        "def test",
        "filesystem",
        "actions",
        "abstraction",
        "folder",
        "copy"
      ],
      "concepts": [
        "tested",
        "abstractions",
        "abstraction",
        "abstracting",
        "paths",
        "mocks",
        "code",
        "dest",
        "file",
        "source"
      ],
      "similar_chapters": [
        {
          "book": "A Philosophy of Software Design",
          "chapter": 5,
          "title": "Segment 5 (pages 36-43)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "AntiPatterns",
          "chapter": 8,
          "title": "Segment 8 (pages 66-73)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 20,
          "title": "Segment 20 (pages 168-175)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 41,
          "title": "Segment 41 (pages 342-350)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 24,
          "title": "Segment 24 (pages 241-248)",
          "relevance_score": 0.47,
          "method": "api"
        }
      ]
    },
    {
      "number": 4,
      "title": "Our First Use",
      "start_page": 122,
      "end_page": 149,
      "detection_method": "regex_chapter_title",
      "chapter_number": 4,
      "summary": "This chapter covers our first use. Key topics include tested, layer, and batches. We’ll also discuss testing: by combining the Service Layer with our\nrepository abstraction over the database, we’re able to write fast tests,\nnot just of our domain model but of the entire workflow for a use case.",
      "keywords": [
        "Service Layer",
        "Service",
        "sku",
        "Layer",
        "api def test",
        "Flask",
        "domain service",
        "API",
        "domain",
        "Flask app",
        "random",
        "domain model",
        "def test",
        "batchref",
        "Flask API"
      ],
      "concepts": [
        "tested",
        "layer",
        "batches",
        "batch",
        "service",
        "session",
        "repository",
        "repositories",
        "models",
        "allocations"
      ],
      "similar_chapters": [
        {
          "book": "Python Microservices Development",
          "chapter": 11,
          "title": "Segment 11 (pages 82-89)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Building Microservices",
          "chapter": 7,
          "title": "Testing",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Microservice APIs Using Python Flask FastAPI",
          "chapter": 19,
          "title": "Segment 19 (pages 178-191)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Microservice APIs Using Python Flask FastAPI",
          "chapter": 21,
          "title": "Segment 21 (pages 201-209)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Microservice APIs Using Python Flask FastAPI",
          "chapter": 34,
          "title": "Segment 34 (pages 352-361)",
          "relevance_score": 0.47,
          "method": "api"
        }
      ]
    },
    {
      "number": 5,
      "title": "TDD in High Gear",
      "start_page": 150,
      "end_page": 163,
      "detection_method": "regex_chapter_title",
      "chapter_number": 5,
      "summary": "In this chapter we’ll discuss the trade-offs\ninvolved in moving those tests up to the service-layer level, and some\nmore general testing guidelines Key topics include services, batch, and batches.",
      "keywords": [
        "service layer",
        "batch",
        "service",
        "layer",
        "domain",
        "API",
        "unit tests",
        "repo",
        "def test",
        "unit",
        "TEST PYRAMID",
        "add",
        "eta",
        "sku",
        "session"
      ],
      "concepts": [
        "services",
        "batch",
        "batches",
        "session",
        "api",
        "eta",
        "changed",
        "change",
        "function",
        "functions"
      ],
      "similar_chapters": [
        {
          "book": "Building Microservices",
          "chapter": 7,
          "title": "Testing",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 9,
          "title": "Segment 9 (pages 72-79)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 10,
          "title": "[ 329 ]",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 11,
          "title": "Segment 11 (pages 82-89)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Microservice APIs Using Python Flask FastAPI",
          "chapter": 33,
          "title": "Segment 33 (pages 343-351)",
          "relevance_score": 0.42,
          "method": "api"
        }
      ]
    },
    {
      "number": 6,
      "title": "Unit of Work",
      "start_page": 164,
      "end_page": 184,
      "detection_method": "regex_chapter_title",
      "chapter_number": 6,
      "summary": "Unit of Work\nPattern\nIn this chapter we’ll introduce the final piece of the puzzle that ties\ntogether the Repository and Service Layer patterns: the Unit of Work\npattern Key topics include batches, batch, and session.",
      "keywords": [
        "UoW",
        "session",
        "Unit",
        "Unit of Work",
        "Work",
        "Service Layer",
        "Service",
        "Layer",
        "Work pattern",
        "commit",
        "sku",
        "batch",
        "factory",
        "Repository",
        "code"
      ],
      "concepts": [
        "batches",
        "batch",
        "session",
        "sessions",
        "commit",
        "committed",
        "code",
        "sqlalchemy",
        "unit",
        "allocate"
      ],
      "similar_chapters": [
        {
          "book": "Microservice APIs Using Python Flask FastAPI",
          "chapter": 21,
          "title": "Segment 21 (pages 201-209)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 38,
          "title": "Segment 38 (pages 308-316)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 34,
          "title": "Segment 34 (pages 335-344)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 44,
          "title": "Segment 44 (pages 448-455)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Microservice APIs Using Python Flask FastAPI",
          "chapter": 19,
          "title": "Segment 19 (pages 178-191)",
          "relevance_score": 0.45,
          "method": "api"
        }
      ]
    },
    {
      "number": 7,
      "title": "Aggregates and",
      "start_page": 185,
      "end_page": 222,
      "detection_method": "regex_chapter_title",
      "chapter_number": 7,
      "summary": "Aggregates and\nConsistency Boundaries\nIn this chapter, we’d like to revisit our domain model to talk about\ninvariants and constraints, and see how our domain objects can\nmaintain their own internal consistency, both conceptually and in\npersistent storage Key topics include batches, batch, and aggregates.",
      "keywords": [
        "Product",
        "SKU",
        "aggregate",
        "n’t",
        "batches",
        "Product aggregate",
        "allocate",
        "version",
        "domain",
        "model",
        "time",
        "domain model",
        "update",
        "version numbers",
        "number"
      ],
      "concepts": [
        "batches",
        "batch",
        "aggregates",
        "concurrency",
        "concurrent",
        "allocate",
        "allocated",
        "allocation",
        "allocations",
        "product"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 40,
          "title": "Segment 40 (pages 392-405)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 546-554)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 23,
          "title": "Segment 23 (pages 216-227)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 47,
          "title": "Segment 47 (pages 474-481)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "Building Microservices",
          "chapter": 3,
          "title": "How to Model Services",
          "relevance_score": 0.43,
          "method": "api"
        }
      ]
    },
    {
      "number": 8,
      "title": "Events and the",
      "start_page": 223,
      "end_page": 246,
      "detection_method": "regex_chapter_title",
      "chapter_number": 8,
      "summary": "This chapter covers events and the. Key topics include events, product, and production. In practice, though, we find that it’s not the obvious features that make\na mess of our codebases: it’s the goop around the edge.",
      "keywords": [
        "Message Bus",
        "Events",
        "Service Layer",
        "Message",
        "Bus",
        "Domain Events",
        "product",
        "Domain",
        "domain model",
        "Service",
        "sku",
        "Model Raises Events",
        "Model",
        "layer",
        "Raises Events"
      ],
      "concepts": [
        "events",
        "product",
        "production",
        "allocate",
        "allocation",
        "allocated",
        "allocations",
        "model",
        "classes",
        "services"
      ],
      "similar_chapters": [
        {
          "book": "Microservices Up and Running",
          "chapter": 9,
          "title": "Segment 9 (pages 86-94)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 46,
          "title": "Segment 46 (pages 459-473)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 39,
          "title": "Segment 39 (pages 317-324)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 10,
          "title": "Segment 10 (pages 95-107)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 11,
          "title": "Segment 11 (pages 108-118)",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    },
    {
      "number": 9,
      "title": "Going to Town on",
      "start_page": 247,
      "end_page": 275,
      "detection_method": "regex_chapter_title",
      "chapter_number": 9,
      "summary": "Going to Town on\nthe Message Bus\nIn this chapter, we’ll start to make events more fundamental to the\ninternal structure of our application Key topics include event, handler, and batch.",
      "keywords": [
        "Message Bus",
        "event",
        "Message",
        "UoW",
        "Handler",
        "Bus",
        "event handlers",
        "batch",
        "quantity",
        "unit",
        "sku",
        "change",
        "API",
        "batchref",
        "service"
      ],
      "concepts": [
        "event",
        "handler",
        "batch",
        "classes",
        "allocated",
        "allocation",
        "allocate",
        "allocations",
        "chapters",
        "services"
      ],
      "similar_chapters": [
        {
          "book": "Microservices Up and Running",
          "chapter": 9,
          "title": "Segment 9 (pages 86-94)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 46,
          "title": "Segment 46 (pages 459-473)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 39,
          "title": "Segment 39 (pages 317-324)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 8,
          "title": "Segment 8 (pages 76-85)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 11,
          "title": "Segment 11 (pages 108-118)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 10,
      "title": "Commands and",
      "start_page": 276,
      "end_page": 290,
      "detection_method": "regex_chapter_title",
      "chapter_number": 10,
      "summary": "This chapter covers commands and. Key topics include commands, events, and handler. To achieve that, we converted all our use-case functions to event\nhandlers.",
      "keywords": [
        "event",
        "Commands",
        "message",
        "Handler",
        "uow",
        "handling event",
        "message bus",
        "handle",
        "Exception handling event",
        "order",
        "customer",
        "system",
        "Command Handler",
        "Exception Handling",
        "Exception"
      ],
      "concepts": [
        "commands",
        "events",
        "handler",
        "message",
        "handles",
        "customer",
        "different",
        "differences",
        "orders",
        "error"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 37,
          "title": "Segment 37 (pages 741-758)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 11,
          "title": "Segment 11 (pages 88-95)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 23,
          "title": "Segment 23 (pages 457-476)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 53,
          "title": "Segment 53 (pages 1064-1083)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 27,
          "title": "Segment 27 (pages 222-231)",
          "relevance_score": 0.52,
          "method": "api"
        }
      ]
    },
    {
      "number": 11,
      "title": "Event-Driven",
      "start_page": 291,
      "end_page": 309,
      "detection_method": "regex_chapter_title",
      "chapter_number": 11,
      "summary": "In this chapter, we’d like to show how the events metaphor can be\nextended to encompass the way that we handle incoming and outgoing\nmessages from the system Key topics include event, messages, and messaging.",
      "keywords": [
        "Redis",
        "event",
        "system",
        "message",
        "order",
        "batch",
        "allocated",
        "Channel",
        "CONNASCENCE",
        "Redis pub",
        "API",
        "warehouse system",
        "Redis message",
        "Simple Redis message",
        "change"
      ],
      "concepts": [
        "event",
        "messages",
        "messaging",
        "allocated",
        "allocation",
        "allocate",
        "services",
        "publish",
        "publishers",
        "publishes"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 46,
          "title": "Segment 46 (pages 459-473)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 11,
          "title": "Segment 11 (pages 108-118)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 9,
          "title": "Segment 9 (pages 86-94)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 39,
          "title": "Segment 39 (pages 317-324)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Microservices Up and Running",
          "chapter": 8,
          "title": "Segment 8 (pages 76-85)",
          "relevance_score": 0.47,
          "method": "api"
        }
      ]
    },
    {
      "number": 12,
      "title": "Command-Query",
      "start_page": 310,
      "end_page": 334,
      "detection_method": "regex_chapter_title",
      "chapter_number": 12,
      "summary": "Command-Query\nResponsibility Segregation\n(CQRS)\nIn this chapter, we’re going to start with a fairly uncontroversial\ninsight: reads (queries) and writes (commands) are different, so they\nshould be treated differently (or have their responsibilities segregated,\nif you will) Key topics include allocate, allocated, and allocation.",
      "keywords": [
        "read model",
        "model",
        "READ",
        "domain model",
        "allocation",
        "orderid",
        "sku",
        "uow",
        "view",
        "Domain",
        "Redis read model",
        "view model",
        "’re",
        "data",
        "batch"
      ],
      "concepts": [
        "allocate",
        "allocated",
        "allocation",
        "models",
        "queries",
        "query",
        "reads",
        "batch",
        "batches",
        "views"
      ],
      "similar_chapters": [
        {
          "book": "Microservice Architecture",
          "chapter": 12,
          "title": "Segment 12 (pages 90-97)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 23,
          "title": "Segment 23 (pages 454-475)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 39,
          "title": "Segment 39 (pages 357-366)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 27,
          "title": "Segment 27 (pages 536-554)",
          "relevance_score": 0.43,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 29,
          "title": "Segment 29 (pages 272-282)",
          "relevance_score": 0.43,
          "method": "api"
        }
      ]
    },
    {
      "number": 13,
      "title": "Dependency",
      "start_page": 335,
      "end_page": 497,
      "detection_method": "regex_chapter_title",
      "chapter_number": 13,
      "summary": "In this chapter, we’ll explore some of the pain points in our code that\nlead us to consider using DI, and we’ll present some options for how\nto do it, leaving it to you to pick which you think is most Pythonic Key topics include tested, event, and messaging.",
      "keywords": [
        "message bus",
        "service layer",
        "unit testing event",
        "testing event handlers",
        "fake message bus",
        "domain model",
        "Domain Layer Tests",
        "message",
        "unit testing domain",
        "events",
        "service",
        "unit",
        "Typical Service Function",
        "Unit of work",
        "unit test"
      ],
      "concepts": [
        "tested",
        "event",
        "messaging",
        "messages",
        "model",
        "handlers",
        "allocate",
        "allocation",
        "allocations",
        "allocating"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 51,
          "title": "Segment 51 (pages 488-496)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 8,
          "title": "Modules, Packages, and Distribution",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 14,
          "title": "Segment 14 (pages 125-136)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 5,
          "title": "Segment 5 (pages 33-40)",
          "relevance_score": 0.45,
          "method": "api"
        }
      ]
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "O'REILLY”\n\nArchitecture\nPatterns\nwith Python\n\nEnabling Test-Driven Development,\nDomain-Driven Design, and Event-Driven\nMicroservices\n\nHarry J.W. Percival\n& Bob Gregory",
      "content_length": 167,
      "extraction_method": "OCR"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "1. Preface\na. Managing Complexity, Solving Business\nProblems\nb. Why Python?\nc. TDD, DDD, and Event-Driven Architecture\nd. Who Should Read This Book\ne. A Brief Overview of What You’ll Learn\ni. Part I, Building an Architecture to\nSupport Domain Modeling\nii. Part II, Event-Driven Architecture\niii. Addtional Content\nf. Example Code and Coding Along\ng. License\nh. Conventions Used in This Book\ni. O’Reilly Online Learning\nj. How to Contact O’Reilly\nk. Acknowledgments\n2. Introduction\na. Why Do Our Designs Go Wrong?\nb. Encapsulation and Abstractions\nc. Layering\nd. The Dependency Inversion Principle",
      "content_length": 596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "e. A Place for All Our Business Logic: The\nDomain Model\n3. I. Building an Architecture to Support Domain\nModeling\n4. 1. Domain Modeling\na. What Is a Domain Model?\nb. Exploring the Domain Language\nc. Unit Testing Domain Models\ni. Dataclasses Are Great for Value\nObjects\nii. Value Objects and Entities\nd. Not Everything Has to Be an Object: A Domain\nService Function\ni. Python’s Magic Methods Let Us Use\nOur Models with Idiomatic Python\nii. Exceptions Can Express Domain\nConcepts Too\n5. 2. Repository Pattern\na. Persisting Our Domain Model\nb. Some Pseudocode: What Are We Going to\nNeed?\nc. Applying the DIP to Data Access\nd. Reminder: Our Model\ni. The “Normal” ORM Way: Model\nDepends on ORM",
      "content_length": 688,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "ii. Inverting the Dependency: ORM\nDepends on Model\ne. Introducing the Repository Pattern\ni. The Repository in the Abstract\nii. What Is the Trade-Off?\nf. Building a Fake Repository for Tests Is Now\nTrivial!\ng. What Is a Port and What Is an Adapter, in\nPython?\nh. Wrap-Up\n6. 3. A Brief Interlude: On Coupling and Abstractions\na. Abstracting State Aids Testability\nb. Choosing the Right Abstraction(s)\nc. Implementing Our Chosen Abstractions\ni. Testing Edge to Edge with Fakes and\nDependency Injection\nii. Why Not Just Patch It Out?\nd. Wrap-Up\n7. 4. Our First Use Case: Flask API and Service Layer\na. Connecting Our Application to the Real World\nb. A First End-to-End Test\nc. The Straightforward Implementation\nd. Error Conditions That Require Database\nChecks",
      "content_length": 756,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "e. Introducing a Service Layer, and Using\nFakeRepository to Unit Test It\ni. A Typical Service Function\nf. Why Is Everything Called a Service?\ng. Putting Things in Folders to See Where It All\nBelongs\nh. Wrap-Up\ni. The DIP in Action\n8. 5. TDD in High Gear and Low Gear\na. How Is Our Test Pyramid Looking?\nb. Should Domain Layer Tests Move to the\nService Layer?\nc. On Deciding What Kind of Tests to Write\nd. High and Low Gear\ne. Fully Decoupling the Service-Layer Tests from\nthe Domain\ni. Mitigation: Keep All Domain\nDependencies in Fixture Functions\nii. Adding a Missing Service\nf. Carrying the Improvement Through to the E2E\nTests\ng. Wrap-Up\n9. 6. Unit of Work Pattern\na. The Unit of Work Collaborates with the\nRepository",
      "content_length": 720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "b. Test-Driving a UoW with Integration Tests\nc. Unit of Work and Its Context Manager\ni. The Real Unit of Work Uses\nSQLAlchemy Sessions\nii. Fake Unit of Work for Testing\nd. Using the UoW in the Service Layer\ne. Explicit Tests for Commit/Rollback Behavior\nf. Explicit Versus Implicit Commits\ng. Examples: Using UoW to Group Multiple\nOperations into an Atomic Unit\ni. Example 1: Reallocate\nii. Example 2: Change Batch Quantity\nh. Tidying Up the Integration Tests\ni. Wrap-Up\n10. 7. Aggregates and Consistency Boundaries\na. Why Not Just Run Everything in a\nSpreadsheet?\nb. Invariants, Constraints, and Consistency\ni. Invariants, Concurrency, and Locks\nc. What Is an Aggregate?\nd. Choosing an Aggregate\ne. One Aggregate = One Repository\nf. What About Performance?",
      "content_length": 757,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "g. Optimistic Concurrency with Version Numbers\ni. Implementation Options for Version\nNumbers\nh. Testing for Our Data Integrity Rules\ni. Enforcing Concurrency Rules by Using\nDatabase Transaction Isolation Levels\nii. Pessimistic Concurrency Control\nExample: SELECT FOR UPDATE\ni. Wrap-Up\nj. Part I Recap\n11. II. Event-Driven Architecture\n12. 8. Events and the Message Bus\na. Avoiding Making a Mess\ni. First, Let’s Avoid Making a Mess of\nOur Web Controllers\nii. And Let’s Not Make a Mess of Our\nModel Either\niii. Or the Service Layer!\nb. Single Responsibility Principle\nc. All Aboard the Message Bus!\ni. The Model Records Events\nii. Events Are Simple Dataclasses\niii. The Model Raises Events",
      "content_length": 687,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "iv. The Message Bus Maps Events to\nHandlers\nd. Option 1: The Service Layer Takes Events from\nthe Model and Puts Them on the Message Bus\ne. Option 2: The Service Layer Raises Its Own\nEvents\nf. Option 3: The UoW Publishes Events to the\nMessage Bus\ng. Wrap-Up\n13. 9. Going to Town on the Message Bus\na. A New Requirement Leads Us to a New\nArchitecture\ni. Imagining an Architecture Change:\nEverything Will Be an Event Handler\nb. Refactoring Service Functions to Message\nHandlers\ni. The Message Bus Now Collects Events\nfrom the UoW\nii. Our Tests Are All Written in Terms of\nEvents Too\niii. A Temporary Ugly Hack: The Message\nBus Has to Return Results\niv. Modifying Our API to Work with\nEvents\nc. Implementing Our New Requirement\ni. Our New Event",
      "content_length": 740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "d. Test-Driving a New Handler\ni. Implementation\nii. A New Method on the Domain Model\ne. Optionally: Unit Testing Event Handlers in\nIsolation with a Fake Message Bus\nf. Wrap-Up\ni. What Have We Achieved?\nii. Why Have We Achieved?\n14. 10. Commands and Command Handler\na. Commands and Events\nb. Differences in Exception Handling\nc. Discussion: Events, Commands, and Error\nHandling\nd. Recovering from Errors Synchronously\ne. Wrap-Up\n15. 11. Event-Driven Architecture: Using Events to\nIntegrate Microservices\na. Distributed Ball of Mud, and Thinking in\nNouns\nb. Error Handling in Distributed Systems\nc. The Alternative: Temporal Decoupling Using\nAsynchronous Messaging\nd. Using a Redis Pub/Sub Channel for\nIntegration",
      "content_length": 711,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "e. Test-Driving It All Using an End-to-End Test\ni. Redis Is Another Thin Adapter Around\nOur Message Bus\nii. Our New Outgoing Event\nf. Internal Versus External Events\ng. Wrap-Up\n16. 12. Command-Query Responsibility Segregation\n(CQRS)\na. Domain Models Are for Writing\nb. Most Users Aren’t Going to Buy Your\nFurniture\nc. Post/Redirect/Get and CQS\nd. Hold On to Your Lunch, Folks\ne. Testing CQRS Views\nf. “Obvious” Alternative 1: Using the Existing\nRepository\ng. Your Domain Model Is Not Optimized for\nRead Operations\nh. “Obvious” Alternative 2: Using the ORM\ni. SELECT N+1 and Other Performance\nConsiderations\nj. Time to Completely Jump the Shark\ni. Updating a Read Model Table Using an\nEvent Handler",
      "content_length": 697,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "k. Changing Our Read Model Implementation Is\nEasy\nl. Wrap-Up\n17. 13. Dependency Injection (and Bootstrapping)\na. Implicit Versus Explicit Dependencies\nb. Aren’t Explicit Dependencies Totally Weird\nand Java-y?\nc. Preparing Handlers: Manual DI with Closures\nand Partials\nd. An Alternative Using Classes\ne. A Bootstrap Script\nf. Message Bus Is Given Handlers at Runtime\ng. Using Bootstrap in Our Entrypoints\nh. Initializing DI in Our Tests\ni. Building an Adapter “Properly”: A Worked\nExample\ni. Define the Abstract and Concrete\nImplementations\nii. Make a Fake Version for Your Tests\niii. Figure Out How to Integration Test the\nReal Thing\nj. Wrap-Up\n18. Epilogue\na. What Now?",
      "content_length": 671,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "b. How Do I Get There from Here?\nc. Separating Entangled Responsibilities\nd. Identifying Aggregates and Bounded Contexts\ne. An Event-Driven Approach to Go to\nMicroservices via Strangler Pattern\nf. Convincing Your Stakeholders to Try\nSomething New\ng. Questions Our Tech Reviewers Asked That We\nCouldn’t Work into Prose\nh. Footguns\ni. More Required Reading\nj. Wrap-Up\n19. A. Summary Diagram and Table\n20. B. A Template Project Structure\na. Env Vars, 12-Factor, and Config, Inside and\nOutside Containers\nb. Config.py\nc. Docker-Compose and Containers Config\nd. Installing Your Source as a Package\ne. Dockerfile\nf. Tests\ng. Wrap-Up\n21. C. Swapping Out the Infrastructure: Do Everything\nwith CSVs",
      "content_length": 690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "a. Implementing a Repository and Unit of Work\nfor CSVs\n22. D. Repository and Unit of Work Patterns with Django\na. Repository Pattern with Django\ni. Custom Methods on Django ORM\nClasses to Translate to/from Our\nDomain Model\nb. Unit of Work Pattern with Django\nc. API: Django Views Are Adapters\nd. Why Was This All So Hard?\ne. What to Do If You Already Have Django\nf. Steps Along the Way\n23. E. Validation\na. What Is Validation, Anyway?\nb. Validating Syntax\nc. Postel’s Law and the Tolerant Reader Pattern\nd. Validating at the Edge\ne. Validating Semantics\nf. Validating Pragmatics\n24. Index",
      "content_length": 588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "Architecture Patterns with\nPython\nEnabling Test-Driven Development, Domain-\nDriven Design, and Event-Driven Microservices\nHarry Percival and Bob Gregory",
      "content_length": 152,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "Architecture Patterns with Python\nby Harry Percival and Bob Gregory\nCopyright © 2020 Harry Percival and Bob Gregory. All rights\nreserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North,\nSebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales\npromotional use. Online editions are also available for most titles\n(http://oreilly.com). For more information, contact our\ncorporate/institutional sales department: 800-998-9938 or\ncorporate@oreilly.com.\nAcquisitions Editor: Ryan Shaw\nDevelopment Editor: Corbin Collins\nProduction Editor: Katherine Tozer\nCopyeditor: Sharon Wilkey\nProofreader: Arthur Johnson\nIndexer: Ellen Troutman-Zaig\nInterior Designer: David Futato",
      "content_length": 756,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "Cover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nMarch 2020: First Edition\nRevision History for the First Edition\n2020-03-05: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492052203 for\nrelease details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc.\nArchitecture Patterns with Python, the cover image, and related trade\ndress are trademarks of O’Reilly Media, Inc.\nThe views expressed in this work are those of the authors and do not\nrepresent the publisher’s views. While the publisher and the authors\nhave used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the\nauthors disclaim all responsibility for errors or omissions, including\nwithout limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions\ncontained in this work is at your own risk. If any code samples or other\ntechnology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your\nresponsibility to ensure that your use thereof complies with such\nlicenses and/or rights.\n978-1-492-05220-3",
      "content_length": 1218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "(st)",
      "content_length": 4,
      "extraction_method": "OCR"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "Preface\nYou may be wondering who we are and why we wrote this book.\nAt the end of Harry’s last book, Test-Driven Development with\nPython (O’Reilly), he found himself asking a bunch of questions about\narchitecture, such as, What’s the best way of structuring your\napplication so that it’s easy to test? More specifically, so that your\ncore business logic is covered by unit tests, and so that you minimize\nthe number of integration and end-to-end tests you need? He made\nvague references to “Hexagonal Architecture” and “Ports and\nAdapters” and “Functional Core, Imperative Shell,” but if he was\nhonest, he’d have to admit that these weren’t things he really\nunderstood or had done in practice.\nAnd then he was lucky enough to run into Bob, who has the answers to\nall these questions.\nBob ended up a software architect because nobody else on his team\nwas doing it. He turned out to be pretty bad at it, but he was lucky\nenough to run into Ian Cooper, who taught him new ways of writing\nand thinking about code.\nManaging Complexity, Solving Business\nProblems",
      "content_length": 1056,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "We both work for MADE.com, a European ecommerce company that\nsells furniture online; there, we apply the techniques in this book to\nbuild distributed systems that model real-world business problems.\nOur example domain is the first system Bob built for MADE, and this\nbook is an attempt to write down all the stuff we have to teach new\nprogrammers when they join one of our teams.\nMADE.com operates a global supply chain of freight partners and\nmanufacturers. To keep costs low, we try to optimize the delivery of\nstock to our warehouses so that we don’t have unsold goods lying\naround the place.\nIdeally, the sofa that you want to buy will arrive in port on the very day\nthat you decide to buy it, and we’ll ship it straight to your house\nwithout ever storing it. Getting the timing right is a tricky balancing act\nwhen goods take three months to arrive by container ship. Along the\nway, things get broken or water damaged, storms cause unexpected\ndelays, logistics partners mishandle goods, paperwork goes missing,\ncustomers change their minds and amend their orders, and so on.\nWe solve those problems by building intelligent software representing\nthe kinds of operations taking place in the real world so that we can\nautomate as much of the business as possible.\nWhy Python?\nIf you’re reading this book, we probably don’t need to convince you\nthat Python is great, so the real question is “Why does the Python\ncommunity need a book like this?” The answer is about Python’s",
      "content_length": 1475,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "popularity and maturity: although Python is probably the world’s\nfastest-growing programming language and is nearing the top of the\nabsolute popularity tables, it’s only just starting to take on the kinds of\nproblems that the C# and Java world has been working on for years.\nStartups become real businesses; web apps and scripted automations\nare becoming (whisper it) enterprise software.\nIn the Python world, we often quote the Zen of Python: “There should\nbe one—and preferably only one—obvious way to do it.”\nUnfortunately, as project size grows, the most obvious way of doing\nthings isn’t always the way that helps you manage complexity and\nevolving requirements.\nNone of the techniques and patterns we discuss in this book are new,\nbut they are mostly new to the Python world. And this book isn’t a\nreplacement for the classics in the field such as Eric Evans’s Domain-\nDriven Design or Martin Fowler’s Patterns of Enterprise\nApplication Architecture (both published by Addison-Wesley\nProfessional)—which we often refer to and encourage you to go and\nread.\nBut all the classic code examples in the literature do tend to be written\nin Java or C++/#, and if you’re a Python person and haven’t used\neither of those languages in a long time (or indeed ever), those code\nlistings can be quite…trying. There’s a reason the latest edition of that\nother classic text, Fowler’s Refactoring (Addison-Wesley\nProfessional), is in JavaScript.\n1",
      "content_length": 1436,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "TDD, DDD, and Event-Driven Architecture\nIn order of notoriety, we know of three tools for managing complexity:\n1. Test-driven development (TDD) helps us to build code that is\ncorrect and enables us to refactor or add new features,\nwithout fear of regression. But it can be hard to get the best\nout of our tests: How do we make sure that they run as fast as\npossible? That we get as much coverage and feedback from\nfast, dependency-free unit tests and have the minimum number\nof slower, flaky end-to-end tests?\n2. Domain-driven design (DDD) asks us to focus our efforts on\nbuilding a good model of the business domain, but how do we\nmake sure that our models aren’t encumbered with\ninfrastructure concerns and don’t become hard to change?\n3. Loosely coupled (micro)services integrated via messages\n(sometimes called reactive microservices) are a well-\nestablished answer to managing complexity across multiple\napplications or business domains. But it’s not always obvious\nhow to make them fit with the established tools of the Python\nworld—Flask, Django, Celery, and so on.\nNOTE\nDon’t be put off if you’re not working with (or interested in) microservices. The\nvast majority of the patterns we discuss, including much of the event-driven\narchitecture material, is absolutely applicable in a monolithic architecture.\nOur aim with this book is to introduce several classic architectural\npatterns and show how they support TDD, DDD, and event-driven\nservices. We hope it will serve as a reference for implementing them",
      "content_length": 1514,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "in a Pythonic way, and that people can use it as a first step toward\nfurther research in this field.\nWho Should Read This Book\nHere are a few things we assume about you, dear reader:\nYou’ve been close to some reasonably complex Python\napplications.\nYou’ve seen some of the pain that comes with trying to\nmanage that complexity.\nYou don’t necessarily know anything about DDD or any of the\nclassic application architecture patterns.\nWe structure our explorations of architectural patterns around an\nexample app, building it up chapter by chapter. We use TDD at work,\nso we tend to show listings of tests first, followed by implementation.\nIf you’re not used to working test-first, it may feel a little strange at the\nbeginning, but we hope you’ll soon get used to seeing code “being\nused” (i.e., from the outside) before you see how it’s built on the\ninside.\nWe use some specific Python frameworks and technologies, including\nFlask, SQLAlchemy, and pytest, as well as Docker and Redis. If\nyou’re already familiar with them, that won’t hurt, but we don’t think\nit’s required. One of our main aims with this book is to build an\narchitecture for which specific technology choices become minor\nimplementation details.",
      "content_length": 1211,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "A Brief Overview of What You’ll Learn\nThe book is divided into two parts; here’s a look at the topics we’ll\ncover and the chapters they live in.",
      "content_length": 144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "Part I, Building an Architecture to Support Domain\nModeling\nDomain modeling and DDD (Chapters 1 and 7)\nAt some level, everyone has learned the lesson that complex\nbusiness problems need to be reflected in code, in the form of a\nmodel of the domain. But why does it always seem to be so hard to\ndo without getting tangled up with infrastructure concerns, our web\nframeworks, or whatever else? In the first chapter we give a broad\noverview of domain modeling and DDD, and we show how to get\nstarted with a model that has no external dependencies, and fast\nunit tests. Later we return to DDD patterns to discuss how to\nchoose the right aggregate, and how this choice relates to questions\nof data integrity.\nRepository, Service Layer, and Unit of Work patterns (Chapters 2, 4,\nand 5)\nIn these three chapters we present three closely related and\nmutually reinforcing patterns that support our ambition to keep the\nmodel free of extraneous dependencies. We build a layer of\nabstraction around persistent storage, and we build a service layer\nto define the entrypoints to our system and capture the primary use\ncases. We show how this layer makes it easy to build thin\nentrypoints to our system, whether it’s a Flask API or a CLI.\nSome thoughts on testing and abstractions (Chapters 3 and 6)\nAfter presenting the first abstraction (the Repository pattern), we\ntake the opportunity for a general discussion of how to choose\nabstractions, and what their role is in choosing how our software\nis coupled together. After we introduce the Service Layer pattern,\nwe talk a bit about achieving a test pyramid and writing unit tests\nat the highest possible level of abstraction.",
      "content_length": 1662,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "Part II, Event-Driven Architecture\nEvent-driven architecture (Chapters 8–11)\nWe introduce three more mutually reinforcing patterns: the Domain\nEvents, Message Bus, and Handler patterns. Domain events are a\nvehicle for capturing the idea that some interactions with a system\nare triggers for others. We use a message bus to allow actions to\ntrigger events and call appropriate handlers. We move on to\ndiscuss how events can be used as a pattern for integration\nbetween services in a microservices architecture. Finally, we\ndistinguish between commands and events. Our application is now\nfundamentally a message-processing system.\nCommand-query responsibility segregation (Chapter 12)\nWe present an example of command-query responsibility\nsegregation, with and without events.\nDependency injection (Chapter 13)\nWe tidy up our explicit and implicit dependencies and implement a\nsimple dependency injection framework.\nAddtional Content\nHow do I get there from here? (Epilogue)\nImplementing architectural patterns always looks easy when you\nshow a simple example, starting from scratch, but many of you will\nprobably be wondering how to apply these principles to existing\nsoftware. We’ll provide a few pointers in the epilogue and some\nlinks to further reading.\nExample Code and Coding Along",
      "content_length": 1286,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "You’re reading a book, but you’ll probably agree with us when we say\nthat the best way to learn about code is to code. We learned most of\nwhat we know from pairing with people, writing code with them, and\nlearning by doing, and we’d like to re-create that experience as much\nas possible for you in this book.\nAs a result, we’ve structured the book around a single example project\n(although we do sometimes throw in other examples). We’ll build up\nthis project as the chapters progress, as if you’ve paired with us and\nwe’re explaining what we’re doing and why at each step.\nBut to really get to grips with these patterns, you need to mess about\nwith the code and get a feel for how it works. You’ll find all the code\non GitHub; each chapter has its own branch. You can find a list of the\nbranches on GitHub as well.\nHere are three ways you might code along with the book:\nStart your own repo and try to build up the app as we do,\nfollowing the examples from listings in the book, and\noccasionally looking to our repo for hints. A word of\nwarning, however: if you’ve read Harry’s previous book and\ncoded along with that, you’ll find that this book requires you\nto figure out more on your own; you may need to lean pretty\nheavily on the working versions on GitHub.\nTry to apply each pattern, chapter by chapter, to your own\n(preferably small/toy) project, and see if you can make it\nwork for your use case. This is high risk/high reward (and\nhigh effort besides!). It may take quite some work to get things\nworking for the specifics of your project, but on the other\nhand, you’re likely to learn the most.",
      "content_length": 1603,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "For less effort, in each chapter we outline an “Exercise for\nthe Reader,” and point you to a GitHub location where you\ncan download some partially finished code for the chapter\nwith a few missing parts to write yourself.\nParticularly if you’re intending to apply some of these patterns in your\nown projects, working through a simple example is a great way to\nsafely practice.\nTIP\nAt the very least, do a git checkout of the code from our repo as you read each\nchapter. Being able to jump in and see the code in the context of an actual\nworking app will help answer a lot of questions as you go, and makes everything\nmore real. You’ll find instructions for how to do that at the beginning of each\nchapter.\nLicense\nThe code (and the online version of the book) is licensed under a\nCreative Commons CC BY-NC-ND license, which means you are free\nto copy and share it with anyone you like, for non-commercial\npurposes, as long as you give attribution. If you want to re-use any of\nthe content from this book and you have any worries about the license,\ncontact O’Reilly at permissions@oreilly.com.\nThe print edition is licensed differently; please see the copyright page.\nConventions Used in This Book",
      "content_length": 1195,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "The following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file\nextensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to\nprogram elements such as variable or function names, databases,\ndata types, environment variables, statements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the\nuser.\nConstant width italic\nShows text that should be replaced with user-supplied values or by\nvalues determined by context.\nTIP\nThis element signifies a tip or suggestion.\nNOTE\nThis element signifies a general note.",
      "content_length": 657,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "WARNING\nThis element indicates a warning or caution.\nO’Reilly Online Learning\nNOTE\nFor more than 40 years, O’Reilly Media has provided technology and business\ntraining, knowledge, and insight to help companies succeed.\nOur unique network of experts and innovators share their knowledge\nand expertise through books, articles, conferences, and our online\nlearning platform. O’Reilly’s online learning platform gives you on-\ndemand access to live training courses, in-depth learning paths,\ninteractive coding environments, and a vast collection of text and\nvideo from O’Reilly and 200+ other publishers. For more information,\nplease visit http://oreilly.com.\nHow to Contact O’Reilly\nPlease address comments and questions concerning this book to the\npublisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North",
      "content_length": 808,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "Sebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and\nany additional information. You can access this page at\nhttps://oreil.ly/architecture-patterns-python.\nEmail bookquestions@oreilly.com to comment or ask technical\nquestions about this book.\nFor more information about our books, courses, conferences, and\nnews, see our website at http://www.oreilly.com.\nFind us on Facebook: http://facebook.com/oreilly\nFollow us on Twitter: http://twitter.com/oreillymedia\nWatch us on YouTube: http://www.youtube.com/oreillymedia\nAcknowledgments\nTo our tech reviewers, David Seddon, Ed Jung, and Hynek Schlawack:\nwe absolutely do not deserve you. You are all incredibly dedicated,\nconscientious, and rigorous. Each one of you is immensely smart, and\nyour different points of view were both useful and complementary to\neach other. Thank you from the bottom of our hearts.",
      "content_length": 997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "Gigantic thanks also to our Early Release readers for their comments\nand suggestions: Ian Cooper, Abdullah Ariff, Jonathan Meier, Gil\nGonçalves, Matthieu Choplin, Ben Judson, James Gregory, Łukasz\nLechowicz, Clinton Roy, Vitorino Araújo, Susan Goodbody, Josh\nHarwood, Daniel Butler, Liu Haibin, Jimmy Davies, Ignacio Vergara\nKausel, Gaia Canestrani, Renne Rocha, pedroabi, Ashia Zawaduk,\nJostein Leira, Brandon Rhodes, and many more; our apologies if we\nmissed you on this list.\nSuper-mega-thanks to our editor Corbin Collins for his gentle\nchivvying, and for being a tireless advocate of the reader. Similarly-\nsuperlative thanks to the production staff, Katherine Tozer, Sharon\nWilkey, Ellen Troutman-Zaig, and Rebecca Demarest, for your\ndedication, professionalism, and attention to detail. This book is\nimmeasurably improved thanks to you.\nAny errors remaining in the book are our own, naturally.\n1  python -c \"import this\"",
      "content_length": 927,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "Introduction\nWhy Do Our Designs Go Wrong?\nWhat comes to mind when you hear the word chaos? Perhaps you think\nof a noisy stock exchange, or your kitchen in the morning—everything\nconfused and jumbled. When you think of the word order, perhaps you\nthink of an empty room, serene and calm. For scientists, though, chaos\nis characterized by homogeneity (sameness), and order by complexity\n(difference).\nFor example, a well-tended garden is a highly ordered system.\nGardeners define boundaries with paths and fences, and they mark out\nflower beds or vegetable patches. Over time, the garden evolves,\ngrowing richer and thicker; but without deliberate effort, the garden\nwill run wild. Weeds and grasses will choke out other plants, covering\nover the paths, until eventually every part looks the same again—wild\nand unmanaged.\nSoftware systems, too, tend toward chaos. When we first start building\na new system, we have grand ideas that our code will be clean and\nwell ordered, but over time we find that it gathers cruft and edge cases\nand ends up a confusing morass of manager classes and util modules.\nWe find that our sensibly layered architecture has collapsed into itself\nlike an oversoggy trifle. Chaotic software systems are characterized by\na sameness of function: API handlers that have domain knowledge and\nsend email and perform logging; “business logic” classes that perform",
      "content_length": 1381,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "no calculations but do perform I/O; and everything coupled to\neverything else so that changing any part of the system becomes fraught\nwith danger. This is so common that software engineers have their own\nterm for chaos: the Big Ball of Mud anti-pattern (Figure P-1).\nFigure P-1. A real-life dependency diagram (source: “Enterprise Dependency: Big Ball\nof Yarn” by Alex Papadimoulis)",
      "content_length": 382,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "TIP\nA big ball of mud is the natural state of software in the same way that wilderness\nis the natural state of your garden. It takes energy and direction to prevent the\ncollapse.\nFortunately, the techniques to avoid creating a big ball of mud aren’t\ncomplex.\nEncapsulation and Abstractions\nEncapsulation and abstraction are tools that we all instinctively reach\nfor as programmers, even if we don’t all use these exact words. Allow\nus to dwell on them for a moment, since they are a recurring\nbackground theme of the book.\nThe term encapsulation covers two closely related ideas: simplifying\nbehavior and hiding data. In this discussion, we’re using the first\nsense. We encapsulate behavior by identifying a task that needs to be\ndone in our code and giving that task to a well-defined object or\nfunction. We call that object or function an abstraction.\nTake a look at the following two snippets of Python code:\nDo a search with urllib\nimport json\nfrom urllib.request import urlopen\nfrom urllib.parse import urlencode \n \nparams = dict(q='Sausages', format='json')",
      "content_length": 1063,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "handle = urlopen('http://api.duckduckgo.com' + '?' + urlencode(params))\nraw_text = handle.read().decode('utf8')\nparsed = json.loads(raw_text) \n \nresults = parsed['RelatedTopics']\nfor r in results: \n    if 'Text' in r: \n        print(r['FirstURL'] + ' - ' + r['Text'])\nDo a search with requests\nimport requests \n \nparams = dict(q='Sausages', format='json')\nparsed = requests.get('http://api.duckduckgo.com/', params=params).json() \n \nresults = parsed['RelatedTopics']\nfor r in results: \n    if 'Text' in r: \n        print(r['FirstURL'] + ' - ' + r['Text'])\nBoth code listings do the same thing: they submit form-encoded values\nto a URL in order to use a search engine API. But the second is\nsimpler to read and understand because it operates at a higher level of\nabstraction.\nWe can take this one step further still by identifying and naming the\ntask we want the code to perform for us and using an even higher-level\nabstraction to make it explicit:\nDo a search with the duckduckgo module\nimport duckduckgo\nfor r in duckduckgo.query('Sausages').results: \n    print(r.url + ' - ' + r.text)",
      "content_length": 1087,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "Encapsulating behavior by using abstractions is a powerful tool for\nmaking code more expressive, more testable, and easier to maintain.\nNOTE\nIn the literature of the object-oriented (OO) world, one of the classic\ncharacterizations of this approach is called responsibility-driven design; it uses\nthe words roles and responsibilities rather than tasks. The main point is to think\nabout code in terms of behavior, rather than in terms of data or algorithms.\nABSTRACTIONS AND ABCS\nIn a traditional OO language like Java or C#, you might use an abstract base class (ABC) or an\ninterface to define an abstraction. In Python you can (and we sometimes do) use ABCs, but you\ncan also happily rely on duck typing.\nThe abstraction can just mean “the public API of the thing you’re using”—a function name plus\nsome arguments, for example.\nMost of the patterns in this book involve choosing an abstraction, so\nyou’ll see plenty of examples in each chapter. In addition, Chapter 3\nspecifically discusses some general heuristics for choosing\nabstractions.\nLayering\nEncapsulation and abstraction help us by hiding details and protecting\nthe consistency of our data, but we also need to pay attention to the\ninteractions between our objects and functions. When one function,\nmodule, or object uses another, we say that the one depends on the\nother. These dependencies form a kind of network or graph.\n1",
      "content_length": 1386,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "In a big ball of mud, the dependencies are out of control (as you saw in\nFigure P-1). Changing one node of the graph becomes difficult because\nit has the potential to affect many other parts of the system. Layered\narchitectures are one way of tackling this problem. In a layered\narchitecture, we divide our code into discrete categories or roles, and\nwe introduce rules about which categories of code can call each other.\nOne of the most common examples is the three-layered architecture\nshown in Figure P-2.\nFigure P-2. Layered architecture\nLayered architecture is perhaps the most common pattern for building\nbusiness software. In this model we have user-interface components,",
      "content_length": 678,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "which could be a web page, an API, or a command line; these user-\ninterface components communicate with a business logic layer that\ncontains our business rules and our workflows; and finally, we have a\ndatabase layer that’s responsible for storing and retrieving data.\nFor the rest of this book, we’re going to be systematically turning this\nmodel inside out by obeying one simple principle.\nThe Dependency Inversion Principle\nYou might be familiar with the dependency inversion principle (DIP)\nalready, because it’s the D in SOLID.\nUnfortunately, we can’t illustrate the DIP by using three tiny code\nlistings as we did for encapsulation. However, the whole of Part I is\nessentially a worked example of implementing the DIP throughout an\napplication, so you’ll get your fill of concrete examples.\nIn the meantime, we can talk about DIP’s formal definition:\n1. High-level modules should not depend on low-level modules.\nBoth should depend on abstractions.\n2. Abstractions should not depend on details. Instead, details\nshould depend on abstractions.\nBut what does this mean? Let’s take it bit by bit.\nHigh-level modules are the code that your organization really cares\nabout. Perhaps you work for a pharmaceutical company, and your high-\nlevel modules deal with patients and trials. Perhaps you work for a\n2",
      "content_length": 1306,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "bank, and your high-level modules manage trades and exchanges. The\nhigh-level modules of a software system are the functions, classes, and\npackages that deal with our real-world concepts.\nBy contrast, low-level modules are the code that your organization\ndoesn’t care about. It’s unlikely that your HR department gets excited\nabout filesystems or network sockets. It’s not often that you discuss\nSMTP, HTTP, or AMQP with your finance team. For our nontechnical\nstakeholders, these low-level concepts aren’t interesting or relevant.\nAll they care about is whether the high-level concepts work correctly.\nIf payroll runs on time, your business is unlikely to care whether that’s\na cron job or a transient function running on Kubernetes.\nDepends on doesn’t mean imports or calls, necessarily, but rather a\nmore general idea that one module knows about or needs another\nmodule.\nAnd we’ve mentioned abstractions already: they’re simplified\ninterfaces that encapsulate behavior, in the way that our duckduckgo\nmodule encapsulated a search engine’s API.\nAll problems in computer science can be solved by adding another\nlevel of indirection.\n—David Wheeler\nSo the first part of the DIP says that our business code shouldn’t\ndepend on technical details; instead, both should use abstractions.\nWhy? Broadly, because we want to be able to change them\nindependently of each other. High-level modules should be easy to",
      "content_length": 1405,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "change in response to business needs. Low-level modules (details) are\noften, in practice, harder to change: think about refactoring to change a\nfunction name versus defining, testing, and deploying a database\nmigration to change a column name. We don’t want business logic\nchanges to slow down because they are closely coupled to low-level\ninfrastructure details. But, similarly, it is important to be able to\nchange your infrastructure details when you need to (think about\nsharding a database, for example), without needing to make changes to\nyour business layer. Adding an abstraction between them (the famous\nextra layer of indirection) allows the two to change (more)\nindependently of each other.\nThe second part is even more mysterious. “Abstractions should not\ndepend on details” seems clear enough, but “Details should depend on\nabstractions” is hard to imagine. How can we have an abstraction that\ndoesn’t depend on the details it’s abstracting? By the time we get to\nChapter 4, we’ll have a concrete example that should make this all a\nbit clearer.\nA Place for All Our Business Logic: The\nDomain Model\nBut before we can turn our three-layered architecture inside out, we\nneed to talk more about that middle layer: the high-level modules or\nbusiness logic. One of the most common reasons that our designs go\nwrong is that business logic becomes spread throughout the layers of\nour application, making it hard to identify, understand, and change.",
      "content_length": 1454,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "Chapter 1 shows how to build a business layer with a Domain Model\npattern. The rest of the patterns in Part I show how we can keep the\ndomain model easy to change and free of low-level concerns by\nchoosing the right abstractions and continuously applying the DIP.\n1  If you’ve come across class-responsibility-collaborator (CRC) cards, they’re driving at\nthe same thing: thinking about responsibilities helps you decide how to split things up.\n2  SOLID is an acronym for Robert C. Martin’s five principles of object-oriented design:\nsingle responsibility, open for extension but closed for modification, Liskov substitution,\ninterface segregation, and dependency inversion. See “S.O.L.I.D: The First 5 Principles\nof Object-Oriented Design” by Samuel Oloruntoba.",
      "content_length": 761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "Part I. Building an\nArchitecture to Support\nDomain Modeling\nMost developers have never seen a domain model, only a data\nmodel.\n—Cyrille Martraire, DDD EU 2017\nMost developers we talk to about architecture have a nagging sense\nthat things could be better. They are often trying to rescue a system that\nhas gone wrong somehow, and are trying to put some structure back\ninto a ball of mud. They know that their business logic shouldn’t be\nspread all over the place, but they have no idea how to fix it.\nWe’ve found that many developers, when asked to design a new\nsystem, will immediately start to build a database schema, with the\nobject model treated as an afterthought. This is where it all starts to go\nwrong. Instead, behavior should come first and drive our storage\nrequirements. After all, our customers don’t care about the data model.\nThey care about what the system does; otherwise they’d just use a\nspreadsheet.\nThe first part of the book looks at how to build a rich object model\nthrough TDD (in Chapter 1), and then we’ll show how to keep that",
      "content_length": 1053,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "model decoupled from technical concerns. We show how to build\npersistence-ignorant code and how to create stable APIs around our\ndomain so that we can refactor aggressively.\nTo do that, we present four key design patterns:\nThe Repository pattern, an abstraction over the idea of\npersistent storage\nThe Service Layer pattern to clearly define where our use\ncases begin and end\nThe Unit of Work pattern to provide atomic operations\nThe Aggregate pattern to enforce the integrity of our data\nIf you’d like a picture of where we’re going, take a look at Figure I-1,\nbut don’t worry if none of it makes sense yet! We introduce each box in\nthe figure, one by one, throughout this part of the book.",
      "content_length": 691,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "Figure I-1. A component diagram for our app at the end of Part I\nWe also take a little time out to talk about coupling and abstractions,\nillustrating it with a simple example that shows how and why we\nchoose our abstractions.\nThree appendices are further explorations of the content from Part I:\nAppendix B is a write-up of the infrastructure for our example\ncode: how we build and run the Docker images, where we",
      "content_length": 413,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "manage configuration info, and how we run different types of\ntests.\nAppendix C is a “proof is in the pudding” kind of content,\nshowing how easy it is to swap out our entire infrastructure—\nthe Flask API, the ORM, and Postgres—for a totally different\nI/O model involving a CLI and CSVs.\nFinally, Appendix D may be of interest if you’re wondering\nhow these patterns might look if using Django instead of Flask\nand SQLAlchemy.",
      "content_length": 423,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "Chapter 1. Domain Modeling\nThis chapter looks into how we can model business processes with\ncode, in a way that’s highly compatible with TDD. We’ll discuss why\ndomain modeling matters, and we’ll look at a few key patterns for\nmodeling domains: Entity, Value Object, and Domain Service.\nFigure 1-1 is a simple visual placeholder for our Domain Model\npattern. We’ll fill in some details in this chapter, and as we move on to\nother chapters, we’ll build things around the domain model, but you\nshould always be able to find these little shapes at the core.",
      "content_length": 553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "Domain Model",
      "content_length": 12,
      "extraction_method": "OCR"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "Figure 1-1. A placeholder illustration of our domain model\nWhat Is a Domain Model?\nIn the introduction, we used the term business logic layer to describe\nthe central layer of a three-layered architecture. For the rest of the\nbook, we’re going to use the term domain model instead. This is a\nterm from the DDD community that does a better job of capturing our\nintended meaning (see the next sidebar for more on DDD).\nThe domain is a fancy way of saying the problem you’re trying to\nsolve. Your authors currently work for an online retailer of furniture.\nDepending on which system you’re talking about, the domain might be\npurchasing and procurement, or product design, or logistics and\ndelivery. Most programmers spend their days trying to improve or\nautomate business processes; the domain is the set of activities that\nthose processes support.\nA model is a map of a process or phenomenon that captures a useful\nproperty. Humans are exceptionally good at producing models of\nthings in their heads. For example, when someone throws a ball\ntoward you, you’re able to predict its movement almost unconsciously,\nbecause you have a model of the way objects move in space. Your\nmodel isn’t perfect by any means. Humans have terrible intuitions\nabout how objects behave at near-light speeds or in a vacuum because\nour model was never designed to cover those cases. That doesn’t mean\nthe model is wrong, but it does mean that some predictions fall outside\nof its domain.",
      "content_length": 1462,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "The domain model is the mental map that business owners have of\ntheir businesses. All business people have these mental maps—they’re\nhow humans think about complex processes.\nYou can tell when they’re navigating these maps because they use\nbusiness speak. Jargon arises naturally among people who are\ncollaborating on complex systems.\nImagine that you, our unfortunate reader, were suddenly transported\nlight years away from Earth aboard an alien spaceship with your\nfriends and family and had to figure out, from first principles, how to\nnavigate home.\nIn your first few days, you might just push buttons randomly, but soon\nyou’d learn which buttons did what, so that you could give one another\ninstructions. “Press the red button near the flashing doohickey and then\nthrow that big lever over by the radar gizmo,” you might say.\nWithin a couple of weeks, you’d become more precise as you adopted\nwords to describe the ship’s functions: “Increase oxygen levels in\ncargo bay three” or “turn on the little thrusters.” After a few months,\nyou’d have adopted language for entire complex processes: “Start\nlanding sequence” or “prepare for warp.” This process would happen\nquite naturally, without any formal effort to build a shared glossary.",
      "content_length": 1239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "THIS IS NOT A DDD BOOK. YOU SHOULD READ A DDD BOOK.\nDomain-driven design, or DDD, popularized the concept of domain modeling,  and it’s been a\nhugely successful movement in transforming the way people design software by focusing on the\ncore business domain. Many of the architecture patterns that we cover in this book—including\nEntity, Aggregate, Value Object (see Chapter 7), and Repository (in the next chapter)—come from\nthe DDD tradition.\nIn a nutshell, DDD says that the most important thing about software is that it provides a useful\nmodel of a problem. If we get that model right, our software delivers value and makes new things\npossible.\nIf we get the model wrong, it becomes an obstacle to be worked around. In this book, we can\nshow the basics of building a domain model, and building an architecture around it that leaves\nthe model as free as possible from external constraints, so that it’s easy to evolve and change.\nBut there’s a lot more to DDD and to the processes, tools, and techniques for developing a\ndomain model. We hope to give you a taste of it, though, and cannot encourage you enough to go\non and read a proper DDD book:\nThe original “blue book,” Domain-Driven Design by Eric Evans (Addison-Wesley\nProfessional)\nThe “red book,” Implementing Domain-Driven Design by Vaughn Vernon (Addison-\nWesley Professional)\nSo it is in the mundane world of business. The terminology used by\nbusiness stakeholders represents a distilled understanding of the\ndomain model, where complex ideas and processes are boiled down\nto a single word or phrase.\nWhen we hear our business stakeholders using unfamiliar words, or\nusing terms in a specific way, we should listen to understand the\ndeeper meaning and encode their hard-won experience into our\nsoftware.\nWe’re going to use a real-world domain model throughout this book,\nspecifically a model from our current employment. MADE.com is a\n1",
      "content_length": 1899,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "successful furniture retailer. We source our furniture from\nmanufacturers all over the world and sell it across Europe.\nWhen you buy a sofa or a coffee table, we have to figure out how best\nto get your goods from Poland or China or Vietnam and into your living\nroom.\nAt a high level, we have separate systems that are responsible for\nbuying stock, selling stock to customers, and shipping goods to\ncustomers. A system in the middle needs to coordinate the process by\nallocating stock to a customer’s orders; see Figure 1-2.",
      "content_length": 523,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "wes\n\nCistomer\n\n\\Wanttobuy funiture\n\nsystem\nPacis\n\nWanages wero for bying\nstock fom supplies\n\nsystem\nActin\n\n«stem\n\nVarebose\n\nAlotessocktocsomer\nodes\n\nManages wero fo\nshppinggoodstocxtoness",
      "content_length": 187,
      "extraction_method": "OCR"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "Figure 1-2. Context diagram for the allocation service\nFor the purposes of this book, we’re imagining that the business\ndecides to implement an exciting new way of allocating stock. Until\nnow, the business has been presenting stock and lead times based on\nwhat is physically available in the warehouse. If and when the\nwarehouse runs out, a product is listed as “out of stock” until the next\nshipment arrives from the manufacturer.\nHere’s the innovation: if we have a system that can keep track of all\nour shipments and when they’re due to arrive, we can treat the goods\non those ships as real stock and part of our inventory, just with slightly\nlonger lead times. Fewer goods will appear to be out of stock, we’ll\nsell more, and the business can save money by keeping lower\ninventory in the domestic warehouse.\nBut allocating orders is no longer a trivial matter of decrementing a\nsingle quantity in the warehouse system. We need a more complex\nallocation mechanism. Time for some domain modeling.\nExploring the Domain Language\nUnderstanding the domain model takes time, and patience, and Post-it\nnotes. We have an initial conversation with our business experts and\nagree on a glossary and some rules for the first minimal version of the\ndomain model. Wherever possible, we ask for concrete examples to\nillustrate each rule.",
      "content_length": 1325,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "We make sure to express those rules in the business jargon (the\nubiquitous language in DDD terminology). We choose memorable\nidentifiers for our objects so that the examples are easier to talk about.\n“Some Notes on Allocation” shows some notes we might have taken\nwhile having a conversation with our domain experts about allocation.\nSOME NOTES ON ALLOCATION\nA product is identified by a SKU, pronounced “skew,” which is short for stock-keeping unit.\nCustomers place orders. An order is identified by an order reference and comprises multiple\norder lines, where each line has a SKU and a quantity. For example:\n10 units of RED-CHAIR\n1 unit of TASTELESS-LAMP\nThe purchasing department orders small batches of stock. A batch of stock has a unique ID\ncalled a reference, a SKU, and a quantity.\nWe need to allocate order lines to batches. When we’ve allocated an order line to a batch, we will\nsend stock from that specific batch to the customer’s delivery address. When we allocate x units\nof stock to a batch, the available quantity is reduced by x. For example:\nWe have a batch of 20 SMALL-TABLE, and we allocate an order line for 2 SMALL-TABLE.\nThe batch should have 18 SMALL-TABLE remaining.\nWe can’t allocate to a batch if the available quantity is less than the quantity of the order line. For\nexample:\nWe have a batch of 1 BLUE-CUSHION, and an order line for 2 BLUE-CUSHION.\nWe should not be able to allocate the line to the batch.\nWe can’t allocate the same line twice. For example:\nWe have a batch of 10 BLUE-VASE, and we allocate an order line for 2 BLUE-VASE.\nIf we allocate the order line again to the same batch, the batch should still have an\navailable quantity of 8.\nBatches have an ETA if they are currently shipping, or they may be in warehouse stock. We\nallocate to warehouse stock in preference to shipment batches. We allocate to shipment batches\nin order of which has the earliest ETA.",
      "content_length": 1903,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "Unit Testing Domain Models\nWe’re not going to show you how TDD works in this book, but we\nwant to show you how we would construct a model from this business\nconversation.\nEXERCISE FOR THE READER\nWhy not have a go at solving this problem yourself? Write a few unit tests to see if you can\ncapture the essence of these business rules in nice, clean code.\nYou’ll find some placeholder unit tests on GitHub, but you could just start from scratch, or\ncombine/rewrite them however you like.\nHere’s what one of our first tests might look like:\nA first test for allocation (test_batches.py)\ndef test_allocating_to_a_batch_reduces_the_available_quantity(): \n    batch = Batch(\"batch-001\", \"SMALL-TABLE\", qty=20, eta=date.today()) \n    line = OrderLine('order-ref', \"SMALL-TABLE\", 2) \n \n    batch.allocate(line) \n \n    assert batch.available_quantity == 18\nThe name of our unit test describes the behavior that we want to see\nfrom the system, and the names of the classes and variables that we use\nare taken from the business jargon. We could show this code to our\nnontechnical coworkers, and they would agree that this correctly\ndescribes the behavior of the system.\nAnd here is a domain model that meets our requirements:",
      "content_length": 1213,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "First cut of a domain model for batches (model.py)\n@dataclass(frozen=True)  \nclass OrderLine:\n    orderid: str\n    sku: str\n    qty: int\nclass Batch:\n    def __init__(\n        self, ref: str, sku: str, qty: int, eta: Optional[date]  \n    ):\n        self.reference = ref\n        self.sku = sku\n        self.eta = eta\n        self.available_quantity = qty\n    def allocate(self, line: OrderLine):\n        self.available_quantity -= line.qty  \nOrderLine is an immutable dataclass with no behavior.\nWe’re not showing imports in most code listings, in an attempt to\nkeep them clean. We’re hoping you can guess that this came via\nfrom dataclasses import dataclass; likewise,\ntyping.Optional and datetime.date. If you want to double-\ncheck anything, you can see the full working code for each chapter\nin its branch (e.g., chapter_01_domain_model).\nType hints are still a matter of controversy in the Python world.\nFor domain models, they can sometimes help to clarify or\ndocument what the expected arguments are, and people with IDEs\nare often grateful for them. You may decide the price paid in terms\nof readability is too high.\nOur implementation here is trivial: a Batch just wraps an integer\navailable_quantity, and we decrement that value on allocation.\n2",
      "content_length": 1253,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "We’ve written quite a lot of code just to subtract one number from\nanother, but we think that modeling our domain precisely will pay off.\nLet’s write some new failing tests:\nTesting logic for what we can allocate (test_batches.py)\ndef make_batch_and_line(sku, batch_qty, line_qty): \n    return ( \n        Batch(\"batch-001\", sku, batch_qty, eta=date.today()), \n        OrderLine(\"order-123\", sku, line_qty) \n    ) \n \n \ndef test_can_allocate_if_available_greater_than_required(): \n    large_batch, small_line = make_batch_and_line(\"ELEGANT-LAMP\", 20, 2) \n    assert large_batch.can_allocate(small_line) \n \ndef test_cannot_allocate_if_available_smaller_than_required(): \n    small_batch, large_line = make_batch_and_line(\"ELEGANT-LAMP\", 2, 20) \n    assert small_batch.can_allocate(large_line) is False \n \ndef test_can_allocate_if_available_equal_to_required(): \n    batch, line = make_batch_and_line(\"ELEGANT-LAMP\", 2, 2) \n    assert batch.can_allocate(line) \n \ndef test_cannot_allocate_if_skus_do_not_match(): \n    batch = Batch(\"batch-001\", \"UNCOMFORTABLE-CHAIR\", 100, eta=None) \n    different_sku_line = OrderLine(\"order-123\", \"EXPENSIVE-TOASTER\", 10) \n    assert batch.can_allocate(different_sku_line) is False\nThere’s nothing too unexpected here. We’ve refactored our test suite so\nthat we don’t keep repeating the same lines of code to create a batch\nand a line for the same SKU; and we’ve written four simple tests for a\nnew method can_allocate. Again, notice that the names we use\n3",
      "content_length": 1487,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "mirror the language of our domain experts, and the examples we\nagreed upon are directly written into code.\nWe can implement this straightforwardly, too, by writing the\ncan_allocate method of Batch:\nA new method in the model (model.py)\n    def can_allocate(self, line: OrderLine) -> bool: \n        return self.sku == line.sku and self.available_quantity >= line.qty\nSo far, we can manage the implementation by just incrementing and\ndecrementing Batch.available_quantity, but as we get into\ndeallocate() tests, we’ll be forced into a more intelligent solution:\nThis test is going to require a smarter model (test_batches.py)\ndef test_can_only_deallocate_allocated_lines(): \n    batch, unallocated_line = make_batch_and_line(\"DECORATIVE-TRINKET\", 20, 2) \n    batch.deallocate(unallocated_line) \n    assert batch.available_quantity == 20\nIn this test, we’re asserting that deallocating a line from a batch has no\neffect unless the batch previously allocated the line. For this to work,\nour Batch needs to understand which lines have been allocated. Let’s\nlook at the implementation:\nThe domain model now tracks allocations (model.py)\nclass Batch: \n    def __init__( \n        self, ref: str, sku: str, qty: int, eta: Optional[date] \n    ): \n        self.reference = ref",
      "content_length": 1264,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "self.sku = sku \n        self.eta = eta \n        self._purchased_quantity = qty \n        self._allocations = set()  # type: Set[OrderLine] \n \n    def allocate(self, line: OrderLine): \n        if self.can_allocate(line): \n            self._allocations.add(line) \n \n    def deallocate(self, line: OrderLine): \n        if line in self._allocations: \n            self._allocations.remove(line) \n \n    @property \n    def allocated_quantity(self) -> int: \n        return sum(line.qty for line in self._allocations) \n \n    @property \n    def available_quantity(self) -> int: \n        return self._purchased_quantity - self.allocated_quantity \n \n    def can_allocate(self, line: OrderLine) -> bool: \n        return self.sku == line.sku and self.available_quantity >= line.qty\nFigure 1-3 shows the model in UML.\nFigure 1-3. Our model in UML\nNow we’re getting somewhere! A batch now keeps track of a set of\nallocated OrderLine objects. When we allocate, if we have enough",
      "content_length": 960,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "available quantity, we just add to the set. Our available_quantity is\nnow a calculated property: purchased quantity minus allocated\nquantity.\nYes, there’s plenty more we could do. It’s a little disconcerting that\nboth allocate() and deallocate() can fail silently, but we have the\nbasics.\nIncidentally, using a set for ._allocations makes it simple for us to\nhandle the last test, because items in a set are unique:\nLast batch test! (test_batches.py)\ndef test_allocation_is_idempotent(): \n    batch, line = make_batch_and_line(\"ANGULAR-DESK\", 20, 2) \n    batch.allocate(line) \n    batch.allocate(line) \n    assert batch.available_quantity == 18\nAt the moment, it’s probably a valid criticism to say that the domain\nmodel is too trivial to bother with DDD (or even object orientation!).\nIn real life, any number of business rules and edge cases crop up:\ncustomers can ask for delivery on specific future dates, which means\nwe might not want to allocate them to the earliest batch. Some SKUs\naren’t in batches, but ordered on demand directly from suppliers, so\nthey have different logic. Depending on the customer’s location, we\ncan allocate to only a subset of warehouses and shipments that are in\ntheir region—except for some SKUs we’re happy to deliver from a\nwarehouse in a different region if we’re out of stock in the home\nregion. And so on. A real business in the real world knows how to pile\non complexity faster than we can show on the page!",
      "content_length": 1448,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "But taking this simple domain model as a placeholder for something\nmore complex, we’re going to extend our simple domain model in the\nrest of the book and plug it into the real world of APIs and databases\nand spreadsheets. We’ll see how sticking rigidly to our principles of\nencapsulation and careful layering will help us to avoid a ball of mud.\nMORE TYPES FOR MORE TYPE HINTS\nIf you really want to go to town with type hints, you could go so far as wrapping primitive types by\nusing typing.NewType:\nJust taking it way too far, Bob\nfrom dataclasses import dataclass\nfrom typing import NewType \n \nQuantity = NewType(\"Quantity\", int)\nSku = NewType(\"Sku\", str)\nReference = NewType(\"Reference\", str)\n... \n \nclass Batch: \n    def __init__(self, ref: Reference, sku: Sku, qty: Quantity): \n        self.sku = sku \n        self.reference = ref \n        self._purchased_quantity = qty\nThat would allow our type checker to make sure that we don’t pass a Sku where a Reference is\nexpected, for example.\nWhether you think this is wonderful or appalling is a matter of debate.\nDataclasses Are Great for Value Objects\nWe’ve used line liberally in the previous code listings, but what is a\nline? In our business language, an order has multiple line items, where\neach line has a SKU and a quantity. We can imagine that a simple\nYAML file containing order information might look like this:\nOrder info as YAML\n4",
      "content_length": 1394,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "Order_reference: 12345\nLines: \n  - sku: RED-CHAIR \n    qty: 25 \n  - sku: BLU-CHAIR \n    qty: 25 \n  - sku: GRN-CHAIR \n    qty: 25\nNotice that while an order has a reference that uniquely identifies it, a\nline does not. (Even if we add the order reference to the OrderLine\nclass, it’s not something that uniquely identifies the line itself.)\nWhenever we have a business concept that has data but no identity, we\noften choose to represent it using the Value Object pattern. A value\nobject is any domain object that is uniquely identified by the data it\nholds; we usually make them immutable:\nOrderLine is a value object\n@dataclass(frozen=True)\nclass OrderLine: \n    orderid: OrderReference \n    sku: ProductReference \n    qty: Quantity\nOne of the nice things that dataclasses (or namedtuples) give us is\nvalue equality, which is the fancy way of saying, “Two lines with the\nsame orderid, sku, and qty are equal.”\nMore examples of value objects\nfrom dataclasses import dataclass\nfrom typing import NamedTuple\nfrom collections import namedtuple",
      "content_length": 1039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "@dataclass(frozen=True)\nclass Name: \n    first_name: str \n    surname: str \n \nclass Money(NamedTuple): \n    currency: str \n    value: int \n \nLine = namedtuple('Line', ['sku', 'qty']) \n \ndef test_equality(): \n    assert Money('gbp', 10) == Money('gbp', 10) \n    assert Name('Harry', 'Percival') != Name('Bob', 'Gregory') \n    assert Line('RED-CHAIR', 5) == Line('RED-CHAIR', 5)\nThese value objects match our real-world intuition about how their\nvalues work. It doesn’t matter which £10 note we’re talking about,\nbecause they all have the same value. Likewise, two names are equal\nif both the first and last names match; and two lines are equivalent if\nthey have the same customer order, product code, and quantity. We can\nstill have complex behavior on a value object, though. In fact, it’s\ncommon to support operations on values; for example, mathematical\noperators:\nMath with value objects\nfiver = Money('gbp', 5)\ntenner = Money('gbp', 10) \n \ndef can_add_money_values_for_the_same_currency(): \n    assert fiver + fiver == tenner \n \ndef can_subtract_money_values(): \n    assert tenner - fiver == fiver",
      "content_length": 1101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "def adding_different_currencies_fails(): \n    with pytest.raises(ValueError): \n        Money('usd', 10) + Money('gbp', 10) \n \ndef can_multiply_money_by_a_number(): \n    assert fiver * 5 == Money('gbp', 25) \n \ndef multiplying_two_money_values_is_an_error(): \n    with pytest.raises(TypeError): \n        tenner * fiver\nValue Objects and Entities\nAn order line is uniquely identified by its order ID, SKU, and quantity;\nif we change one of those values, we now have a new line. That’s the\ndefinition of a value object: any object that is identified only by its\ndata and doesn’t have a long-lived identity. What about a batch,\nthough? That is identified by a reference.\nWe use the term entity to describe a domain object that has long-lived\nidentity. On the previous page, we introduced a Name class as a value\nobject. If we take the name Harry Percival and change one letter, we\nhave the new Name object Barry Percival.\nIt should be clear that Harry Percival is not equal to Barry Percival:\nA name itself cannot change…\ndef test_name_equality(): \n    assert Name(\"Harry\", \"Percival\") != Name(\"Barry\", \"Percival\")\nBut what about Harry as a person? People do change their names, and\ntheir marital status, and even their gender, but we continue to recognize",
      "content_length": 1251,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "them as the same individual. That’s because humans, unlike names,\nhave a persistent identity:\nBut a person can!\nclass Person: \n \n    def __init__(self, name: Name): \n        self.name = name \n \n \ndef test_barry_is_harry(): \n    harry = Person(Name(\"Harry\", \"Percival\")) \n    barry = harry \n \n    barry.name = Name(\"Barry\", \"Percival\") \n \n    assert harry is barry and barry is harry\nEntities, unlike values, have identity equality. We can change their\nvalues, and they are still recognizably the same thing. Batches, in our\nexample, are entities. We can allocate lines to a batch, or change the\ndate that we expect it to arrive, and it will still be the same entity.\nWe usually make this explicit in code by implementing equality\noperators on entities:\nImplementing equality operators (model.py)\nclass Batch: \n    ... \n \n    def __eq__(self, other): \n        if not isinstance(other, Batch): \n            return False \n        return other.reference == self.reference",
      "content_length": 967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "def __hash__(self): \n        return hash(self.reference)\nPython’s __eq__ magic method defines the behavior of the class for\nthe == operator.\nFor both entity and value objects, it’s also worth thinking through how\n__hash__ will work. It’s the magic method Python uses to control the\nbehavior of objects when you add them to sets or use them as dict\nkeys; you can find more info in the Python docs.\nFor value objects, the hash should be based on all the value attributes,\nand we should ensure that the objects are immutable. We get this for\nfree by specifying @frozen=True on the dataclass.\nFor entities, the simplest option is to say that the hash is None, meaning\nthat the object is not hashable and cannot, for example, be used in a\nset. If for some reason you decide you really do want to use set or dict\noperations with entities, the hash should be based on the attribute(s),\nsuch as .reference, that defines the entity’s unique identity over time.\nYou should also try to somehow make that attribute read-only.\nWARNING\nThis is tricky territory; you shouldn’t modify __hash__ without also modifying\n__eq__. If you’re not sure what you’re doing, further reading is suggested.\n“Python Hashes and Equality” by our tech reviewer Hynek Schlawack is a good\nplace to start.\n5",
      "content_length": 1270,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "Not Everything Has to Be an Object: A\nDomain Service Function\nWe’ve made a model to represent batches, but what we actually need\nto do is allocate order lines against a specific set of batches that\nrepresent all our stock.\nSometimes, it just isn’t a thing.\n—Eric Evans, Domain-Driven Design\nEvans discusses the idea of Domain Service operations that don’t have\na natural home in an entity or value object.  A thing that allocates an\norder line, given a set of batches, sounds a lot like a function, and we\ncan take advantage of the fact that Python is a multiparadigm language\nand just make it a function.\nLet’s see how we might test-drive such a function:\nTesting our domain service (test_allocate.py)\ndef test_prefers_current_stock_batches_to_shipments(): \n    in_stock_batch = Batch(\"in-stock-batch\", \"RETRO-CLOCK\", 100, eta=None) \n    shipment_batch = Batch(\"shipment-batch\", \"RETRO-CLOCK\", 100, eta=tomorrow) \n    line = OrderLine(\"oref\", \"RETRO-CLOCK\", 10) \n \n    allocate(line, [in_stock_batch, shipment_batch]) \n \n    assert in_stock_batch.available_quantity == 90 \n    assert shipment_batch.available_quantity == 100 \n \n \ndef test_prefers_earlier_batches(): \n    earliest = Batch(\"speedy-batch\", \"MINIMALIST-SPOON\", 100, eta=today) \n    medium = Batch(\"normal-batch\", \"MINIMALIST-SPOON\", 100, eta=tomorrow) \n    latest = Batch(\"slow-batch\", \"MINIMALIST-SPOON\", 100, eta=later) \n6",
      "content_length": 1388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "line = OrderLine(\"order1\", \"MINIMALIST-SPOON\", 10) \n \n    allocate(line, [medium, earliest, latest]) \n \n    assert earliest.available_quantity == 90 \n    assert medium.available_quantity == 100 \n    assert latest.available_quantity == 100 \n \n \ndef test_returns_allocated_batch_ref(): \n    in_stock_batch = Batch(\"in-stock-batch-ref\", \"HIGHBROW-POSTER\", 100, \neta=None) \n    shipment_batch = Batch(\"shipment-batch-ref\", \"HIGHBROW-POSTER\", 100, \neta=tomorrow) \n    line = OrderLine(\"oref\", \"HIGHBROW-POSTER\", 10) \n    allocation = allocate(line, [in_stock_batch, shipment_batch]) \n    assert allocation == in_stock_batch.reference\nAnd our service might look like this:\nA standalone function for our domain service (model.py)\ndef allocate(line: OrderLine, batches: List[Batch]) -> str: \n    batch = next( \n        b for b in sorted(batches) if b.can_allocate(line) \n    ) \n    batch.allocate(line) \n    return batch.reference\nPython’s Magic Methods Let Us Use Our Models\nwith Idiomatic Python\nYou may or may not like the use of next() in the preceding code, but\nwe’re pretty sure you’ll agree that being able to use sorted() on our\nlist of batches is nice, idiomatic Python.\nTo make it work, we implement __gt__ on our domain model:",
      "content_length": 1229,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "Magic methods can express domain semantics (model.py)\nclass Batch: \n    ... \n \n    def __gt__(self, other): \n        if self.eta is None: \n            return False \n        if other.eta is None: \n            return True \n        return self.eta > other.eta\nThat’s lovely.\nExceptions Can Express Domain Concepts Too\nWe have one final concept to cover: exceptions can be used to express\ndomain concepts too. In our conversations with domain experts, we’ve\nlearned about the possibility that an order cannot be allocated because\nwe are out of stock, and we can capture that by using a domain\nexception:\nTesting out-of-stock exception (test_allocate.py)\ndef test_raises_out_of_stock_exception_if_cannot_allocate(): \n    batch = Batch('batch1', 'SMALL-FORK', 10, eta=today) \n    allocate(OrderLine('order1', 'SMALL-FORK', 10), [batch]) \n \n    with pytest.raises(OutOfStock, match='SMALL-FORK'): \n        allocate(OrderLine('order2', 'SMALL-FORK', 1), [batch])",
      "content_length": 954,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "DOMAIN MODELING RECAP\nDomain modeling\nThis is the part of your code that is closest to the business, the most likely to change, and the\nplace where you deliver the most value to the business. Make it easy to understand and\nmodify.\nDistinguish entities from value objects\nA value object is defined by its attributes. It’s usually best implemented as an immutable\ntype. If you change an attribute on a Value Object, it represents a different object. In contrast,\nan entity has attributes that may vary over time and it will still be the same entity. It’s important\nto define what does uniquely identify an entity (usually some sort of name or reference field).\nNot everything has to be an object\nPython is a multiparadigm language, so let the “verbs” in your code be functions. For every\nFooManager, BarBuilder, or BazFactory, there’s often a more expressive and readable\nmanage_foo(), build_bar(), or get_baz() waiting to happen.\nThis is the time to apply your best OO design principles\nRevisit the SOLID principles and all the other good heuristics like “has a versus is-a,” “prefer\ncomposition over inheritance,” and so on.\nYou’ll also want to think about consistency boundaries and aggregates\nBut that’s a topic for Chapter 7.\nWe won’t bore you too much with the implementation, but the main\nthing to note is that we take care in naming our exceptions in the\nubiquitous language, just as we do our entities, value objects, and\nservices:\nRaising a domain exception (model.py)\nclass OutOfStock(Exception): \n    pass \n \n \ndef allocate(line: OrderLine, batches: List[Batch]) -> str: \n    try: \n        batch = next( \n        ...",
      "content_length": 1626,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "except StopIteration: \n        raise OutOfStock(f'Out of stock for sku {line.sku}')\nFigure 1-4 is a visual representation of where we’ve ended up.",
      "content_length": 146,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "Figure 1-4. Our domain model at the end of the chapter\nThat’ll probably do for now! We have a domain service that we can\nuse for our first use case. But first we’ll need a database…\n1  DDD did not originate domain modeling. Eric Evans refers to the 2002 book Object\nDesign by Rebecca Wirfs-Brock and Alan McKean (Addison-Wesley Professional),\nwhich introduced responsibility-driven design, of which DDD is a special case dealing\nwith the domain. But even that is too late, and OO enthusiasts will tell you to look further\nback to Ivar Jacobson and Grady Booch; the term has been around since the mid-1980s.\n2  In previous Python versions, we might have used a namedtuple. You could also check out\nHynek Schlawack’s excellent attrs.\n3  Or perhaps you think there’s not enough code? What about some sort of check that the\nSKU in the OrderLine matches Batch.sku? We saved some thoughts on validation for\nAppendix E.\n4  It is appalling. Please, please don’t do this. —Harry\n5  The __eq__ method is pronounced “dunder-EQ.” By some, at least.\n6  Domain services are not the same thing as the services from the service layer, although\nthey are often closely related. A domain service represents a business concept or\nprocess, whereas a service-layer service represents a use case for your application.\nOften the service layer will call a domain service.",
      "content_length": 1346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "Chapter 2. Repository Pattern\nIt’s time to make good on our promise to use the dependency inversion\nprinciple as a way of decoupling our core logic from infrastructural\nconcerns.\nWe’ll introduce the Repository pattern, a simplifying abstraction over\ndata storage, allowing us to decouple our model layer from the data\nlayer. We’ll present a concrete example of how this simplifying\nabstraction makes our system more testable by hiding the complexities\nof the database.\nFigure 2-1 shows a little preview of what we’re going to build: a\nRepository object that sits between our domain model and the\ndatabase.",
      "content_length": 605,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "Figure 2-1. Before and after the Repository pattern",
      "content_length": 51,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "TIP\nThe code for this chapter is in the chapter_02_repository branch on GitHub.\ngit clone https://github.com/cosmicpython/code.git \ncd code \ngit checkout chapter_02_repository \n# or to code along, checkout the previous chapter: \ngit checkout chapter_01_domain_model\nPersisting Our Domain Model\nIn Chapter 1 we built a simple domain model that can allocate orders\nto batches of stock. It’s easy for us to write tests against this code\nbecause there aren’t any dependencies or infrastructure to set up. If we\nneeded to run a database or an API and create test data, our tests\nwould be harder to write and maintain.\nSadly, at some point we’ll need to put our perfect little model in the\nhands of users and contend with the real world of spreadsheets and\nweb browsers and race conditions. For the next few chapters we’re\ngoing to look at how we can connect our idealized domain model to\nexternal state.\nWe expect to be working in an agile manner, so our priority is to get to\na minimum viable product as quickly as possible. In our case, that’s\ngoing to be a web API. In a real project, you might dive straight in\nwith some end-to-end tests and start plugging in a web framework,\ntest-driving things outside-in.",
      "content_length": 1207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "But we know that, no matter what, we’re going to need some form of\npersistent storage, and this is a textbook, so we can allow ourselves a\ntiny bit more bottom-up development and start to think about storage\nand databases.\nSome Pseudocode: What Are We Going\nto Need?\nWhen we build our first API endpoint, we know we’re going to have\nsome code that looks more or less like the following.\nWhat our first API endpoint will look like\n@flask.route.gubbins\ndef allocate_endpoint(): \n    # extract order line from request \n    line = OrderLine(request.params, ...) \n    # load all batches from the DB \n    batches = ... \n    # call our domain service \n    allocate(line, batches) \n    # then save the allocation back to the database somehow \n    return 201\nNOTE\nWe’ve used Flask because it’s lightweight, but you don’t need to be a Flask user\nto understand this book. In fact, we’ll show you how to make your choice of\nframework a minor detail.\nWe’ll need a way to retrieve batch info from the database and\ninstantiate our domain model objects from it, and we’ll also need a",
      "content_length": 1067,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "way of saving them back to the database.\nWhat? Oh, “gubbins” is a British word for “stuff.” You can just\nignore that. It’s pseudocode, OK?\nApplying the DIP to Data Access\nAs mentioned in the introduction, a layered architecture is a common\napproach to structuring a system that has a UI, some logic, and a\ndatabase (see Figure 2-2).\nFigure 2-2. Layered architecture",
      "content_length": 365,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "Django’s Model-View-Template structure is closely related, as is\nModel-View-Controller (MVC). In any case, the aim is to keep the\nlayers separate (which is a good thing), and to have each layer depend\nonly on the one below it.\nBut we want our domain model to have no dependencies whatsoever.\nWe don’t want infrastructure concerns bleeding over into our domain\nmodel and slowing our unit tests or our ability to make changes.\nInstead, as discussed in the introduction, we’ll think of our model as\nbeing on the “inside,” and dependencies flowing inward to it; this is\nwhat people sometimes call onion architecture (see Figure 2-3).\n1",
      "content_length": 631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "Figure 2-3. Onion architecture\nIS THIS PORTS AND ADAPTERS?\nIf you’ve been reading about architectural patterns, you may be asking yourself questions like\nthis:\nIs this ports and adapters? Or is it hexagonal architecture? Is that the same as onion\narchitecture? What about the clean architecture? What’s a port, and what’s an adapter? Why\ndo you people have so many words for the same thing?\nAlthough some people like to nitpick over the differences, all these are pretty much names for the\nsame thing, and they all boil down to the dependency inversion principle: high-level modules (the\ndomain) should not depend on low-level ones (the infrastructure).\nWe’ll get into some of the nitty-gritty around “depending on abstractions,” and whether there is a\nPythonic equivalent of interfaces, later in the book. See also “What Is a Port and What Is an\nAdapter, in Python?”.\n2",
      "content_length": 870,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "Reminder: Our Model\nLet’s remind ourselves of our domain model (see Figure 2-4): an\nallocation is the concept of linking an OrderLine to a Batch. We’re\nstoring the allocations as a collection on our Batch object.\nFigure 2-4. Our model\nLet’s see how we might translate this to a relational database.\nThe “Normal” ORM Way: Model Depends on ORM\nThese days, it’s unlikely that your team members are hand-rolling their\nown SQL queries. Instead, you’re almost certainly using some kind of\nframework to generate SQL for you based on your model objects.\nThese frameworks are called object-relational mappers (ORMs)\nbecause they exist to bridge the conceptual gap between the world of\nobjects and domain modeling and the world of databases and\nrelational algebra.\nThe most important thing an ORM gives us is persistence ignorance:\nthe idea that our fancy domain model doesn’t need to know anything",
      "content_length": 888,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "about how data is loaded or persisted. This helps keep our domain\nclean of direct dependencies on particular database technologies.\nBut if you follow the typical SQLAlchemy tutorial, you’ll end up with\nsomething like this:\nSQLAlchemy “declarative” syntax, model depends on ORM (orm.py)\nfrom sqlalchemy import Column, ForeignKey, Integer, String\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship \n \nBase = declarative_base() \n \nclass Order(Base): \n    id = Column(Integer, primary_key=True) \n \nclass OrderLine(Base): \n    id = Column(Integer, primary_key=True) \n    sku = Column(String(250)) \n    qty = Integer(String(250)) \n    order_id = Column(Integer, ForeignKey('order.id')) \n    order = relationship(Order) \n \nclass Allocation(Base): \n    ...\nYou don’t need to understand SQLAlchemy to see that our pristine\nmodel is now full of dependencies on the ORM and is starting to look\nugly as hell besides. Can we really say this model is ignorant of the\ndatabase? How can it be separate from storage concerns when our\nmodel properties are directly coupled to database columns?\n3",
      "content_length": 1122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "DJANGO’S ORM IS ESSENTIALLY THE SAME, BUT MORE\nRESTRICTIVE\nIf you’re more used to Django, the preceding “declarative” SQLAlchemy snippet translates to\nsomething like this:\nDjango ORM example\nclass Order(models.Model): \n    pass \n \nclass OrderLine(models.Model): \n    sku = models.CharField(max_length=255) \n    qty = models.IntegerField() \n    order = models.ForeignKey(Order) \n \nclass Allocation(models.Model): \n    ...\nThe point is the same—our model classes inherit directly from ORM classes, so our model\ndepends on the ORM. We want it to be the other way around.\nDjango doesn’t provide an equivalent for SQLAlchemy’s classical mapper, but see Appendix D\nfor examples of how to apply dependency inversion and the Repository pattern to Django.\nInverting the Dependency: ORM Depends on\nModel\nWell, thankfully, that’s not the only way to use SQLAlchemy. The\nalternative is to define your schema separately, and to define an\nexplicit mapper for how to convert between the schema and our\ndomain model, what SQLAlchemy calls a classical mapping:\nExplicit ORM mapping with SQLAlchemy Table objects (orm.py)\nfrom sqlalchemy.orm import mapper, relationship\nimport model  \nmetadata = MetaData()",
      "content_length": 1188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "order_lines = Table(  \n    'order_lines', metadata,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('sku', String(255)),\n    Column('qty', Integer, nullable=False),\n    Column('orderid', String(255)),\n)\n...\ndef start_mappers():\n    lines_mapper = mapper(model.OrderLine, order_lines)  \nThe ORM imports (or “depends on” or “knows about”) the domain\nmodel, and not the other way around.\nWe define our database tables and columns by using\nSQLAlchemy’s abstractions.\nWhen we call the mapper function, SQLAlchemy does its magic to\nbind our domain model classes to the various tables we’ve\ndefined.\nThe end result will be that, if we call start_mappers, we will be\nable to easily load and save domain model instances from and to the\ndatabase. But if we never call that function, our domain model classes\nstay blissfully unaware of the database.\nThis gives us all the benefits of SQLAlchemy, including the ability to\nuse alembic for migrations, and the ability to transparently query\nusing our domain classes, as we’ll see.\nWhen you’re first trying to build your ORM config, it can be useful to\nwrite tests for it, as in the following example:\n4",
      "content_length": 1166,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "Testing the ORM directly (throwaway tests) (test_orm.py)\ndef test_orderline_mapper_can_load_lines(session):  \n    session.execute(\n        'INSERT INTO order_lines (orderid, sku, qty) VALUES '\n        '(\"order1\", \"RED-CHAIR\", 12),'\n        '(\"order1\", \"RED-TABLE\", 13),'\n        '(\"order2\", \"BLUE-LIPSTICK\", 14)'\n    )\n    expected = [\n        model.OrderLine(\"order1\", \"RED-CHAIR\", 12),\n        model.OrderLine(\"order1\", \"RED-TABLE\", 13),\n        model.OrderLine(\"order2\", \"BLUE-LIPSTICK\", 14),\n    ]\n    assert session.query(model.OrderLine).all() == expected\ndef test_orderline_mapper_can_save_lines(session):\n    new_line = model.OrderLine(\"order1\", \"DECORATIVE-WIDGET\", 12)\n    session.add(new_line)\n    session.commit()\n    rows = list(session.execute('SELECT orderid, sku, qty FROM \"order_lines\"'))\n    assert rows == [(\"order1\", \"DECORATIVE-WIDGET\", 12)]\nIf you haven’t used pytest, the session argument to this test needs\nexplaining. You don’t need to worry about the details of pytest or\nits fixtures for the purposes of this book, but the short explanation\nis that you can define common dependencies for your tests as\n“fixtures,” and pytest will inject them to the tests that need them by\nlooking at their function arguments. In this case, it’s a SQLAlchemy\ndatabase session.\nYou probably wouldn’t keep these tests around—as you’ll see shortly,\nonce you’ve taken the step of inverting the dependency of ORM and\ndomain model, it’s only a small additional step to implement another\nabstraction called the Repository pattern, which will be easier to write",
      "content_length": 1563,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "tests against and will provide a simple interface for faking out later in\ntests.\nBut we’ve already achieved our objective of inverting the traditional\ndependency: the domain model stays “pure” and free from\ninfrastructure concerns. We could throw away SQLAlchemy and use a\ndifferent ORM, or a totally different persistence system, and the\ndomain model doesn’t need to change at all.\nDepending on what you’re doing in your domain model, and especially\nif you stray far from the OO paradigm, you may find it increasingly\nhard to get the ORM to produce the exact behavior you need, and you\nmay need to modify your domain model.  As so often happens with\narchitectural decisions, you’ll need to consider a trade-off. As the Zen\nof Python says, “Practicality beats purity!”\nAt this point, though, our API endpoint might look something like the\nfollowing, and we could get it to work just fine:\nUsing SQLAlchemy directly in our API endpoint\n@flask.route.gubbins\ndef allocate_endpoint(): \n    session = start_session() \n \n    # extract order line from request \n    line = OrderLine( \n        request.json['orderid'], \n        request.json['sku'], \n        request.json['qty'], \n    ) \n \n    # load all batches from the DB \n5",
      "content_length": 1217,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "batches = session.query(Batch).all() \n \n    # call our domain service \n    allocate(line, batches) \n \n    # save the allocation back to the database \n    session.commit() \n \n    return 201\nIntroducing the Repository Pattern\nThe Repository pattern is an abstraction over persistent storage. It\nhides the boring details of data access by pretending that all of our\ndata is in memory.\nIf we had infinite memory in our laptops, we’d have no need for\nclumsy databases. Instead, we could just use our objects whenever we\nliked. What would that look like?\nYou have to get your data from somewhere\nimport all_my_data \n \ndef create_a_batch(): \n    batch = Batch(...) \n    all_my_data.batches.add(batch) \n \ndef modify_a_batch(batch_id, new_quantity): \n    batch = all_my_data.batches.get(batch_id) \n    batch.change_initial_quantity(new_quantity)\nEven though our objects are in memory, we need to put them\nsomewhere so we can find them again. Our in-memory data would let\nus add new objects, just like a list or a set. Because the objects are in",
      "content_length": 1035,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "memory, we never need to call a .save() method; we just fetch the\nobject we care about and modify it in memory.\nThe Repository in the Abstract\nThe simplest repository has just two methods: add() to put a new item\nin the repository, and get() to return a previously added item.  We\nstick rigidly to using these methods for data access in our domain and\nour service layer. This self-imposed simplicity stops us from coupling\nour domain model to the database.\nHere’s what an abstract base class (ABC) for our repository would\nlook like:\nThe simplest possible repository (repository.py)\nclass AbstractRepository(abc.ABC):\n    @abc.abstractmethod  \n    def add(self, batch: model.Batch):\n        raise NotImplementedError  \n    @abc.abstractmethod\n    def get(self, reference) -> model.Batch:\n        raise NotImplementedError\nPython tip: @abc.abstractmethod is one of the only things that\nmakes ABCs actually “work” in Python. Python will refuse to let\nyou instantiate a class that does not implement all the\nabstractmethods defined in its parent class.\nraise NotImplementedError is nice, but it’s neither necessary\nnor sufficient. In fact, your abstract methods can have real behavior\nthat subclasses can call out to, if you really want.\n6\n7",
      "content_length": 1238,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "ABSTRACT BASE CLASSES, DUCK TYPING, AND PROTOCOLS\nWe’re using abstract base classes in this book for didactic reasons: we hope they help explain\nwhat the interface of the repository abstraction is.\nIn real life, we’ve sometimes found ourselves deleting ABCs from our production code, because\nPython makes it too easy to ignore them, and they end up unmaintained and, at worst,\nmisleading. In practice we often just rely on Python’s duck typing to enable abstractions. To a\nPythonista, a repository is any object that has add(thing) and get(id) methods.\nAn alternative to look into is PEP 544 protocols. These give you typing without the possibility of\ninheritance, which “prefer composition over inheritance” fans will particularly like.\nWhat Is the Trade-Off?\nYou know they say economists know the price of everything and\nthe value of nothing? Well, programmers know the benefits of\neverything and the trade-offs of nothing.\n—Rich Hickey\nWhenever we introduce an architectural pattern in this book, we’ll\nalways ask, “What do we get for this? And what does it cost us?”\nUsually, at the very least, we’ll be introducing an extra layer of\nabstraction, and although we may hope it will reduce complexity\noverall, it does add complexity locally, and it has a cost in terms of the\nraw numbers of moving parts and ongoing maintenance.\nThe Repository pattern is probably one of the easiest choices in the\nbook, though, if you’re already heading down the DDD and\ndependency inversion route. As far as our code is concerned, we’re\nreally just swapping the SQLAlchemy abstraction\n(session.query(Batch)) for a different one (batches_repo.get)\nthat we designed.",
      "content_length": 1650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "We will have to write a few lines of code in our repository class each\ntime we add a new domain object that we want to retrieve, but in\nreturn we get a simple abstraction over our storage layer, which we\ncontrol. The Repository pattern would make it easy to make\nfundamental changes to the way we store things (see Appendix C), and\nas we’ll see, it is easy to fake out for unit tests.\nIn addition, the Repository pattern is so common in the DDD world\nthat, if you do collaborate with programmers who have come to Python\nfrom the Java and C# worlds, they’re likely to recognize it. Figure 2-5\nillustrates the pattern.\nFigure 2-5. Repository pattern\nAs always, we start with a test. This would probably be classified as\nan integration test, since we’re checking that our code (the repository)\nis correctly integrated with the database; hence, the tests tend to mix\nraw SQL with calls and assertions on our own code.\nTIP\nUnlike the ORM tests from earlier, these tests are good candidates for staying\npart of your codebase longer term, particularly if any parts of your domain model\nmean the object-relational map is nontrivial.",
      "content_length": 1124,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "Repository test for saving an object (test_repository.py)\ndef test_repository_can_save_a_batch(session):\n    batch = model.Batch(\"batch1\", \"RUSTY-SOAPDISH\", 100, eta=None)\n    repo = repository.SqlAlchemyRepository(session)\n    repo.add(batch)  \n    session.commit()  \n    rows = list(session.execute(\n        'SELECT reference, sku, _purchased_quantity, eta FROM \"batches\"'  \n    ))\n    assert rows == [(\"batch1\", \"RUSTY-SOAPDISH\", 100, None)]\nrepo.add() is the method under test here.\nWe keep the .commit() outside of the repository and make it the\nresponsibility of the caller. There are pros and cons for this; some\nof our reasons will become clearer when we get to Chapter 6.\nWe use the raw SQL to verify that the right data has been saved.\nThe next test involves retrieving batches and allocations, so it’s more\ncomplex:\nRepository test for retrieving a complex object (test_repository.py)\ndef insert_order_line(session):\n    session.execute(  \n        'INSERT INTO order_lines (orderid, sku, qty)'\n        ' VALUES (\"order1\", \"GENERIC-SOFA\", 12)'\n    )\n    [[orderline_id]] = session.execute(\n        'SELECT id FROM order_lines WHERE orderid=:orderid AND sku=:sku',\n        dict(orderid=\"order1\", sku=\"GENERIC-SOFA\")\n    )\n    return orderline_id",
      "content_length": 1254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "def insert_batch(session, batch_id):  \n    ...\ndef test_repository_can_retrieve_a_batch_with_allocations(session):\n    orderline_id = insert_order_line(session)\n    batch1_id = insert_batch(session, \"batch1\")\n    insert_batch(session, \"batch2\")\n    insert_allocation(session, orderline_id, batch1_id)  \n    repo = repository.SqlAlchemyRepository(session)\n    retrieved = repo.get(\"batch1\")\n    expected = model.Batch(\"batch1\", \"GENERIC-SOFA\", 100, eta=None)\n    assert retrieved == expected  # Batch.__eq__ only compares reference  \n    assert retrieved.sku == expected.sku  \n    assert retrieved._purchased_quantity == expected._purchased_quantity\n    assert retrieved._allocations == {  \n        model.OrderLine(\"order1\", \"GENERIC-SOFA\", 12),\n    }\nThis tests the read side, so the raw SQL is preparing data to be\nread by the repo.get().\nWe’ll spare you the details of insert_batch and\ninsert_allocation; the point is to create a couple of batches,\nand, for the batch we’re interested in, to have one existing order\nline allocated to it.\nAnd that’s what we verify here. The first assert == checks that\nthe types match, and that the reference is the same (because, as you\nremember, Batch is an entity, and we have a custom eq for it).\nSo we also explicitly check on its major attributes, including\n._allocations, which is a Python set of OrderLine value\nobjects.\nWhether or not you painstakingly write tests for every model is a\njudgment call. Once you have one class tested for create/modify/save,",
      "content_length": 1499,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "you might be happy to go on and do the others with a minimal round-\ntrip test, or even nothing at all, if they all follow a similar pattern. In\nour case, the ORM config that sets up the ._allocations set is a little\ncomplex, so it merited a specific test.\nYou end up with something like this:\nA typical repository (repository.py)\nclass SqlAlchemyRepository(AbstractRepository): \n \n    def __init__(self, session): \n        self.session = session \n \n    def add(self, batch): \n        self.session.add(batch) \n \n    def get(self, reference): \n        return \nself.session.query(model.Batch).filter_by(reference=reference).one() \n \n    def list(self): \n        return self.session.query(model.Batch).all()\nAnd now our Flask endpoint might look something like the following:\nUsing our repository directly in our API endpoint\n@flask.route.gubbins\ndef allocate_endpoint(): \n    batches = SqlAlchemyRepository.list() \n    lines = [ \n        OrderLine(l['orderid'], l['sku'], l['qty']) \n         for l in request.params... \n    ] \n    allocate(lines, batches)",
      "content_length": 1052,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "session.commit() \n    return 201\nEXERCISE FOR THE READER\nWe bumped into a friend at a DDD conference the other day who said, “I haven’t used an ORM in\n10 years.” The Repository pattern and an ORM both act as abstractions in front of raw SQL, so\nusing one behind the other isn’t really necessary. Why not have a go at implementing our\nrepository without using the ORM? You’ll find the code on GitHub.\nWe’ve left the repository tests, but figuring out what SQL to write is up to you. Perhaps it’ll be\nharder than you think; perhaps it’ll be easier. But the nice thing is, the rest of your application just\ndoesn’t care.\nBuilding a Fake Repository for Tests Is\nNow Trivial!\nHere’s one of the biggest benefits of the Repository pattern:\nA simple fake repository using a set (repository.py)\nclass FakeRepository(AbstractRepository): \n \n    def __init__(self, batches): \n        self._batches = set(batches) \n \n    def add(self, batch): \n        self._batches.add(batch) \n \n    def get(self, reference): \n        return next(b for b in self._batches if b.reference == reference) \n \n    def list(self): \n        return list(self._batches)\nBecause it’s a simple wrapper around a set, all the methods are one-\nliners.",
      "content_length": 1208,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "Using a fake repo in tests is really easy, and we have a simple\nabstraction that’s easy to use and reason about:\nExample usage of fake repository (test_api.py)\nfake_repo = FakeRepository([batch1, batch2, batch3])\nYou’ll see this fake in action in the next chapter.\nTIP\nBuilding fakes for your abstractions is an excellent way to get design feedback: if\nit’s hard to fake, the abstraction is probably too complicated.\nWhat Is a Port and What Is an Adapter, in\nPython?\nWe don’t want to dwell on the terminology too much here because the\nmain thing we want to focus on is dependency inversion, and the\nspecifics of the technique you use don’t matter too much. Also, we’re\naware that different people use slightly different definitions.\nPorts and adapters came out of the OO world, and the definition we\nhold onto is that the port is the interface between our application and\nwhatever it is we wish to abstract away, and the adapter is the\nimplementation behind that interface or abstraction.\nNow Python doesn’t have interfaces per se, so although it’s usually\neasy to identify an adapter, defining the port can be harder. If you’re",
      "content_length": 1128,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "using an abstract base class, that’s the port. If not, the port is just the\nduck type that your adapters conform to and that your core application\nexpects—the function and method names in use, and their argument\nnames and types.\nConcretely, in this chapter, AbstractRepository is the port, and\nSqlAlchemyRepository and FakeRepository are the adapters.\nWrap-Up\nBearing the Rich Hickey quote in mind, in each chapter we summarize\nthe costs and benefits of each architectural pattern we introduce. We\nwant to be clear that we’re not saying every single application needs\nto be built this way; only sometimes does the complexity of the app\nand domain make it worth investing the time and effort in adding these\nextra layers of indirection.\nWith that in mind, Table 2-1 shows some of the pros and cons of the\nRepository pattern and our persistence-ignorant model.",
      "content_length": 858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "Table 2-1. Repository pattern and persistence ignorance: the \ntrade-offs\nPros\nCons",
      "content_length": 82,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "Pros\nCons\nWe have a simple interface \nbetween persistent storage \nand our domain model.\nIt’s easy to make a fake \nversion of the repository for \nunit testing, or to \nswap out different storage \nsolutions, because we’ve fully \ndecoupled the model \nfrom infrastructure concerns.\nWriting the domain model \nbefore thinking about \npersistence helps us focus on \nthe business problem at hand. \nIf we ever want to radically \nchange our approach, \nwe can do that in our model, \nwithout needing to worry \nabout foreign keys \nor migrations until later.\nOur database schema is \nreally simple because we \nhave complete control over \nhow we map our objects to \ntables.\nAn ORM already buys you some \ndecoupling. Changing foreign keys \nmight be hard, \nbut it should be pretty easy to \nswap between MySQL and \nPostgres if you \never need to.\nMaintaining ORM mappings by \nhand requires extra work and \nextra code.\nAny extra layer of indirection \nalways increases maintenance \ncosts and \nadds a “WTF factor” for Python \nprogrammers who’ve never seen \nthe Repository pattern \nbefore.",
      "content_length": 1063,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "Figure 2-6 shows the basic thesis: yes, for simple cases, a decoupled\ndomain model is harder work than a simple ORM/ActiveRecord\npattern.\nTIP\nIf your app is just a simple CRUD (create-read-update-delete) wrapper around a\ndatabase, then you don’t need a domain model or a repository.\nBut the more complex the domain, the more an investment in freeing\nyourself from infrastructure concerns will pay off in terms of the ease\nof making changes.\n8",
      "content_length": 442,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "Figure 2-6. Domain model trade-offs as a diagram\nOur example code isn’t complex enough to give more than a hint of\nwhat the right-hand side of the graph looks like, but the hints are there.\nImagine, for example, if we decide one day that we want to change\nallocations to live on the OrderLine instead of on the Batch object: if\nwe were using Django, say, we’d have to define and think through the\ndatabase migration before we could run any tests. As it is, because our\nmodel is just plain old Python objects, we can change a set() to being\na new attribute, without needing to think about the database until later.",
      "content_length": 613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "REPOSITORY PATTERN RECAP\nApply dependency inversion to your ORM\nOur domain model should be free of infrastructure concerns, so your ORM should import\nyour model, and not the other way around.\nThe Repository pattern is a simple abstraction around permanent storage\nThe repository gives you the illusion of a collection of in-memory objects. It makes it easy to\ncreate a FakeRepository for testing and to swap fundamental details of your infrastructure\nwithout disrupting your core application. See Appendix C for an example.\nYou’ll be wondering, how do we instantiate these repositories, fake or\nreal? What will our Flask app actually look like? You’ll find out in the\nnext exciting installment, the Service Layer pattern.\nBut first, a brief digression.\n1  I suppose we mean “no stateful dependencies.” Depending on a helper library is fine;\ndepending on an ORM or a web framework is not.\n2  Mark Seemann has an excellent blog post on the topic.\n3  In this sense, using an ORM is already an example of the DIP. Instead of depending on\nhardcoded SQL, we depend on an abstraction, the ORM. But that’s not enough for us—\nnot in this book!\n4  Even in projects where we don’t use an ORM, we often use SQLAlchemy alongside\nAlembic to declaratively create schemas in Python and to manage migrations,\nconnections, and sessions.\n5  Shout-out to the amazingly helpful SQLAlchemy maintainers, and to Mike Bayer in\nparticular.\n6  You may be thinking, “What about list or delete or update?” However, in an ideal\nworld, we modify our model objects one at a time, and delete is usually handled as a soft-\ndelete—i.e., batch.cancel(). Finally, update is taken care of by the Unit of Work\npattern, as you’ll see in Chapter 6.",
      "content_length": 1707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "7  To really reap the benefits of ABCs (such as they may be), be running helpers like\npylint and mypy.\n8  Diagram inspired by a post called “Global Complexity, Local Simplicity” by Rob Vens.",
      "content_length": 190,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "Chapter 3. A Brief Interlude:\nOn Coupling and\nAbstractions\nAllow us a brief digression on the subject of abstractions, dear reader.\nWe’ve talked about abstractions quite a lot. The Repository pattern is\nan abstraction over permanent storage, for example. But what makes a\ngood abstraction? What do we want from abstractions? And how do\nthey relate to testing?\nTIP\nThe code for this chapter is in the chapter_03_abstractions branch on GitHub:\ngit clone https://github.com/cosmicpython/code.git \ngit checkout chapter_03_abstractions\nA key theme in this book, hidden among the fancy patterns, is that we\ncan use simple abstractions to hide messy details. When we’re writing\ncode for fun, or in a kata,  we get to play with ideas freely, hammering\nthings out and refactoring aggressively. In a large-scale system, though,\nwe become constrained by the decisions made elsewhere in the\nsystem.\n1",
      "content_length": 888,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "When we’re unable to change component A for fear of breaking\ncomponent B, we say that the components have become coupled.\nLocally, coupling is a good thing: it’s a sign that our code is working\ntogether, each component supporting the others, all of them fitting in\nplace like the gears of a watch. In jargon, we say this works when\nthere is high cohesion between the coupled elements.\nGlobally, coupling is a nuisance: it increases the risk and the cost of\nchanging our code, sometimes to the point where we feel unable to\nmake any changes at all. This is the problem with the Ball of Mud\npattern: as the application grows, if we’re unable to prevent coupling\nbetween elements that have no cohesion, that coupling increases\nsuperlinearly until we are no longer able to effectively change our\nsystems.\nWe can reduce the degree of coupling within a system (Figure 3-1) by\nabstracting away the details (Figure 3-2).\nFigure 3-1. Lots of coupling\nFigure 3-2. Less coupling\nIn both diagrams, we have a pair of subsystems, with one dependent on\nthe other. In Figure 3-1, there is a high degree of coupling between the",
      "content_length": 1110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "two; the number of arrows indicates lots of kinds of dependencies\nbetween the two. If we need to change system B, there’s a good chance\nthat the change will ripple through to system A.\nIn Figure 3-2, though, we have reduced the degree of coupling by\ninserting a new, simpler abstraction. Because it is simpler, system A\nhas fewer kinds of dependencies on the abstraction. The abstraction\nserves to protect us from change by hiding away the complex details of\nwhatever system B does—we can change the arrows on the right\nwithout changing the ones on the left.\nAbstracting State Aids Testability\nLet’s see an example. Imagine we want to write code for synchronizing\ntwo file directories, which we’ll call the source and the destination:\nIf a file exists in the source but not in the destination, copy the\nfile over.\nIf a file exists in the source, but it has a different name than in\nthe destination, rename the destination file to match.\nIf a file exists in the destination but not in the source, remove\nit.\nOur first and third requirements are simple enough: we can just\ncompare two lists of paths. Our second is trickier, though. To detect\nrenames, we’ll have to inspect the content of files. For this, we can use\na hashing function like MD5 or SHA-1. The code to generate a SHA-1\nhash from a file is simple enough:\nHashing a file (sync.py)",
      "content_length": 1341,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "BLOCKSIZE = 65536 \n \ndef hash_file(path): \n    hasher = hashlib.sha1() \n    with path.open(\"rb\") as file: \n        buf = file.read(BLOCKSIZE) \n        while buf: \n            hasher.update(buf) \n            buf = file.read(BLOCKSIZE) \n    return hasher.hexdigest()\nNow we need to write the bit that makes decisions about what to do—\nthe business logic, if you will.\nWhen we have to tackle a problem from first principles, we usually try\nto write a simple implementation and then refactor toward better\ndesign. We’ll use this approach throughout the book, because it’s how\nwe write code in the real world: start with a solution to the smallest\npart of the problem, and then iteratively make the solution richer and\nbetter designed.\nOur first hackish approach looks something like this:\nBasic sync algorithm (sync.py)\nimport hashlib\nimport os\nimport shutil\nfrom pathlib import Path \n \ndef sync(source, dest): \n    # Walk the source folder and build a dict of filenames and their hashes \n    source_hashes = {} \n    for folder, _, files in os.walk(source): \n        for fn in files: \n            source_hashes[hash_file(Path(folder) / fn)] = fn",
      "content_length": 1141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "seen = set()  # Keep track of the files we've found in the target \n \n    # Walk the target folder and get the filenames and hashes \n    for folder, _, files in os.walk(dest): \n        for fn in files: \n            dest_path = Path(folder) / fn \n            dest_hash = hash_file(dest_path) \n            seen.add(dest_hash) \n \n            # if there's a file in target that's not in source, delete it \n            if dest_hash not in source_hashes: \n                dest_path.remove() \n \n            # if there's a file in target that has a different path in source, \n            # move it to the correct path \n            elif dest_hash in source_hashes and fn != source_hashes[dest_hash]: \n                shutil.move(dest_path, Path(folder) / source_hashes[dest_hash]) \n \n    # for every file that appears in source but not target, copy the file to \n    # the target \n    for src_hash, fn in source_hashes.items(): \n        if src_hash not in seen: \n            shutil.copy(Path(source) / fn, Path(dest) / fn)\nFantastic! We have some code and it looks OK, but before we run it on\nour hard drive, maybe we should test it. How do we go about testing\nthis sort of thing?\nSome end-to-end tests (test_sync.py)\ndef test_when_a_file_exists_in_the_source_but_not_the_destination(): \n    try: \n        source = tempfile.mkdtemp() \n        dest = tempfile.mkdtemp() \n \n        content = \"I am a very useful file\" \n        (Path(source) / 'my-file').write_text(content) \n \n        sync(source, dest)",
      "content_length": 1490,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "expected_path = Path(dest) /  'my-file' \n        assert expected_path.exists() \n        assert expected_path.read_text() == content \n \n    finally: \n        shutil.rmtree(source) \n        shutil.rmtree(dest) \n \n \ndef test_when_a_file_has_been_renamed_in_the_source(): \n    try: \n        source = tempfile.mkdtemp() \n        dest = tempfile.mkdtemp() \n \n        content = \"I am a file that was renamed\" \n        source_path = Path(source) / 'source-filename' \n        old_dest_path = Path(dest) / 'dest-filename' \n        expected_dest_path = Path(dest) / 'source-filename' \n        source_path.write_text(content) \n        old_dest_path.write_text(content) \n \n        sync(source, dest) \n \n        assert old_dest_path.exists() is False \n        assert expected_dest_path.read_text() == content \n \n \n    finally: \n        shutil.rmtree(source) \n        shutil.rmtree(dest)\nWowsers, that’s a lot of setup for two simple cases! The problem is\nthat our domain logic, “figure out the difference between two\ndirectories,” is tightly coupled to the I/O code. We can’t run our\ndifference algorithm without calling the pathlib, shutil, and\nhashlib modules.",
      "content_length": 1148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "And the trouble is, even with our current requirements, we haven’t\nwritten enough tests: the current implementation has several bugs (the\nshutil.move() is wrong, for example). Getting decent coverage and\nrevealing these bugs means writing more tests, but if they’re all as\nunwieldy as the preceding ones, that’s going to get real painful real\nquickly.\nOn top of that, our code isn’t very extensible. Imagine trying to\nimplement a --dry-run flag that gets our code to just print out what\nit’s going to do, rather than actually do it. Or what if we wanted to\nsync to a remote server, or to cloud storage?\nOur high-level code is coupled to low-level details, and it’s making\nlife hard. As the scenarios we consider get more complex, our tests\nwill get more unwieldy. We can definitely refactor these tests (some of\nthe cleanup could go into pytest fixtures, for example) but as long as\nwe’re doing filesystem operations, they’re going to stay slow and be\nhard to read and write.\nChoosing the Right Abstraction(s)\nWhat could we do to rewrite our code to make it more testable?\nFirst, we need to think about what our code needs from the filesystem.\nReading through the code, we can see that three distinct things are\nhappening. We can think of these as three distinct responsibilities that\nthe code has:",
      "content_length": 1298,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "1. We interrogate the filesystem by using os.walk and determine\nhashes for a series of paths. This is similar in both the source\nand the destination cases.\n2. We decide whether a file is new, renamed, or redundant.\n3. We copy, move, or delete files to match the source.\nRemember that we want to find simplifying abstractions for each of\nthese responsibilities. That will let us hide the messy details so we can\nfocus on the interesting logic.\nNOTE\nIn this chapter, we’re refactoring some gnarly code into a more testable structure\nby identifying the separate tasks that need to be done and giving each task to a\nclearly defined actor, along similar lines to the duckduckgo example.\nFor steps 1 and 2, we’ve already intuitively started using an\nabstraction, a dictionary of hashes to paths. You may already have\nbeen thinking, “Why not build up a dictionary for the destination folder\nas well as the source, and then we just compare two dicts?” That\nseems like a nice way to abstract the current state of the filesystem:\nsource_files = {'hash1': 'path1', 'hash2': 'path2'} \ndest_files = {'hash1': 'path1', 'hash2': 'pathX'}\nWhat about moving from step 2 to step 3? How can we abstract out the\nactual move/copy/delete filesystem interaction?\nWe’ll apply a trick here that we’ll employ on a grand scale later in the\nbook. We’re going to separate what we want to do from how to do it.\n2",
      "content_length": 1382,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "We’re going to make our program output a list of commands that look\nlike this:\n(\"COPY\", \"sourcepath\", \"destpath\"), \n(\"MOVE\", \"old\", \"new\"),\nNow we could write tests that just use two filesystem dicts as inputs,\nand we would expect lists of tuples of strings representing actions as\noutputs.\nInstead of saying, “Given this actual filesystem, when I run my\nfunction, check what actions have happened,” we say, “Given this\nabstraction of a filesystem, what abstraction of filesystem actions\nwill happen?”\nSimplified inputs and outputs in our tests (test_sync.py)\n    def test_when_a_file_exists_in_the_source_but_not_the_destination(): \n        src_hashes = {'hash1': 'fn1'} \n        dst_hashes = {} \n        expected_actions = [('COPY', '/src/fn1', '/dst/fn1')] \n        ... \n \n    def test_when_a_file_has_been_renamed_in_the_source(): \n        src_hashes = {'hash1': 'fn1'} \n        dst_hashes = {'hash1': 'fn2'} \n        expected_actions == [('MOVE', '/dst/fn2', '/dst/fn1')] \n        ...\nImplementing Our Chosen Abstractions\nThat’s all very well, but how do we actually write those new tests,\nand how do we change our implementation to make it all work?",
      "content_length": 1155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "Our goal is to isolate the clever part of our system, and to be able to\ntest it thoroughly without needing to set up a real filesystem. We’ll\ncreate a “core” of code that has no dependencies on external state and\nthen see how it responds when we give it input from the outside world\n(this kind of approach was characterized by Gary Bernhardt as\nFunctional Core, Imperative Shell, or FCIS).\nLet’s start off by splitting the code to separate the stateful parts from\nthe logic.\nAnd our top-level function will contain almost no logic at all; it’s just\nan imperative series of steps: gather inputs, call our logic, apply\noutputs:\nSplit our code into three (sync.py)\ndef sync(source, dest):\n    # imperative shell step 1, gather inputs\n    source_hashes = read_paths_and_hashes(source)  \n    dest_hashes = read_paths_and_hashes(dest)  \n    # step 2: call functional core\n    actions = determine_actions(source_hashes, dest_hashes, source, dest)  \n    # imperative shell step 3, apply outputs\n    for action, *paths in actions:\n        if action == 'copy':\n            shutil.copyfile(*paths)\n        if action == 'move':\n            shutil.move(*paths)\n        if action == 'delete':\n            os.remove(paths[0])",
      "content_length": 1210,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "Here’s the first function we factor out,\nread_paths_and_hashes(), which isolates the I/O part of our\napplication.\nHere is where carve out the functional core, the business logic.\nThe code to build up the dictionary of paths and hashes is now trivially\neasy to write:\nA function that just does I/O (sync.py)\ndef read_paths_and_hashes(root): \n    hashes = {} \n    for folder, _, files in os.walk(root): \n        for fn in files: \n            hashes[hash_file(Path(folder) / fn)] = fn \n    return hashes\nThe determine_actions() function will be the core of our business\nlogic, which says, “Given these two sets of hashes and filenames, what\nshould we copy/move/delete?”. It takes simple data structures and\nreturns simple data structures:\nA function that just does business logic (sync.py)\ndef determine_actions(src_hashes, dst_hashes, src_folder, dst_folder): \n    for sha, filename in src_hashes.items(): \n        if sha not in dst_hashes: \n            sourcepath = Path(src_folder) / filename \n            destpath = Path(dst_folder) / filename \n            yield 'copy', sourcepath, destpath \n \n        elif dst_hashes[sha] != filename: \n            olddestpath = Path(dst_folder) / dst_hashes[sha] \n            newdestpath = Path(dst_folder) / filename \n            yield 'move', olddestpath, newdestpath",
      "content_length": 1306,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "for sha, filename in dst_hashes.items(): \n        if sha not in src_hashes: \n            yield 'delete', dst_folder / filename\nOur tests now act directly on the determine_actions() function:\nNicer-looking tests (test_sync.py)\ndef test_when_a_file_exists_in_the_source_but_not_the_destination(): \n    src_hashes = {'hash1': 'fn1'} \n    dst_hashes = {} \n    actions = determine_actions(src_hashes, dst_hashes, Path('/src'), \nPath('/dst')) \n    assert list(actions) == [('copy', Path('/src/fn1'), Path('/dst/fn1'))]\n... \n \ndef test_when_a_file_has_been_renamed_in_the_source(): \n    src_hashes = {'hash1': 'fn1'} \n    dst_hashes = {'hash1': 'fn2'} \n    actions = determine_actions(src_hashes, dst_hashes, Path('/src'), \nPath('/dst')) \n    assert list(actions) == [('move', Path('/dst/fn2'), Path('/dst/fn1'))]\nBecause we’ve disentangled the logic of our program—the code for\nidentifying changes—from the low-level details of I/O, we can easily\ntest the core of our code.\nWith this approach, we’ve switched from testing our main entrypoint\nfunction, sync(), to testing a lower-level function,\ndetermine_actions(). You might decide that’s fine because sync()\nis now so simple. Or you might decide to keep some\nintegration/acceptance tests to test that sync(). But there’s another\noption, which is to modify the sync() function so it can be unit tested",
      "content_length": 1346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "and end-to-end tested; it’s an approach Bob calls edge-to-edge\ntesting.\nTesting Edge to Edge with Fakes and Dependency\nInjection\nWhen we start writing a new system, we often focus on the core logic\nfirst, driving it with direct unit tests. At some point, though, we want to\ntest bigger chunks of the system together.\nWe could return to our end-to-end tests, but those are still as tricky to\nwrite and maintain as before. Instead, we often write tests that invoke\na whole system together but fake the I/O, sort of edge to edge:\nExplicit dependencies (sync.py)\ndef sync(reader, filesystem, source_root, dest_root): \n    source_hashes = reader(source_root) \n    dest_hashes = reader(dest_root)\n    for sha, filename in src_hashes.items():\n        if sha not in dest_hashes:\n            sourcepath = source_root / filename\n            destpath = dest_root / filename\n            filesystem.copy(destpath, sourcepath) \n        elif dest_hashes[sha] != filename:\n            olddestpath = dest_root / dest_hashes[sha]\n            newdestpath = dest_root / filename\n            filesystem.move(olddestpath, newdestpath)\n    for sha, filename in dst_hashes.items():\n        if sha not in source_hashes:\n            filesystem.delete(dest_root/filename)",
      "content_length": 1244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "Our top-level function now exposes two new dependencies, a\nreader and a filesystem.\nWe invoke the reader to produce our files dict.\nWe invoke the filesystem to apply the changes we detect.\nTIP\nAlthough we’re using dependency injection, there is no need to define an abstract\nbase class or any kind of explicit interface. In this book, we often show ABCs\nbecause we hope they help you understand what the abstraction is, but they’re not\nnecessary. Python’s dynamic nature means we can always rely on duck typing.\nTests using DI\nclass FakeFileSystem(list): \n    def copy(self, src, dest): \n        self.append(('COPY', src, dest))\n    def move(self, src, dest):\n        self.append(('MOVE', src, dest))\n    def delete(self, dest):\n        self.append(('DELETE', src, dest))\ndef test_when_a_file_exists_in_the_source_but_not_the_destination():\n    source = {\"sha1\": \"my-file\" }\n    dest = {}\n    filesystem = FakeFileSystem()\n    reader = {\"/source\": source, \"/dest\": dest}\n    synchronise_dirs(reader.pop, filesystem, \"/source\", \"/dest\")",
      "content_length": 1035,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "assert filesystem == [(\"COPY\", \"/source/my-file\", \"/dest/my-file\")]\ndef test_when_a_file_has_been_renamed_in_the_source():\n    source = {\"sha1\": \"renamed-file\" }\n    dest = {\"sha1\": \"original-file\" }\n    filesystem = FakeFileSystem()\n    reader = {\"/source\": source, \"/dest\": dest}\n    synchronise_dirs(reader.pop, filesystem, \"/source\", \"/dest\")\n    assert filesystem == [(\"MOVE\", \"/dest/original-file\", \"/dest/renamed-file\")]\nBob loves using lists to build simple test doubles, even though his\ncoworkers get mad. It means we can write tests like assert foo\nnot in database.\nEach method in our FakeFileSystem just appends something to\nthe list so we can inspect it later. This is an example of a spy\nobject.\nThe advantage of this approach is that our tests act on the exact same\nfunction that’s used by our production code. The disadvantage is that\nwe have to make our stateful components explicit and pass them\naround. David Heinemeier Hansson, the creator of Ruby on Rails,\nfamously described this as “test-induced design damage.”\nIn either case, we can now work on fixing all the bugs in our\nimplementation; enumerating tests for all the edge cases is now much\neasier.\nWhy Not Just Patch It Out?\nAt this point you may be scratching your head and thinking, “Why don’t\nyou just use mock.patch and save yourself the effort?\"”",
      "content_length": 1326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "We avoid using mocks in this book and in our production code too.\nWe’re not going to enter into a Holy War, but our instinct is that\nmocking frameworks, particularly monkeypatching, are a code smell.\nInstead, we like to clearly identify the responsibilities in our\ncodebase, and to separate those responsibilities into small, focused\nobjects that are easy to replace with a test double.\nNOTE\nYou can see an example in Chapter 8, where we mock.patch() out an email-\nsending module, but eventually we replace that with an explicit bit of dependency\ninjection in Chapter 13.\nWe have three closely related reasons for our preference:\nPatching out the dependency you’re using makes it possible to\nunit test the code, but it does nothing to improve the design.\nUsing mock.patch won’t let your code work with a --dry-\nrun flag, nor will it help you run against an FTP server. For\nthat, you’ll need to introduce abstractions.\nTests that use mocks tend to be more coupled to the\nimplementation details of the codebase. That’s because mock\ntests verify the interactions between things: did we call\nshutil.copy with the right arguments? This coupling\nbetween code and test tends to make tests more brittle, in our\nexperience.\nOveruse of mocks leads to complicated test suites that fail to\nexplain the code.",
      "content_length": 1295,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "NOTE\nDesigning for testability really means designing for extensibility. We trade off a\nlittle more complexity for a cleaner design that admits novel use cases.\nMOCKS VERSUS FAKES; CLASSIC-STYLE VERSUS LONDON-\nSCHOOL TDD\nHere’s a short and somewhat simplistic definition of the difference between mocks and fakes:\nMocks are used to verify how something gets used; they have methods like\nassert_called_once_with(). They’re associated with London-school TDD.\nFakes are working implementations of the thing they’re replacing, but they’re designed\nfor use only in tests. They wouldn’t work “in real life”; our in-memory repository is a\ngood example. But you can use them to make assertions about the end state of a\nsystem rather than the behaviors along the way, so they’re associated with classic-style\nTDD.\nWe’re slightly conflating mocks with spies and fakes with stubs here, and you can read the long,\ncorrect answer in Martin Fowler’s classic essay on the subject called “Mocks Aren’t Stubs”.\nIt also probably doesn’t help that the MagicMock objects provided by unittest.mock aren’t, strictly\nspeaking, mocks; they’re spies, if anything. But they’re also often used as stubs or dummies.\nThere, we promise we’re done with the test double terminology nitpicks now.\nWhat about London-school versus classic-style TDD? You can read more about those two in\nMartin Fowler’s article that we just cited, as well as on the Software Engineering Stack Exchange\nsite, but in this book we’re pretty firmly in the classicist camp. We like to build our tests around\nstate both in setup and in assertions, and we like to work at the highest level of abstraction\npossible rather than doing checks on the behavior of intermediary collaborators.\nRead more on this in “On Deciding What Kind of Tests to Write”.\nWe view TDD as a design practice first and a testing practice second.\nThe tests act as a record of our design choices and serve to explain the\nsystem to us when we return to the code after a long absence.\nTests that use too many mocks get overwhelmed with setup code that\nhides the story we care about.\n3",
      "content_length": 2095,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "Steve Freeman has a great example of overmocked tests in his talk\n“Test-Driven Development”. You should also check out this PyCon\ntalk, “Mocking and Patching Pitfalls”, by our esteemed tech reviewer,\nEd Jung, which also addresses mocking and its alternatives. And while\nwe’re recommending talks, don’t miss Brandon Rhodes talking about\n“Hoisting Your I/O”, which really nicely covers the issues we’re\ntalking about, using another simple example.\nTIP\nIn this chapter, we’ve spent a lot of time replacing end-to-end tests with unit tests.\nThat doesn’t mean we think you should never use E2E tests! In this book we’re\nshowing techniques to get you to a decent test pyramid with as many unit tests as\npossible, and with the minimum number of E2E tests you need to feel confident.\nRead on to “Recap: Rules of Thumb for Different Types of Test” for more\ndetails.\nSO WHICH DO WE USE IN THIS BOOK? FUNCTIONAL OR\nOBJECT-ORIENTED COMPOSITION?\nBoth. Our domain model is entirely free of dependencies and side effects, so that’s our functional\ncore. The service layer that we build around it (in Chapter 4) allows us to drive the system edge to\nedge, and we use dependency injection to provide those services with stateful components, so\nwe can still unit test them.\nSee Chapter 13 for more exploration of making our dependency injection more explicit and\ncentralized.\nWrap-Up\nWe’ll see this idea come up again and again in the book: we can make\nour systems easier to test and maintain by simplifying the interface",
      "content_length": 1502,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "between our business logic and messy I/O. Finding the right\nabstraction is tricky, but here are a few heuristics and questions to ask\nyourself:\nCan I choose a familiar Python data structure to represent the\nstate of the messy system and then try to imagine a single\nfunction that can return that state?\nWhere can I draw a line between my systems, where can I\ncarve out a seam to stick that abstraction in?\nWhat is a sensible way of dividing things into components\nwith different responsibilities? What implicit concepts can I\nmake explicit?\nWhat are the dependencies, and what is the core business\nlogic?\nPractice makes less imperfect! And now back to our regular\nprogramming…\n1  A code kata is a small, contained programming challenge often used to practice TDD.\nSee “Kata—The Only Way to Learn TDD” by Peter Provost.\n2  If you’re used to thinking in terms of interfaces, that’s what we’re trying to define here.\n3  Which is not to say that we think the London school people are wrong. Some insanely\nsmart people work that way. It’s just not what we’re used to.",
      "content_length": 1062,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "Chapter 4. Our First Use\nCase: Flask API and Service\nLayer\nBack to our allocations project! Figure 4-1 shows the point we\nreached at the end of Chapter 2, which covered the Repository pattern.",
      "content_length": 192,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "Figure 4-1. Before: we drive our app by talking to repositories and\nthe domain model\nIn this chapter, we discuss the differences between orchestration logic,\nbusiness logic, and interfacing code, and we introduce the Service\nLayer pattern to take care of orchestrating our workflows and defining\nthe use cases of our system.\nWe’ll also discuss testing: by combining the Service Layer with our\nrepository abstraction over the database, we’re able to write fast tests,\nnot just of our domain model but of the entire workflow for a use case.\nFigure 4-2 shows what we’re aiming for: we’re going to add a Flask\nAPI that will talk to the service layer, which will serve as the\nentrypoint to our domain model. Because our service layer depends on\nthe AbstractRepository, we can unit test it by using\nFakeRepository but run our production code using\nSqlAlchemyRepository.",
      "content_length": 863,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "Figure 4-2. The service layer will become the main way into our app\nIn our diagrams, we are using the convention that new components are\nhighlighted with bold text/lines (and yellow/orange color, if you’re\nreading a digital version).\nTIP\nThe code for this chapter is in the chapter_04_service_layer branch on GitHub:\ngit clone https://github.com/cosmicpython/code.git \ncd code \ngit checkout chapter_04_service_layer \n# or to code along, checkout Chapter 2: \ngit checkout chapter_02_repository\nConnecting Our Application to the Real\nWorld\nLike any good agile team, we’re hustling to try to get an MVP out and\nin front of the users to start gathering feedback. We have the core of\nour domain model and the domain service we need to allocate orders,\nand we have the repository interface for permanent storage.\nLet’s plug all the moving parts together as quickly as we can and then\nrefactor toward a cleaner architecture. Here’s our plan:\n1. Use Flask to put an API endpoint in front of our allocate\ndomain service. Wire up the database session and our\nrepository. Test it with an end-to-end test and some quick-\nand-dirty SQL to prepare test data.",
      "content_length": 1144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "2. Refactor out a service layer that can serve as an abstraction to\ncapture the use case and that will sit between Flask and our\ndomain model. Build some service-layer tests and show how\nthey can use FakeRepository.\n3. Experiment with different types of parameters for our service\nlayer functions; show that using primitive data types allows\nthe service layer’s clients (our tests and our Flask API) to be\ndecoupled from the model layer.\nA First End-to-End Test\nNo one is interested in getting into a long terminology debate about\nwhat counts as an end-to-end (E2E) test versus a functional test versus\nan acceptance test versus an integration test versus a unit test. Different\nprojects need different combinations of tests, and we’ve seen perfectly\nsuccessful projects just split things into “fast tests” and “slow tests.”\nFor now, we want to write one or maybe two tests that are going to\nexercise a “real” API endpoint (using HTTP) and talk to a real\ndatabase. Let’s call them end-to-end tests because it’s one of the most\nself-explanatory names.\nThe following shows a first cut:\nA first API test (test_api.py)\n@pytest.mark.usefixtures('restart_api')\ndef test_api_returns_allocation(add_stock):\n    sku, othersku = random_sku(), random_sku('other')  \n    earlybatch = random_batchref(1)\n    laterbatch = random_batchref(2)\n    otherbatch = random_batchref(3)",
      "content_length": 1362,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "add_stock([  \n        (laterbatch, sku, 100, '2011-01-02'),\n        (earlybatch, sku, 100, '2011-01-01'),\n        (otherbatch, othersku, 100, None),\n    ])\n    data = {'orderid': random_orderid(), 'sku': sku, 'qty': 3}\n    url = config.get_api_url()  \n    r = requests.post(f'{url}/allocate', json=data)\n    assert r.status_code == 201\n    assert r.json()['batchref'] == earlybatch\nrandom_sku(), random_batchref(), and so on are little helper\nfunctions that generate randomized characters by using the uuid\nmodule. Because we’re running against an actual database now,\nthis is one way to prevent various tests and runs from interfering\nwith each other.\nadd_stock is a helper fixture that just hides away the details of\nmanually inserting rows into the database using SQL. We’ll show a\nnicer way of doing this later in the chapter.\nconfig.py is a module in which we keep configuration information.\nEveryone solves these problems in different ways, but you’re going to\nneed some way of spinning up Flask, possibly in a container, and of\ntalking to a Postgres database. If you want to see how we did it, check\nout Appendix B.\nThe Straightforward Implementation\nImplementing things in the most obvious way, you might get something\nlike this:\nFirst cut of Flask app (flask_app.py)",
      "content_length": 1275,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "from flask import Flask, jsonify, request\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker \n \nimport config\nimport model\nimport orm\nimport repository \n \n \norm.start_mappers()\nget_session = sessionmaker(bind=create_engine(config.get_postgres_uri()))\napp = Flask(__name__) \n \n@app.route(\"/allocate\", methods=['POST'])\ndef allocate_endpoint(): \n    session = get_session() \n    batches = repository.SqlAlchemyRepository(session).list() \n    line = model.OrderLine( \n        request.json['orderid'], \n        request.json['sku'], \n        request.json['qty'], \n    ) \n \n    batchref = model.allocate(line, batches) \n \n    return jsonify({'batchref': batchref}), 201\nSo far, so good. No need for too much more of your “architecture\nastronaut” nonsense, Bob and Harry, you may be thinking.\nBut hang on a minute—there’s no commit. We’re not actually saving\nour allocation to the database. Now we need a second test, either one\nthat will inspect the database state after (not very black-boxy), or\nmaybe one that checks that we can’t allocate a second line if a first\nshould have already depleted the batch:\nTest allocations are persisted (test_api.py)",
      "content_length": 1171,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "@pytest.mark.usefixtures('restart_api')\ndef test_allocations_are_persisted(add_stock): \n    sku = random_sku() \n    batch1, batch2 = random_batchref(1), random_batchref(2) \n    order1, order2 = random_orderid(1), random_orderid(2) \n    add_stock([ \n        (batch1, sku, 10, '2011-01-01'), \n        (batch2, sku, 10, '2011-01-02'), \n    ]) \n    line1 = {'orderid': order1, 'sku': sku, 'qty': 10} \n    line2 = {'orderid': order2, 'sku': sku, 'qty': 10} \n    url = config.get_api_url() \n \n    # first order uses up all stock in batch 1 \n    r = requests.post(f'{url}/allocate', json=line1) \n    assert r.status_code == 201 \n    assert r.json()['batchref'] == batch1 \n \n    # second order should go to batch 2 \n    r = requests.post(f'{url}/allocate', json=line2) \n    assert r.status_code == 201 \n    assert r.json()['batchref'] == batch2\nNot quite so lovely, but that will force us to add the commit.\nError Conditions That Require Database\nChecks\nIf we keep going like this, though, things are going to get uglier and\nuglier.\nSuppose we want to add a bit of error handling. What if the domain\nraises an error, for a SKU that’s out of stock? Or what about a SKU\nthat doesn’t even exist? That’s not something the domain even knows\nabout, nor should it. It’s more of a sanity check that we should",
      "content_length": 1292,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "implement at the database layer, before we even invoke the domain\nservice.\nNow we’re looking at two more end-to-end tests:\nYet more tests at the E2E layer (test_api.py)\n@pytest.mark.usefixtures('restart_api')\ndef test_400_message_for_out_of_stock(add_stock):  \n    sku, smalL_batch, large_order = random_sku(), random_batchref(), \nrandom_orderid()\n    add_stock([\n        (smalL_batch, sku, 10, '2011-01-01'),\n    ])\n    data = {'orderid': large_order, 'sku': sku, 'qty': 20}\n    url = config.get_api_url()\n    r = requests.post(f'{url}/allocate', json=data)\n    assert r.status_code == 400\n    assert r.json()['message'] == f'Out of stock for sku {sku}'\n@pytest.mark.usefixtures('restart_api')\ndef test_400_message_for_invalid_sku():  \n    unknown_sku, orderid = random_sku(), random_orderid()\n    data = {'orderid': orderid, 'sku': unknown_sku, 'qty': 20}\n    url = config.get_api_url()\n    r = requests.post(f'{url}/allocate', json=data)\n    assert r.status_code == 400\n    assert r.json()['message'] == f'Invalid sku {unknown_sku}'\nIn the first test, we’re trying to allocate more units than we have in\nstock.\nIn the second, the SKU just doesn’t exist (because we never called\nadd_stock), so it’s invalid as far as our app is concerned.\nAnd sure, we could implement it in the Flask app too:",
      "content_length": 1294,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "Flask app starting to get crufty (flask_app.py)\ndef is_valid_sku(sku, batches): \n    return sku in {b.sku for b in batches} \n \n@app.route(\"/allocate\", methods=['POST'])\ndef allocate_endpoint(): \n    session = get_session() \n    batches = repository.SqlAlchemyRepository(session).list() \n    line = model.OrderLine( \n        request.json['orderid'], \n        request.json['sku'], \n        request.json['qty'], \n    ) \n \n    if not is_valid_sku(line.sku, batches): \n        return jsonify({'message': f'Invalid sku {line.sku}'}), 400 \n \n    try: \n        batchref = model.allocate(line, batches) \n    except model.OutOfStock as e: \n        return jsonify({'message': str(e)}), 400 \n \n    session.commit() \n    return jsonify({'batchref': batchref}), 201\nBut our Flask app is starting to look a bit unwieldy. And our number of\nE2E tests is starting to get out of control, and soon we’ll end up with\nan inverted test pyramid (or “ice-cream cone model,” as Bob likes to\ncall it).\nIntroducing a Service Layer, and Using\nFakeRepository to Unit Test It\nIf we look at what our Flask app is doing, there’s quite a lot of what\nwe might call orchestration—fetching stuff out of our repository,",
      "content_length": 1181,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "validating our input against database state, handling errors, and\ncommitting in the happy path. Most of these things don’t have anything\nto do with having a web API endpoint (you’d need them if you were\nbuilding a CLI, for example; see Appendix C), and they’re not really\nthings that need to be tested by end-to-end tests.\nIt often makes sense to split out a service layer, sometimes called an\norchestration layer or a use-case layer.\nDo you remember the FakeRepository that we prepared in\nChapter 3?\nOur fake repository, an in-memory collection of batches\n(test_services.py)\nclass FakeRepository(repository.AbstractRepository): \n \n    def __init__(self, batches): \n        self._batches = set(batches) \n \n    def add(self, batch): \n        self._batches.add(batch) \n \n    def get(self, reference): \n        return next(b for b in self._batches if b.reference == reference) \n \n    def list(self): \n        return list(self._batches)\nHere’s where it will come in useful; it lets us test our service layer\nwith nice, fast unit tests:\nUnit testing with fakes at the service layer (test_services.py)",
      "content_length": 1095,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "def test_returns_allocation():\n    line = model.OrderLine(\"o1\", \"COMPLICATED-LAMP\", 10)\n    batch = model.Batch(\"b1\", \"COMPLICATED-LAMP\", 100, eta=None)\n    repo = FakeRepository([batch])  \n    result = services.allocate(line, repo, FakeSession())  \n    assert result == \"b1\"\ndef test_error_for_invalid_sku():\n    line = model.OrderLine(\"o1\", \"NONEXISTENTSKU\", 10)\n    batch = model.Batch(\"b1\", \"AREALSKU\", 100, eta=None)\n    repo = FakeRepository([batch])  \n    with pytest.raises(services.InvalidSku, match=\"Invalid sku NONEXISTENTSKU\"):\n        services.allocate(line, repo, FakeSession())  \nFakeRepository holds the Batch objects that will be used by our\ntest.\nOur services module (services.py) will define an allocate()\nservice-layer function. It will sit between our\nallocate_endpoint() function in the API layer and the\nallocate() domain service function from our domain model.\nWe also need a FakeSession to fake out the database session, as\nshown in the following code snippet.\nA fake database session (test_services.py)\nclass FakeSession(): \n    committed = False \n \n    def commit(self): \n        self.committed = True\nThis fake session is only a temporary solution. We’ll get rid of it and\nmake things even nicer soon, in Chapter 6. But in the meantime the fake\n1",
      "content_length": 1274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": ".commit() lets us migrate a third test from the E2E layer:\nA second test at the service layer (test_services.py)\ndef test_commits(): \n    line = model.OrderLine('o1', 'OMINOUS-MIRROR', 10) \n    batch = model.Batch('b1', 'OMINOUS-MIRROR', 100, eta=None) \n    repo = FakeRepository([batch]) \n    session = FakeSession() \n \n    services.allocate(line, repo, session) \n    assert session.committed is True\nA Typical Service Function\nWe’ll write a service function that looks something like this:\nBasic allocation service (services.py)\nclass InvalidSku(Exception):\n    pass\ndef is_valid_sku(sku, batches):\n    return sku in {b.sku for b in batches}\ndef allocate(line: OrderLine, repo: AbstractRepository, session) -> str:\n    batches = repo.list()  \n    if not is_valid_sku(line.sku, batches):  \n        raise InvalidSku(f'Invalid sku {line.sku}')\n    batchref = model.allocate(line, batches)  \n    session.commit()  \n    return batchref\nTypical service-layer functions have similar steps:\nWe fetch some objects from the repository.",
      "content_length": 1027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "We make some checks or assertions about the request against the\ncurrent state of the world.\nWe call a domain service.\nIf all is well, we save/update any state we’ve changed.\nThat last step is a little unsatisfactory at the moment, as our service\nlayer is tightly coupled to our database layer. We’ll improve that in\nChapter 6 with the Unit of Work pattern.\nDEPEND ON ABSTRACTIONS\nNotice one more thing about our service-layer function:\ndef allocate(line: OrderLine, repo: AbstractRepository, session) -> str:\nIt depends on a repository. We’ve chosen to make the dependency explicit, and we’ve used the\ntype hint to say that we depend on AbstractRepository. This means it’ll work both when the tests\ngive it a FakeRepository and when the Flask app gives it a SqlAlchemyRepository.\nIf you remember “The Dependency Inversion Principle”, this is what we mean when we say we\nshould “depend on abstractions.” Our high-level module, the service layer, depends on the\nrepository abstraction. And the details of the implementation for our specific choice of persistent\nstorage also depend on that same abstraction. See Figures 4-3 and 4-4.\nSee also in Appendix C a worked example of swapping out the details of which persistent\nstorage system to use while leaving the abstractions intact.\nBut the essentials of the service layer are there, and our Flask app now\nlooks a lot cleaner:\nFlask app delegating to service layer (flask_app.py)\n@app.route(\"/allocate\", methods=['POST'])\ndef allocate_endpoint():\n    session = get_session()  \n    repo = repository.SqlAlchemyRepository(session)",
      "content_length": 1575,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "line = model.OrderLine(\n        request.json['orderid'],  \n        request.json['sku'],  \n        request.json['qty'],  \n    )\n    try:\n        batchref = services.allocate(line, repo, session)  \n    except (model.OutOfStock, services.InvalidSku) as e:\n        return jsonify({'message': str(e)}), 400  \n    return jsonify({'batchref': batchref}), 201  \nWe instantiate a database session and some repository objects.\nWe extract the user’s commands from the web request and pass\nthem to a domain service.\nWe return some JSON responses with the appropriate status codes.\nThe responsibilities of the Flask app are just standard web stuff: per-\nrequest session management, parsing information out of POST\nparameters, response status codes, and JSON. All the orchestration\nlogic is in the use case/service layer, and the domain logic stays in the\ndomain.\nFinally, we can confidently strip down our E2E tests to just two, one\nfor the happy path and one for the unhappy path:\nE2E tests only happy and unhappy paths (test_api.py)\n@pytest.mark.usefixtures('restart_api')\ndef test_happy_path_returns_201_and_allocated_batch(add_stock): \n    sku, othersku = random_sku(), random_sku('other') \n    earlybatch = random_batchref(1) \n    laterbatch = random_batchref(2) \n    otherbatch = random_batchref(3)",
      "content_length": 1291,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "add_stock([ \n        (laterbatch, sku, 100, '2011-01-02'), \n        (earlybatch, sku, 100, '2011-01-01'), \n        (otherbatch, othersku, 100, None), \n    ]) \n    data = {'orderid': random_orderid(), 'sku': sku, 'qty': 3} \n    url = config.get_api_url() \n    r = requests.post(f'{url}/allocate', json=data) \n    assert r.status_code == 201 \n    assert r.json()['batchref'] == earlybatch \n \n \n@pytest.mark.usefixtures('restart_api')\ndef test_unhappy_path_returns_400_and_error_message(): \n    unknown_sku, orderid = random_sku(), random_orderid() \n    data = {'orderid': orderid, 'sku': unknown_sku, 'qty': 20} \n    url = config.get_api_url() \n    r = requests.post(f'{url}/allocate', json=data) \n    assert r.status_code == 400 \n    assert r.json()['message'] == f'Invalid sku {unknown_sku}'\nWe’ve successfully split our tests into two broad categories: tests\nabout web stuff, which we implement end to end; and tests about\norchestration stuff, which we can test against the service layer in\nmemory.",
      "content_length": 999,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "EXERCISE FOR THE READER\nNow that we have an allocate service, why not build out a service for deallocate? We’ve added an\nE2E test and a few stub service-layer tests for you to get started on GitHub.\nIf that’s not enough, continue into the E2E tests and flask_app.py, and refactor the Flask adapter\nto be more RESTful. Notice how doing so doesn’t require any change to our service layer or\ndomain layer!\nTIP\nIf you decide you want to build a read-only endpoint for retrieving allocation info,\njust do “the simplest thing that can possibly work,” which is repo.get() right in\nthe Flask handler. We’ll talk more about reads versus writes in Chapter 12.\nWhy Is Everything Called a Service?\nSome of you are probably scratching your heads at this point trying to\nfigure out exactly what the difference is between a domain service and\na service layer.\nWe’re sorry—we didn’t choose the names, or we’d have much cooler\nand friendlier ways to talk about this stuff.\nWe’re using two things called a service in this chapter. The first is an\napplication service (our service layer). Its job is to handle requests\nfrom the outside world and to orchestrate an operation. What we mean\nis that the service layer drives the application by following a bunch of\nsimple steps:\nGet some data from the database\nUpdate the domain model",
      "content_length": 1311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "Persist any changes\nThis is the kind of boring work that has to happen for every operation\nin your system, and keeping it separate from business logic helps to\nkeep things tidy.\nThe second type of service is a domain service. This is the name for a\npiece of logic that belongs in the domain model but doesn’t sit\nnaturally inside a stateful entity or value object. For example, if you\nwere building a shopping cart application, you might choose to build\ntaxation rules as a domain service. Calculating tax is a separate job\nfrom updating the cart, and it’s an important part of the model, but it\ndoesn’t seem right to have a persisted entity for the job. Instead a\nstateless TaxCalculator class or a calculate_tax function can do the\njob.\nPutting Things in Folders to See Where It\nAll Belongs\nAs our application gets bigger, we’ll need to keep tidying our\ndirectory structure. The layout of our project gives us useful hints\nabout what kinds of object we’ll find in each file.\nHere’s one way we could organize things:\nSome subfolders\n. \n├── config.py \n├── domain   \n│   ├── __init__.py \n│   └── model.py",
      "content_length": 1103,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "├── service_layer   \n│   ├── __init__.py \n│   └── services.py \n├── adapters   \n│   ├── __init__.py \n│   ├── orm.py \n│   └── repository.py \n├── entrypoints   \n│   ├── __init__.py \n│   └── flask_app.py \n└── tests \n    ├── __init__.py \n    ├── conftest.py \n    ├── unit \n    │   ├── test_allocate.py \n    │   ├── test_batches.py \n    │   └── test_services.py \n    ├── integration \n    │   ├── test_orm.py \n    │   └── test_repository.py \n    └── e2e \n        └── test_api.py\nLet’s have a folder for our domain model. Currently that’s just one\nfile, but for a more complex application, you might have one file\nper class; you might have helper parent classes for Entity,\nValueObject, and Aggregate, and you might add an\nexceptions.py for domain-layer exceptions and, as you’ll see in\nPart II, commands.py and events.py.\nWe’ll distinguish the service layer. Currently that’s just one file\ncalled services.py for our service-layer functions. You could add\nservice-layer exceptions here, and as you’ll see in Chapter 5,\nwe’ll add unit_of_work.py.\nAdapters is a nod to the ports and adapters terminology. This will\nfill up with any other abstractions around external I/O (e.g., a\nredis_client.py). Strictly speaking, you would call these secondary\nadapters or driven adapters, or sometimes inward-facing adapters.",
      "content_length": 1304,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "Entrypoints are the places we drive our application from. In the\nofficial ports and adapters terminology, these are adapters too, and\nare referred to as primary, driving, or outward-facing adapters.\nWhat about ports? As you may remember, they are the abstract\ninterfaces that the adapters implement. We tend to keep them in the\nsame file as the adapters that implement them.\nWrap-Up\nAdding the service layer has really bought us quite a lot:\nOur Flask API endpoints become very thin and easy to write:\ntheir only responsibility is doing “web stuff,” such as parsing\nJSON and producing the right HTTP codes for happy or\nunhappy cases.\nWe’ve defined a clear API for our domain, a set of use cases\nor entrypoints that can be used by any adapter without needing\nto know anything about our domain model classes—whether\nthat’s an API, a CLI (see Appendix C), or the tests! They’re\nan adapter for our domain too.\nWe can write tests in “high gear” by using the service layer,\nleaving us free to refactor the domain model in any way we\nsee fit. As long as we can still deliver the same use cases, we\ncan experiment with new designs without needing to rewrite a\nload of tests.\nAnd our test pyramid is looking good—the bulk of our tests\nare fast unit tests, with just the bare minimum of E2E and\nintegration tests.\nThe DIP in Action",
      "content_length": 1321,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "Figure 4-3 shows the dependencies of our service layer: the domain\nmodel and AbstractRepository (the port, in ports and adapters\nterminology).\nWhen we run the tests, Figure 4-4 shows how we implement the\nabstract dependencies by using FakeRepository (the adapter).\nAnd when we actually run our app, we swap in the “real” dependency\nshown in Figure 4-5.\nFigure 4-3. Abstract dependencies of the service layer",
      "content_length": 407,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "Figure 4-4. Tests provide an implementation of the abstract\ndependency",
      "content_length": 70,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "(fa |—\n\nAbstractRepository\n\ngets\n\nnode SalachemyRepositoy.|¢~\nefnitions iene\nfrom \\*\n\nOni\n(another abstraction)\n\n| talks to",
      "content_length": 123,
      "extraction_method": "OCR"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "Figure 4-5. Dependencies at runtime\nWonderful.\nLet’s pause for Table 4-1, in which we consider the pros and cons of\nhaving a service layer at all.",
      "content_length": 146,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "Table 4-1. Service layer: the trade-offs\nPros\nCons",
      "content_length": 50,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "Pros\nCons\nWe have a single place to \ncapture all the use cases for our \napplication.\nWe’ve placed our clever domain \nlogic behind an API, which \nleaves us free to \nrefactor.\nWe have cleanly separated \n“stuff that talks HTTP” from \n“stuff that talks \nallocation.”\nWhen combined with the \nRepository pattern and FakeRep\nository, we have \na nice way of writing tests at a \nhigher level than the domain \nlayer; \nwe can test more of our \nworkflow without needing to \nuse integration tests \n(read on to Chapter 5 for more \nelaboration on this).\nIf your app is purely a web \napp, your controllers/view \nfunctions can be \nthe single place to capture all \nthe use cases.\nIt’s yet another layer of \nabstraction.\nPutting too much logic into the \nservice layer can lead to the \nAnemic Domain \nanti-pattern. It’s better to \nintroduce this layer after you \nspot orchestration \nlogic creeping into your \ncontrollers.\nYou can get a lot of the benefits \nthat come from having rich \ndomain models \nby simply pushing logic out of \nyour controllers and down to \nthe model layer, \nwithout needing to add an extra \nlayer in between (aka “fat \nmodels, thin \ncontrollers”).",
      "content_length": 1149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "But there are still some bits of awkwardness to tidy up:\nThe service layer is still tightly coupled to the domain,\nbecause its API is expressed in terms of OrderLine objects.\nIn Chapter 5, we’ll fix that and talk about the way that the\nservice layer enables more productive TDD.\nThe service layer is tightly coupled to a session object. In\nChapter 6, we’ll introduce one more pattern that works\nclosely with the Repository and Service Layer patterns, the\nUnit of Work pattern, and everything will be absolutely\nlovely. You’ll see!\n1  Service-layer services and domain services do have confusingly similar names. We\ntackle this topic later in “Why Is Everything Called a Service?”.",
      "content_length": 680,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "Chapter 5. TDD in High Gear\nand Low Gear\nWe’ve introduced the service layer to capture some of the additional\norchestration responsibilities we need from a working application.\nThe service layer helps us clearly define our use cases and the\nworkflow for each: what we need to get from our repositories, what\npre-checks and current state validation we should do, and what we\nsave at the end.\nBut currently, many of our unit tests operate at a lower level, acting\ndirectly on the model. In this chapter we’ll discuss the trade-offs\ninvolved in moving those tests up to the service-layer level, and some\nmore general testing guidelines.\nHARRY SAYS: SEEING A TEST PYRAMID IN ACTION WAS A LIGHT-\nBULB MOMENT\nHere are a few words from Harry directly:\nI was initially skeptical of all Bob’s architectural patterns, but seeing an actual test pyramid made\nme a convert.\nOnce you implement domain modeling and the service layer, you really actually can get to a\nstage where unit tests outnumber integration and end-to-end tests by an order of magnitude.\nHaving worked in places where the E2E test build would take hours (“wait ‘til tomorrow,”\nessentially), I can’t tell you what a difference it makes to be able to run all your tests in minutes or\nseconds.\nRead on for some guidelines on how to decide what kinds of tests to write and at which level. The\nhigh gear versus low gear way of thinking really changed my testing life.",
      "content_length": 1418,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "How Is Our Test Pyramid Looking?\nLet’s see what this move to using a service layer, with its own service-\nlayer tests, does to our test pyramid:\nCounting types of tests\n$ grep -c test_ test_*.py \ntests/unit/test_allocate.py:4 \ntests/unit/test_batches.py:8 \ntests/unit/test_services.py:3 \n \ntests/integration/test_orm.py:6 \ntests/integration/test_repository.py:2 \n \ntests/e2e/test_api.py:2\nNot bad! We have 15 unit tests, 8 integration tests, and just 2 end-to-\nend tests. That’s already a healthy-looking test pyramid.\nShould Domain Layer Tests Move to the\nService Layer?\nLet’s see what happens if we take this a step further. Since we can test\nour software against the service layer, we don’t really need tests for\nthe domain model anymore. Instead, we could rewrite all of the\ndomain-level tests from Chapter 1 in terms of the service layer:\nRewriting a domain test at the service layer\n(tests/unit/test_services.py)\n# domain-layer test:\ndef test_prefers_current_stock_batches_to_shipments(): \n    in_stock_batch = Batch(\"in-stock-batch\", \"RETRO-CLOCK\", 100, eta=None) \n    shipment_batch = Batch(\"shipment-batch\", \"RETRO-CLOCK\", 100, eta=tomorrow)",
      "content_length": 1150,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "line = OrderLine(\"oref\", \"RETRO-CLOCK\", 10) \n \n    allocate(line, [in_stock_batch, shipment_batch]) \n \n    assert in_stock_batch.available_quantity == 90 \n    assert shipment_batch.available_quantity == 100 \n \n \n# service-layer test:\ndef test_prefers_warehouse_batches_to_shipments(): \n    in_stock_batch = Batch(\"in-stock-batch\", \"RETRO-CLOCK\", 100, eta=None) \n    shipment_batch = Batch(\"shipment-batch\", \"RETRO-CLOCK\", 100, eta=tomorrow) \n    repo = FakeRepository([in_stock_batch, shipment_batch]) \n    session = FakeSession() \n \n    line = OrderLine('oref', \"RETRO-CLOCK\", 10) \n \n    services.allocate(line, repo, session) \n \n    assert in_stock_batch.available_quantity == 90 \n    assert shipment_batch.available_quantity == 100\nWhy would we want to do that?\nTests are supposed to help us change our system fearlessly, but often\nwe see teams writing too many tests against their domain model. This\ncauses problems when they come to change their codebase and find\nthat they need to update tens or even hundreds of unit tests.\nThis makes sense if you stop to think about the purpose of automated\ntests. We use tests to enforce that a property of the system doesn’t\nchange while we’re working. We use tests to check that the API\ncontinues to return 200, that the database session continues to commit,\nand that orders are still being allocated.\nIf we accidentally change one of those behaviors, our tests will break.\nThe flip side, though, is that if we want to change the design of our",
      "content_length": 1488,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "code, any tests relying directly on that code will also fail.\nAs we get further into the book, you’ll see how the service layer forms\nan API for our system that we can drive in multiple ways. Testing\nagainst this API reduces the amount of code that we need to change\nwhen we refactor our domain model. If we restrict ourselves to testing\nonly against the service layer, we won’t have any tests that directly\ninteract with “private” methods or attributes on our model objects,\nwhich leaves us freer to refactor them.\nTIP\nEvery line of code that we put in a test is like a blob of glue, holding the system in\na particular shape. The more low-level tests we have, the harder it will be to\nchange things.\nOn Deciding What Kind of Tests to Write\nYou might be asking yourself, “Should I rewrite all my unit tests, then?\nIs it wrong to write tests against the domain model?” To answer those\nquestions, it’s important to understand the trade-off between coupling\nand design feedback (see Figure 5-1).",
      "content_length": 992,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "Figure 5-1. The test spectrum\nExtreme programming (XP) exhorts us to “listen to the code.” When\nwe’re writing tests, we might find that the code is hard to use or notice\na code smell. This is a trigger for us to refactor, and to reconsider our\ndesign.\nWe only get that feedback, though, when we’re working closely with\nthe target code. A test for the HTTP API tells us nothing about the fine-\ngrained design of our objects, because it sits at a much higher level of\nabstraction.\nOn the other hand, we can rewrite our entire application and, so long\nas we don’t change the URLs or request formats, our HTTP tests will\ncontinue to pass. This gives us confidence that large-scale changes,\nlike changing the database schema, haven’t broken our code.\nAt the other end of the spectrum, the tests we wrote in Chapter 1\nhelped us to flesh out our understanding of the objects we need. The\ntests guided us to a design that makes sense and reads in the domain\nlanguage. When our tests read in the domain language, we feel",
      "content_length": 1011,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "comfortable that our code matches our intuition about the problem\nwe’re trying to solve.\nBecause the tests are written in the domain language, they act as living\ndocumentation for our model. A new team member can read these tests\nto quickly understand how the system works and how the core\nconcepts interrelate.\nWe often “sketch” new behaviors by writing tests at this level to see\nhow the code might look. When we want to improve the design of the\ncode, though, we will need to replace or delete these tests, because\nthey are tightly coupled to a particular implementation.\nHigh and Low Gear\nMost of the time, when we are adding a new feature or fixing a bug,\nwe don’t need to make extensive changes to the domain model. In these\ncases, we prefer to write tests against services because of the lower\ncoupling and higher coverage.\nFor example, when writing an add_stock function or a cancel_order\nfeature, we can work more quickly and with less coupling by writing\ntests against the service layer.\nWhen starting a new project or when hitting a particularly gnarly\nproblem, we will drop back down to writing tests against the domain\nmodel so we get better feedback and executable documentation of our\nintent.",
      "content_length": 1207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "The metaphor we use is that of shifting gears. When starting a journey,\nthe bicycle needs to be in a low gear so that it can overcome inertia.\nOnce we’re off and running, we can go faster and more efficiently by\nchanging into a high gear; but if we suddenly encounter a steep hill or\nare forced to slow down by a hazard, we again drop down to a low\ngear until we can pick up speed again.\nFully Decoupling the Service-Layer Tests\nfrom the Domain\nWe still have direct dependencies on the domain in our service-layer\ntests, because we use domain objects to set up our test data and to\ninvoke our service-layer functions.\nTo have a service layer that’s fully decoupled from the domain, we\nneed to rewrite its API to work in terms of primitives.\nOur service layer currently takes an OrderLine domain object:\nBefore: allocate takes a domain object (service_layer/services.py)\ndef allocate(line: OrderLine, repo: AbstractRepository, session) -> str:\nHow would it look if its parameters were all primitive types?\nAfter: allocate takes strings and ints (service_layer/services.py)\ndef allocate( \n        orderid: str, sku: str, qty: int, repo: AbstractRepository, session\n) -> str:",
      "content_length": 1172,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "We rewrite the tests in those terms as well:\nTests now use primitives in function call (tests/unit/test_services.py)\ndef test_returns_allocation(): \n    batch = model.Batch(\"batch1\", \"COMPLICATED-LAMP\", 100, eta=None) \n    repo = FakeRepository([batch]) \n \n    result = services.allocate(\"o1\", \"COMPLICATED-LAMP\", 10, repo, \nFakeSession()) \n    assert result == \"batch1\"\nBut our tests still depend on the domain, because we still manually\ninstantiate Batch objects. So, if one day we decide to massively\nrefactor how our Batch model works, we’ll have to change a bunch of\ntests.\nMitigation: Keep All Domain Dependencies in\nFixture Functions\nWe could at least abstract that out to a helper function or a fixture in\nour tests. Here’s one way you could do that, adding a factory function\non FakeRepository:\nFactory functions for fixtures are one possibility\n(tests/unit/test_services.py)\nclass FakeRepository(set): \n \n    @staticmethod \n    def for_batch(ref, sku, qty, eta=None): \n        return FakeRepository([ \n            model.Batch(ref, sku, qty, eta), \n        ])",
      "content_length": 1068,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "... \n \n \ndef test_returns_allocation(): \n    repo = FakeRepository.for_batch(\"batch1\", \"COMPLICATED-LAMP\", 100, eta=None) \n    result = services.allocate(\"o1\", \"COMPLICATED-LAMP\", 10, repo, \nFakeSession()) \n    assert result == \"batch1\"\nAt least that would move all of our tests’ dependencies on the domain\ninto one place.\nAdding a Missing Service\nWe could go one step further, though. If we had a service to add stock,\nwe could use that and make our service-layer tests fully expressed in\nterms of the service layer’s official use cases, removing all\ndependencies on the domain:\nTest for new add_batch service (tests/unit/test_services.py)\ndef test_add_batch(): \n    repo, session = FakeRepository([]), FakeSession() \n    services.add_batch(\"b1\", \"CRUNCHY-ARMCHAIR\", 100, None, repo, session) \n    assert repo.get(\"b1\") is not None \n    assert session.committed\nTIP\nIn general, if you find yourself needing to do domain-layer stuff directly in your\nservice-layer tests, it may be an indication that your service layer is incomplete.\nAnd the implementation is just two lines:",
      "content_length": 1075,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "A new service for add_batch (service_layer/services.py)\ndef add_batch( \n        ref: str, sku: str, qty: int, eta: Optional[date], \n        repo: AbstractRepository, session,\n): \n    repo.add(model.Batch(ref, sku, qty, eta)) \n    session.commit() \n \n \ndef allocate( \n        orderid: str, sku: str, qty: int, repo: AbstractRepository, session\n) -> str: \n    ...\nNOTE\nShould you write a new service just because it would help remove dependencies\nfrom your tests? Probably not. But in this case, we almost definitely would need\nan add_batch service one day anyway.\nThat now allows us to rewrite all of our service-layer tests purely in\nterms of the services themselves, using only primitives, and without\nany dependencies on the model:\nServices tests now use only services (tests/unit/test_services.py)\ndef test_allocate_returns_allocation(): \n    repo, session = FakeRepository([]), FakeSession() \n    services.add_batch(\"batch1\", \"COMPLICATED-LAMP\", 100, None, repo, session) \n    result = services.allocate(\"o1\", \"COMPLICATED-LAMP\", 10, repo, session) \n    assert result == \"batch1\" \n \n \ndef test_allocate_errors_for_invalid_sku():",
      "content_length": 1132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "repo, session = FakeRepository([]), FakeSession() \n    services.add_batch(\"b1\", \"AREALSKU\", 100, None, repo, session) \n \n    with pytest.raises(services.InvalidSku, match=\"Invalid sku NONEXISTENTSKU\"): \n        services.allocate(\"o1\", \"NONEXISTENTSKU\", 10, repo, FakeSession())\nThis is a really nice place to be in. Our service-layer tests depend on\nonly the service layer itself, leaving us completely free to refactor the\nmodel as we see fit.\nCarrying the Improvement Through to the\nE2E Tests\nIn the same way that adding add_batch helped decouple our service-\nlayer tests from the model, adding an API endpoint to add a batch\nwould remove the need for the ugly add_stock fixture, and our E2E\ntests could be free of those hardcoded SQL queries and the direct\ndependency on the database.\nThanks to our service function, adding the endpoint is easy, with just a\nlittle JSON wrangling and a single function call required:\nAPI for adding a batch (entrypoints/flask_app.py)\n@app.route(\"/add_batch\", methods=['POST'])\ndef add_batch(): \n    session = get_session() \n    repo = repository.SqlAlchemyRepository(session) \n    eta = request.json['eta'] \n    if eta is not None: \n        eta = datetime.fromisoformat(eta).date() \n    services.add_batch( \n        request.json['ref'], request.json['sku'], request.json['qty'], eta,",
      "content_length": 1319,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "repo, session \n    ) \n    return 'OK', 201\nNOTE\nAre you thinking to yourself, POST to /add_batch? That’s not very RESTful!\nYou’re quite right. We’re being happily sloppy, but if you’d like to make it all more\nRESTy, maybe a POST to /batches, then knock yourself out! Because Flask is a\nthin adapter, it’ll be easy. See the next sidebar.\nAnd our hardcoded SQL queries from conftest.py get replaced with\nsome API calls, meaning the API tests have no dependencies other than\nthe API, which is also nice:\nAPI tests can now add their own batches (tests/e2e/test_api.py)\ndef post_to_add_batch(ref, sku, qty, eta): \n    url = config.get_api_url() \n    r = requests.post( \n        f'{url}/add_batch', \n        json={'ref': ref, 'sku': sku, 'qty': qty, 'eta': eta} \n    ) \n    assert r.status_code == 201 \n \n \n@pytest.mark.usefixtures('postgres_db')\n@pytest.mark.usefixtures('restart_api')\ndef test_happy_path_returns_201_and_allocated_batch(): \n    sku, othersku = random_sku(), random_sku('other') \n    earlybatch = random_batchref(1) \n    laterbatch = random_batchref(2) \n    otherbatch = random_batchref(3) \n    post_to_add_batch(laterbatch, sku, 100, '2011-01-02') \n    post_to_add_batch(earlybatch, sku, 100, '2011-01-01')",
      "content_length": 1219,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "post_to_add_batch(otherbatch, othersku, 100, None) \n    data = {'orderid': random_orderid(), 'sku': sku, 'qty': 3} \n    url = config.get_api_url() \n    r = requests.post(f'{url}/allocate', json=data) \n    assert r.status_code == 201 \n    assert r.json()['batchref'] == earlybatch\nWrap-Up\nOnce you have a service layer in place, you really can move the\nmajority of your test coverage to unit tests and develop a healthy test\npyramid.\nRECAP: RULES OF THUMB FOR DIFFERENT TYPES OF TEST\nAim for one end-to-end test per feature\nThis might be written against an HTTP API, for example. The objective is to demonstrate that\nthe feature works, and that all the moving parts are glued together correctly.\nWrite the bulk of your tests against the service layer\nThese edge-to-edge tests offer a good trade-off between coverage, runtime, and efficiency.\nEach test tends to cover one code path of a feature and use fakes for I/O. This is the place to\nexhaustively cover all the edge cases and the ins and outs of your business logic.\nMaintain a small core of tests written against your domain model\nThese tests have highly focused coverage and are more brittle, but they have the highest\nfeedback. Don’t be afraid to delete these tests if the functionality is later covered by tests at\nthe service layer.\nError handling counts as a feature\nIdeally, your application will be structured such that all errors that bubble up to your\nentrypoints (e.g., Flask) are handled in the same way. This means you need to test only the\nhappy path for each feature, and to reserve one end-to-end test for all unhappy paths (and\nmany unhappy path unit tests, of course).\nA few things will help along the way:\n1",
      "content_length": 1679,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "Express your service layer in terms of primitives rather than\ndomain objects.\nIn an ideal world, you’ll have all the services you need to be\nable to test entirely against the service layer, rather than\nhacking state via repositories or the database. This pays off in\nyour end-to-end tests as well.\nOnto the next chapter!\n1  A valid concern about writing tests at a higher level is that it can lead to combinatorial\nexplosion for more complex use cases. In these cases, dropping down to lower-level unit\ntests of the various collaborating domain objects can be useful. But see also Chapter 8\nand “Optionally: Unit Testing Event Handlers in Isolation with a Fake Message Bus”.",
      "content_length": 674,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "Chapter 6. Unit of Work\nPattern\nIn this chapter we’ll introduce the final piece of the puzzle that ties\ntogether the Repository and Service Layer patterns: the Unit of Work\npattern.\nIf the Repository pattern is our abstraction over the idea of persistent\nstorage, the Unit of Work (UoW) pattern is our abstraction over the\nidea of atomic operations. It will allow us to finally and fully\ndecouple our service layer from the data layer.\nFigure 6-1 shows that, currently, a lot of communication occurs across\nthe layers of our infrastructure: the API talks directly to the database\nlayer to start a session, it talks to the repository layer to initialize\nSQLAlchemyRepository, and it talks to the service layer to ask it to\nallocate.\nTIP\nThe code for this chapter is in the chapter_06_uow branch on GitHub:\ngit clone https://github.com/cosmicpython/code.git \ncd code \ngit checkout chapter_06_uow \n# or to code along, checkout Chapter 4: \ngit checkout chapter_04_service_layer",
      "content_length": 973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "Figure 6-1. Without UoW: API talks directly to three layers\nFigure 6-2 shows our target state. The Flask API now does only two\nthings: it initializes a unit of work, and it invokes a service. The\nservice collaborates with the UoW (we like to think of the UoW as\nbeing part of the service layer), but neither the service function itself\nnor Flask now needs to talk directly to the database.\nAnd we’ll do it all using a lovely piece of Python syntax, a context\nmanager.",
      "content_length": 467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "Figure 6-2. With UoW: UoW now manages database state\nThe Unit of Work Collaborates with the\nRepository\nLet’s see the unit of work (or UoW, which we pronounce “you-wow”)\nin action. Here’s how the service layer will look when we’re finished:\nPreview of unit of work in action\n(src/allocation/service_layer/services.py)\ndef allocate(\n        orderid: str, sku: str, qty: int,\n        uow: unit_of_work.AbstractUnitOfWork\n) -> str:\n    line = OrderLine(orderid, sku, qty)\n    with uow:  \n        batches = uow.batches.list()  \n        ...\n        batchref = model.allocate(line, batches)\n        uow.commit()  \nWe’ll start a UoW as a context manager.\nuow.batches is the batches repo, so the UoW provides us access\nto our permanent storage.\nWhen we’re done, we commit or roll back our work, using the\nUoW.\nThe UoW acts as a single entrypoint to our persistent storage, and it\nkeeps track of what objects were loaded and of the latest state.\nThis gives us three useful things:\n1",
      "content_length": 972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "A stable snapshot of the database to work with, so the objects\nwe use aren’t changing halfway through an operation\nA way to persist all of our changes at once, so if something\ngoes wrong, we don’t end up in an inconsistent state\nA simple API to our persistence concerns and a handy place\nto get a repository\nTest-Driving a UoW with Integration Tests\nHere are our integration tests for the UOW:\nA basic “round-trip” test for a UoW (tests/integration/test_uow.py)\ndef test_uow_can_retrieve_a_batch_and_allocate_to_it(session_factory):\n    session = session_factory()\n    insert_batch(session, 'batch1', 'HIPSTER-WORKBENCH', 100, None)\n    session.commit()\n    uow = unit_of_work.SqlAlchemyUnitOfWork(session_factory)  \n    with uow:\n        batch = uow.batches.get(reference='batch1')  \n        line = model.OrderLine('o1', 'HIPSTER-WORKBENCH', 10)\n        batch.allocate(line)\n        uow.commit()  \n    batchref = get_allocated_batch_ref(session, 'o1', 'HIPSTER-WORKBENCH')\n    assert batchref == 'batch1'\nWe initialize the UoW by using our custom session factory and get\nback a uow object to use in our with block.\nThe UoW gives us access to the batches repository via\nuow.batches.\nWe call commit() on it when we’re done.",
      "content_length": 1222,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "For the curious, the insert_batch and get_allocated_batch_ref\nhelpers look like this:\nHelpers for doing SQL stuff (tests/integration/test_uow.py)\ndef insert_batch(session, ref, sku, qty, eta): \n    session.execute( \n        'INSERT INTO batches (reference, sku, _purchased_quantity, eta)' \n        ' VALUES (:ref, :sku, :qty, :eta)', \n        dict(ref=ref, sku=sku, qty=qty, eta=eta) \n    ) \n \n \ndef get_allocated_batch_ref(session, orderid, sku): \n    [[orderlineid]] = session.execute( \n        'SELECT id FROM order_lines WHERE orderid=:orderid AND sku=:sku', \n        dict(orderid=orderid, sku=sku) \n    ) \n    [[batchref]] = session.execute( \n        'SELECT b.reference FROM allocations JOIN batches AS b ON batch_id = \nb.id' \n        ' WHERE orderline_id=:orderlineid', \n        dict(orderlineid=orderlineid) \n    ) \n    return batchref\nUnit of Work and Its Context Manager\nIn our tests we’ve implicitly defined an interface for what a UoW\nneeds to do. Let’s make that explicit by using an abstract base class:\nAbstract UoW context manager\n(src/allocation/service_layer/unit_of_work.py)\nclass AbstractUnitOfWork(abc.ABC):\n    batches: repository.AbstractRepository",
      "content_length": 1171,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "def __exit__(self, *args):  \n        self.rollback()  \n    @abc.abstractmethod\n    def commit(self):  \n        raise NotImplementedError\n    @abc.abstractmethod\n    def rollback(self):  \n        raise NotImplementedError\nThe UoW provides an attribute called .batches, which will give\nus access to the batches repository.\nIf you’ve never seen a context manager, __enter__ and __exit__\nare the two magic methods that execute when we enter the with\nblock and when we exit it, respectively. They’re our setup and\nteardown phases.\nWe’ll call this method to explicitly commit our work when we’re\nready.\nIf we don’t commit, or if we exit the context manager by raising an\nerror, we do a rollback. (The rollback has no effect if commit()\nhas been called. Read on for more discussion of this.)\nThe Real Unit of Work Uses SQLAlchemy\nSessions\nThe main thing that our concrete implementation adds is the database\nsession:\nThe real SQLAlchemy UoW\n(src/allocation/service_layer/unit_of_work.py)",
      "content_length": 980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "DEFAULT_SESSION_FACTORY = sessionmaker(bind=create_engine(  \n    config.get_postgres_uri(),\n))\nclass SqlAlchemyUnitOfWork(AbstractUnitOfWork):\n    def __init__(self, session_factory=DEFAULT_SESSION_FACTORY):\n        self.session_factory = session_factory  \n    def __enter__(self):\n        self.session = self.session_factory()  # type: Session  \n        self.batches = repository.SqlAlchemyRepository(self.session)  \n        return super().__enter__()\n    def __exit__(self, *args):\n        super().__exit__(*args)\n        self.session.close()  \n    def commit(self):  \n        self.session.commit()\n    def rollback(self):  \n        self.session.rollback()\nThe module defines a default session factory that will connect to\nPostgres, but we allow that to be overridden in our integration tests\nso that we can use SQLite instead.\nThe __enter__ method is responsible for starting a database\nsession and instantiating a real repository that can use that session.\nWe close the session on exit.\nFinally, we provide concrete commit() and rollback() methods\nthat use our database session.\nFake Unit of Work for Testing\nHere’s how we use a fake UoW in our service-layer tests:",
      "content_length": 1169,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "Fake UoW (tests/unit/test_services.py)\nclass FakeUnitOfWork(unit_of_work.AbstractUnitOfWork):\n    def __init__(self):\n        self.batches = FakeRepository([])  \n        self.committed = False  \n    def commit(self):\n        self.committed = True  \n    def rollback(self):\n        pass\ndef test_add_batch():\n    uow = FakeUnitOfWork()  \n    services.add_batch(\"b1\", \"CRUNCHY-ARMCHAIR\", 100, None, uow)  \n    assert uow.batches.get(\"b1\") is not None\n    assert uow.committed\ndef test_allocate_returns_allocation():\n    uow = FakeUnitOfWork()  \n    services.add_batch(\"batch1\", \"COMPLICATED-LAMP\", 100, None, uow)  \n    result = services.allocate(\"o1\", \"COMPLICATED-LAMP\", 10, uow)  \n    assert result == \"batch1\"\n...\nFakeUnitOfWork and FakeRepository are tightly coupled, just\nlike the real UnitofWork and Repository classes. That’s fine\nbecause we recognize that the objects are collaborators.\nNotice the similarity with the fake commit() function from\nFakeSession (which we can now get rid of). But it’s a substantial\nimprovement because we’re now faking out code that we wrote\nrather than third-party code. Some people say, “Don’t mock what\nyou don’t own”.",
      "content_length": 1158,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "In our tests, we can instantiate a UoW and pass it to our service\nlayer, rather than passing a repository and a session. This is\nconsiderably less cumbersome.\nDON’T MOCK WHAT YOU DON’T OWN\nWhy do we feel more comfortable mocking the UoW than the session? Both of our fakes achieve\nthe same thing: they give us a way to swap out our persistence layer so we can run tests in\nmemory instead of needing to talk to a real database. The difference is in the resulting design.\nIf we cared only about writing tests that run quickly, we could create mocks that replace\nSQLAlchemy and use those throughout our codebase. The problem is that Session is a complex\nobject that exposes lots of persistence-related functionality. It’s easy to use Session to make\narbitrary queries against the database, but that quickly leads to data access code being\nsprinkled all over the codebase. To avoid that, we want to limit access to our persistence layer so\neach component has exactly what it needs and nothing more.\nBy coupling to the Session interface, you’re choosing to couple to all the complexity of\nSQLAlchemy. Instead, we want to choose a simpler abstraction and use that to clearly separate\nresponsibilities. Our UoW is much simpler than a session, and we feel comfortable with the\nservice layer being able to start and stop units of work.\n“Don’t mock what you don’t own” is a rule of thumb that forces us to build these simple\nabstractions over messy subsystems. This has the same performance benefit as mocking the\nSQLAlchemy session but encourages us to think carefully about our designs.\nUsing the UoW in the Service Layer\nHere’s what our new service layer looks like:\nService layer using UoW (src/allocation/service_layer/services.py)\ndef add_batch(\n        ref: str, sku: str, qty: int, eta: Optional[date],\n        uow: unit_of_work.AbstractUnitOfWork  \n):\n    with uow:\n        uow.batches.add(model.Batch(ref, sku, qty, eta))\n        uow.commit()",
      "content_length": 1942,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "def allocate(\n        orderid: str, sku: str, qty: int,\n        uow: unit_of_work.AbstractUnitOfWork  \n) -> str:\n    line = OrderLine(orderid, sku, qty)\n    with uow:\n        batches = uow.batches.list()\n        if not is_valid_sku(line.sku, batches):\n            raise InvalidSku(f'Invalid sku {line.sku}')\n        batchref = model.allocate(line, batches)\n        uow.commit()\n    return batchref\nOur service layer now has only the one dependency, once again on\nan abstract UoW.\nExplicit Tests for Commit/Rollback\nBehavior\nTo convince ourselves that the commit/rollback behavior works, we\nwrote a couple of tests:\nIntegration tests for rollback behavior\n(tests/integration/test_uow.py)\ndef test_rolls_back_uncommitted_work_by_default(session_factory): \n    uow = unit_of_work.SqlAlchemyUnitOfWork(session_factory) \n    with uow: \n        insert_batch(uow.session, 'batch1', 'MEDIUM-PLINTH', 100, None) \n \n    new_session = session_factory() \n    rows = list(new_session.execute('SELECT * FROM \"batches\"')) \n    assert rows == []",
      "content_length": 1029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "def test_rolls_back_on_error(session_factory): \n    class MyException(Exception): \n        pass \n \n    uow = unit_of_work.SqlAlchemyUnitOfWork(session_factory) \n    with pytest.raises(MyException): \n        with uow: \n            insert_batch(uow.session, 'batch1', 'LARGE-FORK', 100, None) \n            raise MyException() \n \n    new_session = session_factory() \n    rows = list(new_session.execute('SELECT * FROM \"batches\"')) \n    assert rows == []\nTIP\nWe haven’t shown it here, but it can be worth testing some of the more “obscure”\ndatabase behavior, like transactions, against the “real” database—that is, the same\nengine. For now, we’re getting away with using SQLite instead of Postgres, but in\nChapter 7, we’ll switch some of the tests to using the real database. It’s\nconvenient that our UoW class makes that easy!\nExplicit Versus Implicit Commits\nNow we briefly digress on different ways of implementing the UoW\npattern.\nWe could imagine a slightly different version of the UoW that commits\nby default and rolls back only if it spots an exception:\nA UoW with implicit commit… (src/allocation/unit_of_work.py)\nclass AbstractUnitOfWork(abc.ABC):",
      "content_length": 1153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "def __enter__(self):\n        return self\n    def __exit__(self, exn_type, exn_value, traceback):\n        if exn_type is None:\n            self.commit()  \n        else:\n            self.rollback()  \nShould we have an implicit commit in the happy path?\nAnd roll back only on exception?\nIt would allow us to save a line of code and to remove the explicit\ncommit from our client code:\n...would save us a line of code\n(src/allocation/service_layer/services.py)\ndef add_batch(ref: str, sku: str, qty: int, eta: Optional[date], uow): \n    with uow: \n        uow.batches.add(model.Batch(ref, sku, qty, eta)) \n        # uow.commit()\nThis is a judgment call, but we tend to prefer requiring the explicit\ncommit so that we have to choose when to flush state.\nAlthough we use an extra line of code, this makes the software safe by\ndefault. The default behavior is to not change anything. In turn, that\nmakes our code easier to reason about because there’s only one code\npath that leads to changes in the system: total success and an explicit\ncommit. Any other code path, any exception, any early exit from the\nUoW’s scope leads to a safe state.",
      "content_length": 1132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "Similarly, we prefer to roll back by default because it’s easier to\nunderstand; this rolls back to the last commit, so either the user did\none, or we blow their changes away. Harsh but simple.\nExamples: Using UoW to Group Multiple\nOperations into an Atomic Unit\nHere are a few examples showing the Unit of Work pattern in use. You\ncan see how it leads to simple reasoning about what blocks of code\nhappen together.\nExample 1: Reallocate\nSuppose we want to be able to deallocate and then reallocate orders:\nReallocate service function\ndef reallocate(line: OrderLine, uow: AbstractUnitOfWork) -> str:\n    with uow:\n        batch = uow.batches.get(sku=line.sku)\n        if batch is None:\n            raise InvalidSku(f'Invalid sku {line.sku}')\n        batch.deallocate(line)  \n        allocate(line)  \n        uow.commit()\nIf deallocate() fails, we don’t want to call allocate(),\nobviously.\nIf allocate() fails, we probably don’t want to actually commit\nthe deallocate() either.\nExample 2: Change Batch Quantity",
      "content_length": 1008,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "Our shipping company gives us a call to say that one of the container\ndoors opened, and half our sofas have fallen into the Indian Ocean.\nOops!\nChange quantity\ndef change_batch_quantity(batchref: str, new_qty: int, uow: AbstractUnitOfWork):\n    with uow:\n        batch = uow.batches.get(reference=batchref)\n        batch.change_purchased_quantity(new_qty)\n        while batch.available_quantity < 0:\n            line = batch.deallocate_one()  \n        uow.commit()\nHere we may need to deallocate any number of lines. If we get a\nfailure at any stage, we probably want to commit none of the\nchanges.\nTidying Up the Integration Tests\nWe now have three sets of tests, all essentially pointing at the\ndatabase: test_orm.py, test_repository.py, and test_uow.py. Should\nwe throw any away?\n└── tests \n    ├── conftest.py \n    ├── e2e \n    │   └── test_api.py \n    ├── integration \n    │   ├── test_orm.py \n    │   ├── test_repository.py \n    │   └── test_uow.py \n    ├── pytest.ini \n    └── unit \n        ├── test_allocate.py \n        ├── test_batches.py \n        └── test_services.py",
      "content_length": 1077,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "You should always feel free to throw away tests if you think they’re not\ngoing to add value longer term. We’d say that test_orm.py was\nprimarily a tool to help us learn SQLAlchemy, so we won’t need that\nlong term, especially if the main things it’s doing are covered in\ntest_repository.py. That last test, you might keep around, but we could\ncertainly see an argument for just keeping everything at the highest\npossible level of abstraction (just as we did for the unit tests).\nEXERCISE FOR THE READER\nFor this chapter, probably the best thing to try is to implement a UoW from scratch. The code, as\nalways, is on GitHub. You could either follow the model we have quite closely, or perhaps\nexperiment with separating the UoW (whose responsibilities are commit(), rollback(), and\nproviding the .batches repository) from the context manager, whose job is to initialize things, and\nthen do the commit or rollback on exit. If you feel like going all-functional rather than messing\nabout with all these classes, you could use @contextmanager from contextlib.\nWe’ve stripped out both the actual UoW and the fakes, as well as paring back the abstract UoW.\nWhy not send us a link to your repo if you come up with something you’re particularly proud of?\nTIP\nThis is another example of the lesson from Chapter 5: as we build better\nabstractions, we can move our tests to run against them, which leaves us free to\nchange the underlying details.\nWrap-Up\nHopefully we’ve convinced you that the Unit of Work pattern is useful,\nand that the context manager is a really nice Pythonic way of visually\ngrouping code into blocks that we want to happen atomically.",
      "content_length": 1644,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "This pattern is so useful, in fact, that SQLAlchemy already uses a UoW\nin the shape of the Session object. The Session object in\nSQLAlchemy is the way that your application loads data from the\ndatabase.\nEvery time you load a new entity from the database, the session begins\nto track changes to the entity, and when the session is flushed, all your\nchanges are persisted together. Why do we go to the effort of\nabstracting away the SQLAlchemy session if it already implements the\npattern we want?\nTable 6-1 discusses some of the trade-offs.",
      "content_length": 539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "Table 6-1. Unit of Work pattern: the trade-offs\nPros\nCons\nWe have a nice abstraction \nover the concept of atomic \noperations, and the \ncontext manager makes it easy \nto see, visually, what blocks of \ncode are \ngrouped together atomically.\nWe have explicit control over \nwhen a transaction starts and \nfinishes, and our \napplication fails in a way that is \nsafe by default. We never have \nto worry \nthat an operation is partially \ncommitted.\nIt’s a nice place to put all your \nrepositories so client code can \naccess them.\nAs you’ll see in later chapters, \natomicity isn’t only about \ntransactions; it \ncan help us work with events \nand the message bus.\nYour ORM probably already \nhas some perfectly good \nabstractions around \natomicity. SQLAlchemy even \nhas context managers. You can \ngo a long way \njust passing a session around.\nWe’ve made it look easy, but \nyou have to think quite carefully \nabout \nthings like rollbacks, \nmultithreading, and nested \ntransactions. Perhaps just \nsticking to what Django or \nFlask-SQLAlchemy gives you \nwill keep your life \nsimpler.",
      "content_length": 1068,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "For one thing, the Session API is rich and supports operations that we\ndon’t want or need in our domain. Our UnitOfWork simplifies the\nsession to its essential core: it can be started, committed, or thrown\naway.\nFor another, we’re using the UnitOfWork to access our Repository\nobjects. This is a neat bit of developer usability that we couldn’t do\nwith a plain SQLAlchemy Session.\nUNIT OF WORK PATTERN RECAP\nThe Unit of Work pattern is an abstraction around data integrity\nIt helps to enforce the consistency of our domain model, and improves performance, by\nletting us perform a single flush operation at the end of an operation.\nIt works closely with the Repository and Service Layer patterns\nThe Unit of Work pattern completes our abstractions over data access by representing\natomic updates. Each of our service-layer use cases runs in a single unit of work that\nsucceeds or fails as a block.\nThis is a lovely case for a context manager\nContext managers are an idiomatic way of defining scope in Python. We can use a context\nmanager to automatically roll back our work at the end of a request, which means the system\nis safe by default.\nSQLAlchemy already implements this pattern\nWe introduce an even simpler abstraction over the SQLAlchemy Session object in order to\n“narrow” the interface between the ORM and our code. This helps to keep us loosely\ncoupled.\nLastly, we’re motivated again by the dependency inversion principle:\nour service layer depends on a thin abstraction, and we attach a\nconcrete implementation at the outside edge of the system. This lines\nup nicely with SQLAlchemy’s own recommendations:",
      "content_length": 1616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "Keep the life cycle of the session (and usually the transaction)\nseparate and external. The most comprehensive approach,\nrecommended for more substantial applications, will try to keep\nthe details of session, transaction, and exception management as\nfar as possible from the details of the program doing its work.\n—SQLALchemy “Session Basics” Documentation\n1  You may have come across the use of the word collaborators to describe objects that\nwork together to achieve a goal. The unit of work and the repository are a great example\nof collaborators in the object-modeling sense. In responsibility-driven design, clusters of\nobjects that collaborate in their roles are called object neighborhoods, which is, in our\nprofessional opinion, totally adorable.",
      "content_length": 754,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "Chapter 7. Aggregates and\nConsistency Boundaries\nIn this chapter, we’d like to revisit our domain model to talk about\ninvariants and constraints, and see how our domain objects can\nmaintain their own internal consistency, both conceptually and in\npersistent storage. We’ll discuss the concept of a consistency\nboundary and show how making it explicit can help us to build high-\nperformance software without compromising maintainability.\nFigure 7-1 shows a preview of where we’re headed: we’ll introduce a\nnew model object called Product to wrap multiple batches, and we’ll\nmake the old allocate() domain service available as a method on\nProduct instead.",
      "content_length": 653,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "Figure 7-1. Adding the Product aggregate\nWhy? Let’s find out.",
      "content_length": 61,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "TIP\nThe code for this chapter is in the appendix_csvs branch on GitHub:\ngit clone https://github.com/cosmicpython/code.git \ncd code \ngit checkout appendix_csvs \n# or to code along, checkout the previous chapter: \ngit checkout chapter_06_uow\nWhy Not Just Run Everything in a\nSpreadsheet?\nWhat’s the point of a domain model, anyway? What’s the fundamental\nproblem we’re trying to address?\nCouldn’t we just run everything in a spreadsheet? Many of our users\nwould be delighted by that. Business users like spreadsheets because\nthey’re simple, familiar, and yet enormously powerful.\nIn fact, an enormous number of business processes do operate by\nmanually sending spreadsheets back and forth over email. This “CSV\nover SMTP” architecture has low initial complexity but tends not to\nscale very well because it’s difficult to apply logic and maintain\nconsistency.\nWho is allowed to view this particular field? Who’s allowed to update\nit? What happens when we try to order –350 chairs, or 10,000,000\ntables? Can an employee have a negative salary?",
      "content_length": 1040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "These are the constraints of a system. Much of the domain logic we\nwrite exists to enforce these constraints in order to maintain the\ninvariants of the system. The invariants are the things that have to be\ntrue whenever we finish an operation.\nInvariants, Constraints, and Consistency\nThe two words are somewhat interchangeable, but a constraint is a\nrule that restricts the possible states our model can get into, while an\ninvariant is defined a little more precisely as a condition that is\nalways true.\nIf we were writing a hotel-booking system, we might have the\nconstraint that double bookings are not allowed. This supports the\ninvariant that a room cannot have more than one booking for the same\nnight.\nOf course, sometimes we might need to temporarily bend the rules.\nPerhaps we need to shuffle the rooms around because of a VIP\nbooking. While we’re moving bookings around in memory, we might\nbe double booked, but our domain model should ensure that, when\nwe’re finished, we end up in a final consistent state, where the\ninvariants are met. If we can’t find a way to accommodate all our\nguests, we should raise an error and refuse to complete the operation.\nLet’s look at a couple of concrete examples from our business\nrequirements; we’ll start with this one:",
      "content_length": 1268,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "An order line can be allocated to only one batch at a time.\n—The business\nThis is a business rule that imposes an invariant. The invariant is that\nan order line is allocated to either zero or one batch, but never more\nthan one. We need to make sure that our code never accidentally calls\nBatch.allocate() on two different batches for the same line, and\ncurrently, there’s nothing there to explicitly stop us from doing that.\nInvariants, Concurrency, and Locks\nLet’s look at another one of our business rules:\nWe can’t allocate to a batch if the available quantity is less than\nthe quantity of the order line.\n—The business\nHere the constraint is that we can’t allocate more stock than is\navailable to a batch, so we never oversell stock by allocating two\ncustomers to the same physical cushion, for example. Every time we\nupdate the state of the system, our code needs to ensure that we don’t\nbreak the invariant, which is that the available quantity must be greater\nthan or equal to zero.\nIn a single-threaded, single-user application, it’s relatively easy for us\nto maintain this invariant. We can just allocate stock one line at a time,\nand raise an error if there’s no stock available.\nThis gets much harder when we introduce the idea of concurrency.\nSuddenly we might be allocating stock for multiple order lines",
      "content_length": 1317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "simultaneously. We might even be allocating order lines at the same\ntime as processing changes to the batches themselves.\nWe usually solve this problem by applying locks to our database\ntables. This prevents two operations from happening simultaneously on\nthe same row or same table.\nAs we start to think about scaling up our app, we realize that our\nmodel of allocating lines against all available batches may not scale. If\nwe process tens of thousands of orders per hour, and hundreds of\nthousands of order lines, we can’t hold a lock over the whole batches\ntable for every single one—we’ll get deadlocks or performance\nproblems at the very least.\nWhat Is an Aggregate?\nOK, so if we can’t lock the whole database every time we want to\nallocate an order line, what should we do instead? We want to protect\nthe invariants of our system but allow for the greatest degree of\nconcurrency. Maintaining our invariants inevitably means preventing\nconcurrent writes; if multiple users can allocate DEADLY-SPOON at the\nsame time, we run the risk of overallocating.\nOn the other hand, there’s no reason we can’t allocate DEADLY-SPOON\nat the same time as FLIMSY-DESK. It’s safe to allocate two products at\nthe same time because there’s no invariant that covers them both. We\ndon’t need them to be consistent with each other.",
      "content_length": 1314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "The Aggregate pattern is a design pattern from the DDD community\nthat helps us to resolve this tension. An aggregate is just a domain\nobject that contains other domain objects and lets us treat the whole\ncollection as a single unit.\nThe only way to modify the objects inside the aggregate is to load the\nwhole thing, and to call methods on the aggregate itself.\nAs a model gets more complex and grows more entity and value\nobjects, referencing each other in a tangled graph, it can be hard to\nkeep track of who can modify what. Especially when we have\ncollections in the model as we do (our batches are a collection), it’s a\ngood idea to nominate some entities to be the single entrypoint for\nmodifying their related objects. It makes the system conceptually\nsimpler and easy to reason about if you nominate some objects to be in\ncharge of consistency for the others.\nFor example, if we’re building a shopping site, the Cart might make a\ngood aggregate: it’s a collection of items that we can treat as a single\nunit. Importantly, we want to load the entire basket as a single blob\nfrom our data store. We don’t want two requests to modify the basket\nat the same time, or we run the risk of weird concurrency errors.\nInstead, we want each change to the basket to run in a single database\ntransaction.\nWe don’t want to modify multiple baskets in a transaction, because\nthere’s no use case for changing the baskets of several customers at the\nsame time. Each basket is a single consistency boundary responsible\nfor maintaining its own invariants.",
      "content_length": 1543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "An AGGREGATE is a cluster of associated objects that we treat as\na unit for the purpose of data changes.\n—Eric Evans, Domain-Driven Design blue book\nPer Evans, our aggregate has a root entity (the Cart) that encapsulates\naccess to items. Each item has its own identity, but other parts of the\nsystem will always refer to the Cart only as an indivisible whole.\nTIP\nJust as we sometimes use _leading_underscores to mark methods or\nfunctions as “private,” you can think of aggregates as being the “public” classes\nof our model, and the rest of the entities and value objects as “private.”\nChoosing an Aggregate\nWhat aggregate should we use for our system? The choice is somewhat\narbitrary, but it’s important. The aggregate will be the boundary where\nwe make sure every operation ends in a consistent state. This helps us\nto reason about our software and prevent weird race issues. We want\nto draw a boundary around a small number of objects—the smaller, the\nbetter, for performance—that have to be consistent with one another,\nand we need to give this boundary a good name.\nThe object we’re manipulating under the covers is Batch. What do we\ncall a collection of batches? How should we divide all the batches in\nthe system into discrete islands of consistency?\nWe could use Shipment as our boundary. Each shipment contains\nseveral batches, and they all travel to our warehouse at the same time.",
      "content_length": 1392,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "Or perhaps we could use Warehouse as our boundary: each warehouse\ncontains many batches, and counting all the stock at the same time\ncould make sense.\nNeither of these concepts really satisfies us, though. We should be able\nto allocate DEADLY-SPOONs and FLIMSY-DESKs at the same time, even\nif they’re in the same warehouse or the same shipment. These concepts\nhave the wrong granularity.\nWhen we allocate an order line, we’re interested only in batches that\nhave the same SKU as the order line. Some sort of concept like\nGlobalSkuStock could work: a collection of all the batches for a\ngiven SKU.\nIt’s an unwieldy name, though, so after some bikeshedding via\nSkuStock, Stock, ProductStock, and so on, we decided to simply\ncall it Product—after all, that was the first concept we came across in\nour exploration of the domain language back in Chapter 1.\nSo the plan is this: when we want to allocate an order line, instead of\nFigure 7-2, where we look up all the Batch objects in the world and\npass them to the allocate() domain service…",
      "content_length": 1035,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "Figure 7-2. Before: allocate against all batches using the domain service\n…we’ll move to the world of Figure 7-3, in which there is a new\nProduct object for the particular SKU of our order line, and it will be\nin charge of all the batches for that SKU, and we can call a\n.allocate() method on that instead.",
      "content_length": 306,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "Figure 7-3. After: ask Product to allocate against its batches",
      "content_length": 62,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "Let’s see how that looks in code form:\nOur chosen aggregate, Product (src/allocation/domain/model.py)\nclass Product:\n    def __init__(self, sku: str, batches: List[Batch]):\n        self.sku = sku  \n        self.batches = batches  \n    def allocate(self, line: OrderLine) -> str:  \n        try:\n            batch = next(\n                b for b in sorted(self.batches) if b.can_allocate(line)\n            )\n            batch.allocate(line)\n            return batch.reference\n        except StopIteration:\n            raise OutOfStock(f'Out of stock for sku {line.sku}')\nProduct’s main identifier is the sku.\nOur Product class holds a reference to a collection of batches\nfor that SKU.\nFinally, we can move the allocate() domain service to be a\nmethod on the Product aggregate.\nNOTE\nThis Product might not look like what you’d expect a Product model to look like.\nNo price, no description, no dimensions. Our allocation service doesn’t care about\nany of those things. This is the power of bounded contexts; the concept of a\nproduct in one app can be very different from another. See the following sidebar\nfor more discussion.",
      "content_length": 1123,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "AGGREGATES, BOUNDED CONTEXTS, AND MICROSERVICES\nOne of the most important contributions from Evans and the DDD community is the concept of\nbounded contexts.\nIn essence, this was a reaction against attempts to capture entire businesses into a single\nmodel. The word customer means different things to people in sales, customer service, logistics,\nsupport, and so on. Attributes needed in one context are irrelevant in another; more perniciously,\nconcepts with the same name can have entirely different meanings in different contexts. Rather\nthan trying to build a single model (or class, or database) to capture all the use cases, it’s better\nto have several models, draw boundaries around each context, and handle the translation\nbetween different contexts explicitly.\nThis concept translates very well to the world of microservices, where each microservice is free\nto have its own concept of “customer” and its own rules for translating that to and from other\nmicroservices it integrates with.\nIn our example, the allocation service has Product(sku, batches), whereas the ecommerce will\nhave Product(sku, description, price, image_url, dimensions, etc...). As a rule of thumb,\nyour domain models should include only the data that they need for performing calculations.\nWhether or not you have a microservices architecture, a key consideration in choosing your\naggregates is also choosing the bounded context that they will operate in. By restricting the\ncontext, you can keep your number of aggregates low and their size manageable.\nOnce again, we find ourselves forced to say that we can’t give this issue the treatment it deserves\nhere, and we can only encourage you to read up on it elsewhere. The Fowler link at the start of\nthis sidebar is a good starting point, and either (or indeed, any) DDD book will have a chapter or\nmore on bounded contexts.\nOne Aggregate = One Repository\nOnce you define certain entities to be aggregates, we need to apply the\nrule that they are the only entities that are publicly accessible to the\noutside world. In other words, the only repositories we are allowed\nshould be repositories that return aggregates.",
      "content_length": 2145,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "NOTE\nThe rule that repositories should only return aggregates is the main place where\nwe enforce the convention that aggregates are the only way into our domain\nmodel. Be wary of breaking it!\nIn our case, we’ll switch from BatchRepository to\nProductRepository:\nOur new UoW and repository (unit_of_work.py and repository.py)\nclass AbstractUnitOfWork(abc.ABC): \n    products: repository.AbstractProductRepository \n \n... \n \nclass AbstractProductRepository(abc.ABC): \n \n    @abc.abstractmethod \n    def add(self, product): \n        ... \n \n    @abc.abstractmethod \n    def get(self, sku) -> model.Product: \n        ...\nThe ORM layer will need some tweaks so that the right batches\nautomatically get loaded and associated with Product objects. The\nnice thing is, the Repository pattern means we don’t have to worry\nabout that yet. We can just use our FakeRepository and then feed\nthrough the new model into our service layer to see how it looks with\nProduct as its main entrypoint:\nService layer (src/allocation/service_layer/services.py)",
      "content_length": 1032,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "def add_batch( \n        ref: str, sku: str, qty: int, eta: Optional[date], \n        uow: unit_of_work.AbstractUnitOfWork\n): \n    with uow: \n        product = uow.products.get(sku=sku) \n        if product is None: \n            product = model.Product(sku, batches=[]) \n            uow.products.add(product) \n        product.batches.append(model.Batch(ref, sku, qty, eta)) \n        uow.commit() \n \n \ndef allocate( \n        orderid: str, sku: str, qty: int, \n        uow: unit_of_work.AbstractUnitOfWork\n) -> str: \n    line = OrderLine(orderid, sku, qty) \n    with uow: \n        product = uow.products.get(sku=line.sku) \n        if product is None: \n            raise InvalidSku(f'Invalid sku {line.sku}') \n        batchref = product.allocate(line) \n        uow.commit() \n    return batchref\nWhat About Performance?\nWe’ve mentioned a few times that we’re modeling with aggregates\nbecause we want to have high-performance software, but here we are\nloading all the batches when we only need one. You might expect that\nto be inefficient, but there are a few reasons why we’re comfortable\nhere.\nFirst, we’re purposefully modeling our data so that we can make a\nsingle query to the database to read, and a single update to persist our",
      "content_length": 1226,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "changes. This tends to perform much better than systems that issue lots\nof ad hoc queries. In systems that don’t model this way, we often find\nthat transactions slowly get longer and more complex as the software\nevolves.\nSecond, our data structures are minimal and comprise a few strings\nand integers per row. We can easily load tens or even hundreds of\nbatches in a few milliseconds.\nThird, we expect to have only 20 or so batches of each product at a\ntime. Once a batch is used up, we can discount it from our\ncalculations. This means that the amount of data we’re fetching\nshouldn’t get out of control over time.\nIf we did expect to have thousands of active batches for a product,\nwe’d have a couple of options. For one, we could use lazy-loading for\nthe batches in a product. From the perspective of our code, nothing\nwould change, but in the background, SQLAlchemy would page\nthrough data for us. This would lead to more requests, each fetching a\nsmaller number of rows. Because we need to find only a single batch\nwith enough capacity for our order, this might work pretty well.",
      "content_length": 1084,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "EXERCISE FOR THE READER\nYou’ve just seen the main top layers of the code, so this shouldn’t be too hard, but we’d like you\nto implement the Product aggregate starting from Batch, just as we did.\nOf course, you could cheat and copy/paste from the previous listings, but even if you do that, you’ll\nstill have to solve a few challenges on your own, like adding the model to the ORM and making\nsure all the moving parts can talk to each other, which we hope will be instructive.\nYou’ll find the code on GitHub. We’ve put in a “cheating” implementation in the delegates to the\nexisting allocate() function, so you should be able to evolve that toward the real thing.\nWe’ve marked a couple of tests with @pytest.skip(). After you’ve read the rest of this chapter,\ncome back to these tests to have a go at implementing version numbers. Bonus points if you can\nget SQLAlchemy to do them for you by magic!\nIf all else failed, we’d just look for a different aggregate. Maybe we\ncould split up batches by region or by warehouse. Maybe we could\nredesign our data access strategy around the shipment concept. The\nAggregate pattern is designed to help manage some technical\nconstraints around consistency and performance. There isn’t one\ncorrect aggregate, and we should feel comfortable changing our minds\nif we find our boundaries are causing performance woes.\nOptimistic Concurrency with Version\nNumbers\nWe have our new aggregate, so we’ve solved the conceptual problem\nof choosing an object to be in charge of consistency boundaries. Let’s\nnow spend a little time talking about how to enforce data integrity at\nthe database level.",
      "content_length": 1621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "NOTE\nThis section has a lot of implementation details; for example, some of it is\nPostgres-specific. But more generally, we’re showing one way of managing\nconcurrency issues, but it is just one approach. Real requirements in this area vary\na lot from project to project. You shouldn’t expect to be able to copy and paste\ncode from here into production.\nWe don’t want to hold a lock over the entire batches table, but how\nwill we implement holding a lock over just the rows for a particular\nSKU?\nOne answer is to have a single attribute on the Product model that\nacts as a marker for the whole state change being complete and to use\nit as the single resource that concurrent workers can fight over. If two\ntransactions read the state of the world for batches at the same time,\nand both want to update the allocations tables, we force both to also\ntry to update the version_number in the products table, in such a\nway that only one of them can win and the world stays consistent.\nFigure 7-4 illustrates two concurrent transactions doing their read\noperations at the same time, so they see a Product with, for example,\nversion=3. They both call Product.allocate() in order to modify a\nstate. But we set up our database integrity rules such that only one of\nthem is allowed to commit the new Product with version=4, and the\nother update is rejected.",
      "content_length": 1345,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "TIP\nVersion numbers are just one way to implement optimistic locking. You could\nachieve the same thing by setting the Postgres transaction isolation level to\nSERIALIZABLE, but that often comes at a severe performance cost. Version\nnumbers also make implicit concepts explicit.",
      "content_length": 276,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "Figure 7-4. Sequence diagram: two transactions attempting a concurrent update on\nProduct",
      "content_length": 88,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "OPTIMISTIC CONCURRENCY CONTROL AND RETRIES\nWhat we’ve implemented here is called optimistic concurrency control because our default\nassumption is that everything will be fine when two users want to make changes to the\ndatabase. We think it’s unlikely that they will conflict with each other, so we let them go ahead and\njust make sure we have a way to notice if there is a problem.\nPessimistic concurrency control works under the assumption that two users are going to cause\nconflicts, and we want to prevent conflicts in all cases, so we lock everything just to be safe. In\nour example, that would mean locking the whole batches table, or using SELECT FOR UPDATE—\nwe’re pretending that we’ve ruled those out for performance reasons, but in real life you’d want to\ndo some evaluations and measurements of your own.\nWith pessimistic locking, you don’t need to think about handling failures because the database\nwill prevent them for you (although you do need to think about deadlocks). With optimistic locking,\nyou need to explicitly handle the possibility of failures in the (hopefully unlikely) case of a clash.\nThe usual way to handle a failure is to retry the failed operation from the beginning. Imagine we\nhave two customers, Harry and Bob, and each submits an order for SHINY-TABLE. Both threads\nload the product at version 1 and allocate stock. The database prevents the concurrent update,\nand Bob’s order fails with an error. When we retry the operation, Bob’s order loads the product at\nversion 2 and tries to allocate again. If there is enough stock left, all is well; otherwise, he’ll\nreceive OutOfStock. Most operations can be retried this way in the case of a concurrency problem.\nRead more on retries in “Recovering from Errors Synchronously” and “Footguns”.\nImplementation Options for Version Numbers\nThere are essentially three options for implementing version numbers:\n1. version_number lives in the domain; we add it to the\nProduct constructor, and Product.allocate() is\nresponsible for incrementing it.\n2. The service layer could do it! The version number isn’t\nstrictly a domain concern, so instead our service layer could\nassume that the current version number is attached to Product\nby the repository, and the service layer will increment it\nbefore it does the commit().",
      "content_length": 2292,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "3. Since it’s arguably an infrastructure concern, the UoW and\nrepository could do it by magic. The repository has access to\nversion numbers for any products it retrieves, and when the\nUoW does a commit, it can increment the version number for\nany products it knows about, assuming them to have changed.\nOption 3 isn’t ideal, because there’s no real way of doing it without\nhaving to assume that all products have changed, so we’ll be\nincrementing version numbers when we don’t have to.\nOption 2 involves mixing the responsibility for mutating state between\nthe service layer and the domain layer, so it’s a little messy as well.\nSo in the end, even though version numbers don’t have to be a domain\nconcern, you might decide the cleanest trade-off is to put them in the\ndomain:\nOur chosen aggregate, Product (src/allocation/domain/model.py)\nclass Product:\n    def __init__(self, sku: str, batches: List[Batch], version_number: int = 0):  \n        self.sku = sku\n        self.batches = batches\n        self.version_number = version_number  \n    def allocate(self, line: OrderLine) -> str:\n        try:\n            batch = next(\n                b for b in sorted(self.batches) if b.can_allocate(line)\n            )\n            batch.allocate(line)\n            self.version_number += 1  \n            return batch.reference\n1",
      "content_length": 1320,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "except StopIteration:\n            raise OutOfStock(f'Out of stock for sku {line.sku}')\nThere it is!\nTIP\nIf you’re scratching your head at this version number business, it might help to\nremember that the number isn’t important. What’s important is that the Product\ndatabase row is modified whenever we make a change to the Product aggregate.\nThe version number is a simple, human-comprehensible way to model a thing that\nchanges on every write, but it could equally be a random UUID every time.\nTesting for Our Data Integrity Rules\nNow to make sure we can get the behavior we want: if we have two\nconcurrent attempts to do allocation against the same Product, one of\nthem should fail, because they can’t both update the version number.\nFirst, let’s simulate a “slow” transaction using a function that does\nallocation and then does an explicit sleep:\ntime.sleep can reproduce concurrency behavior\n(tests/integration/test_uow.py)\ndef try_to_allocate(orderid, sku, exceptions): \n    line = model.OrderLine(orderid, sku, 10) \n    try: \n        with unit_of_work.SqlAlchemyUnitOfWork() as uow: \n            product = uow.products.get(sku=sku) \n            product.allocate(line) \n            time.sleep(0.2) \n2",
      "content_length": 1204,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "uow.commit() \n    except Exception as e: \n        print(traceback.format_exc()) \n        exceptions.append(e)\nThen we have our test invoke this slow allocation twice, concurrently,\nusing threads:\nAn integration test for concurrency behavior\n(tests/integration/test_uow.py)\ndef \ntest_concurrent_updates_to_version_are_not_allowed(postgres_session_factory):\n    sku, batch = random_sku(), random_batchref()\n    session = postgres_session_factory()\n    insert_batch(session, batch, sku, 100, eta=None, product_version=1)\n    session.commit()\n    order1, order2 = random_orderid(1), random_orderid(2)\n    exceptions = []  # type: List[Exception]\n    try_to_allocate_order1 = lambda: try_to_allocate(order1, sku, exceptions)\n    try_to_allocate_order2 = lambda: try_to_allocate(order2, sku, exceptions)\n    thread1 = threading.Thread(target=try_to_allocate_order1)  \n    thread2 = threading.Thread(target=try_to_allocate_order2)  \n    thread1.start()\n    thread2.start()\n    thread1.join()\n    thread2.join()\n    [[version]] = session.execute(\n        \"SELECT version_number FROM products WHERE sku=:sku\",\n        dict(sku=sku),\n    )\n    assert version == 2  \n    [exception] = exceptions\n    assert 'could not serialize access due to concurrent update' in \nstr(exception)",
      "content_length": 1268,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "orders = list(session.execute(\n        \"SELECT orderid FROM allocations\"\n        \" JOIN batches ON allocations.batch_id = batches.id\"\n        \" JOIN order_lines ON allocations.orderline_id = order_lines.id\"\n        \" WHERE order_lines.sku=:sku\",\n        dict(sku=sku),\n    ))\n    assert len(orders) == 1  \n    with unit_of_work.SqlAlchemyUnitOfWork() as uow:\n        uow.session.execute('select 1')\nWe start two threads that will reliably produce the concurrency\nbehavior we want: read1, read2, write1, write2.\nWe assert that the version number has been incremented only once.\nWe can also check on the specific exception if we like.\nAnd we double-check that only one allocation has gotten through.\nEnforcing Concurrency Rules by Using Database\nTransaction Isolation Levels\nTo get the test to pass as it is, we can set the transaction isolation level\non our session:\nSet isolation level for session\n(src/allocation/service_layer/unit_of_work.py)\nDEFAULT_SESSION_FACTORY = sessionmaker(bind=create_engine( \n    config.get_postgres_uri(), \n    isolation_level=\"REPEATABLE READ\",\n))",
      "content_length": 1078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "TIP\nTransaction isolation levels are tricky stuff, so it’s worth spending time\nunderstanding the Postgres documentation.\nPessimistic Concurrency Control Example:\nSELECT FOR UPDATE\nThere are multiple ways to approach this, but we’ll show one. SELECT\nFOR UPDATE produces different behavior; two concurrent transactions\nwill not be allowed to do a read on the same rows at the same time:\nSELECT FOR UPDATE is a way of picking a row or rows to use as a\nlock (although those rows don’t have to be the ones you update). If two\ntransactions both try to SELECT FOR UPDATE a row at the same time,\none will win, and the other will wait until the lock is released. So this\nis an example of pessimistic concurrency control.\nHere’s how you can use the SQLAlchemy DSL to specify FOR UPDATE\nat query time:\nSQLAlchemy with_for_update\n(src/allocation/adapters/repository.py)\n    def get(self, sku): \n        return self.session.query(model.Product) \\ \n                           .filter_by(sku=sku) \\ \n                           .with_for_update() \\ \n                           .first()\nThis will have the effect of changing the concurrency pattern from\n3",
      "content_length": 1138,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "read1, read2, write1, write2(fail)\nto\nread1, write1, read2, write2(succeed)\nSome people refer to this as the “read-modify-write” failure mode.\nRead “PostgreSQL Anti-Patterns: Read-Modify-Write Cycles” for a\ngood overview.\nWe don’t really have time to discuss all the trade-offs between\nREPEATABLE READ and SELECT FOR UPDATE, or optimistic versus\npessimistic locking in general. But if you have a test like the one\nwe’ve shown, you can specify the behavior you want and see how it\nchanges. You can also use the test as a basis for performing some\nperformance experiments.\nWrap-Up\nSpecific choices around concurrency control vary a lot based on\nbusiness circumstances and storage technology choices, but we’d like\nto bring this chapter back to the conceptual idea of an aggregate: we\nexplicitly model an object as being the main entrypoint to some subset\nof our model, and as being in charge of enforcing the invariants and\nbusiness rules that apply across all of those objects.\nChoosing the right aggregate is key, and it’s a decision you may revisit\nover time. You can read more about it in multiple DDD books. We also\nrecommend these three online papers on effective aggregate design by\nVaughn Vernon (the “red book” author).",
      "content_length": 1226,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "Table 7-1 has some thoughts on the trade-offs of implementing the\nAggregate pattern.",
      "content_length": 84,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "Table 7-1. Aggregates: the trade-offs\nPros\nCons\nPython might not have “official” \npublic and private methods, but \nwe do have \nthe underscores convention, \nbecause it’s often useful to try \nto indicate what’s for \n“internal” use and what’s for \n“outside code” to use. Choosing \naggregates is \njust the next level up: it lets you \ndecide which of your domain \nmodel classes \nare the public ones, and which \naren’t.\nModeling our operations around \nexplicit consistency boundaries \nhelps us avoid \nperformance problems with our \nORM.\nPutting the aggregate in sole \ncharge of state changes to its \nsubsidiary models \nmakes the system easier to \nreason about, and makes it \neasier to control invariants.\nYet another new concept for \nnew developers to take on. \nExplaining entities versus \nvalue objects was already a \nmental load; now there’s a third \ntype of domain \nmodel object?\nSticking rigidly to the rule that \nwe modify only one aggregate \nat a time is a \nbig mental shift.\nDealing with eventual \nconsistency between \naggregates can be complex.",
      "content_length": 1046,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "AGGREGATES AND CONSISTENCY BOUNDARIES RECAP\nAggregates are your entrypoints into the domain model\nBy restricting the number of ways that things can be changed, we make the system easier to\nreason about.\nAggregates are in charge of a consistency boundary\nAn aggregate’s job is to be able to manage our business rules about invariants as they apply\nto a group of related objects. It’s the aggregate’s job to check that the objects within its remit\nare consistent with each other and with our rules, and to reject changes that would break the\nrules.\nAggregates and concurrency issues go together\nWhen thinking about implementing these consistency checks, we end up thinking about\ntransactions and locks. Choosing the right aggregate is about performance as well as\nconceptual organization of your domain.",
      "content_length": 801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "Part I Recap\nDo you remember Figure 7-5, the diagram we showed at the beginning\nof Part I to preview where we were heading?",
      "content_length": 123,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "Figure 7-5. A component diagram for our app at the end of Part I\nSo that’s where we are at the end of Part I. What have we achieved?\nWe’ve seen how to build a domain model that’s exercised by a set of\nhigh-level unit tests. Our tests are living documentation: they describe\nthe behavior of our system—the rules upon which we agreed with our\nbusiness stakeholders—in nice readable code. When our business\nrequirements change, we have confidence that our tests will help us to\nprove the new functionality, and when new developers join the project,\nthey can read our tests to understand how things work.\nWe’ve decoupled the infrastructural parts of our system, like the\ndatabase and API handlers, so that we can plug them into the outside of\nour application. This helps us to keep our codebase well organized\nand stops us from building a big ball of mud.\nBy applying the dependency inversion principle, and by using ports-\nand-adapters-inspired patterns like Repository and Unit of Work,\nwe’ve made it possible to do TDD in both high gear and low gear and\nto maintain a healthy test pyramid. We can test our system edge to\nedge, and the need for integration and end-to-end tests is kept to a\nminimum.\nLastly, we’ve talked about the idea of consistency boundaries. We\ndon’t want to lock our entire system whenever we make a change, so\nwe have to choose which parts are consistent with one another.\nFor a small system, this is everything you need to go and play with the\nideas of domain-driven design. You now have the tools to build",
      "content_length": 1528,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "database-agnostic domain models that represent the shared language of\nyour business experts. Hurrah!\nNOTE\nAt the risk of laboring the point—we’ve been at pains to point out that each\npattern comes at a cost. Each layer of indirection has a price in terms of\ncomplexity and duplication in our code and will be confusing to programmers\nwho’ve never seen these patterns before. If your app is essentially a simple\nCRUD wrapper around a database and isn’t likely to be anything more than that\nin the foreseeable future, you don’t need these patterns. Go ahead and use\nDjango, and save yourself a lot of bother.\nIn Part II, we’ll zoom out and talk about a bigger topic: if aggregates\nare our boundary, and we can update only one at a time, how do we\nmodel processes that cross consistency boundaries?\n1  Perhaps we could get some ORM/SQLAlchemy magic to tell us when an object is dirty,\nbut how would that work in the generic case—for example, for a CsvRepository?\n2  time.sleep() works well in our use case, but it’s not the most reliable or efficient way\nto reproduce concurrency bugs. Consider using semaphores or similar synchronization\nprimitives shared between your threads to get better guarantees of behavior.\n3  If you’re not using Postgres, you’ll need to read different documentation. Annoyingly,\ndifferent databases all have quite different definitions. Oracle’s SERIALIZABLE is\nequivalent to Postgres’s REPEATABLE READ, for example.",
      "content_length": 1440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "Part II. Event-Driven\nArchitecture\nI’m sorry that I long ago coined the term “objects” for this topic\nbecause it gets many people to focus on the lesser idea.\nThe big idea is “messaging.\"…The key in making great and\ngrowable systems is much more to design how its modules\ncommunicate rather than what their internal properties and\nbehaviors should be.\n—Alan Kay\nIt’s all very well being able to write one domain model to manage a\nsingle bit of business process, but what happens when we need to\nwrite many models? In the real world, our applications sit within an\norganization and need to exchange information with other parts of the\nsystem. You may remember our context diagram shown in Figure II-1.\nFaced with this requirement, many teams reach for microservices\nintegrated via HTTP APIs. But if they’re not careful, they’ll end up\nproducing the most chaotic mess of all: the distributed big ball of mud.\nIn Part II, we’ll show how the techniques from Part I can be extended\nto distributed systems. We’ll zoom out to look at how we can compose\na system from many small components that interact through\nasynchronous message passing.",
      "content_length": 1133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "We’ll see how our Service Layer and Unit of Work patterns allow us to\nreconfigure our app to run as an asynchronous message processor, and\nhow event-driven systems help us to decouple aggregates and\napplications from one another.",
      "content_length": 229,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "wes\n\nCistomer\n\n\\Wanttobuy funiture\n\nsystem\nPacis\n\nWanages wero for bying\nstock fom supplies\n\nsystem\nActin\n\n«stem\n\nVarebose\n\nAlotessocktocsomer\nodes\n\nManages wero fo\nshppinggoodstocxtoness",
      "content_length": 187,
      "extraction_method": "OCR"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "Figure II-1. But exactly how will all these systems talk to each other?\nWe’ll look at the following patterns and techniques:\nDomain Events\nTrigger workflows that cross consistency boundaries.\nMessage Bus\nProvide a unified way of invoking use cases from any endpoint.\nCQRS\nSeparating reads and writes avoids awkward compromises in an\nevent-driven architecture and enables performance and scalability\nimprovements.\nPlus, we’ll add a dependency injection framework. This has nothing to\ndo with event-driven architecture per se, but it tidies up an awful lot of\nloose ends.",
      "content_length": 569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "Chapter 8. Events and the\nMessage Bus\nSo far we’ve spent a lot of time and energy on a simple problem that\nwe could easily have solved with Django. You might be asking if the\nincreased testability and expressiveness are really worth all the effort.\nIn practice, though, we find that it’s not the obvious features that make\na mess of our codebases: it’s the goop around the edge. It’s reporting,\nand permissions, and workflows that touch a zillion objects.\nOur example will be a typical notification requirement: when we can’t\nallocate an order because we’re out of stock, we should alert the\nbuying team. They’ll go and fix the problem by buying more stock, and\nall will be well.\nFor a first version, our product owner says we can just send the alert\nby email.\nLet’s see how our architecture holds up when we need to plug in some\nof the mundane stuff that makes up so much of our systems.\nWe’ll start by doing the simplest, most expeditious thing, and talk\nabout why it’s exactly this kind of decision that leads us to the Big Ball\nof Mud.",
      "content_length": 1039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "Then we’ll show how to use the Domain Events pattern to separate\nside effects from our use cases, and how to use a simple Message Bus\npattern for triggering behavior based on those events. We’ll show a\nfew options for creating those events and how to pass them to the\nmessage bus, and finally we’ll show how the Unit of Work pattern can\nbe modified to connect the two together elegantly, as previewed in\nFigure 8-1.",
      "content_length": 415,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "Figure 8-1. Events flowing through the system\nTIP\nThe code for this chapter is in the chapter_08_events_and_message_bus branch\non GitHub:\ngit clone https://github.com/cosmicpython/code.git \ncd code \ngit checkout chapter_08_events_and_message_bus \n# or to code along, checkout the previous chapter: \ngit checkout chapter_07_aggregate\nAvoiding Making a Mess\nSo. Email alerts when we run out of stock. When we have new\nrequirements like ones that really have nothing to do with the core\ndomain, it’s all too easy to start dumping these things into our web\ncontrollers.\nFirst, Let’s Avoid Making a Mess of Our Web\nControllers\nAs a one-off hack, this might be OK:\nJust whack it in the endpoint—what could go wrong?\n(src/allocation/entrypoints/flask_app.py)\n@app.route(\"/allocate\", methods=['POST'])\ndef allocate_endpoint(): \n    line = model.OrderLine( \n        request.json['orderid'], \n        request.json['sku'],",
      "content_length": 911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "request.json['qty'], \n    ) \n    try: \n        uow = unit_of_work.SqlAlchemyUnitOfWork() \n        batchref = services.allocate(line, uow) \n    except (model.OutOfStock, services.InvalidSku) as e: \n        send_mail( \n            'out of stock', \n            'stock_admin@made.com', \n            f'{line.orderid} - {line.sku}' \n        ) \n        return jsonify({'message': str(e)}), 400 \n \n    return jsonify({'batchref': batchref}), 201\n…but it’s easy to see how we can quickly end up in a mess by patching\nthings up like this. Sending email isn’t the job of our HTTP layer, and\nwe’d like to be able to unit test this new feature.\nAnd Let’s Not Make a Mess of Our Model Either\nAssuming we don’t want to put this code into our web controllers,\nbecause we want them to be as thin as possible, we may look at putting\nit right at the source, in the model:\nEmail-sending code in our model isn’t lovely either\n(src/allocation/domain/model.py)\n    def allocate(self, line: OrderLine) -> str: \n        try: \n            batch = next( \n                b for b in sorted(self.batches) if b.can_allocate(line) \n            ) \n            #... \n        except StopIteration:",
      "content_length": 1163,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "email.send_mail('stock@made.com', f'Out of stock for {line.sku}') \n            raise OutOfStock(f'Out of stock for sku {line.sku}')\nBut that’s even worse! We don’t want our model to have any\ndependencies on infrastructure concerns like email.send_mail.\nThis email-sending thing is unwelcome goop messing up the nice clean\nflow of our system. What we’d like is to keep our domain model\nfocused on the rule “You can’t allocate more stuff than is actually\navailable.”\nThe domain model’s job is to know that we’re out of stock, but the\nresponsibility of sending an alert belongs elsewhere. We should be\nable to turn this feature on or off, or to switch to SMS notifications\ninstead, without needing to change the rules of our domain model.\nOr the Service Layer!\nThe requirement “Try to allocate some stock, and send an email if it\nfails” is an example of workflow orchestration: it’s a set of steps that\nthe system has to follow to achieve a goal.\nWe’ve written a service layer to manage orchestration for us, but even\nhere the feature feels out of place:\nAnd in the service layer, it’s out of place\n(src/allocation/service_layer/services.py)\ndef allocate( \n        orderid: str, sku: str, qty: int, \n        uow: unit_of_work.AbstractUnitOfWork\n) -> str:",
      "content_length": 1251,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "line = OrderLine(orderid, sku, qty) \n    with uow: \n        product = uow.products.get(sku=line.sku) \n        if product is None: \n            raise InvalidSku(f'Invalid sku {line.sku}') \n        try: \n            batchref = product.allocate(line) \n            uow.commit() \n            return batchref \n        except model.OutOfStock: \n            email.send_mail('stock@made.com', f'Out of stock for {line.sku}') \n            raise\nCatching an exception and reraising it? It could be worse, but it’s\ndefinitely making us unhappy. Why is it so hard to find a suitable home\nfor this code?\nSingle Responsibility Principle\nReally, this is a violation of the single responsibility principle\n(SRP).  Our use case is allocation. Our endpoint, service function, and\ndomain methods are all called allocate, not\nallocate_and_send_mail_if_out_of_stock.\nTIP\nRule of thumb: if you can’t describe what your function does without using words\nlike “then” or “and,” you might be violating the SRP.\nOne formulation of the SRP is that each class should have only a single\nreason to change. When we switch from email to SMS, we shouldn’t\n1",
      "content_length": 1122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "have to update our allocate() function, because that’s clearly a\nseparate responsibility.\nTo solve the problem, we’re going to split the orchestration into\nseparate steps so that the different concerns don’t get tangled up.  The\ndomain model’s job is to know that we’re out of stock, but the\nresponsibility of sending an alert belongs elsewhere. We should be\nable to turn this feature on or off, or to switch to SMS notifications\ninstead, without needing to change the rules of our domain model.\nWe’d also like to keep the service layer free of implementation details.\nWe want to apply the dependency inversion principle to notifications\nso that our service layer depends on an abstraction, in the same way as\nwe avoid depending on the database by using a unit of work.\nAll Aboard the Message Bus!\nThe patterns we’re going to introduce here are Domain Events and the\nMessage Bus. We can implement them in a few ways, so we’ll show a\ncouple before settling on the one we like most.\nThe Model Records Events\nFirst, rather than being concerned about emails, our model will be in\ncharge of recording events—facts about things that have happened.\nWe’ll use a message bus to respond to events and invoke a new\noperation.\nEvents Are Simple Dataclasses\n2",
      "content_length": 1246,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "An event is a kind of value object. Events don’t have any behavior,\nbecause they’re pure data structures. We always name events in the\nlanguage of the domain, and we think of them as part of our domain\nmodel.\nWe could store them in model.py, but we may as well keep them in\ntheir own file (this might be a good time to consider refactoring out a\ndirectory called domain so that we have domain/model.py and\ndomain/events.py):\nEvent classes (src/allocation/domain/events.py)\nfrom dataclasses import dataclass\nclass Event:  \n    pass\n@dataclass\nclass OutOfStock(Event):  \n    sku: str\nOnce we have a number of events, we’ll find it useful to have a\nparent class that can store common attributes. It’s also useful for\ntype hints in our message bus, as you’ll see shortly.\ndataclasses are great for domain events too.\nThe Model Raises Events\nWhen our domain model records a fact that happened, we say it raises\nan event.\nHere’s what it will look like from the outside; if we ask Product to\nallocate but it can’t, it should raise an event:",
      "content_length": 1033,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "Test our aggregate to raise events (tests/unit/test_product.py)\ndef test_records_out_of_stock_event_if_cannot_allocate():\n    batch = Batch('batch1', 'SMALL-FORK', 10, eta=today)\n    product = Product(sku=\"SMALL-FORK\", batches=[batch])\n    product.allocate(OrderLine('order1', 'SMALL-FORK', 10))\n    allocation = product.allocate(OrderLine('order2', 'SMALL-FORK', 1))\n    assert product.events[-1] == events.OutOfStock(sku=\"SMALL-FORK\")  \n    assert allocation is None\nOur aggregate will expose a new attribute called .events that will\ncontain a list of facts about what has happened, in the form of\nEvent objects.\nHere’s what the model looks like on the inside:\nThe model raises a domain event (src/allocation/domain/model.py)\nclass Product:\n    def __init__(self, sku: str, batches: List[Batch], version_number: int = 0):\n        self.sku = sku\n        self.batches = batches\n        self.version_number = version_number\n        self.events = []  # type: List[events.Event]  \n    def allocate(self, line: OrderLine) -> str:\n        try:\n            #...\n        except StopIteration:\n            self.events.append(events.OutOfStock(line.sku))  \n            # raise OutOfStock(f'Out of stock for sku {line.sku}')  \n            return None\nHere’s our new .events attribute in use.\nRather than invoking some email-sending code directly, we record\nthose events at the place they occur, using only the language of the",
      "content_length": 1415,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "domain.\nWe’re also going to stop raising an exception for the out-of-stock\ncase. The event will do the job the exception was doing.\nNOTE\nWe’re actually addressing a code smell we had until now, which is that we were\nusing exceptions for control flow. In general, if you’re implementing domain\nevents, don’t raise exceptions to describe the same domain concept. As you’ll see\nlater when we handle events in the Unit of Work pattern, it’s confusing to have to\nreason about events and exceptions together.\nThe Message Bus Maps Events to Handlers\nA message bus basically says, “When I see this event, I should invoke\nthe following handler function.” In other words, it’s a simple publish-\nsubscribe system. Handlers are subscribed to receive events, which\nwe publish to the bus. It sounds harder than it is, and we usually\nimplement it with a dict:\nSimple message bus (src/allocation/service_layer/messagebus.py)\ndef handle(event: events.Event): \n    for handler in HANDLERS[type(event)]: \n        handler(event) \n \n \ndef send_out_of_stock_notification(event: events.OutOfStock): \n    email.send_mail( \n        'stock@made.com', \n        f'Out of stock for {event.sku}', \n    )",
      "content_length": 1173,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "HANDLERS = { \n    events.OutOfStock: [send_out_of_stock_notification], \n \n}  # type: Dict[Type[events.Event], List[Callable]]\nNOTE\nNote that the message bus as implemented doesn’t give us concurrency because\nonly one handler will run at a time. Our objective isn’t to support parallel threads\nbut to separate tasks conceptually, and to keep each UoW as small as possible.\nThis helps us to understand the codebase because the “recipe” for how to run\neach use case is written in a single place. See the following sidebar.\nIS THIS LIKE CELERY?\nCelery is a popular tool in the Python world for deferring self-contained chunks of work to an\nasynchronous task queue. The message bus we’re presenting here is very different, so the short\nanswer to the above question is no; our message bus has more in common with a Node.js app,\na UI event loop, or an actor framework.\nIf you do have a requirement for moving work off the main thread, you can still use our event-\nbased metaphors, but we suggest you use external events for that. There’s more discussion in\nTable 11-1, but essentially, if you implement a way of persisting events to a centralized store, you\ncan subscribe other containers or other microservices to them. Then that same concept of using\nevents to separate responsibilities across units of work within a single process/service can be\nextended across multiple processes—which may be different containers within the same\nservice, or totally different microservices.\nIf you follow us in this approach, your API for distributing tasks is your event classes—or a JSON\nrepresentation of them. This allows you a lot of flexibility in who you distribute tasks to; they need\nnot necessarily be Python services. Celery’s API for distributing tasks is essentially “function\nname plus arguments,” which is more restrictive, and Python-only.\nOption 1: The Service Layer Takes\nEvents from the Model and Puts Them on\nthe Message Bus",
      "content_length": 1925,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "Our domain model raises events, and our message bus will call the\nright handlers whenever an event happens. Now all we need is to\nconnect the two. We need something to catch events from the model\nand pass them to the message bus—the publishing step.\nThe simplest way to do this is by adding some code into our service\nlayer:\nThe service layer with an explicit message bus\n(src/allocation/service_layer/services.py)\nfrom . import messagebus\n...\ndef allocate(\n        orderid: str, sku: str, qty: int,\n        uow: unit_of_work.AbstractUnitOfWork\n) -> str:\n    line = OrderLine(orderid, sku, qty)\n    with uow:\n        product = uow.products.get(sku=line.sku)\n        if product is None:\n            raise InvalidSku(f'Invalid sku {line.sku}')\n        try:  \n            batchref = product.allocate(line)\n            uow.commit()\n            return batchref\n        finally:  \n            messagebus.handle(product.events)  \nWe keep the try/finally from our ugly earlier implementation\n(we haven’t gotten rid of all exceptions yet, just OutOfStock).\nBut now, instead of depending directly on an email infrastructure,\nthe service layer is just in charge of passing events from the model\nup to the message bus.",
      "content_length": 1206,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "That already avoids some of the ugliness that we had in our naive\nimplementation, and we have several systems that work like this one,\nin which the service layer explicitly collects events from aggregates\nand passes them to the message bus.\nOption 2: The Service Layer Raises Its\nOwn Events\nAnother variant on this that we’ve used is to have the service layer in\ncharge of creating and raising events directly, rather than having them\nraised by the domain model:\nService layer calls messagebus.handle directly\n(src/allocation/service_layer/services.py)\ndef allocate(\n        orderid: str, sku: str, qty: int,\n        uow: unit_of_work.AbstractUnitOfWork\n) -> str:\n    line = OrderLine(orderid, sku, qty)\n    with uow:\n        product = uow.products.get(sku=line.sku)\n        if product is None:\n            raise InvalidSku(f'Invalid sku {line.sku}')\n        batchref = product.allocate(line)\n        uow.commit() \n        if batchref is None:\n            messagebus.handle(events.OutOfStock(line.sku))\n        return batchref\nAs before, we commit even if we fail to allocate because the code\nis simpler this way and it’s easier to reason about: we always",
      "content_length": 1155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "commit unless something goes wrong. Committing when we\nhaven’t changed anything is safe and keeps the code uncluttered.\nAgain, we have applications in production that implement the pattern\nin this way. What works for you will depend on the particular trade-\noffs you face, but we’d like to show you what we think is the most\nelegant solution, in which we put the unit of work in charge of\ncollecting and raising events.\nOption 3: The UoW Publishes Events to\nthe Message Bus\nThe UoW already has a try/finally, and it knows about all the\naggregates currently in play because it provides access to the\nrepository. So it’s a good place to spot events and pass them to the\nmessage bus:\nThe UoW meets the message bus\n(src/allocation/service_layer/unit_of_work.py)\nclass AbstractUnitOfWork(abc.ABC):\n    ...\n    def commit(self):\n        self._commit()  \n        self.publish_events()  \n    def publish_events(self):  \n        for product in self.products.seen:  \n            while product.events:\n                event = product.events.pop(0)\n                messagebus.handle(event)\n    @abc.abstractmethod",
      "content_length": 1101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "def _commit(self):\n        raise NotImplementedError\n...\nclass SqlAlchemyUnitOfWork(AbstractUnitOfWork):\n    ...\n    def _commit(self):  \n        self.session.commit()\nWe’ll change our commit method to require a private ._commit()\nmethod from subclasses.\nAfter committing, we run through all the objects that our repository\nhas seen and pass their events to the message bus.\nThat relies on the repository keeping track of aggregates that have\nbeen loaded using a new attribute, .seen, as you’ll see in the next\nlisting.\nNOTE\nAre you wondering what happens if one of the handlers fails? We’ll discuss error\nhandling in detail in Chapter 10.\nRepository tracks aggregates that pass through it\n(src/allocation/adapters/repository.py)\nclass AbstractRepository(abc.ABC):\n    def __init__(self):\n        self.seen = set()  # type: Set[model.Product]  \n    def add(self, product: model.Product):  \n        self._add(product)",
      "content_length": 916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "self.seen.add(product)\n    def get(self, sku) -> model.Product:  \n        product = self._get(sku)\n        if product:\n            self.seen.add(product)\n        return product\n    @abc.abstractmethod\n    def _add(self, product: model.Product):  \n        raise NotImplementedError\n    @abc.abstractmethod  \n    def _get(self, sku) -> model.Product:\n        raise NotImplementedError\nclass SqlAlchemyRepository(AbstractRepository):\n    def __init__(self, session):\n        super().__init__()\n        self.session = session\n    def _add(self, product):  \n        self.session.add(product)\n    def _get(self, sku):  \n        return self.session.query(model.Product).filter_by(sku=sku).first()\nFor the UoW to be able to publish new events, it needs to be able\nto ask the repository for which Product objects have been used\nduring this session. We use a set called .seen to store them. That\nmeans our implementations need to call super().__init__().\nThe parent add() method adds things to .seen, and now requires\nsubclasses to implement ._add().\nSimilarly, .get() delegates to a ._get() function, to be\nimplemented by subclasses, in order to capture objects seen.",
      "content_length": 1158,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "NOTE\nThe use of ._underscorey() methods and subclassing is definitely not the\nonly way you could implement these patterns. Have a go at the Exercise for the\nReader in this chapter and experiment with some alternatives.\nAfter the UoW and repository collaborate in this way to automatically\nkeep track of live objects and process their events, the service layer\ncan be totally free of event-handling concerns:\nService layer is clean again\n(src/allocation/service_layer/services.py)\ndef allocate( \n        orderid: str, sku: str, qty: int, \n        uow: unit_of_work.AbstractUnitOfWork\n) -> str: \n    line = OrderLine(orderid, sku, qty) \n    with uow: \n        product = uow.products.get(sku=line.sku) \n        if product is None: \n            raise InvalidSku(f'Invalid sku {line.sku}') \n        batchref = product.allocate(line) \n        uow.commit() \n        return batchref\nWe do also have to remember to change the fakes in the service layer\nand make them call super() in the right places, and to implement\nunderscorey methods, but the changes are minimal:\nService-layer fakes need tweaking (tests/unit/test_services.py)",
      "content_length": 1122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "class FakeRepository(repository.AbstractRepository): \n \n    def __init__(self, products): \n        super().__init__() \n        self._products = set(products) \n \n    def _add(self, product): \n        self._products.add(product) \n \n    def _get(self, sku): \n        return next((p for p in self._products if p.sku == sku), None) \n \n... \n \nclass FakeUnitOfWork(unit_of_work.AbstractUnitOfWork): \n    ... \n \n    def _commit(self): \n        self.committed = True",
      "content_length": 457,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "EXERCISE FOR THE READER\nAre you finding all those ._add() and ._commit() methods “super-gross,” in the words of our\nbeloved tech reviewer Hynek? Does it “make you want to beat Harry around the head with a\nplushie snake”? Hey, our code listings are only meant to be examples, not the perfect solution!\nWhy not go see if you can do better?\nOne composition over inheritance way to go would be to implement a wrapper class:\nA wrapper adds functionality and then delegates (src/adapters/repository.py)\nclass TrackingRepository:\n    seen: Set[model.Product]\n    def __init__(self, repo: AbstractRepository):\n        self.seen = set()  # type: Set[model.Product]\n        self._repo = repo\n    def add(self, product: model.Product):  \n        self._repo.add(product)  \n        self.seen.add(product)\n    def get(self, sku) -> model.Product:\n        product = self._repo.get(sku)\n        if product:\n            self.seen.add(product)\n        return product\nBy wrapping the repository, we can call the actual .add() and .get() methods, avoiding weird\nunderscorey methods.\nSee if you can apply a similar pattern to our UoW class in order to get rid of those Java-y\n_commit() methods too. You can find the code on GitHub.\nSwitching all the ABCs to typing.Protocol is a good way to force yourself to avoid using\ninheritance. Let us know if you come up with something nice!\nYou might be starting to worry that maintaining these fakes is going to\nbe a maintenance burden. There’s no doubt that it is work, but in our\nexperience it’s not a lot of work. Once your project is up and running,\nthe interface for your repository and UoW abstractions really don’t\nchange much. And if you’re using ABCs, they’ll help remind you when\nthings get out of sync.",
      "content_length": 1734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "Wrap-Up\nDomain events give us a way to handle workflows in our system. We\noften find, listening to our domain experts, that they express\nrequirements in a causal or temporal way—for example, “When we try\nto allocate stock but there’s none available, then we should send an\nemail to the buying team.”\nThe magic words “When X, then Y” often tell us about an event that\nwe can make concrete in our system. Treating events as first-class\nthings in our model helps us make our code more testable and\nobservable, and it helps isolate concerns.\nAnd Table 8-1 shows the trade-offs as we see them.",
      "content_length": 588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "Table 8-1. Domain events: the trade-offs\nPros\nCons\nA message bus gives us a \nnice way to separate \nresponsibilities when we have \nto take multiple actions in \nresponse to a request.\nEvent handlers are nicely \ndecoupled from the “core” \napplication logic, \nmaking it easy to change their \nimplementation later.\nDomain events are a great \nway to model the real world, \nand we can use them \nas part of our business \nlanguage when modeling with \nstakeholders.\nThe message bus is an additional \nthing to wrap your head around; \nthe implementation \nin which the unit of work raises \nevents for us is neat but also \nmagic. It’s not \nobvious when we call commit that \nwe’re also going to go and send \nemail to \npeople.\nWhat’s more, that hidden event-\nhandling code executes \nsynchronously, \nmeaning your service-layer \nfunction \ndoesn’t finish until all the handlers \nfor any events are finished. That \ncould cause unexpected \nperformance problems in your \nweb endpoints \n(adding asynchronous processing \nis possible but makes things even \nmore confusing).\nMore generally, event-driven \nworkflows can be confusing \nbecause after things \nare split across a chain of multiple \nhandlers, there is no single place \nin the system where you can \nunderstand how a request will be \nfulfilled.",
      "content_length": 1276,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "Pros\nCons\nYou also open yourself up to the \npossibility of circular \ndependencies between your \nevent handlers, and infinite loops.\nEvents are useful for more than just sending email, though. In Chapter 7\nwe spent a lot of time convincing you that you should define\naggregates, or boundaries where we guarantee consistency. People\noften ask, “What should I do if I need to change multiple aggregates as\npart of a request?” Now we have the tools we need to answer that\nquestion.\nIf we have two things that can be transactionally isolated (e.g., an\norder and a product), then we can make them eventually consistent by\nusing events. When an order is canceled, we should find the products\nthat were allocated to it and remove the allocations.",
      "content_length": 738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "DOMAIN EVENTS AND THE MESSAGE BUS RECAP\nEvents can help with the single responsibility principle\nCode gets tangled up when we mix multiple concerns in one place. Events can help us to\nkeep things tidy by separating primary use cases from secondary ones. We also use events\nfor communicating between aggregates so that we don’t need to run long-running\ntransactions that lock against multiple tables.\nA message bus routes messages to handlers\nYou can think of a message bus as a dict that maps from events to their consumers. It\ndoesn’t “know” anything about the meaning of events; it’s just a piece of dumb infrastructure\nfor getting messages around the system.\nOption 1: Service layer raises events and passes them to message bus\nThe simplest way to start using events in your system is to raise them from handlers by\ncalling bus.handle(some_new_event) after you commit your unit of work.\nOption 2: Domain model raises events, service layer passes them to message bus\nThe logic about when to raise an event really should live with the model, so we can improve\nour system’s design and testability by raising events from the domain model. It’s easy for our\nhandlers to collect events off the model objects after commit and pass them to the bus.\nOption 3: UoW collects events from aggregates and passes them to message bus\nAdding bus.handle(aggregate.events) to every handler is annoying, so we can tidy up by\nmaking our unit of work responsible for raising events that were raised by loaded objects.\nThis is the most complex design and might rely on ORM magic, but it’s clean and easy to\nuse once it’s set up.\nIn Chapter 9, we’ll look at this idea in more detail as we build a more\ncomplex workflow with our new message bus.\n1  This principle is the S in SOLID.\n2  Our tech reviewer Ed Jung likes to say that the move from imperative to event-based\nflow control changes what used to be orchestration into choreography.",
      "content_length": 1917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "Chapter 9. Going to Town on\nthe Message Bus\nIn this chapter, we’ll start to make events more fundamental to the\ninternal structure of our application. We’ll move from the current state\nin Figure 9-1, where events are an optional side effect…",
      "content_length": 241,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "Figure 9-1. Before: the message bus is an optional add-on\n…to the situation in Figure 9-2, where everything goes via the message\nbus, and our app has been transformed fundamentally into a message\nprocessor.",
      "content_length": 206,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "Figure 9-2. The message bus is now the main entrypoint to the service layer\nTIP\nThe code for this chapter is in the chapter_09_all_messagebus branch on GitHub:\ngit clone https://github.com/cosmicpython/code.git \ncd code \ngit checkout chapter_09_all_messagebus \n# or to code along, checkout the previous chapter: \ngit checkout chapter_08_events_and_message_bus\nA New Requirement Leads Us to a New\nArchitecture\nRich Hickey talks about situated software, meaning software that runs\nfor extended periods of time, managing a real-world process.\nExamples include warehouse-management systems, logistics\nschedulers, and payroll systems.\nThis software is tricky to write because unexpected things happen all\nthe time in the real world of physical objects and unreliable humans.\nFor example:\nDuring a stock-take, we discover that three SPRINGY-\nMATTRESSes have been water damaged by a leaky roof.\nA consignment of RELIABLE-FORKs is missing the required\ndocumentation and is held in customs for several weeks.\nThree RELIABLE-FORKs subsequently fail safety testing and\nare destroyed.",
      "content_length": 1072,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "A global shortage of sequins means we’re unable to\nmanufacture our next batch of SPARKLY-BOOKCASE.\nIn these types of situations, we learn about the need to change batch\nquantities when they’re already in the system. Perhaps someone made\na mistake on the number in the manifest, or perhaps some sofas fell off\na truck. Following a conversation with the business,  we model the\nsituation as in Figure 9-3.\nFigure 9-3. Batch quantity changed means deallocate and reallocate\nAn event we’ll call BatchQuantityChanged should lead us to change\nthe quantity on the batch, yes, but also to apply a business rule: if the\nnew quantity drops to less than the total already allocated, we need to\ndeallocate those orders from that batch. Then each one will require a\nnew allocation, which we can capture as an event called\nAllocationRequired.\nPerhaps you’re already anticipating that our internal message bus and\nevents can help implement this requirement. We could define a service\ncalled change_batch_quantity that knows how to adjust batch\nquantities and also how to deallocate any excess order lines, and then\neach deallocation can emit an AllocationRequired event that can be\nforwarded to the existing allocate service, in separate transactions.\n1",
      "content_length": 1238,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "Once again, our message bus helps us to enforce the single\nresponsibility principle, and it allows us to make choices about\ntransactions and data integrity.\nImagining an Architecture Change: Everything\nWill Be an Event Handler\nBut before we jump in, think about where we’re headed. There are two\nkinds of flows through our system:\nAPI calls that are handled by a service-layer function\nInternal events (which might be raised as a side effect of a\nservice-layer function) and their handlers (which in turn call\nservice-layer functions)\nWouldn’t it be easier if everything was an event handler? If we rethink\nour API calls as capturing events, the service-layer functions can be\nevent handlers too, and we no longer need to make a distinction\nbetween internal and external event handlers:\nservices.allocate() could be the handler for an\nAllocationRequired event and could emit Allocated\nevents as its output.\nservices.add_batch() could be the handler for a\nBatchCreated event.\nOur new requirement will fit the same pattern:\nAn event called BatchQuantityChanged can invoke a\nhandler called change_batch_quantity().\n2",
      "content_length": 1113,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "And the new AllocationRequired events that it may raise\ncan be passed on to services.allocate() too, so there is\nno conceptual difference between a brand-new allocation\ncoming from the API and a reallocation that’s internally\ntriggered by a deallocation.\nAll sound like a bit much? Let’s work toward it all gradually. We’ll\nfollow the Preparatory Refactoring workflow, aka “Make the change\neasy; then make the easy change”:\n1. We refactor our service layer into event handlers. We can get\nused to the idea of events being the way we describe inputs to\nthe system. In particular, the existing services.allocate()\nfunction will become the handler for an event called\nAllocationRequired.\n2. We build an end-to-end test that puts\nBatchQuantityChanged events into the system and looks for\nAllocated events coming out.\n3. Our implementation will conceptually be very simple: a new\nhandler for BatchQuantityChanged events, whose\nimplementation will emit AllocationRequired events,\nwhich in turn will be handled by the exact same handler for\nallocations that the API uses.\nAlong the way, we’ll make a small tweak to the message bus and UoW,\nmoving the responsibility for putting new events on the message bus\ninto the message bus itself.\nRefactoring Service Functions to\nMessage Handlers",
      "content_length": 1279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "We start by defining the two events that capture our current API inputs\n—AllocationRequired and BatchCreated:\nBatchCreated and AllocationRequired events\n(src/allocation/domain/events.py)\n@dataclass\nclass BatchCreated(Event): \n    ref: str \n    sku: str \n    qty: int \n    eta: Optional[date] = None \n \n... \n \n@dataclass\nclass AllocationRequired(Event): \n    orderid: str \n    sku: str \n    qty: int\nThen we rename services.py to handlers.py; we add the existing\nmessage handler for send_out_of_stock_notification; and most\nimportantly, we change all the handlers so that they have the same\ninputs, an event and a UoW:\nHandlers and services are the same thing\n(src/allocation/service_layer/handlers.py)\ndef add_batch( \n        event: events.BatchCreated, uow: unit_of_work.AbstractUnitOfWork\n): \n    with uow: \n        product = uow.products.get(sku=event.sku) \n        ...",
      "content_length": 872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "def allocate( \n        event: events.AllocationRequired, uow: unit_of_work.AbstractUnitOfWork\n) -> str: \n    line = OrderLine(event.orderid, event.sku, event.qty) \n    ... \n \n \ndef send_out_of_stock_notification( \n        event: events.OutOfStock, uow: unit_of_work.AbstractUnitOfWork,\n): \n    email.send( \n        'stock@made.com', \n        f'Out of stock for {event.sku}', \n    )\nThe change might be clearer as a diff:\nChanging from services to handlers\n(src/allocation/service_layer/handlers.py)\n def add_batch( \n-        ref: str, sku: str, qty: int, eta: Optional[date],\n-        uow: unit_of_work.AbstractUnitOfWork\n+        event: events.BatchCreated, uow: unit_of_work.AbstractUnitOfWork \n ): \n     with uow: \n-        product = uow.products.get(sku=sku)\n+        product = uow.products.get(sku=event.sku) \n     ... \n \n \n def allocate( \n-        orderid: str, sku: str, qty: int,\n-        uow: unit_of_work.AbstractUnitOfWork\n+        event: events.AllocationRequired, uow: unit_of_work.AbstractUnitOfWork \n ) -> str: \n-    line = OrderLine(orderid, sku, qty)\n+    line = OrderLine(event.orderid, event.sku, event.qty) \n     ... \n \n+",
      "content_length": 1141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "+def send_out_of_stock_notification(\n+        event: events.OutOfStock, uow: unit_of_work.AbstractUnitOfWork,\n+):\n+    email.send( \n     ...\nAlong the way, we’ve made our service-layer’s API more structured\nand more consistent. It was a scattering of primitives, and now it uses\nwell-defined objects (see the following sidebar).\nFROM DOMAIN OBJECTS, VIA PRIMITIVE OBSESSION, TO\nEVENTS AS AN INTERFACE\nSome of you may remember “Fully Decoupling the Service-Layer Tests from the Domain”, in\nwhich we changed our service-layer API from being in terms of domain objects to primitives. And\nnow we’re moving back, but to different objects? What gives?\nIn OO circles, people talk about primitive obsession as an anti-pattern: avoid primitives in public\nAPIs, and instead wrap them with custom value classes, they would say. In the Python world, a lot\nof people would be quite skeptical of that as a rule of thumb. When mindlessly applied, it’s\ncertainly a recipe for unnecessary complexity. So that’s not what we’re doing per se.\nThe move from domain objects to primitives bought us a nice bit of decoupling: our client code\nwas no longer coupled directly to the domain, so the service layer could present an API that stays\nthe same even if we decide to make changes to our model, and vice versa.\nSo have we gone backward? Well, our core domain model objects are still free to vary, but\ninstead we’ve coupled the external world to our event classes. They’re part of the domain too, but\nthe hope is that they vary less often, so they’re a sensible artifact to couple on.\nAnd what have we bought ourselves? Now, when invoking a use case in our application, we no\nlonger need to remember a particular combination of primitives, but just a single event class that\nrepresents the input to our application. That’s conceptually quite nice. On top of that, as you’ll see\nin Appendix E, those event classes can be a nice place to do some input validation.\nThe Message Bus Now Collects Events from the\nUoW\nOur event handlers now need a UoW. In addition, as our message bus\nbecomes more central to our application, it makes sense to put it\nexplicitly in charge of collecting and processing new events. There",
      "content_length": 2189,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "was a bit of a circular dependency between the UoW and message bus\nuntil now, so this will make it one-way:\nHandle takes a UoW and manages a queue\n(src/allocation/service_layer/messagebus.py)\ndef handle(event: events.Event, uow: unit_of_work.AbstractUnitOfWork):  \n    queue = [event]  \n    while queue:\n        event = queue.pop(0)  \n        for handler in HANDLERS[type(event)]:  \n            handler(event, uow=uow)  \n            queue.extend(uow.collect_new_events())  \nThe message bus now gets passed the UoW each time it starts up.\nWhen we begin handling our first event, we start a queue.\nWe pop events from the front of the queue and invoke their\nhandlers (the HANDLERS dict hasn’t changed; it still maps event\ntypes to handler functions).\nThe message bus passes the UoW down to each handler.\nAfter each handler finishes, we collect any new events that have\nbeen generated and add them to the queue.\nIn unit_of_work.py, publish_events() becomes a less active\nmethod, collect_new_events():\nUoW no longer puts events directly on the bus\n(src/allocation/service_layer/unit_of_work.py)\n-from . import messagebus  \n-",
      "content_length": 1119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "class AbstractUnitOfWork(abc.ABC): \n@@ -23,13 +21,11 @@ class AbstractUnitOfWork(abc.ABC): \n     def commit(self): \n         self._commit() \n-        self.publish_events()  \n-    def publish_events(self): \n+    def collect_new_events(self): \n         for product in self.products.seen: \n             while product.events: \n-                event = product.events.pop(0) \n-                messagebus.handle(event) \n+                yield product.events.pop(0)  \nThe unit_of_work module now no longer depends on\nmessagebus.\nWe no longer publish_events automatically on commit. The\nmessage bus is keeping track of the event queue instead.\nAnd the UoW no longer actively puts events on the message bus; it\njust makes them available.\nOur Tests Are All Written in Terms of Events Too\nOur tests now operate by creating events and putting them on the\nmessage bus, rather than invoking service-layer functions directly:\nHandler tests use events (tests/unit/test_handlers.py)\nclass TestAddBatch: \n \n     def test_for_new_product(self): \n         uow = FakeUnitOfWork() \n-        services.add_batch(\"b1\", \"CRUNCHY-ARMCHAIR\", 100, None, uow)\n+        messagebus.handle(\n+            events.BatchCreated(\"b1\", \"CRUNCHY-ARMCHAIR\", 100, None), uow\n+        ) \n         assert uow.products.get(\"CRUNCHY-ARMCHAIR\") is not None",
      "content_length": 1309,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "assert uow.committed \n \n... \n \n class TestAllocate: \n \n     def test_returns_allocation(self): \n         uow = FakeUnitOfWork() \n-        services.add_batch(\"batch1\", \"COMPLICATED-LAMP\", 100, None, uow)\n-        result = services.allocate(\"o1\", \"COMPLICATED-LAMP\", 10, uow)\n+        messagebus.handle(\n+            events.BatchCreated(\"batch1\", \"COMPLICATED-LAMP\", 100, None), uow\n+        )\n+        result = messagebus.handle(\n+            events.AllocationRequired(\"o1\", \"COMPLICATED-LAMP\", 10), uow\n+        ) \n         assert result == \"batch1\"\nA Temporary Ugly Hack: The Message Bus Has to\nReturn Results\nOur API and our service layer currently want to know the allocated\nbatch reference when they invoke our allocate() handler. This\nmeans we need to put in a temporary hack on our message bus to let it\nreturn events:\nMessage bus returns results\n(src/allocation/service_layer/messagebus.py)\n def handle(event: events.Event, uow: unit_of_work.AbstractUnitOfWork): \n+    results = [] \n     queue = [event] \n     while queue: \n         event = queue.pop(0) \n         for handler in HANDLERS[type(event)]: \n-            handler(event, uow=uow)\n+            results.append(handler(event, uow=uow)) \n             queue.extend(uow.collect_new_events()) \n+    return results",
      "content_length": 1273,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "It’s because we’re mixing the read and write responsibilities in our\nsystem. We’ll come back to fix this wart in Chapter 12.\nModifying Our API to Work with Events\nFlask changing to message bus as a diff\n(src/allocation/entrypoints/flask_app.py)\n @app.route(\"/allocate\", methods=['POST']) \n def allocate_endpoint(): \n     try: \n-        batchref = services.allocate( \n-            request.json['orderid'],  \n-            request.json['sku'], \n-            request.json['qty'], \n-            unit_of_work.SqlAlchemyUnitOfWork(), \n+        event = events.AllocationRequired(  \n+            request.json['orderid'], request.json['sku'], request.json['qty'], \n         ) \n+        results = messagebus.handle(event, unit_of_work.SqlAlchemyUnitOfWork())  \n+        batchref = results.pop(0) \n     except InvalidSku as e:\nInstead of calling the service layer with a bunch of primitives\nextracted from the request JSON…\nWe instantiate an event.\nThen we pass it to the message bus.\nAnd we should be back to a fully functional application, but one that’s\nnow fully event-driven:\nWhat used to be service-layer functions are now event\nhandlers.\nThat makes them the same as the functions we invoke for\nhandling internal events raised by our domain model.",
      "content_length": 1241,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "We use events as our data structure for capturing inputs to the\nsystem, as well as for handing off of internal work packages.\nThe entire app is now best described as a message processor,\nor an event processor if you prefer. We’ll talk about the\ndistinction in the next chapter.\nImplementing Our New Requirement\nWe’re done with our refactoring phase. Let’s see if we really have\n“made the change easy.” Let’s implement our new requirement, shown\nin Figure 9-4: we’ll receive as our inputs some new\nBatchQuantityChanged events and pass them to a handler, which in\nturn might emit some AllocationRequired events, and those in turn\nwill go back to our existing handler for reallocation.",
      "content_length": 682,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "Adee ior\n\n“ae",
      "content_length": 13,
      "extraction_method": "OCR"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "Figure 9-4. Sequence diagram for reallocation flow\nWARNING\nWhen you split things out like this across two units of work, you now have two\ndatabase transactions, so you are opening yourself up to integrity issues:\nsomething could happen that means the first transaction completes but the second\none does not. You’ll need to think about whether this is acceptable, and whether\nyou need to notice when it happens and do something about it. See “Footguns” for\nmore discussion.\nOur New Event\nThe event that tells us a batch quantity has changed is simple; it just\nneeds a batch reference and a new quantity:\nNew event (src/allocation/domain/events.py)\n@dataclass\nclass BatchQuantityChanged(Event): \n    ref: str \n    qty: int\nTest-Driving a New Handler\nFollowing the lessons learned in Chapter 4, we can operate in “high\ngear” and write our unit tests at the highest possible level of\nabstraction, in terms of events. Here’s what they might look like:\nHandler tests for change_batch_quantity\n(tests/unit/test_handlers.py)",
      "content_length": 1016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "class TestChangeBatchQuantity:\n    def test_changes_available_quantity(self):\n        uow = FakeUnitOfWork()\n        messagebus.handle(\n            events.BatchCreated(\"batch1\", \"ADORABLE-SETTEE\", 100, None), uow\n        )\n        [batch] = uow.products.get(sku=\"ADORABLE-SETTEE\").batches\n        assert batch.available_quantity == 100  \n        messagebus.handle(events.BatchQuantityChanged(\"batch1\", 50), uow)\n        assert batch.available_quantity == 50  \n    def test_reallocates_if_necessary(self):\n        uow = FakeUnitOfWork()\n        event_history = [\n            events.BatchCreated(\"batch1\", \"INDIFFERENT-TABLE\", 50, None),\n            events.BatchCreated(\"batch2\", \"INDIFFERENT-TABLE\", 50, \ndate.today()),\n            events.AllocationRequired(\"order1\", \"INDIFFERENT-TABLE\", 20),\n            events.AllocationRequired(\"order2\", \"INDIFFERENT-TABLE\", 20),\n        ]\n        for e in event_history:\n            messagebus.handle(e, uow)\n        [batch1, batch2] = uow.products.get(sku=\"INDIFFERENT-TABLE\").batches\n        assert batch1.available_quantity == 10\n        assert batch2.available_quantity == 50\n        messagebus.handle(events.BatchQuantityChanged(\"batch1\", 25), uow)\n        # order1 or order2 will be deallocated, so we'll have 25 - 20\n        assert batch1.available_quantity == 5  \n        # and 20 will be reallocated to the next batch\n        assert batch2.available_quantity == 30  \nThe simple case would be trivially easy to implement; we just\nmodify a quantity.",
      "content_length": 1494,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "But if we try to change the quantity to less than has been allocated,\nwe’ll need to deallocate at least one order, and we expect to\nreallocate it to a new batch.\nImplementation\nOur new handler is very simple:\nHandler delegates to model layer\n(src/allocation/service_layer/handlers.py)\ndef change_batch_quantity( \n        event: events.BatchQuantityChanged, uow: unit_of_work.AbstractUnitOfWork\n): \n    with uow: \n        product = uow.products.get_by_batchref(batchref=event.ref) \n        product.change_batch_quantity(ref=event.ref, qty=event.qty) \n        uow.commit()\nWe realize we’ll need a new query type on our repository:\nA new query type on our repository\n(src/allocation/adapters/repository.py)\nclass AbstractRepository(abc.ABC): \n    ... \n \n    def get(self, sku) -> model.Product: \n        ... \n \n    def get_by_batchref(self, batchref) -> model.Product: \n        product = self._get_by_batchref(batchref) \n        if product: \n            self.seen.add(product) \n        return product \n \n    @abc.abstractmethod",
      "content_length": 1024,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "def _add(self, product: model.Product): \n        raise NotImplementedError \n \n    @abc.abstractmethod \n    def _get(self, sku) -> model.Product: \n        raise NotImplementedError \n \n    @abc.abstractmethod \n    def _get_by_batchref(self, batchref) -> model.Product: \n        raise NotImplementedError \n    ... \n \nclass SqlAlchemyRepository(AbstractRepository): \n    ... \n \n    def _get(self, sku): \n        return self.session.query(model.Product).filter_by(sku=sku).first() \n \n    def _get_by_batchref(self, batchref): \n        return self.session.query(model.Product).join(model.Batch).filter( \n            orm.batches.c.reference == batchref, \n        ).first()\nAnd on our FakeRepository too:\nUpdating the fake repo too (tests/unit/test_handlers.py)\nclass FakeRepository(repository.AbstractRepository): \n    ... \n \n    def _get(self, sku): \n        return next((p for p in self._products if p.sku == sku), None) \n \n    def _get_by_batchref(self, batchref): \n        return next(( \n            p for p in self._products for b in p.batches \n            if b.reference == batchref \n        ), None)",
      "content_length": 1099,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "NOTE\nWe’re adding a query to our repository to make this use case easier to implement.\nSo long as our query is returning a single aggregate, we’re not bending any rules.\nIf you find yourself writing complex queries on your repositories, you might want\nto consider a different design. Methods like get_most_popular_products or\nfind_products_by_order_id in particular would definitely trigger our spidey\nsense. Chapter 11 and the epilogue have some tips on managing complex queries.\nA New Method on the Domain Model\nWe add the new method to the model, which does the quantity change\nand deallocation(s) inline and publishes a new event. We also modify\nthe existing allocate function to publish an event:\nOur model evolves to capture the new requirement\n(src/allocation/domain/model.py)\nclass Product: \n    ... \n \n    def change_batch_quantity(self, ref: str, qty: int): \n        batch = next(b for b in self.batches if b.reference == ref) \n        batch._purchased_quantity = qty \n        while batch.available_quantity < 0: \n            line = batch.deallocate_one() \n            self.events.append( \n                events.AllocationRequired(line.orderid, line.sku, line.qty) \n            )\n... \n \nclass Batch: \n    ... \n \n    def deallocate_one(self) -> OrderLine: \n        return self._allocations.pop()",
      "content_length": 1305,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "We wire up our new handler:\nThe message bus grows\n(src/allocation/service_layer/messagebus.py)\nHANDLERS = { \n    events.BatchCreated: [handlers.add_batch], \n    events.BatchQuantityChanged: [handlers.change_batch_quantity], \n    events.AllocationRequired: [handlers.allocate], \n    events.OutOfStock: [handlers.send_out_of_stock_notification], \n \n}  # type: Dict[Type[events.Event], List[Callable]]\nAnd our new requirement is fully implemented.\nOptionally: Unit Testing Event Handlers in\nIsolation with a Fake Message Bus\nOur main test for the reallocation workflow is edge-to-edge (see the\nexample code in “Test-Driving a New Handler”). It uses the real\nmessage bus, and it tests the whole flow, where the\nBatchQuantityChanged event handler triggers deallocation, and\nemits new AllocationRequired events, which in turn are handled by\ntheir own handlers. One test covers a chain of multiple events and\nhandlers.\nDepending on the complexity of your chain of events, you may decide\nthat you want to test some handlers in isolation from one another. You\ncan do this using a “fake” message bus.",
      "content_length": 1090,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "In our case, we actually intervene by modifying the\npublish_events() method on FakeUnitOfWork and decoupling it\nfrom the real message bus, instead making it record what events it\nsees:\nFake message bus implemented in UoW (tests/unit/test_handlers.py)\nclass FakeUnitOfWorkWithFakeMessageBus(FakeUnitOfWork): \n \n    def __init__(self): \n        super().__init__() \n        self.events_published = []  # type: List[events.Event] \n \n    def publish_events(self): \n        for product in self.products.seen: \n            while product.events: \n                self.events_published.append(product.events.pop(0))\nNow when we invoke messagebus.handle() using the\nFakeUnitOfWorkWithFakeMessageBus, it runs only the handler for\nthat event. So we can write a more isolated unit test: instead of\nchecking all the side effects, we just check that\nBatchQuantityChanged leads to AllocationRequired if the\nquantity drops below the total already allocated:\nTesting reallocation in isolation (tests/unit/test_handlers.py)\ndef test_reallocates_if_necessary_isolated(): \n    uow = FakeUnitOfWorkWithFakeMessageBus() \n \n    # test setup as before \n    event_history = [ \n        events.BatchCreated(\"batch1\", \"INDIFFERENT-TABLE\", 50, None), \n        events.BatchCreated(\"batch2\", \"INDIFFERENT-TABLE\", 50, date.today()), \n        events.AllocationRequired(\"order1\", \"INDIFFERENT-TABLE\", 20),",
      "content_length": 1370,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "events.AllocationRequired(\"order2\", \"INDIFFERENT-TABLE\", 20), \n    ] \n    for e in event_history: \n        messagebus.handle(e, uow) \n    [batch1, batch2] = uow.products.get(sku=\"INDIFFERENT-TABLE\").batches \n    assert batch1.available_quantity == 10 \n    assert batch2.available_quantity == 50 \n \n    messagebus.handle(events.BatchQuantityChanged(\"batch1\", 25), uow) \n \n    # assert on new events emitted rather than downstream side-effects \n    [reallocation_event] = uow.events_published \n    assert isinstance(reallocation_event, events.AllocationRequired) \n    assert reallocation_event.orderid in {'order1', 'order2'} \n    assert reallocation_event.sku == 'INDIFFERENT-TABLE'\nWhether you want to do this or not depends on the complexity of your\nchain of events. We say, start out with edge-to-edge testing, and resort\nto this only if necessary.",
      "content_length": 850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "EXERCISE FOR THE READER\nA great way to force yourself to really understand some code is to refactor it. In the discussion of\ntesting handlers in isolation, we used something called FakeUnitOfWorkWithFakeMessageBus,\nwhich is unnecessarily complicated and violates the SRP.\nIf we change the message bus to being a class,  then building a FakeMessageBus is more\nstraightforward:\nAn abstract message bus and its real and fake versions\nclass AbstractMessageBus: \n    HANDLERS: Dict[Type[events.Event], List[Callable]] \n \n    def handle(self, event: events.Event): \n        for handler in self.HANDLERS[type(event)]: \n            handler(event) \n \n \nclass MessageBus(AbstractMessageBus): \n    HANDLERS = { \n        events.OutOfStock: [send_out_of_stock_notification], \n \n    } \n \n \nclass FakeMessageBus(messagebus.AbstractMessageBus): \n    def __init__(self): \n        self.events_published = []  # type: List[events.Event] \n        self.handlers = { \n            events.OutOfStock: [lambda e: self.events_published.append(e)] \n        }\nSo jump into the code on GitHub and see if you can get a class-based version working, and then\nwrite a version of test_reallocates_if_necessary_isolated() from earlier.\nWe use a class-based message bus in Chapter 13, if you need more inspiration.\nWrap-Up\nLet’s look back at what we’ve achieved, and think about why we did\nit.\nWhat Have We Achieved?\n3",
      "content_length": 1382,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "Events are simple dataclasses that define the data structures for inputs\nand internal messages within our system. This is quite powerful from a\nDDD standpoint, since events often translate really well into business\nlanguage (look up event storming if you haven’t already).\nHandlers are the way we react to events. They can call down to our\nmodel or call out to external services. We can define multiple handlers\nfor a single event if we want to. Handlers can also raise other events.\nThis allows us to be very granular about what a handler does and\nreally stick to the SRP.\nWhy Have We Achieved?\nOur ongoing objective with these architectural patterns is to try to have\nthe complexity of our application grow more slowly than its size.\nWhen we go all in on the message bus, as always we pay a price in\nterms of architectural complexity (see Table 9-1), but we buy\nourselves a pattern that can handle almost arbitrarily complex\nrequirements without needing any further conceptual or architectural\nchange to the way we do things.\nHere we’ve added quite a complicated use case (change quantity,\ndeallocate, start new transaction, reallocate, publish external\nnotification), but architecturally, there’s been no cost in terms of\ncomplexity. We’ve added new events, new handlers, and a new\nexternal adapter (for email), all of which are existing categories of\nthings in our architecture that we understand and know how to reason\nabout, and that are easy to explain to newcomers. Our moving parts",
      "content_length": 1490,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "each have one job, they’re connected to each other in well-defined\nways, and there are no unexpected side effects.\nTable 9-1. Whole app is a message bus: the trade-offs\nPros\nCons\nHandlers \nand \nservices \nare the \nsame thing, \nso that’s \nsimpler.\nWe have a \nnice data \nstructure \nfor inputs \nto the \nsystem.\nA message bus is still a slightly unpredictable way of \ndoing things from \na web point of view. You don’t know in advance \nwhen things are going to end.\nThere will be duplication of fields and structure \nbetween model objects and events, which will have a \nmaintenance cost. Adding a field to one usually \nmeans adding a field to at least \none of the others.\nNow, you may be wondering, where are those\nBatchQuantityChanged events going to come from? The answer is\nrevealed in a couple chapters’ time. But first, let’s talk about events\nversus commands.\n1  Event-based modeling is so popular that a practice called event storming has been\ndeveloped for facilitating event-based requirements gathering and domain model",
      "content_length": 1023,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "elaboration.\n2  If you’ve done a bit of reading about event-driven architectures, you may be thinking,\n“Some of these events sound more like commands!” Bear with us! We’re trying to\nintroduce one concept at a time. In the next chapter, we’ll introduce the distinction\nbetween commands and events.\n3  The “simple” implementation in this chapter essentially uses the messagebus.py module\nitself to implement the Singleton Pattern.",
      "content_length": 428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "Chapter 10. Commands and\nCommand Handler\nIn the previous chapter, we talked about using events as a way of\nrepresenting the inputs to our system, and we turned our application\ninto a message-processing machine.\nTo achieve that, we converted all our use-case functions to event\nhandlers. When the API receives a POST to create a new batch, it\nbuilds a new BatchCreated event and handles it as if it were an\ninternal event. This might feel counterintuitive. After all, the batch\nhasn’t been created yet; that’s why we called the API. We’re going to\nfix that conceptual wart by introducing commands and showing how\nthey can be handled by the same message bus but with slightly different\nrules.\nTIP\nThe code for this chapter is in the chapter_10_commands branch on GitHub:\ngit clone https://github.com/cosmicpython/code.git \ncd code \ngit checkout chapter_10_commands \n# or to code along, checkout the previous chapter: \ngit checkout chapter_09_all_messagebus\nCommands and Events",
      "content_length": 974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "Like events, commands are a type of message—instructions sent by\none part of a system to another. We usually represent commands with\ndumb data structures and can handle them in much the same way as\nevents.\nThe differences between commands and events, though, are important.\nCommands are sent by one actor to another specific actor with the\nexpectation that a particular thing will happen as a result. When we\npost a form to an API handler, we are sending a command. We name\ncommands with imperative mood verb phrases like “allocate stock” or\n“delay shipment.”\nCommands capture intent. They express our wish for the system to do\nsomething. As a result, when they fail, the sender needs to receive\nerror information.\nEvents are broadcast by an actor to all interested listeners. When we\npublish BatchQuantityChanged, we don’t know who’s going to pick\nit up. We name events with past-tense verb phrases like “order\nallocated to stock” or “shipment delayed.”\nWe often use events to spread the knowledge about successful\ncommands.\nEvents capture facts about things that happened in the past. Since we\ndon’t know who’s handling an event, senders should not care whether\nthe receivers succeeded or failed. Table 10-1 recaps the differences.",
      "content_length": 1233,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "Table 10-1. Events versus commands\nEvent\nCommand\nNamed\nPast tense\nImperative mood\nError handling\nFail independently\nFail noisily\nSent to\nAll listeners\nOne recipient\nWhat kinds of commands do we have in our system right now?\nPulling out some commands (src/allocation/domain/commands.py)\nclass Command:\n    pass\n@dataclass\nclass Allocate(Command):  \n    orderid: str\n    sku: str\n    qty: int\n@dataclass\nclass CreateBatch(Command):  \n    ref: str\n    sku: str\n    qty: int\n    eta: Optional[date] = None\n@dataclass\nclass ChangeBatchQuantity(Command):  \n    ref: str\n    qty: int\ncommands.Allocate will replace\nevents.AllocationRequired.",
      "content_length": 634,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "commands.CreateBatch will replace events.BatchCreated.\ncommands.ChangeBatchQuantity will replace\nevents.BatchQuantityChanged.\nDifferences in Exception Handling\nJust changing the names and verbs is all very well, but that won’t\nchange the behavior of our system. We want to treat events and\ncommands similarly, but not exactly the same. Let’s see how our\nmessage bus changes:\nDispatch events and commands differently\n(src/allocation/service_layer/messagebus.py)\nMessage = Union[commands.Command, events.Event]\ndef handle(message: Message, uow: unit_of_work.AbstractUnitOfWork):  \n    results = []\n    queue = [message]\n    while queue:\n        message = queue.pop(0)\n        if isinstance(message, events.Event):\n            handle_event(message, queue, uow)  \n        elif isinstance(message, commands.Command):\n            cmd_result = handle_command(message, queue, uow)  \n            results.append(cmd_result)\n        else:\n            raise Exception(f'{message} was not an Event or Command')\n    return results\nIt still has a main handle() entrypoint that takes a message, which\nmay be a command or an event.",
      "content_length": 1114,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "We dispatch events and commands to two different helper\nfunctions, shown next.\nHere’s how we handle events:\nEvents cannot interrupt the flow\n(src/allocation/service_layer/messagebus.py)\ndef handle_event(\n    event: events.Event,\n    queue: List[Message],\n    uow: unit_of_work.AbstractUnitOfWork\n):\n    for handler in EVENT_HANDLERS[type(event)]:  \n        try:\n            logger.debug('handling event %s with handler %s', event, handler)\n            handler(event, uow=uow)\n            queue.extend(uow.collect_new_events())\n        except Exception:\n            logger.exception('Exception handling event %s', event)\n            continue  \nEvents go to a dispatcher that can delegate to multiple handlers per\nevent.\nIt catches and logs errors but doesn’t let them interrupt message\nprocessing.\nAnd here’s how we do commands:\nCommands reraise exceptions\n(src/allocation/service_layer/messagebus.py)\ndef handle_command(\n    command: commands.Command,\n    queue: List[Message],",
      "content_length": 977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "uow: unit_of_work.AbstractUnitOfWork\n):\n    logger.debug('handling command %s', command)\n    try:\n        handler = COMMAND_HANDLERS[type(command)]  \n        result = handler(command, uow=uow)\n        queue.extend(uow.collect_new_events())\n        return result  \n    except Exception:\n        logger.exception('Exception handling command %s', command)\n        raise  \nThe command dispatcher expects just one handler per command.\nIf any errors are raised, they fail fast and will bubble up.\nreturn result is only temporary; as mentioned in “A Temporary\nUgly Hack: The Message Bus Has to Return Results”, it’s a\ntemporary hack to allow the message bus to return the batch\nreference for the API to use. We’ll fix this in Chapter 12.\nWe also change the single HANDLERS dict into different ones for\ncommands and events. Commands can have only one handler,\naccording to our convention:\nNew handlers dicts (src/allocation/service_layer/messagebus.py)\nEVENT_HANDLERS = { \n    events.OutOfStock: [handlers.send_out_of_stock_notification],\n}  # type: Dict[Type[events.Event], List[Callable]] \n \nCOMMAND_HANDLERS = { \n    commands.Allocate: handlers.allocate, \n    commands.CreateBatch: handlers.add_batch, \n    commands.ChangeBatchQuantity: handlers.change_batch_quantity,\n}  # type: Dict[Type[commands.Command], Callable]",
      "content_length": 1313,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "Discussion: Events, Commands, and\nError Handling\nMany developers get uncomfortable at this point and ask, “What\nhappens when an event fails to process? How am I supposed to make\nsure the system is in a consistent state?” If we manage to process half\nof the events during messagebus.handle before an out-of-memory\nerror kills our process, how do we mitigate problems caused by the\nlost messages?\nLet’s start with the worst case: we fail to handle an event, and the\nsystem is left in an inconsistent state. What kind of error would cause\nthis? Often in our systems we can end up in an inconsistent state when\nonly half an operation is completed.\nFor example, we could allocate three units of DESIRABLE_BEANBAG to\na customer’s order but somehow fail to reduce the amount of remaining\nstock. This would cause an inconsistent state: the three units of stock\nare both allocated and available, depending on how you look at it.\nLater, we might allocate those same beanbags to another customer,\ncausing a headache for customer support.\nIn our allocation service, though, we’ve already taken steps to prevent\nthat happening. We’ve carefully identified aggregates that act as\nconsistency boundaries, and we’ve introduced a UoW that manages the\natomic success or failure of an update to an aggregate.\nFor example, when we allocate stock to an order, our consistency\nboundary is the Product aggregate. This means that we can’t",
      "content_length": 1413,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "accidentally overallocate: either a particular order line is allocated to\nthe product, or it is not—there’s no room for inconsistent states.\nBy definition, we don’t require two aggregates to be immediately\nconsistent, so if we fail to process an event and update only a single\naggregate, our system can still be made eventually consistent. We\nshouldn’t violate any constraints of the system.\nWith this example in mind, we can better understand the reason for\nsplitting messages into commands and events. When a user wants to\nmake the system do something, we represent their request as a\ncommand. That command should modify a single aggregate and either\nsucceed or fail in totality. Any other bookkeeping, cleanup, and\nnotification we need to do can happen via an event. We don’t require\nthe event handlers to succeed in order for the command to be\nsuccessful.\nLet’s look at another example (from a different, imaginary projet) to\nsee why not.\nImagine we are building an ecommerce website that sells expensive\nluxury goods. Our marketing department wants to reward customers for\nrepeat visits. We will flag customers as VIPs after they make their\nthird purchase, and this will entitle them to priority treatment and\nspecial offers. Our acceptance criteria for this story reads as follows:\nGiven a customer with two orders in their history,\nWhen the customer places a third order,\nThen they should be flagged as a VIP. \n \nWhen a customer first becomes a VIP\nThen we should send them an email to congratulate them",
      "content_length": 1510,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "Using the techniques we’ve already discussed in this book, we decide\nthat we want to build a new History aggregate that records orders and\ncan raise domain events when rules are met. We will structure the code\nlike this:\nVIP customer (example code for a different project)\nclass History:  # Aggregate\n    def __init__(self, customer_id: int):\n        self.orders = set() # Set[HistoryEntry]\n        self.customer_id = customer_id\n    def record_order(self, order_id: str, order_amount: int): \n        entry = HistoryEntry(order_id, order_amount)\n        if entry in self.orders:\n            return\n        self.orders.add(entry)\n        if len(self.orders) == 3:\n            self.events.append(\n                CustomerBecameVIP(self.customer_id)\n            )\ndef create_order_from_basket(uow, cmd: CreateOrder): \n    with uow:\n        order = Order.from_basket(cmd.customer_id, cmd.basket_items)\n        uow.orders.add(order)\n        uow.commit() # raises OrderCreated\ndef update_customer_history(uow, event: OrderCreated): \n    with uow:\n        history = uow.order_history.get(event.customer_id)\n        history.record_order(event.order_id, event.order_amount)\n        uow.commit() # raises CustomerBecameVIP",
      "content_length": 1212,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "def congratulate_vip_customer(uow, event: CustomerBecameVip): \n    with uow:\n        customer = uow.customers.get(event.customer_id)\n        email.send(\n            customer.email_address,\n            f'Congratulations {customer.first_name}!'\n        )\nThe History aggregate captures the rules indicating when a\ncustomer becomes a VIP. This puts us in a good place to handle\nchanges when the rules become more complex in the future.\nOur first handler creates an order for the customer and raises a\ndomain event OrderCreated.\nOur second handler updates the History object to record that an\norder was created.\nFinally, we send an email to the customer when they become a VIP.\nUsing this code, we can gain some intuition about error handling in an\nevent-driven system.\nIn our current implementation, we raise events about an aggregate after\nwe persist our state to the database. What if we raised those events\nbefore we persisted, and committed all our changes at the same time?\nThat way, we could be sure that all the work was complete. Wouldn’t\nthat be safer?\nWhat happens, though, if the email server is slightly overloaded? If all\nthe work has to complete at the same time, a busy email server can\nstop us from taking money for orders.",
      "content_length": 1236,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "What happens if there is a bug in the implementation of the History\naggregate? Should we fail to take your money just because we can’t\nrecognize you as a VIP?\nBy separating out these concerns, we have made it possible for things\nto fail in isolation, which improves the overall reliability of the\nsystem. The only part of this code that has to complete is the command\nhandler that creates an order. This is the only part that a customer cares\nabout, and it’s the part that our business stakeholders should prioritize.\nNotice how we’ve deliberately aligned our transactional boundaries to\nthe start and end of the business processes. The names that we use in\nthe code match the jargon used by our business stakeholders, and the\nhandlers we’ve written match the steps of our natural language\nacceptance criteria. This concordance of names and structure helps us\nto reason about our systems as they grow larger and more complex.\nRecovering from Errors Synchronously\nHopefully we’ve convinced you that it’s OK for events to fail\nindependently from the commands that raised them. What should we\ndo, then, to make sure we can recover from errors when they\ninevitably occur?\nThe first thing we need is to know when an error has occurred, and for\nthat we usually rely on logs.\nLet’s look again at the handle_event method from our message bus:",
      "content_length": 1334,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "Current handle function\n(src/allocation/service_layer/messagebus.py)\ndef handle_event( \n    event: events.Event, \n    queue: List[Message], \n    uow: unit_of_work.AbstractUnitOfWork\n): \n    for handler in EVENT_HANDLERS[type(event)]: \n        try: \n            logger.debug('handling event %s with handler %s', event, handler) \n            handler(event, uow=uow) \n            queue.extend(uow.collect_new_events()) \n        except Exception: \n            logger.exception('Exception handling event %s', event) \n            continue\nWhen we handle a message in our system, the first thing we do is write\na log line to record what we’re about to do. For our\nCustomerBecameVIP use case, the logs might read as follows:\nHandling event CustomerBecameVIP(customer_id=12345) \nwith handler <function congratulate_vip_customer at 0x10ebc9a60>\nBecause we’ve chosen to use dataclasses for our message types, we\nget a neatly printed summary of the incoming data that we can copy and\npaste into a Python shell to re-create the object.\nWhen an error occurs, we can use the logged data to either reproduce\nthe problem in a unit test or replay the message into the system.\nManual replay works well for cases where we need to fix a bug before\nwe can re-process an event, but our systems will always experience\nsome background level of transient failure. This includes things like",
      "content_length": 1363,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "network hiccups, table deadlocks, and brief downtime caused by\ndeployments.\nFor most of those cases, we can recover elegantly by trying again. As\nthe proverb says, “If at first you don’t succeed, retry the operation with\nan exponentially increasing back-off period.”\nHandle with retry (src/allocation/service_layer/messagebus.py)\nfrom tenacity import Retrying, RetryError, stop_after_attempt, wait_exponential \n...\ndef handle_event(\n    event: events.Event,\n    queue: List[Message],\n    uow: unit_of_work.AbstractUnitOfWork\n):\n    for handler in EVENT_HANDLERS[type(event)]:\n        try:\n            for attempt in Retrying(  \n                stop=stop_after_attempt(3),\n                wait=wait_exponential()\n            ):\n                with attempt:\n                    logger.debug('handling event %s with handler %s', event, \nhandler)\n                    handler(event, uow=uow)\n                    queue.extend(uow.collect_new_events())\n        except RetryError as retry_failure:\n            logger.error(\n                'Failed to handle event %s times, giving up!,\n                retry_failure.last_attempt.attempt_number\n            )\n            continue",
      "content_length": 1171,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "Tenacity is a Python library that implements common patterns for\nretrying.\nHere we configure our message bus to retry operations up to three\ntimes, with an exponentially increasing wait between attempts.\nRetrying operations that might fail is probably the single best way to\nimprove the resilience of our software. Again, the Unit of Work and\nCommand Handler patterns mean that each attempt starts from a\nconsistent state and won’t leave things half-finished.\nWARNING\nAt some point, regardless of tenacity, we’ll have to give up trying to process the\nmessage. Building reliable systems with distributed messages is hard, and we have\nto skim over some tricky bits. There are pointers to more reference materials in\nthe epilogue.\nWrap-Up\nIn this book we decided to introduce the concept of events before the\nconcept of commands, but other guides often do it the other way\naround. Making explicit the requests that our system can respond to by\ngiving them a name and their own data structure is quite a fundamental\nthing to do. You’ll sometimes see people use the name Command\nHandler pattern to describe what we’re doing with Events,\nCommands, and Message Bus.\nTable 10-2 discusses some of the things you should think about before\nyou jump on board.",
      "content_length": 1247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "Table 10-2. Splitting commands and events: the trade-offs\nPros\nCons\nTreating commands and events \ndifferently helps us understand \nwhich things \nhave to succeed and which \nthings we can tidy up later.\nCreateBatch is definitely a \nless confusing name than Batc\nhCreated. We are \nbeing explicit about the intent \nof our users, and explicit is \nbetter than \nimplicit, right?\nThe semantic differences \nbetween commands and events \ncan be subtle. Expect \nbikeshedding arguments over the \ndifferences.\nWe’re expressly inviting failure. \nWe know that sometimes things \nwill break, and \nwe’re choosing to handle that by \nmaking the failures smaller and \nmore isolated. \nThis can make the system harder \nto reason about and requires \nbetter monitoring.\nIn Chapter 11 we’ll talk about using events as an integration pattern.",
      "content_length": 814,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "Chapter 11. Event-Driven\nArchitecture: Using Events to\nIntegrate Microservices\nIn the preceding chapter, we never actually spoke about how we would\nreceive the “batch quantity changed” events, or indeed, how we might\nnotify the outside world about reallocations.\nWe have a microservice with a web API, but what about other ways of\ntalking to other systems? How will we know if, say, a shipment is\ndelayed or the quantity is amended? How will we tell the warehouse\nsystem that an order has been allocated and needs to be sent to a\ncustomer?\nIn this chapter, we’d like to show how the events metaphor can be\nextended to encompass the way that we handle incoming and outgoing\nmessages from the system. Internally, the core of our application is\nnow a message processor. Let’s follow through on that so it becomes a\nmessage processor externally as well. As shown in Figure 11-1, our\napplication will receive events from external sources via an external\nmessage bus (we’ll use Redis pub/sub queues as an example) and\npublish its outputs, in the form of events, back there as well.",
      "content_length": 1075,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "Figure 11-1. Our application is a message processor\nTIP\nThe code for this chapter is in the chapter_11_external_events branch on GitHub:\ngit clone https://github.com/cosmicpython/code.git \ncd code \ngit checkout chapter_11_external_events \n# or to code along, checkout the previous chapter: \ngit checkout chapter_10_commands\nDistributed Ball of Mud, and Thinking in\nNouns\nBefore we get into that, let’s talk about the alternatives. We regularly\ntalk to engineers who are trying to build out a microservices\narchitecture. Often they are migrating from an existing application, and\ntheir first instinct is to split their system into nouns.\nWhat nouns have we introduced so far in our system? Well, we have\nbatches of stock, orders, products, and customers. So a naive attempt\nat breaking up the system might have looked like Figure 11-2 (notice\nthat we’ve named our system after a noun, Batches, instead of\nAllocation).",
      "content_length": 916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "Figure 11-2. Context diagram with noun-based services\nEach “thing” in our system has an associated service, which exposes\nan HTTP API.\nLet’s work through an example happy-path flow in Figure 11-3: our\nusers visit a website and can choose from products that are in stock.\nWhen they add an item to their basket, we will reserve some stock for\nthem. When an order is complete, we confirm the reservation, which\ncauses us to send dispatch instructions to the warehouse. Let’s also",
      "content_length": 476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "say, if this is the customer’s third order, we want to update the\ncustomer record to flag them as a VIP.\nFigure 11-3. Command flow 1\nWe can think of each of these steps as a command in our system:\nReserveStock, ConfirmReservation, DispatchGoods,\nMakeCustomerVIP, and so forth.\nThis style of architecture, where we create a microservice per\ndatabase table and treat our HTTP APIs as CRUD interfaces to anemic\nmodels, is the most common initial way for people to approach\nservice-oriented design.\nThis works fine for systems that are very simple, but it can quickly\ndegrade into a distributed ball of mud.",
      "content_length": 603,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "To see why, let’s consider another case. Sometimes, when stock\narrives at the warehouse, we discover that items have been water\ndamaged during transit. We can’t sell water-damaged sofas, so we\nhave to throw them away and request more stock from our partners. We\nalso need to update our stock model, and that might mean we need to\nreallocate a customer’s order.\nWhere does this logic go?\nWell, the Warehouse system knows that the stock has been damaged, so\nmaybe it should own this process, as shown in Figure 11-4.\nFigure 11-4. Command flow 2",
      "content_length": 542,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "This sort of works too, but now our dependency graph is a mess. To\nallocate stock, the Orders service drives the Batches system, which\ndrives Warehouse; but in order to handle problems at the warehouse,\nour Warehouse system drives Batches, which drives Orders.\nMultiply this by all the other workflows we need to provide, and you\ncan see how services quickly get tangled up.\nError Handling in Distributed Systems\n“Things break” is a universal law of software engineering. What\nhappens in our system when one of our requests fails? Let’s say that a\nnetwork error happens right after we take a user’s order for three\nMISBEGOTTEN-RUG, as shown in Figure 11-5.\nWe have two options here: we can place the order anyway and leave it\nunallocated, or we can refuse to take the order because the allocation\ncan’t be guaranteed. The failure state of our batches service has\nbubbled up and is affecting the reliability of our order service.\nWhen two things have to be changed together, we say that they are\ncoupled. We can think of this failure cascade as a kind of temporal\ncoupling: every part of the system has to work at the same time for any\npart of it to work. As the system gets bigger, there is an exponentially\nincreasing probability that some part is degraded.",
      "content_length": 1258,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "Figure 11-5. Command flow with error\nCONNASCENCE\nWe’re using the term coupling here, but there’s another way to describe the relationships\nbetween our systems. Connascence is a term used by some authors to describe the different\ntypes of coupling.\nConnascence isn’t bad, but some types of connascence are stronger than others. We want to\nhave strong connascence locally, as when two classes are closely related, but weak\nconnascence at a distance.\nIn our first example of a distributed ball of mud, we see Connascence of Execution: multiple\ncomponents need to know the correct order of work for an operation to be successful.\nWhen thinking about error conditions here, we’re talking about Connascence of Timing: multiple\nthings have to happen, one after another, for the operation to work.\nWhen we replace our RPC-style system with events, we replace both of these types of\nconnascence with a weaker type. That’s Connascence of Name: multiple components need to\nagree only on the name of an event and the names of fields it carries.\nWe can never completely avoid coupling, except by having our software not talk to any other\nsoftware. What we want is to avoid inappropriate coupling. Connascence provides a mental\nmodel for understanding the strength and type of coupling inherent in different architectural\nstyles. Read all about it at connascence.io.",
      "content_length": 1352,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "The Alternative: Temporal Decoupling\nUsing Asynchronous Messaging\nHow do we get appropriate coupling? We’ve already seen part of the\nanswer, which is that we should think in terms of verbs, not nouns. Our\ndomain model is about modeling a business process. It’s not a static\ndata model about a thing; it’s a model of a verb.\nSo instead of thinking about a system for orders and a system for\nbatches, we think about a system for ordering and a system for\nallocating, and so on.\nWhen we separate things this way, it’s a little easier to see which\nsystem should be responsible for what. When thinking about ordering,\nreally we want to make sure that when we place an order, the order is\nplaced. Everything else can happen later, so long as it happens.\nNOTE\nIf this sounds familiar, it should! Segregating responsibilities is the same process\nwe went through when designing our aggregates and commands.\nLike aggregates, microservices should be consistency boundaries.\nBetween two services, we can accept eventual consistency, and that\nmeans we don’t need to rely on synchronous calls. Each service\naccepts commands from the outside world and raises events to record\nthe result. Other services can listen to those events to trigger the next\nsteps in the workflow.",
      "content_length": 1257,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "To avoid the Distributed Ball of Mud anti-pattern, instead of\ntemporally coupled HTTP API calls, we want to use asynchronous\nmessaging to integrate our systems. We want our\nBatchQuantityChanged messages to come in as external messages\nfrom upstream systems, and we want our system to publish Allocated\nevents for downstream systems to listen to.\nWhy is this better? First, because things can fail independently, it’s\neasier to handle degraded behavior: we can still take orders if the\nallocation system is having a bad day.\nSecond, we’re reducing the strength of coupling between our systems.\nIf we need to change the order of operations or to introduce new steps\nin the process, we can do that locally.\nUsing a Redis Pub/Sub Channel for\nIntegration\nLet’s see how it will all work concretely. We’ll need some way of\ngetting events out of one system and into another, like our message bus,\nbut for services. This piece of infrastructure is often called a message\nbroker. The role of a message broker is to take messages from\npublishers and deliver them to subscribers.\nAt MADE.com, we use Event Store; Kafka or RabbitMQ are valid\nalternatives. A lightweight solution based on Redis pub/sub channels\ncan also work just fine, and because Redis is much more generally\nfamiliar to people, we thought we’d use it for this book.",
      "content_length": 1321,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "NOTE\nWe’re glossing over the complexity involved in choosing the right messaging\nplatform. Concerns like message ordering, failure handling, and idempotency all\nneed to be thought through. For a few pointers, see “Footguns”.\nOur new flow will look like Figure 11-6: Redis provides the\nBatchQuantityChanged event that kicks off the whole process, and\nour Allocated event is published back out to Redis again at the end.",
      "content_length": 418,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "Figure 11-6. Sequence diagram for reallocation flow\nTest-Driving It All Using an End-to-End\nTest\nHere’s how we might start with an end-to-end test. We can use our\nexisting API to create batches, and then we’ll test both inbound and\noutbound messages:\nAn end-to-end test for our pub/sub model\n(tests/e2e/test_external_events.py)\ndef test_change_batch_quantity_leading_to_reallocation():\n    # start with two batches and an order allocated to one of them  \n    orderid, sku = random_orderid(), random_sku()\n    earlier_batch, later_batch = random_batchref('old'), \nrandom_batchref('newer')\n    api_client.post_to_add_batch(earlier_batch, sku, qty=10, eta='2011-01-02')  \n    api_client.post_to_add_batch(later_batch, sku, qty=10, eta='2011-01-02')\n    response = api_client.post_to_allocate(orderid, sku, 10)  \n    assert response.json()['batchref'] == earlier_batch\n    subscription = redis_client.subscribe_to('line_allocated')  \n    # change quantity on allocated batch so it's less than our order  \n    redis_client.publish_message('change_batch_quantity', {  \n        'batchref': earlier_batch, 'qty': 5\n    })\n    # wait until we see a message saying the order has been reallocated  \n    messages = []\n    for attempt in Retrying(stop=stop_after_delay(3), reraise=True):  \n        with attempt:\n            message = subscription.get_message(timeout=1)\n            if message:",
      "content_length": 1380,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "messages.append(message)\n                print(messages)\n            data = json.loads(messages[-1]['data'])\n            assert data['orderid'] == orderid\n            assert data['batchref'] == later_batch\nYou can read the story of what’s going on in this test from the\ncomments: we want to send an event into the system that causes an\norder line to be reallocated, and we see that reallocation come out\nas an event in Redis too.\napi_client is a little helper that we refactored out to share\nbetween our two test types; it wraps our calls to requests.post.\nredis_client is another little test helper, the details of which\ndon’t really matter; its job is to be able to send and receive\nmessages from various Redis channels. We’ll use a channel called\nchange_batch_quantity to send in our request to change the\nquantity for a batch, and we’ll listen to another channel called\nline_allocated to look out for the expected reallocation.\nBecause of the asynchronous nature of the system under test, we\nneed to use the tenacity library again to add a retry loop—first,\nbecause it may take some time for our new line_allocated\nmessage to arrive, but also because it won’t be the only message\non that channel.\nRedis Is Another Thin Adapter Around Our\nMessage Bus\nOur Redis pub/sub listener (we call it an event consumer) is very\nmuch like Flask: it translates from the outside world to our events:\nSimple Redis message listener\n(src/allocation/entrypoints/redis_eventconsumer.py)",
      "content_length": 1470,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "r = redis.Redis(**config.get_redis_host_and_port())\ndef main():\n    orm.start_mappers()\n    pubsub = r.pubsub(ignore_subscribe_messages=True)\n    pubsub.subscribe('change_batch_quantity')  \n    for m in pubsub.listen():\n        handle_change_batch_quantity(m)\ndef handle_change_batch_quantity(m):\n    logging.debug('handling %s', m)\n    data = json.loads(m['data'])  \n    cmd = commands.ChangeBatchQuantity(ref=data['batchref'], qty=data['qty'])  \n    messagebus.handle(cmd, uow=unit_of_work.SqlAlchemyUnitOfWork())\nmain() subscribes us to the change_batch_quantity channel on\nload.\nOur main job as an entrypoint to the system is to deserialize JSON,\nconvert it to a Command, and pass it to the service layer—much as\nthe Flask adapter does.\nWe also build a new downstream adapter to do the opposite job—\nconverting domain events to public events:\nSimple Redis message publisher\n(src/allocation/adapters/redis_eventpublisher.py)\nr = redis.Redis(**config.get_redis_host_and_port())\ndef publish(channel, event: events.Event):  \n    logging.debug('publishing: channel=%s, event=%s', channel, event)\n    r.publish(channel, json.dumps(asdict(event)))",
      "content_length": 1144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "We take a hardcoded channel here, but you could also store a\nmapping between event classes/names and the appropriate channel,\nallowing one or more message types to go to different channels.\nOur New Outgoing Event\nHere’s what the Allocated event will look like:\nNew event (src/allocation/domain/events.py)\n@dataclass\nclass Allocated(Event): \n    orderid: str \n    sku: str \n    qty: int \n    batchref: str\nIt captures everything we need to know about an allocation: the details\nof the order line, and which batch it was allocated to.\nWe add it into our model’s allocate() method (having added a test\nfirst, naturally):\nProduct.allocate() emits new event to record what happened\n(src/allocation/domain/model.py)\nclass Product: \n    ... \n    def allocate(self, line: OrderLine) -> str: \n        ... \n \n            batch.allocate(line) \n            self.version_number += 1 \n            self.events.append(events.Allocated( \n                orderid=line.orderid, sku=line.sku, qty=line.qty,",
      "content_length": 986,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "batchref=batch.reference, \n            )) \n            return batch.reference\nThe handler for ChangeBatchQuantity already exists, so all we need\nto add is a handler that publishes the outgoing event:\nThe message bus grows\n(src/allocation/service_layer/messagebus.py)\nHANDLERS = { \n    events.Allocated: [handlers.publish_allocated_event], \n    events.OutOfStock: [handlers.send_out_of_stock_notification],\n}  # type: Dict[Type[events.Event], List[Callable]]\nPublishing the event uses our helper function from the Redis wrapper:\nPublish to Redis (src/allocation/service_layer/handlers.py)\ndef publish_allocated_event( \n        event: events.Allocated, uow: unit_of_work.AbstractUnitOfWork,\n): \n    redis_eventpublisher.publish('line_allocated', event)\nInternal Versus External Events\nIt’s a good idea to keep the distinction between internal and external\nevents clear. Some events may come from the outside, and some events\nmay get upgraded and published externally, but not all of them will.\nThis is particularly important if you get into event sourcing (very much\na topic for another book, though).",
      "content_length": 1099,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "TIP\nOutbound events are one of the places it’s important to apply validation. See\nAppendix E for some validation philosophy and examples.\nEXERCISE FOR THE READER\nA nice simple one for this chapter: make it so that the main allocate() use case can also be\ninvoked by an event on a Redis channel, as well as (or instead of) via the API.\nYou will likely want to add a new E2E test and feed through some changes into\nredis_eventconsumer.py.\nWrap-Up\nEvents can come from the outside, but they can also be published\nexternally—our publish handler converts an event to a message on a\nRedis channel. We use events to talk to the outside world. This kind of\ntemporal decoupling buys us a lot of flexibility in our application\nintegrations, but as always, it comes at a cost.\nEvent notification is nice because it implies a low level of\ncoupling, and is pretty simple to set up. It can become\nproblematic, however, if there really is a logical flow that runs\nover various event notifications...It can be hard to see such a flow\nas it’s not explicit in any program text....This can make it hard to\ndebug and modify.\n—Martin Fowler, “What do you mean by ‘Event-\nDriven’”\nTable 11-1 shows some trade-offs to think about.",
      "content_length": 1207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "Table 11-1. Event-based microservices integration: the trade-offs\nPros\nCons\nAvoids the distributed big ball \nof mud.\nServices are decoupled: it’s \neasier to change individual \nservices and add \nnew ones.\nThe overall flows of information \nare harder to see.\nEventual consistency is a new \nconcept to deal with.\nMessage reliability and choices \naround at-least-once versus at-\nmost-once delivery \nneed thinking through.\nMore generally, if you’re moving from a model of synchronous\nmessaging to an async one, you also open up a whole host of problems\nhaving to do with message reliability and eventual consistency. Read\non to “Footguns”.",
      "content_length": 634,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "Chapter 12. Command-Query\nResponsibility Segregation\n(CQRS)\nIn this chapter, we’re going to start with a fairly uncontroversial\ninsight: reads (queries) and writes (commands) are different, so they\nshould be treated differently (or have their responsibilities segregated,\nif you will). Then we’re going to push that insight as far as we can.\nIf you’re anything like Harry, this will all seem extreme at first, but\nhopefully we can make the argument that it’s not totally unreasonable.\nFigure 12-1 shows where we might end up.\nTIP\nThe code for this chapter is in the chapter_12_cqrs branch on GitHub.\ngit clone https://github.com/cosmicpython/code.git \ncd code \ngit checkout chapter_12_cqrs \n# or to code along, checkout the previous chapter: \ngit checkout chapter_11_external_events\nFirst, though, why bother?",
      "content_length": 809,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "Figure 12-1. Separating reads from writes\nDomain Models Are for Writing\nWe’ve spent a lot of time in this book talking about how to build\nsoftware that enforces the rules of our domain. These rules, or\nconstraints, will be different for every application, and they make up\nthe interesting core of our systems.\nIn this book, we’ve set explicit constraints like “You can’t allocate\nmore stock than is available,” as well as implicit constraints like\n“Each order line is allocated to a single batch.”\nWe wrote down these rules as unit tests at the beginning of the book:\nOur basic domain tests (tests/unit/test_batches.py)\ndef test_allocating_to_a_batch_reduces_the_available_quantity(): \n    batch = Batch(\"batch-001\", \"SMALL-TABLE\", qty=20, eta=date.today()) \n    line = OrderLine('order-ref', \"SMALL-TABLE\", 2) \n \n    batch.allocate(line) \n \n    assert batch.available_quantity == 18 \n \n... \n \ndef test_cannot_allocate_if_available_smaller_than_required(): \n    small_batch, large_line = make_batch_and_line(\"ELEGANT-LAMP\", 2, 20) \n    assert small_batch.can_allocate(large_line) is False\nTo apply these rules properly, we needed to ensure that operations\nwere consistent, and so we introduced patterns like Unit of Work and\nAggregate that help us commit small chunks of work.",
      "content_length": 1276,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "To communicate changes between those small chunks, we introduced\nthe Domain Events pattern so we can write rules like “When stock is\ndamaged or lost, adjust the available quantity on the batch, and\nreallocate orders if necessary.”\nAll of this complexity exists so we can enforce rules when we change\nthe state of our system. We’ve built a flexible set of tools for writing\ndata.\nWhat about reads, though?\nMost Users Aren’t Going to Buy Your\nFurniture\nAt MADE.com, we have a system very like the allocation service. In a\nbusy day, we might process one hundred orders in an hour, and we\nhave a big gnarly system for allocating stock to those orders.\nIn that same busy day, though, we might have one hundred product\nviews per second. Each time somebody visits a product page, or a\nproduct listing page, we need to figure out whether the product is still\nin stock and how long it will take us to deliver it.\nThe domain is the same—we’re concerned with batches of stock, and\ntheir arrival date, and the amount that’s still available—but the access\npattern is very different. For example, our customers won’t notice if\nthe query is a few seconds out of date, but if our allocate service is\ninconsistent, we’ll make a mess of their orders. We can take advantage",
      "content_length": 1254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "of this difference by making our reads eventually consistent in order\nto make them perform better.\nIS READ CONSISTENCY TRULY ATTAINABLE?\nThis idea of trading consistency against performance makes a lot of developers nervous at first,\nso let’s talk quickly about that.\nLet’s imagine that our “Get Available Stock” query is 30 seconds out of date when Bob visits the\npage for ASYMMETRICAL-DRESSER. Meanwhile, though, Harry has already bought the last item. When\nwe try to allocate Bob’s order, we’ll get a failure, and we’ll need to either cancel his order or buy\nmore stock and delay his delivery.\nPeople who’ve worked only with relational data stores get really nervous about this problem, but\nit’s worth considering two other scenarios to gain some perspective.\nFirst, let’s imagine that Bob and Harry both visit the page at the same time. Harry goes off to\nmake coffee, and by the time he returns, Bob has already bought the last dresser. When Harry\nplaces his order, we send it to the allocation service, and because there’s not enough stock, we\nhave to refund his payment or buy more stock and delay his delivery.\nAs soon as we render the product page, the data is already stale. This insight is key to\nunderstanding why reads can be safely inconsistent: we’ll always need to check the current state\nof our system when we come to allocate, because all distributed systems are inconsistent. As\nsoon as you have a web server and two customers, you have the potential for stale data.\nOK, let’s assume we solve that problem somehow: we magically build a totally consistent web\napplication where nobody ever sees stale data. This time Harry gets to the page first and buys\nhis dresser.\nUnfortunately for him, when the warehouse staff tries to dispatch his furniture, it falls off the forklift\nand smashes into a zillion pieces. Now what?\nThe only options are to either call Harry and refund his order or buy more stock and delay delivery.\nNo matter what we do, we’re always going to find that our software systems are inconsistent with\nreality, and so we’ll always need business processes to cope with these edge cases. It’s OK to\ntrade performance for consistency on the read side, because stale data is essentially\nunavoidable.\nWe can think of these requirements as forming two halves of a system:\nthe read side and the write side, shown in Table 12-1.",
      "content_length": 2353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "For the write side, our fancy domain architectural patterns help us to\nevolve our system over time, but the complexity we’ve built so far\ndoesn’t buy anything for reading data. The service layer, the unit of\nwork, and the clever domain model are just bloat.\nTable 12-1. Read versus write\nRead side\nWrite side\nBehavior\nSimple read\nComplex business logic\nCacheability\nHighly cacheable\nUncacheable\nConsistency\nCan be stale\nMust be transactionally consistent\nPost/Redirect/Get and CQS\nIf you do web development, you’re probably familiar with the\nPost/Redirect/Get pattern. In this technique, a web endpoint accepts an\nHTTP POST and responds with a redirect to see the result. For\nexample, we might accept a POST to /batches to create a new batch\nand redirect the user to /batches/123 to see their newly created batch.\nThis approach fixes the problems that arise when users refresh the\nresults page in their browser or try to bookmark a results page. In the\ncase of a refresh, it can lead to our users double-submitting data and\nthus buying two sofas when they needed only one. In the case of a\nbookmark, our hapless customers will end up with a broken page when\nthey try to GET a POST endpoint.",
      "content_length": 1190,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "Both these problems happen because we’re returning data in response\nto a write operation. Post/Redirect/Get sidesteps the issue by\nseparating the read and write phases of our operation.\nThis technique is a simple example of command-query separation\n(CQS). In CQS we follow one simple rule: functions should either\nmodify state or answer questions, but never both. This makes software\neasier to reason about: we should always be able to ask, “Are the\nlights on?” without flicking the light switch.\nNOTE\nWhen building APIs, we can apply the same design technique by returning a 201\nCreated, or a 202 Accepted, with a Location header containing the URI of our\nnew resources. What’s important here isn’t the status code we use but the logical\nseparation of work into a write phase and a query phase.\nAs you’ll see, we can use the CQS principle to make our systems\nfaster and more scalable, but first, let’s fix the CQS violation in our\nexisting code. Ages ago, we introduced an allocate endpoint that\ntakes an order and calls our service layer to allocate some stock. At\nthe end of the call, we return a 200 OK and the batch ID. That’s led to\nsome ugly design flaws so that we can get the data we need. Let’s\nchange it to return a simple OK message and instead provide a new\nread-only endpoint to retrieve allocation state:\nAPI test does a GET after the POST (tests/e2e/test_api.py)\n@pytest.mark.usefixtures('postgres_db')\n@pytest.mark.usefixtures('restart_api')",
      "content_length": 1458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "def test_happy_path_returns_202_and_batch_is_allocated(): \n    orderid = random_orderid() \n    sku, othersku = random_sku(), random_sku('other') \n    earlybatch = random_batchref(1) \n    laterbatch = random_batchref(2) \n    otherbatch = random_batchref(3) \n    api_client.post_to_add_batch(laterbatch, sku, 100, '2011-01-02') \n    api_client.post_to_add_batch(earlybatch, sku, 100, '2011-01-01') \n    api_client.post_to_add_batch(otherbatch, othersku, 100, None) \n \n    r = api_client.post_to_allocate(orderid, sku, qty=3) \n    assert r.status_code == 202 \n \n    r = api_client.get_allocation(orderid) \n    assert r.ok \n    assert r.json() == [ \n        {'sku': sku, 'batchref': earlybatch}, \n    ] \n \n \n@pytest.mark.usefixtures('postgres_db')\n@pytest.mark.usefixtures('restart_api')\ndef test_unhappy_path_returns_400_and_error_message(): \n    unknown_sku, orderid = random_sku(), random_orderid() \n    r = api_client.post_to_allocate( \n        orderid, unknown_sku, qty=20, expect_success=False, \n    ) \n    assert r.status_code == 400 \n    assert r.json()['message'] == f'Invalid sku {unknown_sku}' \n \n    r = api_client.get_allocation(orderid) \n    assert r.status_code == 404\nOK, what might the Flask app look like?\nEndpoint for viewing allocations\n(src/allocation/entrypoints/flask_app.py)\nfrom allocation import views\n...",
      "content_length": 1327,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "@app.route(\"/allocations/<orderid>\", methods=['GET'])\ndef allocations_view_endpoint(orderid):\n    uow = unit_of_work.SqlAlchemyUnitOfWork()\n    result = views.allocations(orderid, uow)  \n    if not result:\n        return 'not found', 404\n    return jsonify(result), 200\nAll right, a views.py, fair enough; we can keep read-only stuff in\nthere, and it’ll be a real views.py, not like Django’s, something that\nknows how to build read-only views of our data…\nHold On to Your Lunch, Folks\nHmm, so we can probably just add a list method to our existing\nrepository object:\nViews do…raw SQL? (src/allocation/views.py)\nfrom allocation.service_layer import unit_of_work \n \ndef allocations(orderid: str, uow: unit_of_work.SqlAlchemyUnitOfWork): \n    with uow: \n        results = list(uow.session.execute( \n            'SELECT ol.sku, b.reference' \n            ' FROM allocations AS a' \n            ' JOIN batches AS b ON a.batch_id = b.id' \n            ' JOIN order_lines AS ol ON a.orderline_id = ol.id' \n            ' WHERE ol.orderid = :orderid', \n            dict(orderid=orderid) \n        )) \n    return [{'sku': sku, 'batchref': batchref} for sku, batchref in results]\nExcuse me? Raw SQL?",
      "content_length": 1184,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "If you’re anything like Harry encountering this pattern for the first time,\nyou’ll be wondering what on earth Bob has been smoking. We’re hand-\nrolling our own SQL now, and converting database rows directly to\ndicts? After all the effort we put into building a nice domain model?\nAnd what about the Repository pattern? Isn’t that meant to be our\nabstraction around the database? Why don’t we reuse that?\nWell, let’s explore that seemingly simpler alternative first, and see\nwhat it looks like in practice.\nWe’ll still keep our view in a separate views.py module; enforcing a\nclear distinction between reads and writes in your application is still a\ngood idea. We apply command-query separation, and it’s easy to see\nwhich code modifies state (the event handlers) and which code just\nretrieves read-only state (the views).\nTIP\nSplitting out your read-only views from your state-modifying command and event\nhandlers is probably a good idea, even if you don’t want to go to full-blown\nCQRS.\nTesting CQRS Views\nBefore we get into exploring various options, let’s talk about testing.\nWhichever approaches you decide to go for, you’re probably going to\nneed at least one integration test. Something like this:\nAn integration test for a view (tests/integration/test_views.py)",
      "content_length": 1268,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "def test_allocations_view(sqlite_session_factory):\n    uow = unit_of_work.SqlAlchemyUnitOfWork(sqlite_session_factory)\n    messagebus.handle(commands.CreateBatch('sku1batch', 'sku1', 50, None), uow)  \n    messagebus.handle(commands.CreateBatch('sku2batch', 'sku2', 50, today), uow)\n    messagebus.handle(commands.Allocate('order1', 'sku1', 20), uow)\n    messagebus.handle(commands.Allocate('order1', 'sku2', 20), uow)\n    # add a spurious batch and order to make sure we're getting the right ones\n    messagebus.handle(commands.CreateBatch('sku1batch-later', 'sku1', 50, \ntoday), uow)\n    messagebus.handle(commands.Allocate('otherorder', 'sku1', 30), uow)\n    messagebus.handle(commands.Allocate('otherorder', 'sku2', 10), uow)\n    assert views.allocations('order1', uow) == [\n        {'sku': 'sku1', 'batchref': 'sku1batch'},\n        {'sku': 'sku2', 'batchref': 'sku2batch'},\n    ]\nWe do the setup for the integration test by using the public\nentrypoint to our application, the message bus. That keeps our tests\ndecoupled from any implementation/infrastructure details about\nhow things get stored.\n“Obvious” Alternative 1: Using the\nExisting Repository\nHow about adding a helper method to our products repository?\nA simple view that uses the repository (src/allocation/views.py)\nfrom allocation import unit_of_work\ndef allocations(orderid: str, uow: unit_of_work.AbstractUnitOfWork):\n    with uow:\n        products = uow.products.for_order(orderid=orderid)  \n        batches = [b for p in products for b in p.batches]",
      "content_length": 1519,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "return [\n            {'sku': b.sku, 'batchref': b.reference}\n            for b in batches\n            if orderid in b.orderids  \n        ]\nOur repository returns Product objects, and we need to find all\nthe products for the SKUs in a given order, so we’ll build a new\nhelper method called .for_order() on the repository.\nNow we have products but we actually want batch references, so\nwe get all the possible batches with a list comprehension.\nWe filter again to get just the batches for our specific order. That,\nin turn, relies on our Batch objects being able to tell us which\norder IDs it has allocated.\nWe implement that last using a .orderid property:\nAn arguably unnecessary property on our model\n(src/allocation/domain/model.py)\nclass Batch: \n    ... \n \n    @property \n    def orderids(self): \n        return {l.orderid for l in self._allocations}\nYou can start to see that reusing our existing repository and domain\nmodel classes is not as straightforward as you might have assumed.\nWe’ve had to add new helper methods to both, and we’re doing a\nbunch of looping and filtering in Python, which is work that would be\ndone much more efficiently by the database.",
      "content_length": 1166,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "So yes, on the plus side we’re reusing our existing abstractions, but on\nthe downside, it all feels quite clunky.\nYour Domain Model Is Not Optimized for\nRead Operations\nWhat we’re seeing here are the effects of having a domain model that\nis designed primarily for write operations, while our requirements for\nreads are often conceptually quite different.\nThis is the chin-stroking-architect’s justification for CQRS. As we’ve\nsaid before, a domain model is not a data model—we’re trying to\ncapture the way the business works: workflow, rules around state\nchanges, messages exchanged; concerns about how the system reacts to\nexternal events and user input. Most of this stuff is totally irrelevant\nfor read-only operations.\nTIP\nThis justification for CQRS is related to the justification for the Domain Model\npattern. If you’re building a simple CRUD app, reads and writes are going to be\nclosely related, so you don’t need a domain model or CQRS. But the more\ncomplex your domain, the more likely you are to need both.\nTo make a facile point, your domain classes will have multiple\nmethods for modifying state, and you won’t need any of them for read-\nonly operations.",
      "content_length": 1168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": "As the complexity of your domain model grows, you will find yourself\nmaking more and more choices about how to structure that model,\nwhich make it more and more awkward to use for read operations.\n“Obvious” Alternative 2: Using the ORM\nYou may be thinking, OK, if our repository is clunky, and working with\nProducts is clunky, then I can at least use my ORM and work with\nBatches. That’s what it’s for!\nA simple view that uses the ORM (src/allocation/views.py)\nfrom allocation import unit_of_work, model \n \ndef allocations(orderid: str, uow: unit_of_work.AbstractUnitOfWork): \n    with uow: \n        batches = uow.session.query(model.Batch).join( \n            model.OrderLine, model.Batch._allocations \n        ).filter( \n            model.OrderLine.orderid == orderid \n        ) \n        return [ \n            {'sku': b.sku, 'batchref': b.batchref} \n            for b in batches \n        ]\nBut is that actually any easier to write or understand than the raw SQL\nversion from the code example in “Hold On to Your Lunch, Folks”? It\nmay not look too bad up there, but we can tell you it took several\nattempts, and plenty of digging through the SQLAlchemy docs. SQL is\njust SQL.\nBut the ORM can also expose us to performance problems.",
      "content_length": 1231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "SELECT N+1 and Other Performance\nConsiderations\nThe so-called SELECT N+1 problem is a common performance\nproblem with ORMs: when retrieving a list of objects, your ORM will\noften perform an initial query to, say, get all the IDs of the objects it\nneeds, and then issue individual queries for each object to retrieve\ntheir attributes. This is especially likely if there are any foreign-key\nrelationships on your objects.\nNOTE\nIn all fairness, we should say that SQLAlchemy is quite good at avoiding the\nSELECT N+1 problem. It doesn’t display it in the preceding example, and you can\nrequest eager loading explicitly to avoid it when dealing with joined objects.\nBeyond SELECT N+1, you may have other reasons for wanting to\ndecouple the way you persist state changes from the way that you\nretrieve current state. A set of fully normalized relational tables is a\ngood way to make sure that write operations never cause data\ncorruption. But retrieving data using lots of joins can be slow. It’s\ncommon in such cases to add some denormalized views, build read\nreplicas, or even add caching layers.\nTime to Completely Jump the Shark\nOn that note: have we convinced you that our raw SQL version isn’t so\nweird as it first seemed? Perhaps we were exaggerating for effect?\nJust you wait.",
      "content_length": 1278,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "So, reasonable or not, that hardcoded SQL query is pretty ugly, right?\nWhat if we made it nicer…\nA much nicer query (src/allocation/views.py)\ndef allocations(orderid: str, uow: unit_of_work.SqlAlchemyUnitOfWork): \n    with uow: \n        results = list(uow.session.execute( \n            'SELECT sku, batchref FROM allocations_view WHERE orderid = \n:orderid', \n            dict(orderid=orderid) \n        )) \n        ...\n…by keeping a totally separate, denormalized data store for our\nview model?\nHee hee hee, no foreign keys, just strings, YOLO\n(src/allocation/adapters/orm.py)\nallocations_view = Table( \n    'allocations_view', metadata, \n    Column('orderid', String(255)), \n    Column('sku', String(255)), \n    Column('batchref', String(255)),\n)\nOK, nicer-looking SQL queries wouldn’t be a justification for anything\nreally, but building a denormalized copy of your data that’s optimized\nfor read operations isn’t uncommon, once you’ve reached the limits of\nwhat you can do with indexes.\nEven with well-tuned indexes, a relational database uses a lot of CPU\nto perform joins. The fastest queries will always be SELECT * from",
      "content_length": 1125,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "mytable WHERE key = :value.\nMore than raw speed, though, this approach buys us scale. When we’re\nwriting data to a relational database, we need to make sure that we get\na lock over the rows we’re changing so we don’t run into consistency\nproblems.\nIf multiple clients are changing data at the same time, we’ll have weird\nrace conditions. When we’re reading data, though, there’s no limit to\nthe number of clients that can concurrently execute. For this reason,\nread-only stores can be horizontally scaled out.\nTIP\nBecause read replicas can be inconsistent, there’s no limit to how many we can\nhave. If you’re struggling to scale a system with a complex data store, ask\nwhether you could build a simpler read model.\nKeeping the read model up to date is the challenge! Database views\n(materialized or otherwise) and triggers are a common solution, but\nthat limits you to your database. We’d like to show you how to reuse\nour event-driven architecture instead.\nUpdating a Read Model Table Using an Event\nHandler\nWe add a second handler to the Allocated event:",
      "content_length": 1056,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "Allocated event gets a new handler\n(src/allocation/service_layer/messagebus.py)\nEVENT_HANDLERS = { \n    events.Allocated: [ \n        handlers.publish_allocated_event, \n        handlers.add_allocation_to_read_model \n    ],\nHere’s what our update-view-model code looks like:\nUpdate on allocation (src/allocation/service_layer/handlers.py)\ndef add_allocation_to_read_model( \n        event: events.Allocated, uow: unit_of_work.SqlAlchemyUnitOfWork,\n): \n    with uow: \n        uow.session.execute( \n            'INSERT INTO allocations_view (orderid, sku, batchref)' \n            ' VALUES (:orderid, :sku, :batchref)', \n            dict(orderid=event.orderid, sku=event.sku, batchref=event.batchref) \n        ) \n        uow.commit()\nBelieve it or not, that will pretty much work! And it will work against\nthe exact same integration tests as the rest of our options.\nOK, you’ll also need to handle Deallocated:\nA second listener for read model updates\nevents.Deallocated: [ \n    handlers.remove_allocation_from_read_model, \n    handlers.reallocate\n], \n \n...",
      "content_length": 1051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "def remove_allocation_from_read_model( \n        event: events.Deallocated, uow: unit_of_work.SqlAlchemyUnitOfWork,\n): \n    with uow: \n        uow.session.execute( \n            'DELETE FROM allocations_view ' \n            ' WHERE orderid = :orderid AND sku = :sku',\nFigure 12-2 shows the flow across the two requests.",
      "content_length": 316,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "Figure 12-2. Sequence diagram for read model\nIn Figure 12-2, you can see two transactions in the POST/write\noperation, one to update the write model and one to update the read\nmodel, which the GET/read operation can use.\nREBUILDING FROM SCRATCH\n“What happens when it breaks?” should be the first question we ask as engineers.\nHow do we deal with a view model that hasn’t been updated because of a bug or temporary\noutage? Well, this is just another case where events and commands can fail independently.\nIf we never updated the view model, and the ASYMMETRICAL-DRESSER was forever in stock, that\nwould be annoying for customers, but the allocate service would still fail, and we’d take action to\nfix the problem.\nRebuilding a view model is easy, though. Since we’re using a service layer to update our view\nmodel, we can write a tool that does the following:\nQueries the current state of the write side to work out what’s currently allocated\nCalls the add_allocate_to_read_model handler for each allocated item\nWe can use this technique to create entirely new read models from historical data.\nChanging Our Read Model\nImplementation Is Easy\nLet’s see the flexibility that our event-driven model buys us in action,\nby seeing what happens if we ever decide we want to implement a\nread model by using a totally separate storage engine, Redis.\nJust watch:\nHandlers update a Redis read model\n(src/allocation/service_layer/handlers.py)",
      "content_length": 1429,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "def add_allocation_to_read_model(event: events.Allocated, _): \n    redis_eventpublisher.update_readmodel(event.orderid, event.sku, \nevent.batchref) \n \ndef remove_allocation_from_read_model(event: events.Deallocated, _): \n    redis_eventpublisher.update_readmodel(event.orderid, event.sku, None)\nThe helpers in our Redis module are one-liners:\nRedis read model read and update\n(src/allocation/adapters/redis_eventpublisher.py)\ndef update_readmodel(orderid, sku, batchref): \n    r.hset(orderid, sku, batchref) \n \n \ndef get_readmodel(orderid): \n    return r.hgetall(orderid)\n(Maybe the name redis_eventpublisher.py is a misnomer now, but you\nget the idea.)\nAnd the view itself changes very slightly to adapt to its new backend:\nView adapted to Redis (src/allocation/views.py)\ndef allocations(orderid): \n    batches = redis_eventpublisher.get_readmodel(orderid) \n    return [ \n        {'batchref': b.decode(), 'sku': s.decode()} \n        for s, b in batches.items() \n    ]\nAnd the exact same integration tests that we had before still pass,\nbecause they are written at a level of abstraction that’s decoupled from",
      "content_length": 1109,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "the implementation: setup puts messages on the message bus, and the\nassertions are against our view.\nTIP\nEvent handlers are a great way to manage updates to a read model, if you decide\nyou need one. They also make it easy to change the implementation of that read\nmodel at a later date.\nEXERCISE FOR THE READER\nImplement another view, this time to show the allocation for a single order line.\nHere the trade-offs between using hardcoded SQL versus going via a repository should be much\nmore blurry. Try a few versions (maybe including going to Redis), and see which you prefer.\nWrap-Up\nTable 12-2 proposes some pros and cons for each of our options.\nAs it happens, the allocation service at MADE.com does use “full-\nblown” CQRS, with a read model stored in Redis, and even a second\nlayer of cache provided by Varnish. But its use cases are quite a bit\ndifferent from what we’ve shown here. For the kind of allocation\nservice we’re building, it seems unlikely that you’d need to use a\nseparate read model and event handlers for updating it.\nBut as your domain model becomes richer and more complex, a\nsimplified read model become ever more compelling.",
      "content_length": 1150,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "Table 12-2. Trade-offs of various view model options\nOption\nPros\nCons\nJust use \nrepositories\nSimple, consistent approach.\nExpect performance issues with \ncomplex query patterns.\nUse custom \nqueries with \nyour ORM\nAllows reuse of DB \nconfiguration and model \ndefinitions.\nAdds another query language \nwith its own quirks and syntax.\nUse hand-\nrolled SQL\nOffers fine control over \nperformance with a standard \nquery syntax.\nChanges to DB schema have to \nbe made to your hand-rolled \nqueries and your \n  ORM definitions. Highly \nnormalized schemas may still \nhave performance \n  limitations.\nCreate \nseparate read \nstores with \nevents\nRead-only copies are easy to \nscale out. Views can be \nconstructed when data \n  changes so that queries are as \nsimple as possible.\nComplex technique. Harry will \nbe forever suspicious of your \ntastes and \n  motives.\nOften, your read operations will be acting on the same conceptual\nobjects as your write model, so using the ORM, adding some read\nmethods to your repositories, and using domain model classes for your\nread operations is just fine.\nIn our book example, the read operations act on quite different\nconceptual entities to our domain model. The allocation service thinks\nin terms of Batches for a single SKU, but users care about allocations\nfor a whole order, with multiple SKUs, so using the ORM ends up\nbeing a little awkward. We’d be quite tempted to go with the raw-SQL\nview we showed right at the beginning of the chapter.",
      "content_length": 1471,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "On that note, let’s sally forth into our final chapter.",
      "content_length": 55,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": "Chapter 13. Dependency\nInjection (and Bootstrapping)\nDependency injection (DI) is regarded with suspicion in the Python\nworld. And we’ve managed just fine without it so far in the example\ncode for this book!\nIn this chapter, we’ll explore some of the pain points in our code that\nlead us to consider using DI, and we’ll present some options for how\nto do it, leaving it to you to pick which you think is most Pythonic.\nWe’ll also add a new component to our architecture called\nbootstrap.py; it will be in charge of dependency injection, as well as\nsome other initialization stuff that we often need. We’ll explain why\nthis sort of thing is called a composition root in OO languages, and\nwhy bootstrap script is just fine for our purposes.\nFigure 13-1 shows what our app looks like without a bootstrapper: the\nentrypoints do a lot of initialization and passing around of our main\ndependency, the UoW.\nTIP\nIf you haven’t already, it’s worth reading Chapter 3 before continuing with this\nchapter, particularly the discussion of functional versus object-oriented\ndependency management.",
      "content_length": 1081,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": "Figure 13-1. Without bootstrap: entrypoints do a lot\nTIP\nThe code for this chapter is in the chapter_13_dependency_injection branch on\nGitHub:\ngit clone https://github.com/cosmicpython/code.git \ncd code \ngit checkout chapter_13_dependency_injection \n# or to code along, checkout the previous chapter: \ngit checkout chapter_12_cqrs\nFigure 13-2 shows our bootstrapper taking over those responsibilities.",
      "content_length": 401,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": "Figure 13-2. Bootstrap takes care of all that in one place\nImplicit Versus Explicit Dependencies\nDepending on your particular brain type, you may have a slight feeling\nof unease at the back of your mind at this point. Let’s bring it out into\nthe open. We’ve shown you two ways of managing dependencies and\ntesting them.\nFor our database dependency, we’ve built a careful framework of\nexplicit dependencies and easy options for overriding them in tests.\nOur main handler functions declare an explicit dependency on the\nUoW:\nOur handlers have an explicit dependency on the UoW\n(src/allocation/service_layer/handlers.py)\ndef allocate( \n        cmd: commands.Allocate, uow: unit_of_work.AbstractUnitOfWork\n):\nAnd that makes it easy to swap in a fake UoW in our service-layer\ntests:\nService-layer tests against a fake UoW: (tests/unit/test_services.py)\n    uow = FakeUnitOfWork() \n    messagebus.handle([...], uow)\nThe UoW itself declares an explicit dependency on the session factory:",
      "content_length": 980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": "The UoW depends on a session factory\n(src/allocation/service_layer/unit_of_work.py)\nclass SqlAlchemyUnitOfWork(AbstractUnitOfWork): \n \n    def __init__(self, session_factory=DEFAULT_SESSION_FACTORY): \n        self.session_factory = session_factory \n        ...\nWe take advantage of it in our integration tests to be able to sometimes\nuse SQLite instead of Postgres:\nIntegration tests against a different DB\n(tests/integration/test_uow.py)\ndef test_rolls_back_uncommitted_work_by_default(sqlite_session_factory):\n    uow = unit_of_work.SqlAlchemyUnitOfWork(sqlite_session_factory)  \nIntegration tests swap out the default Postgres session_factory\nfor a SQLite one.\nAren’t Explicit Dependencies Totally\nWeird and Java-y?\nIf you’re used to the way things normally happen in Python, you’ll be\nthinking all this is a bit weird. The standard way to do things is to\ndeclare our dependency implicitly by simply importing it, and then if\nwe ever need to change it for tests, we can monkeypatch, as is Right\nand True in dynamic languages:\nEmail sending as a normal import-based dependency\n(src/allocation/service_layer/handlers.py)",
      "content_length": 1121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": "from allocation.adapters import email, redis_eventpublisher  \n...\ndef send_out_of_stock_notification(\n        event: events.OutOfStock, uow: unit_of_work.AbstractUnitOfWork,\n):\n    email.send(  \n        'stock@made.com',\n        f'Out of stock for {event.sku}',\n    )\nHardcoded import\nCalls specific email sender directly\nWhy pollute our application code with unnecessary arguments just for\nthe sake of our tests? mock.patch makes monkeypatching nice and\neasy:\nmock dot patch, thank you Michael Foord\n(tests/unit/test_handlers.py)\n    with mock.patch(\"allocation.adapters.email.send\") as mock_send_mail: \n        ...\nThe trouble is that we’ve made it look easy because our toy example\ndoesn’t send real email (email.send_mail just does a print), but in\nreal life, you’d end up having to call mock.patch for every single test\nthat might cause an out-of-stock notification. If you’ve worked on\ncodebases with lots of mocks used to prevent unwanted side effects,\nyou’ll know how annoying that mocky boilerplate gets.\nAnd you’ll know that mocks tightly couple us to the implementation.\nBy choosing to monkeypatch email.send_mail, we are tied to doing",
      "content_length": 1146,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": "import email, and if we ever want to do from email import\nsend_mail, a trivial refactor, we’d have to change all our mocks.\nSo it’s a trade-off. Yes, declaring explicit dependencies is\nunnecessary, strictly speaking, and using them would make our\napplication code marginally more complex. But in return, we’d get\ntests that are easier to write and manage.\nOn top of that, declaring an explicit dependency is an example of the\ndependency inversion principle—rather than having an (implicit)\ndependency on a specific detail, we have an (explicit) dependency on\nan abstraction:\nExplicit is better than implicit.\n—The Zen of Python\nThe explicit dependency is more abstract\n(src/allocation/service_layer/handlers.py)\ndef send_out_of_stock_notification( \n        event: events.OutOfStock, send_mail: Callable,\n): \n    send_mail( \n        'stock@made.com', \n        f'Out of stock for {event.sku}', \n    )\nBut if we do change to declaring all these dependencies explicitly,\nwho will inject them, and how? So far, we’ve really been dealing with\nonly passing the UoW around: our tests use FakeUnitOfWork, while\nFlask and Redis eventconsumer entrypoints use the real UoW, and the",
      "content_length": 1169,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": "message bus passes them onto our command handlers. If we add real\nand fake email classes, who will create them and pass them on?\nThat’s extra (duplicated) cruft for Flask, Redis, and our tests.\nMoreover, putting all the responsibility for passing dependencies to\nthe right handler onto the message bus feels like a violation of the SRP.\nInstead, we’ll reach for a pattern called Composition Root (a\nbootstrap script to you and me),  and we’ll do a bit of “manual DI”\n(dependency injection without a framework). See Figure 13-3.\n1\n2",
      "content_length": 531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": "Enis\nFal\n\ntil\n\n|\navd th cue epee iti\nDonte | tests ue es red el sre)\n\npasted anders to\n\nAipthes vat commands ined hands\n|",
      "content_length": 121,
      "extraction_method": "OCR"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": "Figure 13-3. Bootstrapper between entrypoints and message bus\nPreparing Handlers: Manual DI with\nClosures and Partials\nOne way to turn a function with dependencies into one that’s ready to\nbe called later with those dependencies already injected is to use\nclosures or partial functions to compose the function with its\ndependencies:\nExamples of DI using closures or partial functions\n# existing allocate function, with abstract uow dependency\ndef allocate(\n        cmd: commands.Allocate, uow: unit_of_work.AbstractUnitOfWork\n):\n    line = OrderLine(cmd.orderid, cmd.sku, cmd.qty)\n    with uow:\n        ...\n# bootstrap script prepares actual UoW\ndef bootstrap(..):\n    uow = unit_of_work.SqlAlchemyUnitOfWork()\n    # prepare a version of the allocate fn with UoW dependency captured in a \nclosure\n    allocate_composed = lambda cmd: allocate(cmd, uow)\n    # or, equivalently (this gets you a nicer stack trace)\n    def allocate_composed(cmd):\n        return allocate(cmd, uow)\n    # alternatively with a partial\n    import functools\n    allocate_composed = functools.partial(allocate, uow=uow)  \n# later at runtime, we can call the partial function, and it will have",
      "content_length": 1166,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": "# the UoW already bound\nallocate_composed(cmd)\nThe difference between closures (lambdas or named functions) and\nfunctools.partial is that the former use late binding of\nvariables, which can be a source of confusion if any of the\ndependencies are mutable.\nHere’s the same pattern again for the\nsend_out_of_stock_notification() handler, which has different\ndependencies:\nAnother closure and partial functions example\ndef send_out_of_stock_notification( \n        event: events.OutOfStock, send_mail: Callable,\n): \n    send_mail( \n        'stock@made.com', \n        ... \n \n \n# prepare a version of the send_out_of_stock_notification with dependencies\nsosn_composed  = lambda event: send_out_of_stock_notification(event, \nemail.send_mail) \n \n...\n# later, at runtime:\nsosn_composed(event)  # will have email.send_mail already injected in\nAn Alternative Using Classes\nClosures and partial functions will feel familiar to people who’ve\ndone a bit of functional programming. Here’s an alternative using\nclasses, which may appeal to others. It requires rewriting all our\nhandler functions as classes, though:",
      "content_length": 1098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": null,
      "content": "DI using classes\n# we replace the old `def allocate(cmd, uow)` with:\nclass AllocateHandler:\n    def __init__(self, uow: unit_of_work.AbstractUnitOfWork):  \n        self.uow = uow\n    def __call__(self, cmd: commands.Allocate):  \n        line = OrderLine(cmd.orderid, cmd.sku, cmd.qty)\n        with self.uow:\n            # rest of handler method as before\n            ...\n# bootstrap script prepares actual UoW\nuow = unit_of_work.SqlAlchemyUnitOfWork()\n# then prepares a version of the allocate fn with dependencies already injected\nallocate = AllocateHandler(uow)\n...\n# later at runtime, we can call the handler instance, and it will have\n# the UoW already injected\nallocate(cmd)\nThe class is designed to produce a callable function, so it has a\ncall method.\nBut we use the init to declare the dependencies it requires. This\nsort of thing will feel familiar if you’ve ever made class-based\ndescriptors, or a class-based context manager that takes arguments.\nUse whichever you and your team feel more comfortable with.\nA Bootstrap Script\nWe want our bootstrap script to do the following:",
      "content_length": 1086,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": null,
      "content": "1. Declare default dependencies but allow us to override them\n2. Do the “init” stuff that we need to get our app started\n3. Inject all the dependencies into our handlers\n4. Give us back the core object for our app, the message bus\nHere’s a first cut:\nA bootstrap function (src/allocation/bootstrap.py)\ndef bootstrap(\n    start_orm: bool = True,  \n    uow: unit_of_work.AbstractUnitOfWork = unit_of_work.SqlAlchemyUnitOfWork(),  \n    send_mail: Callable = email.send,\n    publish: Callable = redis_eventpublisher.publish,\n) -> messagebus.MessageBus:\n    if start_orm:\n        orm.start_mappers()  \n    dependencies = {'uow': uow, 'send_mail': send_mail, 'publish': publish}\n    injected_event_handlers = {  \n        event_type: [\n            inject_dependencies(handler, dependencies)\n            for handler in event_handlers\n        ]\n        for event_type, event_handlers in handlers.EVENT_HANDLERS.items()\n    }\n    injected_command_handlers = {  \n        command_type: inject_dependencies(handler, dependencies)\n        for command_type, handler in handlers.COMMAND_HANDLERS.items()\n    }\n    return messagebus.MessageBus(  \n        uow=uow,\n        event_handlers=injected_event_handlers,",
      "content_length": 1194,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": null,
      "content": "command_handlers=injected_command_handlers,\n    )\norm.start_mappers() is our example of initialization work that\nneeds to be done once at the beginning of an app. We also see\nthings like setting up the logging module.\nWe can use the argument defaults to define what the\nnormal/production defaults are. It’s nice to have them in a single\nplace, but sometimes dependencies have some side effects at\nconstruction time, in which case you might prefer to default them to\nNone instead.\nWe build up our injected versions of the handler mappings by using\na function called inject_dependencies(), which we’ll show\nnext.\nWe return a configured message bus ready for use.\nHere’s how we inject dependencies into a handler function by\ninspecting it:\nDI by inspecting function signatures (src/allocation/bootstrap.py)\ndef inject_dependencies(handler, dependencies):\n    params = inspect.signature(handler).parameters  \n    deps = {\n        name: dependency\n        for name, dependency in dependencies.items()  \n        if name in params\n    }\n    return lambda message: handler(message, **deps)  \nWe inspect our command/event handler’s arguments.\nWe match them by name to our dependencies.",
      "content_length": 1176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": null,
      "content": "We inject them as kwargs to produce a partial.",
      "content_length": 46,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": null,
      "content": "EVEN-MORE-MANUAL DI WITH LESS MAGIC\nIf you’re finding the preceding inspect code a little harder to grok, this even simpler version may\nappeal to you.\nHarry wrote the code for inject_dependencies() as a first cut of how to do “manual” dependency\ninjection, and when he saw it, Bob accused him of overengineering and writing his own DI\nframework.\nIt honestly didn’t even occur to Harry that you could do it any more plainly, but you can, like this:\nManually creating partial functions inline (src/allocation/bootstrap.py)\n    injected_event_handlers = { \n        events.Allocated: [ \n            lambda e: handlers.publish_allocated_event(e, publish), \n            lambda e: handlers.add_allocation_to_read_model(e, uow), \n        ], \n        events.Deallocated: [ \n            lambda e: handlers.remove_allocation_from_read_model(e, uow), \n            lambda e: handlers.reallocate(e, uow), \n        ], \n        events.OutOfStock: [ \n            lambda e: handlers.send_out_of_stock_notification(e, send_mail) \n        ] \n    } \n    injected_command_handlers = { \n        commands.Allocate: lambda c: handlers.allocate(c, uow), \n        commands.CreateBatch: \\ \n            lambda c: handlers.add_batch(c, uow), \n        commands.ChangeBatchQuantity: \\ \n            lambda c: handlers.change_batch_quantity(c, uow), \n    }\nHarry says he couldn’t even imagine writing out that many lines of code and having to look up that\nmany function arguments manually. This is a perfectly viable solution, though, since it’s only one\nline of code or so per handler you add, and thus not a massive maintenance burden even if you\nhave dozens of handlers.\nOur app is structured in such a way that we always want to do dependency injection in only one\nplace, the handler functions, so this super-manual solution and Harry’s inspect()-based one\nwill both work fine.\nIf you find yourself wanting to do DI in more things and at different times, or if you ever get into\ndependency chains (in which your dependencies have their own dependencies, and so on), you\nmay get some mileage out of a “real” DI framework.\nAt MADE, we’ve used Inject in a few places, and it’s fine, although it makes Pylint unhappy. You\nmight also check out Punq, as written by Bob himself, or the DRY-Python crew’s dependencies.",
      "content_length": 2280,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": null,
      "content": "Message Bus Is Given Handlers at\nRuntime\nOur message bus will no longer be static; it needs to have the already-\ninjected handlers given to it. So we turn it from being a module into a\nconfigurable class:\nMessageBus as a class (src/allocation/service_layer/messagebus.py)\nclass MessageBus:  \n    def __init__(\n        self,\n        uow: unit_of_work.AbstractUnitOfWork,\n        event_handlers: Dict[Type[events.Event], List[Callable]],  \n        command_handlers: Dict[Type[commands.Command], Callable],  \n    ):\n        self.uow = uow\n        self.event_handlers = event_handlers\n        self.command_handlers = command_handlers\n    def handle(self, message: Message):  \n        self.queue = [message]  \n        while self.queue:\n            message = self.queue.pop(0)\n            if isinstance(message, events.Event):\n                self.handle_event(message)\n            elif isinstance(message, commands.Command):\n                self.handle_command(message)\n            else:\n                raise Exception(f'{message} was not an Event or Command')\nThe message bus becomes a class…\n…which is given its already-dependency-injected handlers.",
      "content_length": 1147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": null,
      "content": "The main handle() function is substantially the same, with just a\nfew attributes and methods moved onto self.\nUsing self.queue like this is not thread-safe, which might be a\nproblem if you’re using threads, because the bus instance is global\nin the Flask app context as we’ve written it. Just something to\nwatch out for.\nWhat else changes in the bus?\nEvent and command handler logic stays the same\n(src/allocation/service_layer/messagebus.py)\n    def handle_event(self, event: events.Event):\n        for handler in self.event_handlers[type(event)]:  \n            try:\n                logger.debug('handling event %s with handler %s', event, \nhandler)\n                handler(event)  \n                self.queue.extend(self.uow.collect_new_events())\n            except Exception:\n                logger.exception('Exception handling event %s', event)\n                continue\n    def handle_command(self, command: commands.Command):\n        logger.debug('handling command %s', command)\n        try:\n            handler = self.command_handlers[type(command)]  \n            handler(command)  \n            self.queue.extend(self.uow.collect_new_events())\n        except Exception:\n            logger.exception('Exception handling command %s', command)\n            raise\nhandle_event and handle_command are substantially the same,\nbut instead of indexing into a static EVENT_HANDLERS or",
      "content_length": 1381,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": null,
      "content": "COMMAND_HANDLERS dict, they use the versions on self.\nInstead of passing a UoW into the handler, we expect the handlers\nto already have all their dependencies, so all they need is a single\nargument, the specific event or command.\nUsing Bootstrap in Our Entrypoints\nIn our application’s entrypoints, we now just call\nbootstrap.bootstrap() and get a message bus that’s ready to go,\nrather than configuring a UoW and the rest of it:\nFlask calls bootstrap (src/allocation/entrypoints/flask_app.py)\n-from allocation import views \n+from allocation import bootstrap, views \n app = Flask(__name__) \n-orm.start_mappers()  \n+bus = bootstrap.bootstrap() \n @app.route(\"/add_batch\", methods=['POST']) \n@@ -19,8 +16,7 @@ def add_batch(): \n     cmd = commands.CreateBatch( \n         request.json['ref'], request.json['sku'], request.json['qty'], eta, \n     ) \n-    uow = unit_of_work.SqlAlchemyUnitOfWork()  \n-    messagebus.handle(cmd, uow) \n+    bus.handle(cmd)  \n     return 'OK', 201\nWe no longer need to call start_orm(); the bootstrap script’s\ninitialization stages will do that.\nWe no longer need to explicitly build a particular type of UoW; the\nbootstrap script defaults take care of it.",
      "content_length": 1181,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": null,
      "content": "And our message bus is now a specific instance rather than the\nglobal module.\nInitializing DI in Our Tests\nIn tests, we can use bootstrap.bootstrap() with overridden\ndefaults to get a custom message bus. Here’s an example in an\nintegration test:\nOverriding bootstrap defaults (tests/integration/test_views.py)\n@pytest.fixture\ndef sqlite_bus(sqlite_session_factory):\n    bus = bootstrap.bootstrap(\n        start_orm=True,  \n        uow=unit_of_work.SqlAlchemyUnitOfWork(sqlite_session_factory),  \n        send_mail=lambda *args: None,  \n        publish=lambda *args: None,  \n    )\n    yield bus\n    clear_mappers()\ndef test_allocations_view(sqlite_bus):\n    sqlite_bus.handle(commands.CreateBatch('sku1batch', 'sku1', 50, None))\n    sqlite_bus.handle(commands.CreateBatch('sku2batch', 'sku2', 50, \ndate.today()))\n    ...\n    assert views.allocations('order1', sqlite_bus.uow) == [\n        {'sku': 'sku1', 'batchref': 'sku1batch'},\n        {'sku': 'sku2', 'batchref': 'sku2batch'},\n    ]\nWe do still want to start the ORM…\n…because we’re going to use a real UoW, albeit with an in-\nmemory database.\n3",
      "content_length": 1098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": null,
      "content": "But we don’t need to send email or publish, so we make those\nnoops.\nIn our unit tests, in contrast, we can reuse our FakeUnitOfWork:\nBootstrap in unit test (tests/unit/test_handlers.py)\ndef bootstrap_test_app():\n    return bootstrap.bootstrap(\n        start_orm=False,  \n        uow=FakeUnitOfWork(),  \n        send_mail=lambda *args: None,  \n        publish=lambda *args: None,  \n    )\nNo need to start the ORM…\n…because the fake UoW doesn’t use one.\nWe want to fake out our email and Redis adapters too.\nSo that gets rid of a little duplication, and we’ve moved a bunch of\nsetup and sensible defaults into a single place.\nEXERCISE FOR THE READER 1\nChange all the handlers to being classes as per the DI using classes example, and amend the\nbootstrapper’s DI code as appropriate. This will let you know whether you prefer the functional\napproach or the class-based approach when it comes to your own projects.\nBuilding an Adapter “Properly”: A\nWorked Example",
      "content_length": 959,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": null,
      "content": "To really get a feel for how it all works, let’s work through an example\nof how you might “properly” build an adapter and do dependency\ninjection for it.\nAt the moment, we have two types of dependencies:\nTwo types of dependencies\n(src/allocation/service_layer/messagebus.py)\n    uow: unit_of_work.AbstractUnitOfWork,  \n    send_mail: Callable,  \n    publish: Callable,  \nThe UoW has an abstract base class. This is the heavyweight\noption for declaring and managing your external dependency. We’d\nuse this for the case when the dependency is relatively complex.\nOur email sender and pub/sub publisher are defined as functions.\nThis works just fine for simple dependencies.\nHere are some of the things we find ourselves injecting at work:\nAn S3 filesystem client\nA key/value store client\nA requests session object\nMost of these will have more-complex APIs that you can’t capture as a\nsingle function: read and write, GET and POST, and so on.\nEven though it’s simple, let’s use send_mail as an example to talk\nthrough how you might define a more complex dependency.",
      "content_length": 1062,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": null,
      "content": "Define the Abstract and Concrete\nImplementations\nWe’ll imagine a more generic notifications API. Could be email, could\nbe SMS, could be Slack posts one day.\nAn ABC and a concrete implementation\n(src/allocation/adapters/notifications.py)\nclass AbstractNotifications(abc.ABC): \n \n    @abc.abstractmethod \n    def send(self, destination, message): \n        raise NotImplementedError \n \n... \n \nclass EmailNotifications(AbstractNotifications): \n \n    def __init__(self, smtp_host=DEFAULT_HOST, port=DEFAULT_PORT): \n        self.server = smtplib.SMTP(smtp_host, port=port) \n        self.server.noop() \n \n    def send(self, destination, message): \n        msg = f'Subject: allocation service notification\\n{message}' \n        self.server.sendmail( \n            from_addr='allocations@example.com', \n            to_addrs=[destination], \n            msg=msg \n        )\nWe change the dependency in the bootstrap script:\nNotifications in message bus (src/allocation/bootstrap.py)\n def bootstrap( \n     start_orm: bool = True, \n     uow: unit_of_work.AbstractUnitOfWork = unit_of_work.SqlAlchemyUnitOfWork(), \n-    send_mail: Callable = email.send,",
      "content_length": 1136,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": null,
      "content": "+    notifications: AbstractNotifications = EmailNotifications(), \n     publish: Callable = redis_eventpublisher.publish, \n ) -> messagebus.MessageBus:\nMake a Fake Version for Your Tests\nWe work through and define a fake version for unit testing:\nFake notifications (tests/unit/test_handlers.py)\nclass FakeNotifications(notifications.AbstractNotifications): \n \n    def __init__(self): \n        self.sent = defaultdict(list)  # type: Dict[str, List[str]] \n \n    def send(self, destination, message): \n        self.sent[destination].append(message)\n...\nAnd we use it in our tests:\nTests change slightly (tests/unit/test_handlers.py)\n    def test_sends_email_on_out_of_stock_error(self): \n        fake_notifs = FakeNotifications() \n        bus = bootstrap.bootstrap( \n            start_orm=False, \n            uow=FakeUnitOfWork(), \n            notifications=fake_notifs, \n            publish=lambda *args: None, \n        ) \n        bus.handle(commands.CreateBatch(\"b1\", \"POPULAR-CURTAINS\", 9, None)) \n        bus.handle(commands.Allocate(\"o1\", \"POPULAR-CURTAINS\", 10)) \n        assert fake_notifs.sent['stock@made.com'] == [ \n            f\"Out of stock for POPULAR-CURTAINS\", \n        ]",
      "content_length": 1184,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": null,
      "content": "Figure Out How to Integration Test the Real Thing\nNow we test the real thing, usually with an end-to-end or integration\ntest. We’ve used MailHog as a real-ish email server for our Docker\ndev environment:\nDocker-compose config with real fake email server (docker-\ncompose.yml)\nversion: \"3\" \n \nservices: \n \n  redis_pubsub: \n    build: \n      context: . \n      dockerfile: Dockerfile \n    image: allocation-image \n    ... \n \n  api: \n    image: allocation-image \n    ... \n \n  postgres: \n    image: postgres:9.6 \n    ... \n \n  redis: \n    image: redis:alpine \n    ... \n \n  mailhog: \n    image: mailhog/mailhog \n    ports: \n      - \"11025:1025\" \n      - \"18025:8025\"",
      "content_length": 659,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": null,
      "content": "In our integration tests, we use the real EmailNotifications class,\ntalking to the MailHog server in the Docker cluster:\nIntegration test for email (tests/integration/test_email.py)\n@pytest.fixture\ndef bus(sqlite_session_factory):\n    bus = bootstrap.bootstrap(\n        start_orm=True,\n        uow=unit_of_work.SqlAlchemyUnitOfWork(sqlite_session_factory),\n        notifications=notifications.EmailNotifications(),  \n        publish=lambda *args: None,\n    )\n    yield bus\n    clear_mappers()\ndef get_email_from_mailhog(sku):  \n    host, port = map(config.get_email_host_and_port().get, ['host', \n'http_port'])\n    all_emails = requests.get(f'http://{host}:{port}/api/v2/messages').json()\n    return next(m for m in all_emails['items'] if sku in str(m))\ndef test_out_of_stock_email(bus):\n    sku = random_sku()\n    bus.handle(commands.CreateBatch('batch1', sku, 9, None))  \n    bus.handle(commands.Allocate('order1', sku, 10))\n    email = get_email_from_mailhog(sku)\n    assert email['Raw']['From'] == 'allocations@example.com'  \n    assert email['Raw']['To'] == ['stock@made.com']\n    assert f'Out of stock for {sku}' in email['Raw']['Data']\nWe use our bootstrapper to build a message bus that talks to the\nreal notifications class.\nWe figure out how to fetch emails from our “real” email server.",
      "content_length": 1297,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": null,
      "content": "We use the bus to do our test setup.\nAgainst all the odds, this actually worked, pretty much at the first\ngo!\nAnd that’s it really.\nEXERCISE FOR THE READER 2\nYou could do two things for practice regarding adapters:\n1. Try swapping out our notifications from email to SMS notifications using Twilio, for\nexample, or Slack notifications. Can you find a good equivalent to MailHog for\nintegration testing?\n2. In a similar way to what we did moving from send_mail to a Notifications class, try\nrefactoring our redis_eventpublisher that is currently just a Callable to some sort of\nmore formal adapter/base class/protocol.\nWrap-Up\nOnce you have more than one adapter, you’ll start to feel a lot of pain\nfrom passing dependencies around manually, unless you do some kind\nof dependency injection.\nSetting up dependency injection is just one of many typical\nsetup/initialization activities that you need to do just once when\nstarting your app. Putting this all together into a bootstrap script is\noften a good idea.\nThe bootstrap script is also good as a place to provide sensible default\nconfiguration for your adapters, and as a single place to override those\nadapters with fakes for your tests.",
      "content_length": 1189,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": null,
      "content": "A dependency injection framework can be useful if you find yourself\nneeding to do DI at multiple levels—if you have chained dependencies\nof components that all need DI, for example.\nThis chapter also presented a worked example of changing an\nimplicit/simple dependency into a “proper” adapter, factoring out an\nABC, defining its real and fake implementations, and thinking through\nintegration testing.\nDI AND BOOTSTRAP RECAP\nIn summary:\n1. Define your API using an ABC.\n2. Implement the real thing.\n3. Build a fake and use it for unit/service-layer/handler tests.\n4. Find a less fake version you can put into your Docker environment.\n5. Test the less fake “real” thing.\n6. Profit!\nThese were the last patterns we wanted to cover, which brings us to\nthe end of Part II. In the epilogue, we’ll try to give you some pointers\nfor applying these techniques in the Real World\n.\n1  Because Python is not a “pure” OO language, Python developers aren’t necessarily used\nto the concept of needing to compose a set of objects into a working application. We just\npick our entrypoint and run code from top to bottom.\n2  Mark Seemann calls this Pure DI or sometimes Vanilla DI.\n3  However, it’s still a global in the flask_app module scope, if that makes sense. This may\ncause problems if you ever find yourself wanting to test your Flask app in-process by\nTM",
      "content_length": 1345,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": null,
      "content": "using the Flask Test Client instead of using Docker as we do. It’s worth researching\nFlask app factories if you get into this.",
      "content_length": 126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": null,
      "content": "Epilogue\nWhat Now?\nPhew! We’ve covered a lot of ground in this book, and for most of our\naudience all of these ideas are new. With that in mind, we can’t hope\nto make you experts in these techniques. All we can really do is show\nyou the broad-brush ideas, and just enough code for you to go ahead\nand write something from scratch.\nThe code we’ve shown in this book isn’t battle-hardened production\ncode: it’s a set of Lego blocks that you can play with to make your first\nhouse, spaceship, and skyscraper.\nThat leaves us with two big tasks. We want to talk about how to start\napplying these ideas for real in an existing system, and we need to\nwarn you about some of the things we had to skip. We’ve given you a\nwhole new arsenal of ways to shoot yourself in the foot, so we should\ndiscuss some basic firearms safety.\nHow Do I Get There from Here?\nChances are that a lot of you are thinking something like this:\n“OK Bob and Harry, that’s all well and good, and if I ever get hired to\nwork on a green-field new service, I know what to do. But in the\nmeantime, I’m here with my big ball of Django mud, and I don’t see",
      "content_length": 1115,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": null,
      "content": "any way to get to your nice, clean, perfect, untainted, simplistic model.\nNot from here.”\nWe hear you. Once you’ve already built a big ball of mud, it’s hard to\nknow how to start improving things. Really, we need to tackle things\nstep by step.\nFirst things first: what problem are you trying to solve? Is the software\ntoo hard to change? Is the performance unacceptable? Have you got\nweird, inexplicable bugs?\nHaving a clear goal in mind will help you to prioritize the work that\nneeds to be done and, importantly, communicate the reasons for doing\nit to the rest of the team. Businesses tend to have pragmatic approaches\nto technical debt and refactoring, so long as engineers can make a\nreasoned argument for fixing things.\nTIP\nMaking complex changes to a system is often an easier sell if you link it to feature\nwork. Perhaps you’re launching a new product or opening your service to new\nmarkets? This is the right time to spend engineering resources on fixing the\nfoundations. With a six-month project to deliver, it’s easier to make the argument\nfor three weeks of cleanup work. Bob refers to this as architecture tax.\nSeparating Entangled Responsibilities\nAt the beginning of the book, we said that the main characteristic of a\nbig ball of mud is homogeneity: every part of the system looks the",
      "content_length": 1300,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": null,
      "content": "same, because we haven’t been clear about the responsibilities of each\ncomponent. To fix that, we’ll need to start separating out\nresponsibilities and introducing clear boundaries. One of the first\nthings we can do is to start building a service layer (Figure E-1).\nFigure E-1. Domain of a collaboration system\nThis was the system in which Bob first learned how to break apart a\nball of mud, and it was a doozy. There was logic everywhere—in the\nweb pages, in manager objects, in helpers, in fat service classes that\nwe’d written to abstract the managers and helpers, and in hairy\ncommand objects that we’d written to break apart the services.\nIf you’re working in a system that’s reached this point, the situation can\nfeel hopeless, but it’s never too late to start weeding an overgrown\ngarden. Eventually, we hired an architect who knew what he was\ndoing, and he helped us get things back under control.",
      "content_length": 905,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": null,
      "content": "Start by working out the use cases of your system. If you have a user\ninterface, what actions does it perform? If you have a backend\nprocessing component, maybe each cron job or Celery job is a single\nuse case. Each of your use cases needs to have an imperative name:\nApply Billing Charges, Clean Abandoned Accounts, or Raise Purchase\nOrder, for example.\nIn our case, most of our use cases were part of the manager classes and\nhad names like Create Workspace or Delete Document Version. Each\nuse case was invoked from a web frontend.\nWe aim to create a single function or class for each of these supported\noperations that deals with orchestrating the work to be done. Each use\ncase should do the following:\nStart its own database transaction if needed\nFetch any required data\nCheck any preconditions (see the Ensure pattern in\nAppendix E)\nUpdate the domain model\nPersist any changes\nEach use case should succeed or fail as an atomic unit. You might need\nto call one use case from another. That’s OK; just make a note of it,\nand try to avoid long-running database transactions.",
      "content_length": 1076,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": null,
      "content": "NOTE\nOne of the biggest problems we had was that manager methods called other\nmanager methods, and data access could happen from the model objects\nthemselves. It was hard to understand what each operation did without going on a\ntreasure hunt across the codebase. Pulling all the logic into a single method, and\nusing a UoW to control our transactions, made the system easier to reason about.",
      "content_length": 391,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": null,
      "content": "CASE STUDY: LAYERING AN OVERGROWN SYSTEM\nMany years ago, Bob worked for a software company that had outsourced the first version of its\napplication, an online collaboration platform for sharing and working on files.\nWhen the company brought development in-house, it passed through several generations of\ndevelopers’ hands, and each wave of new developers added more complexity to the code’s\nstructure.\nAt its heart, the system was an ASP.NET Web Forms application, built with an NHibernate ORM.\nUsers would upload documents into workspaces, where they could invite other workspace\nmembers to review, comment on, or modify their work.\nMost of the complexity of the application was in the permissions model because each document\nwas contained in a folder, and folders allowed read, write, and edit permissions, much like a\nLinux filesystem.\nAdditionally, each workspace belonged to an account, and the account had quotas attached to it\nvia a billing package.\nAs a result, every read or write operation against a document had to load an enormous number\nof objects from the database in order to test permissions and quotas. Creating a new workspace\ninvolved hundreds of database queries as we set up the permissions structure, invited users,\nand set up sample content.\nSome of the code for operations was in web handlers that ran when a user clicked a button or\nsubmitted a form; some of it was in manager objects that held code for orchestrating work; and\nsome of it was in the domain model. Model objects would make database calls or copy files on\ndisk, and the test coverage was abysmal.\nTo fix the problem, we first introduced a service layer so that all of the code for creating a\ndocument or workspace was in one place and could be understood. This involved pulling data\naccess code out of the domain model and into command handlers. Likewise, we pulled\norchestration code out of the managers and the web handlers and pushed it into handlers.\nThe resulting command handlers were long and messy, but we’d made a start at introducing\norder to the chaos.\nTIP\nIt’s fine if you have duplication in the use-case functions. We’re not trying to write\nperfect code; we’re just trying to extract some meaningful layers. It’s better to\nduplicate some code in a few places than to have use-case functions calling one\nanother in a long chain.",
      "content_length": 2331,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": null,
      "content": "This is a good opportunity to pull any data-access or orchestration\ncode out of the domain model and into the use cases. We should also\ntry to pull I/O concerns (e.g., sending email, writing files) out of the\ndomain model and up into the use-case functions. We apply the\ntechniques from Chapter 3 on abstractions to keep our handlers unit\ntestable even when they’re performing I/O.\nThese use-case functions will mostly be about logging, data access,\nand error handling. Once you’ve done this step, you’ll have a grasp of\nwhat your program actually does, and a way to make sure each\noperation has a clearly defined start and finish. We’ll have taken a step\ntoward building a pure domain model.\nRead Working Effectively with Legacy Code by Michael C. Feathers\n(Prentice Hall) for guidance on getting legacy code under test and\nstarting separating responsibilities.\nIdentifying Aggregates and Bounded\nContexts\nPart of the problem with the codebase in our case study was that the\nobject graph was highly connected. Each account had many\nworkspaces, and each workspace had many members, all of whom had\ntheir own accounts. Each workspace contained many documents,\nwhich had many versions.\nYou can’t express the full horror of the thing in a class diagram. For\none thing, there wasn’t really a single account related to a user.\nInstead, there was a bizarre rule requiring you to enumerate all of the",
      "content_length": 1393,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": null,
      "content": "accounts associated to the user via the workspaces and take the one\nwith the earliest creation date.\nEvery object in the system was part of an inheritance hierarchy that\nincluded SecureObject and Version. This inheritance hierarchy was\nmirrored directly in the database schema, so that every query had to\njoin across 10 different tables and look at a discriminator column just\nto tell what kind of objects you were working with.\nThe codebase made it easy to “dot” your way through these objects\nlike so:\nuser.account.workspaces[0].documents.versions[1].owner.account.settings[0];\nBuilding a system this way with Django ORM or SQLAlchemy is easy\nbut is to be avoided. Although it’s convenient, it makes it very hard to\nreason about performance because each property might trigger a\nlookup to the database.\nTIP\nAggregates are a consistency boundary. In general, each use case should\nupdate a single aggregate at a time. One handler fetches one aggregate from a\nrepository, modifies its state, and raises any events that happen as a result. If you\nneed data from another part of the system, it’s totally fine to use a read model, but\navoid updating multiple aggregates in a single transaction. When we choose to\nseparate code into different aggregates, we’re explicitly choosing to make them\neventually consistent with one another.",
      "content_length": 1328,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": null,
      "content": "A bunch of operations required us to loop over objects this way—for\nexample:\n# Lock a user's workspaces for nonpayment \n \ndef lock_account(user): \n    for workspace in user.account.workspaces: \n        workspace.archive()\nOr even recurse over collections of folders and documents:\ndef lock_documents_in_folder(folder): \n \n    for doc in folder.documents: \n         doc.archive() \n \n     for child in folder.children: \n         lock_documents_in_folder(child)\nThese operations killed performance, but fixing them meant giving up\nour single object graph. Instead, we began to identify aggregates and to\nbreak the direct links between objects.\nNOTE\nWe talked about the infamous SELECT N+1 problem in Chapter 12, and how we\nmight choose to use different techniques when reading data for queries versus\nreading data for commands.\nMostly we did this by replacing direct references with identifiers.\nBefore aggregates:",
      "content_length": 911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": null,
      "content": "After modeling with aggregates:",
      "content_length": 31,
      "extraction_method": "Direct"
    },
    {
      "page_number": 375,
      "chapter": null,
      "content": "TIP\nBidirectional links are often a sign that your aggregates aren’t right. In our original\ncode, a Document knew about its containing Folder, and the Folder had a\ncollection of Documents. This makes it easy to traverse the object graph but stops\nus from thinking properly about the consistency boundaries we need. We break\napart aggregates by using references instead. In the new model, a Document had\nreference to its parent_folder but had no way to directly access the Folder.\nIf we needed to read data, we avoided writing complex loops and\ntransforms and tried to replace them with straight SQL. For example,\none of our screens was a tree view of folders and documents.\nThis screen was incredibly heavy on the database, because it relied on\nnested for loops that triggered a lazy-loaded ORM.",
      "content_length": 795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": null,
      "content": "TIP\nWe use this same technique in Chapter 11, where we replace a nested loop over\nORM objects with a simple SQL query. It’s the first step in a CQRS approach.\nAfter a lot of head-scratching, we replaced the ORM code with a big,\nugly stored procedure. The code looked horrible, but it was much\nfaster and helped to break the links between Folder and Document.\nWhen we needed to write data, we changed a single aggregate at a\ntime, and we introduced a message bus to handle events. For example,\nin the new model, when we locked an account, we could first query for\nall the affected workspaces via SELECT id FROM workspace\nWHERE account_id = ?.\nWe could then raise a new command for each workspace:\nfor workspace_id in workspaces: \n    bus.handle(LockWorkspace(workspace_id))\nAn Event-Driven Approach to Go to\nMicroservices via Strangler Pattern\nThe Strangler Fig pattern involves creating a new system around the\nedges of an old system, while keeping it running. Bits of old\nfunctionality are gradually intercepted and replaced, until the old\nsystem is left doing nothing at all and can be switched off.",
      "content_length": 1101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 377,
      "chapter": null,
      "content": "When building the availability service, we used a technique called\nevent interception to move functionality from one place to another.\nThis is a three-step process:\n1. Raise events to represent the changes happening in a system\nyou want to replace.\n2. Build a second system that consumes those events and uses\nthem to build its own domain model.\n3. Replace the older system with the new.\nWe used event interception to move from Figure E-2…\nFigure E-2. Before: strong, bidirectional coupling based on XML-RPC\nto Figure E-3.",
      "content_length": 522,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": null,
      "content": "Figure E-3. After: loose coupling with asynchronous events (you can find a high-\nresolution version of this diagram at cosmicpython.com)\nPractically, this was a several month-long project. Our first step was\nto write a domain model that could represent batches, shipments, and\nproducts. We used TDD to build a toy system that could answer a\nsingle question: “If I want N units of HAZARDOUS_RUG, how long\nwill they take to be delivered?”\nTIP\nWhen deploying an event-driven system, start with a “walking skeleton.”\nDeploying a system that just logs its input forces us to tackle all the infrastructural\nquestions and start working in production.",
      "content_length": 643,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": null,
      "content": "CASE STUDY: CARVING OUT A MICROSERVICE TO REPLACE A\nDOMAIN\nMADE.com started out with two monoliths: one for the frontend ecommerce application, and one\nfor the backend fulfillment system.\nThe two systems communicated through XML-RPC. Periodically, the backend system would\nwake up and query the frontend system to find out about new orders. When it had imported all the\nnew orders, it would send RPC commands to update the stock levels.\nOver time this synchronization process became slower and slower until, one Christmas, it took\nlonger than 24 hours to import a single day’s orders. Bob was hired to break the system into a\nset of event-driven services.\nFirst, we identified that the slowest part of the process was calculating and synchronizing the\navailable stock. What we needed was a system that could listen to external events and keep a\nrunning total of how much stock was available.\nWe exposed that information via an API, so that the user’s browser could ask how much stock\nwas available for each product and how long it would take to deliver to their address.\nWhenever a product ran out of stock completely, we would raise a new event that the ecommerce\nplatform could use to take a product off sale. Because we didn’t know how much load we would\nneed to handle, we wrote the system with a CQRS pattern. Whenever the amount of stock\nchanged, we would update a Redis database with a cached view model. Our Flask API queried\nthese view models instead of running the complex domain model.\nAs a result, we could answer the question “How much stock is available?” in 2 to 3 milliseconds,\nand now the API frequently handles hundreds of requests a second for sustained periods.\nIf this all sounds a little familiar, well, now you know where our example app came from!\nOnce we had a working domain model, we switched to building out\nsome infrastructural pieces. Our first production deployment was a\ntiny system that could receive a batch_created event and log its\nJSON representation. This is the “Hello World” of event-driven\narchitecture. It forced us to deploy a message bus, hook up a producer\nand consumer, build a deployment pipeline, and write a simple\nmessage handler.",
      "content_length": 2180,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": null,
      "content": "Given a deployment pipeline, the infrastructure we needed, and a basic\ndomain model, we were off. A couple months later, we were in\nproduction and serving real customers.\nConvincing Your Stakeholders to Try\nSomething New\nIf you’re thinking about carving a new system out of a big ball of mud,\nyou’re probably suffering problems with reliability, performance,\nmaintainability, or all three simultaneously. Deep, intractable problems\ncall for drastic measures!\nWe recommend domain modeling as a first step. In many overgrown\nsystems, the engineers, product owners, and customers no longer speak\nthe same language. Business stakeholders speak about the system in\nabstract, process-focused terms, while developers are forced to speak\nabout the system as it physically exists in its wild and chaotic state.",
      "content_length": 801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": null,
      "content": "CASE STUDY: THE USER MODEL\nWe mentioned earlier that the account and user model in our first system were bound together by\na “bizarre rule.” This is a perfect example of how engineering and business stakeholders can\ndrift apart.\nIn this system, accounts parented workspaces, and users were members of workspaces.\nWorkspaces were the fundamental unit for applying permissions and quotas. If a user joined a\nworkspace and didn’t already have an account, we would associate them with the account that\nowned that workspace.\nThis was messy and ad hoc, but it worked fine until the day a product owner asked for a new\nfeature:\nWhen a user joins a company, we want to add them to some default workspaces for the\ncompany, like the HR workspace or the Company Announcements workspace.\nWe had to explain to them that there was no such thing as a company, and there was no sense in\nwhich a user joined an account. Moreover, a “company” might have many accounts owned by\ndifferent users, and a new user might be invited to any one of them.\nYears of adding hacks and work-arounds to a broken model caught up with us, and we had to\nrewrite the entire user management function as a brand-new system.\nFiguring out how to model your domain is a complex task that’s the\nsubject of many decent books in its own right. We like to use\ninteractive techniques like event storming and CRC modeling, because\nhumans are good at collaborating through play. Event modeling is\nanother technique that brings engineers and product owners together to\nunderstand a system in terms of commands, queries, and events.\nTIP\nCheck out www.eventmodeling.org and www.eventstorming.org for some great\nguides to visual modeling of systems with events.\nThe goal is to be able to talk about the system by using the same\nubiquitous language, so that you can agree on where the complexity",
      "content_length": 1841,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": null,
      "content": "lies.\nWe’ve found a lot of value in treating domain problems as TDD kata.\nFor example, the first code we wrote for the availability service was\nthe batch and order line model. You can treat this as a lunchtime\nworkshop, or as a spike at the beginning of a project. Once you can\ndemonstrate the value of modeling, it’s easier to make the argument for\nstructuring the project to optimize for modeling.",
      "content_length": 399,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": null,
      "content": "CASE STUDY: DAVID SEDDON ON TAKING SMALL STEPS\nHi, I’m David, one of the tech reviewers on this book. I’ve worked on several complex Django\nmonoliths, and so I’ve known the pain that Bob and Harry have made all sorts of grand promises\nabout soothing.\nWhen I was first exposed to the patterns described here, I was rather excited. I had successfully\nused some of the techniques already on smaller projects, but here was a blueprint for much\nlarger, database-backed systems like the one I work on in my day job. So I started trying to figure\nout how I could implement that blueprint at my current organization.\nI chose to tackle a problem area of the codebase that had always bothered me. I began by\nimplementing it as a use case. But I found myself running into unexpected questions. There were\nthings that I hadn’t considered while reading that now made it difficult to see what to do. Was it a\nproblem if my use case interacted with two different aggregates? Could one use case call\nanother? And how was it going to exist within a system that followed different architectural\nprinciples without resulting in a horrible mess?\nWhat happened to that oh-so-promising blueprint? Did I actually understand the ideas well\nenough to put them into practice? Was it even suitable for my application? Even if it was, would\nany of my colleagues agree to such a major change? Were these just nice ideas for me to\nfantasize about while I got on with real life?\nIt took me a while to realize that I could start small. I didn’t need to be a purist or to get it right the\nfirst time: I could experiment, finding what worked for me.\nAnd so that’s what I’ve done. I’ve been able to apply some of the ideas in a few places. I’ve built\nnew features whose business logic can be tested without the database or mocks. And as a team,\nwe’ve introduced a service layer to help define the jobs the system does.\nIf you start trying to apply these patterns in your work, you may go through similar feelings to\nbegin with. When the nice theory of a book meets the reality of your codebase, it can be\ndemoralizing.\nMy advice is to focus on a specific problem and ask yourself how you can put the relevant ideas\nto use, perhaps in an initially limited and imperfect fashion. You may discover, as I did, that the\nfirst problem you pick might be a bit too difficult; if so, move on to something else. Don’t try to boil\nthe ocean, and don’t be too afraid of making mistakes. It will be a learning experience, and you\ncan be confident that you’re moving roughly in a direction that others have found useful.\nSo, if you’re feeling the pain too, give these ideas a try. Don’t feel you need permission to\nrearchitect everything. Just look for somewhere small to start. And above all, do it to solve a\nspecific problem. If you’re successful in solving it, you’ll know you got something right—and\nothers will too.",
      "content_length": 2872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": null,
      "content": "Questions Our Tech Reviewers Asked\nThat We Couldn’t Work into Prose\nHere are some questions we heard during drafting that we couldn’t\nfind a good place to address elsewhere in the book:\nDo I need to do all of this at once? Can I just do a bit at a time?\nNo, you can absolutely adopt these techniques bit by bit. If you\nhave an existing system, we recommend building a service layer to\ntry to keep orchestration in one place. Once you have that, it’s\nmuch easier to push logic into the model and push edge concerns\nlike validation or error handling to the entrypoints.\nIt’s worth having a service layer even if you still have a big, messy\nDjango ORM because it’s a way to start understanding the\nboundaries of operations.\nExtracting use cases will break a lot of my existing code; it’s too\ntangled\nJust copy and paste. It’s OK to cause more duplication in the short\nterm. Think of this as a multistep process. Your code is in a bad\nstate now, so copy and paste it to a new place and then make that\nnew code clean and tidy.\nOnce you’ve done that, you can replace uses of the old code with\ncalls to your new code and finally delete the mess. Fixing large\ncodebases is a messy and painful process. Don’t expect things to\nget instantly better, and don’t worry if some bits of your\napplication stay messy.\nDo I need to do CQRS? That sounds weird. Can’t I just use\nrepositories?\nOf course you can! The techniques we’re presenting in this book\nare intended to make your life easier. They’re not some kind of",
      "content_length": 1499,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": null,
      "content": "ascetic discipline with which to punish yourself.\nIn our first case-study system, we had a lot of View Builder objects\nthat used repositories to fetch data and then performed some\ntransformations to return dumb read models. The advantage is that\nwhen you hit a performance problem, it’s easy to rewrite a view\nbuilder to use custom queries or raw SQL.\nHow should use cases interact across a larger system? Is it a\nproblem for one to call another?\nThis might be an interim step. Again, in the first case study, we had\nhandlers that would need to invoke other handlers. This gets really\nmessy, though, and it’s much better to move to using a message bus\nto separate these concerns.\nGenerally, your system will have a single message bus\nimplementation and a bunch of subdomains that center on a\nparticular aggregate or set of aggregates. When your use case has\nfinished, it can raise an event, and a handler elsewhere can run.\nIs it a code smell for a use case to use multiple\nrepositories/aggregates, and if so, why?\nAn aggregate is a consistency boundary, so if your use case needs\nto update two aggregates atomically (within the same transaction),\nthen your consistency boundary is wrong, strictly speaking. Ideally\nyou should think about moving to a new aggregate that wraps up all\nthe things you want to change at the same time.\nIf you’re actually updating only one aggregate and using the\nother(s) for read-only access, then that’s fine, although you could\nconsider building a read/view model to get you that data instead—\nit makes things cleaner if each use case has only one aggregate.\nIf you do need to modify two aggregates, but the two operations\ndon’t have to be in the same transaction/UoW, then consider\nsplitting the work out into two different handlers and using a",
      "content_length": 1777,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": null,
      "content": "domain event to carry information between the two. You can read\nmore in these papers on aggregate design by Vaughn Vernon.\nWhat if I have a read-only but business-logic-heavy system?\nView models can have complex logic in them. In this book, we’ve\nencouraged you to separate your read and write models because\nthey have different consistency and throughput requirements.\nMostly, we can use simpler logic for reads, but that’s not always\ntrue. In particular, permissions and authorization models can add a\nlot of complexity to our read side.\nWe’ve written systems in which the view models needed extensive\nunit tests. In those systems, we split a view builder from a view\nfetcher, as in Figure E-4.",
      "content_length": 696,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": null,
      "content": "Figure E-4. A view builder and view fetcher (you can find a high-resolution version of\nthis diagram at cosmicpython.com)\n+ This makes it easy to test the view builder by giving it mocked data\n(e.g., a list of dicts). “Fancy CQRS” with event handlers is really a\nway of running our complex view logic whenever we write so that we\ncan avoid running it when we read.\nDo I need to build microservices to do this stuff?\nEgads, no! These techniques predate microservices by a decade or\nso. Aggregates, domain events, and dependency inversion are ways\nto control complexity in large systems. It just so happens that when\nyou’ve built a set of use cases and a model for a business process,",
      "content_length": 681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": null,
      "content": "moving it to its own service is relatively easy, but that’s not a\nrequirement.\nI’m using Django. Can I still do this?\nWe have an entire appendix just for you: Appendix D!\nFootguns\nOK, so we’ve given you a whole bunch of new toys to play with.\nHere’s the fine print. Harry and Bob do not recommend that you copy\nand paste our code into a production system and rebuild your\nautomated trading platform on Redis pub/sub. For reasons of brevity\nand simplicity, we’ve hand-waved a lot of tricky subjects. Here’s a list\nof things we think you should know before trying this for real.\nReliable messaging is hard\nRedis pub/sub is not reliable and shouldn’t be used as a general-\npurpose messaging tool. We picked it because it’s familiar and\neasy to run. At MADE, we run Event Store as our messaging tool,\nbut we’ve had experience with RabbitMQ and Amazon\nEventBridge.\nTyler Treat has some excellent blog posts on his site\nbravenewgeek.com; you should read at least read “You Cannot\nHave Exactly-Once Delivery” and “What You Want Is What You\nDon’t: Understanding Trade-Offs in Distributed Messaging”.\nWe explicitly choose small, focused transactions that can fail\nindependently\nIn Chapter 8, we update our process so that deallocating an order\nline and reallocating the line happen in two separate units of\nwork. You will need monitoring to know when these transactions",
      "content_length": 1360,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": null,
      "content": "fail, and tooling to replay events. Some of this is made easier by\nusing a transaction log as your message broker (e.g., Kafka or\nEventStore). You might also look at the Outbox pattern.\nWe don’t discuss idempotency\nWe haven’t given any real thought to what happens when handlers\nare retried. In practice you will want to make handlers idempotent\nso that calling them repeatedly with the same message will not\nmake repeated changes to state. This is a key technique for building\nreliability, because it enables us to safely retry events when they\nfail.\nThere’s a lot of good material on idempotent message handling, try\nstarting with “How to Ensure Idempotency in an Eventual Consistent\nDDD/CQRS Application” and “(Un)Reliability in Messaging”.\nYour events will need to change their schema over time\nYou’ll need to find some way of documenting your events and\nsharing schema with consumers. We like using JSON schema and\nmarkdown because it’s simple but there is other prior art. Greg\nYoung wrote an entire book on managing event-driven systems\nover time: Versioning in an Event Sourced System (Leanpub).\nMore Required Reading\nA few more books we’d like to recommend to help you on your way:\nClean Architectures in Python by Leonardo Giordani\n(Leanpub), which came out in 2019, is one of the few\nprevious books on application architecture in Python.\nEnterprise Integration Patterns by Gregor Hohpe and Bobby\nWoolf (Addison-Wesley Professional) is a pretty good start",
      "content_length": 1465,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": null,
      "content": "for messaging patterns.\nMonolith to Microservices by Sam Newman (O’Reilly), and\nNewman’s first book, Building Microservices (O’Reilly).\nThe Strangler Fig pattern is mentioned as a favorite, along\nwith many others. These are good to check out if you’re\nthinking of moving to microservices, and they’re also good on\nintegration patterns and the considerations of async\nmessaging-based integration.\nWrap-Up\nPhew! That’s a lot of warnings and reading suggestions; we hope we\nhaven’t scared you off completely. Our goal with this book is to give\nyou just enough knowledge and intuition for you to start building some\nof this for yourself. We would love to hear how you get on and what\nproblems you’re facing with the techniques in your own systems, so\nwhy not get in touch with us over at www.cosmicpython.com?",
      "content_length": 805,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": null,
      "content": "Appendix A. Summary\nDiagram and Table\nHere’s what our architecture looks like by the end of the book:",
      "content_length": 101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 393,
      "chapter": null,
      "content": "Table A-1 recaps each pattern and what it does.",
      "content_length": 47,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": null,
      "content": "Table A-1. The components of our architecture and what they all \ndo\nLayer\nCompo\nnent\nDescription\nDomain\nDefines the business \nlogic.\nEntity\nA domain object whose attributes may \nchange but that has a recognizable identity \nover time.\nValue \nobject\nAn immutable domain object whose attributes \nentirely define it. It is fungible with other \nidentical objects.\nAggregat\ne\nCluster of associated objects that we treat as \na unit for the purpose of data changes. \nDefines and enforces a consistency \nboundary.\nEvent\nRepresents something that happened.\nComman\nd\nRepresents a job the system should perform.\nService Layer\nDefines the jobs the \nsystem should perform \nand orchestrates \ndifferent components.\nHandler\nReceives a command or an event and \nperforms what needs to happen.\nUnit of \nwork\nAbstraction around data integrity. Each unit \nof work represents an atomic update. Makes \nrepositories available. Tracks new events on \nretrieved aggregates.\nMessage \nbus \n(internal)\nHandles commands and events by routing \nthem to the appropriate handler.\nAdapters (Secondary)\nConcrete \nimplementations of an \ninterface that goes \nRepositor\ny\nAbstraction around persistent storage. Each \naggregate has its own repository.",
      "content_length": 1209,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": null,
      "content": "Layer\nCompo\nnent\nDescription\nfrom our system \nto the outside world \n(I/O).\nEvent \npublisher\nPushes events onto the external message bus.\nEntrypoints (Primary \nadapters)\nTranslate external \ninputs into calls into \nthe service layer.\nWeb\nReceives web requests and translates them \ninto commands, passing them to the internal \nmessage bus.\nEvent \nconsumer\nReads events from the external message bus \nand translates them into commands, passing \nthem to the internal message bus.\nN/A\nExternal \nmessage \nbus \n(message \nbroker)\nA piece of infrastructure that different \nservices use to intercommunicate, via events.",
      "content_length": 608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": null,
      "content": "Appendix B. A Template\nProject Structure\nAround Chapter 4, we moved from just having everything in one folder\nto a more structured tree, and we thought it might be of interest to\noutline the moving parts.\nTIP\nThe code for this appendix is in the appendix_project_structure branch on GitHub:\ngit clone https://github.com/cosmicpython/code.git \ncd code \ngit checkout appendix_project_structure\nThe basic folder structure looks like this:\nProject tree\n. \n├── Dockerfile   \n├── Makefile   \n├── README.md \n├── docker-compose.yml   \n├── license.txt \n├── mypy.ini \n├── requirements.txt \n├── src   \n│   ├── allocation \n│   │   ├── __init__.py \n│   │   ├── adapters \n│   │   │   ├── __init__.py",
      "content_length": 685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": null,
      "content": "│   │   │   ├── orm.py \n│   │   │   └── repository.py \n│   │   ├── config.py \n│   │   ├── domain \n│   │   │   ├── __init__.py \n│   │   │   └── model.py \n│   │   ├── entrypoints \n│   │   │   ├── __init__.py \n│   │   │   └── flask_app.py \n│   │   └── service_layer \n│   │       ├── __init__.py \n│   │       └── services.py \n│   └── setup.py   \n└── tests   \n    ├── conftest.py   \n    ├── e2e \n    │   └── test_api.py \n    ├── integration \n    │   ├── test_orm.py \n    │   └── test_repository.py \n    ├── pytest.ini   \n    └── unit \n        ├── test_allocate.py \n        ├── test_batches.py \n        └── test_services.py\nOur docker-compose.yml and our Dockerfile are the main bits of\nconfiguration for the containers that run our app, and they can also\nrun the tests (for CI). A more complex project might have several\nDockerfiles, although we’ve found that minimizing the number of\nimages is usually a good idea.\nA Makefile provides the entrypoint for all the typical commands a\ndeveloper (or a CI server) might want to run during their normal\nworkflow: make build, make test, and so on.  This is optional.\nYou could just use docker-compose and pytest directly, but if\nnothing else, it’s nice to have all the “common commands” in a list\nsomewhere, and unlike documentation, a Makefile is code so it has\nless tendency to become out of date.\nAll the source code for our app, including the domain model, the\nFlask app, and infrastructure code, lives in a Python package\n1\n2",
      "content_length": 1468,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": null,
      "content": "inside src,  which we install using pip install -e and the\nsetup.py file. This makes imports easy. Currently, the structure\nwithin this module is totally flat, but for a more complex project,\nyou’d expect to grow a folder hierarchy that includes\ndomain_model/, infrastructure/, services/, and api/.\nTests live in their own folder. Subfolders distinguish different test\ntypes and allow you to run them separately. We can keep shared\nfixtures (conftest.py) in the main tests folder and nest more\nspecific ones if we wish. This is also the place to keep pytest.ini.\nTIP\nThe pytest docs are really good on test layout and importability.\nLet’s look at a few of these files and concepts in more detail.\nEnv Vars, 12-Factor, and Config, Inside\nand Outside Containers\nThe basic problem we’re trying to solve here is that we need different\nconfig settings for the following:\nRunning code or tests directly from your own dev machine,\nperhaps talking to mapped ports from Docker containers\nRunning on the containers themselves, with “real” ports and\nhostnames\nDifferent container environments (dev, staging, prod, and so\non)\n3",
      "content_length": 1115,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": null,
      "content": "Configuration through environment variables as suggested by the 12-\nfactor manifesto will solve this problem, but concretely, how do we\nimplement it in our code and our containers?\nConfig.py\nWhenever our application code needs access to some config, it’s going\nto get it from a file called config.py. Here are a couple of examples\nfrom our app:\nSample config functions (src/allocation/config.py)\nimport os\ndef get_postgres_uri():  \n    host = os.environ.get('DB_HOST', 'localhost')  \n    port = 54321 if host == 'localhost' else 5432\n    password = os.environ.get('DB_PASSWORD', 'abc123')\n    user, db_name = 'allocation', 'allocation'\n    return f\"postgresql://{user}:{password}@{host}:{port}/{db_name}\"\ndef get_api_url():\n    host = os.environ.get('API_HOST', 'localhost')\n    port = 5005 if host == 'localhost' else 80\n    return f\"http://{host}:{port}\"\nWe use functions for getting the current config, rather than constants\navailable at import time, because that allows client code to modify\nos.environ if it needs to.\nconfig.py also defines some default settings, designed to work\nwhen running the code from the developer’s local machine.4",
      "content_length": 1144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": null,
      "content": "An elegant Python package called environ-config is worth looking at if\nyou get tired of hand-rolling your own environment-based config\nfunctions.\nTIP\nDon’t let this config module become a dumping ground that is full of things only\nvaguely related to config and that is then imported all over the place. Keep things\nimmutable and modify them only via environment variables. If you decide to use a\nbootstrap script, you can make it the only place (other than tests) that config is\nimported to.\nDocker-Compose and Containers Config\nWe use a lightweight Docker container orchestration tool called\ndocker-compose. It’s main configuration is via a YAML file (sigh):\ndocker-compose config file (docker-compose.yml)\nversion: \"3\"\nservices: \n \n  app:  \n    build:\n      context: .\n      dockerfile: Dockerfile\n    depends_on:\n      - postgres\n    environment:  \n      - DB_HOST=postgres  \n      - DB_PASSWORD=abc123\n      - API_HOST=app\n      - PYTHONDONTWRITEBYTECODE=1  \n    volumes:  \n5",
      "content_length": 979,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": null,
      "content": "- ./src:/src\n      - ./tests:/tests\n    ports:\n      - \"5005:80\"   \n \n \n  postgres:\n    image: postgres:9.6  \n    environment:\n      - POSTGRES_USER=allocation\n      - POSTGRES_PASSWORD=abc123\n    ports:\n      - \"54321:5432\"\nIn the docker-compose file, we define the different services\n(containers) that we need for our app. Usually one main image\ncontains all our code, and we can use it to run our API, our tests,\nor any other service that needs access to the domain model.\nYou’ll probably have other infrastructure services, including a\ndatabase. In production you might not use containers for this; you\nmight have a cloud provider instead, but docker-compose gives us\na way of producing a similar service for dev or CI.\nThe environment stanza lets you set the environment variables for\nyour containers, the hostnames and ports as seen from inside the\nDocker cluster. If you have enough containers that information\nstarts to be duplicated in these sections, you can use\nenvironment_file instead. We usually call ours container.env.\nInside a cluster, docker-compose sets up networking such that\ncontainers are available to each other via hostnames named after\ntheir service name.\nPro tip: if you’re mounting volumes to share source folders\nbetween your local dev machine and the container, the\nPYTHONDONTWRITEBYTECODE environment variable tells Python to\nnot write .pyc files, and that will save you from having millions of",
      "content_length": 1425,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": null,
      "content": "root-owned files sprinkled all over your local filesystem, being all\nannoying to delete and causing weird Python compiler errors\nbesides.\nMounting our source and test code as volumes means we don’t\nneed to rebuild our containers every time we make a code change.\nThe ports section allows us to expose the ports from inside the\ncontainers to the outside world —these correspond to the default\nports we set in config.py.\nNOTE\nInside Docker, other containers are available through hostnames named after their\nservice name. Outside Docker, they are available on localhost, at the port\ndefined in the ports section.\nInstalling Your Source as a Package\nAll our application code (everything except tests, really) lives inside\nan src folder:\nThe src folder\n├── src \n│   ├── allocation   \n│   │   ├── config.py \n│   │   └── ... \n│   └── setup.py  \nSubfolders define top-level module names. You can have multiple\nif you like.\n6",
      "content_length": 917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": null,
      "content": "And setup.py is the file you need to make it pip-installable, shown\nnext.\npip-installable modules in three lines (src/setup.py)\nfrom setuptools import setup \n \nsetup( \n    name='allocation', \n    version='0.1', \n    packages=['allocation'],\n)\nThat’s all you need. packages= specifies the names of subfolders that\nyou want to install as top-level modules. The name entry is just\ncosmetic, but it’s required. For a package that’s never actually going to\nhit PyPI, it’ll do fine.\nDockerfile\nDockerfiles are going to be very project-specific, but here are a few\nkey stages you’ll expect to see:\nOur Dockerfile (Dockerfile)\nFROM python:3.8-alpine\nRUN apk add --no-cache --virtual .build-deps gcc postgresql-dev musl-dev \npython3-dev\nRUN apk add libpq\nCOPY requirements.txt /tmp/\nRUN pip install -r /tmp/requirements.txt\nRUN apk del --no-cache .build-deps\n7",
      "content_length": 851,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": null,
      "content": "RUN mkdir -p /src\nCOPY src/ /src/\nRUN pip install -e /src\nCOPY tests/ /tests/\nWORKDIR /src\nENV FLASK_APP=allocation/entrypoints/flask_app.py FLASK_DEBUG=1 \nPYTHONUNBUFFERED=1\nCMD flask run --host=0.0.0.0 --port=80\nInstalling system-level dependencies\nInstalling our Python dependencies (you may want to split out your\ndev from prod dependencies; we haven’t here, for simplicity)\nCopying and installing our source\nOptionally configuring a default startup command (you’ll probably\noverride this a lot from the command line)\nTIP\nOne thing to note is that we install things in the order of how frequently they are\nlikely to change. This allows us to maximize Docker build cache reuse. I can’t tell\nyou how much pain and frustration underlies this lesson. For this and many more\nPython Dockerfile improvement tips, check out “Production-Ready Docker\nPackaging”.\nTests\nOur tests are kept alongside everything else, as shown here:\nTests folder tree",
      "content_length": 941,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": null,
      "content": "└── tests \n    ├── conftest.py \n    ├── e2e \n    │   └── test_api.py \n    ├── integration \n    │   ├── test_orm.py \n    │   └── test_repository.py \n    ├── pytest.ini \n    └── unit \n        ├── test_allocate.py \n        ├── test_batches.py \n        └── test_services.py\nNothing particularly clever here, just some separation of different test\ntypes that you’re likely to want to run separately, and some files for\ncommon fixtures, config, and so on.\nThere’s no src folder or setup.py in the test folders because we usually\nhaven’t needed to make tests pip-installable, but if you have\ndifficulties with import paths, you might find it helps.\nWrap-Up\nThese are our basic building blocks:\nSource code in an src folder, pip-installable using setup.py\nSome Docker config for spinning up a local cluster that\nmirrors production as far as possible\nConfiguration via environment variables, centralized in a\nPython file called config.py, with defaults allowing things to\nrun outside containers\nA Makefile for useful command-line, um, commands",
      "content_length": 1034,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": null,
      "content": "We doubt that anyone will end up with exactly the same solutions we\ndid, but we hope you find some inspiration here.\n1  Splitting out images for production and testing is sometimes a good idea, but we’ve\ntended to find that going further and trying to split out different images for different types\nof application code (e.g., Web API versus pub/sub client) usually ends up being more\ntrouble than it’s worth; the cost in terms of complexity and longer rebuild/CI times is too\nhigh. YMMV.\n2  A pure-Python alternative to Makefiles is Invoke, worth checking out if everyone on your\nteam knows Python (or at least knows it better than Bash!).\n3  “Testing and Packaging” by Hynek Schlawack provides more information on src folders.\n4  This gives us a local development setup that “just works” (as much as possible). You\nmay prefer to fail hard on missing environment variables instead, particularly if any of the\ndefaults would be insecure in production.\n5  Harry is a bit YAML-weary. It’s everywhere, and yet he can never remember the\nsyntax or how it’s supposed to indent.\n6  On a CI server, you may not be able to expose arbitrary ports reliably, but it’s only a\nconvenience for local dev. You can find ways of making these port mappings optional\n(e.g., with docker-compose.override.yml).\n7  For more setup.py tips, see this article on packaging by Hynek.",
      "content_length": 1354,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": null,
      "content": "Appendix C. Swapping Out\nthe Infrastructure: Do\nEverything with CSVs\nThis appendix is intended as a little illustration of the benefits of the\nRepository, Unit of Work, and Service Layer patterns. It’s intended to\nfollow from Chapter 6.\nJust as we finish building out our Flask API and getting it ready for\nrelease, the business comes to us apologetically, saying they’re not\nready to use our API and asking if we could build a thing that reads\njust batches and orders from a couple of CSVs and outputs a third CSV\nwith allocations.\nOrdinarily this is the kind of thing that might have a team cursing and\nspitting and making notes for their memoirs. But not us! Oh no, we’ve\nensured that our infrastructure concerns are nicely decoupled from our\ndomain model and service layer. Switching to CSVs will be a simple\nmatter of writing a couple of new Repository and UnitOfWork\nclasses, and then we’ll be able to reuse all of our logic from the\ndomain layer and the service layer.\nHere’s an E2E test to show you how the CSVs flow in and out:\nA first CSV test (tests/e2e/test_csv.py)",
      "content_length": 1077,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": null,
      "content": "def test_cli_app_reads_csvs_with_batches_and_orders_and_outputs_allocations( \n        make_csv\n): \n    sku1, sku2 = random_ref('s1'), random_ref('s2') \n    batch1, batch2, batch3 = random_ref('b1'), random_ref('b2'), \nrandom_ref('b3') \n    order_ref = random_ref('o') \n    make_csv('batches.csv', [ \n        ['ref', 'sku', 'qty', 'eta'], \n        [batch1, sku1, 100, ''], \n        [batch2, sku2, 100, '2011-01-01'], \n        [batch3, sku2, 100, '2011-01-02'], \n    ]) \n    orders_csv = make_csv('orders.csv', [ \n        ['orderid', 'sku', 'qty'], \n        [order_ref, sku1, 3], \n        [order_ref, sku2, 12], \n    ]) \n \n    run_cli_script(orders_csv.parent) \n \n    expected_output_csv = orders_csv.parent / 'allocations.csv' \n    with open(expected_output_csv) as f: \n        rows = list(csv.reader(f)) \n    assert rows == [ \n        ['orderid', 'sku', 'qty', 'batchref'], \n        [order_ref, sku1, '3', batch1], \n        [order_ref, sku2, '12', batch2], \n    ]\nDiving in and implementing without thinking about repositories and all\nthat jazz, you might start with something like this:\nA first cut of our CSV reader/writer (src/bin/allocate-from-csv)\n#!/usr/bin/env python\nimport csv\nimport sys\nfrom datetime import datetime",
      "content_length": 1226,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": null,
      "content": "from pathlib import Path \n \nfrom allocation import model \n \ndef load_batches(batches_path): \n    batches = [] \n    with batches_path.open() as inf: \n        reader = csv.DictReader(inf) \n        for row in reader: \n            if row['eta']: \n                eta = datetime.strptime(row['eta'], '%Y-%m-%d').date() \n            else: \n                eta = None \n            batches.append(model.Batch( \n                ref=row['ref'], \n                sku=row['sku'], \n                qty=int(row['qty']), \n                eta=eta \n            )) \n    return batches \n \n \n \ndef main(folder): \n    batches_path = Path(folder) / 'batches.csv' \n    orders_path = Path(folder) / 'orders.csv' \n    allocations_path = Path(folder) / 'allocations.csv' \n \n    batches = load_batches(batches_path) \n \n    with orders_path.open() as inf, allocations_path.open('w') as outf: \n        reader = csv.DictReader(inf) \n        writer = csv.writer(outf) \n        writer.writerow(['orderid', 'sku', 'batchref']) \n        for row in reader: \n            orderid, sku = row['orderid'], row['sku'] \n            qty = int(row['qty']) \n            line = model.OrderLine(orderid, sku, qty) \n            batchref = model.allocate(line, batches) \n            writer.writerow([line.orderid, line.sku, batchref])",
      "content_length": 1285,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": null,
      "content": "if __name__ == '__main__': \n    main(sys.argv[1])\nIt’s not looking too bad! And we’re reusing our domain model objects\nand our domain service.\nBut it’s not going to work. Existing allocations need to also be part of\nour permanent CSV storage. We can write a second test to force us to\nimprove things:\nAnd another one, with existing allocations (tests/e2e/test_csv.py)\ndef test_cli_app_also_reads_existing_allocations_and_can_append_to_them( \n        make_csv\n): \n    sku = random_ref('s') \n    batch1, batch2 = random_ref('b1'), random_ref('b2') \n    old_order, new_order = random_ref('o1'), random_ref('o2') \n    make_csv('batches.csv', [ \n        ['ref', 'sku', 'qty', 'eta'], \n        [batch1, sku, 10, '2011-01-01'], \n        [batch2, sku, 10, '2011-01-02'], \n    ]) \n    make_csv('allocations.csv', [ \n        ['orderid', 'sku', 'qty', 'batchref'], \n        [old_order, sku, 10, batch1], \n    ]) \n    orders_csv = make_csv('orders.csv', [ \n        ['orderid', 'sku', 'qty'], \n        [new_order, sku, 7], \n    ]) \n \n    run_cli_script(orders_csv.parent) \n \n    expected_output_csv = orders_csv.parent / 'allocations.csv' \n    with open(expected_output_csv) as f:",
      "content_length": 1167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": null,
      "content": "rows = list(csv.reader(f)) \n    assert rows == [ \n        ['orderid', 'sku', 'qty', 'batchref'], \n        [old_order, sku, '10', batch1], \n        [new_order, sku, '7', batch2], \n    ]\nAnd we could keep hacking about and adding extra lines to that\nload_batches function, and some sort of way of tracking and saving\nnew allocations—but we already have a model for doing that! It’s\ncalled our Repository and Unit of Work patterns.\nAll we need to do (“all we need to do”) is reimplement those same\nabstractions, but with CSVs underlying them instead of a database.\nAnd as you’ll see, it really is relatively straightforward.\nImplementing a Repository and Unit of\nWork for CSVs\nHere’s what a CSV-based repository could look like. It abstracts away\nall the logic for reading CSVs from disk, including the fact that it has\nto read two different CSVs (one for batches and one for allocations),\nand it gives us just the familiar .list() API, which provides the\nillusion of an in-memory collection of domain objects:\nA repository that uses CSV as its storage mechanism\n(src/allocation/service_layer/csv_uow.py)\nclass CsvRepository(repository.AbstractRepository): \n \n    def __init__(self, folder): \n        self._batches_path = Path(folder) / 'batches.csv'",
      "content_length": 1247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": null,
      "content": "self._allocations_path = Path(folder) / 'allocations.csv' \n        self._batches = {}  # type: Dict[str, model.Batch] \n        self._load() \n \n    def get(self, reference): \n        return self._batches.get(reference) \n \n    def add(self, batch): \n        self._batches[batch.reference] = batch \n \n    def _load(self): \n        with self._batches_path.open() as f: \n            reader = csv.DictReader(f) \n            for row in reader: \n                ref, sku = row['ref'], row['sku'] \n                qty = int(row['qty']) \n                if row['eta']: \n                    eta = datetime.strptime(row['eta'], '%Y-%m-%d').date() \n                else: \n                    eta = None \n                self._batches[ref] = model.Batch( \n                    ref=ref, sku=sku, qty=qty, eta=eta \n                ) \n        if self._allocations_path.exists() is False: \n            return \n        with self._allocations_path.open() as f: \n            reader = csv.DictReader(f) \n            for row in reader: \n                batchref, orderid, sku = row['batchref'], row['orderid'], \nrow['sku'] \n                qty = int(row['qty']) \n                line = model.OrderLine(orderid, sku, qty) \n                batch = self._batches[batchref] \n                batch._allocations.add(line) \n \n    def list(self): \n        return list(self._batches.values())\nAnd here’s what a UoW for CSVs would look like:",
      "content_length": 1407,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": null,
      "content": "A UoW for CSVs: commit = csv.writer\n(src/allocation/service_layer/csv_uow.py)\nclass CsvUnitOfWork(unit_of_work.AbstractUnitOfWork): \n \n    def __init__(self, folder): \n        self.batches = CsvRepository(folder) \n \n    def commit(self): \n        with self.batches._allocations_path.open('w') as f: \n            writer = csv.writer(f) \n            writer.writerow(['orderid', 'sku', 'qty', 'batchref']) \n            for batch in self.batches.list(): \n                for line in batch._allocations: \n                    writer.writerow( \n                        [line.orderid, line.sku, line.qty, batch.reference] \n                    ) \n \n    def rollback(self): \n        pass\nAnd once we have that, our CLI app for reading and writing batches\nand allocations to CSV is pared down to what it should be—a bit of\ncode for reading order lines, and a bit of code that invokes our\nexisting service layer:\nAllocation with CSVs in nine lines (src/bin/allocate-from-csv)\ndef main(folder): \n    orders_path = Path(folder) / 'orders.csv' \n    uow = csv_uow.CsvUnitOfWork(folder) \n    with orders_path.open() as f: \n        reader = csv.DictReader(f) \n        for row in reader: \n            orderid, sku = row['orderid'], row['sku'] \n            qty = int(row['qty']) \n            services.allocate(orderid, sku, qty, uow)",
      "content_length": 1313,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": null,
      "content": "Ta-da! Now are y’all impressed or what?\nMuch love,\nBob and Harry",
      "content_length": 64,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": null,
      "content": "Appendix D. Repository and\nUnit of Work Patterns with\nDjango\nSuppose you wanted to use Django instead of SQLAlchemy and Flask.\nHow might things look? The first thing is to choose where to install it.\nWe put it in a separate package next to our main allocation code:\n├── src\n│   ├── allocation\n│   │   ├── __init__.py\n│   │   ├── adapters\n│   │   │   ├── __init__.py\n...\n│   ├── djangoproject\n│   │   ├── alloc\n│   │   │   ├── __init__.py\n│   │   │   ├── apps.py\n│   │   │   ├── migrations\n│   │   │   │   ├── 0001_initial.py\n│   │   │   │   └── __init__.py\n│   │   │   ├── models.py\n│   │   │   └── views.py\n│   │   ├── django_project\n│   │   │   ├── __init__.py\n│   │   │   ├── settings.py\n│   │   │   ├── urls.py\n│   │   │   └── wsgi.py\n│   │   └── manage.py\n│   └── setup.py\n└── tests \n    ├── conftest.py \n    ├── e2e \n    │   └── test_api.py \n    ├── integration",
      "content_length": 867,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": null,
      "content": "│   ├── test_repository.py\n...\nTIP\nThe code for this appendix is in the appendix_django branch on GitHub:\ngit clone https://github.com/cosmicpython/code.git \ncd code \ngit checkout appendix_django\nRepository Pattern with Django\nWe used a plug-in called pytest-django to help with test database\nmanagement.\nRewriting the first repository test was a minimal change—just\nrewriting some raw SQL with a call to the Django ORM/QuerySet\nlanguage:\nFirst repository test adapted (tests/integration/test_repository.py)\nfrom djangoproject.alloc import models as django_models \n \n \n@pytest.mark.django_db\ndef test_repository_can_save_a_batch(): \n    batch = model.Batch(\"batch1\", \"RUSTY-SOAPDISH\", 100, eta=date(2011, 12, 25)) \n \n    repo = repository.DjangoRepository() \n    repo.add(batch) \n \n    [saved_batch] = django_models.Batch.objects.all() \n    assert saved_batch.reference == batch.reference \n    assert saved_batch.sku == batch.sku",
      "content_length": 929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": null,
      "content": "assert saved_batch.qty == batch._purchased_quantity \n    assert saved_batch.eta == batch.eta\nThe second test is a bit more involved since it has allocations, but it is\nstill made up of familiar-looking Django code:\nSecond repository test is more involved\n(tests/integration/test_repository.py)\n@pytest.mark.django_db\ndef test_repository_can_retrieve_a_batch_with_allocations(): \n    sku = \"PONY-STATUE\" \n    d_line = django_models.OrderLine.objects.create(orderid=\"order1\", sku=sku, \nqty=12) \n    d_b1 = django_models.Batch.objects.create( \n    reference=\"batch1\", sku=sku, qty=100, eta=None\n) \n    d_b2 = django_models.Batch.objects.create( \n    reference=\"batch2\", sku=sku, qty=100, eta=None\n) \n    django_models.Allocation.objects.create(line=d_line, batch=d_batch1) \n \n    repo = repository.DjangoRepository() \n    retrieved = repo.get(\"batch1\") \n \n    expected = model.Batch(\"batch1\", sku, 100, eta=None) \n    assert retrieved == expected  # Batch.__eq__ only compares reference \n    assert retrieved.sku == expected.sku \n    assert retrieved._purchased_quantity == expected._purchased_quantity \n    assert retrieved._allocations == { \n        model.OrderLine(\"order1\", sku, 12), \n    }\nHere’s how the actual repository ends up looking:\nA Django repository (src/allocation/adapters/repository.py)\nclass DjangoRepository(AbstractRepository):",
      "content_length": 1345,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": null,
      "content": "def add(self, batch): \n        super().add(batch) \n        self.update(batch) \n \n    def update(self, batch): \n        django_models.Batch.update_from_domain(batch) \n \n    def _get(self, reference): \n        return django_models.Batch.objects.filter( \n            reference=reference \n        ).first().to_domain() \n \n    def list(self): \n        return [b.to_domain() for b in django_models.Batch.objects.all()]\nYou can see that the implementation relies on the Django models\nhaving some custom methods for translating to and from our domain\nmodel.\nCustom Methods on Django ORM Classes to\nTranslate to/from Our Domain Model\nThose custom methods look something like this:\nDjango ORM with custom methods for domain model conversion\n(src/djangoproject/alloc/models.py)\nfrom django.db import models\nfrom allocation.domain import model as domain_model\nclass Batch(models.Model):\n    reference = models.CharField(max_length=255)\n    sku = models.CharField(max_length=255)\n    qty = models.IntegerField()\n    eta = models.DateField(blank=True, null=True)\n    @staticmethod\n    def update_from_domain(batch: domain_model.Batch):\n1",
      "content_length": 1123,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": null,
      "content": "try:\n            b = Batch.objects.get(reference=batch.reference)  \n        except Batch.DoesNotExist:\n            b = Batch(reference=batch.reference)  \n        b.sku = batch.sku\n        b.qty = batch._purchased_quantity\n        b.eta = batch.eta  \n        b.save()\n        b.allocation_set.set(\n            Allocation.from_domain(l, b)  \n            for l in batch._allocations\n        )\n    def to_domain(self) -> domain_model.Batch:\n        b = domain_model.Batch(\n            ref=self.reference, sku=self.sku, qty=self.qty, eta=self.eta\n        )\n        b._allocations = set(\n            a.line.to_domain()\n            for a in self.allocation_set.all()\n        )\n        return b\nclass OrderLine(models.Model):\n    #...\nFor value objects, objects.get_or_create can work, but for\nentities, you probably need an explicit try-get/except to handle the\nupsert.\nWe’ve shown the most complex example here. If you do decide to\ndo this, be aware that there will be boilerplate! Thankfully it’s not\nvery complex boilerplate.\nRelationships also need some careful, custom handling.\n2",
      "content_length": 1078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": null,
      "content": "NOTE\nAs in Chapter 2, we use dependency inversion. The ORM (Django) depends on\nthe model and not the other way around.\nUnit of Work Pattern with Django\nThe tests don’t change too much:\nAdapted UoW tests (tests/integration/test_uow.py)\ndef insert_batch(ref, sku, qty, eta):  \n    django_models.Batch.objects.create(reference=ref, sku=sku, qty=qty, eta=eta)\ndef get_allocated_batch_ref(orderid, sku):  \n    return django_models.Allocation.objects.get(\n        line__orderid=orderid, line__sku=sku\n    ).batch.reference\n@pytest.mark.django_db(transaction=True)\ndef test_uow_can_retrieve_a_batch_and_allocate_to_it():\n    insert_batch('batch1', 'HIPSTER-WORKBENCH', 100, None)\n    uow = unit_of_work.DjangoUnitOfWork()\n    with uow:\n        batch = uow.batches.get(reference='batch1')\n        line = model.OrderLine('o1', 'HIPSTER-WORKBENCH', 10)\n        batch.allocate(line)\n        uow.commit()\n    batchref = get_allocated_batch_ref('o1', 'HIPSTER-WORKBENCH')\n    assert batchref == 'batch1'\n@pytest.mark.django_db(transaction=True)",
      "content_length": 1031,
      "extraction_method": "Direct"
    },
    {
      "page_number": 421,
      "chapter": null,
      "content": "def test_rolls_back_uncommitted_work_by_default():\n    ...\n@pytest.mark.django_db(transaction=True)  \ndef test_rolls_back_on_error():\n    ...\nBecause we had little helper functions in these tests, the actual\nmain bodies of the tests are pretty much the same as they were with\nSQLAlchemy.\nThe pytest-django mark.django_db(transaction=True) is\nrequired to test our custom transaction/rollback behaviors.\nAnd the implementation is quite simple, although it took me a few tries\nto find which invocation of Django’s transaction magic would work:\nUoW adapted for Django\n(src/allocation/service_layer/unit_of_work.py)\nclass DjangoUnitOfWork(AbstractUnitOfWork):\n    def __enter__(self):\n        self.batches = repository.DjangoRepository()\n        transaction.set_autocommit(False)  \n        return super().__enter__()\n    def __exit__(self, *args):\n        super().__exit__(*args)\n        transaction.set_autocommit(True)\n    def commit(self):\n        for batch in self.batches.seen:  \n            self.batches.update(batch)  \n        transaction.commit()  \n    def rollback(self):\n        transaction.rollback()",
      "content_length": 1106,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": null,
      "content": "set_autocommit(False) was the best way to tell Django to stop\nautomatically committing each ORM operation immediately, and to\nbegin a transaction.\nThen we use the explicit rollback and commits.\nOne difficulty: because, unlike with SQLAlchemy, we’re not\ninstrumenting the domain model instances themselves, the\ncommit() command needs to explicitly go through all the objects\nthat have been touched by every repository and manually update\nthem back to the ORM.\nAPI: Django Views Are Adapters\nThe Django views.py file ends up being almost identical to the old\nflask_app.py, because our architecture means it’s a very thin wrapper\naround our service layer (which didn’t change at all, by the way):\nFlask app → Django views (src/djangoproject/alloc/views.py)\nos.environ['DJANGO_SETTINGS_MODULE'] = 'djangoproject.django_project.settings'\ndjango.setup() \n \n@csrf_exempt\ndef add_batch(request): \n    data = json.loads(request.body) \n    eta = data['eta'] \n    if eta is not None: \n        eta = datetime.fromisoformat(eta).date() \n    services.add_batch( \n        data['ref'], data['sku'], data['qty'], eta, \n        unit_of_work.DjangoUnitOfWork(), \n    ) \n    return HttpResponse('OK', status=201) \n \n@csrf_exempt",
      "content_length": 1208,
      "extraction_method": "Direct"
    },
    {
      "page_number": 423,
      "chapter": null,
      "content": "def allocate(request): \n    data = json.loads(request.body) \n    try: \n        batchref = services.allocate( \n            data['orderid'], \n            data['sku'], \n            data['qty'], \n            unit_of_work.DjangoUnitOfWork(), \n        ) \n    except (model.OutOfStock, services.InvalidSku) as e: \n        return JsonResponse({'message': str(e)}, status=400) \n \n    return JsonResponse({'batchref': batchref}, status=201)\nWhy Was This All So Hard?\nOK, it works, but it does feel like more effort than Flask/SQLAlchemy.\nWhy is that?\nThe main reason at a low level is because Django’s ORM doesn’t\nwork in the same way. We don’t have an equivalent of the\nSQLAlchemy classical mapper, so our ActiveRecord and our domain\nmodel can’t be the same object. Instead we have to build a manual\ntranslation layer behind the repository. That’s more work (although\nonce it’s done, the ongoing maintenance burden shouldn’t be too high).\nBecause Django is so tightly coupled to the database, you have to use\nhelpers like pytest-django and think carefully about test databases,\nright from the very first line of code, in a way that we didn’t have to\nwhen we started out with our pure domain model.",
      "content_length": 1188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": null,
      "content": "But at a higher level, the entire reason that Django is so great is that\nit’s designed around the sweet spot of making it easy to build CRUD\napps with minimal boilerplate. But the entire thrust of our book is\nabout what to do when your app is no longer a simple CRUD app.\nAt that point, Django starts hindering more than it helps. Things like\nthe Django admin, which are so awesome when you start out, become\nactively dangerous if the whole point of your app is to build a complex\nset of rules and modeling around the workflow of state changes. The\nDjango admin bypasses all of that.\nWhat to Do If You Already Have Django\nSo what should you do if you want to apply some of the patterns in this\nbook to a Django app? We’d say the following:\nThe Repository and Unit of Work patterns are going to be\nquite a lot of work. The main thing they will buy you in the\nshort term is faster unit tests, so evaluate whether that benefit\nfeels worth it in your case. In the longer term, they decouple\nyour app from Django and the database, so if you anticipate\nwanting to migrate away from either of those, Repository and\nUoW are a good idea.\nThe Service Layer pattern might be of interest if you’re seeing\na lot of duplication in your views.py. It can be a good way of\nthinking about your use cases separately from your web\nendpoints.\nYou can still theoretically do DDD and domain modeling with\nDjango models, tightly coupled as they are to the database;\nyou may be slowed by migrations, but it shouldn’t be fatal. So",
      "content_length": 1504,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": null,
      "content": "as long as your app is not too complex and your tests not too\nslow, you may be able to get something out of the fat models\napproach: push as much logic down to your models as\npossible, and apply patterns like Entity, Value Object, and\nAggregate. However, see the following caveat.\nWith that said, word in the Django community is that people find that\nthe fat models approach runs into scalability problems of its own,\nparticularly around managing interdependencies between apps. In\nthose cases, there’s a lot to be said for extracting out a business logic\nor domain layer to sit between your views and forms and your\nmodels.py, which you can then keep as minimal as possible.\nSteps Along the Way\nSuppose you’re working on a Django project that you’re not sure is\ngoing to get complex enough to warrant the patterns we recommend,\nbut you still want to put a few steps in place to make your life easier,\nboth in the medium term and if you want to migrate to some of our\npatterns later. Consider the following:\nOne piece of advice we’ve heard is to put a logic.py into\nevery Django app from day one. This gives you a place to put\nbusiness logic, and to keep your forms, views, and models\nfree of business logic. It can become a stepping-stone for\nmoving to a fully decoupled domain model and/or service\nlayer later.\nA business-logic layer might start out working with Django\nmodel objects and only later become fully decoupled from the\nframework and work on plain Python data structures.",
      "content_length": 1484,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": null,
      "content": "For the read side, you can get some of the benefits of CQRS\nby putting reads into one place, avoiding ORM calls\nsprinkled all over the place.\nWhen separating out modules for reads and modules for\ndomain logic, it may be worth decoupling yourself from the\nDjango apps hierarchy. Business concerns will cut across\nthem.\nNOTE\nWe’d like to give a shout-out to David Seddon and Ashia Zawaduk for talking\nthrough some of the ideas in this appendix. They did their best to stop us from\nsaying anything really stupid about a topic we don’t really have enough personal\nexperience of, but they may have failed.\nFor more thoughts and actual lived experience dealing with existing\napplications, refer to the epilogue.\n1  The DRY-Python project people have built a tool called mappers that looks like it might\nhelp minimize boilerplate for this sort of thing.\n2  @mr-bo-jangles suggested you might be able to use update_or_create, but that’s\nbeyond our Django-fu.",
      "content_length": 950,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": null,
      "content": "Appendix E. Validation\nWhenever we’re teaching and talking about these techniques, one\nquestion that comes up over and over is “Where should I do\nvalidation? Does that belong with my business logic in the domain\nmodel, or is that an infrastructural concern?”\nAs with any architectural question, the answer is: it depends!\nThe most important consideration is that we want to keep our code\nwell separated so that each part of the system is simple. We don’t want\nto clutter our code with irrelevant detail.\nWhat Is Validation, Anyway?\nWhen people use the word validation, they usually mean a process\nwhereby they test the inputs of an operation to make sure that they\nmatch certain criteria. Inputs that match the criteria are considered\nvalid, and inputs that don’t are invalid.\nIf the input is invalid, the operation can’t continue but should exit with\nsome kind of error. In other words, validation is about creating\npreconditions. We find it useful to separate our preconditions into\nthree subtypes: syntax, semantics, and pragmatics.\nValidating Syntax",
      "content_length": 1053,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": null,
      "content": "In linguistics, the syntax of a language is the set of rules that govern the\nstructure of grammatical sentences. For example, in English, the\nsentence “Allocate three units of TASTELESS-LAMP to order twenty-\nseven” is grammatically sound, while the phrase “hat hat hat hat hat hat\nwibble” is not. We can describe grammatically correct sentences as\nwell formed.\nHow does this map to our application? Here are some examples of\nsyntactic rules:\nAn Allocate command must have an order ID, a SKU, and a\nquantity.\nA quantity is a positive integer.\nA SKU is a string.\nThese are rules about the shape and structure of incoming data. An\nAllocate command without a SKU or an order ID isn’t a valid\nmessage. It’s the equivalent of the phrase “Allocate three to.”\nWe tend to validate these rules at the edge of the system. Our rule of\nthumb is that a message handler should always receive only a message\nthat is well-formed and contains all required information.\nOne option is to put your validation logic on the message type itself:\nValidation on the message class (src/allocation/commands.py)\nfrom schema import And, Schema, Use\n@dataclass",
      "content_length": 1129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": null,
      "content": "class Allocate(Command):\n    _schema = Schema({  \n        'orderid': int,\n         sku: str,\n         qty: And(Use(int), lambda n: n > 0)\n     }, ignore_extra_keys=True)\n    orderid: str\n    sku: str\n    qty: int\n    @classmethod\n    def from_json(cls, data):  \n       data = json.loads(data)\n       return cls(**_schema.validate(data))\nThe schema library lets us describe the structure and validation of\nour messages in a nice declarative way.\nThe from_json method reads a string as JSON and turns it into our\nmessage type.\nThis can get repetitive, though, since we need to specify our fields\ntwice, so we might want to introduce a helper library that can unify the\nvalidation and declaration of our message types:\nA command factory with schema (src/allocation/commands.py)\ndef command(name, **fields):  \n    schema = Schema(And(Use(json.loads), fields), ignore_extra_keys=True)  \n    cls = make_dataclass(name, fields.keys())\n    cls.from_json = lambda s: cls(**schema.validate(s))  \n    return cls\ndef greater_than_zero(x):\n    return x > 0",
      "content_length": 1043,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": null,
      "content": "quantity = And(Use(int), greater_than_zero)  \nAllocate = command(  \n    orderid=int,\n    sku=str,\n    qty=quantity\n)\nAddStock = command(\n    sku=str,\n    qty=quantity\nThe command function takes a message name, plus kwargs for the\nfields of the message payload, where the name of the kwarg is the\nname of the field and the value is the parser.\nWe use the make_dataclass function from the dataclass module to\ndynamically create our message type.\nWe patch the from_json method onto our dynamic dataclass.\nWe can create reusable parsers for quantity, SKU, and so on to\nkeep things DRY.\nDeclaring a message type becomes a one-liner.\nThis comes at the expense of losing the types on your dataclass, so\nbear that trade-off in mind.\nPostel’s Law and the Tolerant Reader\nPattern\nPostel’s law, or the robustness principle, tells us, “Be liberal in what\nyou accept, and conservative in what you emit.” We think this applies\nparticularly well in the context of integration with our other systems.\nThe idea here is that we should be strict whenever we’re sending",
      "content_length": 1049,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": null,
      "content": "messages to other systems, but as lenient as possible when we’re\nreceiving messages from others.\nFor example, our system could validate the format of a SKU. We’ve\nbeen using made-up SKUs like UNFORGIVING-CUSHION and\nMISBEGOTTEN-POUFFE. These follow a simple pattern: two words,\nseparated by dashes, where the second word is the type of product and\nthe first word is an adjective.\nDevelopers love to validate this kind of thing in their messages, and\nreject anything that looks like an invalid SKU. This causes horrible\nproblems down the line when some anarchist releases a product named\nCOMFY-CHAISE-LONGUE or when a snafu at the supplier results in a\nshipment of CHEAP-CARPET-2.\nReally, as the allocation system, it’s none of our business what the\nformat of a SKU might be. All we need is an identifier, so we can\nsimply describe it as a string. This means that the procurement system\ncan change the format whenever they like, and we won’t care.\nThis same principle applies to order numbers, customer phone\nnumbers, and much more. For the most part, we can ignore the internal\nstructure of strings.\nSimilarly, developers love to validate incoming messages with tools\nlike JSON Schema, or to build libraries that validate incoming\nmessages and share them among systems. This likewise fails the\nrobustness test.",
      "content_length": 1310,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": null,
      "content": "Let’s imagine, for example, that the procurement system adds new\nfields to the ChangeBatchQuantity message that record the reason\nfor the change and the email of the user responsible for the change.\nSince these fields don’t matter to the allocation service, we should\nsimply ignore them. We can do that in the schema library by passing\nthe keyword arg ignore_extra_keys=True.\nThis pattern, whereby we extract only the fields we care about and do\nminimal validation of them, is the Tolerant Reader pattern.\nTIP\nValidate as little as possible. Read only the fields you need, and don’t overspecify\ntheir contents. This will help your system stay robust when other systems change\nover time. Resist the temptation to share message definitions between systems:\ninstead, make it easy to define the data you depend on. For more info, see Martin\nFowler’s article on the Tolerant Reader pattern.\nIS POSTEL ALWAYS RIGHT?\nMentioning Postel can be quite triggering to some people. They will tell you that Postel is the\nprecise reason that everything on the internet is broken and we can’t have nice things. Ask Hynek\nabout SSLv3 one day.\nWe like the Tolerant Reader approach in the particular context of event-based integration\nbetween services that we control, because it allows for independent evolution of those services.\nIf you’re in charge of an API that’s open to the public on the big bad internet, there might be good\nreasons to be more conservative about what inputs you allow.\nValidating at the Edge",
      "content_length": 1496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 433,
      "chapter": null,
      "content": "Earlier, we said that we want to avoid cluttering our code with\nirrelevant details. In particular, we don’t want to code defensively\ninside our domain model. Instead, we want to make sure that requests\nare known to be valid before our domain model or use-case handlers\nsee them. This helps our code stay clean and maintainable over the\nlong term. We sometimes refer to this as validating at the edge of the\nsystem.\nIn addition to keeping your code clean and free of endless checks and\nasserts, bear in mind that invalid data wandering through your system\nis a time bomb; the deeper it gets, the more damage it can do, and the\nfewer tools you have to respond to it.\nBack in Chapter 8, we said that the message bus was a great place to\nput cross-cutting concerns, and validation is a perfect example of that.\nHere’s how we might change our bus to perform validation for us:\nValidation\nclass MessageBus: \n \n    def handle_message(self, name: str, body: str): \n        try: \n            message_type = next(mt for mt in EVENT_HANDLERS if mt.__name__ == \nname) \n            message = message_type.from_json(body) \n            self.handle([message]) \n        except StopIteration: \n            raise KeyError(f\"Unknown message name {name}\") \n        except ValidationError as e: \n            logging.error( \n                f'invalid message of type {name}\\n' \n                f'{body}\\n' \n                f'{e}'",
      "content_length": 1406,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": null,
      "content": ") \n            raise e\nHere’s how we might use that method from our Flask API endpoint:\nAPI bubbles up validation errors (src/allocation/flask_app.py)\n@app.route(\"/change_quantity\", methods=['POST'])\ndef change_batch_quantity(): \n    try: \n        bus.handle_message('ChangeBatchQuantity', request.body) \n    except ValidationError as e: \n        return bad_request(e) \n    except exceptions.InvalidSku as e: \n        return jsonify({'message': str(e)}), 400 \n \ndef bad_request(e: ValidationError): \n    return e.code, 400\nAnd here’s how we might plug it in to our asynchronous message\nprocessor:\nValidation errors when handling Redis messages\n(src/allocation/redis_pubsub.py)\ndef handle_change_batch_quantity(m, bus: messagebus.MessageBus): \n    try: \n        bus.handle_message('ChangeBatchQuantity', m) \n    except ValidationError: \n       print('Skipping invalid message') \n    except exceptions.InvalidSku as e: \n        print(f'Unable to change stock for missing sku {e}')\nNotice that our entrypoints are solely concerned with how to get a\nmessage from the outside world and how to report success or failure.\nOur message bus takes care of validating our requests and routing them",
      "content_length": 1185,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": null,
      "content": "to the correct handler, and our handlers are exclusively focused on the\nlogic of our use case.\nTIP\nWhen you receive an invalid message, there’s usually little you can do but log the\nerror and continue. At MADE we use metrics to count the number of messages a\nsystem receives, and how many of those are successfully processed, skipped, or\ninvalid. Our monitoring tools will alert us if we see spikes in the numbers of bad\nmessages.\nValidating Semantics\nWhile syntax is concerned with the structure of messages, semantics is\nthe study of meaning in messages. The sentence “Undo no dogs from\nellipsis four” is syntactically valid and has the same structure as the\nsentence “Allocate one teapot to order five,\"” but it is meaningless.\nWe can read this JSON blob as an Allocate command but can’t\nsuccessfully execute it, because it’s nonsense:\nA meaningless message\n{ \n  \"orderid\": \"superman\", \n  \"sku\": \"zygote\", \n  \"qty\": -1\n}\nWe tend to validate semantic concerns at the message-handler layer\nwith a kind of contract-based programming:",
      "content_length": 1033,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": null,
      "content": "Preconditions (src/allocation/ensure.py)\n\"\"\" \nThis module contains preconditions that we apply to our handlers. \n\"\"\"\nclass MessageUnprocessable(Exception):  \n    def __init__(self, message):\n        self.message = message\nclass ProductNotFound(MessageUnprocessable):  \n   \"\"\"\" \n   This exception is raised when we try to perform an action on a product \n   that doesn't exist in our database. \n   \"\"\"\"\n    def __init__(self, message):\n        super().__init__(message)\n        self.sku = message.sku\ndef product_exists(event, uow):  \n    product = uow.products.get(event.sku)\n    if product is None:\n        raise ProductNotFound(event)\nWe use a common base class for errors that mean a message is\ninvalid.\nUsing a specific error type for this problem makes it easier to\nreport on and handle the error. For example, it’s easy to map\nProductNotFound to a 404 in Flask.\nproduct_exists is a precondition. If the condition is False, we\nraise an error.\nThis keeps the main flow of our logic in the service layer clean and\ndeclarative:",
      "content_length": 1028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": null,
      "content": "Ensure calls in services (src/allocation/services.py)\n# services.py \n \nfrom allocation import ensure \n \ndef allocate(event, uow): \n    line = mode.OrderLine(event.orderid, event.sku, event.qty) \n    with uow: \n        ensure.product_exists(uow, event) \n \n        product = uow.products.get(line.sku) \n        product.allocate(line) \n        uow.commit()\nWe can extend this technique to make sure that we apply messages\nidempotently. For example, we want to make sure that we don’t insert\na batch of stock more than once.\nIf we get asked to create a batch that already exists, we’ll log a\nwarning and continue to the next message:\nRaise SkipMessage exception for ignorable events\n(src/allocation/services.py)\nclass SkipMessage (Exception): \n    \"\"\"\"\n    This exception is raised when a message can't be processed, but there's no\n    incorrect behavior. For example, we might receive the same message multiple\n    times, or we might receive a message that is now out of date.\n    \"\"\"\" \n \n    def __init__(self, reason): \n        self.reason = reason \n \ndef batch_is_new(self, event, uow): \n    batch = uow.batches.get(event.batchid)",
      "content_length": 1130,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": null,
      "content": "if batch is not None: \n        raise SkipMessage(f\"Batch with id {event.batchid} already exists\")\nIntroducing a SkipMessage exception lets us handle these cases in a\ngeneric way in our message bus:\nThe bus now knows how to skip (src/allocation/messagebus.py)\nclass MessageBus: \n \n    def handle_message(self, message): \n        try: \n           ... \n       except SkipMessage as e: \n           logging.warn(f\"Skipping message {message.id} because {e.reason}\")\nThere are a couple of pitfalls to be aware of here. First, we need to be\nsure that we’re using the same UoW that we use for the main logic of\nour use case. Otherwise, we open ourselves to irritating concurrency\nbugs.\nSecond, we should try to avoid putting all our business logic into these\nprecondition checks. As a rule of thumb, if a rule can be tested inside\nour domain model, then it should be tested in the domain model.\nValidating Pragmatics\nPragmatics is the study of how we understand language in context.\nAfter we have parsed a message and grasped its meaning, we still\nneed to process it in context. For example, if you get a comment on a\npull request saying, “I think this is very brave,” it may mean that the\nreviewer admires your courage—unless they’re British, in which case,",
      "content_length": 1249,
      "extraction_method": "Direct"
    },
    {
      "page_number": 439,
      "chapter": null,
      "content": "they’re trying to tell you that what you’re doing is insanely risky, and\nonly a fool would attempt it. Context is everything.\nVALIDATION RECAP\nValidation means different things to different people\nWhen talking about validation, make sure you’re clear about what you’re validating. We find it\nuseful to think about syntax, semantics, and pragmatics: the structure of messages, the\nmeaningfulness of messages, and the business logic governing our response to\nmessages.\nValidate at the edge when possible\nValidating required fields and the permissible ranges of numbers is boring, and we want to\nkeep it out of our nice clean codebase. Handlers should always receive only valid\nmessages.\nOnly validate what you require\nUse the Tolerant Reader pattern: read only the fields your application needs and don’t\noverspecify their internal structure. Treating fields as opaque strings buys you a lot of\nflexibility.\nSpend time writing helpers for validation\nHaving a nice declarative way to validate incoming messages and apply preconditions to\nyour handlers will make your codebase much cleaner. It’s worth investing time to make\nboring code easy to maintain.\nLocate each of the three types of validation in the right place\nValidating syntax can happen on message classes, validating semantics can happen in the\nservice layer or on the message bus, and validating pragmatics belongs in the domain\nmodel.\nTIP\nOnce you’ve validated the syntax and semantics of your commands at the edges\nof your system, the domain is the place for the rest of your validation. Validation\nof pragmatics is often a core part of your business rules.",
      "content_length": 1618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": null,
      "content": "In software terms, the pragmatics of an operation are usually managed\nby the domain model. When we receive a message like “allocate three\nmillion units of SCARCE-CLOCK to order 76543,” the message is\nsyntactically valid and semantically valid, but we’re unable to\ncomply because we don’t have the stock available.",
      "content_length": 313,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": null,
      "content": "Index\nSYMBOLS\n@abc.abstractmethod, The Repository in the Abstract\nA\nabstract base classes (ABCs)\nABC for the repository, The Repository in the Abstract\ndefining for notifications, Define the Abstract and Concrete\nImplementations\nswitching to typing.Protocol, Option 3: The UoW Publishes\nEvents to the Message Bus\nusing duck typing and protocols instead of, The Repository in the\nAbstract\nusing for ports, What Is a Port and What Is an Adapter, in Python?\nabstract methods, The Repository in the Abstract\nabstractions, A Brief Interlude: On Coupling and Abstractions-Wrap-\nUp\nabstracting state to aid testability, Abstracting State Aids\nTestability-Abstracting State Aids Testability\nAbstractRepository, service function depending on, A Typical\nService Function\nAbstractUnitOfWork, Unit of Work and Its Context Manager",
      "content_length": 817,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": null,
      "content": "choosing right abstraction, Choosing the Right Abstraction(s)-\nImplementing Our Chosen Abstractions\nexplicit dependencies are more abstract, Aren’t Explicit\nDependencies Totally Weird and Java-y?\nimplementing chosen abstraction, Implementing Our Chosen\nAbstractions-Wrap-Up\nedge-to-edge testing with fakes and dependency injection,\nTesting Edge to Edge with Fakes and Dependency Injection-\nTesting Edge to Edge with Fakes and Dependency Injection\nnot using mock.patch for testing, Why Not Just Patch It Out?\nsimplifying interface between business logic and I/O, Wrap-Up\nusing to reduce coupling, A Brief Interlude: On Coupling and\nAbstractions\nadapters\nbuilding adapter and doing dependency injection for it, Building\nan Adapter “Properly”: A Worked Example-Wrap-Up\ndefining abstract and concrete implementations, Define the\nAbstract and Concrete Implementations\ndefined, What Is a Port and What Is an Adapter, in Python?\nDjango views, API: Django Views Are Adapters\nexercise for the reader, Figure Out How to Integration Test the\nReal Thing\nports-and-adapters inspired patterns, Part I Recap\nputting into folder, Putting Things in Folders to See Where It All\nBelongs",
      "content_length": 1167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 443,
      "chapter": null,
      "content": "Aggregate pattern, What Is an Aggregate?\naggregates\nabout, What Is an Aggregate?\nacting as consistency boundaries, Discussion: Events, Commands,\nand Error Handling\nand consistency boundaries recap, Wrap-Up\nchanging multiple aggregates in a request, Wrap-Up\nchoosing an aggregrate, Choosing an Aggregate-Choosing an\nAggregate\nexercise for the reader, What About Performance?\nHistory aggregate recording orders and raising domain events,\nDiscussion: Events, Commands, and Error Handling\nidentifying aggregates and bounded contexts, Identifying\nAggregates and Bounded Contexts-An Event-Driven Approach to\nGo to Microservices via Strangler Pattern\none aggregrate = one repository, One Aggregate = One\nRepository\noptimistic concurrency with version numbers, Optimistic\nConcurrency with Version Numbers-Implementation Options for\nVersion Numbers\nperformance and, What About Performance?\nProduct aggregate, Aggregates and Consistency Boundaries\npros and cons or trade-offs, Wrap-Up\nquery on repository returning single aggregate, Implementation",
      "content_length": 1037,
      "extraction_method": "Direct"
    },
    {
      "page_number": 444,
      "chapter": null,
      "content": "raising events about, Discussion: Events, Commands, and Error\nHandling\nrepository keeping track of aggregates passing through it, Option\n3: The UoW Publishes Events to the Message Bus\ntesting for data integrity rules, Testing for Our Data Integrity\nRules-Pessimistic Concurrency Control Example: SELECT FOR\nUPDATE\ntesting Product object to raise events, The Model Raises Events\nUoW collecting events from and passing them to message bus,\nWrap-Up\nallocate service\nallocating against all batches with, Choosing an Aggregate\nmoving to be a method on Product aggregate, Choosing an\nAggregate\nAllocated event, Our New Outgoing Event\nAllocationRequired event, Refactoring Service Functions to Message\nHandlers\npassing to services.allocate, Imagining an Architecture Change:\nEverything Will Be an Event Handler\nAnemic Domain anti-pattern, The DIP in Action\nAPIs\nadding API for adding a batch, Carrying the Improvement\nThrough to the E2E Tests\nDjango views as adapters, API: Django Views Are Adapters\nend-to-end test of allocate API, A First End-to-End Test",
      "content_length": 1049,
      "extraction_method": "Direct"
    },
    {
      "page_number": 445,
      "chapter": null,
      "content": "modifying API to work with events, A Temporary Ugly Hack: The\nMessage Bus Has to Return Results\nusing repository directly in API endpoint, What Is the Trade-Off?\nwithout Unit of Work pattern, talking directly to three layers, Unit\nof Work Pattern\napplication services, Why Is Everything Called a Service?\narchitecture, summary diagram and table, Summary Diagram and\nTable-Summary Diagram and Table\nasynchronous messaging, temporal decoupling with, The Alternative:\nTemporal Decoupling Using Asynchronous Messaging\natomic operations, Unit of Work Pattern\nUnit of Work as abstraction over, Wrap-Up\nusing Unit of Work to group operations into atomic unit,\nExamples: Using UoW to Group Multiple Operations into an\nAtomic Unit-Example 2: Change Batch Quantity\nB\nBall of Mud pattern, A Brief Interlude: On Coupling and Abstractions\ndistributed ball of mud and thinking in nouns, Distributed Ball of\nMud, and Thinking in Nouns-Distributed Ball of Mud, and\nThinking in Nouns\nseparating responsibilities, Separating Entangled Responsibilities\nBatchCreated event, Refactoring Service Functions to Message\nHandlers\nservices.add_batch as handler for, Imagining an Architecture\nChange: Everything Will Be an Event Handler",
      "content_length": 1208,
      "extraction_method": "Direct"
    },
    {
      "page_number": 446,
      "chapter": null,
      "content": "batches\nallocating against all batches using domain service, Choosing an\nAggregate\nasking Product to allocate against, Choosing an Aggregate\nbatch quantities changed means deallocate and reallocate, A New\nRequirement Leads Us to a New Architecture\ncollection of, Choosing an Aggregate\nBatchQuantityChanged event\nimplementing, Our New Event\ninvoking handler change_batch_quantity, Imagining an\nArchitecture Change: Everything Will Be an Event Handler\nBernhardt, Gary, Implementing Our Chosen Abstractions\nbootstrapping, Dependency Injection (and Bootstrapping)\nbootstrapping script, capabilities of, A Bootstrap Script\nchanging notifications dependency in bootstrap script, Define the\nAbstract and Concrete Implementations\ndependency injection and bootstrap recap, Wrap-Up\ndependency injection with, Aren’t Explicit Dependencies Totally\nWeird and Java-y?\ninitializing dependency injection in tests, Initializing DI in Our\nTests\nusing in entrypoints, Using Bootstrap in Our Entrypoints\nusing to build message bus that talks to real notification class,\nFigure Out How to Integration Test the Real Thing",
      "content_length": 1099,
      "extraction_method": "Direct"
    },
    {
      "page_number": 447,
      "chapter": null,
      "content": "bounded contexts, Choosing an Aggregate\nidentifying aggregates and, Identifying Aggregates and Bounded\nContexts-An Event-Driven Approach to Go to Microservices via\nStrangler Pattern\nproduct concept and, Choosing an Aggregate\nbusiness logic\nabstractions simplifying interface with messy I/O, Wrap-Up\nseparating from state in code, Implementing Our Chosen\nAbstractions\nbusiness logic layer, What Is a Domain Model?\nbusiness rules\ninvariants, concurrency, and locks, Invariants, Concurrency, and\nLocks\ninvariants, constraints, and consistency, Invariants, Constraints,\nand Consistency\nC\nCelery tool, The Message Bus Maps Events to Handlers\nchange_batch_quantity\nhandler tests for, Test-Driving a New Handler\nimplementation, handler delegating to model layer,\nImplementation\nchoreography, Single Responsibility Principle\nclasses, dependency injection using, An Alternative Using Classes",
      "content_length": 882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 448,
      "chapter": null,
      "content": "classical mapping, Inverting the Dependency: ORM Depends on\nModel\nclosures\ndependency injection using, Preparing Handlers: Manual DI with\nClosures and Partials\ndifference from partial functions, Preparing Handlers: Manual DI\nwith Closures and Partials\ncohesion, high, between coupled elements, A Brief Interlude: On\nCoupling and Abstractions\ncollaborators, The Unit of Work Collaborates with the Repository\ncollections, What Is an Aggregate?\nCommand Handler pattern, Wrap-Up\ncommand-query responsibility segregation (CQRS), Command-Query\nResponsibility Segregation (CQRS)-Wrap-Up\nbuilding read-only views into our data, Hold On to Your Lunch,\nFolks\nchanging read model implementation to use Redis, Changing Our\nRead Model Implementation Is Easy\ndenormalized copy of your data optimized for read operations,\nTime to Completely Jump the Shark\ndomain model not optimized for read operations, Your Domain\nModel Is Not Optimized for Read Operations\ndomain models for writing, Domain Models Are for Writing\nfull-blown CQRS versus simpler options, Wrap-Up\nPost/Redirect/Get pattern and CQS, Post/Redirect/Get and CQS",
      "content_length": 1109,
      "extraction_method": "Direct"
    },
    {
      "page_number": 449,
      "chapter": null,
      "content": "read side and write side, Most Users Aren’t Going to Buy Your\nFurniture\nreads, Most Users Aren’t Going to Buy Your Furniture\nconsistency of, Most Users Aren’t Going to Buy Your\nFurniture\nrebuilding view model from scratch, Updating a Read Model\nTable Using an Event Handler\nSELECT N+1 and other performance problems, SELECT N+1\nand Other Performance Considerations\nsimple view using existing repository, “Obvious” Alternative 1:\nUsing the Existing Repository\ntesting views, Testing CQRS Views\ntrade-offs for view model options, Wrap-Up\nupdating read model table using event handler, Time to\nCompletely Jump the Shark\nview that uses the ORM, “Obvious” Alternative 2: Using the\nORM\ncommands, Commands and Command Handler-Wrap-Up\ncommand flow to reserve stock, confirm reservation, dispatch\ngoods, and make customer VIP, Distributed Ball of Mud, and\nThinking in Nouns\ncommand flow when warehouse knows stock is damaged,\nDistributed Ball of Mud, and Thinking in Nouns\ncommand flow with error, Error Handling in Distributed Systems\ncommand handler logic in message bus, Message Bus Is Given\nHandlers at Runtime",
      "content_length": 1105,
      "extraction_method": "Direct"
    },
    {
      "page_number": 450,
      "chapter": null,
      "content": "events versus, Commands and Events-Commands and Events\nevents, commands, and error handling, Discussion: Events,\nCommands, and Error Handling-Discussion: Events, Commands,\nand Error Handling\nrecovering from errors synchronously, Recovering from\nErrors Synchronously\nexception handling, Differences in Exception Handling\nhandlers for, Differences in Exception Handling\nin our system now, Commands and Events\nprogram output as list of commands, Choosing the Right\nAbstraction(s)\nsplitting commands and events, trade-offs, Wrap-Up\ncommits\ncommit method, The Real Unit of Work Uses SQLAlchemy\nSessions\nexplicit tests for, Explicit Tests for Commit/Rollback Behavior\nexplicit versus implicit, Explicit Versus Implicit Commits\ncomponent diagram at end of Part One, Part I Recap\ncomposition over inheritance in TrackingRepository wrapper class,\nOption 3: The UoW Publishes Events to the Message Bus\ncomposition root, Dependency Injection (and Bootstrapping), Aren’t\nExplicit Dependencies Totally Weird and Java-y?\nconcurrency, Invariants, Concurrency, and Locks\naggregates and concurrency issues, Wrap-Up",
      "content_length": 1097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 451,
      "chapter": null,
      "content": "allowing for greatest degree of, What Is an Aggregate?\nenforcing rules using database transactions, Enforcing\nConcurrency Rules by Using Database Transaction Isolation\nLevels\nintegration test for, Testing for Our Data Integrity Rules\nnot provided by message bus implementation, The Message Bus\nMaps Events to Handlers\noptimistic concurrency with version numbers, Optimistic\nConcurrency with Version Numbers-Implementation Options for\nVersion Numbers\npessimistic concurrency example, SELECT FOR UPDATE,\nPessimistic Concurrency Control Example: SELECT FOR\nUPDATE\nreproducing behavior with time.sleep function, Testing for Our\nData Integrity Rules\nconnascence, Error Handling in Distributed Systems\nconsistency, Invariants, Constraints, and Consistency\nattainment of read consistency, Most Users Aren’t Going to Buy\nYour Furniture\neventually consistent reads, Most Users Aren’t Going to Buy Your\nFurniture\nconsistency boundaries, Aggregates and Consistency Boundaries, What\nIs an Aggregate?\naggregates acting as, Discussion: Events, Commands, and Error\nHandling",
      "content_length": 1058,
      "extraction_method": "Direct"
    },
    {
      "page_number": 452,
      "chapter": null,
      "content": "microservices as, The Alternative: Temporal Decoupling Using\nAsynchronous Messaging\nrecap, Wrap-Up\nconstraints, Invariants, Constraints, and Consistency\ncontext manager, Unit of Work Pattern\nstarting Unit of Work as, The Unit of Work Collaborates with the\nRepository\nUnit of Work and, Unit of Work and Its Context Manager-Fake\nUnit of Work for Testing\ncontrol flow, using exceptions for, The Model Raises Events\ncoupling, A Brief Interlude: On Coupling and Abstractions\navoiding inappropriate coupling, Error Handling in Distributed\nSystems\ndisadvantages of, A Brief Interlude: On Coupling and\nAbstractions\ndomain logic coupled with I/O, Abstracting State Aids Testability\nfailure cascade as temporal coupling, Error Handling in\nDistributed Systems\nin tests that use mocks, Why Not Just Patch It Out?\nreducing by abstracting away details, A Brief Interlude: On\nCoupling and Abstractions\nseparating what you want to do from how to do it, Choosing the\nRight Abstraction(s)\ntemporal decoupling using asynchronous messaging, The\nAlternative: Temporal Decoupling Using Asynchronous",
      "content_length": 1076,
      "extraction_method": "Direct"
    },
    {
      "page_number": 453,
      "chapter": null,
      "content": "Messaging\ntrade-off between design feedback and, On Deciding What Kind\nof Tests to Write\nCQRS (see command-query responsibility segregation)\nCQS (command-query separation), Post/Redirect/Get and CQS\nCRUD wrapper around a database, Part I Recap\nCSV over SMTP architecture, Why Not Just Run Everything in a\nSpreadsheet?\nCSVs, doing everything with, Swapping Out the Infrastructure: Do\nEverything with CSVs-Implementing a Repository and Unit of Work\nfor CSVs\nD\ndata access, applying dependency inversion principle to, Applying the\nDIP to Data Access\ndata integrity\nissues arising from splitting operation across two UoWs,\nImplementing Our New Requirement\ntesting for, Testing for Our Data Integrity Rules-Pessimistic\nConcurrency Control Example: SELECT FOR UPDATE\ndata storage, Repository pattern and, Repository Pattern\ndatabases\nSQLAlchemy adding session for Unit of Work, The Real Unit of\nWork Uses SQLAlchemy Sessions\ntesting allocations persisted to database, The Straightforward\nImplementation",
      "content_length": 996,
      "extraction_method": "Direct"
    },
    {
      "page_number": 454,
      "chapter": null,
      "content": "testing transactions against real database, Explicit Tests for\nCommit/Rollback Behavior\nUnit of Work pattern managing state for, Unit of Work Pattern\ndataclasses\nevents, Events Are Simple Dataclasses\nuse for message types, Recovering from Errors Synchronously\nuse for value objects, Dataclasses Are Great for Value Objects\ndeallocate service, building (exerise), A Typical Service Function\ndependencies\nabstract dependencies of service layer, The DIP in Action\ntesting, The DIP in Action\ncircular dependencies between event handlers, Wrap-Up\ndepending on abstractions, A Typical Service Function\nedge-to-edge testing with dependency injection, Testing Edge to\nEdge with Fakes and Dependency Injection-Testing Edge to Edge\nwith Fakes and Dependency Injection\nkeeping all domain dependencies in fixture functions, Mitigation:\nKeep All Domain Dependencies in Fixture Functions\nnone in domain model, Applying the DIP to Data Access\nreal service layer dependencies at runtime, The DIP in Action\nservice layer dependency on abstract UoW, Using the UoW in the\nService Layer\nUoW no longer dependent on message bus, The Message Bus\nNow Collects Events from the UoW",
      "content_length": 1155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 455,
      "chapter": null,
      "content": "dependency chains, A Bootstrap Script\ndependency injection, Dependency Injection (and Bootstrapping)-An\nAlternative Using Classes\nby inspecting function signatures, A Bootstrap Script\nexplicit dependencies are better than implicit dependencies,\nAren’t Explicit Dependencies Totally Weird and Java-y?\nimplicit versus explicit dependencies, Implicit Versus Explicit\nDependencies\nmanual creation of partial functions inline, A Bootstrap Script\nmanual DI with closures or partial functions, Preparing Handlers:\nManual DI with Closures and Partials\nrecap of DI and bootstrap, Wrap-Up\nusing classes, An Alternative Using Classes\nusing DI framework, A Bootstrap Script\ndependency inversion principle, Applying the DIP to Data Access,\nWrap-Up\ndeclaring explicit dependency as example of, Aren’t Explicit\nDependencies Totally Weird and Java-y?\nORM depends on the data model, Inverting the Dependency:\nORM Depends on Model\ndictionaries\ndictionary of hashes to paths, Implementing Our Chosen\nAbstractions\nfor filesystem operations, Choosing the Right Abstraction(s)",
      "content_length": 1054,
      "extraction_method": "Direct"
    },
    {
      "page_number": 456,
      "chapter": null,
      "content": "HANDLERS dicts for commands and events, Differences in\nException Handling\ndirectory structure, putting project into folders, Putting Things in\nFolders to See Where It All Belongs\nDistributed Ball of Mud anti-pattern\nand thinking in nouns, Distributed Ball of Mud, and Thinking in\nNouns-Distributed Ball of Mud, and Thinking in Nouns\navoiding, The Alternative: Temporal Decoupling Using\nAsynchronous Messaging\nDjango, Repository and Unit of Work Patterns with Django-Steps\nAlong the Way\napplying patterns to Django app, What to Do If You Already Have\nDjango\nsteps along the way, Steps Along the Way\ninstalling, Repository and Unit of Work Patterns with Django\nORM example, The “Normal” ORM Way: Model Depends on\nORM\nRepository pattern with, Repository Pattern with Django-Custom\nMethods on Django ORM Classes to Translate to/from Our\nDomain Model\nUnit of Work pattern with, Unit of Work Pattern with Django-Unit\nof Work Pattern with Django\nusing, difficulty of, Why Was This All So Hard?\nviews are adapters, API: Django Views Are Adapters\nDocker dev environment with real fake email server, Figure Out How\nto Integration Test the Real Thing",
      "content_length": 1139,
      "extraction_method": "Direct"
    },
    {
      "page_number": 457,
      "chapter": null,
      "content": "domain driven design (DDD), Domain Modeling, What Is a Domain\nModel?\n(see also domain model; domain modeling)\nAggregate pattern, What Is an Aggregate?\nbounded contexts, Choosing an Aggregate\nchoosing the right aggregate, references on, Wrap-Up\ndomain, defined, What Is a Domain Model?\nRepository pattern and, What Is the Trade-Off?\nDomain Events pattern, Events and the Message Bus\ndomain exceptions, Exceptions Can Express Domain Concepts Too\ndomain language, Exploring the Domain Language\ndomain layer\nfully decoupling service layer from, Fully Decoupling the\nService-Layer Tests from the Domain-Adding a Missing Service\ntests moving to service layer, Should Domain Layer Tests Move\nto the Service Layer?\nreasons for, Should Domain Layer Tests Move to the\nService Layer?\ndomain model, Reminder: Our Model-Introducing the Repository\nPattern\ndeciding whether to write tests against, On Deciding What Kind\nof Tests to Write\nDjango custom ORM methods for conversion, Custom Methods\non Django ORM Classes to Translate to/from Our Domain Model",
      "content_length": 1039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 458,
      "chapter": null,
      "content": "email sending code in, avoiding, And Let’s Not Make a Mess of\nOur Model Either\nevents from, passing to message bus in service layer, Option 1:\nThe Service Layer Takes Events from the Model and Puts Them\non the Message Bus\nfolder for, Putting Things in Folders to See Where It All Belongs\ngetting benefits of rich model, The DIP in Action\ninvariants, constraints, and consistency, Invariants, Constraints,\nand Consistency\nmaintaining small core of tests written against, Wrap-Up\nnew method on, change_batch_quantity, A New Method on the\nDomain Model\nnot optimized for read operations, Your Domain Model Is Not\nOptimized for Read Operations\npersisting, Persisting Our Domain Model\nraising events, The Model Raises Events\nraising events and service layer passing them to message bus,\nWrap-Up\ntrade-offs as a diagram, Wrap-Up\ntranslating to relational database\nnormal ORM way, model depends on ORM, The “Normal”\nORM Way: Model Depends on ORM\nORM depends on the model, Inverting the Dependency:\nORM Depends on Model",
      "content_length": 1010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 459,
      "chapter": null,
      "content": "using spreadsheets instead of, Why Not Just Run Everything in a\nSpreadsheet?\nwriting data, Domain Models Are for Writing\nwriting tests against, High and Low Gear\ndomain modeling, Domain Modeling-Exceptions Can Express Domain\nConcepts Too\ndomain language, Exploring the Domain Language\nfunctions for domain services, Not Everything Has to Be an\nObject: A Domain Service Function-Exceptions Can Express\nDomain Concepts Too\nunit testing domain models, Unit Testing Domain Models-Value\nObjects and Entities\ndataclasses for value objects, Dataclasses Are Great for\nValue Objects\nvalue objects and entities, Value Objects and Entities\ndomain services, Not Everything Has to Be an Object: A Domain\nService Function, Why Is Everything Called a Service?\nfunction for, Not Everything Has to Be an Object: A Domain\nService Function\ndriven adapters, Putting Things in Folders to See Where It All Belongs\nduck typing, The Repository in the Abstract\nfor ports, What Is a Port and What Is an Adapter, in Python?\nE\nE2E tests (see end-to-end tests)",
      "content_length": 1031,
      "extraction_method": "Direct"
    },
    {
      "page_number": 460,
      "chapter": null,
      "content": "eager loading, SELECT N+1 and Other Performance Considerations\nedge-to-edge testing, Implementing Our Chosen Abstractions-Testing\nEdge to Edge with Fakes and Dependency Injection\nEffective Aggregate Design (Vernon), Wrap-Up\nemail alerts, sending when out of stock, Avoiding Making a Mess-Or\nthe Service Layer!\nend-to-end tests\naiming for one test per feature, Wrap-Up\ndecoupling of service layer from domain, carrying through to,\nCarrying the Improvement Through to the E2E Tests\nof allocate API, A First End-to-End Test\nreplacement with unit tests, Why Not Just Patch It Out?\n__enter__ and __exit__ magic methods, Unit of Work and Its Context\nManager, The Real Unit of Work Uses SQLAlchemy Sessions\nentities\ndefined, Value Objects and Entities\nidentity equality, Value Objects and Entities\nvalue objects versus, Exceptions Can Express Domain Concepts\nToo\nentrypoints, Putting Things in Folders to See Where It All Belongs\n__eq__magic method, Value Objects and Entities\nequality operators, implementing on entities, Value Objects and\nEntities\nerror handling",
      "content_length": 1057,
      "extraction_method": "Direct"
    },
    {
      "page_number": 461,
      "chapter": null,
      "content": "counting as a feature, Wrap-Up\nevents, commands, and, Discussion: Events, Commands, and\nError Handling-Discussion: Events, Commands, and Error\nHandling\nin distributed systems, Error Handling in Distributed Systems-\nThe Alternative: Temporal Decoupling Using Asynchronous\nMessaging\nerrors, recovering from synchronously, Recovering from Errors\nSynchronously\nEvans, Eric, What Is an Aggregate?\nevent handlers\nimagined architecture in which everything is an event handler,\nImagining an Architecture Change: Everything Will Be an Event\nHandler\nin message bus, Message Bus Is Given Handlers at Runtime\nmanaging updates to read model, Changing Our Read Model\nImplementation Is Easy\nupdating read model table using, Time to Completely Jump the\nShark\nevent storming, A New Requirement Leads Us to a New Architecture\nevent-driven architecture\ngoing to microservices via Strangler pattern, An Event-Driven\nApproach to Go to Microservices via Strangler Pattern-An\nEvent-Driven Approach to Go to Microservices via Strangler\nPattern",
      "content_length": 1019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 462,
      "chapter": null,
      "content": "using events to integrate microservices, Event-Driven\nArchitecture: Using Events to Integrate Microservices-Wrap-Up\nevents\nchanging schema over time, Footguns\ncommands versus, Commands and Events-Differences in\nException Handling\nevents, commands, and error handling, Discussion: Events,\nCommands, and Error Handling-Discussion: Events, Commands,\nand Error Handling\ninternal versus external, Internal Versus External Events\nsplitting command and events, trade-offs, Wrap-Up\nevents and the message bus, Events and the Message Bus-Wrap-Up\ndomain events and message bus recap, Wrap-Up\ndomain model raising events, The Model Raises Events\nevents as simple dataclasses, Events Are Simple Dataclasses\nevents flowing through the system, Events and the Message Bus\nmessage bus mapping events to handlers, The Message Bus Maps\nEvents to Handlers\npros and cons or trade-offs, Wrap-Up\nrecording events, The Model Records Events\nsending email alerts when out of stock, Avoiding Making a Mess-\nOr the Service Layer!\navoiding messing up domain model, And Let’s Not Make a\nMess of Our Model Either",
      "content_length": 1082,
      "extraction_method": "Direct"
    },
    {
      "page_number": 463,
      "chapter": null,
      "content": "avoiding messing up web controllers, First, Let’s Avoid\nMaking a Mess of Our Web Controllers\nout of place in the service layer, Or the Service Layer!\nviolating the single responsibility principle, Single\nResponsibility Principle\nservice layer raising its own events, Option 2: The Service Layer\nRaises Its Own Events\nservice layer with explicit message bus, Option 1: The Service\nLayer Takes Events from the Model and Puts Them on the\nMessage Bus\ntransforming our app into message processor, Going to Town on\nthe Message Bus-Why Have We Achieved?\nimagined architecture, everything will be an event handler,\nImagining an Architecture Change: Everything Will Be an\nEvent Handler\nimplementing the new requirement, Implementing Our New\nRequirement-Test-Driving a New Handler\nmodifying API to work with events, A Temporary Ugly\nHack: The Message Bus Has to Return Results\nnew requirement and new architecture, A New Requirement\nLeads Us to a New Architecture\nrefactoring service functions to message handlers,\nRefactoring Service Functions to Message Handlers\ntemporary hack, message bus returning results, A Temporary\nUgly Hack: The Message Bus Has to Return Results\ntest driving new handler, Test-Driving a New Handler",
      "content_length": 1215,
      "extraction_method": "Direct"
    },
    {
      "page_number": 464,
      "chapter": null,
      "content": "tests writtern to in terms of events, Our Tests Are All\nWritten in Terms of Events Too\nunit testing event handlers with fake message bus,\nOptionally: Unit Testing Event Handlers in Isolation with a\nFake Message Bus\nwhole app as message bus, trade-offs, Why Have We\nAchieved?\nUoW publishes events to message bus, Option 3: The UoW\nPublishes Events to the Message Bus\neventually consistent reads, Most Users Aren’t Going to Buy Your\nFurniture\nexception handling, differences for events and commands, Differences\nin Exception Handling\nexceptions\nexpressing domain concepts, Exceptions Can Express Domain\nConcepts Too\nusing for control flow, The Model Raises Events\nexternal events, The Message Bus Maps Events to Handlers, Event-\nDriven Architecture: Using Events to Integrate Microservices-Wrap-\nUp\nextreme programming (XP), exhortation to listen to the code, On\nDeciding What Kind of Tests to Write\nF\nfaking\nFakeNotifications for unit testing, Make a Fake Version for Your\nTests",
      "content_length": 977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 465,
      "chapter": null,
      "content": "FakeRepository, Introducing a Service Layer, and Using\nFakeRepository to Unit Test It\nadding fixture function on, Mitigation: Keep All Domain\nDependencies in Fixture Functions\nnew query type on, Implementation\nusing to unit test the service layer, Introducing a Service\nLayer, and Using FakeRepository to Unit Test It\nfakes versus mocks, Why Not Just Patch It Out?\nFakeSession, using to unit test the service layer, Introducing a\nService Layer, and Using FakeRepository to Unit Test It\nFakeUnitOfWork for service layer testing, Fake Unit of Work for\nTesting\nfaking I/O in edge-to-edge test, Testing Edge to Edge with Fakes\nand Dependency Injection\ntweaking fakes in service layer to call super and implement\nunderscorey methods, Option 3: The UoW Publishes Events to the\nMessage Bus\nfilesystems\nwriting code to synchronize source and target directories,\nAbstracting State Aids Testability-Abstracting State Aids\nTestability\nchoosing right abstraction, Choosing the Right\nAbstraction(s)-Implementing Our Chosen Abstractions\nimplementing chosen abstraction, Implementing Our Chosen\nAbstractions-Wrap-Up",
      "content_length": 1100,
      "extraction_method": "Direct"
    },
    {
      "page_number": 466,
      "chapter": null,
      "content": "fixture functions, keeping all domain dependencies in, Mitigation:\nKeep All Domain Dependencies in Fixture Functions\nFlask framework, Some Pseudocode: What Are We Going to Need?\nAPI endpoint, What Is the Trade-Off?\ncalling bootstrap in entrypoints, Using Bootstrap in Our\nEntrypoints\nendpoint for viewing allocations, Post/Redirect/Get and CQS\nFlask API and service layer, Our First Use Case: Flask API and\nService Layer-The DIP in Action\napp delegating to service layer, A Typical Service Function\nconnecting the app to real world, Connecting Our\nApplication to the Real World\ndifferent types of services, Why Is Everything Called a\nService?\nend-to-end tests for happy and unhappy paths, A Typical\nService Function\nerror conditions requiring database checks, Error Conditions\nThat Require Database Checks\nfirst API end-to-end test, A First End-to-End Test-A First\nEnd-to-End Test\nfirst cut of the app, The Straightforward Implementation-The\nStraightforward Implementation\nintroducing service layer and fake repo to unit test it,\nIntroducing a Service Layer, and Using FakeRepository to\nUnit Test It-A Typical Service Function",
      "content_length": 1126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 467,
      "chapter": null,
      "content": "putting project into folders, Putting Things in Folders to See\nWhere It All Belongs\nservice layer benefits, Wrap-Up\nservice layer dependencies, The DIP in Action\nservice layer pros and cons, The DIP in Action\ntypical service layer function, A Typical Service Function\nputting API endpoint in front of allocate domain service,\nConnecting Our Application to the Real World\nFowler, Martin, Why Not Just Patch It Out?, Wrap-Up\nFreeman, Steve, Why Not Just Patch It Out?\nFunctional Core, Imperative Shell (FCIS), Implementing Our Chosen\nAbstractions\nfunctions, Exceptions Can Express Domain Concepts Too\nfor domain services, Not Everything Has to Be an Object: A\nDomain Service Function\nservice layer, A Typical Service Function\nG\n\"Global Complexity, Local Simplicity\" post, Wrap-Up\n__gt__ magic method, Python’s Magic Methods Let Us Use Our\nModels with Idiomatic Python\nH\nhandlers",
      "content_length": 876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 468,
      "chapter": null,
      "content": "event and command handlers in message bus, Message Bus Is\nGiven Handlers at Runtime\nnew HANDLERS dicts for commands and events, Differences in\nException Handling\n__hash__ magic method, Value Objects and Entities\nhashing a file, Abstracting State Aids Testability\ndictionary of hashes to paths, Choosing the Right Abstraction(s)\nhoisting I/O, Why Not Just Patch It Out?\nI\nI/O\ndisentangling details from program logic, Implementing Our\nChosen Abstractions\ndomain logic tightly coupled to, Abstracting State Aids\nTestability\nsimplifying interface with business logic using abstractions,\nWrap-Up\nidempotent message handling, Footguns\nidentity equality (entities), Value Objects and Entities\nimplicit versus explicit commits, Explicit Versus Implicit Commits\nimporting dependenies, Aren’t Explicit Dependencies Totally Weird\nand Java-y?\ninheritance, avoiding use of with wrapper class, Option 3: The UoW\nPublishes Events to the Message Bus\nintegration tests",
      "content_length": 952,
      "extraction_method": "Direct"
    },
    {
      "page_number": 469,
      "chapter": null,
      "content": "for concurrency behavior, Testing for Our Data Integrity Rules\ntest-driving Unit of Work with, Test-Driving a UoW with\nIntegration Tests\ninterfaces, Python and, What Is a Port and What Is an Adapter, in\nPython?\ninvariants\ninvariants, concurrency, and locks, Invariants, Concurrency, and\nLocks\ninvariants, constraints, and consistency, Invariants, Constraints,\nand Consistency\nprotecting while allowing concurrency, What Is an Aggregate?\ninward-facing adapters, Putting Things in Folders to See Where It All\nBelongs\nisolation levels (transaction), Enforcing Concurrency Rules by Using\nDatabase Transaction Isolation Levels\nJ\nJung, Ed, Why Not Just Patch It Out?\nK\nkatas, A Brief Interlude: On Coupling and Abstractions\nL\nlayered architecture, Applying the DIP to Data Access\ncase study, layering an overgrown system, Separating Entangled\nResponsibilities",
      "content_length": 853,
      "extraction_method": "Direct"
    },
    {
      "page_number": 470,
      "chapter": null,
      "content": "locks on database tables, Invariants, Concurrency, and Locks\noptimistic locking, Optimistic Concurrency with Version\nNumbers, Optimistic Concurrency with Version Numbers\npessimistic locking, Optimistic Concurrency with Version\nNumbers\nLondon-school versus classic-style TDD, Why Not Just Patch It Out?\nM\nmagic methods\nallowing use of domain model with idiomatic Python, Python’s\nMagic Methods Let Us Use Our Models with Idiomatic Python\n__enter__ and __exit__, Unit of Work and Its Context Manager\n__eq__, Value Objects and Entities\n__hash__, Value Objects and Entities\nMagicMock objects, Why Not Just Patch It Out?\nmappers, Inverting the Dependency: ORM Depends on Model\nmessage brokers, Using a Redis Pub/Sub Channel for Integration\nmessage bus\nabstract message bus and its real and fake versions, Optionally:\nUnit Testing Event Handlers in Isolation with a Fake Message\nBus\nbefore, message buse as optional add-on, Going to Town on the\nMessage Bus\nCelery and, The Message Bus Maps Events to Handlers",
      "content_length": 1002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 471,
      "chapter": null,
      "content": "class given handlers at runtime, Message Bus Is Given Handlers\nat Runtime\ndispatching events and commands differently, Differences in\nException Handling\nevent and command handler logic staying the same, Message Bus\nIs Given Handlers at Runtime\ngetting custom with overridden bootstrap defaults, Initializing DI\nin Our Tests\nhandler publishing outgoing event, Our New Outgoing Event\nhandle_event method, Recovering from Errors Synchronously\nhandle_event with retries, Recovering from Errors Synchronously\nmapping events to handlers, The Message Bus Maps Events to\nHandlers\nnow collecting events from UoW, The Message Bus Now\nCollects Events from the UoW\nnow the main entrypoint to service layer, Going to Town on the\nMessage Bus\npros and cons or trade-offs, Wrap-Up\nrecap, Wrap-Up\nRedis pub/sub listener as thin adapter around, Redis Is Another\nThin Adapter Around Our Message Bus\nreturning results in temporary hack, A Temporary Ugly Hack: The\nMessage Bus Has to Return Results\nservice layer raising events and calling messagebus.handle,\nOption 2: The Service Layer Raises Its Own Events",
      "content_length": 1087,
      "extraction_method": "Direct"
    },
    {
      "page_number": 472,
      "chapter": null,
      "content": "service layer with explicit message bus, Option 1: The Service\nLayer Takes Events from the Model and Puts Them on the\nMessage Bus\nUnit of Work publishing events to, Option 3: The UoW Publishes\nEvents to the Message Bus\nunit testing event handlers with fake message bus, Optionally:\nUnit Testing Event Handlers in Isolation with a Fake Message\nBus\nwhole app as, trade-offs, Why Have We Achieved?\nwiring up new event handlers to, A New Method on the Domain\nModel\nMessage Bus pattern, Events and the Message Bus\nmessaging\nasynchronous, temporal decoupling with, The Alternative:\nTemporal Decoupling Using Asynchronous Messaging\nidempotent message handling, Footguns\nreliable messaging is hard, Footguns\nusing Redis pub/sub channel for microservices integration, Using\na Redis Pub/Sub Channel for Integration\nmicroservices\nbounded contexts and, Choosing an Aggregate\nevent-based integration, Event-Driven Architecture: Using Events\nto Integrate Microservices-Wrap-Up\ndistributed Ball of Mud and thinking in nouns, Distributed\nBall of Mud, and Thinking in Nouns-Distributed Ball of\nMud, and Thinking in Nouns",
      "content_length": 1103,
      "extraction_method": "Direct"
    },
    {
      "page_number": 473,
      "chapter": null,
      "content": "error handling in distributed systems, Error Handling in\nDistributed Systems-The Alternative: Temporal Decoupling\nUsing Asynchronous Messaging\ntemporal decoupling using asynchronous messaging, The\nAlternative: Temporal Decoupling Using Asynchronous\nMessaging\ntesting with end-to-end test, Test-Driving It All Using an\nEnd-to-End Test-Internal Versus External Events\ntrade-offs, Wrap-Up\nusing Redis pub/sub channel for ntegration, Using a Redis\nPub/Sub Channel for Integration\nevent-driven approach, using Strangler pattern, An Event-Driven\nApproach to Go to Microservices via Strangler Pattern-An\nEvent-Driven Approach to Go to Microservices via Strangler\nPattern\nminimum viable product, Persisting Our Domain Model\nmock.patch method, Why Not Just Patch It Out?, Aren’t Explicit\nDependencies Totally Weird and Java-y?\nmocking\navoiding use of mock.patch, Why Not Just Patch It Out?\ndon't mock what you don't own, Fake Unit of Work for Testing\nmocks versus fakes, Why Not Just Patch It Out?\novermocked tests, pitfalls of, Why Not Just Patch It Out?\n\"Mocks Aren't Stubs\" (Fowler), Why Not Just Patch It Out?\nmodel (domain), What Is a Domain Model?",
      "content_length": 1144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 474,
      "chapter": null,
      "content": "N\nnamed tuples, Dataclasses Are Great for Value Objects\n(see also dataclasses)\nnouns, splitting system into, Distributed Ball of Mud, and Thinking in\nNouns-Distributed Ball of Mud, and Thinking in Nouns\nO\nobject neighborhoods, The Unit of Work Collaborates with the\nRepository\nobject-oriented composition, Why Not Just Patch It Out?\nobject-oriented design principles, Exceptions Can Express Domain\nConcepts Too\nobject-relational mappers (ORMs), The “Normal” ORM Way: Model\nDepends on ORM\nassociating right batches with Product objects, One Aggregate =\nOne Repository\nDjango ORM example, The “Normal” ORM Way: Model\nDepends on ORM\nDjango, custom methods to translate to/from domain model,\nCustom Methods on Django ORM Classes to Translate to/from\nOur Domain Model\nORM depends on the data model, Inverting the Dependency:\nORM Depends on Model\ntesting the ORM, Inverting the Dependency: ORM Depends\non Model\norm.start_mappers function, A Bootstrap Script",
      "content_length": 951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 475,
      "chapter": null,
      "content": "Repository pattern and, What Is the Trade-Off?\nSELECT N+1 performance problem, SELECT N+1 and Other\nPerformance Considerations\nsimple view using the ORM, “Obvious” Alternative 2: Using the\nORM\nSQLAlchemy, model depends on ORM, The “Normal” ORM\nWay: Model Depends on ORM\nonion architecture, Applying the DIP to Data Access\noptimistic concurrency with version numbers, Optimistic Concurrency\nwith Version Numbers-Implementation Options for Version Numbers\norchestration, Introducing a Service Layer, and Using FakeRepository\nto Unit Test It\nchanging to choreography, Single Responsibility Principle\nusing application service, Why Is Everything Called a Service?\norchestration layer (see service layer)\nOutbox pattern, Footguns\nP\npartial functions\ndependency injection with, Preparing Handlers: Manual DI with\nClosures and Partials\ndifference from closures, Preparing Handlers: Manual DI with\nClosures and Partials\nmanually creating inline, A Bootstrap Script\npatterns, deciding whether you need to use them, Part I Recap",
      "content_length": 1018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 476,
      "chapter": null,
      "content": "PEP 544 protocols, The Repository in the Abstract\nperformance\nconsistency boundaries and, Aggregates and Consistency\nBoundaries, Wrap-Up\nimpact of using aggregates, Choosing an Aggregate, What About\nPerformance?\npersistence ignorance, The “Normal” ORM Way: Model Depends on\nORM\ntrade-offs, Wrap-Up\npessimistic concurrency, Optimistic Concurrency with Version\nNumbers\nexample, SELECT FOR UPDATE, Pessimistic Concurrency\nControl Example: SELECT FOR UPDATE\nports\ndefined, What Is a Port and What Is an Adapter, in Python?\nports-and-adapters inspired patterns, Part I Recap\nputting in folder with adapters, Putting Things in Folders to See\nWhere It All Belongs\nPost/Redirect/Get pattern, Post/Redirect/Get and CQS\ncommand-query separation (CQS), Post/Redirect/Get and CQS\nPostgreSQL\nAnti-Patterns: Read-Modify-Write Cycles, Pessimistic\nConcurrency Control Example: SELECT FOR UPDATE\ndocumentation for transaction isolation levels, Enforcing\nConcurrency Rules by Using Database Transaction Isolation",
      "content_length": 994,
      "extraction_method": "Direct"
    },
    {
      "page_number": 477,
      "chapter": null,
      "content": "Levels\nmanaging concurrency issues, Optimistic Concurrency with\nVersion Numbers\nSERIALIZABLE transaction isolation level, Optimistic\nConcurrency with Version Numbers\npreparatory refactoring workflow, Imagining an Architecture Change:\nEverything Will Be an Event Handler\nprimitives\nmoving from domain objects to, in service layer, Refactoring\nService Functions to Message Handlers\nprimitive obsession, Refactoring Service Functions to Message\nHandlers\nProduct object, Aggregates and Consistency Boundaries\nacting as consistency boundary, Discussion: Events, Commands,\nand Error Handling\nasking Product to allocate against its batches, Choosing an\nAggregate\ncode for, Choosing an Aggregate\nservice layer using, One Aggregate = One Repository\ntwo transactions attempting concurrent update on, Optimistic\nConcurrency with Version Numbers\nversion numbers implemented on, Implementation Options for\nVersion Numbers\nProductRepository object, One Aggregate = One Repository\nprojects",
      "content_length": 974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 478,
      "chapter": null,
      "content": "organizing into folders, Putting Things in Folders to See Where It\nAll Belongs\ntemplate project structure, A Template Project Structure-Wrap-\nUp\nprotocols, abstract base classes, duck typing, and, The Repository in\nthe Abstract\npublish-subscribe system\nmessage bus as\nhandlers subscribed to receive events, The Message Bus\nMaps Events to Handlers\npublishing step, Option 1: The Service Layer Takes Events\nfrom the Model and Puts Them on the Message Bus\nusing Redis pub/sub channel for microservices integration, Using\na Redis Pub/Sub Channel for Integration\nPyCon talk on Mocking Pitfalls, Why Not Just Patch It Out?\npytest\n@pytest.skip, What About Performance?\nfixtures, Abstracting State Aids Testability\npytest-django plug-in, Repository Pattern with Django, Why Was\nThis All So Hard?\nsession argument, Inverting the Dependency: ORM Depends on\nModel\nQ\nqueries, Command-Query Responsibility Segregation (CQRS)",
      "content_length": 911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 479,
      "chapter": null,
      "content": "(see also command-query responsibility segregation)\nquestions from tech reviewers, Questions Our Tech Reviewers Asked\nThat We Couldn’t Work into Prose-Footguns\nR\nread-modify-write failure mode, Pessimistic Concurrency Control\nExample: SELECT FOR UPDATE\nreallocate service function, Example 1: Reallocate\nreallocation\nsequence diagram for flow, Implementing Our New Requirement\ntesting in isolation using fake message bus, Optionally: Unit\nTesting Event Handlers in Isolation with a Fake Message Bus\nRedis pub/sub channel, using for microservices integration, Using a\nRedis Pub/Sub Channel for Integration\ntesting pub/sub model, Test-Driving It All Using an End-to-End\nTest\npublishing outgoing event, Our New Outgoing Event\nRedis as thin adapter around message bus, Redis Is Another\nThin Adapter Around Our Message Bus\nRedis, changing read model implementation to use, Changing Our Read\nModel Implementation Is Easy\nrepositories\nadding list method to existing repository object, Hold On to Your\nLunch, Folks\nCSV-based repository, Implementing a Repository and Unit of\nWork for CSVs",
      "content_length": 1080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 480,
      "chapter": null,
      "content": "new query type on our repository, Implementation\none aggregrate = one repository, One Aggregate = One\nRepository\nrepository keeping track of aggregates passing through it, Option\n3: The UoW Publishes Events to the Message Bus\nservice layer function depending on abstract repository, A\nTypical Service Function\nsimple view using existing repository, “Obvious” Alternative 1:\nUsing the Existing Repository\nTrackerRepository wrapper class, Option 3: The UoW Publishes\nEvents to the Message Bus\nUnit of Work collaborating with, The Unit of Work Collaborates\nwith the Repository\nRepository pattern, Repository Pattern, Introducing the Repository\nPattern-Wrap-Up\nand persistence ignorance, trade-offs, Wrap-Up\nbuilding fake repository for tests, Building a Fake Repository for\nTests Is Now Trivial!\nORMs and, What Is the Trade-Off?\nrecap of important points, Wrap-Up\nsimplest possible repository, The Repository in the Abstract\ntesting the repository with retrieving a complex object, What Is\nthe Trade-Off?\ntesting the repository with saving an object, What Is the Trade-\nOff?",
      "content_length": 1071,
      "extraction_method": "Direct"
    },
    {
      "page_number": 481,
      "chapter": null,
      "content": "trade-offs, What Is the Trade-Off?\ntypical repository, What Is the Trade-Off?\nusing repository directly in API endpoint, What Is the Trade-Off?\nwith Django, Repository Pattern with Django-Custom Methods on\nDjango ORM Classes to Translate to/from Our Domain Model\nresources, additional required reading, More Required Reading\nresponsibilities of code, Choosing the Right Abstraction(s)\nseparating responsibilities, Separating Entangled Responsibilities\ncase study, layering overgrown system, Separating Entangled\nResponsibilities\nretries\nmessage bus handle_event with, Recovering from Errors\nSynchronously\noptimistic concurrency control and, Optimistic Concurrency with\nVersion Numbers\nTenacity library for, Recovering from Errors Synchronously\nRhodes, Brandon, Why Not Just Patch It Out?\nrollbacks, Unit of Work and Its Context Manager\nexplicit tests for, Explicit Tests for Commit/Rollback Behavior\nrollback method, The Real Unit of Work Uses SQLAlchemy\nSessions\nS\nseams, Wrap-Up",
      "content_length": 980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 482,
      "chapter": null,
      "content": "secondary adapters, Putting Things in Folders to See Where It All\nBelongs\nSeemann, Mark, blog post, Applying the DIP to Data Access\nSELECT * FROM WHERE queries, Time to Completely Jump the\nShark\nSELECT FOR UPDATE statement, Optimistic Concurrency with\nVersion Numbers\npessimistic concurrency control example with, Pessimistic\nConcurrency Control Example: SELECT FOR UPDATE\nSELECT N+1, SELECT N+1 and Other Performance Considerations\nservice functions\nmaking them event handlers, Imagining an Architecture Change:\nEverything Will Be an Event Handler\nrefactoring to message handlers, Refactoring Service Functions to\nMessage Handlers\nservice layer, Our First Use Case: Flask API and Service Layer-The\nDIP in Action\nbenefits of, Wrap-Up\nbenefits to test-driven development, Wrap-Up\nconnecting our application to real world, Connecting Our\nApplication to the Real World\ndependencies of, The DIP in Action\nreal dependencies at runtime, The DIP in Action\ntesting, The DIP in Action",
      "content_length": 975,
      "extraction_method": "Direct"
    },
    {
      "page_number": 483,
      "chapter": null,
      "content": "difference between domain service and, Why Is Everything\nCalled a Service?\ndomain layer tests moving to, Should Domain Layer Tests Move\nto the Service Layer?\nreasons for, Should Domain Layer Tests Move to the\nService Layer?\nend-to-end test of allocate API, testing happy and unhappy paths,\nA Typical Service Function\nerror conditions requiring database checks in Flask app, Error\nConditions That Require Database Checks\nfirst cut of Flask app, The Straightforward Implementation-The\nStraightforward Implementation\nFlask app delegating to, A Typical Service Function\nfrom domain objects to primitives to events as interface,\nRefactoring Service Functions to Message Handlers\nfully decoupling from the domain, Fully Decoupling the Service-\nLayer Tests from the Domain-Adding a Missing Service\nintroducing and using FakeRepository to unit test it, Introducing a\nService Layer, and Using FakeRepository to Unit Test It-Why Is\nEverything Called a Service?\nmessage bus as main entrypoint, Going to Town on the Message\nBus\npros and cons or trade-offs, The DIP in Action\nputting project in folders, Putting Things in Folders to See Where\nIt All Belongs\nraising events and passing them to message bus, Wrap-Up",
      "content_length": 1200,
      "extraction_method": "Direct"
    },
    {
      "page_number": 484,
      "chapter": null,
      "content": "raising its own events, Option 2: The Service Layer Raises Its\nOwn Events\nsending email alerts when out of stock, avoiding, Or the Service\nLayer!\ntaking events from model and putting them on message bus,\nOption 1: The Service Layer Takes Events from the Model and\nPuts Them on the Message Bus\ntotally free of event handling concerns, Option 3: The UoW\nPublishes Events to the Message Bus\ntweaking fakes in to call super and implement underscorey\nmethods, Option 3: The UoW Publishes Events to the Message\nBus\ntypical service function, A Typical Service Function\nusing Product objects, One Aggregate = One Repository\nusing Unit of Work in, Using the UoW in the Service Layer\nusing, test pyramid and, How Is Our Test Pyramid Looking?\nwriting bulk of tests against, Wrap-Up\nwriting tests against, High and Low Gear\nservice-layer services vs. domain services, Not Everything Has to Be\nan Object: A Domain Service Function\nservices\napplication service and domain service, Why Is Everything\nCalled a Service?\nservice layer tests only using services, Adding a Missing Service\nSession object, Wrap-Up",
      "content_length": 1092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 485,
      "chapter": null,
      "content": "set, fake repository as wrapper around, Building a Fake Repository for\nTests Is Now Trivial!\nsimplifying abstractions, Choosing the Right Abstraction(s)\nsingle responsibility principle (SRP), Single Responsibility Principle\nSingleton pattern, messagebus.py implementing, Optionally: Unit\nTesting Event Handlers in Isolation with a Fake Message Bus\nsituated software, A New Requirement Leads Us to a New\nArchitecture\nSoftware Engineering Stack Exchange site, Why Not Just Patch It Out?\nspreadsheets, using instead of domain model, Why Not Just Run\nEverything in a Spreadsheet?\nspy objects, Testing Edge to Edge with Fakes and Dependency\nInjection\nSQL\ngenerating for domain model objects, The “Normal” ORM Way:\nModel Depends on ORM\nhelpers for Unit of Work, Test-Driving a UoW with Integration\nTests\nORM and Repository pattern as abstractions in front of, What Is\nthe Trade-Off?\nraw SQL in views, Hold On to Your Lunch, Folks\nrepository test for retrieving complex object, What Is the Trade-\nOff?\nrepository test for saving an object, What Is the Trade-Off?\nSQLAlchemy",
      "content_length": 1066,
      "extraction_method": "Direct"
    },
    {
      "page_number": 486,
      "chapter": null,
      "content": "database session for Unit of Work, The Real Unit of Work Uses\nSQLAlchemy Sessions\nnot mocking, Fake Unit of Work for Testing\ndeclarative syntax, model depends on ORM, The “Normal” ORM\nWay: Model Depends on ORM\nexplicit ORM mapping with SQLAlchemy Table objects,\nInverting the Dependency: ORM Depends on Model\nSELECT N+1 problem and, SELECT N+1 and Other\nPerformance Considerations\nSession object, Wrap-Up\nusing directly in API endpoint, Inverting the Dependency: ORM\nDepends on Model\nusing DSL to specify FOR UPDATE, Pessimistic Concurrency\nControl Example: SELECT FOR UPDATE\nstakeholders, convincing to try something new, Convincing Your\nStakeholders to Try Something New-Questions Our Tech Reviewers\nAsked That We Couldn’t Work into Prose\nstate\nabstracting to aid testability, Abstracting State Aids Testability-\nAbstracting State Aids Testability\nsplitting off from logic in the program, Implementing Our Chosen\nAbstractions\nstorage, Repository Pattern\n(see also repositories; Repository pattern)\npermanent, UoW providing entrypoint to, The Unit of Work\nCollaborates with the Repository",
      "content_length": 1089,
      "extraction_method": "Direct"
    },
    {
      "page_number": 487,
      "chapter": null,
      "content": "Strangler pattern, going to microservices via, An Event-Driven\nApproach to Go to Microservices via Strangler Pattern-An Event-\nDriven Approach to Go to Microservices via Strangler Pattern\nstubbing, mocks and stubs, Why Not Just Patch It Out?\nsuper function, Option 3: The UoW Publishes Events to the Message\nBus\ntweaking fakes in service layer to call, Option 3: The UoW\nPublishes Events to the Message Bus\nsynchronous execution of event-handling code, Wrap-Up\nT\ntemporal coupling, Error Handling in Distributed Systems\ntemporal decoupling using asynchronous messaging, The Alternative:\nTemporal Decoupling Using Asynchronous Messaging\nTenacity library, Recovering from Errors Synchronously\ntest doubles\nmocks versus fakes, Why Not Just Patch It Out?\nmocks versus stubs, Why Not Just Patch It Out?\nusing lists to build, Testing Edge to Edge with Fakes and\nDependency Injection\n\"Test-Driven Development: That's Not What We Meant\", Why Not Just\nPatch It Out?\ntest-driven development (TDD), TDD in High Gear and Low Gear-\nWrap-Up\nbenefits of service layer to, Wrap-Up",
      "content_length": 1064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 488,
      "chapter": null,
      "content": "classic versus London-school, Why Not Just Patch It Out?\ndeciding what kinds of tests to write, On Deciding What Kind of\nTests to Write\ndomain layer tests moving to service layer, Should Domain Layer\nTests Move to the Service Layer?\nfully decoupling service layer from the domain, Fully Decoupling\nthe Service-Layer Tests from the Domain-Adding a Missing\nService\nadding missing service, Adding a Missing Service\ncarrying improvement through to E2E tests, Carrying the\nImprovement Through to the E2E Tests\nkeeping all domain dependencies in fixture functions,\nMitigation: Keep All Domain Dependencies in Fixture\nFunctions\nhigh and low gear, High and Low Gear\ntest pyramid with service layer added, How Is Our Test Pyramid\nLooking?\ntest pyramid, examining, TDD in High Gear and Low Gear\ntypes of tests, rules of thumb for, Wrap-Up\nunit tests operating at lower level, acting directly on model, TDD\nin High Gear and Low Gear\ntesting\nabstracting state to aid testability, Abstracting State Aids\nTestability-Abstracting State Aids Testability\nafter implementing chosen abstraction, Implementing Our Chosen\nAbstractions-Wrap-Up",
      "content_length": 1121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 489,
      "chapter": null,
      "content": "avoiding use of mock.patch, Why Not Just Patch It Out?-\nWrap-Up\nedge-to-edge testing with fakes and dependency injection,\nTesting Edge to Edge with Fakes and Dependency Injection-\nTesting Edge to Edge with Fakes and Dependency Injection\nend-to-end test of pub/sub model, Test-Driving It All Using an\nEnd-to-End Test\nfake database session at service layer, Introducing a Service\nLayer, and Using FakeRepository to Unit Test It\nfake UoW for service layer testing, Fake Unit of Work for Testing\nfor data integrity rules, Testing for Our Data Integrity Rules-\nPessimistic Concurrency Control Example: SELECT FOR\nUPDATE\nintegration test for CQRS view, Testing CQRS Views\nintegration test for overriding bootstrap defaults, Initializing DI in\nOur Tests\nintegration tests for rollback behavior, Explicit Tests for\nCommit/Rollback Behavior\ntests folder tree, Tests\ntests written in terms of events, Our Tests Are All Written in\nTerms of Events Too\nhandler tests for change_batch_quantity, Test-Driving a New\nHandler\nunit testing event handlers with fake message bus,\nOptionally: Unit Testing Event Handlers in Isolation with a\nFake Message Bus",
      "content_length": 1135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 490,
      "chapter": null,
      "content": "Unit of Work with integration tests, Test-Driving a UoW with\nIntegration Tests\ntidying up tests, Tidying Up the Integration Tests\nunit test for bootstrap, Initializing DI in Our Tests\nunit testing with fakes at service layer, Introducing a Service\nLayer, and Using FakeRepository to Unit Test It\ntime.sleep function, Testing for Our Data Integrity Rules\nreproducing concurrency behavior with, Testing for Our Data\nIntegrity Rules\ntransactions\nconcurrent, attempting update on Product, Optimistic Concurrency\nwith Version Numbers\nsimulating a slow transaction, Testing for Our Data Integrity\nRules\nUnit of Work and, Wrap-Up\nusing to enforce concurrency rules, Enforcing Concurrency Rules\nby Using Database Transaction Isolation Levels\ntype hints, Unit Testing Domain Models, Unit Testing Domain Models\nU\nunderscorey methods\navoiding by implementing TrackingRepository wrapper class,\nOption 3: The UoW Publishes Events to the Message Bus\ntweaking fakes in service layer to implement, Option 3: The UoW\nPublishes Events to the Message Bus",
      "content_length": 1035,
      "extraction_method": "Direct"
    },
    {
      "page_number": 491,
      "chapter": null,
      "content": "Unit of Work pattern, The Repository in the Abstract, Unit of Work\nPattern-Wrap-Up\nand its context manager, Unit of Work and Its Context Manager\nfake UoW for testing, Fake Unit of Work for Testing\nreal UoW using SQLAlchemy session, The Real Unit of\nWork Uses SQLAlchemy Sessions\nbenefits of using, Wrap-Up\ncollaboration with repository, The Unit of Work Collaborates\nwith the Repository\nexplicit tests for commit/rollback behavior, Explicit Tests for\nCommit/Rollback Behavior\nexplicit versus implicit commits, Explicit Versus Implicit\nCommits\nfake message bus implemented in UoW, Optionally: Unit Testing\nEvent Handlers in Isolation with a Fake Message Bus\ngetting rid of underscorey methods in UoW class, Option 3: The\nUoW Publishes Events to the Message Bus\nmanaging database state, Unit of Work Pattern\nmessage bus now collecting events from UoW, The Message Bus\nNow Collects Events from the UoW\nmodifying to connect domain events and message bus, Events and\nthe Message Bus\npros and cons or trade-offs, Wrap-Up\nrecap of important points, Wrap-Up",
      "content_length": 1049,
      "extraction_method": "Direct"
    },
    {
      "page_number": 492,
      "chapter": null,
      "content": "splitting operations across two UoWs, Implementing Our New\nRequirement\ntest driving with integration tests, Test-Driving a UoW with\nIntegration Tests\ntidying up integration tests, Tidying Up the Integration Tests\nUoW and product repository, One Aggregate = One Repository\nUoW collecting events from aggregates and passing them to\nmessage bus, Wrap-Up\nUoW for CSVs, Implementing a Repository and Unit of Work for\nCSVs\nUoW managing success or failure of aggregate update,\nDiscussion: Events, Commands, and Error Handling\nUoW publishing events to message bus, Option 3: The UoW\nPublishes Events to the Message Bus\nusing UoW in service layer, Using the UoW in the Service Layer\nusing UoW to group multiple operations into atomic unit,\nExamples: Using UoW to Group Multiple Operations into an\nAtomic Unit-Example 2: Change Batch Quantity\nchanging batch quantity example, Example 2: Change Batch\nQuantity\nreallocate function example, Example 1: Reallocate\nwith Django, Unit of Work Pattern with Django-Unit of Work\nPattern with Django\nwithout, API talking directly to three layers, Unit of Work Pattern\nunit testing, Introducing a Service Layer, and Using FakeRepository to\nUnit Test It",
      "content_length": 1180,
      "extraction_method": "Direct"
    },
    {
      "page_number": 493,
      "chapter": null,
      "content": "(see also test-driven development; testing)\nof domain models, Unit Testing Domain Models-Value Objects\nand Entities\nunit tests replacing end-to-end tests, Why Not Just Patch It Out?\nunittest.mock function, Why Not Just Patch It Out?\nUoW (see Unit of Work pattern)\nuse-case layer (see service layer)\nV\nvalidation, Validation-Validating Pragmatics\nvalue objects\ndefined, Dataclasses Are Great for Value Objects\nand entities, Value Objects and Entities\nentities versus, Exceptions Can Express Domain Concepts Too\nmath with, Dataclasses Are Great for Value Objects\nusing dataclasses for, Dataclasses Are Great for Value Objects\nVens, Rob, Wrap-Up\nVernon, Vaughn, Wrap-Up\nversion numbers\nimplementation options for, Implementation Options for Version\nNumbers\nin the products table, implementing optimistic locking, Optimistic\nConcurrency with Version Numbers\nviews",
      "content_length": 859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 494,
      "chapter": null,
      "content": "Django views as adapters, API: Django Views Are Adapters\nkeeping totally separate, denormalized datastore for view model,\nTime to Completely Jump the Shark\nread-only, Post/Redirect/Get and CQS\nrebuilding view model from scratch, Updating a Read Model\nTable Using an Event Handler\nsimple view that uses the ORM, “Obvious” Alternative 2: Using\nthe ORM\nsimple view that uses the repository, “Obvious” Alternative 1:\nUsing the Existing Repository\ntesting CQRS views, Testing CQRS Views\ntrade-offs for view model options, Wrap-Up\nupdating read model table using event handler, Time to\nCompletely Jump the Shark\nW\nweb controllers, sending email alerts via, avoiding, Avoiding Making\na Mess",
      "content_length": 683,
      "extraction_method": "Direct"
    },
    {
      "page_number": 495,
      "chapter": null,
      "content": "About the Authors\nHarry Percival spent a few years being deeply unhappy as a\nmanagement consultant. Soon he rediscovered his true geek nature and\nwas lucky enough to fall in with a bunch of XP fanatics, working on\npioneering the sadly defunct Resolver One spreadsheet. He worked at\nPythonAnywhere LLP, spreading the gospel of TDD worldwide at\ntalks, workshops, and conferences. He is now with MADE.com.\nBob Gregory is a UK-based software architect with MADE.com. He\nhas been building event-driven systems with domain-driven design for\nmore than a decade.",
      "content_length": 554,
      "extraction_method": "Direct"
    },
    {
      "page_number": 496,
      "chapter": null,
      "content": "Colophon\nThe animal on the cover of Architecture Patterns with Python is a\nBurmese python (Python bivitattus). As you might expect, the Burmese\npython is native to Southeast Asia. Today it lives in jungles and\nmarshes in South Asia, Myanmar, China, and Indonesia; it’s also\ninvasive in Florida’s Everglades.\nBurmese pythons are one of the world’s largest species of snakes.\nThese nocturnal, carnivorous constrictors can grow to 23 feet and 200\npounds. Females are larger than males. They can lay up to a hundred\neggs in one clutch. In the wild, Burmese pythons live an average of 20\nto 25 years.\nThe markings on a Burmese python begin with an arrow-shaped spot of\nlight brown on top of the head and continue along the body in\nrectangles that stand out against its otherwise tan scales. Before they\nreach their full size, which takes two to three years, Burmese pythons\nlive in trees hunting small mammals and birds. They also swim for\nlong stretches of time—going up to 30 minutes without air.\nBecause of habitat destruction, the Burmese python has a conservation\nstatus of Vulnerable. Many of the animals on O’Reilly’s covers are\nendangered; all of them are important to the world.\nThe color illustration is by Jose Marzan, based on a black-and-white\nengraving from Encyclopedie D’Histoire Naturelle. The cover fonts\nare URW Typewriter and Guardian Sans. The text font is Adobe",
      "content_length": 1378,
      "extraction_method": "Direct"
    },
    {
      "page_number": 497,
      "chapter": null,
      "content": "Minion Pro; the heading font is Adobe Myriad Condensed; and the\ncode font is Dalton Maag’s Ubuntu Mono.",
      "content_length": 103,
      "extraction_method": "Direct"
    }
  ],
  "enrichment": {
    "version": "1.0.0",
    "generated_by": "generate_chapter_metadata.py",
    "contains": [
      "keywords",
      "concepts",
      "summary"
    ]
  }
}