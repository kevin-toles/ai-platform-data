{
  "metadata": {
    "title": "Effective-Python",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 469,
    "conversion_date": "2025-11-28T13:05:06.189059",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Effective-Python.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 2-9)",
      "start_page": 2,
      "end_page": 9,
      "detection_method": "topic_boundary",
      "content": "“This is a great book for both novice and experienced programmers. The code \nexamples and explanations are well thought out and explained concisely and \nthoroughly. The second edition updates the advice for Python 3, and it’s fantastic! \nI’ve been using Python for almost 20 years, and I learned something new every \nfew pages. The advice given in this book will serve anyone well.”\n—Titus Brown, Associate Professor at UC Davis\n“Once again, Brett Slatkin has managed to condense a wide range of solid prac-\ntices from the community into a single volume. From exotic topics like metaclasses \nand concurrency to crucial basics like robustness, testing, and collaboration, the \nupdated Effective Python makes a consensus view of what’s ‘Pythonic’ available to \na wide audience.”\n—Brandon Rhodes, Author of python-patterns.guide\n\n\nEffective Python\nSecond Edition\n\n\nThis page intentionally left blank \n\n\nEffective Python\n90 SPECIFIC WAYS TO WRITE BETTER PYTHON\nSecond Edition\nBrett Slatkin\n\n\nMany of the designations used by manufacturers and sellers to distinguish their \nproducts are claimed as trademarks. Where those designations appear in this \nbook, and the publisher was aware of a trademark claim, the designations have \nbeen printed with initial capital letters or in all capitals.\nThe author and publisher have taken care in the preparation of this book, but \nmake no expressed or implied warranty of any kind and assume no responsibility \nfor errors or omissions. No liability is assumed for incidental or consequential \ndamages in connection with or arising out of the use of the information or \nprograms contained herein.\nFor information about buying this title in bulk quantities, or for special \nsales opportunities (which may include electronic versions; custom cover \ndesigns; and content particular to your business, training goals, marketing \nfocus, or branding interests), please contact our corporate sales department \nat corpsales@pearsoned.com or (800) 382-3419.\nFor government sales inquiries, please contact governmentsales@pearsoned.com. \nFor questions about sales outside the U.S., please contact intlcs@pearson.com. \nVisit us on the Web: informit.com/aw\nLibrary of Congress Control Number: On file\nCopyright © 2020 Pearson Education, Inc.\nAll rights reserved. This publication is protected by copyright, and permission \nmust be obtained from the publisher prior to any prohibited reproduction, storage \nin a retrieval system, or transmission in any form or by any means, electronic, \nmechanical, photocopying, recording, or likewise. For information regarding \npermissions, request forms and the appropriate contacts within the Pearson \nEducation Global Rights & Permissions Department, please visit www.pearson.\ncom/permissions/.\nISBN-13: 978-0-13-485398-7\nISBN-10: 0-13-485398-9\nScoutAutomatedPrintCode\nEditor-in-Chief\nMark L. Taub\nExecutive Editor\nDeborah Williams\nDevelopment Editor\nChris Zahn\nManaging Editor\nSandra Schroeder\nSenior Project Editor\nLori Lyons\nProduction Manager\nAswini Kumar/codeMantra\nCopy Editor\nCatherine D. Wilson\nIndexer\nCheryl Lenser\nProofreader\nGill Editorial Services\nCover Designer\nChuti Prasertsith\nCompositor\ncodeMantra\n\n\nTo our family\n\n\nContents at a Glance\nPreface \nxvii\nAcknowledgments \nxxi\nAbout the Author \nxxiii\nChapter 1: Pythonic Thinking \n1\nChapter 2: Lists and Dictionaries \n43\nChapter 3: Functions \n77\nChapter 4: Comprehensions and Generators \n107\nChapter 5: Classes and Interfaces \n145\nChapter 6: Metaclasses and Attributes \n181\nChapter 7: Concurrency and Parallelism \n225\nChapter 8: Robustness and Performance \n299\nChapter 9: Testing and Debugging \n353\nChapter 10: Collaboration \n389\nIndex \n435\n\n\nContents\nPreface \nxvii\nAcknowledgments \nxxi\nAbout the Author \nxxiii\nChapter 1 Pythonic Thinking \n1\nItem 1: Know Which Version of Python You’re Using \n1\nItem 2: Follow the PEP 8 Style Guide \n2\nItem 3: Know the Differences Between bytes and str \n5\nItem 4:  Prefer Interpolated F-Strings Over C-style \nFormat Strings and str.format \n11\nItem 5:  Write Helper Functions Instead of \nComplex Expressions \n21\nItem 6:  Prefer Multiple Assignment Unpacking \nOver Indexing \n24\nItem 7: Prefer enumerate Over range \n28\nItem 8: Use zip to Process Iterators in Parallel \n30\nItem 9: Avoid else Blocks After for and while Loops \n32\nItem 10:  Prevent Repetition with Assignment Expressions \n35\nChapter 2 Lists and Dictionaries \n43\nItem 11: Know How to Slice Sequences \n43\nItem 12:  Avoid Striding and Slicing in a Single Expression \n46\nItem 13: Prefer Catch-All Unpacking Over Slicing \n48\nItem 14:  Sort by Complex Criteria Using the key Parameter \n52\n",
      "page_number": 2,
      "chapter_number": 1,
      "summary": "This chapter covers segment 1 (pages 2-9). Key topics include python, pythonic, and editor. I’ve been working with \nPython for nearly twenty years and I still learned a bunch of useful tricks, espe-\ncially around newer features introduced by Python 3.",
      "keywords": [
        "Python",
        "Effective Python",
        "Python Pandas project",
        "Effective Python makes",
        "updated Effective Python",
        "blank Effective Python",
        "Python Pandas",
        "Effective",
        "edition",
        "practical Python programming",
        "Python programming wisdom",
        "unique features Python",
        "book",
        "advice",
        "Brett Slatkin"
      ],
      "concepts": [
        "python",
        "pythonic",
        "editor",
        "sales",
        "advice",
        "programming",
        "programs",
        "features",
        "lists",
        "edition"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.8,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.79,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.79,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 39,
          "title": "Segment 39 (pages 353-355)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.72,
          "method": "api"
        }
      ]
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 10-18)",
      "start_page": 10,
      "end_page": 18,
      "detection_method": "topic_boundary",
      "content": "xii \nContents\nItem 15:  Be Cautious When Relying on dict \nInsertion Ordering \n58\nItem 16:  Prefer get Over in and KeyError to \nHandle Missing Dictionary Keys \n65\nItem 17:  Prefer defaultdict Over setdefault to \nHandle Missing Items in Internal State \n70\nItem 18:  Know How to Construct Key-Dependent \nDefault Values with __missing__ \n73\nChapter 3 Functions \n77\nItem 19:  Never Unpack More Than Three Variables \nWhen Functions Return Multiple Values \n77\nItem 20: Prefer Raising Exceptions to Returning None \n80\nItem 21:  Know How Closures Interact with Variable Scope \n83\nItem 22:  Reduce Visual Noise with Variable \nPositional Arguments \n87\nItem 23:  Provide Optional Behavior with Keyword Arguments \n90\nItem 24:  Use None and Docstrings to Specify \nDynamic Default Arguments \n94\nItem 25:  Enforce Clarity with Keyword-Only and \nPositional-Only Arguments \n97\nItem 26:  Define Function Decorators with functools.wraps \n102\nChapter 4 Comprehensions and Generators \n107\nItem 27:  Use Comprehensions Instead of map and filter \n107\nItem 28:  Avoid More Than Two Control Subexpressions in \nComprehensions \n109\nItem 29:  Avoid Repeated Work in Comprehensions by Using \nAssignment Expressions \n111\nItem 30:  Consider Generators Instead of Returning Lists \n114\nItem 31: Be Defensive When Iterating Over Arguments \n117\nItem 32:  Consider Generator Expressions for Large List \nComprehensions \n122\nItem 33: Compose Multiple Generators with yield from \n124\nItem 34:  Avoid Injecting Data into Generators with send \n127\nItem 35:  Avoid Causing State Transitions in \nGenerators with throw \n133\n\n\n \nContents \nxiii\nItem 36:  Consider itertools for Working with Iterators \nand Generators \n138\nChapter 5 Classes and Interfaces \n145\nItem 37:  Compose Classes Instead of Nesting \nMany Levels of Built-in Types \n145\nItem 38:  Accept Functions Instead of Classes for \nSimple Interfaces \n152\nItem 39:  Use @classmethod Polymorphism to \nConstruct Objects Generically \n155\nItem 40: Initialize Parent Classes with super \n160\nItem 41:  Consider Composing Functionality \nwith Mix-in Classes \n165\nItem 42: Prefer Public Attributes Over Private Ones \n170\nItem 43:  Inherit from collections.abc for \nCustom Container Types \n175\nChapter 6 Metaclasses and Attributes \n181\nItem 44:  Use Plain Attributes Instead of Setter and \nGetter Methods \n181\nItem 45:  Consider @property Instead of \nRefactoring Attributes \n186\nItem 46:  Use Descriptors for Reusable @property Methods \n190\nItem 47:  Use __getattr__, __getattribute__, and \n__setattr__ for Lazy Attributes \n195\nItem 48: Validate Subclasses with __init_subclass__ \n201\nItem 49:  Register Class Existence with __init_subclass__ \n208\nItem 50: Annotate Class Attributes with __set_name__ \n214\nItem 51:  Prefer Class Decorators Over Metaclasses for \nComposable Class Extensions \n218\nChapter 7 Concurrency and Parallelism \n225\nItem 52: Use subprocess to Manage Child Processes \n226\nItem 53:  Use Threads for Blocking I/O, Avoid for Parallelism \n230\nItem 54: Use Lock to Prevent Data Races in Threads \n235\nItem 55:  Use Queue to Coordinate Work Between Threads \n238\nItem 56:  Know How to Recognize When Concurrency \nIs Necessary \n248\n\n\nxiv \nContents\nItem 57:  Avoid Creating New Thread Instances for \nOn-demand Fan-out \n252\nItem 58:  Understand How Using Queue for \nConcurrency Requires Refactoring \n257\nItem 59:  Consider ThreadPoolExecutor When Threads \nAre Necessary for Concurrency \n264\nItem 60:  Achieve Highly Concurrent I/O with Coroutines \n266\nItem 61: Know How to Port Threaded I/O to asyncio \n271\nItem 62:  Mix Threads and Coroutines to Ease the \nTransition to asyncio \n282\nItem 63:  Avoid Blocking the asyncio Event Loop to \nMaximize Responsiveness \n289\nItem 64:  Consider concurrent.futures for True Parallelism \n292\nChapter 8 Robustness and Performance \n299\nItem 65:  Take Advantage of Each Block in try/except \n/else/finally \n299\nItem 66:  Consider contextlib and with Statements \nfor Reusable try/finally Behavior \n304\nItem 67: Use datetime Instead of time for Local Clocks \n308\nItem 68: Make pickle Reliable with copyreg \n312\nItem 69: Use decimal When Precision Is Paramount \n319\nItem 70: Profile Before Optimizing \n322\nItem 71: Prefer deque for Producer–Consumer Queues \n326\nItem 72:  Consider Searching Sorted Sequences with bisect \n334\nItem 73: Know How to Use heapq for Priority Queues \n336\nItem 74:  Consider memoryview and bytearray for \nZero-Copy Interactions with bytes \n346\nChapter 9 Testing and Debugging \n353\nItem 75: Use repr Strings for Debugging Output \n354\nItem 76:  Verify Related Behaviors in TestCase Subclasses \n357\nItem 77:  Isolate Tests from Each Other with setUp, \ntearDown, setUpModule, and tearDownModule \n365\nItem 78:  Use Mocks to Test Code with \nComplex Dependencies \n367\n\n\n \nContents \nxv\nItem 79:  Encapsulate Dependencies to Facilitate \nMocking and Testing \n375\nItem 80: Consider Interactive Debugging with pdb \n379\nItem 81:  Use tracemalloc to Understand Memory \nUsage and Leaks \n384\nChapter 10 Collaboration \n389\nItem 82: Know Where to Find Community-Built Modules \n389\nItem 83:  Use Virtual Environments for Isolated and \nReproducible Dependencies \n390\nItem 84:  Write Docstrings for Every Function, \nClass, and Module \n396\nItem 85:  Use Packages to Organize Modules and \nProvide Stable APIs \n401\nItem 86:  Consider Module-Scoped Code to \nConfigure Deployment Environments \n406\nItem 87: Define a Root Exception to Insulate \nCallers from APIs \n408\nItem 88:  Know How to Break Circular Dependencies \n413\nItem 89:  Consider warnings to Refactor and Migrate Usage \n418\nItem 90:  Consider Static Analysis via typing to Obviate Bugs 425\nIndex \n435\n\n\nThis page intentionally left blank \n\n\nPreface\nThe Python programming language has unique strengths and charms \nthat can be hard to grasp. Many programmers familiar with other \nlanguages often approach Python from a limited mindset instead of \nembracing its full expressivity. Some programmers go too far in the \nother direction, overusing Python features that can cause big prob-\nlems later.\nThis book provides insight into the Pythonic way of writing programs: \nthe best way to use Python. It builds on a fundamental understanding \nof the language that I assume you already have. Novice programmers \nwill learn the best practices of Python’s capabilities. Experienced pro-\ngrammers will learn how to embrace the strangeness of a new tool \nwith confidence.\nMy goal is to prepare you to make a big impact with Python.\nWhat This Book Covers\nEach chapter in this book contains a broad but related set of items. \nFeel free to jump between items and follow your interest. Each item \ncontains concise and specific guidance explaining how you can write \nPython programs more effectively. Items include advice on what to \ndo, what to avoid, how to strike the right balance, and why this is the \nbest choice. Items reference each other to make it easier to fill in the \ngaps as you read.\nThis second edition of this book is focused exclusively on Python 3 \n(see Item 1: “Know Which Version of Python You’re Using”), up to and \nincluding version 3.8. Most of the original items from the first edi-\ntion have been revised and included, but many have undergone sub-\nstantial updates. For some items, my advice has completely changed \nbetween the two editions of the book due to best practices evolving as \nPython has matured. If you’re still primarily using Python 2, despite \n\n\nxviii \nPreface\nits end-of-life on January 1st, 2020, the previous edition of the book \nmay be more useful to you.\nPython takes a “batteries included” approach to its standard library, \nin comparison to many other languages that ship with a small \n number of common packages and require you to look elsewhere \nfor important functionality. Many of these built-in packages are so \nclosely intertwined with idiomatic Python that they may as well be \npart of the language specification. The full set of standard modules \nis too large to cover in this book, but I’ve included the ones that I feel \nare critical to be aware of and use.\nChapter 1: Pythonic Thinking\nThe Python community has come to use the adjective Pythonic to \ndescribe code that follows a particular style. The idioms of Python \nhave emerged over time through experience using the language and \nworking with others. This chapter covers the best way to do the most \ncommon things in Python.\nChapter 2: Lists and Dictionaries\nIn Python, the most common way to organize information is in a \nsequence of values stored in a list. A list’s natural complement is the \ndict that stores lookup keys mapped to corresponding values. This \nchapter covers how to build programs with these versatile building \nblocks.\nChapter 3: Functions\nFunctions in Python have a variety of extra features that make a \n programmer’s life easier. Some are similar to capabilities in other pro-\ngramming languages, but many are unique to Python. This chapter \ncovers how to use functions to clarify intention, promote reuse, and \nreduce bugs.\nChapter 4: Comprehensions and Generators\nPython has special syntax for quickly iterating through lists, dictio-\nnaries, and sets to generate derivative data structures. It also allows \nfor a stream of iterable values to be incrementally returned by a \nfunction. This chapter covers how these features can provide better \n performance, reduced memory usage, and improved readability.\nChapter 5: Classes and Interfaces\nPython is an object-oriented language. Getting things done in Python \noften requires writing new classes and defining how they interact \n\n\n \nPreface \nxix\nthrough their interfaces and hierarchies. This chapter covers how to \nuse classes to express your intended behaviors with objects.\nChapter 6: Metaclasses and Attributes\nMetaclasses and dynamic attributes are powerful Python features. \nHowever, they also enable you to implement extremely bizarre and \nunexpected behaviors. This chapter covers the common idioms for \nusing these mechanisms to ensure that you follow the rule of least \nsurprise.\nChapter 7: Concurrency and Parallelism\nPython makes it easy to write concurrent programs that do many \ndifferent things seemingly at the same time. Python can also be used \nto do parallel work through system calls, subprocesses, and C exten-\nsions. This chapter covers how to best utilize Python in these subtly \ndifferent situations.\nChapter 8: Robustness and Performance\nPython has built-in features and modules that aid in hardening your \nprograms so they are dependable. Python also includes tools to help \nyou achieve higher performance with minimal effort. This chapter \ncovers how to use Python to optimize your programs to maximize \ntheir reliability and efficiency in production.\nChapter 9: Testing and Debugging\nYou should always test your code, regardless of what language it’s \nwritten in. However, Python’s dynamic features can increase the risk \nof runtime errors in unique ways. Luckily, they also make it easier to \nwrite tests and diagnose malfunctioning programs. This chapter cov-\ners Python’s built-in tools for testing and debugging.\nChapter 10: Collaboration\nCollaborating on Python programs requires you to be deliberate about \nhow you write your code. Even if you’re working alone, you’ll want to \nunderstand how to use modules written by others. This chapter cov-\ners the standard tools and best practices that enable people to work \ntogether on Python programs.\nConventions Used in This Book\nPython code snippets in this book are in monospace font and have \nsyntax highlighting. When lines are long, I use ¯ characters to show \n\n\nxx \nPreface\nwhen they wrap. I truncate some snippets with ellipses (...) to indi-\ncate regions where code exists that isn’t essential for expressing the \npoint. You’ll need to download the full example code (see below on \nwhere to get it) to get these truncated snippets to run correctly on \nyour computer.\nI take some artistic license with the Python style guide in order to \nmake the code examples better fit the format of a book, or to highlight \nthe most important parts. I’ve also left out embedded documentation \nto reduce the size of code examples. I strongly suggest that you don’t \nemulate this in your projects; instead, you should follow the style \nguide (see Item 2: “Follow the PEP 8 Style Guide”) and write documen-\ntation (see Item 84: “Write Docstrings for Every Function, Class, and \nModule”).\nMost code snippets in this book are accompanied by the correspond-\ning output from running the code. When I say “output,” I mean console \nor terminal output: what you see when running the Python program \nin an interactive interpreter. Output sections are in monospace font \nand are preceded by a >>> line (the Python interactive prompt). The \nidea is that you could type the code snippets into a Python shell and \nreproduce the expected output.\nFinally, there are some other sections in monospace font that are not \npreceded by a >>> line. These represent the output of running pro-\ngrams besides the normal Python interpreter. These examples often \nbegin with $ characters to indicate that I’m running programs from a \n command-line shell like Bash. If you’re running these commands on \nWindows or another type of system, you may need to adjust the pro-\ngram names and arguments accordingly.\nWhere to Get the Code and Errata\nIt’s useful to view some of the examples in this book as whole \n programs without interleaved prose. This also gives you a chance to \ntinker with the code yourself and understand why the program works \nas described. You can find the source code for all code snippets in \nthis book on the book’s website at https://effectivepython.com. The \n website also includes any corrections to the book, as well as how to \nreport errors.\n",
      "page_number": 10,
      "chapter_number": 2,
      "summary": "This chapter covers segment 2 (pages 10-18). Key topics include item, useful, and pythonic.",
      "keywords": [
        "Item",
        "Python",
        "chapter covers",
        "Prefer",
        "Avoid",
        "Generators",
        "Attributes",
        "Classes",
        "Python programs",
        "Prefer Multiple Assignment",
        "Functions",
        "book",
        "Comprehensions",
        "Covers",
        "Prefer Public Attributes"
      ],
      "concepts": [
        "item",
        "useful",
        "pythonic",
        "python",
        "consider",
        "classes",
        "prefer",
        "functions",
        "function",
        "functionality"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.85,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.82,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 39,
          "title": "Segment 39 (pages 353-355)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.73,
          "method": "api"
        }
      ]
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 19-26)",
      "start_page": 19,
      "end_page": 26,
      "detection_method": "topic_boundary",
      "content": "Acknowledgments\nThis book would not have been possible without the guidance, \n support, and encouragement from many people in my life.\nThanks to Scott Meyers for the Effective Software Development series. \nI first read Effective C++ when I was 15 years old and fell in love with \nprogramming. There’s no doubt that Scott’s books led to my academic \nexperience and first job. I’m thrilled to have had the opportunity to \nwrite this book.\nThanks to my technical reviewers for the depth and thoroughness \nof their feedback for the second edition of this book: Andy Chu, Nick \nCohron, Andrew Dolan, Asher Mancinelli, and Alex Martelli. Thanks \nto my colleagues at Google for their review and input. Without all of \nyour help, this book would have been inscrutable.\nThanks to everyone at Pearson involved in making this second edi-\ntion a reality. Thanks to my executive editor Debra Williams for being \nsupportive throughout the process. Thanks to the team who were \ninstrumental: development editor Chris Zahn, marketing manager \nStephane Nakib, copy editor Catherine Wilson, senior project editor \nLori Lyons, and cover designer Chuti Prasertsith.\nThanks to everyone who supported me in creating the first edition of \nthis book: Trina MacDonald, Brett Cannon, Tavis Rudd, Mike  Taylor, \nLeah Culver, Adrian Holovaty, Michael Levine, Marzia  Niccolai, Ade \nOshineye, Katrina Sostek, Tom Cirtin, Chris Zahn, Olivia  Basegio, \nStephane Nakib, Stephanie Geels, Julie Nahil, and Toshiaki \n Kurokawa. Thanks to all of the readers who reported errors and room \nfor improvement. Thanks to all of the translators who made the book \navailable in other languages around the world.\nThanks to the wonderful Python programmers I’ve known and \nworked with: Anthony Baxter, Brett Cannon, Wesley Chun, Jeremy \nHylton, Alex Martelli, Neal Norwitz, Guido van Rossum, Andy Smith, \n\n\nxxii \nAcknowledgments\nGreg Stein, and Ka-Ping Yee. I appreciate your tutelage and leader-\nship. Python has an excellent community, and I feel lucky to be a part \nof it.\nThanks to my teammates over the years for letting me be the worst \nplayer in the band. Thanks to Kevin Gibbs for helping me take risks. \nThanks to Ken Ashcraft, Ryan Barrett, and Jon McAlister for showing \nme how it’s done. Thanks to Brad Fitzpatrick for taking it to the next \nlevel. Thanks to Paul McDonald for being an amazing co-founder. \nThanks to Jeremy Ginsberg, Jack Hebert, John Skidgel, Evan  Martin, \nTony Chang, Troy Trimble, Tessa Pupius, and Dylan Lorimer for help-\ning me learn. Thanks to Sagnik Nandy and Waleed Ojeil for your \nmentorship.\nThanks to the inspiring programming and engineering teachers \nthat I’ve had: Ben Chelf, Glenn Cowan, Vince Hugo, Russ Lewin, Jon \nStemmle, Derek Thomson, and Daniel Wang. Without your instruc-\ntion, I would never have pursued our craft or gained the perspective \nrequired to teach others.\nThanks to my mother for giving me a sense of purpose and \n encouraging me to become a programmer. Thanks to my brother, my \ngrandparents, and the rest of my family and childhood friends for \nbeing role models as I grew up and found my passion.\nFinally, thanks to my wife, Colleen, for her love, support, and  laughter \nthrough the journey of life.\n\n\nAbout the Author\nBrett Slatkin is a principal software engineer at Google. He is \nthe technical co-founder of Google Surveys, the co-creator of the \n PubSubHubbub protocol, and he launched Google’s first cloud com-\nputing product (App Engine). Fourteen years ago, he cut his teeth \nusing Python to manage Google’s enormous fleet of servers.\nOutside of his day job, he likes to play piano and surf (both poorly). He \nalso enjoys writing about programming-related topics on his  personal \nwebsite (https://onebigfluke.com). He earned his B.S. in computer \nengineering from Columbia University in the City of New York. He \nlives in San Francisco.\n\n\nThis page intentionally left blank \n\n\n1\nPythonic Thinking\nThe idioms of a programming language are defined by its users. \nOver the years, the Python community has come to use the adjective \nPythonic to describe code that follows a particular style. The Pythonic \nstyle isn’t regimented or enforced by the compiler. It has emerged \nover time through experience using the language and working with \n others. Python programmers prefer to be explicit, to choose simple \nover  complex, and to maximize readability. (Type import this into \nyour interpreter to read The Zen of Python.)\nProgrammers familiar with other languages may try to write Python \nas if it’s C++, Java, or whatever they know best. New programmers \nmay still be getting comfortable with the vast range of concepts \nthat can be expressed in Python. It’s important for you to know the \nbest—the Pythonic—way to do the most common things in Python. \nThese patterns will affect every program you write.\nItem 1: Know Which Version of Python You’re Using\nThroughout this book, the majority of example code is in the syntax \nof Python 3.7 (released in June 2018). This book also provides some \nexamples in the syntax of Python 3.8 (released in October 2019) to \nhighlight new features that will be more widely available soon. This \nbook does not cover Python 2.\nMany computers come with multiple versions of the standard CPython \nruntime preinstalled. However, the default meaning of python on the \ncommand line may not be clear. python is usually an alias for python2.7, \nbut it can sometimes be an alias for even older versions, like python2.6 \nor python2.5. To find out exactly which version of Python you’re using, \nyou can use the --version flag:\n$ python --version\nPython 2.7.10\n\n\n2 \nChapter 1 Pythonic Thinking\nPython 3 is usually available under the name python3:\n$ python3 --version\nPython 3.8.0\nYou can also figure out the version of Python you’re using at runtime \nby inspecting values in the sys built-in module:\nimport sys\nprint(sys.version_info)\nprint(sys.version)\n>>>\nsys.version_info(major=3, minor=8, micro=0, \n¯releaselevel='final', serial=0)\n3.8.0 (default, Oct 21 2019, 12:51:32) \n[Clang 6.0 (clang-600.0.57)]\nPython 3 is actively maintained by the Python core developers and \ncommunity, and it is constantly being improved. Python 3 includes \na variety of powerful new features that are covered in this book. The \nmajority of Python’s most common open source libraries are compat-\nible with and focused on Python 3. I strongly encourage you to use \nPython 3 for all your Python projects.\nPython 2 is scheduled for end of life after January 1, 2020, at which \npoint all forms of bug fixes, security patches, and backports of fea-\ntures will cease. Using Python 2 after that date is a liability because it \nwill no longer be officially maintained. If you’re still stuck working in \na Python 2 codebase, you should consider using helpful tools like 2to3 \n(preinstalled with Python) and six (available as a community pack-\nage; see Item 82: “Know Where to Find Community-Built  Modules”) to \nhelp you make the transition to Python 3.\nThings to Remember\n✦ Python 3 is the most up-to-date and well-supported version of \nPython, and you should use it for your projects.\n✦ Be sure that the command-line executable for running Python on \nyour system is the version you expect it to be.\n✦ Avoid Python 2 because it will no longer be maintained after January 1, \n2020.\nItem 2: Follow the PEP 8 Style Guide\nPython Enhancement Proposal #8, otherwise known as PEP 8, is \nthe style guide for how to format Python code. You are welcome to \n\n\n \nItem 2: Follow the PEP 8 Style Guide \n3\nwrite Python code any way you want, as long as it has valid syntax. \n However, using a consistent style makes your code more approach-\nable and easier to read. Sharing a common style with other Python \n programmers in the larger community facilitates collaboration on \nprojects. But even if you are the only one who will ever read your \ncode, following the style guide will make it easier for you to change \nthings later, and can help you avoid many common errors.\nPEP 8 provides a wealth of details about how to write clear Python \ncode. It continues to be updated as the Python language evolves. \nIt’s worth reading the whole guide online (https://www.python.org/\ndev/peps/pep-0008/). Here are a few rules you should be sure to \nfollow.\nWhitespace\nIn Python, whitespace is syntactically significant. Python program-\nmers are especially sensitive to the effects of whitespace on code \n clarity. Follow these guidelines related to whitespace:\n \n■Use spaces instead of tabs for indentation.\n \n■Use four spaces for each level of syntactically significant indenting.\n \n■Lines should be 79 characters in length or less.\n \n■Continuations of long expressions onto additional lines should \nbe indented by four extra spaces from their normal indentation \nlevel.\n \n■In a file, functions and classes should be separated by two blank \nlines.\n \n■In a class, methods should be separated by one blank line.\n \n■In a dictionary, put no whitespace between each key and colon, \nand put a single space before the corresponding value if it fits on \nthe same line.\n \n■Put one—and only one—space before and after the = operator in a \nvariable assignment.\n \n■For type annotations, ensure that there is no separation between \nthe variable name and the colon, and use a space before the type \ninformation.\nNaming\nPEP 8 suggests unique styles of naming for different parts in the \nlanguage. These conventions make it easy to distinguish which type \n\n\n4 \nChapter 1 Pythonic Thinking\ncorresponds to each name when reading code. Follow these guidelines \nrelated to naming:\n \n■Functions, variables, and attributes should be in lowercase_\nunderscore format.\n \n■Protected instance attributes should be in _leading_underscore \nformat.\n \n■Private instance attributes should be in __double_leading_\nunderscore format.\n \n■Classes (including exceptions) should be in CapitalizedWord \nformat.\n \n■Module-level constants should be in ALL_CAPS format.\n \n■Instance methods in classes should use self, which refers to the \nobject, as the name of the first parameter.\n \n■Class methods should use cls, which refers to the class, as the \nname of the first parameter.\nExpressions and Statements\nThe Zen of Python states: “There should be one—and preferably only \none—obvious way to do it.” PEP 8 attempts to codify this style in its \nguidance for expressions and statements:\n \n■Use inline negation (if a is not b) instead of negation of positive \nexpressions (if not a is b).\n \n■Don’t check for empty containers or sequences (like [] or '') \nby comparing the length to zero (if len(somelist) == 0). Use \nif not somelist and assume that empty values will implicitly \nevaluate to False.\n \n■The same thing goes for non-empty containers or sequences (like \n[1] or 'hi'). The statement if somelist is implicitly True for non-\nempty values.\n \n■Avoid single-line if statements, for and while loops, and except \ncompound statements. Spread these over multiple lines for \nclarity.\n \n■If you can’t fit an expression on one line, surround it with paren-\ntheses and add line breaks and indentation to make it easier to \nread.\n \n■Prefer surrounding multiline expressions with parentheses over \nusing the \\ line continuation character.\n",
      "page_number": 19,
      "chapter_number": 3,
      "summary": "This chapter covers segment 3 (pages 19-26). Key topics include pythonic, thanks, and styles. I truncate some snippets with ellipses (...) to indi-\ncate regions where code exists that isn’t essential for expressing the \npoint.",
      "keywords": [
        "Python",
        "Python code",
        "code",
        "book",
        "write Python code",
        "style guide",
        "Python style guide",
        "Python programmers",
        "style",
        "write Python",
        "Python program",
        "format Python code",
        "clear Python code",
        "code snippets",
        "guide"
      ],
      "concepts": [
        "pythonic",
        "thanks",
        "styles",
        "editor",
        "output",
        "program",
        "programming",
        "code",
        "write",
        "writing"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 3,
          "title": "Segment 3 (pages 19-26)",
          "relevance_score": 0.68,
          "method": "api"
        }
      ]
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 27-34)",
      "start_page": 27,
      "end_page": 34,
      "detection_method": "topic_boundary",
      "content": " \nItem 3: Know the Differences Between bytes and str \n5\nImports\nPEP 8 suggests some guidelines for how to import modules and use \nthem in your code:\n \n■Always put import statements (including from x import y) at the \ntop of a file.\n \n■Always use absolute names for modules when importing them, not \nnames relative to the current module’s own path. For example, to \nimport the foo module from within the bar package, you should \nuse from bar import foo, not just import foo.\n \n■If you must do relative imports, use the explicit syntax \nfrom . import foo.\n \n■Imports should be in sections in the following order: standard \nlibrary modules, third-party modules, your own modules. Each \nsubsection should have imports in alphabetical order.\nNote\nThe Pylint tool (https://www.pylint.org) is a popular static analyzer for Python \nsource code. Pylint provides automated enforcement of the PEP 8 style guide and \ndetects many other types of common errors in Python programs. Many IDEs and \neditors also include linting tools or support similar plug-ins.\nThings to Remember\n✦ Always follow the Python Enhancement Proposal #8 (PEP 8) style \nguide when writing Python code.\n✦ Sharing a common style with the larger Python community facili-\ntates collaboration with others.\n✦ Using a consistent style makes it easier to modify your own code later.\nItem 3: Know the Differences Between bytes and str\nIn Python, there are two types that represent sequences of character \ndata: bytes and str. Instances of bytes contain raw, unsigned 8-bit \nvalues (often displayed in the ASCII encoding):\na = b'h\\x65llo'\nprint(list(a))\nprint(a)\n>>>\n[104, 101, 108, 108, 111]\nb'hello'\n\n\n6 \nChapter 1 Pythonic Thinking\nInstances of str contain Unicode code points that represent textual \ncharacters from human languages:\na = 'a\\u0300 propos'\nprint(list(a))\nprint(a)\n>>>\n['a', '`', ' ', 'p', 'r', 'o', 'p', 'o', 's']\nà propos\nImportantly, str instances do not have an associated binary encod-\ning, and bytes instances do not have an associated text encoding. To \nconvert Unicode data to binary data, you must call the encode method \nof str. To convert binary data to Unicode data, you must call the \ndecode method of bytes. You can explicitly specify the encoding you \nwant to use for these methods, or accept the system default, which is \ncommonly UTF-8 (but not always—see more on that below).\nWhen you’re writing Python programs, it’s important to do encoding \nand decoding of Unicode data at the furthest boundary of your inter-\nfaces; this approach is often called the Unicode sandwich. The core \nof your program should use the str type containing Unicode data \nand should not assume anything about character encodings. This \napproach allows you to be very accepting of alternative text encodings \n(such as Latin-1, Shift JIS, and Big5) while being strict about your \n output text encoding (ideally, UTF-8).\nThe split between character types leads to two common situations in \nPython code:\n \n■You want to operate on raw 8-bit sequences that contain \nUTF-8-encoded strings (or some other encoding).\n \n■You want to operate on Unicode strings that have no specific \nencoding.\nYou’ll often need two helper functions to convert between these cases \nand to ensure that the type of input values matches your code’s \nexpectations.\nThe first function takes a bytes or str instance and always returns \na str:\ndef to_str(bytes_or_str):\n    if isinstance(bytes_or_str, bytes):\n        value = bytes_or_str.decode('utf-8')\n    else:\n        value = bytes_or_str\n    return value  # Instance of str\n\n\n \nItem 3: Know the Differences Between bytes and str \n7\nprint(repr(to_str(b'foo')))\nprint(repr(to_str('bar')))\n>>>\n'foo'\n'bar'\nThe second function takes a bytes or str instance and always returns \na bytes:\ndef to_bytes(bytes_or_str):\n    if isinstance(bytes_or_str, str):\n        value = bytes_or_str.encode('utf-8')\n    else:\n        value = bytes_or_str\n    return value  # Instance of bytes\nprint(repr(to_bytes(b'foo')))\nprint(repr(to_bytes('bar')))\nThere are two big gotchas when dealing with raw 8-bit values and \nUnicode strings in Python.\nThe first issue is that bytes and str seem to work the same way, but \ntheir instances are not compatible with each other, so you must be \ndeliberate about the types of character sequences that you’re passing \naround.\nBy using the + operator, you can add bytes to bytes and str to str, \nrespectively:\nprint(b'one' + b'two')\nprint('one' + 'two')\n>>>\nb'onetwo'\nonetwo\nBut you can’t add str instances to bytes instances:\nb'one' + 'two'\n>>>\nTraceback ...\nTypeError: can't concat str to bytes\nNor can you add bytes instances to str instances:\n'one' + b'two'\n>>>\nTraceback ...\nTypeError: can only concatenate str (not \"bytes\") to str\n\n\n8 \nChapter 1 Pythonic Thinking\nBy using binary operators, you can compare bytes to bytes and str to \nstr, respectively:\nassert b'red' > b'blue'\nassert 'red' > 'blue'\nBut you can’t compare a str instance to a bytes instance:\nassert 'red' > b'blue'\n>>>\nTraceback ...\nTypeError: '>' not supported between instances of 'str' and \n¯'bytes'\nNor can you compare a bytes instance to a str instance:\nassert b'blue' < 'red'\n>>>\nTraceback ...\nTypeError: '<' not supported between instances of 'bytes' \n¯and 'str'\nComparing bytes and str instances for equality will always evaluate \nto False, even when they contain exactly the same characters (in this \ncase, ASCII-encoded “foo”):\nprint(b'foo' == 'foo')\n>>>\nFalse\nThe % operator works with format strings for each type, respectively:\nprint(b'red %s' % b'blue')\nprint('red %s' % 'blue')\n>>>\nb'red blue'\nred blue\nBut you can’t pass a str instance to a bytes format string because \nPython doesn’t know what binary text encoding to use:\nprint(b'red %s' % 'blue')\n>>>\nTraceback ...\nTypeError: %b requires a bytes-like object, or an object that \n¯implements __bytes__, not 'str'\n\n\n \nItem 3: Know the Differences Between bytes and str \n9\nYou can pass a bytes instance to a str format string using the \n%  operator, but it doesn’t do what you’d expect:\nprint('red %s' % b'blue')\n>>>\nred b'blue'\nThis code actually invokes the __repr__ method (see Item 75: “Use \nrepr Strings for Debugging Output”) on the bytes instance and sub-\nstitutes that in place of the %s, which is why b'blue' remains escaped \nin the output.\nThe second issue is that operations involving file handles (returned by \nthe open built-in function) default to requiring Unicode strings instead \nof raw bytes. This can cause surprising failures, especially for pro-\ngrammers accustomed to Python 2. For example, say that I want to \nwrite some binary data to a file. This seemingly simple code breaks:\nwith open('data.bin', 'w') as f:\n    f.write(b'\\xf1\\xf2\\xf3\\xf4\\xf5')\n>>>\nTraceback ...\nTypeError: write() argument must be str, not bytes\nThe cause of the exception is that the file was opened in write text \nmode ('w') instead of write binary mode ('wb'). When a file is in text \nmode, write operations expect str instances containing Unicode data \ninstead of bytes instances containing binary data. Here, I fix this by \nchanging the open mode to 'wb':\nwith open('data.bin', 'wb') as f:\n    f.write(b'\\xf1\\xf2\\xf3\\xf4\\xf5')\nA similar problem also exists for reading data from files. For example, \nhere I try to read the binary file that was written above:\nwith open('data.bin', 'r') as f:\n    data = f.read()\n>>>\nTraceback ...\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in \n¯position 0: invalid continuation byte\nThis fails because the file was opened in read text mode ('r') \ninstead of read binary mode ('rb'). When a handle is in text mode, \nit uses the system’s default text encoding to interpret binary data \n\n\n10 \nChapter 1 Pythonic Thinking\nusing the bytes.encode (for writing) and str.decode (for reading) \n methods. On most systems, the default encoding is UTF-8, which \ncan’t accept the binary data b'\\xf1\\xf2\\xf3\\xf4\\xf5', thus causing \nthe error above. Here, I solve this problem by changing the open \nmode to 'rb':\nwith open('data.bin', 'rb') as f:\n    data = f.read()\nassert data == b'\\xf1\\xf2\\xf3\\xf4\\xf5'\nAlternatively, I can explicitly specify the encoding parameter to \nthe open function to make sure that I’m not surprised by any \n platform-specific behavior. For example, here I assume that the \nbinary data in the file was actually meant to be a string encoded as \n'cp1252' (a legacy  Windows encoding):\nwith open('data.bin', 'r', encoding='cp1252') as f:\n    data = f.read()\nassert data == 'ñòóôõ'\nThe exception is gone, and the string interpretation of the file’s con-\ntents is very different from what was returned when reading raw \nbytes. The lesson here is that you should check the default encod-\ning on your system (using python3 -c 'import locale; print(locale.\ngetpreferredencoding())') to understand how it differs from your \nexpectations. When in doubt, you should explicitly pass the encoding \nparameter to open.\nThings to Remember\n✦ bytes contains sequences of 8-bit values, and str contains \nsequences of Unicode code points.\n✦ Use helper functions to ensure that the inputs you operate on \nare the type of character sequence that you expect (8-bit values, \nUTF-8-encoded strings, Unicode code points, etc).\n✦ bytes and str instances can’t be used together with operators (like \n>, ==, +, and %).\n✦ If you want to read or write binary data to/from a file, always open \nthe file using a binary mode (like 'rb' or 'wb').\n✦ If you want to read or write Unicode data to/from a file, be care-\nful about your system’s default text encoding. Explicitly pass the \nencoding parameter to open if you want to avoid surprises.\n\n\n \nItem 4: Prefer Interpolated F-Strings \n11\nItem 4:  Prefer Interpolated F-Strings Over C-style \nFormat Strings and str.format\nStrings are present throughout Python codebases. They’re used for \nrendering messages in user interfaces and command-line utilities. \nThey’re used for writing data to files and sockets. They’re used for \nspecifying what’s gone wrong in Exception details (see Item 27: “Use \nComprehensions Instead of map and filter”). They’re used in debug-\nging (see Item 80: “Consider Interactive Debugging with pdb” and Item \n75: “Use repr Strings for Debugging Output”).\nFormatting is the process of combining predefined text with data val-\nues into a single human-readable message that’s stored as a string. \nPython has four different ways of formatting strings that are built \ninto the language and standard library. All but one of them, which is \ncovered last in this item, have serious shortcomings that you should \nunderstand and avoid.\nThe most common way to format a string in Python is by using the \n% formatting operator. The predefined text template is provided on the \nleft side of the operator in a format string. The values to insert into \nthe template are provided as a single value or tuple of multiple values \non the right side of the format operator. For example, here I use the \n% operator to convert difficult-to-read binary and hexadecimal values \nto integer strings:\na = 0b10111011\nb = 0xc5f\nprint('Binary is %d, hex is %d' % (a, b))\n>>>\nBinary is 187, hex is 3167\nThe format string uses format specifiers (like %d) as placeholders that \nwill be replaced by values from the right side of the formatting expres-\nsion. The syntax for format specifiers comes from C’s printf function, \nwhich has been inherited by Python (as well as by other programming \nlanguages). Python supports all the usual options you’d expect from \nprintf, such as %s, %x, and %f format specifiers, as well as control over \ndecimal places, padding, fill, and alignment. Many programmers who \nare new to Python start with C-style format strings because they’re \nfamiliar and simple to use.\nThere are four problems with C-style format strings in Python.\nThe first problem is that if you change the type or order of data val-\nues in the tuple on the right side of a formatting expression, you can \n\n\n12 \nChapter 1 Pythonic Thinking\nget errors due to type conversion incompatibility. For example, this \n simple formatting expression works:\nkey = 'my_var'\nvalue = 1.234\nformatted = '%-10s = %.2f' % (key, value)\nprint(formatted)\n>>>\nmy_var     = 1.23\nBut if you swap key and value, you get an exception at runtime:\nreordered_tuple = '%-10s = %.2f' % (value, key)\n>>>\nTraceback ...\nTypeError: must be real number, not str\nSimilarly, leaving the right side parameters in the original order but \nchanging the format string results in the same error:\nreordered_string = '%.2f = %-10s' % (key, value)\n>>>\nTraceback ...\nTypeError: must be real number, not str\nTo avoid this gotcha, you need to constantly check that the two sides \nof the % operator are in sync; this process is error prone because it \nmust be done manually for every change.\nThe second problem with C-style formatting expressions is that they \nbecome difficult to read when you need to make small modifications to \nvalues before formatting them into a string—and this is an extremely \ncommon need. Here, I list the contents of my kitchen pantry without \nmaking inline changes:\npantry = [\n    ('avocados', 1.25),\n    ('bananas', 2.5),\n    ('cherries', 15),\n]\nfor i, (item, count) in enumerate(pantry):\n    print('#%d: %-10s = %.2f' % (i, item, count))\n>>>\n#0: avocados   = 1.25\n#1: bananas    = 2.50\n#2: cherries   = 15.00\n",
      "page_number": 27,
      "chapter_number": 4,
      "summary": "This chapter covers segment 4 (pages 27-34). Key topics include format, formatting, and encoding. Follow these guidelines \nrelated to naming:\n \n■Functions, variables, and attributes should be in lowercase_\nunderscore format.",
      "keywords": [
        "bytes",
        "str",
        "Python",
        "str instances",
        "data",
        "Unicode data",
        "Unicode",
        "Instances",
        "bytes instances",
        "format",
        "binary",
        "binary data",
        "encoding",
        "Item",
        "Unicode strings"
      ],
      "concepts": [
        "format",
        "formatting",
        "encoding",
        "encode",
        "pythonic",
        "python",
        "bytes",
        "uses",
        "code",
        "imports"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 8,
          "title": "Segment 8 (pages 147-169)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 3,
          "title": "Segment 3 (pages 19-26)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 9,
          "title": "Segment 9 (pages 170-191)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.68,
          "method": "api"
        }
      ]
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 35-42)",
      "start_page": 35,
      "end_page": 42,
      "detection_method": "topic_boundary",
      "content": " \nItem 4: Prefer Interpolated F-Strings \n13\nNow, I make a few modifications to the values that I’m formatting \nto make the printed message more useful. This causes the tuple in \nthe formatting expression to become so long that it needs to be split \nacross multiple lines, which hurts readability:\nfor i, (item, count) in enumerate(pantry):\n    print('#%d: %-10s = %d' % (\n        i + 1,\n        item.title(),\n        round(count)))\n>>>\n#1: Avocados   = 1\n#2: Bananas    = 2\n#3: Cherries   = 15\nThe third problem with formatting expressions is that if you want \nto use the same value in a format string multiple times, you have to \nrepeat it in the right side tuple:\ntemplate = '%s loves food. See %s cook.'\nname = 'Max'\nformatted = template % (name, name)\nprint(formatted)\n>>>\nMax loves food. See Max cook.\nThis is especially annoying and error prone if you have to repeat \nsmall modifications to the values being formatted. For example, here \nI remembered to call the title() method multiple times, but I could \nhave easily added the method call to one reference to name and not the \nother, which would cause mismatched output:\nname = 'brad'\nformatted = template % (name.title(), name.title())\nprint(formatted)\n>>>\nBrad loves food. See Brad cook.\nTo help solve some of these problems, the % operator in Python has \nthe ability to also do formatting with a dictionary instead of a tuple. The \nkeys from the dictionary are matched with format specifiers with the \ncorresponding name, such as %(key)s. Here, I use this functionality to \nchange the order of values on the right side of the formatting expres-\nsion with no effect on the output, thus solving problem #1 from above:\nkey = 'my_var'\nvalue = 1.234\n\n\n14 \nChapter 1 Pythonic Thinking\nold_way = '%-10s = %.2f' % (key, value)\nnew_way = '%(key)-10s = %(value).2f' % {\n    'key': key, 'value': value}  # Original\nreordered = '%(key)-10s = %(value).2f' % {\n    'value': value, 'key': key}  # Swapped\nassert old_way == new_way == reordered\nUsing dictionaries in formatting expressions also solves problem #3 \nfrom above by allowing multiple format specifiers to reference the same \nvalue, thus making it unnecessary to supply that value more than once:\nname = 'Max'\ntemplate = '%s loves food. See %s cook.'\nbefore = template % (name, name)   # Tuple\ntemplate = '%(name)s loves food. See %(name)s cook.'\nafter = template % {'name': name}  # Dictionary\nassert before == after\nHowever, dictionary format strings introduce and exacerbate other \nissues. For problem #2 above, regarding small modifications to values \nbefore formatting them, formatting expressions become longer and \nmore visually noisy because of the presence of the dictionary key and \ncolon operator on the right side. Here, I render the same string with \nand without dictionaries to show this problem:\nfor i, (item, count) in enumerate(pantry):\n    before = '#%d: %-10s = %d' % (\n        i + 1,\n        item.title(),\n        round(count))\n    after = '#%(loop)d: %(item)-10s = %(count)d' % {\n        'loop': i + 1,\n        'item': item.title(),\n        'count': round(count),\n    }\n    assert before == after\n\n\n \nItem 4: Prefer Interpolated F-Strings \n15\nUsing dictionaries in formatting expressions also increases verbosity, \nwhich is problem #4 with C-style formatting expressions in Python. \nEach key must be specified at least twice—once in the format speci-\nfier, once in the dictionary as a key, and potentially once more for the \nvariable name that contains the dictionary value:\nsoup = 'lentil'\nformatted = 'Today\\'s soup is %(soup)s.' % {'soup': soup}\nprint(formatted)\n>>>\nToday's soup is lentil.\nBesides the duplicative characters, this redundancy causes format-\nting expressions that use dictionaries to be long. These expressions \noften must span multiple lines, with the format strings being concat-\nenated across multiple lines and the dictionary assignments having \none line per value to use in formatting:\nmenu = {\n    'soup': 'lentil',\n    'oyster': 'kumamoto',\n    'special': 'schnitzel',\n}\ntemplate = ('Today\\'s soup is %(soup)s, '\n            'buy one get two %(oyster)s oysters, '\n            'and our special entrée is %(special)s.')\nformatted = template % menu\nprint(formatted)\n>>>\nToday's soup is lentil, buy one get two kumamoto oysters, and \n¯our special entrée is schnitzel.\nTo understand what this formatting expression is going to produce, \nyour eyes have to keep going back and forth between the lines of the \nformat string and the lines of the dictionary. This disconnect makes \nit hard to spot bugs, and readability gets even worse if you need to \nmake small modifications to any of the values before formatting.\nThere must be a better way.\nThe format Built-in and str.format\nPython 3 added support for advanced string formatting that is more \nexpressive than the old C-style format strings that use the % operator. \nFor individual Python values, this new functionality can be accessed \nthrough the format built-in function. For example, here I use some of \n\n\n16 \nChapter 1 Pythonic Thinking\nthe new options (, for thousands separators and ^ for centering) to \nformat values:\na = 1234.5678\nformatted = format(a, ',.2f')\nprint(formatted)\nb = 'my string'\nformatted = format(b, '^20s')\nprint('*', formatted, '*')\n>>>\n1,234.57\n*      my string       *\nYou can use this functionality to format multiple values together \nby calling the new format method of the str type. Instead of using \nC-style format specifiers like %d, you can specify placeholders with {}. \nBy default the placeholders in the format string are replaced by the \ncorresponding positional arguments passed to the format method in \nthe order in which they appear:\nkey = 'my_var'\nvalue = 1.234\nformatted = '{} = {}'.format(key, value)\nprint(formatted)\n>>>\nmy_var = 1.234\nWithin each placeholder you can optionally provide a colon char-\nacter followed by format specifiers to customize how values will be \nconverted into strings (see help('FORMATTING') for the full range of \noptions):\nformatted = '{:<10} = {:.2f}'.format(key, value)\nprint(formatted)\n>>>\nmy_var     = 1.23\nThe way to think about how this works is that the format specifiers \nwill be passed to the format built-in function along with the value \n(format(value, '.2f') in the example above). The result of that func-\ntion call is what replaces the placeholder in the overall formatted \nstring. The formatting behavior can be customized per class using \nthe __format__ special method.\n\n\n \nItem 4: Prefer Interpolated F-Strings \n17\nWith C-style format strings, you need to escape the % character (by \ndoubling it) so it’s not interpreted as a placeholder accidentally. With \nthe str.format method you need to similarly escape braces:\nprint('%.2f%%' % 12.5)\nprint('{} replaces {{}}'.format(1.23))\n>>>\n12.50%\n1.23 replaces {}\nWithin the braces you may also specify the positional index of an \nargument passed to the format method to use for replacing the place-\nholder. This allows the format string to be updated to reorder the \noutput without requiring you to also change the right side of the for-\nmatting expression, thus addressing problem #1 from above:\nformatted = '{1} = {0}'.format(key, value)\nprint(formatted)\n>>>\n1.234 = my_var\nThe same positional index may also be referenced multiple times in \nthe format string without the need to pass the value to the format \nmethod more than once, which solves problem #3 from above:\nformatted = '{0} loves food. See {0} cook.'.format(name)\nprint(formatted)\n>>>\nMax loves food. See Max cook.\nUnfortunately, the new format method does nothing to address prob-\nlem #2 from above, leaving your code difficult to read when you need \nto make small modifications to values before formatting them. There’s \nlittle difference in readability between the old and new options, which \nare similarly noisy:\nfor i, (item, count) in enumerate(pantry):\n    old_style = '#%d: %-10s = %d' % (\n        i + 1,\n        item.title(),\n        round(count))\n    new_style = '#{}: {:<10s} = {}'.format(\n        i + 1,\n        item.title(),\n        round(count))\n    assert old_style == new_style\n\n\n18 \nChapter 1 Pythonic Thinking\nThere are even more advanced options for the specifiers used with \nthe str.format method, such as using combinations of dictionary keys \nand list indexes in placeholders, and coercing values to Unicode and \nrepr strings:\nformatted = 'First letter is {menu[oyster][0]!r}'.format(\n    menu=menu)\nprint(formatted)\n>>>\nFirst letter is 'k'\nBut these features don’t help reduce the redundancy of repeated keys \nfrom problem #4 above. For example, here I compare the verbosity of \nusing dictionaries in C-style formatting expressions to the new style \nof passing keyword arguments to the format method:\nold_template = (\n    'Today\\'s soup is %(soup)s, '\n    'buy one get two %(oyster)s oysters, '\n    'and our special entrée is %(special)s.')\nold_formatted = template % {\n    'soup': 'lentil',\n    'oyster': 'kumamoto',\n    'special': 'schnitzel',\n}\nnew_template = (\n    'Today\\'s soup is {soup}, '\n    'buy one get two {oyster} oysters, '\n    'and our special entrée is {special}.')\nnew_formatted = new_template.format(\n    soup='lentil',\n    oyster='kumamoto',\n    special='schnitzel',\n)\nassert old_formatted == new_formatted\nThis style is slightly less noisy because it eliminates some quotes in \nthe dictionary and a few characters in the format specifiers, but it’s \nhardly compelling. Further, the advanced features of using dictionary \nkeys and indexes within placeholders only provides a tiny subset of \nPython’s expression functionality. This lack of expressiveness is so \nlimiting that it undermines the value of the format method from str \noverall.\n\n\n \nItem 4: Prefer Interpolated F-Strings \n19\nGiven these shortcomings and the problems from C-style formatting \nexpressions that remain (problems #2 and #4 from above), I suggest \nthat you avoid the str.format method in general. It’s important to \nknow about the new mini language used in format specifiers (every-\nthing after the colon) and how to use the format built-in function. But \nthe rest of the str.format method should be treated as a historical \nartifact to help you understand how Python’s new f-strings work and \nwhy they’re so great.\nInterpolated Format Strings\nPython 3.6 added interpolated format strings—f-strings for short—to \nsolve these issues once and for all. This new language syntax requires \nyou to prefix format strings with an f character, which is similar to \nhow byte strings are prefixed with a b character and raw (unescaped) \nstrings are prefixed with an r character.\nF-strings take the expressiveness of format strings to the extreme, \nsolving problem #4 from above by completely eliminating the redun-\ndancy of providing keys and values to be formatted. They achieve \nthis pithiness by allowing you to reference all names in the current \nPython scope as part of a formatting expression:\nkey = 'my_var'\nvalue = 1.234\nformatted = f'{key} = {value}'\nprint(formatted)\n>>>\nmy_var = 1.234\nAll of the same options from the new format built-in mini language \nare available after the colon in the placeholders within an f-string, as \nis the ability to coerce values to Unicode and repr strings similar to \nthe str.format method:\nformatted = f'{key!r:<10} = {value:.2f}'\nprint(formatted)\n>>>\n'my_var'   = 1.23\nFormatting with f-strings is shorter than using C-style format strings \nwith the % operator and the str.format method in all cases. Here, \nI show all these options together in order of shortest to longest, and \n\n\n20 \nChapter 1 Pythonic Thinking\nline up the left side of the assignment so you can easily compare \nthem:\nf_string = f'{key:<10} = {value:.2f}'\nc_tuple  = '%-10s = %.2f' % (key, value)\nstr_args = '{:<10} = {:.2f}'.format(key, value)\nstr_kw   = '{key:<10} = {value:.2f}'.format(key=key,\n  \n \n \n \n \n \n    value=value)\nc_dict   = '%(key)-10s = %(value).2f' % {'key': key,\n \n \n \n \n \n \n 'value': value}\nassert c_tuple == c_dict == f_string\nassert str_args == str_kw == f_string\nF-strings also enable you to put a full Python expression within the \nplaceholder braces, solving problem #2 from above by allowing small \nmodifications to the values being formatted with concise syntax. \nWhat took multiple lines with C-style formatting and the str.format \nmethod now easily fits on a single line:\nfor i, (item, count) in enumerate(pantry):\n    old_style = '#%d: %-10s = %d' % (\n        i + 1,\n        item.title(),\n        round(count))\n    new_style = '#{}: {:<10s} = {}'.format(\n        i + 1,\n        item.title(),\n        round(count))\n    f_string = f'#{i+1}: {item.title():<10s} = {round(count)}'\n    assert old_style == new_style == f_string\nOr, if it’s clearer, you can split an f-string over multiple lines by rely-\ning on adjacent-string concatenation (similar to C). Even though this \nis longer than the single-line version, it’s still much clearer than any \nof the other multiline approaches:\nfor i, (item, count) in enumerate(pantry):\n    print(f'#{i+1}: '\n",
      "page_number": 35,
      "chapter_number": 5,
      "summary": "This chapter covers segment 5 (pages 35-42). Key topics include formatted, strings. To help solve some of these problems, the % operator in Python has \nthe ability to also do formatting with a dictionary instead of a tuple.",
      "keywords": [
        "format",
        "format strings",
        "C-style format strings",
        "formatted",
        "C-style formatting expressions",
        "key",
        "format method",
        "C-style format",
        "formatting expressions",
        "Interpolated Format Strings",
        "format specifiers",
        "strings",
        "soup",
        "string",
        "C-style formatting"
      ],
      "concepts": [
        "formatted",
        "strings",
        "key",
        "keys",
        "value",
        "multiple",
        "pythonic",
        "python",
        "printed",
        "problem"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 3,
          "title": "Segment 3 (pages 19-26)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 7,
          "title": "String Fundamentals",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 43-50)",
      "start_page": 43,
      "end_page": 50,
      "detection_method": "topic_boundary",
      "content": " \nItem 5: Write Helper Functions Instead of Complex Expressions \n21\n          f'{item.title():<10s} = '\n          f'{round(count)}')\n>>>\n#1: Avocados   = 1\n#2: Bananas    = 2\n#3: Cherries   = 15\nPython expressions may also appear within the format specifier \noptions. For example, here I parameterize the number of digits to print \nby using a variable instead of hard-coding it in the format string:\nplaces = 3\nnumber = 1.23456\nprint(f'My number is {number:.{places}f}')\n>>>\nMy number is 1.235\nThe combination of expressiveness, terseness, and clarity provided \nby f-strings makes them the best built-in option for Python pro-\ngrammers. Any time you find yourself needing to format values into \nstrings, choose f-strings over the alternatives.\nThings to Remember\n✦ C-style format strings that use the % operator suffer from a variety \nof gotchas and verbosity problems.\n✦ The str.format method introduces some useful concepts in its for-\nmatting specifiers mini language, but it otherwise repeats the mis-\ntakes of C-style format strings and should be avoided.\n✦ F-strings are a new syntax for formatting values into strings that \nsolves the biggest problems with C-style format strings.\n✦ F-strings are succinct yet powerful because they allow for arbi-\ntrary Python expressions to be directly embedded within format \nspecifiers.\nItem 5:  Write Helper Functions Instead of Complex \nExpressions\nPython’s pithy syntax makes it easy to write single-line expressions \nthat implement a lot of logic. For example, say that I want to decode \nthe query string from a URL. Here, each query string parameter rep-\nresents an integer value:\nfrom urllib.parse import parse_qs\n\n\n22 \nChapter 1 Pythonic Thinking\nmy_values = parse_qs('red=5&blue=0&green=',\n                     keep_blank_values=True)\nprint(repr(my_values))\n>>>\n{'red': ['5'], 'blue': ['0'], 'green': ['']}\nSome query string parameters may have multiple values, some may \nhave single values, some may be present but have blank values, and \nsome may be missing entirely. Using the get method on the result dic-\ntionary will return different values in each circumstance:\nprint('Red:     ', my_values.get('red'))\nprint('Green:   ', my_values.get('green'))\nprint('Opacity: ', my_values.get('opacity'))\n>>>\nRed:      ['5']\nGreen:    ['']\nOpacity:  None\nIt’d be nice if a default value of 0 were assigned when a parameter isn’t \nsupplied or is blank. I might choose to do this with Boolean expres-\nsions because it feels like this logic doesn’t merit a whole if statement \nor helper function quite yet.\nPython’s syntax makes this choice all too easy. The trick here is that \nthe empty string, the empty list, and zero all evaluate to False implic-\nitly. Thus, the expressions below will evaluate to the subexpression \nafter the or operator when the first subexpression is False:\n# For query string 'red=5&blue=0&green='\nred = my_values.get('red', [''])[0] or 0\ngreen = my_values.get('green', [''])[0] or 0\nopacity = my_values.get('opacity', [''])[0] or 0\nprint(f'Red:     {red!r}')\nprint(f'Green:   {green!r}')\nprint(f'Opacity: {opacity!r}')\n>>>\nRed:     '5'\nGreen:   0\nOpacity: 0\nThe red case works because the key is present in the my_values dictio-\nnary. The value is a list with one member: the string '5'. This string \nimplicitly evaluates to True, so red is assigned to the first part of the \nor expression.\n\n\n \nItem 5: Write Helper Functions Instead of Complex Expressions \n23\nThe green case works because the value in the my_values dictionary is \na list with one member: an empty string. The empty string implicitly \nevaluates to False, causing the or expression to evaluate to 0.\nThe opacity case works because the value in the my_values dictionary \nis missing altogether. The behavior of the get method is to return its \nsecond argument if the key doesn’t exist in the dictionary (see Item 16: \n“Prefer get Over in and KeyError to Handle Missing Dictionary Keys”). \nThe default value in this case is a list with one member: an empty \nstring. When opacity isn’t found in the dictionary, this code does \nexactly the same thing as the green case.\nHowever, this expression is difficult to read, and it still doesn’t do \neverything I need. I’d also want to ensure that all the parameter val-\nues are converted to integers so I can immediately use them in math-\nematical expressions. To do that, I’d wrap each expression with the \nint built-in function to parse the string as an integer:\nred = int(my_values.get('red', [''])[0] or 0)\nThis is now extremely hard to read. There’s so much visual noise. The \ncode isn’t approachable. A new reader of the code would have to spend \ntoo much time picking apart the expression to figure out what it actu-\nally does. Even though it’s nice to keep things short, it’s not worth \ntrying to fit this all on one line.\nPython has if/else conditional—or ternary—expressions to make \ncases like this clearer while keeping the code short:\nred_str = my_values.get('red', [''])\nred = int(red_str[0]) if red_str[0] else 0\nThis is better. For less complicated situations, if/else conditional \nexpressions can make things very clear. But the example above is \nstill not as clear as the alternative of a full if/else statement over \nmultiple lines. Seeing all of the logic spread out like this makes the \ndense version seem even more complex:\ngreen_str = my_values.get('green', [''])\nif green_str[0]:\n    green = int(green_str[0])\nelse:\n    green = 0\nIf you need to reuse this logic repeatedly—even just two or three times, \nas in this example—then writing a helper function is the way to go:\ndef get_first_int(values, key, default=0):\n    found = values.get(key, [''])\n\n\n24 \nChapter 1 Pythonic Thinking\n    if found[0]:\n        return int(found[0])\n    return default\nThe calling code is much clearer than the complex expression using \nor and the two-line version using the if/else expression:\ngreen = get_first_int(my_values, 'green')\nAs soon as expressions get complicated, it’s time to consider split-\nting them into smaller pieces and moving logic into helper functions. \nWhat you gain in readability always outweighs what brevity may have \nafforded you. Avoid letting Python’s pithy syntax for complex expres-\nsions from getting you into a mess like this. Follow the DRY principle: \nDon’t repeat yourself.\nThings to Remember\n✦ Python’s syntax makes it easy to write single-line expressions that \nare overly complicated and difficult to read.\n✦ Move complex expressions into helper functions, especially if you \nneed to use the same logic repeatedly.\n✦ An if/else expression provides a more readable alternative to using \nthe Boolean operators or and and in expressions.\nItem 6:  Prefer Multiple Assignment Unpacking Over \nIndexing\nPython has a built-in tuple type that can be used to create immutable, \nordered sequences of values. In the simplest case, a tuple is a pair of \ntwo values, such as keys and values from a dictionary:\nsnack_calories = {\n    'chips': 140,\n    'popcorn': 80,\n    'nuts': 190,\n}\nitems = tuple(snack_calories.items())\nprint(items)\n>>>\n(('chips', 140), ('popcorn', 80), ('nuts', 190))\nThe values in tuples can be accessed through numerical indexes:\nitem = ('Peanut butter', 'Jelly')\nfirst = item[0]\nsecond = item[1]\nprint(first, 'and', second)\n\n\n \nItem 6: Prefer Multiple Assignment Unpacking Over Indexing \n25\n>>>\nPeanut butter and Jelly\nOnce a tuple is created, you can’t modify it by assigning a new value \nto an index:\npair = ('Chocolate', 'Peanut butter')\npair[0] = 'Honey'\n>>>\nTraceback ...\nTypeError: 'tuple' object does not support item assignment\nPython also has syntax for unpacking, which allows for assigning \nmultiple values in a single statement. The patterns that you specify in \nunpacking assignments look a lot like trying to mutate tuples—which \nisn’t allowed—but they actually work quite differently. For example, if \nyou know that a tuple is a pair, instead of using indexes to access its \nvalues, you can assign it to a tuple of two variable names:\nitem = ('Peanut butter', 'Jelly')\nfirst, second = item  # Unpacking\nprint(first, 'and', second)\n>>>\nPeanut butter and Jelly\nUnpacking has less visual noise than accessing the tuple’s indexes, \nand it often requires fewer lines. The same pattern matching syntax \nof unpacking works when assigning to lists, sequences, and multiple \nlevels of arbitrary iterables within iterables. I don’t recommend doing \nthe following in your code, but it’s important to know that it’s possible \nand how it works:\nfavorite_snacks = {\n    'salty': ('pretzels', 100),\n    'sweet': ('cookies', 180),\n    'veggie': ('carrots', 20),\n}\n((type1, (name1, cals1)),\n (type2, (name2, cals2)),\n (type3, (name3, cals3))) = favorite_snacks.items()\n\n\n26 \nChapter 1 Pythonic Thinking\nprint(f'Favorite {type1} is {name1} with {cals1} calories')\nprint(f'Favorite {type2} is {name2} with {cals2} calories')\nprint(f'Favorite {type3} is {name3} with {cals3} calories')\n>>>\nFavorite salty is pretzels with 100 calories\nFavorite sweet is cookies with 180 calories\nFavorite veggie is carrots with 20 calories\nNewcomers to Python may be surprised to learn that unpacking can \neven be used to swap values in place without the need to create tem-\nporary variables. Here, I use typical syntax with indexes to swap the \nvalues between two positions in a list as part of an ascending order \nsorting algorithm:\ndef bubble_sort(a):\n    for _ in range(len(a)):\n        for i in range(1, len(a)):\n            if a[i] < a[i-1]:\n                temp = a[i]\n                a[i] = a[i-1]\n                a[i-1] = temp\nnames = ['pretzels', 'carrots', 'arugula', 'bacon']\nbubble_sort(names)\nprint(names)\n>>>\n['arugula', 'bacon', 'carrots', 'pretzels']\nHowever, with unpacking syntax, it’s possible to swap indexes in a \nsingle line:\ndef bubble_sort(a):\n    for _ in range(len(a)):\n        for i in range(1, len(a)):\n            if a[i] < a[i-1]:\n                a[i-1], a[i] = a[i], a[i-1]  # Swap\nnames = ['pretzels', 'carrots', 'arugula', 'bacon']\nbubble_sort(names)\nprint(names)\n>>>\n['arugula', 'bacon', 'carrots', 'pretzels']\nThe way this swap works is that the right side of the assignment \n(a[i], a[i-1]) is evaluated first, and its values are put into a new tem-\nporary, unnamed tuple (such as ('carrots', 'pretzels') on the first \n\n\n \nItem 6: Prefer Multiple Assignment Unpacking Over Indexing \n27\niteration of the loops). Then, the unpacking pattern from the left side \nof the assignment (a[i-1], a[i]) is used to receive that tuple value \nand assign it to the variable names a[i-1] and a[i], respectively. \nThis replaces 'pretzels' with 'carrots' at index 0 and 'carrots' with \n'pretzels' at index 1. Finally, the temporary unnamed tuple silently \ngoes away.\nAnother valuable application of unpacking is in the target list of for \nloops and similar constructs, such as comprehensions and generator \nexpressions (see Item 27: “Use Comprehensions Instead of map and \nfilter” for those). As an example for contrast, here I iterate over a \nlist of snacks without using unpacking:\nsnacks = [('bacon', 350), ('donut', 240), ('muffin', 190)]\nfor i in range(len(snacks)):\n    item = snacks[i]\n    name = item[0]\n    calories = item[1]\n    print(f'#{i+1}: {name} has {calories} calories')\n>>>\n#1: bacon has 350 calories\n#2: donut has 240 calories\n#3: muffin has 190 calories\nThis works, but it’s noisy. There are a lot of extra characters required \nin order to index into the various levels of the snacks structure. \nHere, I achieve the same output by using unpacking along with the \nenumerate built-in function (see Item 7: “Prefer enumerate Over range”):\nfor rank, (name, calories) in enumerate(snacks, 1):\n    print(f'#{rank}: {name} has {calories} calories')\n>>>\n#1: bacon has 350 calories\n#2: donut has 240 calories\n#3: muffin has 190 calories\nThis is the Pythonic way to write this type of loop; it’s short and easy to \nunderstand. There’s usually no need to access anything using indexes.\nPython provides additional unpacking functionality for list con-\nstruction (see Item 13: “Prefer Catch-All Unpacking Over Slicing”), \nfunction arguments (see Item 22: “Reduce Visual Noise with Variable \nPositional Arguments”), keyword arguments (see Item 23: “Provide \nOptional Behavior with Keyword Arguments”), multiple return val-\nues (see Item 19: “Never Unpack More Than Three Variables When \n Functions Return Multiple Values”), and more.\nUsing unpacking wisely will enable you to avoid indexing when possi-\nble, resulting in clearer and more Pythonic code.\n\n\n28 \nChapter 1 Pythonic Thinking\nThings to Remember\n✦ Python has special syntax called unpacking for assigning multiple \nvalues in a single statement.\n✦ Unpacking is generalized in Python and can be applied to any \n iterable, including many levels of iterables within iterables.\n✦ Reduce visual noise and increase code clarity by using unpacking \nto avoid explicitly indexing into sequences.\nItem 7: Prefer enumerate Over range\nThe range built-in function is useful for loops that iterate over a set of \nintegers:\nfrom random import randint\nrandom_bits = 0\nfor i in range(32):\n    if randint(0, 1):\n        random_bits |= 1 << i\nprint(bin(random_bits))\n>>>\n0b11101000100100000111000010000001\nWhen you have a data structure to iterate over, like a list of strings, \nyou can loop directly over the sequence:\nflavor_list = ['vanilla', 'chocolate', 'pecan', 'strawberry']\nfor flavor in flavor_list:\n    print(f'{flavor} is delicious')\n>>>\nvanilla is delicious\nchocolate is delicious\npecan is delicious\nstrawberry is delicious\nOften, you’ll want to iterate over a list and also know the index of \nthe current item in the list. For example, say that I want to print the \nranking of my favorite ice cream flavors. One way to do it is by using \nrange:\nfor i in range(len(flavor_list)):\n    flavor = flavor_list[i]\n    print(f'{i + 1}: {flavor}')\n",
      "page_number": 43,
      "chapter_number": 6,
      "summary": "✦ The str.format method introduces some useful concepts in its for-\nmatting specifiers mini language, but it otherwise repeats the mis-\ntakes of C-style format strings and should be avoided Key topics include item, value, and pythonic.",
      "keywords": [
        "item",
        "C-style format strings",
        "green",
        "string",
        "calories",
        "Unpacking",
        "Expressions",
        "red",
        "Prefer Multiple Assignment",
        "Python",
        "Write Helper Functions",
        "Multiple Assignment Unpacking",
        "Helper Functions",
        "multiple",
        "key"
      ],
      "concepts": [
        "item",
        "value",
        "pythonic",
        "expressions",
        "expressiveness",
        "string",
        "strings",
        "unpacking",
        "line",
        "indexing"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 3,
          "title": "Segment 3 (pages 19-26)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 7,
          "title": "String Fundamentals",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 51,
          "title": "Segment 51 (pages 465-475)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 35,
          "title": "Segment 35 (pages 316-323)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 51-58)",
      "start_page": 51,
      "end_page": 58,
      "detection_method": "topic_boundary",
      "content": " \nItem 7: Prefer enumerate Over range \n29\n>>>\n1: vanilla\n2: chocolate\n3: pecan\n4: strawberry\nThis looks clumsy compared with the other examples of iterating over \nflavor_list or range. I have to get the length of the list. I have to \nindex into the array. The multiple steps make it harder to read.\nPython provides the enumerate built-in function to address this situa-\ntion. enumerate wraps any iterator with a lazy generator (see Item 30: \n“Consider Generators Instead of Returning Lists”). enumerate yields \npairs of the loop index and the next value from the given iterator. \nHere, I manually advance the returned iterator with the next built-in \nfunction to demonstrate what it does:\nit = enumerate(flavor_list)\nprint(next(it))\nprint(next(it))\n>>>\n(0, 'vanilla')\n(1, 'chocolate')\nEach pair yielded by enumerate can be succinctly unpacked in a for \nstatement (see Item 6: “Prefer Multiple Assignment Unpacking Over \nIndexing” for how that works). The resulting code is much clearer:\nfor i, flavor in enumerate(flavor_list):\n    print(f'{i + 1}: {flavor}')\n>>>\n1: vanilla\n2: chocolate\n3: pecan\n4: strawberry\nI can make this even shorter by specifying the number from which \nenumerate should begin counting (1 in this case) as the second \nparameter:\nfor i, flavor in enumerate(flavor_list, 1):\n    print(f'{i}: {flavor}')\nThings to Remember\n✦ enumerate provides concise syntax for looping over an iterator and \ngetting the index of each item from the iterator as you go.\n\n\n30 \nChapter 1 Pythonic Thinking\n✦ Prefer enumerate instead of looping over a range and indexing into a \nsequence.\n✦ You can supply a second parameter to enumerate to specify the \nnumber from which to begin counting (zero is the default).\nItem 8: Use zip to Process Iterators in Parallel\nOften in Python you find yourself with many lists of related objects. \nList comprehensions make it easy to take a source list and get a \nderived list by applying an expression (see Item 27: “Use Comprehen-\nsions Instead of map and filter”):\nnames = ['Cecilia', 'Lise', 'Marie']\ncounts = [len(n) for n in names]\nprint(counts)\n>>>\n[7, 4, 5]\nThe items in the derived list are related to the items in the source \nlist by their indexes. To iterate over both lists in parallel, I can iterate \nover the length of the names source list:\nlongest_name = None\nmax_count = 0\nfor i in range(len(names)):\n    count = counts[i]\n    if count > max_count:\n        longest_name = names[i]\n        max_count = count\nprint(longest_name)\n>>>\nCecilia\nThe problem is that this whole loop statement is visually noisy. The \nindexes into names and counts make the code hard to read. Indexing \ninto the arrays by the loop index i happens twice. Using enumerate \n(see Item 7: “Prefer enumerate Over range”) improves this slightly, but \nit’s still not ideal:\nfor i, name in enumerate(names):\n    count = counts[i]\n    if count > max_count:\n        longest_name = name\n        max_count = count\n\n\n \nItem 8: Use zip to Process Iterators in Parallel \n31\nTo make this code clearer, Python provides the zip built-in function. \nzip wraps two or more iterators with a lazy generator. The zip gener-\nator yields tuples containing the next value from each iterator. These \ntuples can be unpacked directly within a for statement (see Item 6: \n“Prefer Multiple Assignment Unpacking Over Indexing”). The resulting \ncode is much cleaner than the code for indexing into multiple lists:\nfor name, count in zip(names, counts):\n    if count > max_count:\n        longest_name = name\n        max_count = count\nzip consumes the iterators it wraps one item at a time, which means \nit can be used with infinitely long inputs without risk of a program \nusing too much memory and crashing.\nHowever, beware of zip’s behavior when the input iterators are of \ndifferent lengths. For example, say that I add another item to names \nabove but forget to update counts. Running zip on the two input lists \nwill have an unexpected result:\nnames.append('Rosalind')\nfor name, count in zip(names, counts):\n    print(name)\n>>>\nCecilia\nLise\nMarie\nThe new item for 'Rosalind' isn’t there. Why not? This is just how \nzip works. It keeps yielding tuples until any one of the wrapped iter-\nators is exhausted. Its output is as long as its shortest input. This \napproach works fine when you know that the iterators are of the \nsame length, which is often the case for derived lists created by list \ncomprehensions.\nBut in many other cases, the truncating behavior of zip is surprising \nand bad. If you don’t expect the lengths of the lists passed to zip to \nbe equal, consider using the zip_longest function from the itertools \nbuilt-in module instead:\nimport itertools\nfor name, count in itertools.zip_longest(names, counts):\n    print(f'{name}: {count}')\n\n\n32 \nChapter 1 Pythonic Thinking\n>>>\nCecilia: 7\nLise: 4\nMarie: 5\nRosalind: None\nzip_longest replaces missing values—the length of the string \n'Rosalind' in this case—with whatever fillvalue is passed to it, which \ndefaults to None.\nThings to Remember\n✦ The zip built-in function can be used to iterate over multiple itera-\ntors in parallel.\n✦ zip creates a lazy generator that produces tuples, so it can be used \non infinitely long inputs.\n✦ zip truncates its output silently to the shortest iterator if you supply \nit with iterators of different lengths.\n✦ Use the zip_longest function from the itertools built-in mod-\nule if you want to use zip on iterators of unequal lengths without \ntruncation.\nItem 9: Avoid else Blocks After for and while Loops\nPython loops have an extra feature that is not available in most other \nprogramming languages: You can put an else block immediately after \na loop’s repeated interior block:\nfor i in range(3):\n    print('Loop', i)\nelse:\n    print('Else block!')\n>>>\nLoop 0\nLoop 1\nLoop 2\nElse block!\nSurprisingly, the else block runs immediately after the loop finishes. \nWhy is the clause called “else”? Why not “and”? In an if/else state-\nment, else means “Do this if the block before this doesn’t happen.” In \na try/except statement, except has the same definition: “Do this if \ntrying the block before this failed.”\n\n\n \nItem 9: Avoid else Blocks After for and while Loops \n33\nSimilarly, else from try/except/else follows this pattern (see Item 65: \n“Take Advantage of Each Block in try/except/else/finally”) because it \nmeans “Do this if there was no exception to handle.” try/finally is also \nintuitive because it means “Always do this after trying the block before.”\nGiven all the uses of else, except, and finally in Python, a new pro-\ngrammer might assume that the else part of for/else means “Do this \nif the loop wasn’t completed.” In reality, it does exactly the opposite. \nUsing a break statement in a loop actually skips the else block:\nfor i in range(3):\n    print('Loop', i)\n    if i == 1:\n        break\nelse:\n    print('Else block!')\n>>>\nLoop 0\nLoop 1\nAnother surprise is that the else block runs immediately if you loop \nover an empty sequence:\nfor x in []:\n    print('Never runs')\nelse:\n    print('For Else block!')\n>>>\nFor Else block!\nThe else block also runs when while loops are initially False:\nwhile False:\n    print('Never runs')\nelse:\n    print('While Else block!')\n>>>\nWhile Else block!\nThe rationale for these behaviors is that else blocks after loops are \nuseful when using loops to search for something. For example, say \nthat I want to determine whether two numbers are coprime (that is, \ntheir only common divisor is 1). Here, I iterate through every pos-\nsible common divisor and test the numbers. After every option has \n\n\n34 \nChapter 1 Pythonic Thinking\nbeen tried, the loop ends. The else block runs when the numbers are \ncoprime because the loop doesn’t encounter a break:\na = 4\nb = 9\nfor i in range(2, min(a, b) + 1):\n    print('Testing', i)\n    if a % i == 0 and b % i == 0:\n        print('Not coprime')\n        break\nelse:\n    print('Coprime')\n>>>\nTesting 2\nTesting 3\nTesting 4\nCoprime\nIn practice, I wouldn’t write the code this way. Instead, I’d write a \nhelper function to do the calculation. Such a helper function is writ-\nten in two common styles.\nThe first approach is to return early when I find the condition I’m look-\ning for. I return the default outcome if I fall through the loop:\ndef coprime(a, b):\n    for i in range(2, min(a, b) + 1):\n        if a % i == 0 and b % i == 0:\n            return False\n    return True\nassert coprime(4, 9)\nassert not coprime(3, 6)\nThe second way is to have a result variable that indicates whether I’ve \nfound what I’m looking for in the loop. I break out of the loop as soon \nas I find something:\ndef coprime_alternate(a, b):\n    is_coprime = True\n    for i in range(2, min(a, b) + 1):\n        if a % i == 0 and b % i == 0:\n            is_coprime = False\n            break\n    return is_coprime\n\n\n \nItem 10: Prevent Repetition with Assignment Expressions \n35\nassert coprime_alternate(4, 9)\nassert not coprime_alternate(3, 6)\nBoth approaches are much clearer to readers of unfamiliar code. \nDepending on the situation, either may be a good choice. However, the \nexpressivity you gain from the else block doesn’t outweigh the burden \nyou put on people (including yourself) who want to understand your \ncode in the future. Simple constructs like loops should be self-evident \nin Python. You should avoid using else blocks after loops entirely.\nThings to Remember\n✦ Python has special syntax that allows else blocks to immediately \nfollow for and while loop interior blocks.\n✦ The else block after a loop runs only if the loop body did not encoun-\nter a break statement.\n✦ Avoid using else blocks after loops because their behavior isn’t \n intuitive and can be confusing.\nItem 10:  Prevent Repetition with Assignment \nExpressions\nAn assignment expression—also known as the walrus operator—is a \nnew syntax introduced in Python 3.8 to solve a long-standing problem \nwith the language that can cause code duplication. Whereas normal \nassignment statements are written a = b and pronounced “a equals b,” \nthese assignments are written a := b and pronounced “a walrus b” \n(because := looks like a pair of eyeballs and tusks).\nAssignment expressions are useful because they enable you to assign \nvariables in places where assignment statements are disallowed, such \nas in the conditional expression of an if statement. An assignment \nexpression’s value evaluates to whatever was assigned to the identi-\nfier on the left side of the walrus operator.\nFor example, say that I have a basket of fresh fruit that I’m trying to \nmanage for a juice bar. Here, I define the contents of the basket:\nfresh_fruit = {\n    'apple': 10,\n    'banana': 8,\n    'lemon': 5,\n}\n\n\n36 \nChapter 1 Pythonic Thinking\nWhen a customer comes to the counter to order some lemonade, \nI need to make sure there is at least one lemon in the basket to \nsqueeze. Here, I do this by retrieving the count of lemons and then \nusing an if statement to check for a non-zero value:\ndef make_lemonade(count):\n    ...\ndef out_of_stock():\n    ...\ncount = fresh_fruit.get('lemon', 0)\nif count:\n    make_lemonade(count)\nelse:\n    out_of_stock()\nThe problem with this seemingly simple code is that it’s noisier than \nit needs to be. The count variable is used only within the first block \nof the if statement. Defining count above the if statement causes it \nto appear to be more important than it really is, as if all code that fol-\nlows, including the else block, will need to access the count variable, \nwhen in fact that is not the case.\nThis pattern of fetching a value, checking to see if it’s non-zero, and \nthen using it is extremely common in Python. Many programmers \ntry to work around the multiple references to count with a variety \nof tricks that hurt readability (see Item 5: “Write Helper Functions \nInstead of Complex Expressions” for an example). Luckily, assign-\nment expressions were added to the language to streamline exactly \nthis type of code. Here, I rewrite this example using the walrus \noperator:\nif count := fresh_fruit.get('lemon', 0):\n    make_lemonade(count)\nelse:\n    out_of_stock()\nThough this is only one line shorter, it’s a lot more readable because \nit’s now clear that count is only relevant to the first block of the if \nstatement. The assignment expression is first assigning a value to the \ncount variable, and then evaluating that value in the context of the if \nstatement to determine how to proceed with flow control. This two-\nstep behavior—assign and then evaluate—is the fundamental nature \nof the walrus operator.\nLemons are quite potent, so only one is needed for my lemonade rec-\nipe, which means a non-zero check is good enough. If a customer \n",
      "page_number": 51,
      "chapter_number": 7,
      "summary": "Python provides the enumerate built-in function to address this situa-\ntion Key topics include loops, item, and code. ✦ Unpacking is generalized in Python and can be applied to any \n iterable, including many levels of iterables within iterables.",
      "keywords": [
        "Item",
        "loop",
        "block",
        "list",
        "count",
        "zip",
        "flavor",
        "enumerate",
        "Python",
        "range",
        "Prefer enumerate",
        "coprime",
        "Multiple Assignment Unpacking",
        "Prefer Multiple Assignment",
        "Pythonic Thinking"
      ],
      "concepts": [
        "loops",
        "item",
        "code",
        "counting",
        "list",
        "names",
        "enumerate",
        "iterate",
        "iterating",
        "assignment"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 37,
          "title": "Segment 37 (pages 333-344)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 5,
          "title": "Segment 5 (pages 84-106)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 9,
          "title": "Tuples, Files, and Everything Else",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 9,
          "title": "Segment 9 (pages 71-78)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 1,
          "title": "Data Structures and Algorithms",
          "relevance_score": 0.52,
          "method": "api"
        }
      ]
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 59-69)",
      "start_page": 59,
      "end_page": 69,
      "detection_method": "topic_boundary",
      "content": " \nItem 10: Prevent Repetition with Assignment Expressions \n37\norders a cider, though, I need to make sure that I have at least four \napples. Here, I do this by fetching the count from the fruit_basket \ndictionary, and then using a comparison in the if statement condi-\ntional expression:\ndef make_cider(count):\n    ...\ncount = fresh_fruit.get('apple', 0)\nif count >= 4:\n    make_cider(count)\nelse:\n    out_of_stock()\nThis has the same problem as the lemonade example, where the \nassignment of count puts distracting emphasis on that variable. \nHere, I improve the clarity of this code by also using the walrus \noperator:\nif (count := fresh_fruit.get('apple', 0)) >= 4:\n    make_cider(count)\nelse:\n    out_of_stock()\nThis works as expected and makes the code one line shorter. It’s \nimportant to note how I needed to surround the assignment expres-\nsion with parentheses to compare it with 4 in the if statement. In \nthe lemonade example, no surrounding parentheses were required \nbecause the assignment expression stood on its own as a non-zero \ncheck; it wasn’t a subexpression of a larger expression. As with other \nexpressions, you should avoid surrounding assignment expressions \nwith parentheses when possible.\nAnother common variation of this repetitive pattern occurs when I \nneed to assign a variable in the enclosing scope depending on some \ncondition, and then reference that variable shortly afterward in a \nfunction call. For example, say that a customer orders some banana \nsmoothies. In order to make them, I need to have at least two bananas’ \nworth of slices, or else an OutOfBananas exception will be raised. Here, \nI implement this logic in a typical way:\ndef slice_bananas(count):\n    ...\nclass OutOfBananas(Exception):\n    pass\n\n\n38 \nChapter 1 Pythonic Thinking\ndef make_smoothies(count):\n    ...\npieces = 0\ncount = fresh_fruit.get('banana', 0)\nif count >= 2:\n    pieces = slice_bananas(count)\ntry:\n    smoothies = make_smoothies(pieces)\nexcept OutOfBananas:\n    out_of_stock()\nThe other common way to do this is to put the pieces = 0 assignment \nin the else block:\ncount = fresh_fruit.get('banana', 0)\nif count >= 2:\n    pieces = slice_bananas(count)\nelse:\n    pieces = 0\ntry:\n    smoothies = make_smoothies(pieces)\nexcept OutOfBananas:\n    out_of_stock()\nThis second approach can feel odd because it means that the \npieces variable has two different locations—in each block of the if \nstatement—where it can be initially defined. This split definition tech-\nnically works because of Python’s scoping rules (see Item 21: “Know \nHow Closures Interact with Variable Scope”), but it isn’t easy to read \nor discover, which is why many people prefer the construct above, \nwhere the pieces = 0 assignment is first.\nThe walrus operator can again be used to shorten this example by \none line of code. This small change removes any emphasis on the \ncount variable. Now, it’s clearer that pieces will be important beyond \nthe if statement:\npieces = 0\nif (count := fresh_fruit.get('banana', 0)) >= 2:\n    pieces = slice_bananas(count)\ntry:\n    smoothies = make_smoothies(pieces)\nexcept OutOfBananas:\n    out_of_stock()\n\n\n \nItem 10: Prevent Repetition with Assignment Expressions \n39\nUsing the walrus operator also improves the readability of splitting \nthe definition of pieces across both parts of the if statement. It’s eas-\nier to trace the pieces variable when the count definition no longer \nprecedes the if statement:\nif (count := fresh_fruit.get('banana', 0)) >= 2:\n    pieces = slice_bananas(count)\nelse:\n    pieces = 0\ntry:\n    smoothies = make_smoothies(pieces)\nexcept OutOfBananas:\n    out_of_stock()\nOne frustration that programmers who are new to Python often have \nis the lack of a flexible switch/case statement. The general style for \napproximating this type of functionality is to have a deep nesting of \nmultiple if, elif, and else statements.\nFor example, imagine that I want to implement a system of precedence \nso that each customer automatically gets the best juice available and \ndoesn’t have to order. Here, I define logic to make it so banana smooth-\nies are served first, followed by apple cider, and then finally lemonade:\ncount = fresh_fruit.get('banana', 0)\nif count >= 2:\n    pieces = slice_bananas(count)\n    to_enjoy = make_smoothies(pieces)\nelse:\n    count = fresh_fruit.get('apple', 0)\n    if count >= 4:\n        to_enjoy = make_cider(count)\n    else:\n        count = fresh_fruit.get('lemon', 0)\n        if count:\n            to_enjoy = make_lemonade(count)\n        else:\n            to_enjoy‘= 'Nothing'\nUgly constructs like this are surprisingly common in Python code. \nLuckily, the walrus operator provides an elegant solution that can feel \nnearly as versatile as dedicated syntax for switch/case statements:\nif (count := fresh_fruit.get('banana', 0)) >= 2:\n    pieces = slice_bananas(count)\n    to_enjoy = make_smoothies(pieces)\n\n\n40 \nChapter 1 Pythonic Thinking\nelif (count := fresh_fruit.get('apple', 0)) >= 4:\n    to_enjoy = make_cider(count)\nelif count := fresh_fruit.get('lemon', 0):\n    to_enjoy = make_lemonade(count)\nelse:\n    to_enjoy = 'Nothing'\nThe version that uses assignment expressions is only five lines shorter \nthan the original, but the improvement in readability is vast due to \nthe reduction in nesting and indentation. If you ever see such ugly \nconstructs emerge in your code, I suggest that you move them over to \nusing the walrus operator if possible.\nAnother common frustration of new Python programmers is the lack \nof a do/while loop construct. For example, say that I want to bottle \njuice as new fruit is delivered until there’s no fruit remaining. Here, \nI implement this logic with a while loop:\ndef pick_fruit():\n    ...\ndef make_juice(fruit, count):\n    ...\nbottles = []\nfresh_fruit = pick_fruit()\nwhile fresh_fruit:\n    for fruit, count in fresh_fruit.items():\n        batch = make_juice(fruit, count)\n        bottles.extend(batch)\n    fresh_fruit = pick_fruit()\nThis is repetitive because it requires two separate fresh_fruit = \npick_fruit() calls: one before the loop to set initial conditions, and \nanother at the end of the loop to replenish the list of delivered fruit.\nA strategy for improving code reuse in this situation is to use the \nloop-and-a-half idiom. This eliminates the redundant lines, but it \nalso undermines the while loop’s contribution by making it a dumb \ninfinite loop. Now, all of the flow control of the loop depends on the \nconditional break statement:\nbottles = []\nwhile True:                     # Loop\n    fresh_fruit = pick_fruit()\n    if not fresh_fruit:         # And a half\n        break\n\n\n \nItem 10: Prevent Repetition with Assignment Expressions \n41\n    for fruit, count in fresh_fruit.items():\n        batch = make_juice(fruit, count)\n        bottles.extend(batch)\nThe walrus operator obviates the need for the loop-and-a-half idiom \nby allowing the fresh_fruit variable to be reassigned and then con-\nditionally evaluated each time through the while loop. This solution \nis short and easy to read, and it should be the preferred approach in \nyour code:\nbottles = []\nwhile fresh_fruit := pick_fruit():\n    for fruit, count in fresh_fruit.items():\n        batch = make_juice(fruit, count)\n        bottles.extend(batch)\nThere are many other situations where assignment expressions can \nbe used to eliminate redundancy (see Item 29: “Avoid Repeated Work \nin Comprehensions by Using Assignment Expressions” for another). \nIn general, when you find yourself repeating the same expression or \nassignment multiple times within a grouping of lines, it’s time to con-\nsider using assignment expressions in order to improve readability.\nThings to Remember\n✦ Assignment expressions use the walrus operator (:=) to both assign \nand evaluate variable names in a single expression, thus reducing \nrepetition.\n✦ When an assignment expression is a subexpression of a larger \nexpression, it must be surrounded with parentheses.\n✦ Although switch/case statements and do/while loops are not avail-\nable in Python, their functionality can be emulated much more \nclearly by using assignment expressions.\n\n\nThis page intentionally left blank \n\n\n2\nLists and \nDictionaries\nMany programs are written to automate repetitive tasks that are \n better suited to machines than to humans. In Python, the most \n common way to organize this kind of work is by using a sequence of \nvalues stored in a list type. Lists are extremely versatile and can be \nused to solve a variety of problems.\nA natural complement to lists is the dict type, which stores lookup \nkeys mapped to corresponding values (in what is often called an \nassociative array or a hash table). Dictionaries provide constant time \n(amortized) performance for assignments and accesses, which means \nthey are ideal for bookkeeping dynamic information.\nPython has special syntax and built-in modules that enhance read-\nability and extend the capabilities of lists and dictionaries beyond \nwhat you might expect from simple array, vector, and hash table types \nin other languages.\nItem 11: Know How to Slice Sequences\nPython includes syntax for slicing sequences into pieces. Slicing \nallows you to access a subset of a sequence’s items with minimal \neffort. The simplest uses for slicing are the built-in types list, str, and \nbytes. Slicing can be extended to any Python class that implements \nthe __getitem__ and __setitem__ special methods (see Item 43: \n“Inherit from collections.abc for Custom Container Types”).\nThe basic form of the slicing syntax is somelist[start:end], where \nstart is inclusive and end is exclusive:\na = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nprint('Middle two:  ', a[3:5])\nprint('All but ends:', a[1:7])\n>>>\nMiddle two:   ['d', 'e']\nAll but ends: ['b', 'c', 'd', 'e', 'f', 'g']\n\n\n44 \nChapter 2 Lists and Dictionaries\nWhen slicing from the start of a list, you should leave out the zero \nindex to reduce visual noise:\nassert a[:5] == a[0:5]\nWhen slicing to the end of a list, you should leave out the final index \nbecause it’s redundant:\nassert a[5:] == a[5:len(a)]\nUsing negative numbers for slicing is helpful for doing offsets relative \nto the end of a list. All of these forms of slicing would be clear to \na new reader of your code:\na[:]      # ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\na[:5]     # ['a', 'b', 'c', 'd', 'e']\na[:-1]    # ['a', 'b', 'c', 'd', 'e', 'f', 'g']\na[4:]     #                     ['e', 'f', 'g', 'h']\na[-3:]    #                          ['f', 'g', 'h']\na[2:5]    #           ['c', 'd', 'e']\na[2:-1]   #           ['c', 'd', 'e', 'f', 'g']\na[-3:-1]  #                          ['f', 'g']\nThere are no surprises here, and I encourage you to use these \nvariations.\nSlicing deals properly with start and end indexes that are beyond the \nboundaries of a list by silently omitting missing items. This behav-\nior makes it easy for your code to establish a maximum length to \n consider for an input sequence:\nfirst_twenty_items = a[:20]\nlast_twenty_items = a[-20:]\nIn contrast, accessing the same index directly causes an exception:\na[20]\n>>>\nTraceback ...\nIndexError: list index out of range\nNote\nBeware that indexing a list by a negated variable is one of the few situ-\nations in which you can get surprising results from slicing. For example, \nthe expression somelist[-n:] will work fine when n is greater than one \n(e.g., somelist[-3:]). However, when n is zero, the expression somelist[-0:] \nis equivalent to somelist[:] and will result in a copy of the original list.\n\n\n \nItem 11: Know How to Slice Sequences \n45\nThe result of slicing a list is a whole new list. References to the \nobjects from the original list are maintained. Modifying the result of \nslicing won’t affect the original list:\nb = a[3:]\nprint('Before:   ', b)\nb[1] = 99\nprint('After:    ', b)\nprint('No change:', a)\n>>>\nBefore:    ['d', 'e', 'f', 'g', 'h']\nAfter:     ['d', 99, 'f', 'g', 'h']\nNo change: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nWhen used in assignments, slices replace the specified range \nin the original list. Unlike unpacking assignments (such as \na, b = c[:2]; see Item 6: “Prefer Multiple Assignment Unpacking \nOver Indexing”), the lengths of slice assignments don’t need to be the \nsame. The values before and after the assigned slice will be preserved. \nHere, the list shrinks because the replacement list is shorter than \nthe specified slice:\nprint('Before ', a)\na[2:7] = [99, 22, 14]\nprint('After  ', a)\n>>>\nBefore  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nAfter   ['a', 'b', 99, 22, 14, 'h']\nAnd here the list grows because the assigned list is longer than the \nspecific slice:\nprint('Before ', a)\na[2:3] = [47, 11]\nprint('After  ', a)\n>>>\nBefore  ['a', 'b', 99, 22, 14, 'h']\nAfter   ['a', 'b', 47, 11, 22, 14, 'h']\nIf you leave out both the start and the end indexes when slicing, you \nend up with a copy of the original list:\nb = a[:]\nassert b == a and b is not a\n\n\n46 \nChapter 2 Lists and Dictionaries\nIf you assign to a slice with no start or end indexes, you replace the \nentire contents of the list with a copy of what’s referenced (instead of \nallocating a new list):\nb = a\nprint('Before a', a)\nprint('Before b', b)\na[:] = [101, 102, 103]\nassert a is b             # Still the same list object\nprint('After a ', a)      # Now has different contents\nprint('After b ', b)      # Same list, so same contents as a\n>>>\nBefore a ['a', 'b', 47, 11, 22, 14, 'h']\nBefore b ['a', 'b', 47, 11, 22, 14, 'h']\nAfter a  [101, 102, 103]\nAfter b  [101, 102, 103]\nThings to Remember\n✦ Avoid being verbose when slicing: Don’t supply 0 for the start index \nor the length of the sequence for the end index.\n✦ Slicing is forgiving of start or end indexes that are out of bounds, \nwhich means it’s easy to express slices on the front or back bound-\naries of a sequence (like a[:20] or a[-20:]).\n✦ Assigning to a list slice replaces that range in the original sequence \nwith what’s referenced even if the lengths are different.\nItem 12:  Avoid Striding and Slicing in \na Single Expression\nIn addition to basic slicing (see Item 11: “Know How to Slice \nSequences”), Python has special syntax for the stride of a slice in \nthe form somelist[start:end:stride]. This lets you take every nth item \nwhen slicing a sequence. For example, the stride makes it easy to \ngroup by even and odd indexes in a list:\nx = ['red', 'orange', 'yellow', 'green', 'blue', 'purple']\nodds = x[::2]\nevens = x[1::2]\nprint(odds)\nprint(evens)\n>>>\n['red', 'yellow', 'blue']\n['orange', 'green', 'purple']\n\n\n \nItem 12: Avoid Striding and Slicing in a Single Expression \n47\nThe problem is that the stride syntax often causes unexpected behav-\nior that can introduce bugs. For example, a common Python trick for \nreversing a byte string is to slice the string with a stride of -1:\nx = b'mongoose'\ny = x[::-1]\nprint(y)\n>>>\nb'esoognom'\nThis also works correctly for Unicode strings (see Item 3: “Know the \nDifferences Between bytes and str”):\nx = 'ᇓৌ'\ny = x[::-1]\nprint(y)\n>>>\nৌᇓ\nBut it will break when Unicode data is encoded as a UTF-8 byte string:\nw = 'ᇓৌ'\nx = w.encode('utf-8')\ny = x[::-1]\nz = y.decode('utf-8')\n>>>\nTraceback ...\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xb8 in \nposition 0: invalid start byte\nAre negative strides besides -1 useful? Consider the following \nexamples:\nx = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nx[::2]   # ['a', 'c', 'e', 'g']\nx[::-2]  # ['h', 'f', 'd', 'b']\nHere, ::2 means “Select every second item starting at the beginning.” \nTrickier, ::-2 means “Select every second item starting at the end and \nmoving backward.”\nWhat do you think 2::2 means? What about -2::-2 vs. -2:2:-2 vs. \n2:2:-2?\nx[2::2]     # ['c', 'e', 'g']\nx[-2::-2]   # ['g', 'e', 'c', 'a']\nx[-2:2:-2]  # ['g', 'e']\nx[2:2:-2]   # []\n",
      "page_number": 59,
      "chapter_number": 8,
      "summary": "This chapter covers segment 8 (pages 59-69). Key topics include list, item, and count. count = fresh_fruit.get('lemon', 0)\nif count:\n    make_lemonade(count)\nelse:\n    out_of_stock()\nThe problem with this seemingly simple code is that it’s noisier than \nit needs to be.",
      "keywords": [
        "count",
        "Assignment Expressions",
        "list",
        "make",
        "fresh",
        "assignment",
        "fruit",
        "pieces",
        "slicing",
        "Expressions",
        "slice",
        "Item",
        "Python",
        "fruit.get",
        "count variable"
      ],
      "concepts": [
        "list",
        "item",
        "count",
        "slices",
        "slicing",
        "assign",
        "assignments",
        "expressions",
        "express",
        "pieces"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 8,
          "title": "Segment 8 (pages 62-70)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 32,
          "title": "Segment 32 (pages 637-655)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 3,
          "title": "Segment 3 (pages 19-26)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 37,
          "title": "Segment 37 (pages 333-344)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 13,
          "title": "Segment 13 (pages 108-115)",
          "relevance_score": 0.45,
          "method": "api"
        }
      ]
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 70-82)",
      "start_page": 70,
      "end_page": 82,
      "detection_method": "topic_boundary",
      "content": "48 \nChapter 2 Lists and Dictionaries\nThe point is that the stride part of the slicing syntax can be extremely \nconfusing. Having three numbers within the brackets is hard enough \nto read because of its density. Then, it’s not obvious when the start \nand end indexes come into effect relative to the stride value, espe-\ncially when the stride is negative.\nTo prevent problems, I suggest you avoid using a stride along with \nstart and end indexes. If you must use a stride, prefer making it a \npositive value and omit start and end indexes. If you must use a stride \nwith start or end indexes, consider using one assignment for striding \nand another for slicing:\ny = x[::2]   # ['a', 'c', 'e', 'g']\nz = y[1:-1]  # ['c', 'e']\nStriding and then slicing creates an extra shallow copy of the data. \nThe first operation should try to reduce the size of the resulting slice \nby as much as possible. If your program can’t afford the time or mem-\nory required for two steps, consider using the itertools built-in mod-\nule’s islice method (see Item 36: “Consider itertools for Working with \nIterators and Generators”), which is clearer to read and doesn’t permit \nnegative values for start, end, or stride.\nThings to Remember\n✦ Specifying start, end, and stride in a slice can be extremely \nconfusing.\n✦ Prefer using positive stride values in slices without start or end \nindexes. Avoid negative stride values if possible.\n✦ Avoid using start, end, and stride together in a single slice. If you \nneed all three parameters, consider doing two assignments (one \nto stride and another to slice) or using islice from the itertools \nbuilt-in module.\nItem 13: Prefer Catch-All Unpacking Over Slicing\nOne limitation of basic unpacking (see Item 6: “Prefer Multiple Assign-\nment Unpacking Over Indexing”) is that you must know the length of \nthe sequences you’re unpacking in advance. For example, here I have \na list of the ages of cars that are being traded in at a dealership. \nWhen I try to take the first two items of the list with basic unpack-\ning, an exception is raised at runtime:\ncar_ages = [0, 9, 4, 8, 7, 20, 19, 1, 6, 15]\ncar_ages_descending = sorted(car_ages, reverse=True)\noldest, second_oldest = car_ages_descending\n\n\n \nItem 13: Prefer Catch-All Unpacking Over Slicing \n49\n>>>\nTraceback ...\nValueError: too many values to unpack (expected 2)\nNewcomers to Python often rely on indexing and slicing (see Item 11: \n“Know How to Slice Sequences”) for this situation. For example, here \nI extract the oldest, second oldest, and other car ages from a list of at \nleast two items:\noldest = car_ages_descending[0]\nsecond_oldest = car_ages_descending[1]\nothers = car_ages_descending[2:]\nprint(oldest, second_oldest, others)\n>>>\n20 19 [15, 9, 8, 7, 6, 4, 1, 0]\nThis works, but all of the indexing and slicing is visually noisy. In \npractice, it’s also error prone to divide the members of a sequence into \nvarious subsets this way because you’re much more likely to make \noff-by-one errors; for example, you might change boundaries on one \nline and forget to update the others.\nTo better handle this situation, Python also supports catch-all \nunpacking through a starred expression. This syntax allows one part \nof the unpacking assignment to receive all values that didn’t match \nany other part of the unpacking pattern. Here, I use a starred expres-\nsion to achieve the same result as above without indexing or slicing:\noldest, second_oldest, *others = car_ages_descending\nprint(oldest, second_oldest, others)\n>>>\n20 19 [15, 9, 8, 7, 6, 4, 1, 0]\nThis code is shorter, easier to read, and no longer has the error-prone \nbrittleness of boundary indexes that must be kept in sync between \nlines.\nA starred expression may appear in any position, so you can get the \nbenefits of catch-all unpacking anytime you need to extract one slice:\noldest, *others, youngest = car_ages_descending\nprint(oldest, youngest, others)\n*others, second_youngest, youngest = car_ages_descending\nprint(youngest, second_youngest, others)\n>>>\n20 0 [19, 15, 9, 8, 7, 6, 4, 1]\n0 1 [20, 19, 15, 9, 8, 7, 6, 4]\n\n\n50 \nChapter 2 Lists and Dictionaries\nHowever, to unpack assignments that contain a starred expres-\nsion, you must have at least one required part, or else you’ll get a \nSyntaxError. You can’t use a catch-all expression on its own:\n*others = car_ages_descending\n>>>\nTraceback ...\nSyntaxError: starred assignment target must be in a list or \n¯tuple\nYou also can’t use multiple catch-all expressions in a single-level \nunpacking pattern:\nfirst, *middle, *second_middle, last = [1, 2, 3, 4]\n>>>\nTraceback ...\nSyntaxError: two starred expressions in assignment\nBut it is possible to use multiple starred expressions in an unpacking \nassignment statement, as long as they’re catch-alls for different parts \nof the multilevel structure being unpacked. I don’t recommend doing \nthe following (see Item 19: “Never Unpack More Than Three Variables \nWhen Functions Return Multiple Values” for related guidance), but \nunderstanding it should help you develop an intuition for how starred \nexpressions can be used in unpacking assignments:\ncar_inventory = {\n    'Downtown': ('Silver Shadow', 'Pinto', 'DMC'),\n    'Airport': ('Skyline', 'Viper', 'Gremlin', 'Nova'),\n}\n((loc1, (best1, *rest1)),\n (loc2, (best2, *rest2))) = car_inventory.items()\nprint(f'Best at {loc1} is {best1}, {len(rest1)} others')\nprint(f'Best at {loc2} is {best2}, {len(rest2)} others')\n>>>\nBest at Downtown is Silver Shadow, 2 others\nBest at Airport is Skyline, 3 others\nStarred expressions become list instances in all cases. If there are \nno leftover items from the sequence being unpacked, the catch-all \npart will be an empty list. This is especially useful when you’re pro-\ncessing a sequence that you know in advance has at least N elements:\nshort_list = [1, 2]\nfirst, second, *rest = short_list\nprint(first, second, rest)\n\n\n \nItem 13: Prefer Catch-All Unpacking Over Slicing \n51\n>>>\n1 2 []\nYou can also unpack arbitrary iterators with the unpacking  syntax. \nThis isn’t worth much with a basic multiple-assignment statement. \nFor example, here I unpack the values from iterating over a range \nof length 2. This doesn’t seem useful because it would be easier \nto just assign to a static list that matches the unpacking pattern \n(e.g., [1, 2]):\nit = iter(range(1, 3))\nfirst, second = it\nprint(f'{first} and {second}')\n>>>\n1 and 2\nBut with the addition of starred expressions, the value of unpack-\ning iterators becomes clear. For example, here I have a generator \nthat yields the rows of a CSV file containing all car orders from the \n dealership this week:\ndef generate_csv():\n    yield ('Date', 'Make' , 'Model', 'Year', 'Price')\n    ...\nProcessing the results of this generator using indexes and slices is \nfine, but it requires multiple lines and is visually noisy:\nall_csv_rows = list(generate_csv())\nheader = all_csv_rows[0]\nrows = all_csv_rows[1:]\nprint('CSV Header:', header)\nprint('Row count: ', len(rows))\n>>>\nCSV Header: ('Date', 'Make', 'Model', 'Year', 'Price')\nRow count:  200\nUnpacking with a starred expression makes it easy to process the first \nrow—the header—separately from the rest of the iterator’s  contents. \nThis is much clearer:\nit = generate_csv()\nheader, *rows = it\nprint('CSV Header:', header)\nprint('Row count: ', len(rows))\n>>>\nCSV Header: ('Date', 'Make', 'Model', 'Year', 'Price')\nRow count:  200\n\n\n52 \nChapter 2 Lists and Dictionaries\nKeep in mind, however, that because a starred expression is always \nturned into a list, unpacking an iterator also risks the potential of \nusing up all of the memory on your computer and causing your pro-\ngram to crash. So you should only use catch-all unpacking on itera-\ntors when you have good reason to believe that the result data will all \nfit in memory (see Item 31: “Be Defensive When Iterating Over Argu-\nments” for another approach).\nThings to Remember\n✦ Unpacking assignments may use a starred expression to catch all \nvalues that weren’t assigned to the other parts of the unpacking \npattern into a list.\n✦ Starred expressions may appear in any position, and they will \nalways become a list containing the zero or more values they \nreceive.\n✦ When dividing a list into non-overlapping pieces, catch-all unpack-\ning is much less error prone than slicing and indexing.\nItem 14:  Sort by Complex Criteria Using the key \nParameter\nThe list built-in type provides a sort method for ordering the items \nin a list instance based on a variety of criteria. By default, sort will \norder a list’s contents by the natural ascending order of the items. \nFor example, here I sort a list of integers from smallest to largest:\nnumbers = [93, 86, 11, 68, 70]\nnumbers.sort()\nprint(numbers)\n>>>\n[11, 68, 70, 86, 93]\nThe sort method works for nearly all built-in types (strings, floats, \netc.) that have a natural ordering to them. What does sort do with \nobjects? For example, here I define a class—including a __repr__ \nmethod so instances are printable; see Item 75: “Use repr Strings for \nDebugging Output”—to represent various tools you may need to use \non a construction site:\nclass Tool:\n    def __init__(self, name, weight):\n        self.name = name\n        self.weight = weight\n    def __repr__(self):\n        return f'Tool({self.name!r}, {self.weight})'\n\n\n \nItem 14: Sort by Complex Criteria Using the key Parameter \n53\ntools = [\n    Tool('level', 3.5),\n    Tool('hammer', 1.25),\n    Tool('screwdriver', 0.5),\n    Tool('chisel', 0.25),\n]\nSorting objects of this type doesn’t work because the sort method \ntries to call comparison special methods that aren’t defined by the \nclass:\ntools.sort()\n>>>\nTraceback ...\nTypeError: '<' not supported between instances of 'Tool' and \n'Tool'\nIf your class should have a natural ordering like integers do, then you \ncan define the necessary special methods (see Item 73: “Know How \nto Use heapq for Priority Queues” for an example) to make sort work \nwithout extra parameters. But the more common case is that your \nobjects may need to support multiple orderings, in which case defin-\ning a natural ordering really doesn’t make sense.\nOften there’s an attribute on the object that you’d like to use for sort-\ning. To support this use case, the sort method accepts a key param-\neter that’s expected to be a function. The key function is passed a \nsingle argument, which is an item from the list that is being sorted. \nThe return value of the key function should be a comparable value \n(i.e., with a natural ordering) to use in place of an item for sorting \npurposes.\nHere, I use the lambda keyword to define a function for the key param-\neter that enables me to sort the list of Tool objects alphabetically by \ntheir name:\nprint('Unsorted:', repr(tools))\ntools.sort(key=lambda x: x.name)\nprint('\\nSorted:  ', tools)\n>>>\nUnsorted: [Tool('level',       3.5),\n           Tool('hammer',      1.25),\n           Tool('screwdriver', 0.5),\n           Tool('chisel',      0.25)]\n\n\n54 \nChapter 2 Lists and Dictionaries\nSorted:   [Tool('chisel',      0.25),\n           Tool('hammer',      1.25),\n           Tool('level',       3.5),\n           Tool('screwdriver', 0.5)]\nI can just as easily define another lambda function to sort by weight \nand pass it as the key parameter to the sort method:\ntools.sort(key=lambda x: x.weight)\nprint('By weight:', tools)\n>>>\nBy weight: [Tool('chisel',      0.25),\n            Tool('screwdriver', 0.5),\n            Tool('hammer',      1.25),\n            Tool('level',       3.5)]\nWithin the lambda function passed as the key parameter you can \naccess attributes of items as I’ve done here, index into items (for \nsequences, tuples, and dictionaries), or use any other valid expression.\nFor basic types like strings, you may even want to use the key func-\ntion to do transformations on the values before sorting. For example, \nhere I apply the lower method to each item in a list of place names to \nensure that they’re in alphabetical order, ignoring any capitalization \n(since in the natural lexical ordering of strings, capital letters come \nbefore lowercase letters):\nplaces = ['home', 'work', 'New York', 'Paris']\nplaces.sort()\nprint('Case sensitive:  ', places)\nplaces.sort(key=lambda x: x.lower())\nprint('Case insensitive:', places)\n>>>\nCase sensitive:   ['New York', 'Paris',    'home',  'work']\nCase insensitive: ['home',     'New York', 'Paris', 'work']\nSometimes you may need to use multiple criteria for sorting. For \nexample, say that I have a list of power tools and I want to sort them \nfirst by weight and then by name. How can I accomplish this?\npower_tools = [\n    Tool('drill', 4),\n    Tool('circular saw', 5),\n    Tool('jackhammer', 40),\n    Tool('sander', 4),\n]\n\n\n \nItem 14: Sort by Complex Criteria Using the key Parameter \n55\nThe simplest solution in Python is to use the tuple type. Tuples are \nimmutable sequences of arbitrary Python values. Tuples are compara-\nble by default and have a natural ordering, meaning that they imple-\nment all of the special methods, such as __lt__, that are required by \nthe sort method. Tuples implement these special method comparators \nby iterating over each position in the tuple and comparing the cor-\nresponding values one index at a time. Here, I show how this works \nwhen one tool is heavier than another:\nsaw = (5, 'circular saw')\njackhammer = (40, 'jackhammer')\nassert not (jackhammer < saw)  # Matches expectations\nIf the first position in the tuples being compared are equal—weight \nin this case—then the tuple comparison will move on to the second \nposition, and so on:\ndrill = (4, 'drill')\nsander = (4, 'sander')\nassert drill[0] == sander[0]  # Same weight\nassert drill[1] < sander[1]   # Alphabetically less\nassert drill < sander         # Thus, drill comes first\nYou can take advantage of this tuple comparison behavior in order \nto sort the list of power tools first by weight and then by name. Here, \nI define a key function that returns a tuple containing the two attri-\nbutes that I want to sort on in order of priority:\npower_tools.sort(key=lambda x: (x.weight, x.name))\nprint(power_tools)\n>>>\n[Tool('drill',        4),\n Tool('sander',       4),\n Tool('circular saw', 5),\n Tool('jackhammer',   40)]\nOne limitation of having the key function return a tuple is that the \ndirection of sorting for all criteria must be the same (either all in \nascending order, or all in descending order). If I provide the reverse \nparameter to the sort method, it will affect both criteria in the tuple \nthe same way (note how 'sander' now comes before 'drill' instead of \nafter):\npower_tools.sort(key=lambda x: (x.weight, x.name),\n                 reverse=True)  # Makes all criteria descending\nprint(power_tools)\n\n\n56 \nChapter 2 Lists and Dictionaries\n>>>\n[Tool('jackhammer',   40),\n Tool('circular saw', 5),\n Tool('sander',       4),\n Tool('drill',        4)]\nFor numerical values it’s possible to mix sorting directions by using \nthe unary minus operator in the key function. This negates one of \nthe values in the returned tuple, effectively reversing its sort order \nwhile leaving the others intact. Here, I use this approach to sort by \nweight descending, and then by name ascending (note how 'sander' \nnow comes after 'drill' instead of before):\npower_tools.sort(key=lambda x: (-x.weight, x.name))\nprint(power_tools)\n>>>\n[Tool('jackhammer',   40),\n Tool('circular saw', 5),\n Tool('drill',        4),\n Tool('sander',       4)]\nUnfortunately, unary negation isn’t possible for all types. Here, I try \nto achieve the same outcome by using the reverse argument to sort \nby weight descending and then negating name to put it in ascending \norder:\npower_tools.sort(key=lambda x: (x.weight, -x.name),\n                 reverse=True)\n>>>\nTraceback ...\nTypeError: bad operand type for unary -: 'str'\nFor situations like this, Python provides a stable sorting algorithm. \nThe sort method of the list type will preserve the order of the input \nlist when the key function returns values that are equal to each \nother. This means that I can call sort multiple times on the same \nlist to combine different criteria together. Here, I produce the same \nsort ordering of weight descending and name ascending as I did above \nbut by using two separate calls to sort:\npower_tools.sort(key=lambda x: x.name)   # Name ascending\npower_tools.sort(key=lambda x: x.weight, # Weight descending\n                 reverse=True)\nprint(power_tools)\n\n\n \nItem 14: Sort by Complex Criteria Using the key Parameter \n57\n>>>\n[Tool('jackhammer',   40),\n Tool('circular saw', 5),\n Tool('drill',        4),\n Tool('sander',       4)]\nTo understand why this works, note how the first call to sort puts the \nnames in alphabetical order:\npower_tools.sort(key=lambda x: x.name)\nprint(power_tools)\n>>>\n[Tool('circular saw', 5),\n Tool('drill',        4),\n Tool('jackhammer',   40),\n Tool('sander',       4)]\nWhen the second sort call by weight descending is made, it sees that \nboth 'sander' and 'drill' have a weight of 4. This causes the sort \nmethod to put both items into the final result list in the same order \nthat they appeared in the original list, thus preserving their relative \nordering by name ascending:\npower_tools.sort(key=lambda x: x.weight,\n                 reverse=True)\nprint(power_tools)\n>>>\n[Tool('jackhammer',   40),\n Tool('circular saw', 5),\n Tool('drill',        4),\n Tool('sander',       4)]\nThis same approach can be used to combine as many different types \nof sorting criteria as you’d like in any direction, respectively. You just \nneed to make sure that you execute the sorts in the opposite sequence \nof what you want the final list to contain. In this example, I wanted \nthe sort order to be by weight descending and then by name ascend-\ning, so I had to do the name sort first, followed by the weight sort.\nThat said, the approach of having the key function return a tuple, \nand using unary negation to mix sort orders, is simpler to read and \nrequires less code. I recommend only using multiple calls to sort if \nit’s absolutely necessary.\n\n\n58 \nChapter 2 Lists and Dictionaries\nThings to Remember\n✦ The sort method of the list type can be used to rearrange a list’s \ncontents by the natural ordering of built-in types like strings, inte-\ngers, tuples, and so on.\n✦ The sort method doesn’t work for objects unless they define a natu-\nral ordering using special methods, which is uncommon.\n✦ The key parameter of the sort method can be used to supply a \nhelper function that returns the value to use for sorting in place of \neach item from the list.\n✦ Returning a tuple from the key function allows you to combine mul-\ntiple sorting criteria together. The unary minus operator can be \nused to reverse individual sort orders for types that allow it.\n✦ For types that can’t be negated, you can combine many sorting cri-\nteria together by calling the sort method multiple times using dif-\nferent key functions and reverse values, in the order of lowest rank \nsort call to highest rank sort call.\nItem 15:  Be Cautious When Relying on dict Insertion \nOrdering\nIn Python 3.5 and before, iterating over a dict would return keys in \narbitrary order. The order of iteration would not match the order in \nwhich the items were inserted. For example, here I create a dictionary \nmapping animal names to their corresponding baby names and then \nprint it out (see Item 75: “Use repr Strings for Debugging Output” for \nhow this works):\n# Python 3.5\nbaby_names = {\n    'cat': 'kitten',\n    'dog': 'puppy',\n}\nprint(baby_names)\n>>>\n{'dog': 'puppy', 'cat': 'kitten'}\nWhen I created the dictionary the keys were in the order 'cat', 'dog', \nbut when I printed it the keys were in the reverse order 'dog', 'cat'. \nThis behavior is surprising, makes it harder to reproduce test cases, \nincreases the difficulty of debugging, and is especially confusing to \nnewcomers to Python.\n\n\n \nItem 15: Be Cautious When Relying on dict Insertion Ordering \n59\nThis happened because the dictionary type previously implemented \nits hash table algorithm with a combination of the hash built-in func-\ntion and a random seed that was assigned when the Python inter-\npreter started. Together, these behaviors caused dictionary orderings \nto not match insertion order and to randomly shuffle between pro-\ngram executions.\nStarting with Python 3.6, and officially part of the Python specifica-\ntion in version 3.7, dictionaries will preserve insertion order. Now, this \ncode will always print the dictionary in the same way it was originally \ncreated by the programmer:\nbaby_names = {\n    'cat': 'kitten',\n    'dog': 'puppy',\n}\nprint(baby_names)\n>>>\n{'cat': 'kitten', 'dog': 'puppy'}\nWith Python 3.5 and earlier, all methods provided by dict that relied \non iteration order, including keys, values, items, and popitem, would \nsimilarly demonstrate this random-looking behavior:\n# Python 3.5\nprint(list(baby_names.keys()))\nprint(list(baby_names.values()))\nprint(list(baby_names.items()))\nprint(baby_names.popitem())  # Randomly chooses an item\n>>>\n['dog', 'cat']\n['puppy', 'kitten']\n[('dog', 'puppy'), ('cat', 'kitten')]\n('dog', 'puppy')\nThese methods now provide consistent insertion ordering that you \ncan rely on when you write your programs:\nprint(list(baby_names.keys()))\nprint(list(baby_names.values()))\nprint(list(baby_names.items()))\nprint(baby_names.popitem())  # Last item inserted\n>>>\n['cat', 'dog']\n['kitten', 'puppy']\n[('cat', 'kitten'), ('dog', 'puppy')]\n('dog', 'puppy')\n\n\n60 \nChapter 2 Lists and Dictionaries\nThere are many repercussions of this change on other Python features \nthat are dependent on the dict type and its specific implementation.\nKeyword arguments to functions—including the **kwargs catch-all \nparameter; see Item 23: “Provide Optional Behavior with Keyword \nArguments”—previously would come through in seemingly random \norder, which can make it harder to debug function calls:\n# Python 3.5\ndef my_func(**kwargs):\n    for key, value in kwargs.items():\n        print('%s = %s' % (key, value))\nmy_func(goose='gosling', kangaroo='joey')\n>>>\nkangaroo = joey\ngoose = gosling\nNow, the order of keyword arguments is always preserved to match \nhow the programmer originally called the function:\ndef my_func(**kwargs):\n    for key, value in kwargs.items():\n        print(f'{key} = {value}')\nmy_func(goose='gosling', kangaroo='joey')\n>>>\ngoose = gosling\nkangaroo = joey\nClasses also use the dict type for their instance dictionaries. In pre-\nvious versions of Python, object fields would show the randomizing \nbehavior:\n# Python 3.5\nclass MyClass:\n    def __init__(self):\n        self.alligator = 'hatchling'\n        self.elephant = 'calf'\na = MyClass()\nfor key, value in a.__dict__.items():\n    print('%s = %s' % (key, value))\n>>>\nelephant = calf\nalligator = hatchling\n",
      "page_number": 70,
      "chapter_number": 9,
      "summary": "This chapter covers segment 9 (pages 70-82). Key topics include sorted, printed, and orders. UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb8 in \nposition 0: invalid start byte\nAre negative strides besides -1 useful.",
      "keywords": [
        "Tool",
        "Sort",
        "list",
        "Item",
        "key",
        "sort method",
        "Unpacking",
        "Python",
        "order",
        "key Parameter",
        "key function",
        "method",
        "weight",
        "descending",
        "power"
      ],
      "concepts": [
        "sorted",
        "printed",
        "orders",
        "ordering",
        "item",
        "tools",
        "multiple",
        "lists",
        "value",
        "unpacking"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 8,
          "title": "Segment 8 (pages 147-169)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 5,
          "title": "Segment 5 (pages 84-106)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 83-98)",
      "start_page": 83,
      "end_page": 98,
      "detection_method": "topic_boundary",
      "content": " \nItem 15: Be Cautious When Relying on dict Insertion Ordering \n61\nAgain, you can now assume that the order of assignment for these \ninstance fields will be reflected in __dict__:\nclass MyClass:\n    def __init__(self):\n        self.alligator = 'hatchling'\n        self.elephant = 'calf'\na = MyClass()\nfor key, value in a.__dict__.items():\n    print(f'{key} = {value}')\n>>>\nalligator = hatchling\nelephant = calf\nThe way that dictionaries preserve insertion ordering is now part of \nthe Python language specification. For the language features above, \nyou can rely on this behavior and even make it part of the APIs you \ndesign for your classes and functions.\nNote\nFor a long time the collections built-in module has had an OrderedDict \nclass that preserves insertion ordering. Although this class’s behavior is similar \nto that of the standard dict type (since Python 3.7), the performance charac-\nteristics of OrderedDict are quite different. If you need to handle a high rate \nof key insertions and popitem calls (e.g., to implement a least-recently-used \ncache), OrderedDict may be a better fit than the standard Python dict type \n(see Item 70: “Profile Before Optimizing” on how to make sure you need this).\nHowever, you shouldn’t always assume that insertion ordering behav-\nior will be present when you’re handling dictionaries. Python makes \nit easy for programmers to define their own custom container types \nthat emulate the standard protocols matching list, dict, and other \ntypes (see Item 43: “Inherit from collections.abc for Custom Con-\ntainer Types”). Python is not statically typed, so most code relies on \nduck typing—where an object’s behavior is its de facto type—instead \nof rigid class hierarchies. This can result in surprising gotchas.\nFor example, say that I’m writing a program to show the results of a \ncontest for the cutest baby animal. Here, I start with a dictionary con-\ntaining the total vote count for each one:\nvotes = {\n    'otter': 1281,\n    'polar bear': 587,\n    'fox': 863,\n}\n\n\n62 \nChapter 2 Lists and Dictionaries\nI define a function to process this voting data and save the rank of \neach animal name into a provided empty dictionary. In this case, the \ndictionary could be the data model that powers a UI element:\ndef populate_ranks(votes, ranks):\n    names = list(votes.keys())\n    names.sort(key=votes.get, reverse=True)\n    for i, name in enumerate(names, 1):\n        ranks[name] = i\nI also need a function that will tell me which animal won the contest. \nThis function works by assuming that populate_ranks will assign the \ncontents of the ranks dictionary in ascending order, meaning that the \nfirst key must be the winner:\ndef get_winner(ranks):\n    return next(iter(ranks))\nHere, I can confirm that these functions work as designed and deliver \nthe result that I expected:\nranks = {}\npopulate_ranks(votes, ranks)\nprint(ranks)\nwinner = get_winner(ranks)\nprint(winner)\n>>>\n{'otter': 1, 'fox': 2, 'polar bear': 3}\notter\nNow, imagine that the requirements of this program have changed. \nThe UI element that shows the results should be in alphabet-\nical order instead of rank order. To accomplish this, I can use the \ncollections.abc built-in module to define a new dictionary-like class \nthat iterates its contents in alphabetical order:\nfrom collections.abc import MutableMapping\nclass SortedDict(MutableMapping):\n    def __init__(self):\n        self.data = {}\n    def __getitem__(self, key):\n        return self.data[key]\n    def __setitem__(self, key, value):\n        self.data[key] = value\n\n\n \nItem 15: Be Cautious When Relying on dict Insertion Ordering \n63\n    def __delitem__(self, key):\n        del self.data[key]\n    def __iter__(self):\n        keys = list(self.data.keys())\n        keys.sort()\n        for key in keys:\n            yield key\n    def __len__(self):\n        return len(self.data)\nI can use a SortedDict instance in place of a standard dict with the \nfunctions from before and no errors will be raised since this class \nconforms to the protocol of a standard dictionary. However, the result \nis incorrect:\nsorted_ranks = SortedDict()\npopulate_ranks(votes, sorted_ranks)\nprint(sorted_ranks.data)\nwinner = get_winner(sorted_ranks)\nprint(winner)\n>>>\n{'otter': 1, 'fox': 2, 'polar bear': 3}\nfox\nThe problem here is that the implementation of get_winner assumes \nthat the dictionary’s iteration is in insertion order to match \npopulate_ranks. This code is using SortedDict instead of dict, so that \nassumption is no longer true. Thus, the value returned for the winner \nis 'fox', which is alphabetically first.\nThere are three ways to mitigate this problem. First, I can reimple-\nment the get_winner function to no longer assume that the ranks dic-\ntionary has a specific iteration order. This is the most conservative \nand robust solution:\ndef get_winner(ranks):\n    for name, rank in ranks.items():\n        if rank == 1:\n            return name\nwinner = get_winner(sorted_ranks)\nprint(winner)\n>>>\notter\n\n\n64 \nChapter 2 Lists and Dictionaries\nThe second approach is to add an explicit check to the top of the func-\ntion to ensure that the type of ranks matches my expectations, and \nto raise an exception if not. This solution likely has better runtime \nperformance than the more conservative approach:\ndef get_winner(ranks):\n    if not isinstance(ranks, dict):\n        raise TypeError('must provide a dict instance')\n    return next(iter(ranks))\nget_winner(sorted_ranks)\n>>>\nTraceback ...\nTypeError: must provide a dict instance\nThe third alternative is to use type annotations to enforce that the \nvalue passed to get_winner is a dict instance and not a MutableMapping \nwith dictionary-like behavior (see Item 90: “Consider Static Analysis \nvia typing to Obviate Bugs”). Here, I run the mypy tool in strict mode \non an annotated version of the code above:\nfrom typing import Dict, MutableMapping\ndef populate_ranks(votes: Dict[str, int],\n                   ranks: Dict[str, int]) -> None:\n    names = list(votes.keys())\n    names.sort(key=votes.get, reverse=True)\n    for i, name in enumerate(names, 1):\n        ranks[name] = i\ndef get_winner(ranks: Dict[str, int]) -> str:\n    return next(iter(ranks))\nclass SortedDict(MutableMapping[str, int]):\n    ...\nvotes = {\n    'otter': 1281,\n    'polar bear': 587,\n    'fox': 863,\n}\nsorted_ranks = SortedDict()\npopulate_ranks(votes, sorted_ranks)\nprint(sorted_ranks.data)\nwinner = get_winner(sorted_ranks)\nprint(winner)\n\n\n Item 16: Prefer get Over in and KeyError to Handle Missing Dictionary Keys 65\n$ python3 -m mypy --strict example.py\n.../example.py:48: error: Argument 2 to \"populate_ranks\" has \n¯incompatible type \"SortedDict\"; expected \"Dict[str, int]\"\n.../example.py:50: error: Argument 1 to \"get_winner\" has \n¯incompatible type \"SortedDict\"; expected \"Dict[str, int]\"\nThis correctly detects the mismatch between the dict and \nMutableMapping types and flags the incorrect usage as an error. This \nsolution provides the best mix of static type safety and runtime \nperformance.\nThings to Remember\n✦ Since Python 3.7, you can rely on the fact that iterating a dict \ninstance’s contents will occur in the same order in which the keys \nwere initially added.\n✦ Python makes it easy to define objects that act like dictionaries but \nthat aren’t dict instances. For these types, you can’t assume that \ninsertion ordering will be preserved.\n✦ There are three ways to be careful about dictionary-like classes: \nWrite code that doesn’t rely on insertion ordering, explicitly check \nfor the dict type at runtime, or require dict values using type anno-\ntations and static analysis.\nItem 16:  Prefer get Over in and KeyError to Handle \nMissing Dictionary Keys\nThe three fundamental operations for interacting with dictionar-\nies are accessing, assigning, and deleting keys and their associated \nvalues. The contents of dictionaries are dynamic, and thus it’s entirely \npossible—even likely—that when you try to access or delete a key, \nit won’t already be present.\nFor example, say that I’m trying to determine people’s favorite type of \nbread to devise the menu for a sandwich shop. Here, I define a dictio-\nnary of counters with the current votes for each style:\ncounters = {\n    'pumpernickel': 2,\n    'sourdough': 1,\n}\nTo increment the counter for a new vote, I need to see if the key exists, \ninsert the key with a default counter value of zero if it’s missing, and \nthen increment the counter’s value. This requires accessing the key \ntwo times and assigning it once. Here, I accomplish this task using \n\n\n66 \nChapter 2 Lists and Dictionaries\nan if statement with an in expression that returns True when the key \nis present:\nkey = 'wheat'\nif key in counters:\n    count = counters[key]\nelse:\n    count = 0\ncounters[key] = count + 1\nAnother way to accomplish the same behavior is by relying on how \ndictionaries raise a KeyError exception when you try to get the value \nfor a key that doesn’t exist. This approach is more efficient because it \nrequires only one access and one assignment:\ntry:\n    count = counters[key]\nexcept KeyError:\n    count = 0\ncounters[key] = count + 1\nThis flow of fetching a key that exists or returning a default value \nis so common that the dict built-in type provides the get method to \naccomplish this task. The second parameter to get is the default value \nto return in the case that the key—the first parameter—isn’t present. \nThis also requires only one access and one assignment, but it’s much \nshorter than the KeyError example:\ncount = counters.get(key, 0)\ncounters[key] = count + 1\nIt’s possible to shorten the in expression and KeyError approaches in \nvarious ways, but all of these alternatives suffer from requiring code \nduplication for the assignments, which makes them less readable and \nworth avoiding:\nif key not in counters:\n    counters[key] = 0\ncounters[key] += 1\nif key in counters:\n    counters[key] += 1\nelse:\n    counters[key] = 1\n\n\n Item 16: Prefer get Over in and KeyError to Handle Missing Dictionary Keys 67\ntry:\n    counters[key] += 1\nexcept KeyError:\n    counters[key] = 1\nThus, for a dictionary with simple types, using the get method is the \nshortest and clearest option.\nNote\nIf you’re maintaining dictionaries of counters like this, it’s worth considering \nthe Counter class from the collections built-in module, which provides \nmost of the facilities you are likely to need.\nWhat if the values of the dictionary are a more complex type, like a \nlist? For example, say that instead of only counting votes, I also want \nto know who voted for each type of bread. Here, I do this by associat-\ning a list of names with each key:\nvotes = {\n    'baguette': ['Bob', 'Alice'],\n    'ciabatta': ['Coco', 'Deb'],\n}\nkey = 'brioche'\nwho = 'Elmer'\nif key in votes:\n    names = votes[key]\nelse:\n    votes[key] = names = []\nnames.append(who)\nprint(votes)\n>>>\n{'baguette': ['Bob', 'Alice'],\n 'ciabatta': ['Coco', 'Deb'],\n 'brioche': ['Elmer']}\nRelying on the in expression requires two accesses if the key is pres-\nent, or one access and one assignment if the key is missing. This \nexample is different from the counters example above because the \nvalue for each key can be assigned blindly to the default value of an \nempty list if the key doesn’t already exist. The triple assignment \nstatement (votes[key] = names = []) populates the key in one line \ninstead of two. Once the default value has been inserted into the dic-\ntionary, I don’t need to assign it again because the list is modified by \nreference in the later call to append.\n\n\n68 \nChapter 2 Lists and Dictionaries\nIt’s also possible to rely on the KeyError exception being raised when \nthe dictionary value is a list. This approach requires one key access \nif the key is present, or one key access and one assignment if it’s \nmissing, which makes it more efficient than the in condition:\ntry:\n    names = votes[key]\nexcept KeyError:\n    votes[key] = names = []\nnames.append(who)\nSimilarly, you can use the get method to fetch a list value when the \nkey is present, or do one fetch and one assignment if the key isn’t \npresent:\nnames = votes.get(key)\nif names is None:\n    votes[key] = names = []\nnames.append(who)\nThe approach that involves using get to fetch list values can \n further be shortened by one line if you use an assignment expres-\nsion ( introduced in Python 3.8; see Item 10: “Prevent Repetition \nwith Assignment Expressions”) in the if statement, which improves \nreadability:\nif (names := votes.get(key)) is None:\n    votes[key] = names = []\nnames.append(who)\nThe dict type also provides the setdefault method to help shorten \nthis pattern even further. setdefault tries to fetch the value of a key \nin the dictionary. If the key isn’t present, the method assigns that key \nto the default value provided. And then the method returns the value \nfor that key: either the originally present value or the newly inserted \ndefault value. Here, I use setdefault to implement the same logic as in \nthe get example above:\nnames = votes.setdefault(key, [])\nnames.append(who)\nThis works as expected, and it is shorter than using get with an \nassignment expression. However, the readability of this approach \nisn’t ideal. The method name setdefault doesn’t make its purpose \n\n\n Item 16: Prefer get Over in and KeyError to Handle Missing Dictionary Keys 69\nimmediately obvious. Why is it set when what it’s doing is getting \na value? Why not call it get_or_set? I’m arguing about the color of \nthe bike shed here, but the point is that if you were a new reader of \nthe code and not completely familiar with Python, you might have \ntrouble understanding what this code is trying to accomplish because \nsetdefault isn’t self-explanatory.\nThere’s also one important gotcha: The default value passed to \nsetdefault is assigned directly into the dictionary when the key is \nmissing instead of being copied. Here, I demonstrate the effect of this \nwhen the value is a list:\ndata = {}\nkey = 'foo'\nvalue = []\ndata.setdefault(key, value)\nprint('Before:', data)\nvalue.append('hello')\nprint('After: ', data)\n>>>\nBefore: {'foo': []}\nAfter:  {'foo': ['hello']}\nThis means that I need to make sure that I’m always construct-\ning a new default value for each key I access with setdefault. This \nleads to a significant performance overhead in this example because \nI have to allocate a list instance for each call. If I reuse an object \nfor the default value—which I might try to do to increase efficiency \nor  readability—I might introduce strange behavior and bugs (see \nItem 24: “Use None and Docstrings to Specify Dynamic Default \n Arguments” for another example of this problem).\nGoing back to the earlier example that used counters for dictionary \nvalues instead of lists of who voted: Why not also use the setdefault \nmethod in that case? Here, I reimplement the same example using \nthis approach:\ncount = counters.setdefault(key, 0)\ncounters[key] = count + 1\nThe problem here is that the call to setdefault is superfluous. You \nalways need to assign the key in the dictionary to a new value \nafter you increment the counter, so the extra assignment done by \nsetdefault is unnecessary. The earlier approach of using get for \ncounter updates requires only one access and one assignment, \nwhereas using setdefault requires one access and two assignments.\n\n\n70 \nChapter 2 Lists and Dictionaries\nThere are only a few circumstances in which using setdefault is the \nshortest way to handle missing dictionary keys, such as when the \ndefault values are cheap to construct, mutable, and there’s no poten-\ntial for raising exceptions (e.g., list instances). In these very spe-\ncific cases, it may seem worth accepting the confusing method name \nsetdefault instead of having to write more characters and lines to \nuse get. However, often what you really should do in these situations \nis to use defaultdict instead (see Item 17: “Prefer defaultdict Over \nsetdefault to Handle Missing Items in Internal State”).\nThings to Remember\n✦ There are four common ways to detect and handle missing keys \nin dictionaries: using in expressions, KeyError exceptions, the get \nmethod, and the setdefault method.\n✦ The get method is best for dictionaries that contain basic types \nlike counters, and it is preferable along with assignment expres-\nsions when creating dictionary values has a high cost or may raise \nexceptions.\n✦ When the setdefault method of dict seems like the best fit for your \nproblem, you should consider using defaultdict instead.\nItem 17:  Prefer defaultdict Over setdefault to \nHandle Missing Items in Internal State\nWhen working with a dictionary that you didn’t create, there are a \nvariety of ways to handle missing keys (see Item 16: “Prefer get Over \nin and KeyError to Handle Missing Dictionary Keys”). Although using \nthe get method is a better approach than using in expressions and \nKeyError exceptions, for some use cases setdefault appears to be the \nshortest option.\nFor example, say that I want to keep track of the cities I’ve visited in \ncountries around the world. Here, I do this by using a dictionary that \nmaps country names to a set instance containing corresponding city \nnames:\nvisits = {\n    'Mexico': {'Tulum', 'Puerto Vallarta'},\n    'Japan': {'Hakone'},\n}\nI can use the setdefault method to add new cities to the sets, whether \nthe country name is already present in the dictionary or not. This \napproach is much shorter than achieving the same behavior with the \n\n\n \nItem 17: Prefer defaultdict Over setdefault \n71\nget method and an assignment expression (which is available as of \nPython 3.8):\nvisits.setdefault('France', set()).add('Arles')  # Short\nif (japan := visits.get('Japan')) is None:       # Long\n    visits['Japan'] = japan = set()\njapan.add('Kyoto')\nprint(visits)\n>>>\n{'Mexico': {'Tulum', 'Puerto Vallarta'},\n 'Japan': {'Kyoto', 'Hakone'},\n 'France': {'Arles'}}\nWhat about the situation when you do control creation of the dictio-\nnary being accessed? This is generally the case when you’re using a \ndictionary instance to keep track of the internal state of a class, for \nexample. Here, I wrap the example above in a class with helper meth-\nods to access the dynamic inner state stored in a dictionary:\nclass Visits:\n    def __init__(self):\n        self.data = {}\n    def add(self, country, city):\n        city_set = self.data.setdefault(country, set())\n        city_set.add(city)\nThis new class hides the complexity of calling setdefault correctly, \nand it provides a nicer interface for the programmer:\nvisits = Visits()\nvisits.add('Russia', 'Yekaterinburg')\nvisits.add('Tanzania', 'Zanzibar')\nprint(visits.data)\n>>>\n{'Russia': {'Yekaterinburg'}, 'Tanzania': {'Zanzibar'}}\nHowever, the implementation of the Visits.add method still isn’t ideal. \nThe setdefault method is still confusingly named, which makes it \nmore difficult for a new reader of the code to immediately understand \nwhat’s happening. And the implementation isn’t efficient because it \nconstructs a new set instance on every call, regardless of whether the \ngiven country was already present in the data dictionary.\n\n\n72 \nChapter 2 Lists and Dictionaries\nLuckily, the defaultdict class from the collections built-in module \nsimplifies this common use case by automatically storing a default \nvalue when a key doesn’t exist. All you have to do is provide a  function \nthat will return the default value to use each time a key is missing \n(an example of Item 38: “Accept Functions Instead of Classes for Sim-\nple Interfaces”). Here, I rewrite the Visits class to use defaultdict:\nfrom collections import defaultdict\nclass Visits:\n    def __init__(self):\n        self.data = defaultdict(set)\n    def add(self, country, city):\n        self.data[country].add(city)\nvisits = Visits()\nvisits.add('England', 'Bath')\nvisits.add('England', 'London')\nprint(visits.data)\n>>>\ndefaultdict(<class 'set'>, {'England': {'London', 'Bath'}})\nNow, the implementation of add is short and simple. The code can \nassume that accessing any key in the data dictionary will always \nresult in an existing set instance. No superfluous set instances will \nbe allocated, which could be costly if the add method is called a large \nnumber of times.\nUsing defaultdict is much better than using setdefault for this type \nof situation (see Item 37: “Compose Classes Instead of Nesting Many \nLevels of Built-in Types” for another example). There are still cases in \nwhich defaultdict will fall short of solving your problems, but there \nare even more tools available in Python to work around those limita-\ntions (see Item 18: “Know How to Construct Key-Dependent Default \nValues with __missing__,” Item 43: “Inherit from collections.abc for \nCustom Container Types,” and the collections.Counter built-in class).\nThings to Remember\n✦ If you’re creating a dictionary to manage an arbitrary set of poten-\ntial keys, then you should prefer using a defaultdict instance from \nthe collections built-in module if it suits your problem. \n✦ If a dictionary of arbitrary keys is passed to you, and you don’t con-\ntrol its creation, then you should prefer the get method to access its \nitems. However, it’s worth considering using the setdefault method \nfor the few situations in which it leads to shorter code.\n\n\n \nItem 18: Know How to Construct Key-Dependent Default Values \n73\nItem 18:  Know How to Construct Key-Dependent \nDefault Values with __missing__\nThe built-in dict type’s setdefault method results in shorter code \nwhen handling missing keys in some specific circumstances (see Item \n16: “Prefer get Over in and KeyError to Handle Missing Dictionary \nKeys” for examples). For many of those situations, the better tool for \nthe job is the defaultdict type from the collections built-in module \n(see Item 17: “Prefer defaultdict Over setdefault to Handle Missing \nItems in Internal State” for why). However, there are times when nei-\nther setdefault nor defaultdict is the right fit.\nFor example, say that I’m writing a program to manage social network \nprofile pictures on the filesystem. I need a dictionary to map profile \npicture pathnames to open file handles so I can read and write those \nimages as needed. Here, I do this by using a normal dict instance \nand checking for the presence of keys using the get method and an \nassignment expression (introduced in Python 3.8; see Item 10: “Pre-\nvent Repetition with Assignment Expressions”):\npictures = {}\npath = 'profile_1234.png'\nif (handle := pictures.get(path)) is None:\n    try:\n        handle = open(path, 'a+b')\n    except OSError:\n        print(f'Failed to open path {path}')\n        raise\n    else:\n        pictures[path] = handle\nhandle.seek(0)\nimage_data = handle.read()\nWhen the file handle already exists in the dictionary, this code makes \nonly a single dictionary access. In the case that the file handle doesn’t \nexist, the dictionary is accessed once by get, and then it is assigned \nin the else clause of the try/except block. (This approach also \nworks with finally; see Item 65: “Take Advantage of Each Block in \ntry/except/else/finally.”) The call to the read method stands clearly \nseparate from the code that calls open and handles exceptions.\nAlthough it’s possible to use the in expression or KeyError approaches \nto implement this same logic, those options require more dictionary \naccesses and levels of nesting. Given that these other options work, \nyou might also assume that the setdefault method would work, too:\n\n\n74 \nChapter 2 Lists and Dictionaries\ntry:\n    handle = pictures.setdefault(path, open(path, 'a+b'))\nexcept OSError:\n    print(f'Failed to open path {path}')\n    raise\nelse:\n    handle.seek(0)\n    image_data = handle.read()\nThis code has many problems. The open built-in function to create \nthe file handle is always called, even when the path is already pres-\nent in the dictionary. This results in an additional file handle that \nmay conflict with existing open handles in the same program. Excep-\ntions may be raised by the open call and need to be handled, but it \nmay not be possible to differentiate them from exceptions that may \nbe raised by the setdefault call on the same line (which is possible \nfor other  dictionary-like implementations; see Item 43: “Inherit from \ncollections.abc for Custom Container Types”).\nIf you’re trying to manage internal state, another assumption you \nmight make is that a defaultdict could be used for keeping track of \nthese profile pictures. Here, I attempt to implement the same logic as \nbefore but now using a helper function and the defaultdict class:\nfrom collections import defaultdict\ndef open_picture(profile_path):\n    try:\n        return open(profile_path, 'a+b')\n    except OSError:\n        print(f'Failed to open path {profile_path}')\n        raise\npictures = defaultdict(open_picture)\nhandle = pictures[path]\nhandle.seek(0)\nimage_data = handle.read()\n>>>\nTraceback ...\nTypeError: open_picture() missing 1 required positional \nargument: 'profile_path'\nThe problem is that defaultdict expects that the function passed to \nits constructor doesn’t require any arguments. This means that the \nhelper function that defaultdict calls doesn’t know which specific key \n\n\n \nItem 18: Know How to Construct Key-Dependent Default Values \n75\nis being accessed, which eliminates my ability to call open. In this \nsituation, both setdefault and defaultdict fall short of what I need.\nFortunately, this situation is common enough that Python has \nanother built-in solution. You can subclass the dict type and imple-\nment the __missing__ special method to add custom logic for han-\ndling missing keys. Here, I do this by defining a new class that takes \nadvantage of the same open_picture helper method defined above:\nclass Pictures(dict):\n    def __missing__(self, key):\n        value = open_picture(key)\n        self[key] = value\n        return value\npictures = Pictures()\nhandle = pictures[path]\nhandle.seek(0)\nimage_data = handle.read()\nWhen the pictures[path] dictionary access finds that the path key \nisn’t present in the dictionary, the __missing__ method is called. This \nmethod must create the new default value for the key, insert it into \nthe dictionary, and return it to the caller. Subsequent accesses of \nthe same path will not call __missing__ since the corresponding item \nis already present (similar to the behavior of __getattr__; see Item \n47: “Use __getattr__, __getattribute__, and __setattr__ for Lazy \nAttributes”).\nThings to Remember\n✦ The setdefault method of dict is a bad fit when creating the default \nvalue has high computational cost or may raise exceptions.\n✦ The function passed to defaultdict must not require any argu-\nments, which makes it impossible to have the default value depend \non the key being accessed.\n✦ You can define your own dict subclass with a __missing__ method \nin order to construct default values that must know which key was \nbeing accessed.\n\n\nThis page intentionally left blank \n",
      "page_number": 83,
      "chapter_number": 10,
      "summary": "This chapter covers segment 10 (pages 83-98). Key topics include keys, types, and typed. For the language features above, \nyou can rely on this behavior and even make it part of the APIs you \ndesign for your classes and functions.",
      "keywords": [
        "key",
        "Missing Dictionary Keys",
        "Handle Missing Dictionary",
        "Handle Missing Items",
        "Item",
        "Handle Missing",
        "dictionary",
        "dict",
        "Dictionary Keys",
        "handle missing keys",
        "ranks",
        "Missing",
        "keys",
        "dict type",
        "Missing Dictionary"
      ],
      "concepts": [
        "keys",
        "types",
        "typed",
        "typing",
        "item",
        "classes",
        "values",
        "dictionaries",
        "dictionary",
        "vote"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 6,
          "title": "Segment 6 (pages 107-127)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 7,
          "title": "Segment 7 (pages 128-146)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 8,
          "title": "Lists and Dictionaries",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 66,
          "title": "Segment 66 (pages 2113-2146)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 99-114)",
      "start_page": 99,
      "end_page": 114,
      "detection_method": "topic_boundary",
      "content": "3\nFunctions\nThe first organizational tool programmers use in Python is the \n function. As in other programming languages, functions enable you \nto break large programs into smaller, simpler pieces with names to \nrepresent their intent. They improve readability and make code more \napproachable. They allow for reuse and refactoring.\nFunctions in Python have a variety of extra features that make a \nprogrammer’s life easier. Some are similar to capabilities in other \nprogramming languages, but many are unique to Python. These \nextras can make a function’s purpose more obvious. They can elimi-\nnate noise and clarify the intention of callers. They can significantly \nreduce subtle bugs that are difficult to find.\nItem 19:  Never Unpack More Than Three Variables \nWhen Functions Return Multiple Values\nOne effect of the unpacking syntax (see Item 6: “Prefer Multiple \nAssignment Unpacking Over Indexing”) is that it allows Python func-\ntions to seemingly return more than one value. For example, say \nthat I’m trying to determine various statistics for a population of \nalligators. Given a list of lengths, I need to calculate the minimum \nand  maximum lengths in the population. Here, I do this in a single \n function that appears to return two values:\ndef get_stats(numbers):\n    minimum = min(numbers)\n    maximum = max(numbers)\n    return minimum, maximum\n \nlengths = [63, 73, 72, 60, 67, 66, 71, 61, 72, 70]\n \nminimum, maximum = get_stats(lengths)  # Two return values\n \nprint(f'Min: {minimum}, Max: {maximum}')\n\n\n78 \nChapter 3 Functions\n>>>\nMin: 60, Max: 73\nThe way this works is that multiple values are returned together in a \ntwo-item tuple. The calling code then unpacks the returned tuple by \nassigning two variables. Here, I use an even simpler example to show \nhow an unpacking statement and multiple-return function work the \nsame way:\nfirst, second = 1, 2\nassert first == 1\nassert second == 2\n \ndef my_function():\n    return 1, 2\n \nfirst, second = my_function()\nassert first == 1\nassert second == 2\nMultiple return values can also be received by starred expressions for \ncatch-all unpacking (see Item 13: “Prefer Catch-All Unpacking Over \nSlicing”). For example, say I need another function that calculates \nhow big each alligator is relative to the population average. This func-\ntion returns a list of ratios, but I can receive the longest and shortest \nitems individually by using a starred expression for the middle por-\ntion of the list:\ndef get_avg_ratio(numbers):\n    average = sum(numbers) / len(numbers)\n    scaled = [x / average for x in numbers]\n    scaled.sort(reverse=True)\n    return scaled\n \nlongest, *middle, shortest = get_avg_ratio(lengths)\n \nprint(f'Longest:  {longest:>4.0%}')\nprint(f'Shortest: {shortest:>4.0%}')\n>>>\nLongest:  108%\nShortest:  89%\nNow, imagine that the program’s requirements change, and I need to \nalso determine the average length, median length, and total popula-\ntion size of the alligators. I can do this by expanding the get_stats \n\n\n \nItem 19: Never Unpack More Than Three Return Values \n79\nfunction to also calculate these statistics and return them in the \nresult tuple that is unpacked by the caller:\ndef get_stats(numbers):\n    minimum = min(numbers)\n    maximum = max(numbers)\n    count = len(numbers)\n    average = sum(numbers) / count\n \n    sorted_numbers = sorted(numbers)\n    middle = count // 2\n    if count % 2 == 0:\n        lower = sorted_numbers[middle - 1]\n        upper = sorted_numbers[middle]\n        median = (lower + upper) / 2\n    else:\n        median = sorted_numbers[middle]\n \n    return minimum, maximum, average, median, count\n \nminimum, maximum, average, median, count = get_stats(lengths)\n \nprint(f'Min: {minimum}, Max: {maximum}')\nprint(f'Average: {average}, Median: {median}, Count {count}')\n>>>\nMin: 60, Max: 73\nAverage: 67.5, Median: 68.5, Count 10\nThere are two problems with this code. First, all the return values \nare numeric, so it is all too easy to reorder them accidentally (e.g., \nswapping average and median), which can cause bugs that are hard \nto spot later. Using a large number of return values is extremely error \nprone:\n# Correct:\nminimum, maximum, average, median, count = get_stats(lengths)\n \n# Oops! Median and average swapped:\nminimum, maximum, median, average, count = get_stats(lengths)\nSecond, the line that calls the function and unpacks the values is \nlong, and it likely will need to be wrapped in one of a variety of ways \n(due to PEP8 style; see Item 2: “Follow the PEP 8 Style Guide”), which \nhurts readability:\nminimum, maximum, average, median, count = get_stats(\n    lengths)\n \n\n\n80 \nChapter 3 Functions\nminimum, maximum, average, median, count = \\\n    get_stats(lengths)\n \n(minimum, maximum, average,\n median, count) = get_stats(lengths)\n \n(minimum, maximum, average, median, count\n    ) = get_stats(lengths)\nTo avoid these problems, you should never use more than three vari-\nables when unpacking the multiple return values from a function. \nThese could be individual values from a three-tuple, two variables \nand one catch-all starred expression, or anything shorter. If you \nneed to unpack more return values than that, you’re better off defin-\ning a lightweight class or namedtuple (see Item 37: “Compose Classes \nInstead of Nesting Many Levels of Built-in Types”) and having your \nfunction return an instance of that instead.\nThings to Remember\n✦ You can have functions return multiple values by putting them in a \ntuple and having the caller take advantage of Python’s unpacking \nsyntax.\n✦ Multiple return values from a function can also be unpacked by \ncatch-all starred expressions.\n✦ Unpacking into four or more variables is error prone and should be \navoided; instead, return a small class or namedtuple instance.\nItem 20: Prefer Raising Exceptions to Returning None\nWhen writing utility functions, there’s a draw for Python program-\nmers to give special meaning to the return value of None. It seems to \nmake sense in some cases. For example, say I want a helper function \nthat divides one number by another. In the case of dividing by zero, \nreturning None seems natural because the result is undefined:\ndef careful_divide(a, b):\n    try:\n        return a / b\n    except ZeroDivisionError:\n        return None\nCode using this function can interpret the return value accordingly:\nx, y = 1, 0\nresult = careful_divide(x, y)\nif result is None:\n    print('Invalid inputs')\n\n\n \nItem 20: Prefer Raising Exceptions to Returning None \n81\nWhat happens with the careful_divide function when the numerator \nis zero? If the denominator is not zero, the function returns zero. The \nproblem is that a zero return value can cause issues when you evalu-\nate the result in a condition like an if statement. You might acciden-\ntally look for any False-equivalent value to indicate errors instead of \nonly looking for None (see Item 5: “Write Helper Functions Instead of \nComplex Expressions” for a similar situation):\nx, y = 0, 5\nresult = careful_divide(x, y)\nif not result:\n    print('Invalid inputs')  # This runs! But shouldn't\n>>>\nInvalid inputs\nThis misinterpretation of a False-equivalent return value is a common \nmistake in Python code when None has special meaning. This is why \nreturning None from a function like careful_divide is error prone. \nThere are two ways to reduce the chance of such errors.\nThe first way is to split the return value into a two-tuple (see Item 19: \n“Never Unpack More Than Three Variables When Functions Return \nMultiple Values” for background). The first part of the tuple indicates \nthat the operation was a success or failure. The second part is the \nactual result that was computed:\ndef careful_divide(a, b):\n    try:\n        return True, a / b\n    except ZeroDivisionError:\n        return False, None\nCallers of this function have to unpack the tuple. That forces them \nto consider the status part of the tuple instead of just looking at the \nresult of division:\nsuccess, result = careful_divide(x, y)\nif not success:\n    print('Invalid inputs')\nThe problem is that callers can easily ignore the first part of the tuple \n(using the underscore variable name, a Python convention for unused \nvariables). The resulting code doesn’t look wrong at first glance, but \nthis can be just as error prone as returning None:\n_, result = careful_divide(x, y)\nif not result:\n    print('Invalid inputs')\n\n\n82 \nChapter 3 Functions\nThe second, better way to reduce these errors is to never return \nNone for special cases. Instead, raise an Exception up to the caller \nand have the caller deal with it. Here, I turn a ZeroDivisionError into \na ValueError to indicate to the caller that the input values are bad \n(see Item 87: “Define a Root Exception to Insulate Callers from APIs” \non when you should use Exception subclasses):\ndef careful_divide(a, b):\n    try:\n        return a / b\n    except ZeroDivisionError as e:\n        raise ValueError('Invalid inputs')\nThe caller no longer requires a condition on the return value of the \nfunction. Instead, it can assume that the return value is always \nvalid and use the results immediately in the else block after try \n(see Item 65: “Take Advantage of Each Block in try/except/else/\nfinally” for details):\nx, y = 5, 2\ntry:\n    result = careful_divide(x, y)\nexcept ValueError:\n    print('Invalid inputs')\nelse:\n    print('Result is %.1f' % result)\n>>>\nResult is 2.5\nThis approach can be extended to code using type annotations \n(see Item 90: “Consider Static Analysis via typing to Obviate Bugs” \nfor background). You can specify that a function’s return value will \nalways be a float and thus will never be None. However, Python’s \ngradual typing purposefully doesn’t provide a way to indicate when \nexceptions are part of a function’s interface (also known as checked \nexceptions). Instead, you have to document the exception-raising \nbehavior and expect callers to rely on that in order to know which \nExceptions they should plan to catch (see Item 84: “Write Docstrings \nfor Every Function, Class, and Module”).\nPulling it all together, here’s what this function should look like when \nusing type annotations and docstrings:\n\n\n \nItem 21: Know How Closures Interact with Variable Scope \n83\ndef careful_divide(a: float, b: float) -> float:\n    \"\"\"Divides a by b.\n \n    Raises:\n        ValueError: When the inputs cannot be divided.\n    \"\"\"\n    try:\n        return a / b\n    except ZeroDivisionError as e:\n        raise ValueError('Invalid inputs')\nNow the inputs, outputs, and exceptional behavior is clear, and the \nchance of a caller doing the wrong thing is extremely low.\nThings to Remember\n✦ Functions that return None to indicate special meaning are error \nprone because None and other values (e.g., zero, the empty string) \nall evaluate to False in conditional expressions.\n✦ Raise exceptions to indicate special situations instead of returning \nNone. Expect the calling code to handle exceptions properly when \nthey’re documented.\n✦ Type annotations can be used to make it clear that a function will \nnever return the value None, even in special situations.\nItem 21:  Know How Closures Interact with \nVariable Scope\nSay that I want to sort a list of numbers but prioritize one group of \nnumbers to come first. This pattern is useful when you’re rendering a \nuser interface and want important messages or exceptional events to \nbe displayed before everything else.\nA common way to do this is to pass a helper function as the key argu-\nment to a list’s sort method (see Item 14: “Sort by Complex Criteria \nUsing the key Parameter” for details). The helper’s return value will \nbe used as the value for sorting each item in the list. The helper can \ncheck whether the given item is in the important group and can vary \nthe sorting value accordingly:\ndef sort_priority(values, group):\n    def helper(x):\n        if x in group:\n            return (0, x)\n        return (1, x)\n    values.sort(key=helper)\n\n\n84 \nChapter 3 Functions\nThis function works for simple inputs:\nnumbers = [8, 3, 1, 2, 5, 4, 7, 6]\ngroup = {2, 3, 5, 7}\nsort_priority(numbers, group)\nprint(numbers)\n>>>\n[2, 3, 5, 7, 1, 4, 6, 8]\nThere are three reasons this function operates as expected:\n \n■Python supports closures—that is, functions that refer to variables \nfrom the scope in which they were defined. This is why the helper \nfunction is able to access the group argument for sort_priority.\n \n■Functions are first-class objects in Python, which means you can \nrefer to them directly, assign them to variables, pass them as \narguments to other functions, compare them in expressions and \nif statements, and so on. This is how the sort method can accept \na closure function as the key argument.\n \n■Python has specific rules for comparing sequences (including \ntuples). It first compares items at index zero; then, if those are \nequal, it compares items at index one; if they are still equal, it \ncompares items at index two, and so on. This is why the return \nvalue from the helper closure causes the sort order to have two \ndistinct groups.\nIt’d be nice if this function returned whether higher-priority items \nwere seen at all so the user interface code can act accordingly. Add-\ning such behavior seems straightforward. There’s already a closure \nfunction for deciding which group each number is in. Why not also \nuse the closure to flip a flag when high-priority items are seen? Then, \nthe function can return the flag value after it’s been modified by the \nclosure.\nHere, I try to do that in a seemingly obvious way:\ndef sort_priority2(numbers, group):\n    found = False\n    def helper(x):\n        if x in group:\n            found = True  # Seems simple\n            return (0, x)\n        return (1, x)\n    numbers.sort(key=helper)\n    return found\n\n\n \nItem 21: Know How Closures Interact with Variable Scope \n85\nI can run the function on the same inputs as before:\nfound = sort_priority2(numbers, group)\nprint('Found:', found)\nprint(numbers)\n>>>\nFound: False\n[2, 3, 5, 7, 1, 4, 6, 8]\nThe sorted results are correct, which means items from group were \ndefinitely found in numbers. Yet the found result returned by the func-\ntion is False when it should be True. How could this happen?\nWhen you reference a variable in an expression, the Python  interpreter \ntraverses the scope to resolve the reference in this order:\n 1. The current function’s scope.\n 2. Any enclosing scopes (such as other containing functions).\n 3. The scope of the module that contains the code (also called the \nglobal scope).\n 4. The built-in scope (that contains functions like len and str).\nIf none of these places has defined a variable with the referenced \nname, then a NameError exception is raised:\nfoo = does_not_exist * 5\n>>>\nTraceback ...\nNameError: name 'does_not_exist' is not defined\nAssigning a value to a variable works differently. If the variable is \nalready defined in the current scope, it will just take on the new \nvalue. If the variable doesn’t exist in the current scope, Python treats \nthe assignment as a variable definition. Critically, the scope of the \nnewly defined variable is the function that contains the assignment.\nThis assignment behavior explains the wrong return value of the \nsort_priority2 function. The found variable is assigned to True in the \nhelper closure. The closure’s assignment is treated as a new variable \ndefinition within helper, not as an assignment within sort_priority2:\ndef sort_priority2(numbers, group):\n    found = False         # Scope: 'sort_priority2'\n    def helper(x):\n        if x in group:\n            found = True  # Scope: 'helper' -- Bad!\n            return (0, x)\n        return (1, x)\n\n\n86 \nChapter 3 Functions\n    numbers.sort(key=helper)\n    return found\nThis problem is sometimes called the scoping bug because it can be \nso surprising to newbies. But this behavior is the intended result: It \nprevents local variables in a function from polluting the containing \nmodule. Otherwise, every assignment within a function would put \ngarbage into the global module scope. Not only would that be noise, \nbut the interplay of the resulting global variables could cause obscure \nbugs.\nIn Python, there is special syntax for getting data out of a closure. \nThe nonlocal statement is used to indicate that scope traversal should \nhappen upon assignment for a specific variable name. The only limit \nis that nonlocal won’t traverse up to the module-level scope (to avoid \npolluting globals).\nHere, I define the same function again, now using nonlocal:\ndef sort_priority3(numbers, group):\n    found = False\n    def helper(x):\n        nonlocal found  # Added\n        if x in group:\n            found = True\n            return (0, x)\n        return (1, x)\n    numbers.sort(key=helper)\n    return found\nThe nonlocal statement makes it clear when data is being assigned \nout of a closure and into another scope. It’s complementary to the \nglobal statement, which indicates that a variable’s assignment should \ngo directly into the module scope.\nHowever, much as with the anti-pattern of global variables, I’d cau-\ntion against using nonlocal for anything beyond simple functions. \nThe side effects of nonlocal can be hard to follow. It’s especially hard \nto understand in long functions where the nonlocal statements and \nassignments to associated variables are far apart.\nWhen your usage of nonlocal starts getting complicated, it’s better to \nwrap your state in a helper class. Here, I define a class that achieves \nthe same result as the nonlocal approach; it’s a little longer but much \neasier to read (see Item 38: “Accept Functions Instead of Classes for \nSimple Interfaces” for details on the __call__ special method):\nclass Sorter:\n    def __init__(self, group):\n\n\n \nItem 22: Reduce Visual Noise with Variable Positional Arguments \n87\n        self.group = group\n        self.found = False\n \n    def __call__(self, x):\n        if x in self.group:\n            self.found = True\n            return (0, x)\n        return (1, x)\n \nsorter = Sorter(group)\nnumbers.sort(key=sorter)\nassert sorter.found is True\nThings to Remember\n✦ Closure functions can refer to variables from any of the scopes in \nwhich they were defined.\n✦ By default, closures can’t affect enclosing scopes by assigning \nvariables.\n✦ Use the nonlocal statement to indicate when a closure can modify a \nvariable in its enclosing scopes.\n✦ Avoid using nonlocal statements for anything beyond simple \nfunctions.\nItem 22:  Reduce Visual Noise with Variable Positional \nArguments\nAccepting a variable number of positional arguments can make a \nfunction call clearer and reduce visual noise. (These positional argu-\nments are often called varargs for short, or star args, in reference to \nthe conventional name for the parameter *args.) For example, say \nthat I want to log some debugging information. With a fixed number \nof arguments, I would need a function that takes a message and a \nlist of values:\ndef log(message, values):\n    if not values:\n        print(message)\n    else:\n        values_str = ', '.join(str(x) for x in values)\n        print(f'{message}: {values_str}')\n \nlog('My numbers are', [1, 2])\nlog('Hi there', [])\n\n\n88 \nChapter 3 Functions\n>>>\nMy numbers are: 1, 2\nHi there\nHaving to pass an empty list when I have no values to log is cum-\nbersome and noisy. It’d be better to leave out the second argument \nentirely. I can do this in Python by prefixing the last positional \nparameter name with *. The first parameter for the log message is \nrequired, whereas any number of subsequent positional arguments \nare optional. The function body doesn’t need to change; only the call-\ners do:\ndef log(message, *values):  # The only difference\n    if not values:\n        print(message)\n    else:\n        values_str = ', '.join(str(x) for x in values)\n        print(f'{message}: {values_str}')\n \nlog('My numbers are', 1, 2)\nlog('Hi there')  # Much better\n>>>\nMy numbers are: 1, 2\nHi there\nYou might notice that this syntax works very similarly to the starred \nexpressions used in unpacking assignment statements (see Item 13: \n“Prefer Catch-All Unpacking Over Slicing”).\nIf I already have a sequence (like a list) and want to call a variadic \nfunction like log, I can do this by using the * operator. This instructs \nPython to pass items from the sequence as positional arguments to \nthe function:\nfavorites = [7, 33, 99]\nlog('Favorite colors', *favorites)\n>>>\nFavorite colors: 7, 33, 99\nThere are two problems with accepting a variable number of posi-\ntional arguments.\nThe first issue is that these optional positional arguments are always \nturned into a tuple before they are passed to a function. This means \nthat if the caller of a function uses the * operator on a generator, it \nwill be iterated until it’s exhausted (see Item 30: “Consider Genera-\ntors Instead of Returning Lists” for background). The resulting tuple \n\n\n \nItem 22: Reduce Visual Noise with Variable Positional Arguments \n89\nincludes every value from the generator, which could consume a lot of \nmemory and cause the program to crash:\ndef my_generator():\n    for i in range(10):\n        yield i\n \ndef my_func(*args):\n    print(args)\n \nit = my_generator()\nmy_func(*it)\n>>>\n(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\nFunctions that accept *args are best for situations where you know \nthe number of inputs in the argument list will be reasonably small. \n*args is ideal for function calls that pass many literals or variable \nnames together. It’s primarily for the convenience of the programmer \nand the readability of the code.\nThe second issue with *args is that you can’t add new positional \narguments to a function in the future without migrating every caller. \nIf you try to add a positional argument in the front of the argument \nlist, existing callers will subtly break if they aren’t updated:\ndef log(sequence, message, *values):\n    if not values:\n        print(f'{sequence} - {message}')\n    else:\n        values_str = ', '.join(str(x) for x in values)\n        print(f'{sequence} - {message}: {values_str}')\n \nlog(1, 'Favorites', 7, 33)      # New with *args OK\nlog(1, 'Hi there')              # New message only OK\nlog('Favorite numbers', 7, 33)  # Old usage breaks\n>>>\n1 - Favorites: 7, 33\n1 - Hi there\nFavorite numbers - 7: 33\nThe problem here is that the third call to log used 7 as the message \nparameter because a sequence argument wasn’t given. Bugs like \nthis are hard to track down because the code still runs without \nraising exceptions. To avoid this possibility entirely, you should use \n keyword-only arguments when you want to extend functions that \n\n\n90 \nChapter 3 Functions\naccept *args (see Item 25: “Enforce Clarity with Keyword-Only and \nPositional-Only Arguments”). To be even more defensive, you could \nalso consider using type annotations (see Item 90: “Consider Static \nAnalysis via typing to Obviate Bugs”).\nThings to Remember\n✦ Functions can accept a variable number of positional arguments by \nusing *args in the def statement.\n✦ You can use the items from a sequence as the positional arguments \nfor a function with the * operator.\n✦ Using the * operator with a generator may cause a program to run \nout of memory and crash.\n✦ Adding new positional parameters to functions that accept *args \ncan introduce hard-to-detect bugs.\nItem 23:  Provide Optional Behavior with \nKeyword Arguments\nAs in most other programming languages, in Python you may pass \narguments by position when calling a function:\ndef remainder(number, divisor):\n    return number % divisor\n \nassert remainder(20, 7) == 6\nAll normal arguments to Python functions can also be passed by \nkeyword, where the name of the argument is used in an assignment \nwithin the parentheses of a function call. The keyword arguments \ncan be passed in any order as long as all of the required positional \narguments are specified. You can mix and match keyword and posi-\ntional arguments. These calls are equivalent:\nremainder(20, 7)\nremainder(20, divisor=7)\nremainder(number=20, divisor=7)\nremainder(divisor=7, number=20)\nPositional arguments must be specified before keyword arguments:\nremainder(number=20, 7)\n>>>\nTraceback ...\nSyntaxError: positional argument follows keyword argument\n\n\n \nItem 23: Provide Optional Behavior with Keyword Arguments \n91\nEach argument can be specified only once:\nremainder(20, number=7)\n>>>\nTraceback ...\nTypeError: remainder() got multiple values for argument \n¯'number'\nIf you already have a dictionary, and you want to use its contents to \ncall a function like remainder, you can do this by using the ** opera-\ntor. This instructs Python to pass the values from the dictionary as \nthe corresponding keyword arguments of the function:\nmy_kwargs = {\n    'number': 20,\n    'divisor': 7,\n}\nassert remainder(**my_kwargs) == 6\nYou can mix the ** operator with positional arguments or keyword \narguments in the function call, as long as no argument is repeated:\nmy_kwargs = {\n    'divisor': 7,\n}\nassert remainder(number=20, **my_kwargs) == 6\nYou can also use the ** operator multiple times if you know that the \ndictionaries don’t contain overlapping keys:\nmy_kwargs = {\n    'number': 20,\n}\nother_kwargs = {\n    'divisor': 7,\n}\nassert remainder(**my_kwargs, **other_kwargs) == 6\nAnd if you’d like for a function to receive any named keyword argu-\nment, you can use the **kwargs catch-all parameter to collect those \narguments into a dict that you can then process (see Item 26: “Define \nFunction Decorators with functools.wraps” for when this is especially \nuseful):\ndef print_parameters(**kwargs):\n    for key, value in kwargs.items():\n        print(f'{key} = {value}')\n \nprint_parameters(alpha=1.5, beta=9, gamma=4)\n\n\n92 \nChapter 3 Functions\n>>>\nalpha = 1.5\nbeta = 9\ngamma = 4\nThe flexibility of keyword arguments provides three significant \nbenefits.\nThe first benefit is that keyword arguments make the function call \nclearer to new readers of the code. With the call remainder(20, 7), it’s \nnot evident which argument is number and which is divisor unless \nyou look at the implementation of the remainder method. In the call \nwith keyword arguments, number=20 and divisor=7 make it immedi-\nately obvious which parameter is being used for each purpose.\nThe second benefit of keyword arguments is that they can have \ndefault values specified in the function definition. This allows a func-\ntion to provide additional capabilities when you need them, but you \ncan accept the default behavior most of the time. This eliminates \nrepetitive code and reduces noise.\nFor example, say that I want to compute the rate of fluid flowing into \na vat. If the vat is also on a scale, then I could use the difference \nbetween two weight measurements at two different times to deter-\nmine the flow rate:\ndef flow_rate(weight_diff, time_diff):\n    return weight_diff / time_diff\n \nweight_diff = 0.5\ntime_diff = 3\nflow = flow_rate(weight_diff, time_diff)\nprint(f'{flow:.3} kg per second')\n>>>\n0.167 kg per second\nIn the typical case, it’s useful to know the flow rate in kilograms per \nsecond. Other times, it’d be helpful to use the last sensor measure-\nments to approximate larger time scales, like hours or days. I can \nprovide this behavior in the same function by adding an argument for \nthe time period scaling factor:\ndef flow_rate(weight_diff, time_diff, period):\n    return (weight_diff / time_diff) * period\nThe problem is that now I need to specify the period argument every \ntime I call the function, even in the common case of flow rate per sec-\nond (where the period is 1):\nflow_per_second = flow_rate(weight_diff, time_diff, 1)\n",
      "page_number": 99,
      "chapter_number": 11,
      "summary": "This chapter covers segment 11 (pages 99-114). Key topics include functions, function, and item. Covers function. As in other programming languages, functions enable you \nto break large programs into smaller, simpler pieces with names to \nrepresent their intent.",
      "keywords": [
        "function",
        "Item",
        "Functions",
        "Functions Return Multiple",
        "numbers",
        "Python",
        "arguments",
        "Positional Arguments",
        "variable",
        "Variable Positional Arguments",
        "Functions Return",
        "Scope",
        "Variables",
        "argument",
        "result"
      ],
      "concepts": [
        "functions",
        "function",
        "item",
        "numbers",
        "returned",
        "variables",
        "variable",
        "argument",
        "arguments",
        "scope"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 14,
          "title": "Segment 14 (pages 116-123)",
          "relevance_score": 0.84,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 15,
          "title": "Segment 15 (pages 124-132)",
          "relevance_score": 0.79,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 49,
          "title": "Segment 49 (pages 993-1010)",
          "relevance_score": 0.77,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 13,
          "title": "Segment 13 (pages 250-271)",
          "relevance_score": 0.73,
          "method": "api"
        }
      ]
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 115-125)",
      "start_page": 115,
      "end_page": 125,
      "detection_method": "topic_boundary",
      "content": " \nItem 23: Provide Optional Behavior with Keyword Arguments \n93\nTo make this less noisy, I can give the period argument a default \nvalue:\ndef flow_rate(weight_diff, time_diff, period=1):\n    return (weight_diff / time_diff) * period\nThe period argument is now optional:\nflow_per_second = flow_rate(weight_diff, time_diff)\nflow_per_hour = flow_rate(weight_diff, time_diff, period=3600)\nThis works well for simple default values; it gets tricky for complex \ndefault values (see Item 24: “Use None and Docstrings to Specify \nDynamic Default Arguments” for details).\nThe third reason to use keyword arguments is that they provide a \npowerful way to extend a function’s parameters while remaining \nbackward compatible with existing callers. This means you can pro-\nvide additional functionality without having to migrate a lot of exist-\ning code, which reduces the chance of introducing bugs.\nFor example, say that I want to extend the flow_rate function above \nto calculate flow rates in weight units besides kilograms. I can do this \nby adding a new optional parameter that provides a conversion rate to \nalternative measurement units:\ndef flow_rate(weight_diff, time_diff,\n              period=1, units_per_kg=1):\n    return ((weight_diff * units_per_kg) / time_diff) * period\nThe default argument value for units_per_kg is 1, which makes the \nreturned weight units remain kilograms. This means that all existing \ncallers will see no change in behavior. New callers to flow_rate can \nspecify the new keyword argument to see the new behavior:\npounds_per_hour = flow_rate(weight_diff, time_diff,\n                            period=3600, units_per_kg=2.2)\nProviding backward compatibility using optional keyword arguments \nlike this is also crucial for functions that accept *args (see Item 22: \n“Reduce Visual Noise with Variable Positional Arguments”).\nThe only problem with this approach is that optional keyword argu-\nments like period and units_per_kg may still be specified as posi-\ntional arguments:\npounds_per_hour = flow_rate(weight_diff, time_diff, 3600, 2.2)\nSupplying optional arguments positionally can be confusing because \nit isn’t clear what the values 3600 and 2.2 correspond to. The best \npractice is to always specify optional arguments using the keyword \n\n\n94 \nChapter 3 Functions\nnames and never pass them as positional arguments. As a function \nauthor, you can also require that all callers use this more explicit \nkeyword style to minimize potential errors (see Item 25: “Enforce \nClarity with Keyword-Only and Positional-Only Arguments”).\nThings to Remember\n✦ Function arguments can be specified by position or by keyword.\n✦ Keywords make it clear what the purpose of each argument is when \nit would be confusing with only positional arguments.\n✦ Keyword arguments with default values make it easy to add new \nbehaviors to a function without needing to migrate all existing \ncallers.\n✦ Optional keyword arguments should always be passed by keyword \ninstead of by position.\nItem 24:  Use None and Docstrings to Specify Dynamic \nDefault Arguments\nSometimes you need to use a non-static type as a keyword  argument’s \ndefault value. For example, say I want to print logging messages that \nare marked with the time of the logged event. In the default case, \nI want the message to include the time when the function was \ncalled. I might try the following approach, assuming that the default \n arguments are reevaluated each time the function is called:\nfrom time import sleep\nfrom datetime import datetime\n \ndef log(message, when=datetime.now()):\n    print(f'{when}: {message}')\n \nlog('Hi there!')\nsleep(0.1)\nlog('Hello again!')\n>>>\n2019-07-06 14:06:15.120124: Hi there!\n2019-07-06 14:06:15.120124: Hello again!\nThis doesn’t work as expected. The timestamps are the same because \ndatetime.now is executed only a single time: when the function is \ndefined. A default argument value is evaluated only once per module \n\n\n \nItem 24: Specify Dynamic Default Arguments in Docstrings \n95\nload, which usually happens when a program starts up. After the \nmodule containing this code is loaded, the datetime.now() default \nargument will never be evaluated again.\nThe convention for achieving the desired result in Python is to provide \na default value of None and to document the actual behavior in the \ndocstring (see Item 84: “Write Docstrings for Every Function, Class, \nand Module” for background). When your code sees the argument \nvalue None, you allocate the default value accordingly:\ndef log(message, when=None):\n    \"\"\"Log a message with a timestamp.\n \n    Args:\n        message: Message to print.\n        when: datetime of when the message occurred.\n            Defaults to the present time.\n    \"\"\"\n    if when is None:\n        when = datetime.now()\n    print(f'{when}: {message}')\nNow the timestamps will be different:\nlog('Hi there!')\nsleep(0.1)\nlog('Hello again!')\n>>>\n2019-07-06 14:06:15.222419: Hi there!\n2019-07-06 14:06:15.322555: Hello again!\nUsing None for default argument values is especially important when \nthe arguments are mutable. For example, say that I want to load a \nvalue encoded as JSON data; if decoding the data fails, I want an \nempty dictionary to be returned by default:\nimport json\n \ndef decode(data, default={}):\n    try:\n        return json.loads(data)\n    except ValueError:\n        return default\nThe problem here is the same as in the datetime.now example above. \nThe dictionary specified for default will be shared by all calls to \n\n\n96 \nChapter 3 Functions\ndecode because default argument values are evaluated only once (at \nmodule load time). This can cause extremely surprising behavior:\nfoo = decode('bad data')\nfoo['stuff'] = 5\nbar = decode('also bad')\nbar['meep'] = 1\nprint('Foo:', foo)\nprint('Bar:', bar)\n>>>\nFoo: {'stuff': 5, 'meep': 1}\nBar: {'stuff': 5, 'meep': 1}\nYou might expect two different dictionaries, each with a single key \nand value. But modifying one seems to also modify the other. The cul-\nprit is that foo and bar are both equal to the default parameter. They \nare the same dictionary object:\nassert foo is bar\nThe fix is to set the keyword argument default value to None and then \ndocument the behavior in the function’s docstring:\ndef decode(data, default=None):\n    \"\"\"Load JSON data from a string.\n \n    Args:\n        data: JSON data to decode.\n        default: Value to return if decoding fails.\n            Defaults to an empty dictionary.\n    \"\"\"\n    try:\n        return json.loads(data)\n    except ValueError:\n        if default is None:\n            default = {}\n        return default\nNow, running the same test code as before produces the expected \nresult:\nfoo = decode('bad data')\nfoo['stuff'] = 5\nbar = decode('also bad')\nbar['meep'] = 1\nprint('Foo:', foo)\nprint('Bar:', bar)\nassert foo is not bar\n\n\n \nItem 25: Clarity with Keyword-Only and Positional-Only Arguments \n97\n>>>\nFoo: {'stuff': 5}\nBar: {'meep': 1}\nThis approach also works with type annotations (see Item 90: “Con-\nsider Static Analysis via typing to Obviate Bugs”). Here, the when \nargument is marked as having an Optional value that is a datetime. \nThus, the only two valid choices for when are None or a datetime object:\nfrom typing import Optional\n \ndef log_typed(message: str,\n              when: Optional[datetime]=None) -> None:\n    \"\"\"Log a message with a timestamp.\n \n    Args:\n        message: Message to print.\n        when: datetime of when the message occurred.\n            Defaults to the present time.\n    \"\"\"\n    if when is None:\n        when = datetime.now()\n    print(f'{when}: {message}')\nThings to Remember\n✦ A default argument value is evaluated only once: during function \ndefinition at module load time. This can cause odd behaviors for \ndynamic values (like {}, [], or datetime.now()).\n✦ Use None as the default value for any keyword argument that has a \ndynamic value. Document the actual default behavior in the func-\ntion’s docstring.\n✦ Using None to represent keyword argument default values also \nworks correctly with type annotations.\nItem 25:  Enforce Clarity with Keyword-Only and \nPositional-Only Arguments\nPassing arguments by keyword is a powerful feature of Python func-\ntions (see Item 23: “Provide Optional Behavior with Keyword Argu-\nments”). The flexibility of keyword arguments enables you to write \nfunctions that will be clear to new readers of your code for many use \ncases.\nFor example, say I want to divide one number by another but know \nthat I need to be very careful about special cases. Sometimes, I want \n\n\n98 \nChapter 3 Functions\nto ignore ZeroDivisionError exceptions and return infinity instead. \nOther times, I want to ignore OverflowError exceptions and return \nzero instead:\ndef safe_division(number, divisor,\n                  ignore_overflow,\n                  ignore_zero_division):\n    try:\n        return number / divisor\n    except OverflowError:\n        if ignore_overflow:\n            return 0\n        else:\n            raise\n    except ZeroDivisionError:\n        if ignore_zero_division:\n            return float('inf')\n        else:\n            raise\nUsing this function is straightforward. This call ignores the float \noverflow from division and returns zero:\nresult = safe_division(1.0, 10**500, True, False)\nprint(result)\n>>>\n0\nThis call ignores the error from dividing by zero and returns infinity:\nresult = safe_division(1.0, 0, False, True)\nprint(result)\n>>>\ninf\nThe problem is that it’s easy to confuse the position of the two Bool-\nean arguments that control the exception-ignoring behavior. This can \neasily cause bugs that are hard to track down. One way to improve the \nreadability of this code is to use keyword arguments. By default, the \nfunction can be overly cautious and can always re-raise exceptions:\ndef safe_division_b(number, divisor,\n                    ignore_overflow=False,        # Changed\n                    ignore_zero_division=False):  # Changed\n    ...\n\n\nThen, callers can use keyword arguments to specify which of the \nignore flags they want to set for specific operations, overriding the \ndefault behavior:\nresult = safe_division_b(1.0, 10**500, ignore_overflow=True)\nprint(result)\n \nresult = safe_division_b(1.0, 0, ignore_zero_division=True)\nprint(result)\n>>>\n0\ninf\nThe problem is, since these keyword arguments are optional behavior, \nthere’s nothing forcing callers to use keyword arguments for clarity. \nEven with the new definition of safe_division_b, you can still call it \nthe old way with positional arguments:\nassert safe_division_b(1.0, 10**500, True, False) == 0\nWith complex functions like this, it’s better to require that callers are \nclear about their intentions by defining functions with  keyword-only \narguments. These arguments can only be supplied by keyword, never \nby position.\nHere, I redefine the safe_division function to accept keyword-only \narguments. The * symbol in the argument list indicates the end \nof positional arguments and the beginning of keyword-only \narguments:\ndef safe_division_c(number, divisor, *,  # Changed\n                    ignore_overflow=False,\n                    ignore_zero_division=False):\n    ...\nNow, calling the function with positional arguments for the keyword \narguments won’t work:\nsafe_division_c(1.0, 10**500, True, False)\n>>>\nTraceback ...\nTypeError: safe_division_c() takes 2 positional arguments but 4 \n¯were given\nBut keyword arguments and their default values will work as expected \n(ignoring an exception in one case and raising it in another):\nresult = safe_division_c(1.0, 0, ignore_zero_division=True)\nassert result == float('inf')\n \n \nItem 25: Clarity with Keyword-Only and Positional-Only Arguments \n99\n\n\n100 \nChapter 3 Functions\ntry:\n    result = safe_division_c(1.0, 0)\nexcept ZeroDivisionError:\n    pass  # Expected\nHowever, a problem still remains with the safe_division_c version of \nthis function: Callers may specify the first two required arguments \n(number and divisor) with a mix of positions and keywords:\nassert safe_division_c(number=2, divisor=5) == 0.4\nassert safe_division_c(divisor=5, number=2) == 0.4\nassert safe_division_c(2, divisor=5) == 0.4\nLater, I may decide to change the names of these first two arguments \nbecause of expanding needs or even just because my style preferences \nchange:\ndef safe_division_c(numerator, denominator, *,  # Changed\n                    ignore_overflow=False,\n                    ignore_zero_division=False):\n    ...\nUnfortunately, this seemingly superficial change breaks all the exist-\ning callers that specified the number or divisor arguments using \nkeywords:\nsafe_division_c(number=2, divisor=5)\n>>>\nTraceback ...\nTypeError: safe_division_c() got an unexpected keyword argument \n¯'number'\nThis is especially problematic because I never intended for number and \ndivisor to be part of an explicit interface for this function. These were \njust convenient parameter names that I chose for the implementation, \nand I didn’t expect anyone to rely on them explicitly.\nPython 3.8 introduces a solution to this problem, called positional-only \narguments. These arguments can be supplied only by position and \nnever by keyword (the opposite of the keyword-only arguments \ndemonstrated above).\nHere, I redefine the safe_division function to use positional-only \narguments for the first two required parameters. The / symbol in the \nargument list indicates where positional-only arguments end:\ndef safe_division_d(numerator, denominator, /, *,  # Changed\n                    ignore_overflow=False,\n                    ignore_zero_division=False):\n    ...\n\n\nI can verify that this function works when the required arguments \nare provided positionally:\nassert safe_division_d(2, 5) == 0.4\nBut an exception is raised if keywords are used for the positional-only \nparameters:\nsafe_division_d(numerator=2, denominator=5)\n>>>\nTraceback ...\nTypeError: safe_division_d() got some positional-only arguments \n¯passed as keyword arguments: 'numerator, denominator'\nNow, I can be sure that the first two required positional arguments \nin the definition of the safe_division_d function are decoupled from \ncallers. I won’t break anyone if I change the parameters’ names again.\nOne notable consequence of keyword- and positional-only arguments \nis that any parameter name between the / and * symbols in the argu-\nment list may be passed either by position or by keyword (which is \nthe default for all function arguments in Python). Depending on your \nAPI’s style and needs, allowing both argument passing styles can \nincrease readability and reduce noise. For example, here I’ve added \nanother optional parameter to safe_division that allows callers to \nspecify how many digits to use in rounding the result:\ndef safe_division_e(numerator, denominator, /,\n                    ndigits=10, *,                # Changed\n                    ignore_overflow=False,\n                    ignore_zero_division=False):\n    try:\n        fraction = numerator / denominator        # Changed\n        return round(fraction, ndigits)           # Changed\n    except OverflowError:\n        if ignore_overflow:\n            return 0\n        else:\n            raise\n    except ZeroDivisionError:\n        if ignore_zero_division:\n            return float('inf')\n        else:\n            raise\n \nItem 25: Clarity with Keyword-Only and Positional-Only Arguments \n101\n\n\n102 \nChapter 3 Functions\nNow, I can call this new version of the function in all these differ-\nent ways, since ndigits is an optional parameter that may be passed \neither by position or by keyword:\nresult = safe_division_e(22, 7)\nprint(result)\nresult = safe_division_e(22, 7, 5)\nprint(result)\nresult = safe_division_e(22, 7, ndigits=2)\nprint(result)\n>>>\n3.1428571429\n3.14286\n3.14\nThings to Remember\n✦ Keyword-only arguments force callers to supply certain arguments \nby keyword (instead of by position), which makes the intention of a \nfunction call clearer. Keyword-only arguments are defined after a \nsingle * in the argument list.\n✦ Positional-only arguments ensure that callers can’t supply \n certain parameters using keywords, which helps reduce coupling. \n Positional-only arguments are defined before a single / in the argu-\nment list.\n✦ Parameters between the / and * characters in the argument list \nmay be supplied by position or keyword, which is the default for \nPython parameters.\nItem 26:  Define Function Decorators with \nfunctools.wraps\nPython has special syntax for decorators that can be applied to \nfunctions. A decorator has the ability to run additional code before \nand after each call to a function it wraps. This means decorators \ncan access and modify input arguments, return values, and raised \nexceptions. This functionality can be useful for enforcing semantics, \ndebugging, registering functions, and more.\nFor example, say that I want to print the arguments and return value \nof a function call. This can be especially helpful when debugging \n\n\n \nItem 26: Define Function Decorators with functools.wraps \n103\nthe stack of nested function calls from a recursive function. Here, \nI define such a decorator by using *args and **kwargs (see Item 22: \n“Reduce Visual Noise with Variable Positional Arguments” and Item \n23:  “Provide Optional Behavior with Keyword Arguments”) to pass \nthrough all parameters to the wrapped function:\ndef trace(func):\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        print(f'{func.__name__}({args!r}, {kwargs!r}) '\n              f'-> {result!r}')\n        return result\n    return wrapper\nI can apply this decorator to a function by using the @ symbol:\n@trace\ndef fibonacci(n):\n    \"\"\"Return the n-th Fibonacci number\"\"\"\n    if n in (0, 1):\n        return n\n    return (fibonacci(n - 2) + fibonacci(n - 1))\nUsing the @ symbol is equivalent to calling the decorator on the func-\ntion it wraps and assigning the return value to the original name in \nthe same scope:\nfibonacci = trace(fibonacci)\nThe decorated function runs the wrapper code before and after \nfibonacci runs. It prints the arguments and return value at each \nlevel in the recursive stack:\nfibonacci(4)\n>>>\nfibonacci((0,), {}) -> 0\nfibonacci((1,), {}) -> 1\nfibonacci((2,), {}) -> 1\nfibonacci((1,), {}) -> 1\nfibonacci((0,), {}) -> 0\nfibonacci((1,), {}) -> 1\nfibonacci((2,), {}) -> 1\nfibonacci((3,), {}) -> 2\nfibonacci((4,), {}) -> 3\n",
      "page_number": 115,
      "chapter_number": 12,
      "summary": "92 \nChapter 3 Functions\n>>>\nalpha = 1.5\nbeta = 9\ngamma = 4\nThe flexibility of keyword arguments provides three significant \nbenefits Key topics include argument, functions, and function. Covers function.",
      "keywords": [
        "arguments",
        "keyword arguments",
        "default",
        "keyword",
        "argument",
        "function",
        "division",
        "safe",
        "Positional-Only Arguments",
        "diff",
        "default argument",
        "keyword argument default",
        "Dynamic Default Arguments",
        "optional keyword arguments",
        "time"
      ],
      "concepts": [
        "argument",
        "functions",
        "function",
        "functionality",
        "returned",
        "item",
        "defaults",
        "result",
        "optional",
        "positional"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 14,
          "title": "Segment 14 (pages 116-123)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 14,
          "title": "Segment 14 (pages 272-289)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 15,
          "title": "Segment 15 (pages 124-132)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 12,
          "title": "Segment 12 (pages 364-393)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 126-133)",
      "start_page": 126,
      "end_page": 133,
      "detection_method": "topic_boundary",
      "content": "104 \nChapter 3 Functions\nThis works well, but it has an unintended side effect. The value \nreturned by the decorator—the function that’s called above—doesn’t \nthink it’s named fibonacci:\nprint(fibonacci)\n>>>\n<function trace.<locals>.wrapper at 0x108955dc0>\nThe cause of this isn’t hard to see. The trace function returns the \nwrapper defined within its body. The wrapper function is what’s \nassigned to the fibonacci name in the containing module because \nof the decorator. This behavior is problematic because it undermines \ntools that do introspection, such as debuggers (see Item 80: “Consider \nInteractive Debugging with pdb”).\nFor example, the help built-in function is useless when called on the \ndecorated fibonacci function. It should instead print out the doc-\nstring defined above ('Return the n-th Fibonacci number'):\nhelp(fibonacci)\n>>>\nHelp on function wrapper in module __main__:\n \nwrapper(*args, **kwargs)\nObject serializers (see Item 68: “Make pickle Reliable with copyreg”) \nbreak because they can’t determine the location of the original func-\ntion that was decorated:\nimport pickle\n \npickle.dumps(fibonacci)\n>>>\nTraceback ...\nAttributeError: Can't pickle local object 'trace.<locals>.\n¯wrapper'\nThe solution is to use the wraps helper function from the functools \nbuilt-in module. This is a decorator that helps you write decorators. \nWhen you apply it to the wrapper function, it copies all of the import-\nant metadata about the inner function to the outer function:\nfrom functools import wraps\n \n\n\n \nItem 26: Define Function Decorators with functools.wraps \n105\ndef trace(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        ...\n    return wrapper\n \n@trace\ndef fibonacci(n):\n    ...\nNow, running the help function produces the expected result, even \nthough the function is decorated:\nhelp(fibonacci)\n>>>\nHelp on function fibonacci in module __main__:\n \nfibonacci(n)\n    Return the n-th Fibonacci number\nThe pickle object serializer also works:\nprint(pickle.dumps(fibonacci))\n>>>\nb'\\x80\\x04\\x95\\x1a\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08__main__\\\n¯x94\\x8c\\tfibonacci\\x94\\x93\\x94.'\nBeyond these examples, Python functions have many other standard \nattributes (e.g., __name__, __module__, __annotations__) that must \nbe preserved to maintain the interface of functions in the language. \nUsing wraps ensures that you’ll always get the correct behavior.\nThings to Remember\n✦ Decorators in Python are syntax to allow one function to modify \nanother function at runtime.\n✦ Using decorators can cause strange behaviors in tools that do intro-\nspection, such as debuggers.\n✦ Use the wraps decorator from the functools built-in module when \nyou define your own decorators to avoid issues.\n\n\nThis page intentionally left blank \n\n\n4\nComprehensions \nand Generators\nMany programs are built around processing lists, dictionary \nkey/value pairs, and sets. Python provides a special syntax, called \ncomprehensions, for succinctly iterating through these types and cre-\nating derivative data structures. Comprehensions can significantly \nincrease the readability of code performing these common tasks and \nprovide a number of other benefits.\nThis style of processing is extended to functions with generators, \nwhich enable a stream of values to be incrementally returned by a \nfunction. The result of a call to a generator function can be used any-\nwhere an iterator is appropriate (e.g., for loops, starred expressions). \nGenerators can improve performance, reduce memory usage, and \nincrease readability.\nItem 27:  Use Comprehensions Instead of map \nand filter\nPython provides compact syntax for deriving a new list from another \nsequence or iterable. These expressions are called list comprehensions. \nFor example, say that I want to compute the square of each number \nin a list. Here, I do this by using a simple for loop:\na = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nsquares = []\nfor x in a:\n    squares.append(x**2)\nprint(squares)\n>>>\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\n\n108 \nChapter 4 Comprehensions and Generators\nWith a list comprehension, I can achieve the same outcome by specify-\ning the expression for my computation along with the input sequence \nto loop over:\nsquares = [x**2 for x in a]  # List comprehension\nprint(squares)\n>>>\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\nUnless you’re applying a single-argument function, list comprehen-\nsions are also clearer than the map built-in function for simple cases. \nmap requires the creation of a lambda function for the computation, \nwhich is visually noisy:\nalt = map(lambda x: x ** 2, a)\nUnlike map, list comprehensions let you easily filter items from the \ninput list, removing corresponding outputs from the result. For \nexample, say I want to compute the squares of the numbers that are \ndivisible by 2. Here, I do this by adding a conditional expression to \nthe list comprehension after the loop:\neven_squares = [x**2 for x in a if x % 2 == 0]\nprint(even_squares)\n>>>\n[4, 16, 36, 64, 100]\nThe filter built-in function can be used along with map to achieve the \nsame outcome, but it is much harder to read:\nalt = map(lambda x: x**2, filter(lambda x: x % 2 == 0, a))\nassert even_squares == list(alt)\nDictionaries and sets have their own equivalents of list comprehen-\nsions (called dictionary comprehensions and set comprehensions, \nrespectively). These make it easy to create other types of derivative \ndata structures when writing algorithms:\neven_squares_dict = {x: x**2 for x in a if x % 2 == 0}\nthrees_cubed_set = {x**3 for x in a if x % 3 == 0}\nprint(even_squares_dict)\nprint(threes_cubed_set)\n>>>\n{2: 4, 4: 16, 6: 36, 8: 64, 10: 100}\n{216, 729, 27}\nAchieving the same outcome is possible with map and filter if you \nwrap each call with a corresponding constructor. These statements \n\n\n \nItem 28: Control Subexpressions in Comprehensions \n109\nget so long that you have to break them up across multiple lines, \nwhich is even noisier and should be avoided:\nalt_dict = dict(map(lambda x: (x, x**2),\n                filter(lambda x: x % 2 == 0, a)))\nalt_set = set(map(lambda x: x**3,\n              filter(lambda x: x % 3 == 0, a)))\nThings to Remember\n✦ List comprehensions are clearer than the map and filter built-in \nfunctions because they don’t require lambda expressions.\n✦ List comprehensions allow you to easily skip items from the input \nlist, a behavior that map doesn’t support without help from filter.\n✦ Dictionaries and sets may also be created using comprehensions.\nItem 28:  Avoid More Than Two Control \nSubexpressions in Comprehensions\nBeyond basic usage (see Item 27: “Use Comprehensions Instead of map \nand filter”), comprehensions support multiple levels of looping. For \nexample, say that I want to simplify a matrix (a list containing other \nlist instances) into one flat list of all cells. Here, I do this with a list \ncomprehension by including two for subexpressions. These subex-\npressions run in the order provided, from left to right:\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nflat = [x for row in matrix for x in row]\nprint(flat)\n>>>\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\nThis example is simple, readable, and a reasonable usage of multiple \nloops in a comprehension. Another reasonable usage of multiple loops \ninvolves replicating the two-level-deep layout of the input list. For \nexample, say that I want to square the value in each cell of a two- \ndimensional matrix. This comprehension is noisier because of the \nextra [] characters, but it’s still relatively easy to read:\nsquared = [[x**2 for x in row] for row in matrix]\nprint(squared)\n>>>\n[[1, 4, 9], [16, 25, 36], [49, 64, 81]]\n\n\n110 \nChapter 4 Comprehensions and Generators\nIf this comprehension included another loop, it would get so long that \nI’d have to split it over multiple lines:\nmy_lists = [\n    [[1, 2, 3], [4, 5, 6]],\n    ...\n]\nflat = [x for sublist1 in my_lists\n        for sublist2 in sublist1\n        for x in sublist2]\nAt this point, the multiline comprehension isn’t much shorter than \nthe alternative. Here, I produce the same result using normal loop \nstatements. The indentation of this version makes the looping clearer \nthan the three-level-list comprehension:\nflat = []\nfor sublist1 in my_lists:\n    for sublist2 in sublist1:\n        flat.extend(sublist2)\nComprehensions support multiple if conditions. Multiple conditions \nat the same loop level have an implicit and expression. For example, \nsay that I want to filter a list of numbers to only even values greater \nthan 4. These two list comprehensions are equivalent:\na = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nb = [x for x in a if x > 4 if x % 2 == 0]\nc = [x for x in a if x > 4 and x % 2 == 0]\nConditions can be specified at each level of looping after the for sub-\nexpression. For example, say I want to filter a matrix so the only cells \nremaining are those divisible by 3 in rows that sum to 10 or higher. \nExpressing this with a list comprehension does not require a lot of \ncode, but it is extremely difficult to read:\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nfiltered = [[x for x in row if x % 3 == 0]\n            for row in matrix if sum(row) >= 10]\nprint(filtered)\n>>>\n[[6], [9]]\nAlthough this example is a bit convoluted, in practice you’ll see \n situations arise where such comprehensions seem like a good fit. \nI strongly encourage you to avoid using list, dict, or set comprehen-\nsions that look like this. The resulting code is very difficult for new \nreaders to understand. The potential for confusion is even worse for \n\n\n \nItem 29: Control Subexpressions in Comprehensions \n111\ndict comprehensions since they already need an extra parameter to \nrepresent both the key and the value for each item.\nThe rule of thumb is to avoid using more than two control subexpres-\nsions in a comprehension. This could be two conditions, two loops, \nor one condition and one loop. As soon as it gets more complicated \nthan that, you should use normal if and for statements and write a \nhelper function (see Item 30: “Consider Generators Instead of Return-\ning Lists”).\nThings to Remember\n✦ Comprehensions support multiple levels of loops and multiple con-\nditions per loop level.\n✦ Comprehensions with more than two control subexpressions are \nvery difficult to read and should be avoided.\nItem 29:  Avoid Repeated Work in Comprehensions by \nUsing Assignment Expressions\nA common pattern with comprehensions—including list, dict, and \nset variants—is the need to reference the same computation in mul-\ntiple places. For example, say that I’m writing a program to manage \norders for a fastener company. As new orders come in from customers, \nI need to be able to tell them whether I can fulfill their orders. I need \nto verify that a request is sufficiently in stock and above the mini-\nmum threshold for shipping (in batches of 8):\nstock = {\n    'nails': 125,\n    'screws': 35,\n    'wingnuts': 8,\n    'washers': 24,\n}\n \norder = ['screws', 'wingnuts', 'clips']\ndef get_batches(count, size):\n    return count // size\nresult = {}\nfor name in order:\n  count = stock.get(name, 0)\n  batches = get_batches(count, 8)\n",
      "page_number": 126,
      "chapter_number": 13,
      "summary": "This chapter covers segment 13 (pages 126-133). Key topics include comprehensions, comprehension, and function. The value \nreturned by the decorator—the function that’s called above—doesn’t \nthink it’s named fibonacci:\nprint(fibonacci)\n>>>\n<function trace.<locals>.wrapper at 0x108955dc0>\nThe cause of this isn’t hard to see.",
      "keywords": [
        "fibonacci",
        "Function",
        "list",
        "Comprehensions",
        "Variable Positional Arguments",
        "list comprehensions",
        "Define Function Decorators",
        "Item",
        "wrapper",
        "Reduce Visual Noise",
        "map",
        "filter",
        "decorator",
        "squares",
        "Visual Noise"
      ],
      "concepts": [
        "comprehensions",
        "comprehension",
        "function",
        "functions",
        "list",
        "item",
        "prints",
        "loops",
        "filter",
        "filtered"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 17,
          "title": "Segment 17 (pages 334-353)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 16,
          "title": "Segment 16 (pages 133-141)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 12,
          "title": "Segment 12 (pages 116-123)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 32,
          "title": "Segment 32 (pages 637-655)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 7,
          "title": "Segment 7 (pages 52-60)",
          "relevance_score": 0.52,
          "method": "api"
        }
      ]
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 134-147)",
      "start_page": 134,
      "end_page": 147,
      "detection_method": "topic_boundary",
      "content": "112 \nChapter 4 Comprehensions and Generators\n  if batches:\n    result[name] = batches\nprint(result)\n>>>\n{'screws': 4, 'wingnuts': 1}\nHere, I implement this looping logic more succinctly using a dictio-\nnary comprehension (see Item 27: “Use Comprehensions Instead of \nmap and filter” for best practices):\nfound = {name: get_batches(stock.get(name, 0), 8)\n         for name in order\n         if get_batches(stock.get(name, 0), 8)}\nprint(found)\n>>>\n{'screws': 4, 'wingnuts': 1}\nAlthough this code is more compact, the problem with it is that the \nget_batches(stock.get(name, 0), 8) expression is repeated. This \nhurts readability by adding visual noise that’s technically unneces-\nsary. It also increases the likelihood of introducing a bug if the two \nexpressions aren’t kept in sync. For example, here I’ve changed the \nfirst get_batches call to have 4 as its second parameter instead of 8, \nwhich causes the results to be different:\nhas_bug = {name: get_batches(stock.get(name, 0), 4)\n           for name in order\n           if get_batches(stock.get(name, 0), 8)}\nprint('Expected:', found)\nprint('Found:   ', has_bug)\n>>>\nExpected: {'screws': 4, 'wingnuts': 1}\nFound:    {'screws': 8, 'wingnuts': 2}\nAn easy solution to these problems is to use the walrus operator (:=), \nwhich was introduced in Python 3.8, to form an assignment expres-\nsion as part of the comprehension (see Item 10: “Prevent Repetition \nwith Assignment Expressions” for background):\nfound = {name: batches for name in order\n         if (batches := get_batches(stock.get(name, 0), 8))}\nThe assignment expression (batches := get_batches(...)) allows me \nto look up the value for each order key in the stock dictionary a single \n\n\ntime, call get_batches once, and then store its corresponding value in \nthe batches variable. I can then reference that variable elsewhere in \nthe comprehension to construct the dict’s contents instead of having \nto call get_batches a second time. Eliminating the redundant calls \nto get and get_batches may also improve performance by avoiding \nunnecessary computations for each item in the order list.\nIt’s valid syntax to define an assignment expression in the value \nexpression for a comprehension. But if you try to reference the vari-\nable it defines in other parts of the comprehension, you might get an \nexception at runtime because of the order in which comprehensions \nare evaluated:\nresult = {name: (tenth := count // 10)\n          for name, count in stock.items() if tenth > 0}\n>>>\nTraceback ...\nNameError: name 'tenth' is not defined\nI can fix this example by moving the assignment expression into the \ncondition and then referencing the variable name it defined in the \ncomprehension’s value expression:\nresult = {name: tenth for name, count in stock.items()\n          if (tenth := count // 10) > 0}\nprint(result)\n>>>\n{'nails': 12, 'screws': 3, 'washers': 2}\nIf a comprehension uses the walrus operator in the value part of the \ncomprehension and doesn’t have a condition, it’ll leak the loop vari-\nable into the containing scope (see Item 21: “Know How Closures \nInteract with Variable Scope” for background):\nhalf = [(last := count // 2) for count in stock.values()]\nprint(f'Last item of {half} is {last}')\n>>>\nLast item of [62, 17, 4, 12] is 12\nThis leakage of the loop variable is similar to what happens with a \nnormal for loop:\nfor count in stock.values():  # Leaks loop variable\n    pass\nprint(f'Last item of {list(stock.values())} is {count}')\n \nItem 29: Control Subexpressions in Comprehensions \n113\n\n\n114 \nChapter 4 Comprehensions and Generators\n>>>\nLast item of [125, 35, 8, 24] is 24\nHowever, similar leakage doesn’t happen for the loop variables from \ncomprehensions:\nhalf = [count // 2 for count in stock.values()]\nprint(half)   # Works\nprint(count)  # Exception because loop variable didn't leak\n>>>\n[62, 17, 4, 12]\nTraceback ...\nNameError: name 'count' is not defined\nIt’s better not to leak loop variables, so I recommend using assign-\nment expressions only in the condition part of a comprehension.\nUsing an assignment expression also works the same way in gener-\nator expressions (see Item 32: “Consider Generator Expressions for \nLarge List Comprehensions”). Here, I create an iterator of pairs con-\ntaining the item name and the current count in stock instead of a \ndict instance:\nfound = ((name, batches) for name in order\n         if (batches := get_batches(stock.get(name, 0), 8)))\nprint(next(found))\nprint(next(found))\n>>>\n('screws', 4)\n('wingnuts', 1)\nThings to Remember\n✦ Assignment expressions make it possible for comprehensions and \ngenerator expressions to reuse the value from one condition else-\nwhere in the same comprehension, which can improve readability \nand performance.\n✦ Although it’s possible to use an assignment expression outside of \na comprehension or generator expression’s condition, you should \navoid doing so.\nItem 30:  Consider Generators Instead of Returning \nLists\nThe simplest choice for a function that produces a sequence of results \nis to return a list of items. For example, say that I want to find the \n\n\n \nItem 30: Consider Generators Instead of Returning Lists \n115\nindex of every word in a string. Here, I accumulate results in a list \nusing the append method and return it at the end of the function:\ndef index_words(text):\n    result = []\n    if text:\n        result.append(0)\n    for index, letter in enumerate(text):\n        if letter == ' ':\n            result.append(index + 1)\n    return result\nThis works as expected for some sample input:\naddress = 'Four score and seven years ago...'\nresult = index_words(address)\nprint(result[:10])\n>>>\n[0, 5, 11, 15, 21, 27, 31, 35, 43, 51]\nThere are two problems with the index_words function.\nThe first problem is that the code is a bit dense and noisy. Each time \na new result is found, I call the append method. The method call’s \nbulk (result.append) deemphasizes the value being added to the list \n(index + 1). There is one line for creating the result list and another \nfor returning it. While the function body contains ~130 characters \n(without whitespace), only ~75 characters are important.\nA better way to write this function is by using a generator. Generators \nare produced by functions that use yield expressions. Here, I define a \ngenerator function that produces the same results as before:\ndef index_words_iter(text):\n    if text:\n        yield 0\n    for index, letter in enumerate(text):\n        if letter == ' ':\n            yield index + 1\nWhen called, a generator function does not actually run but instead \nimmediately returns an iterator. With each call to the next built-in \nfunction, the iterator advances the generator to its next yield expres-\nsion. Each value passed to yield by the generator is returned by the \niterator to the caller:\nit = index_words_iter(address)\nprint(next(it))\nprint(next(it))\n\n\n116 \nChapter 4 Comprehensions and Generators\n>>>\n0\n5\nThe index_words_iter function is significantly easier to read because \nall interactions with the result list have been eliminated. Results are \npassed to yield expressions instead. You can easily convert the itera-\ntor returned by the generator to a list by passing it to the list built-in \nfunction if necessary (see Item 32: “Consider Generator Expressions \nfor Large List Comprehensions” for how this works):\nresult = list(index_words_iter(address))\nprint(result[:10])\n>>>\n[0, 5, 11, 15, 21, 27, 31, 35, 43, 51]\nThe second problem with index_words is that it requires all results to \nbe stored in the list before being returned. For huge inputs, this can \ncause a program to run out of memory and crash.\nIn contrast, a generator version of this function can easily be adapted \nto take inputs of arbitrary length due to its bounded memory require-\nments. For example, here I define a generator that streams input from \na file one line at a time and yields outputs one word at a time:\ndef index_file(handle):\n    offset = 0\n    for line in handle:\n        if line:\n            yield offset\n        for letter in line:\n            offset += 1\n            if letter == ' ':\n                yield offset\nThe working memory for this function is limited to the maximum \nlength of one line of input. Running the generator produces the same \nresults (see Item 36: “Consider itertools for Working with Iterators \nand Generators” for more about the islice function):\nwith open('address.txt', 'r') as f:\n    it = index_file(f)\n    results = itertools.islice(it, 0, 10)\n    print(list(results))\n>>>\n[0, 5, 11, 15, 21, 27, 31, 35, 43, 51]\n\n\n \nItem 31: Be Defensive When Iterating Over Arguments \n117\nThe only gotcha with defining generators like this is that the callers \nmust be aware that the iterators returned are stateful and can’t be \nreused (see Item 31: “Be Defensive When Iterating Over Arguments”).\nThings to Remember\n✦ Using generators can be clearer than the alternative of having a \nfunction return a list of accumulated results.\n✦ The iterator returned by a generator produces the set of values \npassed to yield expressions within the generator function’s body.\n✦ Generators can produce a sequence of outputs for arbitrarily large \ninputs because their working memory doesn’t include all inputs and \noutputs.\nItem 31: Be Defensive When Iterating Over Arguments\nWhen a function takes a list of objects as a parameter, it’s often \nimportant to iterate over that list multiple times. For example, say \nthat I want to analyze tourism numbers for the U.S. state of Texas. \nImagine that the data set is the number of visitors to each city (in mil-\nlions per year). I’d like to figure out what percentage of overall tourism \neach city receives.\nTo do this, I need a normalization function that sums the inputs to \ndetermine the total number of tourists per year and then divides each \ncity’s individual visitor count by the total to find that city’s contribu-\ntion to the whole:\ndef normalize(numbers):\n    total = sum(numbers)\n    result = []\n    for value in numbers:\n        percent = 100 * value / total\n        result.append(percent)\n    return result\nThis function works as expected when given a list of visits:\nvisits = [15, 35, 80]\npercentages = normalize(visits)\nprint(percentages)\nassert sum(percentages) == 100.0\n>>>\n[11.538461538461538, 26.923076923076923, 61.53846153846154]\n\n\n118 \nChapter 4 Comprehensions and Generators\nTo scale this up, I need to read the data from a file that contains every \ncity in all of Texas. I define a generator to do this because then I can \nreuse the same function later, when I want to compute tourism num-\nbers for the whole world—a much larger data set with higher memory \nrequirements (see Item 30: “Consider Generators Instead of Returning \nLists” for background):\ndef read_visits(data_path):\n    with open(data_path) as f:\n        for line in f:\n            yield int(line)\nSurprisingly, calling normalize on the read_visits generator’s return \nvalue produces no results:\nit = read_visits('my_numbers.txt')\npercentages = normalize(it)\nprint(percentages)\n>>>\n[]\nThis behavior occurs because an iterator produces its results only \na single time. If you iterate over an iterator or a generator that has \nalready raised a StopIteration exception, you won’t get any results \nthe second time around:\nit = read_visits('my_numbers.txt')\nprint(list(it))\nprint(list(it))  # Already exhausted\n>>>\n[15, 35, 80]\n[]\nConfusingly, you also won’t get errors when you iterate over an \nalready exhausted iterator. for loops, the list constructor, and many \nother functions throughout the Python standard library expect the \nStopIteration exception to be raised during normal operation. These \nfunctions can’t tell the difference between an iterator that has no out-\nput and an iterator that had output and is now exhausted.\nTo solve this problem, you can explicitly exhaust an input iterator and \nkeep a copy of its entire contents in a list. You can then iterate over \nthe list version of the data as many times as you need to. Here’s the \nsame function as before, but it defensively copies the input iterator:\ndef normalize_copy(numbers):\n    numbers_copy = list(numbers)  # Copy the iterator\n\n\n \nItem 31: Be Defensive When Iterating Over Arguments \n119\n    total = sum(numbers_copy)\n    result = []\n    for value in numbers_copy:\n        percent = 100 * value / total\n        result.append(percent)\n    return result\nNow the function works correctly on the read_visits generator’s \nreturn value:\nit = read_visits('my_numbers.txt')\npercentages = normalize_copy(it)\nprint(percentages)\nassert sum(percentages) == 100.0\n>>>\n[11.538461538461538, 26.923076923076923, 61.53846153846154]\nThe problem with this approach is that the copy of the input iterator’s \ncontents could be extremely large. Copying the iterator could cause \nthe program to run out of memory and crash. This potential for scal-\nability issues undermines the reason that I wrote read_visits as a \ngenerator in the first place. One way around this is to accept a func-\ntion that returns a new iterator each time it’s called:\ndef normalize_func(get_iter):\n    total = sum(get_iter())   # New iterator\n    result = []\n    for value in get_iter():  # New iterator\n        percent = 100 * value / total\n        result.append(percent)\n    return result\nTo use normalize_func, I can pass in a lambda expression that calls \nthe generator and produces a new iterator each time:\npath = 'my_numbers.txt'\npercentages = normalize_func(lambda: read_visits(path))\nprint(percentages)\nassert sum(percentages) == 100.0\n>>>\n[11.538461538461538, 26.923076923076923, 61.53846153846154]\nAlthough this works, having to pass a lambda function like this is \nclumsy. A better way to achieve the same result is to provide a new \ncontainer class that implements the iterator protocol.\n\n\n120 \nChapter 4 Comprehensions and Generators\nThe iterator protocol is how Python for loops and related expressions \ntraverse the contents of a container type. When Python sees a state-\nment like for x in foo, it actually calls iter(foo). The iter built-in \nfunction calls the foo.__iter__ special method in turn. The __iter__ \nmethod must return an iterator object (which itself implements the \n__next__ special method). Then, the for loop repeatedly calls the \nnext built-in function on the iterator object until it’s exhausted (indi-\ncated by raising a StopIteration exception).\nIt sounds complicated, but practically speaking, you can achieve all of \nthis behavior for your classes by implementing the __iter__ method \nas a generator. Here, I define an iterable container class that reads \nthe file containing tourism data:\nclass ReadVisits:\n    def __init__(self, data_path):\n        self.data_path = data_path\n \n    def __iter__(self):\n        with open(self.data_path) as f:\n            for line in f:\n                yield int(line)\nThis new container type works correctly when passed to the original \nfunction without modifications:\nvisits = ReadVisits(path)\npercentages = normalize(visits)\nprint(percentages)\nassert sum(percentages) == 100.0\n>>>\n[11.538461538461538, 26.923076923076923, 61.53846153846154]\nThis \nworks \nbecause \nthe \nsum \nmethod \nin \nnormalize \ncalls \nReadVisits.__iter__ to allocate a new iterator object. The for loop to \nnormalize the numbers also calls __iter__ to allocate a second iter-\nator object. Each of those iterators will be advanced and exhausted \nindependently, ensuring that each unique iteration sees all of the \ninput data values. The only downside of this approach is that it reads \nthe input data multiple times.\nNow that you know how containers like ReadVisits work, you can \nwrite your functions and methods to ensure that parameters aren’t \njust iterators. The protocol states that when an iterator is passed \nto the iter built-in function, iter returns the iterator itself. In con-\ntrast, when a container type is passed to iter, a new iterator object is \n\n\n \nItem 31: Be Defensive When Iterating Over Arguments \n121\nreturned each time. Thus, you can test an input value for this behav-\nior and raise a TypeError to reject arguments that can’t be repeatedly \niterated over:\ndef normalize_defensive(numbers):\n    if iter(numbers) is numbers:  # An iterator -- bad!\n        raise TypeError('Must supply a container')\n    total = sum(numbers)\n    result = []\n    for value in numbers:\n        percent = 100 * value / total\n        result.append(percent)\n    return result\nAlternatively, the collections.abc built-in module defines an Iterator \nclass that can be used in an isinstance test to recognize the potential \nproblem (see Item 43: “Inherit from collections.abc for Custom Con-\ntainer Types”):\nfrom collections.abc import Iterator\ndef normalize_defensive(numbers):\n    if isinstance(numbers, Iterator):  # Another way to check\n        raise TypeError('Must supply a container')\n    total = sum(numbers)\n    result = []\n    for value in numbers:\n        percent = 100 * value / total\n        result.append(percent)\n    return result\nThe approach of using a container is ideal if you don’t want to copy \nthe full input iterator, as with the normalize_copy function above, but \nyou also need to iterate over the input data multiple times. This func-\ntion works as expected for list and ReadVisits inputs because they \nare iterable containers that follow the iterator protocol:\nvisits = [15, 35, 80]\npercentages = normalize_defensive(visits)\nassert sum(percentages) == 100.0\nvisits = ReadVisits(path)\npercentages = normalize_defensive(visits)\nassert sum(percentages) == 100.0\n\n\n122 \nChapter 4 Comprehensions and Generators\nThe function raises an exception if the input is an iterator rather than \na container:\nvisits = [15, 35, 80]\nit = iter(visits)\nnormalize_defensive(it)\n>>>\nTraceback ...\nTypeError: Must supply a container\nThe same approach can also be used for asynchronous iterators (see \nItem 61: “Know How to Port Threaded I/O to asyncio” for an example).\nThings to Remember\n✦ Beware of functions and methods that iterate over input argu-\nments multiple times. If these arguments are iterators, you may see \nstrange behavior and missing values.\n✦ Python’s iterator protocol defines how containers and iterators inter-\nact with the iter and next built-in functions, for loops, and related \nexpressions.\n✦ You can easily define your own iterable container type by imple-\nmenting the __iter__ method as a generator.\n✦ You can detect that a value is an iterator (instead of a container) \nif calling iter on it produces the same value as what you passed \nin. Alternatively, you can use the isinstance built-in function along \nwith the collections.abc.Iterator class.\nItem 32:  Consider Generator Expressions for Large \nList Comprehensions\nThe problem with list comprehensions (see Item 27: “Use Comprehen-\nsions Instead of map and filter”) is that they may create new list \ninstances containing one item for each value in input sequences. This \nis fine for small inputs, but for large inputs, this behavior could con-\nsume significant amounts of memory and cause a program to crash.\nFor example, say that I want to read a file and return the number of \ncharacters on each line. Doing this with a list comprehension would \nrequire holding the length of every line of the file in memory. If the \nfile is enormous or perhaps a never-ending network socket, using list \ncomprehensions would be problematic. Here, I use a list comprehen-\nsion in a way that can only handle small input values:\nvalue = [len(x) for x in open('my_file.txt')]\nprint(value)\n\n\n Item 32: Consider Generator Expressions for Large List Comprehensions \n123\n>>>\n[100, 57, 15, 1, 12, 75, 5, 86, 89, 11]\nTo solve this issue, Python provides generator expressions, which are \na generalization of list comprehensions and generators. Generator \nexpressions don’t materialize the whole output sequence when they’re \nrun. Instead, generator expressions evaluate to an iterator that yields \none item at a time from the expression.\nYou create a generator expression by putting list-comprehension-like \nsyntax between () characters. Here, I use a generator expression \nthat is equivalent to the code above. However, the generator expres-\nsion immediately evaluates to an iterator and doesn’t make forward \nprogress:\nit = (len(x) for x in open('my_file.txt'))\nprint(it)\n>>>\n<generator object <genexpr> at 0x108993dd0>\nThe returned iterator can be advanced one step at a time to produce \nthe next output from the generator expression, as needed (using \nthe next built-in function). I can consume as much of the generator \nexpression as I want without risking a blowup in memory usage:\nprint(next(it))\nprint(next(it))\n>>>\n100\n57\nAnother powerful outcome of generator expressions is that they can \nbe composed together. Here, I take the iterator returned by the gen-\nerator expression above and use it as the input for another generator \nexpression:\nroots = ((x, x**0.5) for x in it)\nEach time I advance this iterator, it also advances the interior itera-\ntor, creating a domino effect of looping, evaluating conditional expres-\nsions, and passing around inputs and outputs, all while being as \nmemory efficient as possible:\nprint(next(roots))\n>>>\n(15, 3.872983346207417)\n\n\n124 \nChapter 4 Comprehensions and Generators\nChaining generators together like this executes very quickly in \nPython. When you’re looking for a way to compose functionality that’s \noperating on a large stream of input, generator expressions are a \ngreat choice. The only gotcha is that the iterators returned by gener-\nator expressions are stateful, so you must be careful not to use these \niterators more than once (see Item 31: “Be Defensive When Iterating \nOver Arguments”).\nThings to Remember\n✦ List comprehensions can cause problems for large inputs by using \ntoo much memory.\n✦ Generator expressions avoid memory issues by producing outputs \none at a time as iterators.\n✦ Generator expressions can be composed by passing the iterator from \none generator expression into the for subexpression of another.\n✦ Generator expressions execute very quickly when chained together \nand are memory efficient.\nItem 33: Compose Multiple Generators with yield from\nGenerators provide a variety of benefits (see Item 30: “Consider Gen-\nerators Instead of Returning Lists”) and solutions to common prob-\nlems (see Item 31: “Be Defensive When Iterating Over Arguments”). \nGenerators are so useful that many programs start to look like layers \nof generators strung together.\nFor example, say that I have a graphical program that’s using gener-\nators to animate the movement of images onscreen. To get the visual \neffect I’m looking for, I need the images to move quickly at first, pause \ntemporarily, and then continue moving at a slower pace. Here, I define \ntwo generators that yield the expected onscreen deltas for each part of \nthis animation:\ndef move(period, speed):\n    for _ in range(period):\n        yield speed\n \ndef pause(delay):\n    for _ in range(delay):\n        yield 0\nTo create the final animation, I need to combine move and pause \ntogether to produce a single sequence of onscreen deltas. Here, I do \n\n\n \nItem 33: Compose Multiple Generators with yield from \n125\nthis by calling a generator for each step of the animation, iterating \nover each generator in turn, and then yielding the deltas from all of \nthem in sequence:\ndef animate():\n    for delta in move(4, 5.0):\n        yield delta\n    for delta in pause(3):\n        yield delta\n    for delta in move(2, 3.0):\n        yield delta\nNow, I can render those deltas onscreen as they’re produced by the \nsingle animation generator:\ndef render(delta):\n    print(f'Delta: {delta:.1f}')\n    # Move the images onscreen\n    ...\ndef run(func):\n    for delta in func():\n        render(delta)\nrun(animate)\n>>>\nDelta: 5.0\nDelta: 5.0\nDelta: 5.0\nDelta: 5.0\nDelta: 0.0\nDelta: 0.0\nDelta: 0.0\nDelta: 3.0\nDelta: 3.0\nThe problem with this code is the repetitive nature of the animate \nfunction. The redundancy of the for statements and yield expres-\nsions for each generator adds noise and reduces readability. This \nexample includes only three nested generators and it’s already hurt-\ning clarity; a complex animation with a dozen phases or more would \nbe extremely difficult to follow.\nThe solution to this problem is to use the yield from expression. \nThis advanced generator feature allows you to yield all values from \n",
      "page_number": 134,
      "chapter_number": 14,
      "summary": "This chapter covers segment 14 (pages 134-147). Key topics include iterating, iterate, and iteration. Covers iterator, function. As soon as it gets more complicated \nthan that, you should use normal if and for statements and write a \nhelper function (see Item 30: “Consider Generators Instead of Return-\ning Lists”).",
      "keywords": [
        "Generator Expressions",
        "Generator",
        "Item",
        "List Comprehensions",
        "Large List Comprehensions",
        "iterator",
        "list",
        "Comprehensions",
        "Expressions",
        "function",
        "result",
        "batches",
        "iter",
        "generator function",
        "Comprehensions and Generators"
      ],
      "concepts": [
        "iterating",
        "iterate",
        "iteration",
        "result",
        "lists",
        "expressions",
        "expression",
        "function",
        "functions",
        "functionality"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 32,
          "title": "Segment 32 (pages 637-655)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 8,
          "title": "Segment 8 (pages 62-70)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 9,
          "title": "Segment 9 (pages 71-78)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 31,
          "title": "Segment 31 (pages 615-636)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 37,
          "title": "Segment 37 (pages 333-344)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 148-156)",
      "start_page": 148,
      "end_page": 156,
      "detection_method": "topic_boundary",
      "content": "126 \nChapter 4 Comprehensions and Generators\na nested generator before returning control to the parent generator. \nHere, I reimplement the animation function by using yield from:\ndef animate_composed():\n    yield from move(4, 5.0)\n    yield from pause(3)\n    yield from move(2, 3.0)\n \nrun(animate_composed)\n>>>\nDelta: 5.0\nDelta: 5.0\nDelta: 5.0\nDelta: 5.0\nDelta: 0.0\nDelta: 0.0\nDelta: 0.0\nDelta: 3.0\nDelta: 3.0\nThe result is the same as before, but now the code is clearer and more \nintuitive. yield from essentially causes the Python interpreter to han-\ndle the nested for loop and yield expression boilerplate for you, which \nresults in better performance. Here, I verify the speedup by using the \ntimeit built-in module to run a micro-benchmark:\nimport timeit\n \ndef child():\n    for i in range(1_000_000):\n        yield i\n \ndef slow():\n    for i in child():\n        yield i\n \ndef fast():\n    yield from child()\n \nbaseline = timeit.timeit(\n    stmt='for _ in slow(): pass',\n    globals=globals(),\n    number=50)\nprint(f'Manual nesting {baseline:.2f}s')\n \n\n\n \nItem 34: Avoid Injecting Data into Generators with send \n127\ncomparison = timeit.timeit(\n    stmt='for _ in fast(): pass',\n    globals=globals(),\n    number=50)\nprint(f'Composed nesting {comparison:.2f}s')\n \nreduction = -(comparison - baseline) / baseline\nprint(f'{reduction:.1%} less time')\n>>>\nManual nesting 4.02s\nComposed nesting 3.47s\n13.5% less time\nIf you find yourself composing generators, I strongly encourage you to \nuse yield from when possible.\nThings to Remember\n✦ The yield from expression allows you to compose multiple nested \ngenerators together into a single combined generator.\n✦ yield from provides better performance than manually iterating \nnested generators and yielding their outputs.\nItem 34:  Avoid Injecting Data into Generators \nwith send\nyield expressions provide generator functions with a simple way to \nproduce an iterable series of output values (see Item 30: “Consider \nGenerators Instead of Returning Lists”). However, this channel \nappears to be unidirectional: There’s no immediately obvious way to \nsimultaneously stream data in and out of a generator as it runs. Hav-\ning such bidirectional communication could be valuable for a variety \nof use cases.\nFor example, say that I’m writing a program to transmit signals using \na software-defined radio. Here, I use a function to generate an approx-\nimation of a sine wave with a given number of points:\nimport math\n \ndef wave(amplitude, steps):\n    step_size = 2 * math.pi / steps\n    for step in range(steps):\n        radians = step * step_size\n        fraction = math.sin(radians)\n\n\n128 \nChapter 4 Comprehensions and Generators\n        output = amplitude * fraction\n        yield output\nNow, I can transmit the wave signal at a single specified amplitude by \niterating over the wave generator:\ndef transmit(output):\n    if output is None:\n        print(f'Output is None')\n    else:\n        print(f'Output: {output:>5.1f}')\n \ndef run(it):\n    for output in it:\n        transmit(output)\n \nrun(wave(3.0, 8))\n>>>\nOutput:   0.0\nOutput:   2.1\nOutput:   3.0\nOutput:   2.1\nOutput:   0.0\nOutput:  -2.1\nOutput:  -3.0\nOutput:  -2.1\nThis works fine for producing basic waveforms, but it can’t be used to \nconstantly vary the amplitude of the wave based on a separate input \n(i.e., as required to broadcast AM radio signals). I need a way to mod-\nulate the amplitude on each iteration of the generator.\nPython generators support the send method, which upgrades yield \nexpressions into a two-way channel. The send method can be used to \nprovide streaming inputs to a generator at the same time it’s yielding \noutputs. Normally, when iterating a generator, the value of the yield \nexpression is None:\ndef my_generator():\n    received = yield 1\n    print(f'received = {received}')\n \nit = iter(my_generator())\noutput = next(it)       # Get first generator output\nprint(f'output = {output}')\n \n\n\n \nItem 34: Avoid Injecting Data into Generators with send \n129\ntry:\n    next(it)            # Run generator until it exits\nexcept StopIteration:\n    pass\n>>>\noutput = 1\nreceived = None\nWhen I call the send method instead of iterating the generator with a \nfor loop or the next built-in function, the supplied parameter becomes \nthe value of the yield expression when the generator is resumed. How-\never, when the generator first starts, a yield expression has not been \nencountered yet, so the only valid value for calling send initially is \nNone (any other argument would raise an exception at runtime):\nit = iter(my_generator())\noutput = it.send(None)  # Get first generator output\nprint(f'output = {output}')\n \ntry:\n    it.send('hello!')   # Send value into the generator\nexcept StopIteration:\n    pass\n>>>\noutput = 1\nreceived = hello!\nI can take advantage of this behavior in order to modulate the ampli-\ntude of the sine wave based on an input signal. First, I need to change \nthe wave generator to save the amplitude returned by the yield expres-\nsion and use it to calculate the next generated output:\ndef wave_modulating(steps):\n    step_size = 2 * math.pi / steps\n    amplitude = yield             # Receive initial amplitude\n    for step in range(steps):\n        radians = step * step_size\n        fraction = math.sin(radians)\n        output = amplitude * fraction\n        amplitude = yield output  # Receive next amplitude\nThen, I need to update the run function to stream the modulating \namplitude into the wave_modulating generator on each iteration. The \n\n\n130 \nChapter 4 Comprehensions and Generators\nfirst input to send must be None, since a yield expression would not \nhave occurred within the generator yet:\ndef run_modulating(it):\n    amplitudes = [\n        None, 7, 7, 7, 2, 2, 2, 2, 10, 10, 10, 10, 10]\n    for amplitude in amplitudes:\n        output = it.send(amplitude)\n        transmit(output)\n \nrun_modulating(wave_modulating(12))\n>>>\nOutput is None\nOutput:   0.0\nOutput:   3.5\nOutput:   6.1\nOutput:   2.0\nOutput:   1.7\nOutput:   1.0\nOutput:   0.0\nOutput:  -5.0\nOutput:  -8.7\nOutput: -10.0\nOutput:  -8.7\nOutput:  -5.0\nThis works; it properly varies the output amplitude based on the input \nsignal. The first output is None, as expected, because a value for the \namplitude wasn’t received by the generator until after the initial yield \nexpression.\nOne problem with this code is that it’s difficult for new readers to \nunderstand: Using yield on the right side of an assignment statement \nisn’t intuitive, and it’s hard to see the connection between yield and \nsend without already knowing the details of this advanced generator \nfeature.\nNow, imagine that the program’s requirements get more complicated. \nInstead of using a simple sine wave as my carrier, I need to use a \ncomplex waveform consisting of multiple signals in sequence. One \nway to implement this behavior is by composing multiple generators \ntogether by using the yield from expression (see Item 33: “Compose \nMultiple Generators with yield from”). Here, I confirm that this works \nas expected in the simpler case where the amplitude is fixed:\ndef complex_wave():\n    yield from wave(7.0, 3)\n\n\n \nItem 34: Avoid Injecting Data into Generators with send \n131\n    yield from wave(2.0, 4)\n    yield from wave(10.0, 5)\n \nrun(complex_wave())\n>>>\nOutput:   0.0\nOutput:   6.1\nOutput:  -6.1\nOutput:   0.0\nOutput:   2.0\nOutput:   0.0\nOutput:  -2.0\nOutput:   0.0\nOutput:   9.5\nOutput:   5.9\nOutput:  -5.9\nOutput:  -9.5\nGiven that the yield from expression handles the simpler case, you \nmay expect it to also work properly along with the generator send \nmethod. Here, I try to use it this way by composing multiple calls to \nthe wave_modulating generator together:\ndef complex_wave_modulating():\n    yield from wave_modulating(3)\n    yield from wave_modulating(4)\n    yield from wave_modulating(5)\n \nrun_modulating(complex_wave_modulating())\n>>>\nOutput is None\nOutput:   0.0\nOutput:   6.1\nOutput:  -6.1\nOutput is None\nOutput:   0.0\nOutput:   2.0\nOutput:   0.0\nOutput: -10.0\nOutput is None\nOutput:   0.0\nOutput:   9.5\nOutput:   5.9\n\n\n132 \nChapter 4 Comprehensions and Generators\nThis works to some extent, but the result contains a big surprise: \nThere are many None values in the output! Why does this happen? \nWhen each yield from expression finishes iterating over a nested gen-\nerator, it moves on to the next one. Each nested generator starts with \na bare yield expression—one without a value—in order to receive the \ninitial amplitude from a generator send method call. This causes the \nparent generator to output a None value when it transitions between \nchild generators.\nThis means that assumptions about how the yield from and send \nfeatures behave individually will be broken if you try to use them \ntogether. Although it’s possible to work around this None problem \nby increasing the complexity of the run_modulating function, it’s not \nworth the trouble. It’s already difficult for new readers of the code to \nunderstand how send works. This surprising gotcha with yield from \nmakes it even worse. My advice is to avoid the send method entirely \nand go with a simpler approach.\nThe easiest solution is to pass an iterator into the wave function. The \niterator should return an input amplitude each time the next built-in \nfunction is called on it. This arrangement ensures that each genera-\ntor is progressed in a cascade as inputs and outputs are processed \n(see Item 32: “Consider Generator Expressions for Large List Compre-\nhensions” for another example):\ndef wave_cascading(amplitude_it, steps):\n    step_size = 2 * math.pi / steps\n    for step in range(steps):\n        radians = step * step_size\n        fraction = math.sin(radians)\n        amplitude = next(amplitude_it)  # Get next input\n        output = amplitude * fraction\n        yield output\nI can pass the same iterator into each of the generator functions that \nI’m trying to compose together. Iterators are stateful (see Item 31: “Be \nDefensive When Iterating Over Arguments”), and thus each of the \nnested generators picks up where the previous generator left off:\ndef complex_wave_cascading(amplitude_it):\n    yield from wave_cascading(amplitude_it, 3)\n    yield from wave_cascading(amplitude_it, 4)\n    yield from wave_cascading(amplitude_it, 5)\nNow, I can run the composed generator by simply passing in an itera-\ntor from the amplitudes list:\ndef run_cascading():\n    amplitudes = [7, 7, 7, 2, 2, 2, 2, 10, 10, 10, 10, 10]\n\n\n \nItem 35: Avoid Causing State Transitions in Generators with throw \n133\n    it = complex_wave_cascading(iter(amplitudes))\n    for amplitude in amplitudes:\n        output = next(it)\n        transmit(output)\n \nrun_cascading()\n>>>\nOutput:   0.0\nOutput:   6.1\nOutput:  -6.1\nOutput:   0.0\nOutput:   2.0\nOutput:   0.0\nOutput:  -2.0\nOutput:   0.0\nOutput:   9.5\nOutput:   5.9\nOutput:  -5.9\nOutput:  -9.5\nThe best part about this approach is that the iterator can come from \nanywhere and could be completely dynamic (e.g., implemented using \na generator function). The only downside is that this code assumes \nthat the input generator is completely thread safe, which may not be \nthe case. If you need to cross thread boundaries, async functions may \nbe a better fit (see Item 62: “Mix Threads and Coroutines to Ease the \nTransition to asyncio”).\nThings to Remember\n✦ The send method can be used to inject data into a generator by giv-\ning the yield expression a value that can be assigned to a variable.\n✦ Using send with yield from expressions may cause surprising \nbehavior, such as None values appearing at unexpected times in the \ngenerator output.\n✦ Providing an input iterator to a set of composed generators is a bet-\nter approach than using the send method, which should be avoided.\nItem 35:  Avoid Causing State Transitions in \nGenerators with throw\nIn addition to yield from expressions (see Item 33: “Compose Multi-\nple Generators with yield from”) and the send method (see Item 34: \n“Avoid Injecting Data into Generators with send”), another advanced \n\n\n134 \nChapter 4 Comprehensions and Generators\ngenerator feature is the throw method for re-raising Exception \ninstances within generator functions. The way throw works is simple: \nWhen the method is called, the next occurrence of a yield expression \nre-raises the provided Exception instance after its output is received \ninstead of continuing normally. Here, I show a simple example of this \nbehavior in action:\nclass MyError(Exception):\n    pass\n \ndef my_generator():\n    yield 1\n    yield 2\n    yield 3\n \nit = my_generator()\nprint(next(it))  # Yield 1\nprint(next(it))  # Yield 2\nprint(it.throw(MyError('test error')))\n>>>\n1\n2\nTraceback ...\nMyError: test error\nWhen you call throw, the generator function may catch the injected \nexception with a standard try/except compound statement that sur-\nrounds the last yield expression that was executed (see Item 65: \n“Take Advantage of Each Block in try/except/else/finally” for more \nabout exception handling):\ndef my_generator():\n    yield 1\n \n    try:\n        yield 2\n    except MyError:\n        print('Got MyError!')\n    else:\n        yield 3\n \n    yield 4\n \nit = my_generator()\nprint(next(it))  # Yield 1\n",
      "page_number": 148,
      "chapter_number": 15,
      "summary": "✦ yield from provides better performance than manually iterating \nnested generators and yielding their outputs Key topics include generators, generate, and generated. Covers generator.",
      "keywords": [
        "output",
        "generator",
        "yield",
        "delta",
        "Avoid Injecting Data",
        "wave",
        "amplitude",
        "Item",
        "send",
        "yield expression",
        "Avoid Injecting",
        "Injecting Data",
        "run",
        "generator output",
        "expression"
      ],
      "concepts": [
        "generators",
        "generate",
        "generated",
        "outputs",
        "yield",
        "amplitude",
        "send",
        "wave",
        "run",
        "runs"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 32,
          "title": "Segment 32 (pages 637-655)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 19,
          "title": "Segment 19 (pages 159-166)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 33,
          "title": "Segment 33 (pages 656-676)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 11,
          "title": "Segment 11 (pages 209-230)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 36,
          "title": "Segment 36 (pages 725-745)",
          "relevance_score": 0.46,
          "method": "api"
        }
      ]
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 157-164)",
      "start_page": 157,
      "end_page": 164,
      "detection_method": "topic_boundary",
      "content": " \nItem 35: Avoid Causing State Transitions in Generators with throw \n135\nprint(next(it))  # Yield 2\nprint(it.throw(MyError('test error')))\n>>>\n1\n2\nGot MyError!\n4\nThis functionality provides a two-way communication channel \nbetween a generator and its caller that can be useful in certain situ-\nations (see Item 34: “Avoid Injecting Data into Generators with send” \nfor another one). For example, imagine that I’m trying to write a pro-\ngram with a timer that supports sporadic resets. Here, I implement \nthis behavior by defining a generator that relies on the throw method:\nclass Reset(Exception):\n    pass\n \ndef timer(period):\n    current = period\n    while current:\n        current -= 1\n        try:\n            yield current\n        except Reset:\n            current = period\nIn this code, whenever the Reset exception is raised by the yield \nexpression, the counter resets itself to its original period.\nI can connect this counter reset event to an external input that’s \npolled every second. Then, I can define a run function to drive the \ntimer generator, which injects exceptions with throw to cause resets, \nor calls announce for each generator output:\ndef check_for_reset():\n    # Poll for external event\n    ...\n \ndef announce(remaining):\n    print(f'{remaining} ticks remaining')\n \ndef run():\n    it = timer(4)\n    while True:\n\n\n136 \nChapter 4 Comprehensions and Generators\n        try:\n            if check_for_reset():\n                current = it.throw(Reset())\n            else:\n                current = next(it)\n        except StopIteration:\n            break\n        else:\n            announce(current)\n \nrun()\n>>>\n3 ticks remaining\n2 ticks remaining\n1 ticks remaining\n3 ticks remaining\n2 ticks remaining\n3 ticks remaining\n2 ticks remaining\n1 ticks remaining\n0 ticks remaining\nThis code works as expected, but it’s much harder to read than nec-\nessary. The various levels of nesting required to catch StopIteration \nexceptions or decide to throw, call next, or announce make the code \nnoisy.\nA simpler approach to implementing this functionality is to define a \nstateful closure (see Item 38: “Accept Functions Instead of Classes for \nSimple Interfaces”) using an iterable container object (see Item 31: “Be \nDefensive When Iterating Over Arguments”). Here, I redefine the timer \ngenerator by using such a class:\nclass Timer:\n    def __init__(self, period):\n        self.current = period\n        self.period = period\n \n    def reset(self):\n        self.current = self.period\n \n    def __iter__(self):\n        while self.current:\n            self.current -= 1\n            yield self.current\n\n\n \nItem 35: Avoid Causing State Transitions in Generators with throw \n137\nNow, the run method can do a much simpler iteration by using a for \nstatement, and the code is much easier to follow because of the reduc-\ntion in the levels of nesting:\ndef run():\n    timer = Timer(4)\n    for current in timer:\n        if check_for_reset():\n            timer.reset()\n        announce(current)\n \nrun()\n>>>\n3 ticks remaining\n2 ticks remaining\n1 ticks remaining\n3 ticks remaining\n2 ticks remaining\n3 ticks remaining\n2 ticks remaining\n1 ticks remaining\n0 ticks remaining\nThe output matches the earlier version using throw, but this imple-\nmentation is much easier to understand, especially for new readers \nof the code. Often, what you’re trying to accomplish by mixing gen-\nerators and exceptions is better achieved with asynchronous fea-\ntures (see Item 60: “Achieve Highly Concurrent I/O with Coroutines”). \nThus, I suggest that you avoid using throw entirely and instead use \nan iterable class if you need this type of exceptional behavior.\nThings to Remember\n✦ The throw method can be used to re-raise exceptions within \n generators at the position of the most recently executed yield \nexpression.\n✦ Using throw harms readability because it requires additional nest-\ning and boilerplate in order to raise and catch exceptions.\n✦ A better way to provide exceptional behavior in generators is to use \na class that implements the __iter__ method along with methods to \ncause exceptional state transitions. \n\n\n138 \nChapter 4 Comprehensions and Generators\nItem 36:  Consider itertools for Working with Iterators \nand Generators\nThe itertools built-in module contains a large number of functions \nthat are useful for organizing and interacting with iterators (see Item \n30: “Consider Generators Instead of Returning Lists” and Item 31: “Be \nDefensive When Iterating Over Arguments” for background):\nimport itertools\nWhenever you find yourself dealing with tricky iteration code, it’s \nworth looking at the itertools documentation again to see if there’s \nanything in there for you to use (see help(itertools)). The following \nsections describe the most important functions that you should know \nin three primary categories.\nLinking Iterators Together\nThe itertools built-in module includes a number of functions for \nlinking iterators together.\nchain\nUse chain to combine multiple iterators into a single sequential \niterator:\nit = itertools.chain([1, 2, 3], [4, 5, 6])\nprint(list(it))\n>>>\n[1, 2, 3, 4, 5, 6]\nrepeat\nUse repeat to output a single value forever, or use the second param-\neter to specify a maximum number of times:\nit = itertools.repeat('hello', 3)\nprint(list(it))\n>>>\n['hello', 'hello', 'hello']\ncycle\nUse cycle to repeat an iterator’s items forever:\nit = itertools.cycle([1, 2])\nresult = [next(it) for _ in range (10)]\nprint(result)\n\n\n \nItem 36: Consider itertools for Working with Iterators and Generators \n139\n>>>\n[1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\ntee\nUse tee to split a single iterator into the number of parallel iterators \nspecified by the second parameter. The memory usage of this func-\ntion will grow if the iterators don’t progress at the same speed since \nbuffering will be required to enqueue the pending items:\nit1, it2, it3 = itertools.tee(['first', 'second'], 3)\nprint(list(it1))\nprint(list(it2))\nprint(list(it3))\n>>>\n['first', 'second']\n['first', 'second']\n['first', 'second']\nzip_longest\nThis variant of the zip built-in function (see Item 8: “Use zip to \n Process Iterators in Parallel”) returns a placeholder value when an \niterator is exhausted, which may happen if iterators have different \nlengths:\nkeys = ['one', 'two', 'three']\nvalues = [1, 2]\n \nnormal = list(zip(keys, values))\nprint('zip:        ', normal)\n \nit = itertools.zip_longest(keys, values, fillvalue='nope')\nlongest = list(it)\nprint('zip_longest:', longest)\n>>>\nzip:         [('one', 1), ('two', 2)]\nzip_longest: [('one', 1), ('two', 2), ('three', 'nope')]\nFiltering Items from an Iterator\nThe itertools built-in module includes a number of functions for fil-\ntering items from an iterator.\n\n\n140 \nChapter 4 Comprehensions and Generators\nislice\nUse islice to slice an iterator by numerical indexes without copying. \nYou can specify the end, start and end, or start, end, and step sizes, \nand the behavior is similar to that of standard sequence slicing and \nstriding (see Item 11: “Know How to Slice Sequences” and Item 12: \n“Avoid Striding and Slicing in a Single Expression”):\nvalues = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n \nfirst_five = itertools.islice(values, 5)\nprint('First five: ', list(first_five))\n \nmiddle_odds = itertools.islice(values, 2, 8, 2)\nprint('Middle odds:', list(middle_odds))\n>>>\nFirst five:  [1, 2, 3, 4, 5]\nMiddle odds: [3, 5, 7]\ntakewhile\ntakewhile returns items from an iterator until a predicate function \nreturns False for an item:\nvalues = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nless_than_seven = lambda x: x < 7\nit = itertools.takewhile(less_than_seven, values)\nprint(list(it))\n>>>\n[1, 2, 3, 4, 5, 6]\ndropwhile\ndropwhile, which is the opposite of takewhile, skips items from an \niterator until the predicate function returns True for the first time:\nvalues = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nless_than_seven = lambda x: x < 7\nit = itertools.dropwhile(less_than_seven, values)\nprint(list(it))\n>>>\n[7, 8, 9, 10]\n\n\n \nItem 36: Consider itertools for Working with Iterators and Generators \n141\nfilterfalse\nfilterfalse, which is the opposite of the filter built-in function, \nreturns all items from an iterator where a predicate function returns \nFalse:\nvalues = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nevens = lambda x: x % 2 == 0\n \nfilter_result = filter(evens, values)\nprint('Filter:      ', list(filter_result))\n \nfilter_false_result = itertools.filterfalse(evens, values)\nprint('Filter false:', list(filter_false_result))\n>>>\nFilter:       [2, 4, 6, 8, 10]\nFilter false: [1, 3, 5, 7, 9]\nProducing Combinations of Items from Iterators\nThe itertools built-in module includes a number of functions for \n producing combinations of items from iterators.\naccumulate\naccumulate folds an item from the iterator into a running value by \napplying a function that takes two parameters. It outputs the current \naccumulated result for each input value:\nvalues = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nsum_reduce = itertools.accumulate(values)\nprint('Sum:   ', list(sum_reduce))\n \ndef sum_modulo_20(first, second):\n    output = first + second\n    return output % 20\n \nmodulo_reduce = itertools.accumulate(values, sum_modulo_20)\nprint('Modulo:', list(modulo_reduce))\n>>>\nSum:    [1, 3, 6, 10, 15, 21, 28, 36, 45, 55]\nModulo: [1, 3, 6, 10, 15, 1, 8, 16, 5, 15]\nThis is essentially the same as the reduce function from the functools \nbuilt-in module, but with outputs yielded one step at a time. By default \nit sums the inputs if no binary function is specified.\n\n\n142 \nChapter 4 Comprehensions and Generators\nproduct\nproduct returns the Cartesian product of items from one or more iter-\nators, which is a nice alternative to using deeply nested list compre-\nhensions (see Item 28: “Avoid More Than Two Control Subexpressions \nin Comprehensions” for why to avoid those):\nsingle = itertools.product([1, 2], repeat=2)\nprint('Single:  ', list(single))\n \nmultiple = itertools.product([1, 2], ['a', 'b'])\nprint('Multiple:', list(multiple))\n>>>\nSingle:   [(1, 1), (1, 2), (2, 1), (2, 2)]\nMultiple: [(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]\npermutations\npermutations returns the unique ordered permutations of length N \nwith items from an iterator:\nit = itertools.permutations([1, 2, 3, 4], 2)\nprint(list(it))\n>>>\n[(1, 2),\n (1, 3),\n (1, 4),\n (2, 1),\n (2, 3),\n (2, 4),\n (3, 1),\n (3, 2),\n (3, 4),\n (4, 1),\n (4, 2),\n (4, 3)]\ncombinations\ncombinations returns the unordered combinations of length N with \nunrepeated items from an iterator:\nit = itertools.combinations([1, 2, 3, 4], 2)\nprint(list(it))\n>>>\n[(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
      "page_number": 157,
      "chapter_number": 16,
      "summary": "4\nThis functionality provides a two-way communication channel \nbetween a generator and its caller that can be useful in certain situ-\nations (see Item 34: “Avoid Injecting Data into Generators with send” \nfor another one) Key topics include iterating, iteration, and value.",
      "keywords": [
        "ticks remaining",
        "Item",
        "ticks",
        "yield",
        "remaining",
        "Generators",
        "Iterators",
        "list",
        "Generators generator feature",
        "throw",
        "Reset",
        "function",
        "yield expression",
        "timer",
        "Exception"
      ],
      "concepts": [
        "iterating",
        "iteration",
        "value",
        "item",
        "exception",
        "exceptions",
        "exceptional",
        "lists",
        "resets",
        "functions"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 32,
          "title": "Segment 32 (pages 637-655)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 33,
          "title": "Segment 33 (pages 656-676)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 34,
          "title": "Segment 34 (pages 677-699)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 19,
          "title": "Segment 19 (pages 159-166)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 31,
          "title": "Segment 31 (pages 280-288)",
          "relevance_score": 0.5,
          "method": "api"
        }
      ]
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 165-174)",
      "start_page": 165,
      "end_page": 174,
      "detection_method": "topic_boundary",
      "content": " \nItem 36: Consider itertools for Working with Iterators and Generators \n143\ncombinations_with_replacement\ncombinations_with_replacement is the same as combinations, but \nrepeated values are allowed:\nit = itertools.combinations_with_replacement([1, 2, 3, 4], 2)\nprint(list(it))\n>>>\n[(1, 1),\n (1, 2),\n (1, 3),\n (1, 4),\n (2, 2),\n (2, 3),\n (2, 4),\n (3, 3),\n (3, 4),\n (4, 4)]\nThings to Remember\n✦ The itertools functions fall into three main categories for work-\ning with iterators and generators: linking iterators together, filtering \nitems they output, and producing combinations of items.\n✦ There are more advanced functions, additional parameters, and \nuseful recipes available in the documentation at help(itertools).\n\n\nThis page intentionally left blank \n\n\n5\nClasses and \nInterfaces\nAs an object-oriented programming language, Python supports a full \nrange of features, such as inheritance, polymorphism, and encap-\nsulation. Getting things done in Python often requires writing new \nclasses and defining how they interact through their interfaces and \nhierarchies. \nPython’s classes and inheritance make it easy to express a program’s \nintended behaviors with objects. They allow you to improve and \nexpand functionality over time. They provide flexibility in an envi-\nronment of changing requirements. Knowing how to use them well \nenables you to write maintainable code.\nItem 37:  Compose Classes Instead of Nesting Many \nLevels of Built-in Types\nPython’s built-in dictionary type is wonderful for maintaining \ndynamic internal state over the lifetime of an object. By dynamic, \nI mean situations in which you need to do bookkeeping for an unex-\npected set of identifiers. For example, say that I want to record the \ngrades of a set of students whose names aren’t known in advance. \nI can define a class to store the names in a dictionary instead of using \na predefined attribute for each student:\nclass SimpleGradebook:\n    def __init__(self):\n        self._grades = {}\n \n    def add_student(self, name):\n        self._grades[name] = []\n \n    def report_grade(self, name, score):\n        self._grades[name].append(score)\n \n\n\n146 \nChapter 5 Classes and Interfaces\n    def average_grade(self, name):\n        grades = self._grades[name]\n        return sum(grades) / len(grades)\nUsing the class is simple:\nbook = SimpleGradebook()\nbook.add_student('Isaac Newton')\nbook.report_grade('Isaac Newton', 90)\nbook.report_grade('Isaac Newton', 95)\nbook.report_grade('Isaac Newton', 85)\n \nprint(book.average_grade('Isaac Newton'))\n>>>\n90.0\nDictionaries and their related built-in types are so easy to use that \nthere’s a danger of overextending them to write brittle code. For \nexample, say that I want to extend the SimpleGradebook class to keep \na list of grades by subject, not just overall. I can do this by changing \nthe _grades dictionary to map student names (its keys) to yet another \ndictionary (its values). The innermost dictionary will map subjects \n(its keys) to a list of grades (its values). Here, I do this by using a \ndefaultdict instance for the inner dictionary to handle missing sub-\njects (see Item 17: “Prefer defaultdict Over setdefault to Handle Miss-\ning Items in Internal State” for background):\nfrom collections import defaultdict\n \nclass BySubjectGradebook:\n    def __init__(self):\n        self._grades = {}                       # Outer dict\n \n    def add_student(self, name):\n        self._grades[name] = defaultdict(list)  # Inner dict\nThis \nseems \nstraightforward \nenough. \nThe \nreport_grade \nand \naverage_grade methods gain quite a bit of complexity to deal with the \nmultilevel dictionary, but it’s seemingly manageable:\n    def report_grade(self, name, subject, grade):\n        by_subject = self._grades[name]\n        grade_list = by_subject[subject]\n        grade_list.append(grade)\n \n    def average_grade(self, name):\n        by_subject = self._grades[name]\n\n\n \nItem 37: Compose Classes Instead of Nesting Built-in Types \n147\n        total, count = 0, 0\n        for grades in by_subject.values():\n            total += sum(grades)\n            count += len(grades)\n        return total / count\nUsing the class remains simple:\nbook = BySubjectGradebook()\nbook.add_student('Albert Einstein')\nbook.report_grade('Albert Einstein', 'Math', 75)\nbook.report_grade('Albert Einstein', 'Math', 65)\nbook.report_grade('Albert Einstein', 'Gym', 90)\nbook.report_grade('Albert Einstein', 'Gym', 95)\nprint(book.average_grade('Albert Einstein'))\n>>>\n81.25\nNow, imagine that the requirements change again. I also want to \ntrack the weight of each score toward the overall grade in the class \nso that midterm and final exams are more important than pop quiz-\nzes. One way to implement this feature is to change the innermost \n dictionary; instead of mapping subjects (its keys) to a list of grades \n (its values), I can use the tuple of (score, weight) in the values list:\nclass WeightedGradebook:\n    def __init__(self):\n        self._grades = {}\n \n    def add_student(self, name):\n        self._grades[name] = defaultdict(list)\n \n    def report_grade(self, name, subject, score, weight):\n        by_subject = self._grades[name]\n        grade_list = by_subject[subject]\n        grade_list.append((score, weight))\nAlthough the changes to report_grade seem simple—just make the \ngrade list store tuple instances—the average_grade method now has a \nloop within a loop and is difficult to read:\n    def average_grade(self, name):\n        by_subject = self._grades[name]\n \n        score_sum, score_count = 0, 0\n        for subject, scores in by_subject.items():\n            subject_avg, total_weight = 0, 0\n\n\n148 \nChapter 5 Classes and Interfaces\n            for score, weight in scores:\n                subject_avg += score * weight\n                total_weight += weight\n \n            score_sum += subject_avg / total_weight\n            score_count += 1\n \n        return score_sum / score_count\nUsing the class has also gotten more difficult. It’s unclear what all of \nthe numbers in the positional arguments mean:\nbook = WeightedGradebook()\nbook.add_student('Albert Einstein')\nbook.report_grade('Albert Einstein', 'Math', 75, 0.05)\nbook.report_grade('Albert Einstein', 'Math', 65, 0.15)\nbook.report_grade('Albert Einstein', 'Math', 70, 0.80)\nbook.report_grade('Albert Einstein', 'Gym', 100, 0.40)\nbook.report_grade('Albert Einstein', 'Gym', 85, 0.60)\nprint(book.average_grade('Albert Einstein'))\n>>>\n80.25\nWhen you see complexity like this, it’s time to make the leap from \nbuilt-in types like dictionaries, tuples, sets, and lists to a hierarchy of \nclasses.\nIn the grades example, at first I didn’t know I’d need to support \nweighted grades, so the complexity of creating classes seemed unwar-\nranted. Python’s built-in dictionary and tuple types made it easy to \nkeep going, adding layer after layer to the internal bookkeeping. But \nyou should avoid doing this for more than one level of nesting; using \ndictionaries that contain dictionaries makes your code hard to read \nby other programmers and sets you up for a maintenance nightmare.\nAs soon as you realize that your bookkeeping is getting complicated, \nbreak it all out into classes. You can then provide well-defined inter-\nfaces that better encapsulate your data. This approach also enables \nyou to create a layer of abstraction between your interfaces and your \nconcrete implementations.\nRefactoring to Classes\nThere are many approaches to refactoring (see Item 89: “Consider \nwarnings to Refactor and Migrate Usage” for another). In this case, \n\n\nI can start moving to classes at the bottom of the dependency tree: \na single grade. A class seems too heavyweight for such simple infor-\nmation. A tuple, though, seems appropriate because grades are \nimmutable. Here, I use the tuple of (score, weight) to track grades in \na list:\ngrades = []\ngrades.append((95, 0.45))\ngrades.append((85, 0.55))\ntotal = sum(score * weight for score, weight in grades)\ntotal_weight = sum(weight for _, weight in grades)\naverage_grade = total / total_weight\nI used _ (the underscore variable name, a Python convention for \nunused variables) to capture the first entry in each grade’s tuple and \nignore it when calculating the total_weight.\nThe problem with this code is that tuple instances are positional. For \nexample, if I want to associate more information with a grade, such \nas a set of notes from the teacher, I need to rewrite every usage of the \ntwo-tuple to be aware that there are now three items present instead \nof two, which means I need to use _ further to ignore certain indexes:\ngrades = []\ngrades.append((95, 0.45, 'Great job'))\ngrades.append((85, 0.55, 'Better next time'))\ntotal = sum(score * weight for score, weight, _ in grades)\ntotal_weight = sum(weight for _, weight, _ in grades)\naverage_grade = total / total_weight\nThis pattern of extending tuples longer and longer is similar to deep-\nening layers of dictionaries. As soon as you find yourself going longer \nthan a two-tuple, it’s time to consider another approach.\nThe namedtuple type in the collections built-in module does exactly \nwhat I need in this case: It lets me easily define tiny, immutable data \nclasses:\nfrom collections import namedtuple\n \nGrade = namedtuple('Grade', ('score', 'weight'))\nThese classes can be constructed with positional or keyword argu-\nments. The fields are accessible with named attributes. Having named \nattributes makes it easy to move from a namedtuple to a class later if \nthe requirements change again and I need to, say, support mutability \nor behaviors in the simple data containers.\n \nItem 37: Compose Classes Instead of Nesting Built-in Types \n149\n\n\n150 \nChapter 5 Classes and Interfaces\nLimitations of namedtuple\nAlthough namedtuple is useful in many circumstances, it’s import-\nant to understand when it can do more harm than good:\n \n■You can’t specify default argument values for namedtuple \nclasses. This makes them unwieldy when your data may have \nmany optional properties. If you find yourself using more than \na handful of attributes, using the built-in dataclasses module \nmay be a better choice.\n \n■The attribute values of namedtuple instances are still accessi-\nble using numerical indexes and iteration. Especially in exter-\nnalized APIs, this can lead to unintentional usage that makes \nit harder to move to a real class later. If you’re not in control \nof all of the usage of your namedtuple instances, it’s better to \nexplicitly define a new class.\nNext, I can write a class to represent a single subject that contains a \nset of grades:\nclass Subject:\n    def __init__(self):\n        self._grades = []\n \n    def report_grade(self, score, weight):\n        self._grades.append(Grade(score, weight))\n \n    def average_grade(self):\n        total, total_weight = 0, 0\n        for grade in self._grades:\n            total += grade.score * grade.weight\n            total_weight += grade.weight\n        return total / total_weight\nThen, I write a class to represent a set of subjects that are being stud-\nied by a single student:\nclass Student:\n    def __init__(self):\n        self._subjects = defaultdict(Subject)\n \n    def get_subject(self, name):\n        return self._subjects[name]\n \n\n\n \nItem 37: Compose Classes Instead of Nesting Built-in Types \n151\n    def average_grade(self):\n        total, count = 0, 0\n        for subject in self._subjects.values():\n            total += subject.average_grade()\n            count += 1\n        return total / count\nFinally, I’d write a container for all of the students, keyed dynamically \nby their names:\nclass Gradebook:\n    def __init__(self):\n        self._students = defaultdict(Student)\n \n    def get_student(self, name):\n        return self._students[name]\nThe line count of these classes is almost double the previous imple-\nmentation’s size. But this code is much easier to read. The example \ndriving the classes is also more clear and extensible:\nbook = Gradebook()\nalbert = book.get_student('Albert Einstein')\nmath = albert.get_subject('Math')\nmath.report_grade(75, 0.05)\nmath.report_grade(65, 0.15)\nmath.report_grade(70, 0.80)\ngym = albert.get_subject('Gym')\ngym.report_grade(100, 0.40)\ngym.report_grade(85, 0.60)\nprint(albert.average_grade())\n>>>\n80.25\nIt would also be possible to write backward-compatible methods to \nhelp migrate usage of the old API style to the new hierarchy of objects.\nThings to Remember\n✦ Avoid making dictionaries with values that are dictionaries, long \ntuples, or complex nestings of other built-in types.\n✦ Use namedtuple for lightweight, immutable data containers before \nyou need the flexibility of a full class.\n✦ Move your bookkeeping code to using multiple classes when your \ninternal state dictionaries get complicated.\n\n\n152 \nChapter 5 Classes and Interfaces\nItem 38:  Accept Functions Instead of Classes for \nSimple Interfaces\nMany of Python’s built-in APIs allow you to customize behavior by \npassing in a function. These hooks are used by APIs to call back your \ncode while they execute. For example, the list type’s sort method \ntakes an optional key argument that’s used to determine each index’s \nvalue for sorting (see Item 14: “Sort by Complex Criteria Using the key \nParameter” for details). Here, I sort a list of names based on their \nlengths by providing the len built-in function as the key hook:\nnames = ['Socrates', 'Archimedes', 'Plato', 'Aristotle']\nnames.sort(key=len)\nprint(names)\n>>>\n['Plato', 'Socrates', 'Aristotle', 'Archimedes']\nIn other languages, you might expect hooks to be defined by an \nabstract class. In Python, many hooks are just stateless functions \nwith well-defined arguments and return values. Functions are ideal \nfor hooks because they are easier to describe and simpler to define \nthan classes. Functions work as hooks because Python has first-class \nfunctions: Functions and methods can be passed around and refer-\nenced like any other value in the language.\nFor example, say that I want to customize the behavior of the \ndefaultdict class (see Item 17: “Prefer defaultdict Over setdefault to \nHandle Missing Items in Internal State” for background). This data \nstructure allows you to supply a function that will be called with no \narguments each time a missing key is accessed. The function must \nreturn the default value that the missing key should have in the dic-\ntionary. Here, I define a hook that logs each time a key is missing and \nreturns 0 for the default value:\ndef log_missing():\n    print('Key added')\n    return 0\nGiven an initial dictionary and a set of desired increments, I can \ncause the log_missing function to run and print twice (for 'red' and \n'orange'):\nfrom collections import defaultdict\ncurrent = {'green': 12, 'blue': 3}\nincrements = [\n",
      "page_number": 165,
      "chapter_number": 17,
      "summary": "This chapter covers segment 17 (pages 165-174). Key topics include classes, grades, and dictionaries. ✦ There are more advanced functions, additional parameters, and \nuseful recipes available in the documentation at help(itertools).",
      "keywords": [
        "Albert Einstein",
        "grade",
        "Albert",
        "Albert Einstein book.report",
        "Subexpressions in Comprehensions",
        "Classes",
        "Einstein",
        "weight",
        "Albert Einstein math",
        "total",
        "Cartesian product",
        "subject",
        "self.",
        "score",
        "Isaac Newton"
      ],
      "concepts": [
        "classes",
        "grades",
        "dictionaries",
        "items",
        "book",
        "subject",
        "list",
        "types",
        "writing",
        "write"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Effective_Modern_C++",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 38,
          "title": "Segment 38 (pages 345-352)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 49,
          "title": "Segment 49 (pages 993-1010)",
          "relevance_score": 0.65,
          "method": "api"
        }
      ]
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 175-187)",
      "start_page": 175,
      "end_page": 187,
      "detection_method": "topic_boundary",
      "content": " \nItem 38: Accept Functions Instead of Classes for Simple Interfaces \n153\n    ('red', 5),\n    ('blue', 17),\n    ('orange', 9),\n]\nresult = defaultdict(log_missing, current)\nprint('Before:', dict(result))\nfor key, amount in increments:\n    result[key] += amount\nprint('After: ', dict(result))\n>>>\nBefore: {'green': 12, 'blue': 3}\nKey added\nKey added\nAfter:  {'green': 12, 'blue': 20, 'red': 5, 'orange': 9}\nSupplying functions like log_missing makes APIs easy to build and \ntest because it separates side effects from deterministic behavior. For \nexample, say I now want the default value hook passed to defaultdict \nto count the total number of keys that were missing. One way to \nachieve this is by using a stateful closure (see Item 21: “Know How \nClosures Interact with Variable Scope” for details). Here, I define a \nhelper function that uses such a closure as the default value hook:\ndef increment_with_report(current, increments):\n    added_count = 0\n \n    def missing():\n        nonlocal added_count  # Stateful closure\n        added_count += 1\n        return 0\n \n    result = defaultdict(missing, current)\n    for key, amount in increments:\n        result[key] += amount\n \n    return result, added_count\nRunning this function produces the expected result (2), even though \nthe defaultdict has no idea that the missing hook maintains state. \nAnother benefit of accepting simple functions for interfaces is that it’s \neasy to add functionality later by hiding state in a closure:\nresult, count = increment_with_report(current, increments)\nassert count == 2\n\n\n154 \nChapter 5 Classes and Interfaces\nThe problem with defining a closure for stateful hooks is that it’s \nharder to read than the stateless function example. Another approach \nis to define a small class that encapsulates the state you want to \ntrack:\nclass CountMissing:\n    def __init__(self):\n        self.added = 0\n \n    def missing(self):\n        self.added += 1\n        return 0\nIn other languages, you might expect that now defaultdict would \nhave to be modified to accommodate the interface of CountMissing. \nBut in Python, thanks to first-class functions, you can reference \nthe CountMissing.missing method directly on an object and pass it to \ndefaultdict as the default value hook. It’s trivial to have an object \ninstance’s method satisfy a function interface:\ncounter = CountMissing()\nresult = defaultdict(counter.missing, current)  # Method ref\nfor key, amount in increments:\n    result[key] += amount\nassert counter.added == 2\nUsing a helper class like this to provide the behavior of a stateful \nclosure is clearer than using the increment_with_report function, as \nabove. However, in isolation, it’s still not immediately obvious what the \npurpose of the CountMissing class is. Who constructs a CountMissing \nobject? Who calls the missing method? Will the class need other pub-\nlic methods to be added in the future? Until you see its usage with \ndefaultdict, the class is a mystery.\nTo clarify this situation, Python allows classes to define the __call__ \nspecial method. __call__ allows an object to be called just like a func-\ntion. It also causes the callable built-in function to return True for \nsuch an instance, just like a normal function or method. All objects \nthat can be executed in this manner are referred to as callables:\nclass BetterCountMissing:\n    def __init__(self):\n        self.added = 0\n \n    def __call__(self):\n        self.added += 1\n        return 0\n \n\n\n \nItem 39: Use @classmethod Polymorphism to Construct Objects \n155\ncounter = BetterCountMissing()\nassert counter() == 0\nassert callable(counter)\nHere, I use a BetterCountMissing instance as the default value hook for \na defaultdict to track the number of missing keys that were added:\ncounter = BetterCountMissing()\nresult = defaultdict(counter, current)  # Relies on __call__\nfor key, amount in increments:\n    result[key] += amount\nassert counter.added == 2\nThis is much clearer than the CountMissing.missing example. The \n__call__ method indicates that a class’s instances will be used some-\nwhere a function argument would also be suitable (like API hooks). It \ndirects new readers of the code to the entry point that’s responsible \nfor the class’s primary behavior. It provides a strong hint that the goal \nof the class is to act as a stateful closure.\nBest of all, defaultdict still has no view into what’s going on when \nyou use __call__. All that defaultdict requires is a function for the \ndefault value hook. Python provides many different ways to satisfy a \nsimple function interface, and you can choose the one that works best \nfor what you need to accomplish.\nThings to Remember\n✦ Instead of defining and instantiating classes, you can often simply \nuse functions for simple interfaces between components in Python.\n✦ References to functions and methods in Python are first class, \nmeaning they can be used in expressions (like any other type).\n✦ The __call__ special method enables instances of a class to be \ncalled like plain Python functions.\n✦ When you need a function to maintain state, consider defining a \nclass that provides the __call__ method instead of defining a state-\nful closure.\nItem 39:  Use @classmethod Polymorphism to Construct \nObjects Generically\nIn Python, not only do objects support polymorphism, but classes do \nas well. What does that mean, and what is it good for?\nPolymorphism enables multiple classes in a hierarchy to implement \ntheir own unique versions of a method. This means that many classes \n\n\n156 \nChapter 5 Classes and Interfaces\ncan fulfill the same interface or abstract base class while providing \ndifferent functionality (see Item 43: “Inherit from collections.abc for \nCustom Container Types”).\nFor example, say that I’m writing a MapReduce implementation, and \nI want a common class to represent the input data. Here, I define \nsuch a class with a read method that must be defined by subclasses:\nclass InputData:\n    def read(self):\n        raise NotImplementedError\nI also have a concrete subclass of InputData that reads data from a \nfile on disk:\nclass PathInputData(InputData):\n    def __init__(self, path):\n        super().__init__()\n        self.path = path\n \n    def read(self):\n        with open(self.path) as f:\n            return f.read()\nI could have any number of InputData subclasses, like PathInputData, \nand each of them could implement the standard interface for read to \nreturn the data to process. Other InputData subclasses could read \nfrom the network, decompress data transparently, and so on.\nI’d want a similar abstract interface for the MapReduce worker that \nconsumes the input data in a standard way:\nclass Worker:\n    def __init__(self, input_data):\n        self.input_data = input_data\n        self.result = None\n \n    def map(self):\n        raise NotImplementedError\n \n    def reduce(self, other):\n        raise NotImplementedError\nHere, I define a concrete subclass of Worker to implement the specific \nMapReduce function I want to apply—a simple newline counter:\nclass LineCountWorker(Worker):\n    def map(self):\n        data = self.input_data.read()\n        self.result = data.count('\\n')\n \n\n\n \nItem 39: Use @classmethod Polymorphism to Construct Objects \n157\n    def reduce(self, other):\n        self.result += other.result\nIt may look like this implementation is going great, but I’ve reached the \nbiggest hurdle in all of this. What connects all of these pieces? I have \na nice set of classes with reasonable interfaces and abstractions, but \nthat’s only useful once the objects are constructed. What’s responsi-\nble for building the objects and orchestrating the MapReduce?\nThe simplest approach is to manually build and connect the objects \nwith some helper functions. Here, I list the contents of a directory and \nconstruct a PathInputData instance for each file it contains:\nimport os\n \ndef generate_inputs(data_dir):\n    for name in os.listdir(data_dir):\n        yield PathInputData(os.path.join(data_dir, name))\nNext, I create the LineCountWorker instances by using the InputData \ninstances returned by generate_inputs:\ndef create_workers(input_list):\n    workers = []\n    for input_data in input_list:\n        workers.append(LineCountWorker(input_data))\n    return workers\nI execute these Worker instances by fanning out the map step to multi-\nple threads (see Item 53: “Use Threads for Blocking I/O, Avoid for Par-\nallelism” for background). Then, I call reduce repeatedly to combine \nthe results into one final value:\nfrom threading import Thread\n \ndef execute(workers):\n    threads = [Thread(target=w.map) for w in workers]\n    for thread in threads: thread.start()\n    for thread in threads: thread.join()\n \n    first, *rest = workers\n    for worker in rest:\n        first.reduce(worker)\n    return first.result\n\n\n158 \nChapter 5 Classes and Interfaces\nFinally, I connect all the pieces together in a function to run each \nstep:\ndef mapreduce(data_dir):\n    inputs = generate_inputs(data_dir)\n    workers = create_workers(inputs)\n    return execute(workers)\nRunning this function on a set of test input files works great:\nimport os\nimport random\n \ndef write_test_files(tmpdir):\n    os.makedirs(tmpdir)\n    for i in range(100):\n        with open(os.path.join(tmpdir, str(i)), 'w') as f:\n            f.write('\\n' * random.randint(0, 100))\n \ntmpdir = 'test_inputs'\nwrite_test_files(tmpdir)\n \nresult = mapreduce(tmpdir)\nprint(f'There are {result} lines')\n>>>\nThere are 4360 lines\nWhat’s the problem? The huge issue is that the mapreduce func-\ntion is not generic at all. If I wanted to write another InputData or \nWorker subclass, I would also have to rewrite the generate_inputs, \ncreate_workers, and mapreduce functions to match.\nThis problem boils down to needing a generic way to construct objects. \nIn other languages, you’d solve this problem with constructor poly-\nmorphism, requiring that each InputData subclass provides a spe-\ncial constructor that can be used generically by the helper methods \nthat orchestrate the MapReduce (similar to the factory pattern). The \ntrouble is that Python only allows for the single constructor method \n__init__. It’s unreasonable to require every InputData subclass to \nhave a compatible constructor.\nThe best way to solve this problem is with class method polymor-\nphism. This is exactly like the instance method polymorphism I used \nfor InputData.read, except that it’s for whole classes instead of their \nconstructed objects.\n\n\n \nItem 39: Use @classmethod Polymorphism to Construct Objects \n159\nLet me apply this idea to the MapReduce classes. Here, I extend the \nInputData class with a generic @classmethod that’s responsible for cre-\nating new InputData instances using a common interface:\nclass GenericInputData:\n    def read(self):\n        raise NotImplementedError\n \n    @classmethod\n    def generate_inputs(cls, config):\n        raise NotImplementedError\nI have generate_inputs take a dictionary with a set of configuration \nparameters that the GenericInputData concrete subclass needs to inter-\npret. Here, I use the config to find the directory to list for input files:\nclass PathInputData(GenericInputData):\n    ...\n \n    @classmethod\n    def generate_inputs(cls, config):\n        data_dir = config['data_dir']\n        for name in os.listdir(data_dir):\n            yield cls(os.path.join(data_dir, name))\nSimilarly, I can make the create_workers helper part of the \nGenericWorker class. Here, I use the input_class parameter, which \nmust be a subclass of GenericInputData, to generate the necessary \ninputs. I construct instances of the GenericWorker concrete subclass \nby using cls() as a generic constructor:\nclass GenericWorker:\n    def __init__(self, input_data):\n        self.input_data = input_data\n        self.result = None\n \n    def map(self):\n        raise NotImplementedError\n \n    def reduce(self, other):\n        raise NotImplementedError\n \n    @classmethod\n    def create_workers(cls, input_class, config):\n        workers = []\n        for input_data in input_class.generate_inputs(config):\n            workers.append(cls(input_data))\n        return workers\n\n\n160 \nChapter 5 Classes and Interfaces\nNote that the call to input_class.generate_inputs above is the \nclass polymorphism that I’m trying to show. You can also see how \ncreate_workers calling cls() provides an alternative way to construct \nGenericWorker objects besides using the __init__ method directly.\nThe effect on my concrete GenericWorker subclass is nothing more \nthan changing its parent class:\nclass LineCountWorker(GenericWorker):\n    ...\nFinally, I can rewrite the mapreduce function to be completely generic \nby calling create_workers:\ndef mapreduce(worker_class, input_class, config):\n    workers = worker_class.create_workers(input_class, config)\n    return execute(workers)\nRunning the new worker on a set of test files produces the same \nresult as the old implementation. The difference is that the mapreduce \nfunction requires more parameters so that it can operate generically:\nconfig = {'data_dir': tmpdir}\nresult = mapreduce(LineCountWorker, PathInputData, config)\nprint(f'There are {result} lines')\n>>>\nThere are 4360 lines\nNow, I can write other GenericInputData and GenericWorker sub-\nclasses as I wish, without having to rewrite any of the glue code.\nThings to Remember\n✦ Python only supports a single constructor per class: the __init__ \nmethod.\n✦ Use @classmethod to define alternative constructors for your classes.\n✦ Use class method polymorphism to provide generic ways to build \nand connect many concrete subclasses.\nItem 40: Initialize Parent Classes with super\nThe old, simple way to initialize a parent class from a child class \nis to directly call the parent class’s __init__ method with the child \ninstance:\nclass MyBaseClass:\n    def __init__(self, value):\n        self.value = value\n \n\n\n \nItem 40: Initialize Parent Classes with super \n161\nclass MyChildClass(MyBaseClass):\n    def __init__(self):\n        MyBaseClass.__init__(self, 5)\nThis approach works fine for basic class hierarchies but breaks in \nmany cases.\nIf a class is affected by multiple inheritance (something to avoid in \ngeneral; see Item 41: “Consider Composing Functionality with Mix-in \nClasses”), calling the superclasses’ __init__ methods directly can \nlead to unpredictable behavior.\nOne problem is that the __init__ call order isn’t specified across all \nsubclasses. For example, here I define two parent classes that operate \non the instance’s value field:\nclass TimesTwo:\n    def __init__(self):\n        self.value *= 2\n \nclass PlusFive:\n    def __init__(self):\n        self.value += 5\nThis class defines its parent classes in one ordering:\nclass OneWay(MyBaseClass, TimesTwo, PlusFive):\n    def __init__(self, value):\n        MyBaseClass.__init__(self, value)\n        TimesTwo.__init__(self)\n        PlusFive.__init__(self)\nAnd constructing it produces a result that matches the parent class \nordering:\nfoo = OneWay(5)\nprint('First ordering value is (5 * 2) + 5 =', foo.value)\n>>>\nFirst ordering value is (5 * 2) + 5 = 15\nHere’s another class that defines the same parent classes but in a \ndifferent ordering (PlusFive followed by TimesTwo instead of the other \nway around):\nclass AnotherWay(MyBaseClass, PlusFive, TimesTwo):\n    def __init__(self, value):\n        MyBaseClass.__init__(self, value)\n        TimesTwo.__init__(self)\n        PlusFive.__init__(self)\n\n\n162 \nChapter 5 Classes and Interfaces\nHowever, I left the calls to the parent class constructors— \nPlusFive.__init__ and TimesTwo.__init__—in the same order as before, \nwhich means this class’s behavior doesn’t match the order of the par-\nent classes in its definition. The conflict here between the inheritance \nbase classes and the __init__ calls is hard to spot, which makes this \nespecially difficult for new readers of the code to understand:\nbar = AnotherWay(5)\nprint('Second ordering value is', bar.value)\n>>>\nSecond ordering value is 15\nAnother problem occurs with diamond inheritance. Diamond inher-\nitance happens when a subclass inherits from two separate classes \nthat have the same superclass somewhere in the hierarchy. Diamond \ninheritance causes the common superclass’s __init__ method to \nrun multiple times, causing unexpected behavior. For example, here \nI define two child classes that inherit from MyBaseClass:\nclass TimesSeven(MyBaseClass):\n    def __init__(self, value):\n        MyBaseClass.__init__(self, value)\n        self.value *= 7\n \nclass PlusNine(MyBaseClass):\n    def __init__(self, value):\n        MyBaseClass.__init__(self, value)\n        self.value += 9\nThen, I define a child class that inherits from both of these classes, \nmaking MyBaseClass the top of the diamond:\nclass ThisWay(TimesSeven, PlusNine):\n    def __init__(self, value):\n        TimesSeven.__init__(self, value)\n        PlusNine.__init__(self, value)\n \nfoo = ThisWay(5)\nprint('Should be (5 * 7) + 9 = 44 but is', foo.value)\n>>>\nShould be (5 * 7) + 9 = 44 but is 14\nThe call to the second parent class’s constructor, PlusNine.__init__, \ncauses self.value to be reset back to 5 when MyBaseClass.__init__ gets \ncalled a second time. That results in the calculation of self.value to be \n5 + 9 = 14, completely ignoring the effect of the TimesSeven.__init__ \n\n\n \nItem 40: Initialize Parent Classes with super \n163\nconstructor. This behavior is surprising and can be very difficult to \ndebug in more complex cases.\nTo solve these problems, Python has the super built-in function and \nstandard method resolution order (MRO). super ensures that common \nsuperclasses in diamond hierarchies are run only once (for another \nexample, see Item 48: “Validate Subclasses with __init_subclass__”). \nThe MRO defines the ordering in which superclasses are initialized, \nfollowing an algorithm called C3 linearization.\nHere, I create a diamond-shaped class hierarchy again, but this time \nI use super to initialize the parent class:\nclass TimesSevenCorrect(MyBaseClass):\n    def __init__(self, value):\n        super().__init__(value)\n        self.value *= 7\n \nclass PlusNineCorrect(MyBaseClass):\n    def __init__(self, value):\n        super().__init__(value)\n        self.value += 9\nNow, the top part of the diamond, MyBaseClass.__init__, is run only a \nsingle time. The other parent classes are run in the order specified in \nthe class statement:\nclass GoodWay(TimesSevenCorrect, PlusNineCorrect):\n    def __init__(self, value):\n        super().__init__(value)\n \nfoo = GoodWay(5)\nprint('Should be 7 * (5 + 9) = 98 and is', foo.value)\n>>>\nShould be 7 * (5 + 9) = 98 and is 98\nThis \norder \nmay \nseem \nbackward \nat \nfirst. \nShouldn’t \nTimesSevenCorrect.__init__ have run first? Shouldn’t the result be \n(5 * 7) + 9 = 44? The answer is no. This ordering matches what the \nMRO defines for this class. The MRO ordering is available on a class \nmethod called mro:\nmro_str = '\\n'.join(repr(cls) for cls in GoodWay.mro())\nprint(mro_str)\n>>>\n<class '__main__.GoodWay'>\n<class '__main__.TimesSevenCorrect'>\n\n\n164 \nChapter 5 Classes and Interfaces\n<class '__main__.PlusNineCorrect'>\n<class '__main__.MyBaseClass'>\n<class 'object'>\nWhen I call GoodWay(5), it in turn calls TimesSevenCorrect.__init__, \nwhich calls PlusNineCorrect.__init__, which calls MyBaseClass.__\ninit__. Once this reaches the top of the diamond, all of the initializa-\ntion methods actually do their work in the opposite order from how \ntheir __init__ functions were called. MyBaseClass.__init__ assigns \nvalue to 5. PlusNineCorrect.__init__ adds 9 to make value equal 14. \nTimesSevenCorrect.__init__ multiplies it by 7 to make value equal 98.\nBesides making multiple inheritance robust, the call to super().\n__init__ \nis \nalso \nmuch \nmore \nmaintainable \nthan \ncalling \nMyBaseClass.__init__ directly from within the subclasses. I could \nlater rename MyBaseClass to something else or have TimesSevenCorrect \nand PlusNineCorrect inherit from another superclass without having \nto update their __init__ methods to match.\nThe super function can also be called with two parameters: first the \ntype of the class whose MRO parent view you’re trying to access, and \nthen the instance on which to access that view. Using these optional \nparameters within the constructor looks like this:\nclass ExplicitTrisect(MyBaseClass):\n    def __init__(self, value):\n        super(ExplicitTrisect, self).__init__(value)\n        self.value /= 3\nHowever, these parameters are not required for object instance ini-\ntialization. Python’s compiler automatically provides the correct \nparameters (__class__ and self) for you when super is called with \nzero arguments within a class definition. This means all three of \nthese usages are equivalent:\nclass AutomaticTrisect(MyBaseClass):\n    def __init__(self, value):\n        super(__class__, self).__init__(value)\n        self.value /= 3\n \nclass ImplicitTrisect(MyBaseClass):\n    def __init__(self, value):\n        super().__init__(value)\n        self.value /= 3\n \nassert ExplicitTrisect(9).value == 3\nassert AutomaticTrisect(9).value == 3\nassert ImplicitTrisect(9).value == 3\n\n\n \nItem 41: Consider Composing Functionality with Mix-in Classes \n165\nThe only time you should provide parameters to super is in situa-\ntions where you need to access the specific functionality of a super-\nclass’s implementation from a child class (e.g., to wrap or reuse \nfunctionality).\nThings to Remember\n✦ Python’s standard method resolution order (MRO) solves the prob-\nlems of superclass initialization order and diamond inheritance.\n✦ Use the super built-in function with zero arguments to initialize \nparent classes.\nItem 41:  Consider Composing Functionality with \nMix-in Classes\nPython is an object-oriented language with built-in facilities for mak-\ning multiple inheritance tractable (see Item 40: “Initialize Parent \nClasses with super”). However, it’s better to avoid multiple inheritance \naltogether.\nIf you find yourself desiring the convenience and encapsulation that \ncome with multiple inheritance, but want to avoid the potential head-\naches, consider writing a mix-in instead. A mix-in is a class that \ndefines only a small set of additional methods for its child classes to \nprovide. Mix-in classes don’t define their own instance attributes nor \nrequire their __init__ constructor to be called.\nWriting mix-ins is easy because Python makes it trivial to inspect the \ncurrent state of any object, regardless of its type. Dynamic inspection \nmeans you can write generic functionality just once, in a mix-in, and \nit can then be applied to many other classes. Mix-ins can be com-\nposed and layered to minimize repetitive code and maximize reuse.\nFor example, say I want the ability to convert a Python object from its \nin-memory representation to a dictionary that’s ready for serializa-\ntion. Why not write this functionality generically so I can use it with \nall my classes?\nHere, I define an example mix-in that accomplishes this with a new \npublic method that’s added to any class that inherits from it:\nclass ToDictMixin:\n    def to_dict(self):\n        return self._traverse_dict(self.__dict__)\n",
      "page_number": 175,
      "chapter_number": 18,
      "summary": "This chapter covers segment 18 (pages 175-187). Key topics include classes, method, and functions. Covers function, method. These hooks are used by APIs to call back your \ncode while they execute.",
      "keywords": [
        "init",
        "Classes",
        "function",
        "Item",
        "data",
        "Parent Classes",
        "Python",
        "method",
        "Functions",
        "workers",
        "key",
        "input",
        "parent class",
        "Interfaces",
        "result"
      ],
      "concepts": [
        "classes",
        "method",
        "functions",
        "function",
        "functionality",
        "value",
        "item",
        "worker",
        "result",
        "python"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 13,
          "title": "Segment 13 (pages 250-271)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 14,
          "title": "Segment 14 (pages 116-123)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 9,
          "title": "Metaprogramming",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 7,
          "title": "Program Structure and Control Flow",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 24,
          "title": "Segment 24 (pages 208-218)",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 188-197)",
      "start_page": 188,
      "end_page": 197,
      "detection_method": "topic_boundary",
      "content": "166 \nChapter 5 Classes and Interfaces\nThe implementation details are straightforward and rely on dynamic \nattribute access using hasattr, dynamic type inspection with \nisinstance, and accessing the instance dictionary __dict__:\n    def _traverse_dict(self, instance_dict):\n        output = {}\n        for key, value in instance_dict.items():\n            output[key] = self._traverse(key, value)\n        return output\n \n    def _traverse(self, key, value):\n        if isinstance(value, ToDictMixin):\n            return value.to_dict()\n        elif isinstance(value, dict):\n            return self._traverse_dict(value)\n        elif isinstance(value, list):\n            return [self._traverse(key, i) for i in value]\n        elif hasattr(value, '__dict__'):\n            return self._traverse_dict(value.__dict__)\n        else:\n            return value\nHere, I define an example class that uses the mix-in to make a dictio-\nnary representation of a binary tree:\nclass BinaryTree(ToDictMixin):\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\nTranslating a large number of related Python objects into a dictionary \nbecomes easy:\ntree = BinaryTree(10,\n    left=BinaryTree(7, right=BinaryTree(9)),\n    right=BinaryTree(13, left=BinaryTree(11)))\nprint(tree.to_dict())\n>>>\n{'value': 10,\n 'left': {'value': 7,\n          'left': None,\n          'right': {'value': 9, 'left': None, 'right': None}},\n 'right': {'value': 13,\n           'left': {'value': 11, 'left': None, 'right': None},\n           'right': None}}\n\n\n \nItem 41: Consider Composing Functionality with Mix-in Classes \n167\nThe best part about mix-ins is that you can make their generic func-\ntionality pluggable so behaviors can be overridden when required. For \nexample, here I define a subclass of BinaryTree that holds a reference \nto its parent. This circular reference would cause the default imple-\nmentation of ToDictMixin.to_dict to loop forever:\nclass BinaryTreeWithParent(BinaryTree):\n    def __init__(self, value, left=None,\n                 right=None, parent=None):\n        super().__init__(value, left=left, right=right)\n        self.parent = parent\nThe solution is to override the BinaryTreeWithParent._traverse method \nto only process values that matter, preventing cycles encountered by \nthe mix-in. Here, the _traverse override inserts the parent’s numeri-\ncal value and otherwise defers to the mix-in’s default implementation \nby using the super built-in function:\n    def _traverse(self, key, value):\n        if (isinstance(value, BinaryTreeWithParent) and\n                key == 'parent'):\n            return value.value  # Prevent cycles\n        else:\n            return super()._traverse(key, value)\nCalling BinaryTreeWithParent.to_dict works without issue because \nthe circular referencing properties aren’t followed:\nroot = BinaryTreeWithParent(10)\nroot.left = BinaryTreeWithParent(7, parent=root)\nroot.left.right = BinaryTreeWithParent(9, parent=root.left)\nprint(root.to_dict())\n>>>\n{'value': 10,\n 'left': {'value': 7,\n          'left': None,\n          'right': {'value': 9,\n                    'left': None,\n                    'right': None,\n                    'parent': 7},\n          'parent': 10},\n 'right': None,\n 'parent': None}\n\n\n168 \nChapter 5 Classes and Interfaces\nBy defining BinaryTreeWithParent._traverse, I’ve also enabled any \nclass that has an attribute of type BinaryTreeWithParent to automati-\ncally work with the ToDictMixin:\nclass NamedSubTree(ToDictMixin):\n    def __init__(self, name, tree_with_parent):\n        self.name = name\n        self.tree_with_parent = tree_with_parent\n \nmy_tree = NamedSubTree('foobar', root.left.right)\nprint(my_tree.to_dict())  # No infinite loop\n>>>\n{'name': 'foobar',\n 'tree_with_parent': {'value': 9,\n                      'left': None,\n                      'right': None,\n                      'parent': 7}}\nMix-ins can also be composed together. For example, say I want a \nmix-in that provides generic JSON serialization for any class. I can do \nthis by assuming that a class provides a to_dict method (which may \nor may not be provided by the ToDictMixin class):\nimport json\n \nclass JsonMixin:\n    @classmethod\n    def from_json(cls, data):\n        kwargs = json.loads(data)\n        return cls(**kwargs)\n \n    def to_json(self):\n        return json.dumps(self.to_dict())\nNote how the JsonMixin class defines both instance methods and class \nmethods. Mix-ins let you add either kind of behavior to  subclasses. \nIn this example, the only requirements of a JsonMixin subclass are \nproviding a to_dict method and taking keyword arguments for \nthe __init__ method (see Item 23: “Provide Optional Behavior with \n Keyword Arguments” for background).\nThis mix-in makes it simple to create hierarchies of utility classes \nthat can be serialized to and from JSON with little boilerplate. For \nexample, here I have a hierarchy of data classes representing parts of \na datacenter topology:\n\n\n \nItem 41: Consider Composing Functionality with Mix-in Classes \n169\nclass DatacenterRack(ToDictMixin, JsonMixin):\n    def __init__(self, switch=None, machines=None):\n        self.switch = Switch(**switch)\n        self.machines = [\n            Machine(**kwargs) for kwargs in machines]\n \nclass Switch(ToDictMixin, JsonMixin):\n    def __init__(self, ports=None, speed=None):\n        self.ports = ports\n        self.speed = speed\n \nclass Machine(ToDictMixin, JsonMixin):\n    def __init__(self, cores=None, ram=None, disk=None):\n        self.cores = cores\n        self.ram = ram\n        self.disk = disk\nSerializing these classes to and from JSON is simple. Here, I verify \nthat the data is able to be sent round-trip through serializing and \ndeserializing:\nserialized = \"\"\"{\n    \"switch\": {\"ports\": 5, \"speed\": 1e9},\n    \"machines\": [\n        {\"cores\": 8, \"ram\": 32e9, \"disk\": 5e12},\n        {\"cores\": 4, \"ram\": 16e9, \"disk\": 1e12},\n        {\"cores\": 2, \"ram\": 4e9, \"disk\": 500e9}\n    ]\n}\"\"\"\n \ndeserialized = DatacenterRack.from_json(serialized)\nroundtrip = deserialized.to_json()\nassert json.loads(serialized) == json.loads(roundtrip)\nWhen you use mix-ins like this, it’s fine if the class you apply \nJsonMixin to already inherits from JsonMixin higher up in the class \nhierarchy. The resulting class will behave the same way, thanks to \nthe behavior of super.\nThings to Remember\n✦ Avoid using multiple inheritance with instance attributes and \n__init__ if mix-in classes can achieve the same outcome.\n✦ Use pluggable behaviors at the instance level to provide per-class \ncustomization when mix-in classes may require it.\n\n\n170 \nChapter 5 Classes and Interfaces\n✦ Mix-ins can include instance methods or class methods, depending \non your needs.\n✦ Compose mix-ins to create complex functionality from simple \nbehaviors.\nItem 42: Prefer Public Attributes Over Private Ones\nIn Python, there are only two types of visibility for a class’s attributes: \npublic and private:\nclass MyObject:\n    def __init__(self):\n        self.public_field = 5\n        self.__private_field = 10\n \n    def get_private_field(self):\n        return self.__private_field\nPublic attributes can be accessed by anyone using the dot operator on \nthe object:\nfoo = MyObject()\nassert foo.public_field == 5\nPrivate fields are specified by prefixing an attribute’s name with a \ndouble underscore. They can be accessed directly by methods of the \ncontaining class:\nassert foo.get_private_field() == 10\nHowever, directly accessing private fields from outside the class raises \nan exception:\nfoo.__private_field\n>>>\nTraceback ...\nAttributeError: 'MyObject' object has no attribute \n¯'__private_field'\nClass methods also have access to private attributes because they are \ndeclared within the surrounding class block:\nclass MyOtherObject:\n    def __init__(self):\n        self.__private_field = 71\n \n    @classmethod\n    def get_private_field_of_instance(cls, instance):\n        return instance.__private_field\n \n\n\n \nItem 42: Prefer Public Attributes Over Private Ones \n171\nbar = MyOtherObject()\nassert MyOtherObject.get_private_field_of_instance(bar) == 71\nAs you’d expect with private fields, a subclass can’t access its parent \nclass’s private fields:\nclass MyParentObject:\n    def __init__(self):\n        self.__private_field = 71\n \nclass MyChildObject(MyParentObject):\n    def get_private_field(self):\n        return self.__private_field\n \nbaz = MyChildObject()\nbaz.get_private_field()\n>>>\nTraceback ...\nAttributeError: 'MyChildObject' object has no attribute \n¯'_MyChildObject__private_field'\nThe private attribute behavior is implemented with a sim-\nple transformation of the attribute name. When the Python \ncompiler \nsees \nprivate \nattribute \naccess \nin \nmethods \nlike \nMyChildObject.get_private_field, it translates the __private_field \nattribute access to use the name _MyChildObject__private_field \ninstead. In the example above, __private_field is only defined in \nMyParentObject.__init__, which means the private attribute’s real \nname is _MyParentObject__private_field. Accessing the parent’s pri-\nvate attribute from the child class fails simply because the trans-\nformed attribute name doesn’t exist (_MyChildObject__private_field \ninstead of _MyParentObject__private_field).\nKnowing this scheme, you can easily access the private attributes \nof any class—from a subclass or externally—without asking for \npermission:\nassert baz._MyParentObject__private_field == 71\nIf you look in the object’s attribute dictionary, you can see that private \nattributes are actually stored with the names as they appear after the \ntransformation:\nprint(baz.__dict__)\n>>>\n{'_MyParentObject__private_field': 71}\n\n\n172 \nChapter 5 Classes and Interfaces\nWhy doesn’t the syntax for private attributes actually enforce strict \nvisibility? The simplest answer is one often-quoted motto of Python: \n“We are all consenting adults here.” What this means is that we don’t \nneed the language to prevent us from doing what we want to do. It’s \nour individual choice to extend functionality as we wish and to take \nresponsibility for the consequences of such a risk. Python program-\nmers believe that the benefits of being open—permitting unplanned \nextension of classes by default—outweigh the downsides.\nBeyond that, having the ability to hook language features like attri-\nbute access (see Item 47: “Use __getattr__, __getattribute__, and \n__setattr__ for Lazy Attributes”) enables you to mess around with the \ninternals of objects whenever you wish. If you can do that, what is the \nvalue of Python trying to prevent private attribute access otherwise?\nTo minimize damage from accessing internals unknowingly, Python \nprogrammers follow a naming convention defined in the style guide \n(see Item 2: “Follow the PEP 8 Style Guide”). Fields prefixed by a sin-\ngle underscore (like _protected_field) are protected by convention, \nmeaning external users of the class should proceed with caution.\nHowever, many programmers who are new to Python use private fields \nto indicate an internal API that shouldn’t be accessed by subclasses \nor externally:\nclass MyStringClass:\n    def __init__(self, value):\n        self.__value = value\n \n    def get_value(self):\n        return str(self.__value)\n \nfoo = MyStringClass(5)\nassert foo.get_value() == '5'\nThis is the wrong approach. Inevitably someone—maybe even \nyou—will want to subclass your class to add new behavior or to \nwork around deficiencies in existing methods (e.g., the way that \nMyStringClass.get_value always returns a string). By choosing pri-\nvate attributes, you’re only making subclass overrides and extensions \ncumbersome and brittle. Your potential subclassers will still access \nthe private fields when they absolutely need to do so:\nclass MyIntegerSubclass(MyStringClass):\n    def get_value(self):\n        return int(self._MyStringClass__value)\n \n\n\n \nItem 42: Prefer Public Attributes Over Private Ones \n173\nfoo = MyIntegerSubclass('5')\nassert foo.get_value() == 5\nBut if the class hierarchy changes beneath you, these classes will \nbreak because the private attribute references are no longer valid. \nHere, the MyIntegerSubclass class’s immediate parent, MyStringClass, \nhas had another parent class added, called MyBaseClass:\nclass MyBaseClass:\n    def __init__(self, value):\n        self.__value = value\n \n    def get_value(self):\n        return self.__value\n \nclass MyStringClass(MyBaseClass):\n    def get_value(self):\n        return str(super().get_value())         # Updated\n \nclass MyIntegerSubclass(MyStringClass):\n    def get_value(self):\n        return int(self._MyStringClass__value)  # Not updated\nThe __value attribute is now assigned in the MyBaseClass parent class, \nnot the MyStringClass parent. This causes the private variable refer-\nence self._MyStringClass__value to break in MyIntegerSubclass:\nfoo = MyIntegerSubclass(5)\nfoo.get_value()\n>>>\nTraceback ...\nAttributeError: 'MyIntegerSubclass' object has no attribute \n¯'_MyStringClass__value'\nIn general, it’s better to err on the side of allowing subclasses to do \nmore by using protected attributes. Document each protected field \nand explain which fields are internal APIs available to subclasses and \nwhich should be left alone entirely. This is as much advice to other \nprogrammers as it is guidance for your future self on how to extend \nyour own code safely:\nclass MyStringClass:\n    def __init__(self, value):\n        # This stores the user-supplied value for the object.\n        # It should be coercible to a string. Once assigned in\n        # the object it should be treated as immutable.\n\n\n174 \nChapter 5 Classes and Interfaces\n        self._value = value\n \n    ...\nThe only time to seriously consider using private attributes is when \nyou’re worried about naming conflicts with subclasses. This problem \noccurs when a child class unwittingly defines an attribute that was \nalready defined by its parent class:\nclass ApiClass:\n    def __init__(self):\n        self._value = 5\n \n    def get(self):\n        return self._value\n \nclass Child(ApiClass):\n    def __init__(self):\n        super().__init__()\n        self._value = 'hello'  # Conflicts\n \na = Child()\nprint(f'{a.get()} and {a._value} should be different')\n>>>\nhello and hello should be different\nThis is primarily a concern with classes that are part of a public \nAPI; the subclasses are out of your control, so you can’t refactor to \nfix the problem. Such a conflict is especially possible with attribute \nnames that are very common (like value). To reduce the risk of this \nissue occurring, you can use a private attribute in the parent class \nto ensure that there are no attribute names that overlap with child \nclasses:\nclass ApiClass:\n    def __init__(self):\n        self.__value = 5       # Double underscore\n \n    def get(self):\n        return self.__value    # Double underscore\n \nclass Child(ApiClass):\n    def __init__(self):\n        super().__init__()\n        self._value = 'hello'  # OK!\n \n\n\n \nItem 43: Inherit from collections.abc for Custom Container Types \n175\na = Child()\nprint(f'{a.get()} and {a._value} are different')\n>>>\n5 and hello are different\nThings to Remember\n✦ Private attributes aren’t rigorously enforced by the Python compiler.\n✦ Plan from the beginning to allow subclasses to do more with your \ninternal APIs and attributes instead of choosing to lock them out.\n✦ Use documentation of protected fields to guide subclasses instead of \ntrying to force access control with private attributes.\n✦ Only consider using private attributes to avoid naming conflicts \nwith subclasses that are out of your control.\nItem 43:  Inherit from collections.abc for Custom \nContainer Types\nMuch of programming in Python is defining classes that contain data \nand describing how such objects relate to each other. Every Python \nclass is a container of some kind, encapsulating attributes and func-\ntionality together. Python also provides built-in container types for \nmanaging data: lists, tuples, sets, and dictionaries.\nWhen you’re designing classes for simple use cases like sequences, \nit’s natural to want to subclass Python’s built-in list type directly. \nFor example, say I want to create my own custom list type that has \nadditional methods for counting the frequency of its members:\nclass FrequencyList(list):\n    def __init__(self, members):\n        super().__init__(members)\n \n    def frequency(self):\n        counts = {}\n        for item in self:\n            counts[item] = counts.get(item, 0) + 1\n        return counts\nBy subclassing list, I get all of list’s standard functionality and pre-\nserve the semantics familiar to all Python programmers. I can define \nadditional methods to provide any custom behaviors that I need:\nfoo = FrequencyList(['a', 'b', 'a', 'c', 'b', 'a', 'd'])\nprint('Length is', len(foo))\n",
      "page_number": 188,
      "chapter_number": 19,
      "summary": "For example, say I want a \nmix-in that provides generic JSON serialization for any class Key topics include classes, value, and attribute. Things to Remember\n✦ Python’s standard method resolution order (MRO) solves the prob-\nlems of superclass initialization order and diamond inheritance.",
      "keywords": [
        "Private",
        "Classes",
        "Mix-in Classes",
        "init",
        "self.",
        "parent",
        "field",
        "attribute",
        "Mix-in",
        "dict",
        "private attribute",
        "Item",
        "Private fields",
        "Python",
        "return self."
      ],
      "concepts": [
        "classes",
        "value",
        "attribute",
        "python",
        "method",
        "item",
        "private",
        "returns",
        "functionality",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 26,
          "title": "Segment 26 (pages 517-534)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 20,
          "title": "Segment 20 (pages 392-414)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 47,
          "title": "Segment 47 (pages 952-973)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 18,
          "title": "Segment 18 (pages 170-177)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 27,
          "title": "Segment 27 (pages 238-246)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 198-208)",
      "start_page": 198,
      "end_page": 208,
      "detection_method": "topic_boundary",
      "content": "176 \nChapter 5 Classes and Interfaces\nfoo.pop()\nprint('After pop:', repr(foo))\nprint('Frequency:', foo.frequency())\n>>>\nLength is 7\nAfter pop: ['a', 'b', 'a', 'c', 'b', 'a']\nFrequency: {'a': 3, 'b': 2, 'c': 1}\nNow, imagine that I want to provide an object that feels like a list \nand allows indexing but isn’t a list subclass. For example, say that \nI want to provide sequence semantics (like list or tuple) for a binary \ntree class:\nclass BinaryNode:\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\nHow do you make this class act like a sequence type? Python imple-\nments its container behaviors with instance methods that have spe-\ncial names. When you access a sequence item by index:\nbar = [1, 2, 3]\nbar[0]\nit will be interpreted as:\nbar.__getitem__(0)\nTo make the BinaryNode class act like a sequence, you can provide \na custom implementation of __getitem__ (often pronounced “dunder \ngetitem” as an abbreviation for “double underscore getitem”) that tra-\nverses the object tree depth first:\nclass IndexableNode(BinaryNode):\n    def _traverse(self):\n        if self.left is not None:\n            yield from self.left._traverse()\n        yield self\n        if self.right is not None:\n            yield from self.right._traverse()\n \n    def __getitem__(self, index):\n        for i, item in enumerate(self._traverse()):\n            if i == index:\n                return item.value\n        raise IndexError(f'Index {index} is out of range')\n\n\n \nItem 43: Inherit from collections.abc for Custom Container Types \n177\nYou can construct your binary tree as usual:\ntree = IndexableNode(\n    10,\n    left=IndexableNode(\n        5,\n        left=IndexableNode(2),\n        right=IndexableNode(\n            6,\n            right=IndexableNode(7))),\n    right=IndexableNode(\n        15,\n        left=IndexableNode(11)))\nBut you can also access it like a list in addition to being able to tra-\nverse the tree with the left and right attributes:\nprint('LRR is', tree.left.right.right.value)\nprint('Index 0 is', tree[0])\nprint('Index 1 is', tree[1])\nprint('11 in the tree?', 11 in tree)\nprint('17 in the tree?', 17 in tree)\nprint('Tree is', list(tree))\n>>>\nLRR is 7\nIndex 0 is 2\nIndex 1 is 5\n11 in the tree? True\n17 in the tree? False\nTree is [2, 5, 6, 7, 10, 11, 15]\nThe problem is that implementing __getitem__ isn’t enough to provide \nall of the sequence semantics you’d expect from a list instance:\nlen(tree)\n>>>\nTraceback ...\nTypeError: object of type 'IndexableNode' has no len()\nThe len built-in function requires another special method, named \n__len__, that must have an implementation for a custom sequence \ntype:\nclass SequenceNode(IndexableNode):\n    def __len__(self):\n        for count, _ in enumerate(self._traverse(), 1):\n            pass\n        return count\n\n\n178 \nChapter 5 Classes and Interfaces\ntree = SequenceNode(\n    10,\n    left=SequenceNode(\n        5,\n        left=SequenceNode(2),\n        right=SequenceNode(\n            6,\n            right=SequenceNode(7))),\n    right=SequenceNode(\n        15,\n        left=SequenceNode(11))\n)\n \nprint('Tree length is', len(tree))\n>>>\nTree length is 7\nUnfortunately, this still isn’t enough for the class to fully be a valid \nsequence. Also missing are the count and index methods that a \nPython programmer would expect to see on a sequence like list or \ntuple. It turns out that defining your own container types is much \nharder than it seems.\nTo avoid this difficulty throughout the Python universe, the built-in \ncollections.abc module defines a set of abstract base classes that \nprovide all of the typical methods for each container type. When you \nsubclass from these abstract base classes and forget to implement \nrequired methods, the module tells you something is wrong:\nfrom collections.abc import Sequence\n \nclass BadType(Sequence):\n    pass\n \nfoo = BadType()\n>>>\nTraceback ...\nTypeError: Can't instantiate abstract class BadType with \n¯abstract methods __getitem__, __len__\nWhen you do implement all the methods required by an abstract base \nclass from collections.abc, as I did above with SequenceNode, it pro-\nvides all of the additional methods, like index and count, for free:\nclass BetterNode(SequenceNode, Sequence):\n    pass\n \n\n\n \nItem 43: Inherit from collections.abc for Custom Container Types \n179\ntree = BetterNode(\n    10,\n    left=BetterNode(\n        5,\n        left=BetterNode(2),\n        right=BetterNode(\n            6,\n            right=BetterNode(7))),\n    right=BetterNode(\n        15,\n        left=BetterNode(11))\n)\n \nprint('Index of 7 is', tree.index(7))\nprint('Count of 10 is', tree.count(10))\n>>>\nIndex of 7 is 3\nCount of 10 is 1\nThe benefit of using these abstract base classes is even greater for \nmore complex container types such as Set and MutableMapping, which \nhave a large number of special methods that need to be implemented \nto match Python conventions.\nBeyond the collections.abc module, Python uses a variety of special \nmethods for object comparisons and sorting, which may be provided \nby container classes and non-container classes alike (see Item 73: \n“Know How to Use heapq for Priority Queues” for an example).\nThings to Remember\n✦ Inherit directly from Python’s container types (like list or dict) for \nsimple use cases.\n✦ Beware of the large number of methods required to implement cus-\ntom container types correctly.\n✦ Have your custom container types inherit from the interfaces \ndefined in collections.abc to ensure that your classes match \nrequired interfaces and behaviors.\n\n\nThis page intentionally left blank \n\n\n6\nMetaclasses and \nAttributes\nMetaclasses are often mentioned in lists of Python’s features, but \nfew understand what they accomplish in practice. The name meta-\nclass vaguely implies a concept above and beyond a class. Simply put, \nmetaclasses let you intercept Python’s class statement and provide \nspecial behavior each time a class is defined.\nSimilarly mysterious and powerful are Python’s built-in features for \ndynamically customizing attribute accesses. Along with Python’s \nobject-oriented constructs, these facilities provide wonderful tools to \nease the transition from simple classes to complex ones.\nHowever, with these powers come many pitfalls. Dynamic attributes \nenable you to override objects and cause unexpected side effects. \nMetaclasses can create extremely bizarre behaviors that are unap-\nproachable to newcomers. It’s important that you follow the rule of \nleast surprise and only use these mechanisms to implement well- \nunderstood idioms.\nItem 44:  Use Plain Attributes Instead of Setter and \nGetter Methods\nProgrammers coming to Python from other languages may naturally \ntry to implement explicit getter and setter methods in their classes:\nclass OldResistor:\n    def __init__(self, ohms):\n        self._ohms = ohms\n \n    def get_ohms(self):\n        return self._ohms\n \n    def set_ohms(self, ohms):\n        self._ohms = ohms\n\n\n182 \nChapter 6 Metaclasses and Attributes\nUsing these setters and getters is simple, but it’s not Pythonic:\nr0 = OldResistor(50e3)\nprint('Before:', r0.get_ohms())\nr0.set_ohms(10e3)\nprint('After: ', r0.get_ohms())\n>>>\nBefore: 50000.0\nAfter:  10000.0\nSuch methods are especially clumsy for operations like incrementing \nin place:\nr0.set_ohms(r0.get_ohms() - 4e3)\nassert r0.get_ohms() == 6e3\nThese utility methods do, however, help define the interface for \na class, making it easier to encapsulate functionality, validate usage, \nand define boundaries. Those are important goals when designing a \nclass to ensure that you don’t break callers as the class evolves over \ntime.\nIn Python, however, you never need to implement explicit setter or \ngetter methods. Instead, you should always start your implementa-\ntions with simple public attributes, as I do here:\nclass Resistor:\n    def __init__(self, ohms):\n        self.ohms = ohms\n        self.voltage = 0\n        self.current = 0\n \nr1 = Resistor(50e3)\nr1.ohms = 10e3\nThese attributes make operations like incrementing in place natural \nand clear:\nr1.ohms += 5e3\nLater, if I decide I need special behavior when an attribute is set, I \ncan migrate to the @property decorator (see Item 26: “Define Function \nDecorators with functools.wraps” for background) and its correspond-\ning setter attribute. Here, I define a new subclass of Resistor that \nlets me vary the current by assigning the voltage property. Note that \nin order for this code to work properly, the names of both the setter \nand the getter methods must match the intended property name:\nclass VoltageResistance(Resistor):\n    def __init__(self, ohms):\n\n\n \nItem 44: Use Plain Attributes Instead of Setter and Getter Methods \n183\n        super().__init__(ohms)\n        self._voltage = 0\n \n    @property\n    def voltage(self):\n        return self._voltage\n \n    @voltage.setter\n    def voltage(self, voltage):\n        self._voltage = voltage\n        self.current = self._voltage / self.ohms\nNow, assigning the voltage property will run the voltage setter \nmethod, which in turn will update the current attribute of the object \nto match:\nr2 = VoltageResistance(1e3)\nprint(f'Before: {r2.current:.2f} amps')\nr2.voltage = 10\nprint(f'After:  {r2.current:.2f} amps')\n>>>\nBefore: 0.00 amps\nAfter:  0.01 amps\nSpecifying a setter on a property also enables me to perform type \nchecking and validation on values passed to the class. Here, I define a \nclass that ensures all resistance values are above zero ohms:\nclass BoundedResistance(Resistor):\n    def __init__(self, ohms):\n        super().__init__(ohms)\n \n    @property\n    def ohms(self):\n        return self._ohms\n \n    @ohms.setter\n    def ohms(self, ohms):\n        if ohms <= 0:\n            raise ValueError(f'ohms must be > 0; got {ohms}')\n        self._ohms = ohms\nAssigning an invalid resistance to the attribute now raises an \nexception:\nr3 = BoundedResistance(1e3)\nr3.ohms = 0\n\n\n184 \nChapter 6 Metaclasses and Attributes\n>>>\nTraceback ...\nValueError: ohms must be > 0; got 0\nAn exception is also raised if I pass an invalid value to the constructor:\nBoundedResistance(-5)\n>>>\nTraceback ...\nValueError: ohms must be > 0; got -5\nThis \nhappens \nbecause \nBoundedResistance.__init__ \ncalls \nResistor.__init__, which assigns self.ohms = -5. That assignment \ncauses the @ohms.setter method from BoundedResistance to be called, \nand it immediately runs the validation code before object construc-\ntion has completed.\nI can even use @property to make attributes from parent classes \nimmutable:\nclass FixedResistance(Resistor):\n    def __init__(self, ohms):\n        super().__init__(ohms)\n \n    @property\n    def ohms(self):\n        return self._ohms\n \n    @ohms.setter\n    def ohms(self, ohms):\n        if hasattr(self, '_ohms'):\n            raise AttributeError(\"Ohms is immutable\")\n        self._ohms = ohms\nTrying to assign to the property after construction raises an exception:\nr4 = FixedResistance(1e3)\nr4.ohms = 2e3\n>>>\nTraceback ...\nAttributeError: Ohms is immutable\nWhen you use @property methods to implement setters and getters, \nbe sure that the behavior you implement is not surprising. For exam-\nple, don’t set other attributes in getter property methods:\nclass MysteriousResistor(Resistor):\n    @property\n    def ohms(self):\n\n\n \nItem 44: Use Plain Attributes Instead of Setter and Getter Methods \n185\n        self.voltage = self._ohms * self.current\n        return self._ohms\n \n    @ohms.setter\n    def ohms(self, ohms):\n        self._ohms = ohms\nSetting other attributes in getter property methods leads to extremely \nbizarre behavior:\nr7 = MysteriousResistor(10)\nr7.current = 0.01\nprint(f'Before: {r7.voltage:.2f}')\nr7.ohms\nprint(f'After:  {r7.voltage:.2f}')\n>>>\nBefore: 0.00\nAfter:  0.10\nThe best policy is to modify only related object state in @property.setter \nmethods. Be sure to also avoid any other side effects that the caller \nmay not expect beyond the object, such as importing modules dynam-\nically, running slow helper functions, doing I/O, or making expensive \ndatabase queries. Users of a class will expect its attributes to be like \nany other Python object: quick and easy. Use normal methods to do \nanything more complex or slow.\nThe biggest shortcoming of @property is that the methods for an attri-\nbute can only be shared by subclasses. Unrelated classes can’t share \nthe same implementation. However, Python also supports descriptors \n(see Item 46: “Use Descriptors for Reusable @property Methods”) that \nenable reusable property logic and many other use cases.\nThings to Remember\n✦ Define new class interfaces using simple public attributes and avoid \ndefining setter and getter methods.\n✦ Use @property to define special behavior when attributes are \naccessed on your objects, if necessary.\n✦ Follow the rule of least surprise and avoid odd side effects in your \n@property methods.\n✦ Ensure that @property methods are fast; for slow or complex work—\nespecially involving I/O or causing side effects—use normal meth-\nods instead.\n\n\n186 \nChapter 6 Metaclasses and Attributes\nItem 45:  Consider @property Instead of Refactoring \nAttributes\nThe built-in @property decorator makes it easy for simple accesses \nof an instance’s attributes to act smarter (see Item 44: “Use Plain \nAttributes Instead of Setter and Getter Methods”). One advanced but \ncommon use of @property is transitioning what was once a simple \nnumerical attribute into an on-the-fly calculation. This is extremely \nhelpful because it lets you migrate all existing usage of a class to have \nnew behaviors without requiring any of the call sites to be rewritten \n(which is especially important if there’s calling code that you don’t \ncontrol). @property also provides an important stopgap for improving \ninterfaces over time.\nFor example, say that I want to implement a leaky bucket quota using \nplain Python objects. Here, the Bucket class represents how much \nquota remains and the duration for which the quota will be available:\nfrom datetime import datetime, timedelta\n \nclass Bucket:\n    def __init__(self, period):\n        self.period_delta = timedelta(seconds=period)\n        self.reset_time = datetime.now()\n        self.quota = 0\n \n    def __repr__(self):\n        return f'Bucket(quota={self.quota})'\nThe leaky bucket algorithm works by ensuring that, whenever the \nbucket is filled, the amount of quota does not carry over from one \nperiod to the next:\ndef fill(bucket, amount):\n    now = datetime.now()\n    if (now - bucket.reset_time) > bucket.period_delta:\n        bucket.quota = 0\n        bucket.reset_time = now\n    bucket.quota += amount\nEach time a quota consumer wants to do something, it must first \nensure that it can deduct the amount of quota it needs to use:\ndef deduct(bucket, amount):\n    now = datetime.now()\n    if (now - bucket.reset_time) > bucket.period_delta:\n        return False  # Bucket hasn't been filled this period\n    if bucket.quota - amount < 0:\n        return False  # Bucket was filled, but not enough\n",
      "page_number": 198,
      "chapter_number": 20,
      "summary": "Python also provides built-in container types for \nmanaging data: lists, tuples, sets, and dictionaries Key topics include methods, tree, and ohms. ✦ Plan from the beginning to allow subclasses to do more with your \ninternal APIs and attributes instead of choosing to lock them out.",
      "keywords": [
        "ohms",
        "Python",
        "methods",
        "Custom Container Types",
        "Container Types",
        "attributes",
        "tree",
        "Item",
        "property",
        "self.",
        "Container",
        "Custom Container",
        "Getter Methods",
        "classes",
        "init"
      ],
      "concepts": [
        "methods",
        "tree",
        "ohms",
        "attributes",
        "python",
        "pythonic",
        "uses",
        "define",
        "defined",
        "objects"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 20,
          "title": "Segment 20 (pages 392-414)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 4,
          "title": "Types and Objects",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 10,
          "title": "Segment 10 (pages 192-209)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 4,
          "title": "Introducing Python Object Types",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 29,
          "title": "Segment 29 (pages 576-595)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 209-217)",
      "start_page": 209,
      "end_page": 217,
      "detection_method": "topic_boundary",
      "content": " \nItem 45: Consider @property Instead of Refactoring Attributes \n187\n    bucket.quota -= amount\n    return True       # Bucket had enough, quota consumed\nTo use this class, first I fill the bucket up:\nbucket = Bucket(60)\nfill(bucket, 100)\nprint(bucket)\n>>>\nBucket(quota=100)\nThen, I deduct the quota that I need:\nif deduct(bucket, 99):\n    print('Had 99 quota')\nelse:\n    print('Not enough for 99 quota')\nprint(bucket)\n>>>\nHad 99 quota\nBucket(quota=1)\nEventually, I’m prevented from making progress because I try to \ndeduct more quota than is available. In this case, the bucket’s quota \nlevel remains unchanged:\nif deduct(bucket, 3):\n    print('Had 3 quota')\nelse:\n    print('Not enough for 3 quota')\nprint(bucket)\n>>>\nNot enough for 3 quota\nBucket(quota=1)\nThe problem with this implementation is that I never know what \nquota level the bucket started with. The quota is deducted over the \ncourse of the period until it reaches zero. At that point, deduct will \nalways return False until the bucket is refilled. When that happens, it \nwould be useful to know whether callers to deduct are being blocked \nbecause the Bucket ran out of quota or because the Bucket never had \nquota during this period in the first place.\nTo fix this, I can change the class to keep track of the max_quota \nissued in the period and the quota_consumed in the period:\nclass NewBucket:\n    def __init__(self, period):\n        self.period_delta = timedelta(seconds=period)\n\n\n188 \nChapter 6 Metaclasses and Attributes\n        self.reset_time = datetime.now()\n        self.max_quota = 0\n        self.quota_consumed = 0\n \n    def __repr__(self):\n        return (f'NewBucket(max_quota={self.max_quota}, '\n                f'quota_consumed={self.quota_consumed})')\nTo match the previous interface of the original Bucket class, I use a \n@property method to compute the current level of quota on-the-fly \nusing these new attributes:\n    @property\n    def quota(self):\n        return self.max_quota - self.quota_consumed\nWhen the quota attribute is assigned, I take special action to be com-\npatible with the current usage of the class by the fill and deduct \nfunctions:\n    @quota.setter\n    def quota(self, amount):\n        delta = self.max_quota - amount\n        if amount == 0:\n            # Quota being reset for a new period\n            self.quota_consumed = 0\n            self.max_quota = 0\n        elif delta < 0:\n            # Quota being filled for the new period\n            assert self.quota_consumed == 0\n            self.max_quota = amount\n        else:\n            # Quota being consumed during the period\n            assert self.max_quota >= self.quota_consumed\n            self.quota_consumed += delta\nRerunning the demo code from above produces the same results:\nbucket = NewBucket(60)\nprint('Initial', bucket)\nfill(bucket, 100)\nprint('Filled', bucket)\n \nif deduct(bucket, 99):\n    print('Had 99 quota')\nelse:\n    print('Not enough for 99 quota')\n \n\n\n \nItem 45: Consider @property Instead of Refactoring Attributes \n189\nprint('Now', bucket)\n \nif deduct(bucket, 3):\n    print('Had 3 quota')\nelse:\n    print('Not enough for 3 quota')\n \nprint('Still', bucket)\n>>>\nInitial NewBucket(max_quota=0, quota_consumed=0)\nFilled NewBucket(max_quota=100, quota_consumed=0)\nHad 99 quota\nNow NewBucket(max_quota=100, quota_consumed=99)\nNot enough for 3 quota\nStill NewBucket(max_quota=100, quota_consumed=99)\nThe best part is that the code using Bucket.quota doesn’t have to \nchange or know that the class has changed. New usage of Bucket can \ndo the right thing and access max_quota and quota_consumed directly.\nI especially like @property because it lets you make incremental prog-\nress toward a better data model over time. Reading the Bucket exam-\nple above, you may have thought that fill and deduct should have \nbeen implemented as instance methods in the first place. Although \nyou’re probably right (see Item 37: “Compose Classes Instead of \n Nesting Many Levels of Built-in Types”), in practice there are many \nsituations in which objects start with poorly defined interfaces or act \nas dumb data containers. This happens when code grows over time, \nscope increases, multiple authors contribute without anyone consid-\nering long-term hygiene, and so on.\n@property is a tool to help you address problems you’ll come across in \nreal-world code. Don’t overuse it. When you find yourself repeatedly \nextending @property methods, it’s probably time to refactor your class \ninstead of further paving over your code’s poor design.\nThings to Remember\n✦ Use @property to give existing instance attributes new functionality.\n✦ Make incremental progress toward better data models by using \n@property.\n✦ Consider refactoring a class and all call sites when you find yourself \nusing @property too heavily.\n\n\n190 \nChapter 6 Metaclasses and Attributes\nItem 46:  Use Descriptors for Reusable @property \nMethods\nThe big problem with the @property built-in (see Item 44: “Use \nPlain Attributes Instead of Setter and Getter Methods” and Item 45: \n “Consider @property Instead of Refactoring Attributes”) is reuse. The \nmethods it decorates can’t be reused for multiple attributes of the \nsame class. They also can’t be reused by unrelated classes.\nFor example, say I want a class to validate that the grade received by \na student on a homework assignment is a percentage:\nclass Homework:\n    def __init__(self):\n        self._grade = 0\n \n    @property\n    def grade(self):\n        return self._grade\n \n    @grade.setter\n    def grade(self, value):\n        if not (0 <= value <= 100):\n            raise ValueError(\n                'Grade must be between 0 and 100')\n        self._grade = value\nUsing @property makes this class easy to use:\ngalileo = Homework()\ngalileo.grade = 95\nSay that I also want to give the student a grade for an exam, where \nthe exam has multiple subjects, each with a separate grade:\nclass Exam:\n    def __init__(self):\n        self._writing_grade = 0\n        self._math_grade = 0\n \n    @staticmethod\n    def _check_grade(value):\n        if not (0 <= value <= 100):\n            raise ValueError(\n                'Grade must be between 0 and 100')\n\n\n \nItem 46: Use Descriptors for Reusable @property Methods \n191\nThis quickly gets tedious. For each section of the exam I need to add a \nnew @property and related validation:\n    @property\n    def writing_grade(self):\n        return self._writing_grade\n \n    @writing_grade.setter\n    def writing_grade(self, value):\n        self._check_grade(value)\n        self._writing_grade = value\n \n    @property\n    def math_grade(self):\n        return self._math_grade\n \n    @math_grade.setter\n    def math_grade(self, value):\n        self._check_grade(value)\n        self._math_grade = value\nAlso, this approach is not general. If I want to reuse this percentage \nvalidation in other classes beyond homework and exams, I’ll need to \nwrite the @property boilerplate and _check_grade method over and \nover again.\nThe better way to do this in Python is to use a descriptor. The descrip-\ntor protocol defines how attribute access is interpreted by the lan-\nguage. A descriptor class can provide __get__ and __set__ methods \nthat let you reuse the grade validation behavior without boilerplate. \nFor this purpose, descriptors are also better than mix-ins (see Item \n41: “Consider Composing Functionality with Mix-in Classes”) because \nthey let you reuse the same logic for many different attributes in a \nsingle class.\nHere, I define a new class called Exam with class attributes that are \nGrade instances. The Grade class implements the descriptor protocol:\nclass Grade:\n    def __get__(self, instance, instance_type):\n        ...\n \n    def __set__(self, instance, value):\n        ...\n \n\n\n192 \nChapter 6 Metaclasses and Attributes\nclass Exam:\n    # Class attributes\n    math_grade = Grade()\n    writing_grade = Grade()\n    science_grade = Grade()\nBefore I explain how the Grade class works, it’s important to under-\nstand what Python will do when such descriptor attributes are \naccessed on an Exam instance. When I assign a property:\nexam = Exam()\nexam.writing_grade = 40\nit is interpreted as:\nExam.__dict__['writing_grade'].__set__(exam, 40)\nWhen I retrieve a property:\nexam.writing_grade\nit is interpreted as:\nExam.__dict__['writing_grade'].__get__(exam, Exam)\nWhat drives this behavior is the __getattribute__ method of object \n(see Item 47: “Use __getattr__, __getattribute__, and __setattr__ \nfor Lazy Attributes”). In short, when an Exam instance doesn’t have an \nattribute named writing_grade, Python falls back to the Exam class’s \nattribute instead. If this class attribute is an object that has __get__ \nand __set__ methods, Python assumes that you want to follow the \ndescriptor protocol.\nKnowing this behavior and how I used @property for grade validation \nin the Homework class, here’s a reasonable first attempt at implement-\ning the Grade descriptor:\nclass Grade:\n    def __init__(self):\n        self._value = 0\n \n    def __get__(self, instance, instance_type):\n        return self._value\n \n    def __set__(self, instance, value):\n        if not (0 <= value <= 100):\n            raise ValueError(\n                'Grade must be between 0 and 100')\n        self._value = value\n\n\n \nItem 46: Use Descriptors for Reusable @property Methods \n193\nUnfortunately, this is wrong and results in broken behavior. Access-\ning multiple attributes on a single Exam instance works as expected:\nclass Exam:\n    math_grade = Grade()\n    writing_grade = Grade()\n    science_grade = Grade()\n \nfirst_exam = Exam()\nfirst_exam.writing_grade = 82\nfirst_exam.science_grade = 99\nprint('Writing', first_exam.writing_grade)\nprint('Science', first_exam.science_grade)\n>>>\nWriting 82\nScience 99\nBut accessing these attributes on multiple Exam instances causes \nunexpected behavior:\nsecond_exam = Exam()\nsecond_exam.writing_grade = 75\nprint(f'Second {second_exam.writing_grade} is right')\nprint(f'First  {first_exam.writing_grade} is wrong; '\n      f'should be 82')\n>>>\nSecond 75 is right\nFirst  75 is wrong; should be 82\nThe problem is that a single Grade instance is shared across all Exam \ninstances for the class attribute writing_grade. The Grade instance for \nthis attribute is constructed once in the program lifetime, when the \nExam class is first defined, not each time an Exam instance is created.\nTo solve this, I need the Grade class to keep track of its value for each \nunique Exam instance. I can do this by saving the per-instance state \nin a dictionary:\nclass Grade:\n    def __init__(self):\n        self._values = {}\n \n    def __get__(self, instance, instance_type):\n        if instance is None:\n            return self\n        return self._values.get(instance, 0)\n \n\n\n194 \nChapter 6 Metaclasses and Attributes\n    def __set__(self, instance, value):\n        if not (0 <= value <= 100):\n            raise ValueError(\n                'Grade must be between 0 and 100')\n        self._values[instance] = value\nThis implementation is simple and works well, but there’s still one \ngotcha: It leaks memory. The _values dictionary holds a reference to \nevery instance of Exam ever passed to __set__ over the lifetime of the \nprogram. This causes instances to never have their reference count \ngo to zero, preventing cleanup by the garbage collector (see Item 81: \n“Use tracemalloc to Understand Memory Usage and Leaks” for how to \ndetect this type of problem).\nTo fix this, I can use Python’s weakref built-in module. This module \nprovides a special class called WeakKeyDictionary that can take the \nplace of the simple dictionary used for _values. The unique behavior \nof WeakKeyDictionary is that it removes Exam instances from its set of \nitems when the Python runtime knows it’s holding the instance’s last \nremaining reference in the program. Python does the bookkeeping for \nme and ensures that the _values dictionary will be empty when all \nExam instances are no longer in use:\nfrom weakref import WeakKeyDictionary\n \nclass Grade:\n    def __init__(self):\n        self._values = WeakKeyDictionary()\n \n    def __get__(self, instance, instance_type):\n        ...\n \n    def __set__(self, instance, value):\n        ...\nUsing this implementation of the Grade descriptor, everything works \nas expected:\nclass Exam:\n    math_grade = Grade()\n    writing_grade = Grade()\n    science_grade = Grade()\n \nfirst_exam = Exam()\nfirst_exam.writing_grade = 82\nsecond_exam = Exam()\nsecond_exam.writing_grade = 75\nprint(f'First  {first_exam.writing_grade} is right')\nprint(f'Second {second_exam.writing_grade} is right')\n\n\n \nItem 47: Use __getattr__, etc for Lazy Attributes \n195\n>>>\nFirst  82 is right\nSecond 75 is right\nThings to Remember\n✦ Reuse the behavior and validation of @property methods by defining \nyour own descriptor classes.\n✦ Use WeakKeyDictionary to ensure that your descriptor classes don’t \ncause memory leaks.\n✦ Don’t get bogged down trying to understand exactly how \n__getattribute__ uses the descriptor protocol for getting and set-\nting attributes.\nItem 47:  Use __getattr__, __getattribute__, \nand __setattr__ for Lazy Attributes\nPython’s object hooks make it easy to write generic code for glu-\ning systems together. For example, say that I want to represent the \nrecords in a database as Python objects. The database has its schema \nset already. My code that uses objects corresponding to those records \nmust also know what the database looks like. However, in Python, \nthe code that connects Python objects to the database doesn’t need to \nexplicitly specify the schema of the records; it can be generic.\nHow is that possible? Plain instance attributes, @property methods, \nand descriptors can’t do this because they all need to be defined in \nadvance. Python makes this dynamic behavior possible with the \n__getattr__ special method. If a class defines __getattr__, that \nmethod is called every time an attribute can’t be found in an object’s \ninstance dictionary:\nclass LazyRecord:\n    def __init__(self):\n        self.exists = 5\n \n    def __getattr__(self, name):\n        value = f'Value for {name}'\n        setattr(self, name, value)\n        return value\nHere, I access the missing property foo. This causes Python to call \nthe __getattr__ method above, which mutates the instance dictio-\nnary __dict__:\ndata = LazyRecord()\nprint('Before:', data.__dict__)\n",
      "page_number": 209,
      "chapter_number": 21,
      "summary": "@property also provides an important stopgap for improving \ninterfaces over time Key topics include classes, attributes, and property. This is extremely \nhelpful because it lets you migrate all existing usage of a class to have \nnew behaviors without requiring any of the call sites to be rewritten \n(which is especially important if there’s calling code that you don’t \ncontrol).",
      "keywords": [
        "grade",
        "exam",
        "quota",
        "bucket",
        "Attributes",
        "property",
        "instance",
        "Item",
        "Exam instance",
        "class Exam",
        "self.",
        "class Grade",
        "Refactoring Attributes",
        "writing",
        "Attributes class Exam"
      ],
      "concepts": [
        "classes",
        "attributes",
        "property",
        "exam",
        "grade",
        "useful",
        "instance",
        "value",
        "python",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 43,
          "title": "Segment 43 (pages 863-885)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 23,
          "title": "Segment 23 (pages 192-207)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 11,
          "title": "Segment 11 (pages 210-230)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 22,
          "title": "Segment 22 (pages 184-191)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 25,
          "title": "Segment 25 (pages 219-229)",
          "relevance_score": 0.51,
          "method": "api"
        }
      ]
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 218-231)",
      "start_page": 218,
      "end_page": 231,
      "detection_method": "topic_boundary",
      "content": "196 \nChapter 6 Metaclasses and Attributes\nprint('foo:   ', data.foo)\nprint('After: ', data.__dict__)\n>>>\nBefore: {'exists': 5}\nfoo:    Value for foo\nAfter:  {'exists': 5, 'foo': 'Value for foo'}\nHere, I add logging to LazyRecord to show when __getattr__ is actu-\nally called. Note how I call super().__getattr__() to use the super-\nclass’s implementation of __getattr__ in order to fetch the real \nproperty value and avoid infinite recursion (see Item 40: “Initialize \nParent Classes with super” for background):\nclass LoggingLazyRecord(LazyRecord):\n    def __getattr__(self, name):\n        print(f'* Called __getattr__({name!r}), '\n              f'populating instance dictionary')\n        result = super().__getattr__(name)\n        print(f'* Returning {result!r}')\n        return result\n \ndata = LoggingLazyRecord()\nprint('exists:     ', data.exists)\nprint('First foo:  ', data.foo)\nprint('Second foo: ', data.foo)\n>>>\nexists:      5\n* Called __getattr__('foo'), populating instance dictionary\n* Returning 'Value for foo'\nFirst foo:   Value for foo\nSecond foo:  Value for foo\nThe exists attribute is present in the instance dictionary, so \n__getattr__ is never called for it. The foo attribute is not in the \ninstance dictionary initially, so __getattr__ is called the first time. \nBut the call to __getattr__ for foo also does a setattr, which pop-\nulates foo in the instance dictionary. This is why the second time I \naccess foo, it doesn’t log a call to __getattr__.\nThis behavior is especially helpful for use cases like lazily accessing \nschemaless data. __getattr__ runs once to do the hard work of load-\ning a property; all subsequent accesses retrieve the existing result.\nSay that I also want transactions in this database system. The next \ntime the user accesses a property, I want to know whether the cor-\nresponding record in the database is still valid and whether the \n\n\ntransaction is still open. The __getattr__ hook won’t let me do this \nreliably because it will use the object’s instance dictionary as the fast \npath for existing attributes.\nTo enable this more advanced use case, Python has another object \nhook called __getattribute__. This special method is called every \ntime an attribute is accessed on an object, even in cases where it does \nexist in the attribute dictionary. This enables me to do things like \ncheck global transaction state on every property access. It’s import-\nant to note that such an operation can incur significant overhead \nand negatively impact performance, but sometimes it’s worth it. Here, \nI define ValidatingRecord to log each time __getattribute__ is called:\nclass ValidatingRecord:\n    def __init__(self):\n        self.exists = 5\n \n    def __getattribute__(self, name):\n        print(f'* Called __getattribute__({name!r})')\n        try:\n            value = super().__getattribute__(name)\n            print(f'* Found {name!r}, returning {value!r}')\n            return value\n        except AttributeError:\n            value = f'Value for {name}'\n            print(f'* Setting {name!r} to {value!r}')\n            setattr(self, name, value)\n            return value\n \ndata = ValidatingRecord()\nprint('exists:     ', data.exists)\nprint('First foo:  ', data.foo)\nprint('Second foo: ', data.foo)\n>>>\n* Called __getattribute__('exists')\n* Found 'exists', returning 5\nexists:      5\n* Called __getattribute__('foo')\n* Setting 'foo' to 'Value for foo'\nFirst foo:   Value for foo\n* Called __getattribute__('foo')\n* Found 'foo', returning 'Value for foo'\nSecond foo:  Value for foo\n \nItem 47: Use __getattr__, etc for Lazy Attributes \n197\n\n\n198 \nChapter 6 Metaclasses and Attributes\nIn the event that a dynamically accessed property shouldn’t exist, \nI can raise an AttributeError to cause Python’s standard missing \nproperty behavior for both __getattr__ and __getattribute__:\nclass MissingPropertyRecord:\n    def __getattr__(self, name):\n        if name == 'bad_name':\n            raise AttributeError(f'{name} is missing')\n        ...\n \ndata = MissingPropertyRecord()\ndata.bad_name\n>>>\nTraceback ...\nAttributeError: bad_name is missing\nPython code implementing generic functionality often relies on the \nhasattr built-in function to determine when properties exist, and the \ngetattr built-in function to retrieve property values. These functions \nalso look in the instance dictionary for an attribute name before call-\ning __getattr__:\ndata = LoggingLazyRecord()  # Implements __getattr__\nprint('Before:         ', data.__dict__)\nprint('Has first foo:  ', hasattr(data, 'foo'))\nprint('After:          ', data.__dict__)\nprint('Has second foo: ', hasattr(data, 'foo'))\n>>>\nBefore:          {'exists': 5}\n* Called __getattr__('foo'), populating instance dictionary\n* Returning 'Value for foo'\nHas first foo:   True\nAfter:           {'exists': 5, 'foo': 'Value for foo'}\nHas second foo:  True\nIn the example above, __getattr__ is called only once. In contrast, \nclasses that implement __getattribute__ have that method called \neach time hasattr or getattr is used with an instance:\ndata = ValidatingRecord()  # Implements __getattribute__\nprint('Has first foo:  ', hasattr(data, 'foo'))\nprint('Has second foo: ', hasattr(data, 'foo'))\n>>>\n* Called __getattribute__('foo')\n* Setting 'foo' to 'Value for foo'\nHas first foo:   True\n\n\n* Called __getattribute__('foo')\n* Found 'foo', returning 'Value for foo'\nHas second foo:  True\nNow, say that I want to lazily push data back to the database \nwhen values are assigned to my Python object. I can do this with \n__setattr__, a similar object hook that lets you intercept arbitrary \nattribute assignments. Unlike when retrieving an attribute with \n__getattr__ and __getattribute__, there’s no need for two separate \nmethods. The __setattr__ method is always called every time an \nattribute is assigned on an instance (either directly or through the \nsetattr built-in function):\nclass SavingRecord:\n    def __setattr__(self, name, value):\n        # Save some data for the record\n        ...\n        super().__setattr__(name, value)\nHere, I define a logging subclass of SavingRecord. Its __setattr__ \nmethod is always called on each attribute assignment:\nclass LoggingSavingRecord(SavingRecord):\n    def __setattr__(self, name, value):\n        print(f'* Called __setattr__({name!r}, {value!r})')\n        super().__setattr__(name, value)\n \ndata = LoggingSavingRecord()\nprint('Before: ', data.__dict__)\ndata.foo = 5\nprint('After:  ', data.__dict__)\ndata.foo = 7\nprint('Finally:', data.__dict__)\n>>>\nBefore:  {}\n* Called __setattr__('foo', 5)\nAfter:   {'foo': 5}\n* Called __setattr__('foo', 7)\nFinally: {'foo': 7}\nThe problem with __getattribute__ and __setattr__ is that they’re \ncalled on every attribute access for an object, even when you may not \nwant that to happen. For example, say that I want attribute accesses \non my object to actually look up keys in an associated dictionary:\nclass BrokenDictionaryRecord:\n    def __init__(self, data):\n        self._data = {}\n \n \nItem 47: Use __getattr__, etc for Lazy Attributes \n199\n\n\n200 \nChapter 6 Metaclasses and Attributes\n    def __getattribute__(self, name):\n        print(f'* Called __getattribute__({name!r})')\n        return self._data[name]\nThis requires accessing self._data from the __getattribute__ \nmethod. However, if I actually try to do that, Python will recurse until \nit reaches its stack limit, and then it’ll die:\ndata = BrokenDictionaryRecord({'foo': 3})\ndata.foo\n>>>\n* Called __getattribute__('foo')\n* Called __getattribute__('_data')\n* Called __getattribute__('_data')\n* Called __getattribute__('_data')\n...\nTraceback ...\nRecursionError: maximum recursion depth exceeded while calling \n¯a Python object\nThe problem is that __getattribute__ accesses self._data, which \ncauses __getattribute__ to run again, which accesses self._data \nagain, and so on. The solution is to use the super().__getattribute__ \nmethod to fetch values from the instance attribute dictionary. This \navoids the recursion:\nclass DictionaryRecord:\n    def __init__(self, data):\n        self._data = data\n \n    def __getattribute__(self, name):\n        print(f'* Called __getattribute__({name!r})')\n        data_dict = super().__getattribute__('_data')\n        return data_dict[name]\n \ndata = DictionaryRecord({'foo': 3})\nprint('foo: ', data.foo)\n>>>\n* Called __getattribute__('foo')\nfoo:  3\n__setattr__ methods that modify attributes on an object also need to \nuse super().__setattr__ accordingly.\nThings to Remember\n✦ Use __getattr__ and __setattr__ to lazily load and save attributes \nfor an object.\n\n\n \nItem 48: Validate Subclasses with __init_subclass__ \n201\n✦ Understand that __getattr__ only gets called when accessing a \nmissing attribute, whereas __getattribute__ gets called every time \nany attribute is accessed.\n✦ Avoid infinite recursion in __getattribute__ and __setattr__ \nby using methods from super() (i.e., the object class) to access \ninstance attributes.\nItem 48: Validate Subclasses with __init_subclass__\nOne of the simplest applications of metaclasses is verifying that a \nclass was defined correctly. When you’re building a complex class \nhierarchy, you may want to enforce style, require overriding meth-\nods, or have strict relationships between class attributes. Metaclasses \nenable these use cases by providing a reliable way to run your valida-\ntion code each time a new subclass is defined.\nOften a class’s validation code runs in the __init__ method, when an \nobject of the class’s type is constructed at runtime (see Item 44: “Use \nPlain Attributes Instead of Setter and Getter Methods” for an exam-\nple). Using metaclasses for validation can raise errors much earlier, \nsuch as when the module containing the class is first imported at \nprogram startup.\nBefore I get into how to define a metaclass for validating subclasses, \nit’s important to understand the metaclass action for standard \nobjects. A metaclass is defined by inheriting from type. In the default \ncase, a metaclass receives the contents of associated class statements \nin its __new__ method. Here, I can inspect and modify the class infor-\nmation before the type is actually constructed:\nclass Meta(type):\n    def __new__(meta, name, bases, class_dict):\n        print(f'* Running {meta}.__new__ for {name}')\n        print('Bases:', bases)\n        print(class_dict)\n        return type.__new__(meta, name, bases, class_dict)\n \nclass MyClass(metaclass=Meta):\n    stuff = 123\n \n    def foo(self):\n        pass\n \nclass MySubclass(MyClass):\n    other = 567\n \n    def bar(self):\n        pass\n\n\n202 \nChapter 6 Metaclasses and Attributes\nThe metaclass has access to the name of the class, the parent classes \nit inherits from (bases), and all the class attributes that were defined \nin the class’s body. All classes inherit from object, so it’s not explicitly \nlisted in the tuple of base classes:\n>>>\n* Running <class '__main__.Meta'>.__new__ for MyClass\nBases: ()\n{'__module__': '__main__',\n '__qualname__': 'MyClass',\n 'stuff': 123,\n 'foo': <function MyClass.foo at 0x105a05280>}\n* Running <class '__main__.Meta'>.__new__ for MySubclass\nBases: (<class '__main__.MyClass'>,)\n{'__module__': '__main__',\n '__qualname__': 'MySubclass',\n 'other': 567,\n 'bar': <function MySubclass.bar at 0x105a05310>}\nI can add functionality to the Meta.__new__ method in order to vali-\ndate all of the parameters of an associated class before it’s defined. \nFor example, say that I want to represent any type of multisided \npolygon. I can do this by defining a special validating metaclass and \nusing it in the base class of my polygon class hierarchy. Note that it’s \nimportant not to apply the same validation to the base class:\nclass ValidatePolygon(type):\n    def __new__(meta, name, bases, class_dict):\n        # Only validate subclasses of the Polygon class\n        if bases:\n            if class_dict['sides'] < 3:\n                raise ValueError('Polygons need 3+ sides')\n        return type.__new__(meta, name, bases, class_dict)\n \nclass Polygon(metaclass=ValidatePolygon):\n    sides = None  # Must be specified by subclasses\n \n    @classmethod\n    def interior_angles(cls):\n        return (cls.sides - 2) * 180\n \nclass Triangle(Polygon):\n    sides = 3\n \n\n\nclass Rectangle(Polygon):\n    sides = 4\n \nclass Nonagon(Polygon):\n    sides = 9\n \nassert Triangle.interior_angles() == 180\nassert Rectangle.interior_angles() == 360\nassert Nonagon.interior_angles() == 1260\nIf I try to define a polygon with fewer than three sides, the valida-\ntion will cause the class statement to fail immediately after the class \nstatement body. This means the program will not even be able to start \nrunning when I define such a class (unless it’s defined in a dynam-\nically imported module; see Item 88: “Know How to Break Circular \nDependencies” for how this can happen):\nprint('Before class')\n \nclass Line(Polygon):\n    print('Before sides')\n    sides = 2\n    print('After sides')\n \nprint('After class')\n>>>\nBefore class\nBefore sides\nAfter sides\nTraceback ...\nValueError: Polygons need 3+ sides\nThis seems like quite a lot of machinery in order to get Python to \naccomplish such a basic task. Luckily, Python 3.6 introduced simpli-\nfied syntax—the __init_subclass__ special class method—for achiev-\ning the same behavior while avoiding metaclasses entirely. Here, I use \nthis mechanism to provide the same level of validation as before:\nclass BetterPolygon:\n    sides = None  # Must be specified by subclasses\n \n    def __init_subclass__(cls):\n        super().__init_subclass__()\n        if cls.sides < 3:\n            raise ValueError('Polygons need 3+ sides')\n \n \nItem 48: Validate Subclasses with __init_subclass__ \n203\n\n\n204 \nChapter 6 Metaclasses and Attributes\n    @classmethod\n    def interior_angles(cls):\n        return (cls.sides - 2) * 180\n \nclass Hexagon(BetterPolygon):\n    sides = 6\n \nassert Hexagon.interior_angles() == 720\nThe code is much shorter now, and the ValidatePolygon metaclass is \ngone entirely. It’s also easier to follow since I can access the sides \nattribute directly on the cls instance in __init_subclass__ instead of \nhaving to go into the class’s dictionary with class_dict['sides']. If \nI define an invalid subclass of BetterPolygon, the same exception is \nraised:\nprint('Before class')\n \nclass Point(BetterPolygon):\n    sides = 1\n \nprint('After class')\n>>>\nBefore class\nTraceback ...\nValueError: Polygons need 3+ sides\nAnother problem with the standard Python metaclass machinery \nis that you can only specify a single metaclass per class definition. \nHere, I define a second metaclass that I’d like to use for validating the \nfill color used for a region (not necessarily just polygons):\nclass ValidateFilled(type):\n    def __new__(meta, name, bases, class_dict):\n        # Only validate subclasses of the Filled class\n        if bases:\n            if class_dict['color'] not in ('red', 'green'):\n                raise ValueError('Fill color must be supported')\n        return type.__new__(meta, name, bases, class_dict)\n \nclass Filled(metaclass=ValidateFilled):\n    color = None  # Must be specified by subclasses\n\n\nWhen I try to use the Polygon metaclass and Filled metaclass \ntogether, I get a cryptic error message:\nclass RedPentagon(Filled, Polygon):\n    color = 'red'\n    sides = 5\n>>>\nTraceback ...\nTypeError: metaclass conflict: the metaclass of a derived \n¯class must be a (non-strict) subclass of the metaclasses \n¯of all its bases\nIt’s possible to fix this by creating a complex hierarchy of metaclass \ntype definitions to layer validation:\nclass ValidatePolygon(type):\n    def __new__(meta, name, bases, class_dict):\n        # Only validate non-root classes\n        if not class_dict.get('is_root'):\n            if class_dict['sides'] < 3:\n                raise ValueError('Polygons need 3+ sides')\n        return type.__new__(meta, name, bases, class_dict)\n \nclass Polygon(metaclass=ValidatePolygon):\n    is_root = True\n    sides = None  # Must be specified by subclasses\n \nclass ValidateFilledPolygon(ValidatePolygon):\n    def __new__(meta, name, bases, class_dict):\n        # Only validate non-root classes\n        if not class_dict.get('is_root'):\n            if class_dict['color'] not in ('red', 'green'):\n                raise ValueError('Fill color must be supported')\n        return super().__new__(meta, name, bases, class_dict)\n \nclass FilledPolygon(Polygon, metaclass=ValidateFilledPolygon):\n    is_root = True\n    color = None  # Must be specified by subclasses\nThis requires every FilledPolygon instance to be a Polygon instance:\nclass GreenPentagon(FilledPolygon):\n    color = 'green'\n    sides = 5\n \ngreenie = GreenPentagon()\nassert isinstance(greenie, Polygon)\n \nItem 48: Validate Subclasses with __init_subclass__ \n205\n\n\n206 \nChapter 6 Metaclasses and Attributes\nValidation works for colors:\nclass OrangePentagon(FilledPolygon):\n    color = 'orange'\n    sides = 5\n>>>\nTraceback ...\nValueError: Fill color must be supported\nValidation also works for number of sides:\nclass RedLine(FilledPolygon):\n    color = 'red'\n    sides = 2\n>>>\nTraceback ...\nValueError: Polygons need 3+ sides\nHowever, this approach ruins composability, which is often the pur-\npose of class validation like this (similar to mix-ins; see Item 41: \n“Consider Composing Functionality with Mix-in Classes”). If I want to \napply the color validation logic from ValidateFilledPolygon to another \nhierarchy of classes, I’ll have to duplicate all of the logic again, which \nreduces code reuse and increases boilerplate.\nThe __init_subclass__ special class method can also be used to \nsolve this problem. It can be defined by multiple levels of a class \nhierarchy as long as the super built-in function is used to call any \nparent or sibling __init_subclass__ definitions (see Item 40: “Initial-\nize Parent Classes with super” for a similar example). It’s even com-\npatible with multiple inheritance. Here, I define a class to represent \nregion fill color that can be composed with the BetterPolygon class \nfrom before:\nclass Filled:\n    color = None  # Must be specified by subclasses\n \n    def __init_subclass__(cls):\n        super().__init_subclass__()\n        if cls.color not in ('red', 'green', 'blue'):\n            raise ValueError('Fills need a valid color')\nI can inherit from both classes to define a new class. Both classes call \nsuper().__init_subclass__(), causing their corresponding validation \nlogic to run when the subclass is created:\nclass RedTriangle(Filled, Polygon):\n    color = 'red'\n    sides = 3\n \n\n\nruddy = RedTriangle()\nassert isinstance(ruddy, Filled)\nassert isinstance(ruddy, Polygon)\nIf I specify the number of sides incorrectly, I get a validation error:\nprint('Before class')\n \nclass BlueLine(Filled, Polygon):\n    color = 'blue'\n    sides = 2\n \nprint('After class')\n>>>\nBefore class\nTraceback ...\nValueError: Polygons need 3+ sides\nIf I specify the color incorrectly, I also get a validation error:\nprint('Before class')\n \nclass BeigeSquare(Filled, Polygon):\n    color = 'beige'\n    sides = 4\n \nprint('After class')\n>>>\nBefore class\nTraceback ...\nValueError: Fills need a valid color\nYou can even use __init_subclass__ in complex cases like diamond \ninheritance (see Item 40: “Initialize Parent Classes with super”). Here, \nI define a basic diamond hierarchy to show this in action:\nclass Top:\n    def __init_subclass__(cls):\n        super().__init_subclass__()\n        print(f'Top for {cls}')\n \nclass Left(Top):\n    def __init_subclass__(cls):\n        super().__init_subclass__()\n        print(f'Left for {cls}')\n \nclass Right(Top):\n    def __init_subclass__(cls):\n \nItem 48: Validate Subclasses with __init_subclass__ \n207\n\n\n208 \nChapter 6 Metaclasses and Attributes\n        super().__init_subclass__()\n        print(f'Right for {cls}')\n \nclass Bottom(Left, Right):\n    def __init_subclass__(cls):\n        super().__init_subclass__()\n        print(f'Bottom for {cls}')\n>>>\nTop for <class '__main__.Left'>\nTop for <class '__main__.Right'>\nTop for <class '__main__.Bottom'>\nRight for <class '__main__.Bottom'>\nLeft for <class '__main__.Bottom'>\nAs expected, Top.__init_subclass__ is called only a single time for \neach class, even though there are two paths to it for the Bottom class \nthrough its Left and Right parent classes.\nThings to Remember\n✦ The __new__ method of metaclasses is run after the class state-\nment’s entire body has been processed.\n✦ Metaclasses can be used to inspect or modify a class after it’s \ndefined but before it’s created, but they’re often more heavyweight \nthan what you need.\n✦ Use __init_subclass__ to ensure that subclasses are well formed \nat the time they are defined, before objects of their type are \nconstructed.\n✦ Be sure to call super().__init_subclass__ from within your class’s \n__init_subclass__ definition to enable validation in multiple layers \nof classes and multiple inheritance.\nItem 49:  Register Class Existence with \n__init_subclass__\nAnother common use of metaclasses is to automatically register types \nin a program. Registration is useful for doing reverse lookups, where \nyou need to map a simple identifier back to a corresponding class.\nFor example, say that I want to implement my own serialized repre-\nsentation of a Python object using JSON. I need a way to turn an \nobject into a JSON string. Here, I do this generically by defining a \n\n\n \nItem 49: Register Class Existence with __init_subclass__ \n209\nbase class that records the constructor parameters and turns them \ninto a JSON dictionary:\nimport json\n \nclass Serializable:\n    def __init__(self, *args):\n        self.args = args\n \n    def serialize(self):\n        return json.dumps({'args': self.args})\nThis class makes it easy to serialize simple, immutable data struc-\ntures like Point2D to a string:\nclass Point2D(Serializable):\n    def __init__(self, x, y):\n        super().__init__(x, y)\n        self.x = x\n        self.y = y\n \n    def __repr__(self):\n        return f'Point2D({self.x}, {self.y})'\n \npoint = Point2D(5, 3)\nprint('Object:    ', point)\nprint('Serialized:', point.serialize())\n>>>\nObject:     Point2D(5, 3)\nSerialized: {\"args\": [5, 3]}\nNow, I need to deserialize this JSON string and construct the Point2D \nobject it represents. Here, I define another class that can deserialize \nthe data from its Serializable parent class:\nclass Deserializable(Serializable):\n    @classmethod\n    def deserialize(cls, json_data):\n        params = json.loads(json_data)\n        return cls(*params['args'])\nUsing Deserializable makes it easy to serialize and deserialize sim-\nple, immutable objects in a generic way:\nclass BetterPoint2D(Deserializable):\n    ...\n \n",
      "page_number": 218,
      "chapter_number": 22,
      "summary": "This chapter covers segment 22 (pages 218-231). Key topics include classes, attributes, and data. ✦ Use WeakKeyDictionary to ensure that your descriptor classes don’t \ncause memory leaks.",
      "keywords": [
        "foo",
        "called",
        "getattribute",
        "init",
        "polygon",
        "Attributes",
        "sides",
        "dict",
        "subclass",
        "Python",
        "getattr",
        "Item",
        "data",
        "Metaclasses",
        "super"
      ],
      "concepts": [
        "classes",
        "attributes",
        "data",
        "sides",
        "validation",
        "validate",
        "validating",
        "polygon",
        "access",
        "accessing"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 25,
          "title": "Segment 25 (pages 219-229)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 44,
          "title": "Segment 44 (pages 886-908)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 46,
          "title": "Segment 46 (pages 930-951)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 23,
          "title": "Segment 23 (pages 192-207)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 24,
          "title": "Segment 24 (pages 208-218)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 232-240)",
      "start_page": 232,
      "end_page": 240,
      "detection_method": "topic_boundary",
      "content": "210 \nChapter 6 Metaclasses and Attributes\nbefore = BetterPoint2D(5, 3)\nprint('Before:    ', before)\ndata = before.serialize()\nprint('Serialized:', data)\nafter = BetterPoint2D.deserialize(data)\nprint('After:     ', after)\n>>>\nBefore:     Point2D(5, 3)\nSerialized: {\"args\": [5, 3]}\nAfter:      Point2D(5, 3)\nThe problem with this approach is that it works only if you know \nthe intended type of the serialized data ahead of time (e.g., Point2D, \nBetterPoint2D). Ideally, you’d have a large number of classes serializ-\ning to JSON and one common function that could deserialize any of \nthem back to a corresponding Python object.\nTo do this, I can include the serialized object’s class name in the \nJSON data:\nclass BetterSerializable:\n    def __init__(self, *args):\n        self.args = args\n \n    def serialize(self):\n        return json.dumps({\n            'class': self.__class__.__name__,\n            'args': self.args,\n        })\n \n    def __repr__(self):\n        name = self.__class__.__name__\n        args_str = ', '.join(str(x) for x in self.args)\n        return f'{name}({args_str})'\nThen, I can maintain a mapping of class names back to construc-\ntors for those objects. The general deserialize function works for any \nclasses passed to register_class:\nregistry = {}\n \ndef register_class(target_class):\n    registry[target_class.__name__] = target_class\n \ndef deserialize(data):\n    params = json.loads(data)\n\n\n    name = params['class']\n    target_class = registry[name]\n    return target_class(*params['args'])\nTo ensure that deserialize always works properly, I must call \nregister_class for every class I may want to deserialize in the future:\nclass EvenBetterPoint2D(BetterSerializable):\n    def __init__(self, x, y):\n        super().__init__(x, y)\n        self.x = x\n        self.y = y\n \nregister_class(EvenBetterPoint2D)\nNow, I can deserialize an arbitrary JSON string without having to \nknow which class it contains:\nbefore = EvenBetterPoint2D(5, 3)\nprint('Before:    ', before)\ndata = before.serialize()\nprint('Serialized:', data)\nafter = deserialize(data)\nprint('After:     ', after)\n>>>\nBefore:     EvenBetterPoint2D(5, 3)\nSerialized: {\"class\": \"EvenBetterPoint2D\", \"args\": [5, 3]}\nAfter:      EvenBetterPoint2D(5, 3)\nThe problem with this approach is that it’s possible to forget to call \nregister_class:\nclass Point3D(BetterSerializable):\n    def __init__(self, x, y, z):\n        super().__init__(x, y, z)\n        self.x = x\n        self.y = y\n        self.z = z\n \n# Forgot to call register_class! Whoops!\nThis causes the code to break at runtime, when I finally try to deseri-\nalize an instance of a class I forgot to register:\npoint = Point3D(5, 9, -4)\ndata = point.serialize()\ndeserialize(data)\n \nItem 49: Register Class Existence with __init_subclass__ \n211\n\n\n212 \nChapter 6 Metaclasses and Attributes\n>>>\nTraceback ...\nKeyError: 'Point3D'\nEven though I chose to subclass BetterSerializable, I don’t actually \nget all of its features if I forget to call register_class after the class \nstatement body. This approach is error prone and especially chal-\nlenging for beginners. The same omission can happen with class dec-\norators (see Item 51: “Prefer Class Decorators Over Metaclasses for \nComposable Class Extensions” for when those are appropriate).\nWhat if I could somehow act on the programmer’s intent to use \nBetterSerializable and ensure that register_class is called in all \ncases? Metaclasses enable this by intercepting the class statement \nwhen subclasses are defined (see Item 48: “Validate Subclasses with \n__init_subclass__” for details on the machinery). Here, I use a meta-\nclass to register the new type immediately after the class’s body:\nclass Meta(type):\n    def __new__(meta, name, bases, class_dict):\n        cls = type.__new__(meta, name, bases, class_dict)\n        register_class(cls)\n        return cls\n \nclass RegisteredSerializable(BetterSerializable,\n                             metaclass=Meta):\n    pass\nWhen I define a subclass of RegisteredSerializable, I can be confident \nthat the call to register_class happened and deserialize will always \nwork as expected:\nclass Vector3D(RegisteredSerializable):\n    def __init__(self, x, y, z):\n        super().__init__(x, y, z)\n        self.x, self.y, self.z = x, y, z\n \nbefore = Vector3D(10, -7, 3)\nprint('Before:    ', before)\ndata = before.serialize()\nprint('Serialized:', data)\nprint('After:     ', deserialize(data))\n>>>\nBefore:     Vector3D(10, -7, 3)\nSerialized: {\"class\": \"Vector3D\", \"args\": [10, -7, 3]}\nAfter:      Vector3D(10, -7, 3)\n\n\nAn even better approach is to use the __init_subclass__ special class \nmethod. This simplified syntax, introduced in Python 3.6, reduces \nthe visual noise of applying custom logic when a class is defined. It \nalso makes it more approachable to beginners who may be confused \nby the complexity of metaclass syntax:\nclass BetterRegisteredSerializable(BetterSerializable):\n    def __init_subclass__(cls):\n        super().__init_subclass__()\n        register_class(cls)\n \nclass Vector1D(BetterRegisteredSerializable):\n    def __init__(self, magnitude):\n        super().__init__(magnitude)\n        self.magnitude = magnitude\n \nbefore = Vector1D(6)\nprint('Before:    ', before)\ndata = before.serialize()\nprint('Serialized:', data)\nprint('After:     ', deserialize(data))\n>>>\nBefore:     Vector1D(6)\nSerialized: {\"class\": \"Vector1D\", \"args\": [6]}\nAfter:      Vector1D(6)\nBy using __init_subclass__ (or metaclasses) for class registration, \nyou can ensure that you’ll never miss registering a class as long as \nthe inheritance tree is right. This works well for serialization, as \nI’ve shown, and also applies to database object-relational mappings \n(ORMs), extensible plug-in systems, and callback hooks.\nThings to Remember\n✦ Class registration is a helpful pattern for building modular Python \nprograms.\n✦ Metaclasses let you run registration code automatically each time a \nbase class is subclassed in a program.\n✦ Using metaclasses for class registration helps you avoid errors by \nensuring that you never miss a registration call.\n✦ Prefer __init_subclass__ over standard metaclass machinery \nbecause it’s clearer and easier for beginners to understand.\n \nItem 49: Register Class Existence with __init_subclass__ \n213\n\n\n214 \nChapter 6 Metaclasses and Attributes\nItem 50: Annotate Class Attributes with __set_name__\nOne more useful feature enabled by metaclasses is the ability to mod-\nify or annotate properties after a class is defined but before the class \nis actually used. This approach is commonly used with descriptors \n(see Item 46: “Use Descriptors for Reusable @property Methods”) to \ngive them more introspection into how they’re being used within their \ncontaining class.\nFor example, say that I want to define a new class that represents a \nrow in a customer database. I’d like to have a corresponding property \non the class for each column in the database table. Here, I define a \ndescriptor class to connect attributes to column names:\nclass Field:\n    def __init__(self, name):\n        self.name = name\n        self.internal_name = '_' + self.name\n \n    def __get__(self, instance, instance_type):\n        if instance is None:\n            return self\n        return getattr(instance, self.internal_name, '')\n \n    def __set__(self, instance, value):\n        setattr(instance, self.internal_name, value)\nWith the column name stored in the Field descriptor, I can save all of \nthe per-instance state directly in the instance dictionary as protected \nfields by using the setattr built-in function, and later I can load state \nwith getattr. At first, this seems to be much more convenient than \nbuilding descriptors with the weakref built-in module to avoid mem-\nory leaks.\nDefining the class representing a row requires supplying the data-\nbase table’s column name for each class attribute:\nclass Customer:\n    # Class attributes\n    first_name = Field('first_name')\n    last_name = Field('last_name')\n    prefix = Field('prefix')\n    suffix = Field('suffix')\nUsing the class is simple. Here, you can see how the Field descriptors \nmodify the instance dictionary __dict__ as expected:\ncust = Customer()\nprint(f'Before: {cust.first_name!r} {cust.__dict__}')\n\n\n \nItem 50: Annotate Class Attributes with __set_name__ \n215\ncust.first_name = 'Euclid'\nprint(f'After:  {cust.first_name!r} {cust.__dict__}')\n>>>\nBefore: '' {}\nAfter:  'Euclid' {'_first_name': 'Euclid'}\nBut the class definition seems redundant. I already declared the \nname of the field for the class on the left ('field_name ='). Why do \nI also have to pass a string containing the same information to the \nField constructor (Field('first_name')) on the right?\nclass Customer:\n    # Left side is redundant with right side\n    first_name = Field('first_name')\n    ...\nThe problem is that the order of operations in the Customer class defi-\nnition is the opposite of how it reads from left to right. First, the Field \nconstructor is called as Field('first_name'). Then, the return value \nof that is assigned to Customer.field_name. There’s no way for a Field \ninstance to know upfront which class attribute it will be assigned to.\nTo eliminate this redundancy, I can use a metaclass. Metaclasses \nlet you hook the class statement directly and take action as soon \nas a class body is finished (see Item 48: “Validate Subclasses with \n__init_subclass__” for details on how they work). In this case, I can \nuse the metaclass to assign Field.name and Field.internal_name on \nthe descriptor automatically instead of manually specifying the field \nname multiple times:\nclass Meta(type):\n    def __new__(meta, name, bases, class_dict):\n        for key, value in class_dict.items():\n            if isinstance(value, Field):\n                value.name = key\n                value.internal_name = '_' + key\n        cls = type.__new__(meta, name, bases, class_dict)\n        return cls\nHere, I define a base class that uses the metaclass. All classes repre-\nsenting database rows should inherit from this class to ensure that \nthey use the metaclass:\nclass DatabaseRow(metaclass=Meta):\n    pass\nTo work with the metaclass, the Field descriptor is largely unchanged. \nThe only difference is that it no longer requires arguments to be passed \n\n\n216 \nChapter 6 Metaclasses and Attributes\nto its constructor. Instead, its attributes are set by the Meta.__new__ \nmethod above:\nclass Field:\n    def __init__(self):\n        # These will be assigned by the metaclass.\n        self.name = None\n        self.internal_name = None\n \n    def __get__(self, instance, instance_type):\n        if instance is None:\n            return self\n        return getattr(instance, self.internal_name, '')\n \n    def __set__(self, instance, value):\n        setattr(instance, self.internal_name, value)\nBy using the metaclass, the new DatabaseRow base class, and the new \nField descriptor, the class definition for a database row no longer has \nthe redundancy from before:\nclass BetterCustomer(DatabaseRow):\n    first_name = Field()\n    last_name = Field()\n    prefix = Field()\n    suffix = Field()\nThe behavior of the new class is identical to the behavior of the old \none:\ncust = BetterCustomer()\nprint(f'Before: {cust.first_name!r} {cust.__dict__}')\ncust.first_name = 'Euler'\nprint(f'After:  {cust.first_name!r} {cust.__dict__}')\n>>>\nBefore: '' {}\nAfter:  'Euler' {'_first_name': 'Euler'}\nThe trouble with this approach is that you can’t use the Field class for \nproperties unless you also inherit from DatabaseRow. If you somehow \nforget to subclass DatabaseRow, or if you don’t want to due to other \nstructural requirements of the class hierarchy, the code will break:\nclass BrokenCustomer:\n    first_name = Field()\n    last_name = Field()\n    prefix = Field()\n    suffix = Field()\n \n\n\ncust = BrokenCustomer()\ncust.first_name = 'Mersenne'\n>>>\nTraceback ...\nTypeError: attribute name must be string, not 'NoneType'\nThe solution to this problem is to use the __set_name__ special method \nfor descriptors. This method, introduced in Python 3.6, is called on \nevery descriptor instance when its containing class is defined. It \nreceives as parameters the owning class that contains the descriptor \ninstance and the attribute name to which the descriptor instance was \nassigned. Here, I avoid defining a metaclass entirely and move what \nthe Meta.__new__ method from above was doing into __set_name__:\nclass Field:\n    def __init__(self):\n        self.name = None\n        self.internal_name = None\n \n    def __set_name__(self, owner, name):\n        # Called on class creation for each descriptor\n        self.name = name\n        self.internal_name = '_' + name\n \n    def __get__(self, instance, instance_type):\n        if instance is None:\n            return self\n        return getattr(instance, self.internal_name, '')\n \n    def __set__(self, instance, value):\n        setattr(instance, self.internal_name, value)\nNow, I can get the benefits of the Field descriptor without having to \ninherit from a specific parent class or having to use a metaclass:\nclass FixedCustomer:\n    first_name = Field()\n    last_name = Field()\n    prefix = Field()\n    suffix = Field()\n \ncust = FixedCustomer()\nprint(f'Before: {cust.first_name!r} {cust.__dict__}')\ncust.first_name = 'Mersenne'\nprint(f'After:  {cust.first_name!r} {cust.__dict__}')\n \nItem 50: Annotate Class Attributes with __set_name__ \n217\n\n\n218 \nChapter 6 Metaclasses and Attributes\n>>>\nBefore: '' {}\nAfter:  'Mersenne' {'_first_name': 'Mersenne'}\nThings to Remember\n✦ Metaclasses enable you to modify a class’s attributes before the \nclass is fully defined.\n✦ Descriptors and metaclasses make a powerful combination for \ndeclarative behavior and runtime introspection.\n✦ Define __set_name__ on your descriptor classes to allow them to \ntake into account their surrounding class and its property names.\n✦ Avoid memory leaks and the weakref built-in module by having \ndescriptors store data they manipulate directly within a class’s \ninstance dictionary.\nItem 51:  Prefer Class Decorators Over Metaclasses for \nComposable Class Extensions\nAlthough metaclasses allow you to customize class creation in multi-\nple ways (see Item 48: “Validate Subclasses with __init_subclass__” \nand Item 49: “Register Class Existence with __init_subclass__”), they \nstill fall short of handling every situation that may arise.\nFor example, say that I want to decorate all of the methods of a class \nwith a helper that prints arguments, return values, and exceptions \nraised. Here, I define the debugging decorator (see Item 26: “Define \nFunction Decorators with functools.wraps” for background):\nfrom functools import wraps\n \ndef trace_func(func):\n    if hasattr(func, 'tracing'):  # Only decorate once\n        return func\n \n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        result = None\n        try:\n            result = func(*args, **kwargs)\n            return result\n        except Exception as e:\n            result = e\n            raise\n",
      "page_number": 232,
      "chapter_number": 23,
      "summary": "This chapter covers segment 23 (pages 232-240). Key topics include classes, field, and data. Ideally, you’d have a large number of classes serializ-\ning to JSON and one common function that could deserialize any of \nthem back to a corresponding Python object.",
      "keywords": [
        "Field",
        "init",
        "json class Serializable",
        "class Field",
        "instance",
        "data",
        "Register Class Existence",
        "Annotate Class Attributes",
        "Field descriptor",
        "Register",
        "Class Attributes",
        "Serialized",
        "args",
        "Item",
        "Metaclasses"
      ],
      "concepts": [
        "classes",
        "field",
        "data",
        "requires",
        "requirements",
        "item",
        "point",
        "instance",
        "serialized",
        "serialization"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 20,
          "title": "Segment 20 (pages 392-414)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 47,
          "title": "Segment 47 (pages 952-973)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 10,
          "title": "Segment 10 (pages 192-209)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 20,
          "title": "Segment 20 (pages 168-176)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 241-248)",
      "start_page": 241,
      "end_page": 248,
      "detection_method": "topic_boundary",
      "content": " \nItem 51: Prefer Class Decorators Over Metaclasses \n219\n        finally:\n            print(f'{func.__name__}({args!r}, {kwargs!r}) -> '\n                  f'{result!r}')\n \n    wrapper.tracing = True\n    return wrapper\nI can apply this decorator to various special methods in my new dict \nsubclass (see Item 43: “Inherit from collections.abc for Custom Con-\ntainer Types” for background):\nclass TraceDict(dict):\n    @trace_func\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n \n    @trace_func\n    def __setitem__(self, *args, **kwargs):\n        return super().__setitem__(*args, **kwargs)\n \n    @trace_func\n    def __getitem__(self, *args, **kwargs):\n        return super().__getitem__(*args, **kwargs)\n \n    ...\nAnd I can verify that these methods are decorated by interacting with \nan instance of the class:\ntrace_dict = TraceDict([('hi', 1)])\ntrace_dict['there'] = 2\ntrace_dict['hi']\ntry:\n    trace_dict['does not exist']\nexcept KeyError:\n    pass  # Expected\n>>>\n__init__(({'hi': 1}, [('hi', 1)]), {}) -> None\n__setitem__(({'hi': 1, 'there': 2}, 'there', 2), {}) -> None\n__getitem__(({'hi': 1, 'there': 2}, 'hi'), {}) -> 1\n__getitem__(({'hi': 1, 'there': 2}, 'does not exist'), \n¯{}) -> KeyError('does not exist')\nThe problem with this code is that I had to redefine all of the methods \nthat I wanted to decorate with @trace_func. This is redundant boiler-\nplate that’s hard to read and error prone. Further, if a new method is \n\n\n220 \nChapter 6 Metaclasses and Attributes\nlater added to the dict superclass, it won’t be decorated unless I also \ndefine it in TraceDict.\nOne way to solve this problem is to use a metaclass to automati-\ncally decorate all methods of a class. Here, I implement this behav-\nior by wrapping each function or method in the new type with the \ntrace_func decorator:\nimport types\n \ntrace_types = (\n    types.MethodType,\n    types.FunctionType,\n    types.BuiltinFunctionType,\n    types.BuiltinMethodType,\n    types.MethodDescriptorType,\n    types.ClassMethodDescriptorType)\n \nclass TraceMeta(type):\n    def __new__(meta, name, bases, class_dict):\n        klass = super().__new__(meta, name, bases, class_dict)\n \n        for key in dir(klass):\n            value = getattr(klass, key)\n            if isinstance(value, trace_types):\n                wrapped = trace_func(value)\n                setattr(klass, key, wrapped)\n \n        return klass\nNow, I can declare my dict subclass by using the TraceMeta metaclass \nand verify that it works as expected:\nclass TraceDict(dict, metaclass=TraceMeta):\n    pass\n \ntrace_dict = TraceDict([('hi', 1)])\ntrace_dict['there'] = 2\ntrace_dict['hi']\ntry:\n    trace_dict['does not exist']\nexcept KeyError:\n    pass  # Expected\n>>>\n__new__((<class '__main__.TraceDict'>, [('hi', 1)]), {}) -> {}\n__getitem__(({'hi': 1, 'there': 2}, 'hi'), {}) -> 1\n\n\n__getitem__(({'hi': 1, 'there': 2}, 'does not exist'), \n¯{}) -> KeyError('does not exist')\nThis works, and it even prints out a call to __new__ that was miss-\ning from my earlier implementation. What happens if I try to use \nTraceMeta when a superclass already has specified a metaclass?\nclass OtherMeta(type):\n    pass\n \nclass SimpleDict(dict, metaclass=OtherMeta):\n    pass\n \nclass TraceDict(SimpleDict, metaclass=TraceMeta):\n    pass\n>>>\nTraceback ...\nTypeError: metaclass conflict: the metaclass of a derived \n¯class must be a (non-strict) subclass of the metaclasses \n¯of all its bases\nThis fails because TraceMeta does not inherit from OtherMeta. In the-\nory, I can use metaclass inheritance to solve this problem by having \nOtherMeta inherit from TraceMeta:\nclass TraceMeta(type):\n    ...\n \nclass OtherMeta(TraceMeta):\n    pass\n \nclass SimpleDict(dict, metaclass=OtherMeta):\n    pass\n \nclass TraceDict(SimpleDict, metaclass=TraceMeta):\n    pass\n \ntrace_dict = TraceDict([('hi', 1)])\ntrace_dict['there'] = 2\ntrace_dict['hi']\ntry:\n    trace_dict['does not exist']\nexcept KeyError:\n    pass  # Expected\n \nItem 51: Prefer Class Decorators Over Metaclasses \n221\n\n\n222 \nChapter 6 Metaclasses and Attributes\n>>>\n__init_subclass__((), {}) -> None\n__new__((<class '__main__.TraceDict'>, [('hi', 1)]), {}) -> {}\n__getitem__(({'hi': 1, 'there': 2}, 'hi'), {}) -> 1\n__getitem__(({'hi': 1, 'there': 2}, 'does not exist'), \n¯{}) -> KeyError('does not exist')\nBut this won’t work if the metaclass is from a library that I can’t mod-\nify, or if I want to use multiple utility metaclasses like TraceMeta at \nthe same time. The metaclass approach puts too many constraints on \nthe class that’s being modified.\nTo solve this problem, Python supports class decorators. Class \n decorators work just like function decorators: They’re applied with the \n@ symbol prefixing a function before the class declaration. The func-\ntion is expected to modify or re-create the class accordingly and then \nreturn it:\ndef my_class_decorator(klass):\n    klass.extra_param = 'hello'\n    return klass\n \n@my_class_decorator\nclass MyClass:\n    pass\n \nprint(MyClass)\nprint(MyClass.extra_param)\n>>>\n<class '__main__.MyClass'>\nhello\nI can implement a class decorator to apply trace_func to all methods \nand functions of a class by moving the core of the TraceMeta.__new__ \nmethod above into a stand-alone function. This implementation is \nmuch shorter than the metaclass version:\ndef trace(klass):\n    for key in dir(klass):\n        value = getattr(klass, key)\n        if isinstance(value, trace_types):\n            wrapped = trace_func(value)\n            setattr(klass, key, wrapped)\n    return klass\n\n\nI can apply this decorator to my dict subclass to get the same behav-\nior as I get by using the metaclass approach above:\n@trace\nclass TraceDict(dict):\n    pass\n \ntrace_dict = TraceDict([('hi', 1)])\ntrace_dict['there'] = 2\ntrace_dict['hi']\ntry:\n    trace_dict['does not exist']\nexcept KeyError:\n    pass  # Expected\n>>>\n__new__((<class '__main__.TraceDict'>, [('hi', 1)]), {}) -> {}\n__getitem__(({'hi': 1, 'there': 2}, 'hi'), {}) -> 1\n__getitem__(({'hi': 1, 'there': 2}, 'does not exist'), \n¯{}) -> KeyError('does not exist')\nClass decorators also work when the class being decorated already \nhas a metaclass:\nclass OtherMeta(type):\n    pass\n \n@trace\nclass TraceDict(dict, metaclass=OtherMeta):\n    pass\n \ntrace_dict = TraceDict([('hi', 1)])\ntrace_dict['there'] = 2\ntrace_dict['hi']\ntry:\n    trace_dict['does not exist']\nexcept KeyError:\n    pass  # Expected\n>>>\n__new__((<class '__main__.TraceDict'>, [('hi', 1)]), {}) -> {}\n__getitem__(({'hi': 1, 'there': 2}, 'hi'), {}) -> 1\n__getitem__(({'hi': 1, 'there': 2}, 'does not exist'), \n¯{}) -> KeyError('does not exist')\nWhen you’re looking for composable ways to extend classes, class \ndecorators are the best tool for the job. (See Item 73: “Know How \n \nItem 51: Prefer Class Decorators Over Metaclasses \n223\n\n\n224 \nChapter 6 Metaclasses and Attributes\nto Use heapq for Priority Queues” for a useful class decorator called \nfunctools.total_ordering.)\nThings to Remember\n✦ A class decorator is a simple function that receives a class instance \nas a parameter and returns either a new class or a modified version \nof the original class.\n✦ Class decorators are useful when you want to modify every method \nor attribute of a class with minimal boilerplate.\n✦ Metaclasses can’t be composed together easily, while many class \ndecorators can be used to extend the same class without conflicts.\n\n\n7\nConcurrency and \nParallelism\nConcurrency enables a computer to do many different things  seemingly \nat the same time. For example, on a computer with one CPU core, the \noperating system rapidly changes which program is running on the \nsingle processor. In doing so, it interleaves execution of the programs, \nproviding the illusion that the programs are running simultaneously.\nParallelism, in contrast, involves actually doing many different things \nat the same time. A computer with multiple CPU cores can execute \nmultiple programs simultaneously. Each CPU core runs the instruc-\ntions of a separate program, allowing each program to make forward \nprogress during the same instant.\nWithin a single program, concurrency is a tool that makes it easier \nfor programmers to solve certain types of problems. Concurrent pro-\ngrams enable many distinct paths of execution, including separate \nstreams of I/O, to make forward progress in a way that seems to be \nboth simultaneous and independent.\nThe key difference between parallelism and concurrency is speedup. \nWhen two distinct paths of execution in a program make forward \nprogress in parallel, the time it takes to do the total work is cut in \nhalf; the speed of execution is faster by a factor of two. In contrast, \nconcurrent programs may run thousands of separate paths of execu-\ntion seemingly in parallel but provide no speedup for the total work.\nPython makes it easy to write concurrent programs in a variety of \nstyles. Threads support a relatively small amount of concurrency, \nwhile coroutines enable vast numbers of concurrent functions. \nPython can also be used to do parallel work through system calls, \nsubprocesses, and C extensions. But it can be very difficult to make \nconcurrent Python code truly run in parallel. It’s important to under-\nstand how to best utilize Python in these different situations.\n\n\n226 \nChapter 7 Concurrency and Parallelism\nItem 52: Use subprocess to Manage Child Processes\nPython has battle-hardened libraries for running and managing child \nprocesses. This makes it a great language for gluing together other \ntools, such as command-line utilities. When existing shell scripts get \ncomplicated, as they often do over time, graduating them to a rewrite \nin Python for the sake of readability and maintainability is a natural \nchoice.\nChild processes started by Python are able to run in parallel, enabling \nyou to use Python to consume all of the CPU cores of a machine and \nmaximize the throughput of programs. Although Python itself may \nbe CPU bound (see Item 53: “Use Threads for Blocking I/O, Avoid for \nParallelism”), it’s easy to use Python to drive and coordinate CPU- \nintensive workloads.\nPython has many ways to run subprocesses (e.g., os.popen, os.exec*), \nbut the best choice for managing child processes is to use the \nsubprocess built-in module. Running a child process with subprocess \nis simple. Here, I use the module’s run convenience function to start a \nprocess, read its output, and verify that it terminated cleanly:\nimport subprocess\n \nresult = subprocess.run(\n    ['echo', 'Hello from the child!'],\n    capture_output=True,\n    encoding='utf-8')\n \nresult.check_returncode()  # No exception means clean exit\nprint(result.stdout)\n>>>\nHello from the child!\nNote\nThe examples in this item assume that your system has the echo, sleep, and \nopenssl commands available. On Windows, this may not be the case. Please \nrefer to the full example code for this item to see specific directions on how to \nrun these snippets on Windows.\nChild processes run independently from their parent process, the \nPython interpreter. If I create a subprocess using the Popen class \ninstead of the run function, I can poll child process status periodically \nwhile Python does other work:\nproc = subprocess.Popen(['sleep', '1'])\nwhile proc.poll() is None:\n    print('Working...')\n",
      "page_number": 241,
      "chapter_number": 24,
      "summary": "This chapter covers segment 24 (pages 241-248). Key topics include classes, types, and concurrency. ✦ Descriptors and metaclasses make a powerful combination for \ndeclarative behavior and runtime introspection.",
      "keywords": [
        "Class Decorators",
        "Prefer Class Decorators",
        "trace",
        "dict",
        "Metaclasses",
        "class TraceDict",
        "TraceDict",
        "Item",
        "Decorators",
        "exist",
        "Prefer Class",
        "metaclass",
        "trace class TraceDict",
        "pass class TraceDict",
        "pass class"
      ],
      "concepts": [
        "classes",
        "types",
        "concurrency",
        "concurrent",
        "item",
        "program",
        "python",
        "decorators",
        "decorate",
        "decorated"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 47,
          "title": "Segment 47 (pages 952-973)",
          "relevance_score": 0.78,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 48,
          "title": "Segment 48 (pages 974-992)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 25,
          "title": "Segment 25 (pages 219-229)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 17,
          "title": "Segment 17 (pages 334-353)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.66,
          "method": "api"
        }
      ]
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 249-264)",
      "start_page": 249,
      "end_page": 264,
      "detection_method": "topic_boundary",
      "content": " \nItem 52: Use subprocess to Manage Child Processes \n227\n    # Some time-consuming work here\n    ...\n \nprint('Exit status', proc.poll())\n>>>\nWorking...\nWorking...\nWorking...\nWorking...\nExit status 0\nDecoupling the child process from the parent frees up the parent \n process to run many child processes in parallel. Here, I do this by \nstarting all the child processes together with Popen upfront:\nimport time\n \nstart = time.time()\nsleep_procs = []\nfor _ in range(10):\n    proc = subprocess.Popen(['sleep', '1'])\n    sleep_procs.append(proc)\nLater, I wait for them to finish their I/O and terminate with the \ncommunicate method:\nfor proc in sleep_procs:\n    proc.communicate()\n \nend = time.time()\ndelta = end - start\nprint(f'Finished in {delta:.3} seconds')\n>>>\nFinished in 1.05 seconds\nIf these processes ran in sequence, the total delay would be 10  seconds \nor more rather than the ~1 second that I measured.\nYou can also pipe data from a Python program into a subprocess and \nretrieve its output. This allows you to utilize many other programs to \ndo work in parallel. For example, say that I want to use the openssl \ncommand-line tool to encrypt some data. Starting the child process \nwith command-line arguments and I/O pipes is easy:\nimport os\ndef run_encrypt(data):\n    env = os.environ.copy()\n\n\n228 \nChapter 7 Concurrency and Parallelism\n    env['password'] = 'zf7ShyBhZOraQDdE/FiZpm/m/8f9X+M1'\n    proc = subprocess.Popen(\n        ['openssl', 'enc', '-des3', '-pass', 'env:password'],\n        env=env,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE)\n    proc.stdin.write(data)\n    proc.stdin.flush()  # Ensure that the child gets input\n    return proc\nHere, I pipe random bytes into the encryption function, but in prac-\ntice this input pipe would be fed data from user input, a file handle, a \nnetwork socket, and so on:\nprocs = []\nfor _ in range(3):\n    data = os.urandom(10)\n    proc = run_encrypt(data)\n    procs.append(proc)\nThe child processes run in parallel and consume their input. Here, \nI wait for them to finish and then retrieve their final output. The \n output is random encrypted bytes as expected:\nfor proc in procs:\n    out, _ = proc.communicate()\n    print(out[-10:])\n>>>\nb'\\x8c(\\xed\\xc7m1\\xf0F4\\xe6'\nb'\\x0eD\\x97\\xe9>\\x10h{\\xbd\\xf0'\nb'g\\x93)\\x14U\\xa9\\xdc\\xdd\\x04\\xd2'\nIt’s also possible to create chains of parallel processes, just like \nUNIX pipelines, connecting the output of one child process to the \ninput of another, and so on. Here’s a function that starts the openssl \n command-line tool as a subprocess to generate a Whirlpool hash of \nthe input stream:\ndef run_hash(input_stdin):\n    return subprocess.Popen(\n        ['openssl', 'dgst', '-whirlpool', '-binary'],\n        stdin=input_stdin,\n        stdout=subprocess.PIPE)\nNow, I can kick off one set of processes to encrypt some data and \nanother set of processes to subsequently hash their encrypted output. \nNote that I have to be careful with how the stdout instance of the \n\n\nupstream process is retained by the Python interpreter process that’s \nstarting this pipeline of child processes:\nencrypt_procs = []\nhash_procs = []\nfor _ in range(3):\n    data = os.urandom(100)\n \n    encrypt_proc = run_encrypt(data)\n    encrypt_procs.append(encrypt_proc)\n \n    hash_proc = run_hash(encrypt_proc.stdout)\n    hash_procs.append(hash_proc)\n \n    # Ensure that the child consumes the input stream and\n    # the communicate() method doesn't inadvertently steal\n    # input from the child. Also lets SIGPIPE propagate to\n    # the upstream process if the downstream process dies.\n    encrypt_proc.stdout.close()\n    encrypt_proc.stdout = None\nThe I/O between the child processes happens automatically once they \nare started. All I need to do is wait for them to finish and print the \nfinal output:\nfor proc in encrypt_procs:\n    proc.communicate()\n    assert proc.returncode == 0\n \nfor proc in hash_procs:\n    out, _ = proc.communicate()\n    print(out[-10:])\n    assert proc.returncode == 0\n>>>\nb'\\xe2j\\x98h\\xfd\\xec\\xe7T\\xd84'\nb'\\xf3.i\\x01\\xd74|\\xf2\\x94E'\nb'5_n\\xc3-\\xe6j\\xeb[i'\nIf I’m worried about the child processes never finishing or somehow \nblocking on input or output pipes, I can pass the timeout parameter \nto the communicate method. This causes an exception to be raised if \nthe child process hasn’t finished within the time period, giving me a \nchance to terminate the misbehaving subprocess:\nproc = subprocess.Popen(['sleep', '10'])\ntry:\n    proc.communicate(timeout=0.1)\n \nItem 52: Use subprocess to Manage Child Processes \n229\n\n\n230 \nChapter 7 Concurrency and Parallelism\nexcept subprocess.TimeoutExpired:\n    proc.terminate()\n    proc.wait()\n \nprint('Exit status', proc.poll())\n>>>\nExit status -15\nThings to Remember\n✦ Use the subprocess module to run child processes and manage their \ninput and output streams.\n✦ Child processes run in parallel with the Python interpreter, enabling \nyou to maximize your usage of CPU cores.\n✦ Use the run convenience function for simple usage, and the Popen \nclass for advanced usage like UNIX-style pipelines.\n✦ Use the timeout parameter of the communicate method to avoid dead-\nlocks and hanging child processes.\nItem 53:  Use Threads for Blocking I/O, Avoid for \nParallelism\nThe standard implementation of Python is called CPython. CPython \nruns a Python program in two steps. First, it parses and compiles the \nsource text into bytecode, which is a low-level representation of the \nprogram as 8-bit instructions. (As of Python 3.6, however, it’s tech-\nnically wordcode with 16-bit instructions, but the idea is the same.) \nThen, CPython runs the bytecode using a stack-based interpreter. The \nbytecode interpreter has state that must be maintained and coherent \nwhile the Python program executes. CPython enforces coherence with \na mechanism called the global interpreter lock (GIL).\nEssentially, the GIL is a mutual-exclusion lock (mutex) that prevents \nCPython from being affected by preemptive multithreading, where \none thread takes control of a program by interrupting another thread. \nSuch an interruption could corrupt the interpreter state (e.g., garbage \ncollection reference counts) if it comes at an unexpected time. The \nGIL prevents these interruptions and ensures that every bytecode \ninstruction works correctly with the CPython implementation and its \nC-extension modules.\nThe GIL has an important negative side effect. With programs written \nin languages like C++ or Java, having multiple threads of execution \n\n\n \nItem 53: Use Threads for Blocking I/O, Avoid for Parallelism \n231\nmeans that a program could utilize multiple CPU cores at the same \ntime. Although Python supports multiple threads of execution, the GIL \ncauses only one of them to ever make forward progress at a time. This \nmeans that when you reach for threads to do parallel computation \nand speed up your Python programs, you will be sorely disappointed.\nFor example, say that I want to do something computationally inten-\nsive with Python. Here, I use a naive number factorization algorithm \nas a proxy:\ndef factorize(number):\n    for i in range(1, number + 1):\n        if number % i == 0:\n            yield i\nFactoring a set of numbers in serial takes quite a long time:\nimport time\n \nnumbers = [2139079, 1214759, 1516637, 1852285]\nstart = time.time()\n \nfor number in numbers:\n    list(factorize(number))\n \nend = time.time()\ndelta = end - start\nprint(f'Took {delta:.3f} seconds')\n>>>\nTook 0.399 seconds\nUsing multiple threads to do this computation would make sense in \nother languages because I could take advantage of all the CPU cores \nof my computer. Let me try that in Python. Here, I define a Python \nthread for doing the same computation as before:\nfrom threading import Thread\n \nclass FactorizeThread(Thread):\n    def __init__(self, number):\n        super().__init__()\n        self.number = number\n \n    def run(self):\n        self.factors = list(factorize(self.number))\n\n\n232 \nChapter 7 Concurrency and Parallelism\nThen, I start a thread for each number to factorize in parallel:\nstart = time.time()\n \nthreads = []\nfor number in numbers:\n    thread = FactorizeThread(number)\n    thread.start()\n    threads.append(thread)\nFinally, I wait for all of the threads to finish:\nfor thread in threads:\n    thread.join()\n \nend = time.time()\ndelta = end - start\nprint(f'Took {delta:.3f} seconds')\n>>>\nTook 0.446 seconds\nSurprisingly, this takes even longer than running factorize in serial. \nWith one thread per number, you might expect less than a 4x speedup \nin other languages due to the overhead of creating threads and coor-\ndinating with them. You might expect only a 2x speedup on the dual-\ncore machine I used to run this code. But you wouldn’t expect the \nperformance of these threads to be worse when there are multiple \nCPUs to utilize. This demonstrates the effect of the GIL (e.g., lock con-\ntention and scheduling overhead) on programs running in the stan-\ndard CPython interpreter.\nThere are ways to get CPython to utilize multiple cores, but they \ndon’t work with the standard Thread class (see Item 64: “Consider \nconcurrent.futures for True Parallelism”), and they can require sub-\nstantial effort. Given these limitations, why does Python support \nthreads at all? There are two good reasons.\nFirst, multiple threads make it easy for a program to seem like it’s \ndoing multiple things at the same time. Managing the juggling act \nof simultaneous tasks is difficult to implement yourself (see Item 56: \n“Know How to Recognize When Concurrency Is Necessary” for an \nexample). With threads, you can leave it to Python to run your func-\ntions concurrently. This works because CPython ensures a level of \nfairness between Python threads of execution, even though only one \nof them makes forward progress at a time due to the GIL.\nThe second reason Python supports threads is to deal with blocking \nI/O, which happens when Python does certain types of system calls. \n\n\n \nItem 53: Use Threads for Blocking I/O, Avoid for Parallelism \n233\nA Python program uses system calls to ask the computer’s  operating \nsystem to interact with the external environment on its behalf. Block-\ning I/O includes things like reading and writing files, interacting \nwith networks, communicating with devices like displays, and so on. \nThreads help handle blocking I/O by insulating a program from the \ntime it takes for the operating system to respond to requests.\nFor example, say that I want to send a signal to a remote-controlled \nhelicopter through a serial port. I’ll use a slow system call (select) as \na proxy for this activity. This function asks the operating system to \nblock for 0.1 seconds and then return control to my program, which is \nsimilar to what would happen when using a synchronous serial port:\nimport select\nimport socket\n \ndef slow_systemcall():\n    select.select([socket.socket()], [], [], 0.1)\nRunning this system call in serial requires a linearly increasing \namount of time:\nstart = time.time()\n \nfor _ in range(5):\n    slow_systemcall()\n \nend = time.time()\ndelta = end - start\nprint(f'Took {delta:.3f} seconds')\n>>>\nTook 0.510 seconds\nThe problem is that while the slow_systemcall function is running, my \nprogram can’t make any other progress. My program’s main thread of \nexecution is blocked on the select system call. This situation is awful \nin practice. You need to be able to compute your helicopter’s next move \nwhile you’re sending it a signal; otherwise, it’ll crash. When you find \nyourself needing to do blocking I/O and computation simultaneously, \nit’s time to consider moving your system calls to threads.\nHere, I run multiple invocations of the slow_systemcall function in \nseparate threads. This would allow me to communicate with multiple \nserial ports (and helicopters) at the same time while leaving the main \nthread to do whatever computation is required:\nstart = time.time()\n \n\n\n234 \nChapter 7 Concurrency and Parallelism\nthreads = []\nfor _ in range(5):\n    thread = Thread(target=slow_systemcall)\n    thread.start()\n    threads.append(thread)\nWith the threads started, here I do some work to calculate the next \nhelicopter move before waiting for the system call threads to finish:\ndef compute_helicopter_location(index):\n    ...\n \nfor i in range(5):\n    compute_helicopter_location(i)\n \nfor thread in threads:\n    thread.join()\n \nend = time.time()\ndelta = end - start\nprint(f'Took {delta:.3f} seconds')\n>>>\nTook 0.108 seconds\nThe parallel time is ~5x less than the serial time. This shows that \nall the system calls will run in parallel from multiple Python threads \neven though they’re limited by the GIL. The GIL prevents my Python \ncode from running in parallel, but it doesn’t have an effect on system \ncalls. This works because Python threads release the GIL just before \nthey make system calls, and they reacquire the GIL as soon as the \nsystem calls are done.\nThere are many other ways to deal with blocking I/O besides using \nthreads, such as the asyncio built-in module, and these alternatives \nhave important benefits. But those options might require extra work \nin refactoring your code to fit a different model of execution (see Item \n60: “Achieve Highly Concurrent I/O with Coroutines” and Item 62: \n“Mix Threads and Coroutines to Ease the Transition to asyncio”). \nUsing threads is the simplest way to do blocking I/O in parallel with \nminimal changes to your program.\nThings to Remember\n✦ Python threads can’t run in parallel on multiple CPU cores because \nof the global interpreter lock (GIL).\n\n\n \nItem 54: Use Lock to Prevent Data Races in Threads \n235\n✦ Python threads are still useful despite the GIL because they provide \nan easy way to do multiple things seemingly at the same time.\n✦ Use Python threads to make multiple system calls in parallel. This \nallows you to do blocking I/O at the same time as computation.\nItem 54: Use Lock to Prevent Data Races in Threads\nAfter learning about the global interpreter lock (GIL) (see Item 53: \n“Use Threads for Blocking I/O, Avoid for Parallelism”), many new \nPython programmers assume they can forgo using mutual- exclusion \nlocks (also called mutexes) in their code altogether. If the GIL is \nalready  preventing Python threads from running on multiple CPU \ncores in parallel, it must also act as a lock for a program’s data struc-\ntures, right? Some testing on types like lists and dictionaries may \neven show that this assumption appears to hold.\nBut beware, this is not truly the case. The GIL will not protect you. \nAlthough only one Python thread runs at a time, a thread’s opera-\ntions on data structures can be interrupted between any two byte-\ncode instructions in the Python interpreter. This is dangerous if you \naccess the same objects from multiple threads simultaneously. The \ninvariants of your data structures could be violated at practically any \ntime because of these interruptions, leaving your program in a cor-\nrupted state.\nFor example, say that I want to write a program that counts many \nthings in parallel, like sampling light levels from a whole network of \nsensors. If I want to determine the total number of light samples over \ntime, I can aggregate them with a new class:\nclass Counter:\n    def __init__(self):\n        self.count = 0\n \n    def increment(self, offset):\n        self.count += offset\nImagine that each sensor has its own worker thread because reading \nfrom the sensor requires blocking I/O. After each sensor measure-\nment, the worker thread increments the counter up to a maximum \nnumber of desired readings:\ndef worker(sensor_index, how_many, counter):\n    for _ in range(how_many):\n        # Read from the sensor\n        ...\n        counter.increment(1)\n\n\n236 \nChapter 7 Concurrency and Parallelism\nHere, I run one worker thread for each sensor in parallel and wait for \nthem all to finish their readings:\nfrom threading import Thread\n \nhow_many = 10**5\ncounter = Counter()\n \nthreads = []\nfor i in range(5):\n    thread = Thread(target=worker,\n                    args=(i, how_many, counter))\n    threads.append(thread)\n    thread.start()\n \nfor thread in threads:\n    thread.join()\n \nexpected = how_many * 5\nfound = counter.count\nprint(f'Counter should be {expected}, got {found}')\n>>>\nCounter should be 500000, got 246760\nThis seemed straightforward, and the outcome should have been \nobvious, but the result is way off! What happened here? How could \nsomething so simple go so wrong, especially since only one Python \ninterpreter thread can run at a time?\nThe Python interpreter enforces fairness between all of the threads \nthat are executing to ensure they get roughly equal processing time. \nTo do this, Python suspends a thread as it’s running and resumes \nanother thread in turn. The problem is that you don’t know exactly \nwhen Python will suspend your threads. A thread can even be paused \nseemingly halfway through what looks like an atomic operation. \nThat’s what happened in this case.\nThe body of the Counter object’s increment method looks simple, and is \nequivalent to this statement from the perspective of the worker thread:\ncounter.count += 1\nBut the += operator used on an object attribute actually instructs \nPython to do three separate operations behind the scenes. The state-\nment above is equivalent to this:\nvalue = getattr(counter, 'count')\nresult = value + 1\nsetattr(counter, 'count', result)\n\n\n \nItem 54: Use Lock to Prevent Data Races in Threads \n237\nPython threads incrementing the counter can be suspended between \nany two of these operations. This is problematic if the way the oper-\nations interleave causes old versions of value to be assigned to the \ncounter. Here’s an example of bad interaction between two threads, \nA and B:\n# Running in Thread A\nvalue_a = getattr(counter, 'count')\n# Context switch to Thread B\nvalue_b = getattr(counter, 'count')\nresult_b = value_b + 1\nsetattr(counter, 'count', result_b)\n# Context switch back to Thread A\nresult_a = value_a + 1\nsetattr(counter, 'count', result_a)\nThread B interrupted thread A before it had completely finished. \nThread B ran and finished, but then thread A resumed mid-execution, \noverwriting all of thread B’s progress in incrementing the counter. \nThis is exactly what happened in the light sensor example above.\nTo prevent data races like these, and other forms of data structure \ncorruption, Python includes a robust set of tools in the threading \nbuilt-in module. The simplest and most useful of them is the Lock \nclass, a mutual-exclusion lock (mutex).\nBy using a lock, I can have the Counter class protect its current \nvalue against simultaneous accesses from multiple threads. Only one \nthread will be able to acquire the lock at a time. Here, I use a with \nstatement to acquire and release the lock; this makes it easier to see \nwhich code is executing while the lock is held (see Item 66: “Consider \ncontextlib and with Statements for Reusable try/finally Behavior” \nfor background):\nfrom threading import Lock\n \nclass LockingCounter:\n    def __init__(self):\n        self.lock = Lock()\n        self.count = 0\n \n    def increment(self, offset):\n        with self.lock:\n            self.count += offset\n\n\n238 \nChapter 7 Concurrency and Parallelism\nNow, I run the worker threads as before but use a LockingCounter \ninstead:\ncounter = LockingCounter()\n \nfor i in range(5):\n    thread = Thread(target=worker,\n                    args=(i, how_many, counter))\n    threads.append(thread)\n    thread.start()\n \nfor thread in threads:\n    thread.join()\n \nexpected = how_many * 5\nfound = counter.count\nprint(f'Counter should be {expected}, got {found}')\n>>>\nCounter should be 500000, got 500000\nThe result is exactly what I expect. Lock solved the problem.\nThings to Remember\n✦ Even though Python has a global interpreter lock, you’re still \nresponsible for protecting against data races between the threads in \nyour programs.\n✦ Your programs will corrupt their data structures if you allow mul-\ntiple threads to modify the same objects without mutual-exclusion \nlocks (mutexes).\n✦ Use the Lock class from the threading built-in module to enforce \nyour program’s invariants between multiple threads.\nItem 55:  Use Queue to Coordinate Work Between \nThreads\nPython programs that do many things concurrently often need to \ncoordinate their work. One of the most useful arrangements for con-\ncurrent work is a pipeline of functions.\nA pipeline works like an assembly line used in manufacturing. Pipe-\nlines have many phases in serial, with a specific function for each \nphase. New pieces of work are constantly being added to the begin-\nning of the pipeline. The functions can operate concurrently, each \n\n\n \nItem 55: Use Queue to Coordinate Work Between Threads \n239\nworking on the piece of work in its phase. The work moves forward \nas each function completes until there are no phases remaining. This \napproach is especially good for work that includes blocking I/O or \nsubprocesses—activities that can easily be parallelized using Python \n(see Item 53: “Use Threads for Blocking I/O, Avoid for Parallelism”).\nFor example, say I want to build a system that will take a constant \nstream of images from my digital camera, resize them, and then add \nthem to a photo gallery online. Such a program could be split into \nthree phases of a pipeline. New images are retrieved in the first phase. \nThe downloaded images are passed through the resize function in the \nsecond phase. The resized images are consumed by the upload func-\ntion in the final phase.\nImagine that I’ve already written Python functions that execute the \nphases: download, resize, upload. How do I assemble a pipeline to do \nthe work concurrently?\ndef download(item):\n    ...\n \ndef resize(item):\n    ...\n \ndef upload(item):\n    ...\nThe first thing I need is a way to hand off work between the pipeline \nphases. This can be modeled as a thread-safe producer–consumer \nqueue (see Item 54: “Use Lock to Prevent Data Races in Threads” to \nunderstand the importance of thread safety in Python; see Item 71: \n“Prefer deque for Producer–Consumer Queues” to understand queue \nperformance):\nfrom collections import deque\nfrom threading import Lock\n \nclass MyQueue:\n    def __init__(self):\n        self.items = deque()\n        self.lock = Lock()\nThe producer, my digital camera, adds new images to the end of the \ndeque of pending items:\n    def put(self, item):\n        with self.lock:\n            self.items.append(item)\n\n\n240 \nChapter 7 Concurrency and Parallelism\nThe consumer, the first phase of the processing pipeline, removes \nimages from the front of the deque of pending items:\n    def get(self):\n        with self.lock:\n            return self.items.popleft()\nHere, I represent each phase of the pipeline as a Python thread that \ntakes work from one queue like this, runs a function on it, and puts \nthe result on another queue. I also track how many times the worker \nhas checked for new input and how much work it’s completed:\nfrom threading import Thread\nimport time\n \nclass Worker(Thread):\n    def __init__(self, func, in_queue, out_queue):\n        super().__init__()\n        self.func = func\n        self.in_queue = in_queue\n        self.out_queue = out_queue\n        self.polled_count = 0\n        self.work_done = 0\nThe trickiest part is that the worker thread must properly han-\ndle the case where the input queue is empty because the previous \nphase hasn’t completed its work yet. This happens where I catch the \nIndexError exception below. You can think of this as a holdup in the \nassembly line:\n    def run(self):\n        while True:\n            self.polled_count += 1\n            try:\n                item = self.in_queue.get()\n            except IndexError:\n                time.sleep(0.01)  # No work to do\n            else:\n                result = self.func(item)\n                self.out_queue.put(result)\n                self.work_done += 1\nNow, I can connect the three phases together by creating the queues \nfor their coordination points and the corresponding worker threads:\ndownload_queue = MyQueue()\nresize_queue = MyQueue()\nupload_queue = MyQueue()\n\n\n \nItem 55: Use Queue to Coordinate Work Between Threads \n241\ndone_queue = MyQueue()\nthreads = [\n    Worker(download, download_queue, resize_queue),\n    Worker(resize, resize_queue, upload_queue),\n    Worker(upload, upload_queue, done_queue),\n]\nI can start the threads and then inject a bunch of work into the first \nphase of the pipeline. Here, I use a plain object instance as a proxy \nfor the real data required by the download function:\nfor thread in threads:\n    thread.start()\n \nfor _ in range(1000):\n    download_queue.put(object())\nNow, I wait for all of the items to be processed by the pipeline and end \nup in the done_queue:\nwhile len(done_queue.items) < 1000:\n    # Do something useful while waiting\n    ...\nThis runs properly, but there’s an interesting side effect caused by \nthe threads polling their input queues for new work. The tricky part, \nwhere I catch IndexError exceptions in the run method, executes a \nlarge number of times:\nprocessed = len(done_queue.items)\npolled = sum(t.polled_count for t in threads)\nprint(f'Processed {processed} items after '\n      f'polling {polled} times')\n>>>\nProcessed 1000 items after polling 3035 times\nWhen the worker functions vary in their respective speeds, an ear-\nlier phase can prevent progress in later phases, backing up the pipe-\nline. This causes later phases to starve and constantly check their \ninput queues for new work in a tight loop. The outcome is that worker \nthreads waste CPU time doing nothing useful; they’re constantly rais-\ning and catching IndexError exceptions.\nBut that’s just the beginning of what’s wrong with this implementa-\ntion. There are three more problems that you should also avoid. First, \ndetermining that all of the input work is complete requires yet another \nbusy wait on the done_queue. Second, in Worker, the run method will \nexecute forever in its busy loop. There’s no obvious way to signal to a \nworker thread that it’s time to exit.\n\n\n242 \nChapter 7 Concurrency and Parallelism\nThird, and worst of all, a backup in the pipeline can cause the \n program to crash arbitrarily. If the first phase makes rapid progress \nbut the second phase makes slow progress, then the queue connecting \nthe first phase to the second phase will constantly increase in size. \nThe second phase won’t be able to keep up. Given enough time and \ninput data, the program will eventually run out of memory and die.\nThe lesson here isn’t that pipelines are bad; it’s that it’s hard to build \na good producer–consumer queue yourself. So why even try?\nQueue to the Rescue\nThe Queue class from the queue built-in module provides all of the \nfunctionality you need to solve the problems outlined above.\nQueue eliminates the busy waiting in the worker by making the get \nmethod block until new data is available. For example, here I start a \nthread that waits for some input data on a queue:\nfrom queue import Queue\n \nmy_queue = Queue()\n \ndef consumer():\n    print('Consumer waiting')\n    my_queue.get()              # Runs after put() below\n    print('Consumer done')\n \nthread = Thread(target=consumer)\nthread.start()\nEven though the thread is running first, it won’t finish until an item \nis put on the Queue instance and the get method has something to \nreturn:\nprint('Producer putting')\nmy_queue.put(object())          # Runs before get() above\nprint('Producer done')\nthread.join()\n>>>\nConsumer waiting\nProducer putting\nProducer done\nConsumer done\nTo solve the pipeline backup issue, the Queue class lets you specify \nthe maximum amount of pending work to allow between two phases. \n",
      "page_number": 249,
      "chapter_number": 25,
      "summary": "This chapter covers segment 25 (pages 249-264). Key topics include threads, item, and processes. When existing shell scripts get \ncomplicated, as they often do over time, graduating them to a rewrite \nin Python for the sake of readability and maintainability is a natural \nchoice.",
      "keywords": [
        "Python threads",
        "Threads",
        "Python",
        "Child Processes",
        "Python thread runs",
        "Item",
        "multiple Python threads",
        "Child",
        "Python interpreter thread",
        "multiple threads",
        "Python interpreter",
        "worker thread",
        "lock",
        "run",
        "Python program"
      ],
      "concepts": [
        "threads",
        "item",
        "processes",
        "process",
        "processing",
        "time",
        "python",
        "working",
        "running",
        "run"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 30,
          "title": "Segment 30 (pages 294-301)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 20,
          "title": "Threads and Concurrency",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 36,
          "title": "Segment 36 (pages 718-737)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "Segment 33 (pages 330-337)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 265-274)",
      "start_page": 265,
      "end_page": 274,
      "detection_method": "topic_boundary",
      "content": " \nItem 55: Use Queue to Coordinate Work Between Threads \n243\nThis buffer size causes calls to put to block when the queue is already \nfull. For example, here I define a thread that waits for a while before \nconsuming a queue:\nmy_queue = Queue(1)             # Buffer size of 1\n \ndef consumer():\n    time.sleep(0.1)             # Wait\n    my_queue.get()              # Runs second\n    print('Consumer got 1')\n    my_queue.get()              # Runs fourth\n    print('Consumer got 2')\n    print('Consumer done')\n \nthread = Thread(target=consumer)\nthread.start()\nThe wait should allow the producer thread to put both objects on the \nqueue before the consumer thread ever calls get. But the Queue size \nis one. This means the producer adding items to the queue will have \nto wait for the consumer thread to call get at least once before the \nsecond call to put will stop blocking and add the second item to the \nqueue:\nmy_queue.put(object())          # Runs first\nprint('Producer put 1')\nmy_queue.put(object())          # Runs third\nprint('Producer put 2')\nprint('Producer done')\nthread.join()\n>>>\nProducer put 1\nConsumer got 1\nProducer put 2\nProducer done\nConsumer got 2\nConsumer done\nThe Queue class can also track the progress of work using the \ntask_done method. This lets you wait for a phase’s input queue to \ndrain and eliminates the need to poll the last phase of a pipeline (as \nwith the done_queue above). For example, here I define a consumer \nthread that calls task_done when it finishes working on an item:\nin_queue = Queue()\n \n\n\n244 \nChapter 7 Concurrency and Parallelism\ndef consumer():\n    print('Consumer waiting')\n    work = in_queue.get()       # Runs second\n    print('Consumer working')\n    # Doing work\n    ...\n    print('Consumer done')\n    in_queue.task_done()        # Runs third\n \nthread = Thread(target=consumer)\nthread.start()\nNow, the producer code doesn’t have to join the consumer thread or \npoll. The producer can just wait for the in_queue to finish by calling \njoin on the Queue instance. Even once it’s empty, the in_queue won’t \nbe joinable until after task_done is called for every item that was ever \nenqueued:\nprint('Producer putting')\nin_queue.put(object())         # Runs first\nprint('Producer waiting')\nin_queue.join()                # Runs fourth\nprint('Producer done')\nthread.join()\n>>>\nConsumer waiting\nProducer putting\nProducer waiting\nConsumer working\nConsumer done\nProducer done\nI can put all these behaviors together into a Queue subclass that also \ntells the worker thread when it should stop processing. Here, I define \na close method that adds a special sentinel item to the queue that \nindicates there will be no more input items after it:\nclass ClosableQueue(Queue):\n    SENTINEL = object()\n \n    def close(self):\n        self.put(self.SENTINEL)\nThen, I define an iterator for the queue that looks for this special \nobject and stops iteration when it’s found. This __iter__ method also \ncalls task_done at appropriate times, letting me track the progress of \n\n\n \nItem 55: Use Queue to Coordinate Work Between Threads \n245\nwork on the queue (see Item 31: “Be Defensive When Iterating Over \nArguments” for details about __iter__):\n    def __iter__(self):\n        while True:\n            item = self.get()\n            try:\n                if item is self.SENTINEL:\n                    return  # Cause the thread to exit\n                yield item\n            finally:\n                self.task_done()\nNow, I can redefine my worker thread to rely on the behavior of \nthe ClosableQueue class. The thread will exit when the for loop is \nexhausted:\nclass StoppableWorker(Thread):\n    def __init__(self, func, in_queue, out_queue):\n        super().__init__()\n        self.func = func\n        self.in_queue = in_queue\n        self.out_queue = out_queue\n \n    def run(self):\n        for item in self.in_queue:\n            result = self.func(item)\n            self.out_queue.put(result)\nI re-create the set of worker threads using the new worker class:\ndownload_queue = ClosableQueue()\nresize_queue = ClosableQueue()\nupload_queue = ClosableQueue()\ndone_queue = ClosableQueue()\nthreads = [\n    StoppableWorker(download, download_queue, resize_queue),\n    StoppableWorker(resize, resize_queue, upload_queue),\n    StoppableWorker(upload, upload_queue, done_queue),\n]\nAfter running the worker threads as before, I also send the stop sig-\nnal after all the input work has been injected by closing the input \nqueue of the first phase:\nfor thread in threads:\n    thread.start()\n \n\n\n246 \nChapter 7 Concurrency and Parallelism\nfor _ in range(1000):\n    download_queue.put(object())\n \ndownload_queue.close()\nFinally, I wait for the work to finish by joining the queues that con-\nnect the phases. Each time one phase is done, I signal the next phase \nto stop by closing its input queue. At the end, the done_queue contains \nall of the output objects, as expected:\ndownload_queue.join()\nresize_queue.close()\nresize_queue.join()\nupload_queue.close()\nupload_queue.join()\nprint(done_queue.qsize(), 'items finished')\n \nfor thread in threads:\n    thread.join()\n>>>\n1000 items finished\nThis approach can be extended to use multiple worker threads per \nphase, which can increase I/O parallelism and speed up this type of \nprogram significantly. To do this, first I define some helper functions \nthat start and stop multiple threads. The way stop_threads works \nis by calling close on each input queue once per consuming thread, \nwhich ensures that all of the workers exit cleanly:\ndef start_threads(count, *args):\n    threads = [StoppableWorker(*args) for _ in range(count)]\n    for thread in threads:\n        thread.start()\n    return threads\n \ndef stop_threads(closable_queue, threads):\n    for _ in threads:\n        closable_queue.close()\n \n    closable_queue.join()\n \n    for thread in threads:\n        thread.join()\n\n\n \nItem 55: Use Queue to Coordinate Work Between Threads \n247\nThen, I connect the pieces together as before, putting objects to pro-\ncess into the top of the pipeline, joining queues and threads along the \nway, and finally consuming the results:\ndownload_queue = ClosableQueue()\nresize_queue = ClosableQueue()\nupload_queue = ClosableQueue()\ndone_queue = ClosableQueue()\n \ndownload_threads = start_threads(\n    3, download, download_queue, resize_queue)\nresize_threads = start_threads(\n    4, resize, resize_queue, upload_queue)\nupload_threads = start_threads(\n    5, upload, upload_queue, done_queue)\n \nfor _ in range(1000):\n    download_queue.put(object())\n \nstop_threads(download_queue, download_threads)\nstop_threads(resize_queue, resize_threads)\nstop_threads(upload_queue, upload_threads)\n \nprint(done_queue.qsize(), 'items finished')\n>>>\n1000 items finished\nAlthough Queue works well in this case of a linear pipeline, there \nare many other situations for which there are better tools that you \nshould consider (see Item 60: “Achieve Highly Concurrent I/O with \nCoroutines”).\nThings to Remember\n✦ Pipelines are a great way to organize sequences of work—especially \nI/O-bound programs—that run concurrently using multiple Python \nthreads.\n✦ Be aware of the many problems in building concurrent pipelines: \nbusy waiting, how to tell workers to stop, and potential memory \nexplosion.\n✦ The Queue class has all the facilities you need to build robust \n pipelines: blocking operations, buffer sizes, and joining.\n\n\n248 \nChapter 7 Concurrency and Parallelism\nItem 56:  Know How to Recognize When Concurrency \nIs Necessary\nInevitably, as the scope of a program grows, it also becomes more \ncomplicated. Dealing with expanding requirements in a way that \nmaintains clarity, testability, and efficiency is one of the most difficult \nparts of programming. Perhaps the hardest type of change to handle \nis moving from a single-threaded program to one that needs multiple \nconcurrent lines of execution.\nLet me demonstrate how you might encounter this problem with an \nexample. Say that I want to implement Conway’s Game of Life, a clas-\nsic illustration of finite state automata. The rules of the game are sim-\nple: You have a two-dimensional grid of an arbitrary size. Each cell in \nthe grid can either be alive or empty:\nALIVE = '*'\nEMPTY = '-'\nThe game progresses one tick of the clock at a time. Every tick, each \ncell counts how many of its neighboring eight cells are still alive. \nBased on its neighbor count, a cell decides if it will keep living, die, \nor regenerate. (I’ll explain the specific rules further below.) Here’s an \nexample of a 5 × 5 Game of Life grid after four generations with time \ngoing to the right:\n  0   |   1   |   2   |   3   |   4  \n----- | ----- | ----- | ----- | -----\n-*--- | --*-- | --**- | --*-- | -----\n--**- | --**- | -*--- | -*--- | -**--\n---*- | --**- | --**- | --*-- | -----\n----- | ----- | ----- | ----- | -----\nI can represent the state of each cell with a simple container class. \nThe class must have methods that allow me to get and set the value \nof any coordinate. Coordinates that are out of bounds should wrap \naround, making the grid act like an infinite looping space:\nclass Grid:\n    def __init__(self, height, width):\n        self.height = height\n        self.width = width\n        self.rows = []\n        for _ in range(self.height):\n            self.rows.append([EMPTY] * self.width)\n \n\n\n \nItem 56: Know How to Recognize When Concurrency Is Necessary \n249\n    def get(self, y, x):\n        return self.rows[y % self.height][x % self.width]\n \n    def set(self, y, x, state):\n        self.rows[y % self.height][x % self.width] = state\n \n    def __str__(self):\n        ...\nTo see this class in action, I can create a Grid instance and set its ini-\ntial state to a classic shape called a glider:\ngrid = Grid(5, 9)\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\ngrid.set(2, 4, ALIVE)\nprint(grid)\n>>>\n---*-----\n----*----\n--***----\n---------\n---------\nNow, I need a way to retrieve the status of neighboring cells. I can \ndo this with a helper function that queries the grid and returns the \ncount of living neighbors. I use a simple function for the get param-\neter instead of passing in a whole Grid instance in order to reduce \ncoupling (see Item 38: “Accept Functions Instead of Classes for Simple \nInterfaces” for more about this approach):\ndef count_neighbors(y, x, get):\n    n_ = get(y - 1, x + 0)  # North\n    ne = get(y - 1, x + 1)  # Northeast\n    e_ = get(y + 0, x + 1)  # East\n    se = get(y + 1, x + 1)  # Southeast\n    s_ = get(y + 1, x + 0)  # South\n    sw = get(y + 1, x - 1)  # Southwest\n    w_ = get(y + 0, x - 1)  # West\n    nw = get(y - 1, x - 1)  # Northwest\n    neighbor_states = [n_, ne, e_, se, s_, sw, w_, nw]\n    count = 0\n\n\n250 \nChapter 7 Concurrency and Parallelism\n    for state in neighbor_states:\n        if state == ALIVE:\n            count += 1\n    return count\nNow, I define the simple logic for Conway’s Game of Life, based on the \ngame’s three rules: Die if a cell has fewer than two neighbors, die if a \ncell has more than three neighbors, or become alive if an empty cell \nhas exactly three neighbors:\ndef game_logic(state, neighbors):\n    if state == ALIVE:\n        if neighbors < 2:\n            return EMPTY     # Die: Too few\n        elif neighbors > 3:\n            return EMPTY     # Die: Too many\n    else:\n        if neighbors == 3:\n            return ALIVE     # Regenerate\n    return state\nI can connect count_neighbors and game_logic together in another \nfunction that transitions the state of a cell. This function will be \ncalled each generation to figure out a cell’s current state, inspect the \nneighboring cells around it, determine what its next state should be, \nand update the resulting grid accordingly. Again, I use a function \ninterface for set instead of passing in the Grid instance to make this \ncode more decoupled:\ndef step_cell(y, x, get, set):\n    state = get(y, x)\n    neighbors = count_neighbors(y, x, get)\n    next_state = game_logic(state, neighbors)\n    set(y, x, next_state)\nFinally, I can define a function that progresses the whole grid of cells \nforward by a single step and then returns a new grid containing \nthe state for the next generation. The important detail here is that \nI need all dependent functions to call the get method on the previ-\nous  generation’s Grid instance, and to call the set method on the \nnext generation’s Grid instance. This is how I ensure that all of \nthe cells move in lockstep, which is an essential part of how the game \nworks. This is easy to achieve because I used function interfaces for \nget and set instead of passing Grid instances:\ndef simulate(grid):\n    next_grid = Grid(grid.height, grid.width)\n\n\n \nItem 56: Know How to Recognize When Concurrency Is Necessary \n251\n    for y in range(grid.height):\n        for x in range(grid.width):\n            step_cell(y, x, grid.get, next_grid.set)\n    return next_grid\nNow, I can progress the grid forward one generation at a time. You \ncan see how the glider moves down and to the right on the grid based \non the simple rules from the game_logic function:\nclass ColumnPrinter:\n    ...\n \ncolumns = ColumnPrinter()\nfor i in range(5):\n    columns.append(str(grid))\n    grid = simulate(grid)\n \nprint(columns)\n>>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --*-*---- | ----*---- | ---*----- | ----*----\n--***---- | ---**---- | --*-*---- | ----**--- | -----*---\n--------- | ---*----- | ---**---- | ---**---- | ---***---\n--------- | --------- | --------- | --------- | ---------\nThis works great for a program that can run in one thread on a sin-\ngle machine. But imagine that the program’s requirements have \nchanged—as I alluded to above—and now I need to do some I/O (e.g., \nwith a socket) from within the game_logic function. For example, this \nmight be required if I’m trying to build a massively multiplayer online \ngame where the state transitions are determined by a combination of \nthe grid state and communication with other players over the Internet.\nHow can I extend this implementation to support such functional-\nity? The simplest thing to do is to add blocking I/O directly into the \ngame_logic function:\ndef game_logic(state, neighbors):\n    ...\n    # Do some blocking input/output in here:\n    data = my_socket.recv(100)\n    ...\nThe problem with this approach is that it’s going to slow down the \nwhole program. If the latency of the I/O required is 100 millisec-\nonds (i.e., a reasonably good cross-country, round-trip latency on the \n\n\n252 \nChapter 7 Concurrency and Parallelism\nInternet), and there are 45 cells in the grid, then each generation will \ntake a minimum of 4.5 seconds to evaluate because each cell is pro-\ncessed serially in the simulate function. That’s far too slow and will \nmake the game unplayable. It also scales poorly: If I later wanted to \nexpand the grid to 10,000 cells, I would need over 15 minutes to eval-\nuate each generation.\nThe solution is to do the I/O in parallel so each generation takes \nroughly 100 milliseconds, regardless of how big the grid is. The \nprocess of spawning a concurrent line of execution for each unit of \nwork—a cell in this case—is called fan-out. Waiting for all of those \nconcurrent units of work to finish before moving on to the next phase \nin a coordinated process—a generation in this case—is called fan-in.\nPython provides many built-in tools for achieving fan-out and fan-in \nwith various trade-offs. You should understand the pros and cons \nof each approach and choose the best tool for the job, depending \non the situation. See the items that follow for details based on this \nGame of Life example program (Item 57: “Avoid Creating New Thread \nInstances for On-demand Fan-out,” Item 58: “Understand How Using \nQueue for Concurrency Requires Refactoring,” Item 59: “Consider \nThreadPoolExecutor When Threads Are Necessary for Concurrency,” \nand Item 60: “Achieve Highly Concurrent I/O with Coroutines”).\nThings to Remember\n✦ A program often grows to require multiple concurrent lines of exe-\ncution as its scope and complexity increases.\n✦ The most common types of concurrency coordination are fan-out \n(generating new units of concurrency) and fan-in (waiting for exist-\ning units of concurrency to complete).\n✦ Python has many different ways of achieving fan-out and fan-in.\nItem 57:  Avoid Creating New Thread Instances for \nOn-demand Fan-out\nThreads are the natural first tool to reach for in order to do parallel \nI/O in Python (see Item 53: “Use Threads for Blocking I/O, Avoid for \nParallelism”). However, they have significant downsides when you try \nto use them for fanning out to many concurrent lines of execution.\nTo demonstrate this, I’ll continue with the Game of Life example from \nbefore (see Item 56: “Know How to Recognize When Concurrency Is \nNecessary” for background and the implementations of various func-\ntions and classes below). I’ll use threads to solve the latency problem \n",
      "page_number": 265,
      "chapter_number": 26,
      "summary": "Queue to the Rescue\nThe Queue class from the queue built-in module provides all of the \nfunctionality you need to solve the problems outlined above Key topics include queue, grid, and item.",
      "keywords": [
        "queue",
        "Threads",
        "consumer",
        "grid",
        "producer",
        "item",
        "consumer thread",
        "state",
        "Runs",
        "alive",
        "Game",
        "Queue class",
        "input queue",
        "queue def run",
        "Concurrency"
      ],
      "concepts": [
        "queue",
        "grid",
        "item",
        "state",
        "thread",
        "work",
        "classes",
        "functionality",
        "functions",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 7",
          "chapter": 13,
          "title": "Segment 13 (pages 116-124)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 34,
          "title": "Segment 34 (pages 338-346)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 32,
          "title": "Segment 32 (pages 322-329)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "Segment 33 (pages 330-337)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 41,
          "title": "Segment 41 (pages 405-412)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 275-289)",
      "start_page": 275,
      "end_page": 289,
      "detection_method": "topic_boundary",
      "content": " \nItem 57: Avoid Creating New Thread Instances for On-demand Fan-out \n253\ncaused by doing I/O in the game_logic function. To begin, threads \nrequire coordination using locks to ensure that assumptions within \ndata structures are maintained properly. I can create a subclass of \nthe Grid class that adds locking behavior so an instance can be used \nby multiple threads simultaneously:\nfrom threading import Lock\n \nALIVE = '*'\nEMPTY = '-'\n \nclass Grid:\n    ...\n \nclass LockingGrid(Grid):\n    def __init__(self, height, width):\n        super().__init__(height, width)\n        self.lock = Lock()\n \n    def __str__(self):\n        with self.lock:\n            return super().__str__()\n \n    def get(self, y, x):\n        with self.lock:\n            return super().get(y, x)\n \n    def set(self, y, x, state):\n        with self.lock:\n            return super().set(y, x, state)\nThen, I can reimplement the simulate function to fan out by creating a \nthread for each call to step_cell. The threads will run in parallel and \nwon’t have to wait on each other’s I/O. I can then fan in by waiting for \nall of the threads to complete before moving on to the next generation:\nfrom threading import Thread\n \ndef count_neighbors(y, x, get):\n    ...\n \ndef game_logic(state, neighbors):\n    ...\n    # Do some blocking input/output in here:\n    data = my_socket.recv(100)\n    ...\n \n\n\n254 \nChapter 7 Concurrency and Parallelism\ndef step_cell(y, x, get, set):\n    state = get(y, x)\n    neighbors = count_neighbors(y, x, get)\n    next_state = game_logic(state, neighbors)\n    set(y, x, next_state)\n \ndef simulate_threaded(grid):\n    next_grid = LockingGrid(grid.height, grid.width)\n \n    threads = []\n    for y in range(grid.height):\n        for x in range(grid.width):\n            args = (y, x, grid.get, next_grid.set)\n            thread = Thread(target=step_cell, args=args)\n            thread.start()  # Fan out\n            threads.append(thread)\n \n    for thread in threads:\n        thread.join()       # Fan in\n \n    return next_grid\nI can run this code using the same implementation of step_cell and \nthe same driving code as before with only two lines changed to use \nthe LockingGrid and simulate_threaded implementations:\nclass ColumnPrinter:\n    ...\n \ngrid = LockingGrid(5, 9)            # Changed\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\ngrid.set(2, 4, ALIVE)\n \ncolumns = ColumnPrinter()\nfor i in range(5):\n    columns.append(str(grid))\n    grid = simulate_threaded(grid)  # Changed\n \nprint(columns)\n\n\n>>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --*-*---- | ----*---- | ---*----- | ----*----\n--***---- | ---**---- | --*-*---- | ----**--- | -----*---\n--------- | ---*----- | ---**---- | ---**---- | ---***---\n--------- | --------- | --------- | --------- | ---------\nThis works as expected, and the I/O is now parallelized between the \nthreads. However, this code has three big problems:\n \n■The Thread instances require special tools to coordinate with \neach other safely (see Item 54: “Use Lock to Prevent Data Races in \nThreads”). This makes the code that uses threads harder to rea-\nson about than the procedural, single-threaded code from before. \nThis complexity makes threaded code more difficult to extend and \nmaintain over time.\n \n■Threads require a lot of memory—about 8 MB per executing \nthread. On many computers, that amount of memory doesn’t mat-\nter for the 45 threads I’d need in this example. But if the game \ngrid had to grow to 10,000 cells, I would need to create that many \nthreads, which couldn’t even fit in the memory of my machine. \nRunning a thread per concurrent activity just won’t work.\n \n■Starting a thread is costly, and threads have a negative perfor-\nmance impact when they run due to context switching between \nthem. In this case, all of the threads are started and stopped each \ngeneration of the game, which has high overhead and will increase \nlatency beyond the expected I/O time of 100 milliseconds.\nThis code would also be very difficult to debug if something went wrong. \nFor example, imagine that the game_logic function raises an exception, \nwhich is highly likely due to the generally flaky nature of I/O:\ndef game_logic(state, neighbors):\n    ...\n    raise OSError('Problem with I/O')\n    ...\nI can test what this would do by running a Thread instance pointed at \nthis function and redirecting the sys.stderr output from the program \nto an in-memory StringIO buffer:\nimport contextlib\nimport io\n \n \nItem 57: Avoid Creating New Thread Instances for On-demand Fan-out \n255\n\n\n256 \nChapter 7 Concurrency and Parallelism\nfake_stderr = io.StringIO()\nwith contextlib.redirect_stderr(fake_stderr):\n    thread = Thread(target=game_logic, args=(ALIVE, 3))\n    thread.start()\n    thread.join()\n \nprint(fake_stderr.getvalue())\n>>>\nException in thread Thread-226:\nTraceback (most recent call last):\n  File \"threading.py\", line 917, in _bootstrap_inner\n    self.run()\n  File \"threading.py\", line 865, in run\n    self._target(*self._args, **self._kwargs)\n  File \"example.py\", line 193, in game_logic\n    raise OSError('Problem with I/O')\nOSError: Problem with I/O\nAn OSError exception is raised as expected, but somehow the code \nthat created the Thread and called join on it is unaffected. How can \nthis be? The reason is that the Thread class will independently catch \nany exceptions that are raised by the target function and then write \ntheir traceback to sys.stderr. Such exceptions are never re-raised to \nthe caller that started the thread in the first place.\nGiven all of these issues, it’s clear that threads are not the solution if \nyou need to constantly create and finish new concurrent functions. \nPython provides other solutions that are a better fit (see Item 58: \n“Understand How Using Queue for Concurrency Requires Refactoring,” \nItem 59: “Consider ThreadPoolExecutor When Threads Are Necessary \nfor Concurrency”, and Item 60: “Achieve Highly Concurrent I/O with \nCoroutines”).\nThings to Remember\n✦ Threads have many downsides: They’re costly to start and run \nif you need a lot of them, they each require a significant amount \nof memory, and they require special tools like Lock instances for \ncoordination.\n✦ Threads do not provide a built-in way to raise exceptions back in \nthe code that started a thread or that is waiting for one to finish, \nwhich makes them difficult to debug.\n\n\n \nItem 58: Using Queue for Concurrency Requires Refactoring \n257\nItem 58:  Understand How Using Queue for Concurrency \nRequires Refactoring\nIn the previous item (see Item 57: “Avoid Creating New Thread \nInstances for On-demand Fan-out”) I covered the downsides of using \nThread to solve the parallel I/O problem in the Game of Life example \nfrom earlier (see Item 56: “Know How to Recognize When Concur-\nrency Is Necessary” for background and the implementations of vari-\nous functions and classes below).\nThe next approach to try is to implement a threaded pipeline using \nthe Queue class from the queue built-in module (see Item 55: “Use \nQueue to Coordinate Work Between Threads” for background; I rely on \nthe implementations of ClosableQueue and StoppableWorker from that \nitem in the example code below).\nHere’s the general approach: Instead of creating one thread per cell \nper generation of the Game of Life, I can create a fixed number of \nworker threads upfront and have them do parallelized I/O as needed. \nThis will keep my resource usage under control and eliminate the \noverhead of frequently starting new threads.\nTo do this, I need two ClosableQueue instances to use for communi-\ncating to and from the worker threads that execute the game_logic \nfunction:\nfrom queue import Queue\n \nclass ClosableQueue(Queue):\n    ...\n \nin_queue = ClosableQueue()\nout_queue = ClosableQueue()\nI can start multiple threads that will consume items from the \nin_queue, process them by calling game_logic, and put the results on \nout_queue. These threads will run concurrently, allowing for parallel \nI/O and reduced latency for each generation:\nfrom threading import Thread\n \nclass StoppableWorker(Thread):\n    ...\n \ndef game_logic(state, neighbors):\n    ...\n\n\n258 \nChapter 7 Concurrency and Parallelism\n    # Do some blocking input/output in here:\n    data = my_socket.recv(100)\n    ...\n \ndef game_logic_thread(item):\n    y, x, state, neighbors = item\n    try:\n        next_state = game_logic(state, neighbors)\n    except Exception as e:\n        next_state = e\n    return (y, x, next_state)\n \n# Start the threads upfront\nthreads = []\nfor _ in range(5):\n    thread = StoppableWorker(\n        game_logic_thread, in_queue, out_queue)\n    thread.start()\n    threads.append(thread)\nNow, I can redefine the simulate function to interact with these \nqueues to request state transition decisions and receive correspond-\ning responses. Adding items to in_queue causes fan-out, and consum-\ning items from out_queue until it’s empty causes fan-in:\nALIVE = '*'\nEMPTY = '-'\n \nclass SimulationError(Exception):\n    pass\n \nclass Grid:\n    ...\n \ndef count_neighbors(y, x, get):\n    ...\n \ndef simulate_pipeline(grid, in_queue, out_queue):\n    for y in range(grid.height):\n        for x in range(grid.width):\n            state = grid.get(y, x)\n            neighbors = count_neighbors(y, x, grid.get)\n            in_queue.put((y, x, state, neighbors))  # Fan out\n \n    in_queue.join()\n    out_queue.close()\n \n\n\n    next_grid = Grid(grid.height, grid.width)\n    for item in out_queue:                          # Fan in\n        y, x, next_state = item\n        if isinstance(next_state, Exception):\n            raise SimulationError(y, x) from next_state\n        next_grid.set(y, x, next_state)\n \n    return next_grid\nThe calls to Grid.get and Grid.set both happen within this new \nsimulate_pipeline function, which means I can use the  single-threaded \nimplementation of Grid instead of the implementation that requires \nLock instances for synchronization.\nThis code is also easier to debug than the Thread approach used \nin the previous item. If an exception occurs while doing I/O in the \ngame_logic function, it will be caught, propagated to the out_queue, \nand then re-raised in the main thread:\ndef game_logic(state, neighbors):\n    ...\n    raise OSError('Problem with I/O in game_logic')\n    ...\n \nsimulate_pipeline(Grid(1, 1), in_queue, out_queue)\n>>>\nTraceback ...\nOSError: Problem with I/O in game_logic\n \nThe above exception was the direct cause of the following \n¯exception:\n \nTraceback ...\nSimulationError: (0, 0)\nI can drive this multithreaded pipeline for repeated generations by \ncalling simulate_pipeline in a loop:\nclass ColumnPrinter:\n    ...\n \ngrid = Grid(5, 9)\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\n \nItem 58: Using Queue for Concurrency Requires Refactoring \n259\n\n\n260 \nChapter 7 Concurrency and Parallelism\ngrid.set(2, 4, ALIVE)\n \ncolumns = ColumnPrinter()\nfor i in range(5):\n    columns.append(str(grid))\n    grid = simulate_pipeline(grid, in_queue, out_queue)\n \nprint(columns)\n \nfor thread in threads:\n    in_queue.close()\nfor thread in threads:\n    thread.join()\n>>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --------- | --*-*---- | --------- | ----*----\n--***---- | --------- | ---**---- | --------- | --*-*----\n--------- | --------- | ---*----- | --------- | ---**----\n--------- | --------- | --------- | --------- | ---------\nThe results are the same as before. Although I’ve addressed the mem-\nory explosion problem, startup costs, and debugging issues of using \nthreads on their own, many issues remain:\n \n■The simulate_pipeline function is even harder to follow than the \nsimulate_threaded approach from the previous item.\n \n■Extra support classes were required for ClosableQueue and \nStoppableWorker in order to make the code easier to read, at the \nexpense of increased complexity.\n \n■I have to specify the amount of potential parallelism—the num-\nber of threads running game_logic_thread—upfront based on my \nexpectations of the workload instead of having the system auto-\nmatically scale up parallelism as needed.\n \n■In order to enable debugging, I have to manually catch exceptions \nin worker threads, propagate them on a Queue, and then re-raise \nthem in the main thread.\nHowever, the biggest problem with this code is apparent if the require-\nments change again. Imagine that later I needed to do I/O within \nthe count_neighbors function in addition to the I/O that was needed \nwithin game_logic:\ndef count_neighbors(y, x, get):\n    ...\n\n\n    # Do some blocking input/output in here:\n    data = my_socket.recv(100)\n    ...\nIn order to make this parallelizable, I need to add another stage to the \npipeline that runs count_neighbors in a thread. I need to make sure \nthat exceptions propagate correctly between the worker threads and \nthe main thread. And I need to use a Lock for the Grid class in order \nto ensure safe synchronization between the worker threads (see Item \n54: “Use Lock to Prevent Data Races in Threads” for background and \nItem 57: “Avoid Creating New Thread Instances for On-demand Fan-\nout” for the implementation of LockingGrid):\ndef count_neighbors_thread(item):\n    y, x, state, get = item\n    try:\n        neighbors = count_neighbors(y, x, get)\n    except Exception as e:\n        neighbors = e\n    return (y, x, state, neighbors)\n \ndef game_logic_thread(item):\n    y, x, state, neighbors = item\n    if isinstance(neighbors, Exception):\n        next_state = neighbors\n    else:\n        try:\n            next_state = game_logic(state, neighbors)\n        except Exception as e:\n            next_state = e\n    return (y, x, next_state)\n \nclass LockingGrid(Grid):\n    ...\nI \nhave \nto \ncreate \nanother \nset \nof \nQueue \ninstances \nfor \nthe \ncount_neighbors_thread workers and the corresponding Thread \ninstances:\nin_queue = ClosableQueue()\nlogic_queue = ClosableQueue()\nout_queue = ClosableQueue()\n \nthreads = []\n \n \nItem 58: Using Queue for Concurrency Requires Refactoring \n261\n\n\n262 \nChapter 7 Concurrency and Parallelism\nfor _ in range(5):\n    thread = StoppableWorker(\n        count_neighbors_thread, in_queue, logic_queue)\n    thread.start()\n    threads.append(thread)\n \nfor _ in range(5):\n    thread = StoppableWorker(\n        game_logic_thread, logic_queue, out_queue)\n    thread.start()\n    threads.append(thread)\nFinally, I need to update simulate_pipeline to coordinate the multiple \nphases in the pipeline and ensure that work fans out and back in \ncorrectly:\ndef simulate_phased_pipeline(\n        grid, in_queue, logic_queue, out_queue):\n    for y in range(grid.height):\n        for x in range(grid.width):\n            state = grid.get(y, x)\n            item = (y, x, state, grid.get)\n            in_queue.put(item)          # Fan out\n \n    in_queue.join()\n    logic_queue.join()                  # Pipeline sequencing\n    out_queue.close()\n \n    next_grid = LockingGrid(grid.height, grid.width)\n    for item in out_queue:              # Fan in\n        y, x, next_state = item\n        if isinstance(next_state, Exception):\n            raise SimulationError(y, x) from next_state\n        next_grid.set(y, x, next_state)\n \n    return next_grid\nWith these updated implementations, now I can run the multiphase \npipeline end-to-end:\ngrid = LockingGrid(5, 9)\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\ngrid.set(2, 4, ALIVE)\n \n\n\ncolumns = ColumnPrinter()\nfor i in range(5):\n    columns.append(str(grid))\n    grid = simulate_phased_pipeline(\n        grid, in_queue, logic_queue, out_queue)\n \nprint(columns)\n \nfor thread in threads:\n    in_queue.close()\nfor thread in threads:\n    logic_queue.close()\nfor thread in threads:\n    thread.join()\n>>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --*-*---- | ----*---- | ---*----- | ----*----\n--***---- | ---**---- | --*-*---- | ----**--- | -----*---\n--------- | ---*----- | ---**---- | ---**---- | ---***---\n--------- | --------- | --------- | --------- | ---------\nAgain, this works as expected, but it required a lot of changes and \nboilerplate. The point here is that Queue does make it possible to solve \nfan-out and fan-in problems, but the overhead is very high. Although \nusing Queue is a better approach than using Thread instances on their \nown, it’s still not nearly as good as some of the other tools provided \nby Python (see Item 59: “Consider ThreadPoolExecutor When Threads \nAre Necessary for Concurrency” and Item 60: “Achieve Highly Con-\ncurrent I/O with Coroutines”).\nThings to Remember\n✦ Using Queue instances with a fixed number of worker threads \nimproves the scalability of fan-out and fan-in using threads.\n✦ It takes a significant amount of work to refactor existing code to use \nQueue, especially when multiple stages of a pipeline are required.\n✦ Using Queue fundamentally limits the total amount of I/O paral-\nlelism a program can leverage compared to alternative approaches \nprovided by other built-in Python features and modules.\n \nItem 58: Using Queue for Concurrency Requires Refactoring \n263\n\n\n264 \nChapter 7 Concurrency and Parallelism\nItem 59:  Consider ThreadPoolExecutor When Threads \nAre Necessary for Concurrency\nPython includes the concurrent.futures built-in module, which pro-\nvides the ThreadPoolExecutor class. It combines the best of the Thread \n(see Item 57: “Avoid Creating New Thread Instances for On-demand \nFan-out”) and Queue (see Item 58: “Understand How Using Queue for \nConcurrency Requires Refactoring”) approaches to solving the par-\nallel I/O problem from the Game of Life example (see Item 56: “Know \nHow to Recognize When Concurrency Is Necessary” for background \nand the implementations of various functions and classes below):\nALIVE = '*'\nEMPTY = '-'\n \nclass Grid:\n    ...\n \nclass LockingGrid(Grid):\n    ...\n \ndef count_neighbors(y, x, get):\n    ...\n \ndef game_logic(state, neighbors):\n    ...\n    # Do some blocking input/output in here:\n    data = my_socket.recv(100)\n    ...\n \ndef step_cell(y, x, get, set):\n    state = get(y, x)\n    neighbors = count_neighbors(y, x, get)\n    next_state = game_logic(state, neighbors)\n    set(y, x, next_state)\nInstead of starting a new Thread instance for each Grid square, I can \nfan out by submitting a function to an executor that will be run in a \nseparate thread. Later, I can wait for the result of all tasks in order to \nfan in:\nfrom concurrent.futures import ThreadPoolExecutor\n \ndef simulate_pool(pool, grid):\n    next_grid = LockingGrid(grid.height, grid.width)\n \n\n\n \nItem 59: Consider ThreadPoolExecutor When Threads Are Necessary \n265\n    futures = []\n    for y in range(grid.height):\n        for x in range(grid.width):\n            args = (y, x, grid.get, next_grid.set)\n            future = pool.submit(step_cell, *args)  # Fan out\n            futures.append(future)\n \n    for future in futures:\n        future.result()                             # Fan in\n \n    return next_grid\nThe threads used for the executor can be allocated in advance, which \nmeans I don’t have to pay the startup cost on each execution of \nsimulate_pool. I can also specify the maximum number of threads \nto use for the pool—using the max_workers parameter—to prevent the \nmemory blow-up issues associated with the naive Thread solution to \nthe parallel I/O problem:\nclass ColumnPrinter:\n    ...\n \ngrid = LockingGrid(5, 9)\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\ngrid.set(2, 4, ALIVE)\n \ncolumns = ColumnPrinter()\nwith ThreadPoolExecutor(max_workers=10) as pool:\n    for i in range(5):\n        columns.append(str(grid))\n        grid = simulate_pool(pool, grid)\n \nprint(columns)\n>>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --*-*---- | ----*---- | ---*----- | ----*----\n--***---- | ---**---- | --*-*---- | ----**--- | -----*---\n--------- | ---*----- | ---**---- | ---**---- | ---***---\n--------- | --------- | --------- | --------- | ---------\n\n\n266 \nChapter 7 Concurrency and Parallelism\nThe best part about the ThreadPoolExecutor class is that it automati-\ncally propagates exceptions back to the caller when the result method \nis called on the Future instance returned by the submit method:\ndef game_logic(state, neighbors):\n    ...\n    raise OSError('Problem with I/O')\n    ...\n \nwith ThreadPoolExecutor(max_workers=10) as pool:\n    task = pool.submit(game_logic, ALIVE, 3)\n    task.result()\n>>>\nTraceback ...\nOSError: Problem with I/O\nIf I needed to provide I/O parallelism for the count_neighbors func-\ntion in addition to game_logic, no modifications to the program would \nbe required since ThreadPoolExecutor already runs these functions \nconcurrently as part of step_cell. It’s even possible to achieve CPU \nparallelism by using the same interface if necessary (see Item 64: \n“Consider concurrent.futures for True Parallelism”).\nHowever, the big problem that remains is the limited amount of I/O par-\nallelism that ThreadPoolExecutor provides. Even if I use a max_workers \nparameter of 100, this solution still won’t scale if I need 10,000+ cells \nin the grid that require simultaneous I/O. ThreadPoolExecutor is a \ngood choice for situations where there is no asynchronous solution \n(e.g., file I/O), but there are better ways to maximize I/O parallel-\nism in many cases (see Item 60: “Achieve Highly Concurrent I/O with \nCoroutines”).\nThings to Remember\n✦ ThreadPoolExecutor enables simple I/O parallelism with limited \nrefactoring, easily avoiding the cost of thread startup each time fan-\nout concurrency is required.\n✦ Although ThreadPoolExecutor eliminates the potential memory \nblow-up issues of using threads directly, it also limits I/O parallel-\nism by requiring max_workers to be specified upfront.\nItem 60:  Achieve Highly Concurrent I/O with \nCoroutines\nThe previous items have tried to solve the parallel I/O problem for \nthe Game of Life example with varying degrees of success. (See Item \n56: “Know How to Recognize When Concurrency Is Necessary” for \n\n\n \nItem 60: Achieve Highly Concurrent I/O with Coroutines \n267\nbackground and the implementations of various functions and classes \nbelow.) All of the other approaches fall short in their ability to han-\ndle thousands of simultaneously concurrent functions (see Item 57: \n“Avoid Creating New Thread Instances for On-demand Fan-out,” Item \n58: “Understand How Using Queue for Concurrency Requires Refactor-\ning,” and Item 59: “Consider ThreadPoolExecutor When Threads Are \nNecessary for Concurrency”).\nPython addresses the need for highly concurrent I/O with coroutines. \nCoroutines let you have a very large number of seemingly simultane-\nous functions in your Python programs. They’re implemented using \nthe async and await keywords along with the same infrastructure \nthat powers generators (see Item 30: “Consider Generators Instead of \nReturning Lists,” Item 34: “Avoid Injecting Data into Generators with \nsend,” and Item 35: “Avoid Causing State Transitions in Generators \nwith throw”). \nThe cost of starting a coroutine is a function call. Once a coroutine \nis active, it uses less than 1 KB of memory until it’s exhausted. Like \nthreads, coroutines are independent functions that can consume \ninputs from their environment and produce resulting outputs. The \ndifference is that coroutines pause at each await expression and \nresume executing an async function after the pending awaitable is \nresolved (similar to how yield behaves in generators).\nMany separate async functions advanced in lockstep all seem to \nrun simultaneously, mimicking the concurrent behavior of Python \nthreads. However, coroutines do this without the memory overhead, \nstartup and context switching costs, or complex locking and synchro-\nnization code that’s required for threads. The magical mechanism \npowering coroutines is the event loop, which can do highly concurrent \nI/O efficiently, while rapidly interleaving execution between appropri-\nately written functions.\nI can use coroutines to implement the Game of Life. My goal is to \nallow for I/O to occur within the game_logic function while overcom-\ning the problems from the Thread and Queue approaches in the previ-\nous items. To do this, first I indicate that the game_logic function is a \ncoroutine by defining it using async def instead of def. This will allow \nme to use the await syntax for I/O, such as an asynchronous read \nfrom a socket:\nALIVE = '*'\nEMPTY = '-'\n \n",
      "page_number": 275,
      "chapter_number": 27,
      "summary": "Python provides many built-in tools for achieving fan-out and fan-in \nwith various trade-offs Key topics include thread, items, and grid. Things to Remember\n✦ A program often grows to require multiple concurrent lines of exe-\ncution as its scope and complexity increases.",
      "keywords": [
        "Thread",
        "Concurrency Requires Refactoring",
        "Item",
        "Queue",
        "Thread Instances",
        "grid",
        "Concurrency",
        "Concurrency Requires",
        "ALIVE",
        "game",
        "state",
        "logic",
        "neighbors",
        "Requires Refactoring",
        "Avoid Creating"
      ],
      "concepts": [
        "thread",
        "items",
        "grid",
        "queue",
        "state",
        "concurrency",
        "concurrent",
        "classes",
        "fan",
        "fanning"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 36,
          "title": "Segment 36 (pages 718-737)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 18,
          "title": "Segment 18 (pages 348-366)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 20,
          "title": "Threads and Concurrency",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 54,
          "title": "Segment 54 (pages 590-592)",
          "relevance_score": 0.48,
          "method": "api"
        }
      ]
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 290-298)",
      "start_page": 290,
      "end_page": 298,
      "detection_method": "topic_boundary",
      "content": "268 \nChapter 7 Concurrency and Parallelism\nclass Grid:\n    ...\n \ndef count_neighbors(y, x, get):\n    ...\n \nasync def game_logic(state, neighbors):\n    ...\n    # Do some input/output in here:\n    data = await my_socket.read(50)\n    ...\nSimilarly, I can turn step_cell into a coroutine by adding async to its \ndefinition and using await for the call to the game_logic function:\nasync def step_cell(y, x, get, set):\n    state = get(y, x)\n    neighbors = count_neighbors(y, x, get)\n    next_state = await game_logic(state, neighbors)\n    set(y, x, next_state)\nThe simulate function also needs to become a coroutine:\nimport asyncio\n \nasync def simulate(grid):\n    next_grid = Grid(grid.height, grid.width)\n \n    tasks = []\n    for y in range(grid.height):\n        for x in range(grid.width):\n            task = step_cell(\n                y, x, grid.get, next_grid.set)      # Fan out\n            tasks.append(task)\n \n    await asyncio.gather(*tasks)                    # Fan in\n \n    return next_grid\nThe coroutine version of the simulate function requires some \nexplanation:\n \n■Calling step_cell doesn’t immediately run that function. Instead, \nit returns a coroutine instance that can be used with an await \nexpression at a later time. This is similar to how generator func-\ntions that use yield return a generator instance when they’re \ncalled instead of executing immediately. Deferring execution like \nthis is the mechanism that causes fan-out.\n\n\n \nItem 60: Achieve Highly Concurrent I/O with Coroutines \n269\n \n■The gather function from the asyncio built-in library causes \nfan-in. The await expression on gather instructs the event loop to \nrun the step_cell coroutines concurrently and resume execution \nof the simulate coroutine when all of them have been completed.\n \n■No locks are required for the Grid instance since all execution \noccurs within a single thread. The I/O becomes parallelized as \npart of the event loop that’s provided by asyncio.\nFinally, I can drive this code with a one-line change to the origi-\nnal example. This relies on the asyncio.run function to execute the \nsimulate coroutine in an event loop and carry out its dependent I/O:\nclass ColumnPrinter:\n    ...\n \ngrid = Grid(5, 9)\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\ngrid.set(2, 4, ALIVE)\n \ncolumns = ColumnPrinter()\nfor i in range(5):\n    columns.append(str(grid))\n    grid = asyncio.run(simulate(grid))   # Run the event loop\n \nprint(columns)\n>>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --*-*---- | ----*---- | ---*----- | ----*----\n--***---- | ---**---- | --*-*---- | ----**--- | -----*---\n--------- | ---*----- | ---**---- | ---**---- | ---***---\n--------- | --------- | --------- | --------- | ---------\nThe result is the same as before. All of the overhead associ-\nated with threads has been eliminated. Whereas the Queue and \nThreadPoolExecutor approaches are limited in their exception \n handling—merely re-raising exceptions across thread boundaries—\nwith coroutines I can actually use the interactive debugger to step \nthrough the code line by line (see Item 80: “Consider Interactive \nDebugging with pdb”):\nasync def game_logic(state, neighbors):\n    ...\n\n\n270 \nChapter 7 Concurrency and Parallelism\n    raise OSError('Problem with I/O')\n    ...\n \nasyncio.run(game_logic(ALIVE, 3))\n>>>\nTraceback ...\nOSError: Problem with I/O\nLater, if my requirements change and I also need to do I/O from \nwithin count_neighbors, I can easily accomplish this by adding async \nand await keywords to the existing functions and call sites instead of \nhaving to restructure everything as I would have had to do if I were \nusing Thread or Queue instances (see Item 61: “Know How to Port \nThreaded I/O to asyncio” for another example):\nasync def count_neighbors(y, x, get):\n    ...\n \nasync def step_cell(y, x, get, set):\n    state = get(y, x)\n    neighbors = await count_neighbors(y, x, get)\n    next_state = await game_logic(state, neighbors)\n    set(y, x, next_state)\n \ngrid = Grid(5, 9)\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\ngrid.set(2, 4, ALIVE)\n \ncolumns = ColumnPrinter()\nfor i in range(5):\n    columns.append(str(grid))\n    grid = asyncio.run(simulate(grid))\n \nprint(columns)\n>>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --*-*---- | ----*---- | ---*----- | ----*----\n--***---- | ---**---- | --*-*---- | ----**--- | -----*---\n--------- | ---*----- | ---**---- | ---**---- | ---***---\n--------- | --------- | --------- | --------- | ---------\n\n\n \nItem 61: Know How to Port Threaded I/O to asyncio \n271\nThe beauty of coroutines is that they decouple your code’s instruc-\ntions for the external environment (i.e., I/O) from the implementation \nthat carries out your wishes (i.e., the event loop). They let you focus \non the logic of what you’re trying to do instead of wasting time trying \nto figure out how you’re going to accomplish your goals concurrently.\nThings to Remember\n✦ Functions that are defined using the async keyword are called \ncoroutines. A caller can receive the result of a dependent coroutine \nby using the await keyword.\n✦ Coroutines provide an efficient way to run tens of thousands of \nfunctions seemingly at the same time.\n✦ Coroutines can use fan-out and fan-in in order to parallelize I/O, \nwhile also overcoming all of the problems associated with doing I/O \nin threads.\nItem 61: Know How to Port Threaded I/O to asyncio\nOnce you understand the advantage of coroutines (see Item 60: \n“Achieve Highly Concurrent I/O with Coroutines”), it may seem daunt-\ning to port an existing codebase to use them. Luckily, Python’s sup-\nport for asynchronous execution is well integrated into the language. \nThis makes it straightforward to move code that does threaded, \nblocking I/O over to coroutines and asynchronous I/O.\nFor example, say that I have a TCP-based server for playing a game \ninvolving guessing a number. The server takes lower and upper \nparameters that determine the range of numbers to consider. Then, \nthe server returns guesses for integer values in that range as they are \nrequested by the client. Finally, the server collects reports from the \nclient on whether each of those numbers was closer (warmer) or fur-\nther away (colder) from the client’s secret number.\nThe most common way to build this type of client/server system is by \nusing blocking I/O and threads (see Item 53: “Use Threads for Block-\ning I/O, Avoid for Parallelism”). To do this, I need a helper class that \ncan manage sending and receiving of messages. For my purposes, \neach line sent or received represents a command to be processed:\nclass EOFError(Exception):\n    pass\n \nclass ConnectionBase:\n    def __init__(self, connection):\n\n\n272 \nChapter 7 Concurrency and Parallelism\n        self.connection = connection\n        self.file = connection.makefile('rb')\n \n    def send(self, command):\n        line = command + '\\n'\n        data = line.encode()\n        self.connection.send(data)\n \n    def receive(self):\n        line = self.file.readline()\n        if not line:\n            raise EOFError('Connection closed')\n        return line[:-1].decode()\nThe server is implemented as a class that handles one connection at a \ntime and maintains the client’s session state: \nimport random\n \nWARMER = 'Warmer'\nCOLDER = 'Colder'\nUNSURE = 'Unsure'\nCORRECT = 'Correct'\n \nclass UnknownCommandError(Exception):\n    pass\n \nclass Session(ConnectionBase):\n    def __init__(self, *args):\n        super().__init__(*args)\n        self._clear_state(None, None)\n \n    def _clear_state(self, lower, upper):\n        self.lower = lower\n        self.upper = upper\n        self.secret = None\n        self.guesses = []\nIt has one primary method that handles incoming commands from \nthe client and dispatches them to methods as needed. Note that here \nI’m using an assignment expression (introduced in Python 3.8; see \nItem 10: “Prevent Repetition with Assignment Expressions”) to keep \nthe code short:\n    def loop(self):\n        while command := self.receive():\n\n\n            parts = command.split(' ')\n            if parts[0] == 'PARAMS':\n                self.set_params(parts)\n            elif parts[0] == 'NUMBER':\n                self.send_number()\n            elif parts[0] == 'REPORT':\n                self.receive_report(parts)\n            else:\n                raise UnknownCommandError(command)\nThe first command sets the lower and upper bounds for the numbers \nthat the server is trying to guess:\n    def set_params(self, parts):\n        assert len(parts) == 3\n        lower = int(parts[1])\n        upper = int(parts[2])\n        self._clear_state(lower, upper)\nThe second command makes a new guess based on the previous state \nthat’s stored in the client’s Session instance. Specifically, this code \nensures that the server will never try to guess the same number more \nthan once per parameter assignment:\n    def next_guess(self):\n        if self.secret is not None:\n            return self.secret\n \n        while True:\n            guess = random.randint(self.lower, self.upper)\n            if guess not in self.guesses:\n                return guess\n \n    def send_number(self):\n        guess = self.next_guess()\n        self.guesses.append(guess)\n        self.send(format(guess))\nThe third command receives the decision from the client of whether \nthe guess was warmer or colder, and it updates the Session state \naccordingly:\n    def receive_report(self, parts):\n        assert len(parts) == 2\n        decision = parts[1]\n \n        last = self.guesses[-1]\n \nItem 61: Know How to Port Threaded I/O to asyncio \n273\n\n\n274 \nChapter 7 Concurrency and Parallelism\n        if decision == CORRECT:\n            self.secret = last\n \n        print(f'Server: {last} is {decision}')\nThe client is also implemented using a stateful class:\nimport contextlib\nimport math\n \nclass Client(ConnectionBase):\n    def __init__(self, *args):\n        super().__init__(*args)\n        self._clear_state()\n \n    def _clear_state(self):\n        self.secret = None\n        self.last_distance = None\nThe parameters of each guessing game are set using a with state-\nment to ensure that state is correctly managed on the server side (see \nItem 66: “Consider contextlib and with Statements for Reusable try/\nfinally Behavior” for background and Item 63: “Avoid Blocking the \nasyncio Event Loop to Maximize Responsiveness” for another exam-\nple). This method sends the first command to the server:\n    @contextlib.contextmanager\n    def session(self, lower, upper, secret):\n        print(f'Guess a number between {lower} and {upper}!'\n              f' Shhhhh, it\\'s {secret}.')\n        self.secret = secret\n        self.send(f'PARAMS {lower} {upper}')\n        try:\n            yield\n        finally:\n            self._clear_state()\n            self.send('PARAMS 0 -1')\nNew guesses are requested from the server, using another method \nthat implements the second command:\n    def request_numbers(self, count):\n        for _ in range(count):\n            self.send('NUMBER')\n            data = self.receive()\n            yield int(data)\n            if self.last_distance == 0:\n                return\n\n\nWhether each guess from the server was warmer or colder than the \nlast is reported using the third command in the final method:\n    def report_outcome(self, number):\n        new_distance = math.fabs(number - self.secret)\n        decision = UNSURE\n \n        if new_distance == 0:\n            decision = CORRECT\n        elif self.last_distance is None:\n            pass\n        elif new_distance < self.last_distance:\n            decision = WARMER\n        elif new_distance > self.last_distance:\n            decision = COLDER\n \n        self.last_distance = new_distance\n \n        self.send(f'REPORT {decision}')\n        return decision\nI can run the server by having one thread listen on a socket and \nspawn additional threads to handle the new connections:\nimport socket\nfrom threading import Thread\n \ndef handle_connection(connection):\n    with connection:\n        session = Session(connection)\n        try:\n            session.loop()\n        except EOFError:\n            pass\n \ndef run_server(address):\n    with socket.socket() as listener:\n        listener.bind(address)\n        listener.listen()\n        while True:\n            connection, _ = listener.accept()\n            thread = Thread(target=handle_connection,\n                            args=(connection,),\n                            daemon=True)\n            thread.start()\n \nItem 61: Know How to Port Threaded I/O to asyncio \n275\n\n\n276 \nChapter 7 Concurrency and Parallelism\nThe client runs in the main thread and returns the results of the \nguessing game to the caller. This code explicitly exercises a variety \nof Python language features (for loops, with statements, generators, \ncomprehensions) so that below I can show what it takes to port these \nover to using coroutines:\ndef run_client(address):\n    with socket.create_connection(address) as connection:\n        client = Client(connection)\n \n        with client.session(1, 5, 3):\n            results = [(x, client.report_outcome(x))\n                       for x in client.request_numbers(5)]\n \n        with client.session(10, 15, 12):\n            for number in client.request_numbers(5):\n                outcome = client.report_outcome(number)\n                results.append((number, outcome))\n \n    return results\nFinally, I can glue all of this together and confirm that it works as \nexpected:\ndef main():\n    address = ('127.0.0.1', 1234)\n    server_thread = Thread(\n        target=run_server, args=(address,), daemon=True)\n    server_thread.start()\n \n    results = run_client(address)\n    for number, outcome in results:\n        print(f'Client: {number} is {outcome}')\n \nmain()\n>>>\nGuess a number between 1 and 5! Shhhhh, it's 3.\nServer: 4 is Unsure\nServer: 1 is Colder\nServer: 5 is Unsure\nServer: 3 is Correct\nGuess a number between 10 and 15! Shhhhh, it's 12.\nServer: 11 is Unsure\nServer: 10 is Colder\nServer: 12 is Correct\n",
      "page_number": 290,
      "chapter_number": 28,
      "summary": "This chapter covers segment 28 (pages 290-298). Key topics include thread, functions, and function. Python addresses the need for highly concurrent I/O with coroutines.",
      "keywords": [
        "Item",
        "Coroutines",
        "ALIVE",
        "State",
        "Achieve Highly Concurrent",
        "Grid",
        "Highly Concurrent",
        "Port Threaded",
        "async def",
        "async",
        "server",
        "await",
        "function",
        "Thread",
        "neighbors"
      ],
      "concepts": [
        "thread",
        "functions",
        "function",
        "grid",
        "item",
        "classes",
        "state",
        "connection",
        "connections",
        "command"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 40,
          "title": "Segment 40 (pages 798-818)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 37,
          "title": "Segment 37 (pages 738-758)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 6,
          "title": "Segment 6 (pages 41-48)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "Segment 21 (pages 193-216)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 299-314)",
      "start_page": 299,
      "end_page": 314,
      "detection_method": "topic_boundary",
      "content": "Client: 4 is Unsure\nClient: 1 is Colder\nClient: 5 is Unsure\nClient: 3 is Correct\nClient: 11 is Unsure\nClient: 10 is Colder\nClient: 12 is Correct\nHow much effort is needed to convert this example to using async, \nawait, and the asyncio built-in module? \nFirst, I need to update my ConnectionBase class to provide coroutines \nfor send and receive instead of blocking I/O methods. I’ve marked \neach line that’s changed with a # Changed comment to make it clear \nwhat the delta is between this new example and the code above:\nclass AsyncConnectionBase:\n    def __init__(self, reader, writer):             # Changed\n        self.reader = reader                        # Changed\n        self.writer = writer                        # Changed\n \n    async def send(self, command):\n        line = command + '\\n'\n        data = line.encode()\n        self.writer.write(data)                     # Changed\n        await self.writer.drain()                   # Changed\n \n    async def receive(self):\n        line = await self.reader.readline()         # Changed\n        if not line:\n            raise EOFError('Connection closed')\n        return line[:-1].decode()\nI can create another stateful class to represent the session state for \na single connection. The only changes here are the class’s name and \ninheriting from AsyncConnectionBase instead of ConnectionBase:\nclass AsyncSession(AsyncConnectionBase):            # Changed\n    def __init__(self, *args):\n        ...\n \n    def _clear_values(self, lower, upper):\n        ...\nThe primary entry point for the server’s command processing loop \nrequires only minimal changes to become a coroutine:\n    async def loop(self):                           # Changed\n \nItem 61: Know How to Port Threaded I/O to asyncio \n277\n\n\n278 \nChapter 7 Concurrency and Parallelism\n        while command := await self.receive():      # Changed\n            parts = command.split(' ')\n            if parts[0] == 'PARAMS':\n                self.set_params(parts)\n            elif parts[0] == 'NUMBER':\n                await self.send_number()            # Changed\n            elif parts[0] == 'REPORT':\n                self.receive_report(parts)\n            else:\n                raise UnknownCommandError(command)\nNo changes are required for handling the first command:\n    def set_params(self, parts):\n        ...\nThe only change required for the second command is allowing asyn-\nchronous I/O to be used when guesses are transmitted to the client:\n    def next_guess(self):\n        ...\n \n    async def send_number(self):                    # Changed\n        guess = self.next_guess()\n        self.guesses.append(guess)\n        await self.send(format(guess))              # Changed\nNo changes are required for processing the third command:\n    def receive_report(self, parts):\n        ...\nSimilarly, the client class needs to be reimplemented to inherit from \nAsyncConnectionBase:\nclass AsyncClient(AsyncConnectionBase):             # Changed\n    def __init__(self, *args):\n        ...\n \n    def _clear_state(self):\n        ...\nThe first command method for the client requires a few async and await \nkeywords to be added. It also needs to use the asynccontextmanager \nhelper function from the contextlib built-in module:\n    @contextlib.asynccontextmanager                 # Changed\n    async def session(self, lower, upper, secret):  # Changed\n        print(f'Guess a number between {lower} and {upper}!'\n              f' Shhhhh, it\\'s {secret}.')\n\n\n        self.secret = secret\n        await self.send(f'PARAMS {lower} {upper}')  # Changed\n        try:\n            yield\n        finally:\n            self._clear_state()\n            await self.send('PARAMS 0 -1')          # Changed\nThe second command again only requires the addition of async and \nawait anywhere coroutine behavior is required:\n    async def request_numbers(self, count):         # Changed\n        for _ in range(count):\n            await self.send('NUMBER')               # Changed\n            data = await self.receive()             # Changed\n            yield int(data)\n            if self.last_distance == 0:\n                return\nThe third command only requires adding one async and one await \nkeyword:\n    async def report_outcome(self, number):         # Changed\n        ...\n        await self.send(f'REPORT {decision}')       # Changed\n        ...\nThe code that runs the server needs to be completely reimplemented \nto use the asyncio built-in module and its start_server function:\nimport asyncio\n \nasync def handle_async_connection(reader, writer):\n    session = AsyncSession(reader, writer)\n    try:\n        await session.loop()\n    except EOFError:\n        pass\n \nasync def run_async_server(address):\n    server = await asyncio.start_server(\n        handle_async_connection, *address)\n    async with server:\n        await server.serve_forever()\nThe run_client function that initiates the game requires changes on \nnearly every line. Any code that previously interacted with the block-\ning socket instances has to be replaced with asyncio versions of \n \nItem 61: Know How to Port Threaded I/O to asyncio \n279\n\n\n280 \nChapter 7 Concurrency and Parallelism\nsimilar functionality (which are marked with # New below). All other \nlines in the function that require interaction with coroutines need to \nuse async and await keywords as appropriate. If you forget to add one \nof these keywords in a necessary place, an exception will be raised at \nruntime.\nasync def run_async_client(address):\n    streams = await asyncio.open_connection(*address)   # New\n    client = AsyncClient(*streams)                      # New\n \n    async with client.session(1, 5, 3):\n        results = [(x, await client.report_outcome(x))\n                   async for x in client.request_numbers(5)]\n \n    async with client.session(10, 15, 12):\n        async for number in client.request_numbers(5):\n            outcome = await client.report_outcome(number)\n            results.append((number, outcome))\n \n    _, writer = streams                                 # New\n    writer.close()                                      # New\n    await writer.wait_closed()                          # New\n \n    return results\nWhat’s most interesting about run_async_client is that I didn’t have \nto restructure any of the substantive parts of interacting with the \nAsyncClient in order to port this function over to use coroutines. Each \nof the language features that I needed has a corresponding asynchro-\nnous version, which made the migration easy to do.\nThis won’t always be the case, though. There are currently no asyn-\nchronous versions of the next and iter built-in functions (see Item \n31: “Be Defensive When Iterating Over Arguments” for background); \nyou have to await on the __anext__ and __aiter__ methods directly. \nThere’s also no asynchronous version of yield from (see Item 33: \n“Compose Multiple Generators with yield from”), which makes it \nnoisier to compose generators. But given the rapid pace at which \nasync functionality is being added to Python, it’s only a matter of time \nbefore these features become available.\nFinally, the glue needs to be updated to run this new asynchro-\nnous example end-to-end. I use the asyncio.create_task function to \nenqueue the server for execution on the event loop so that it runs in \nparallel with the client when the await expression is reached. This is \n\n\nanother approach to causing fan-out with different behavior than the \nasyncio.gather function:\nasync def main_async():\n    address = ('127.0.0.1', 4321)\n \n    server = run_async_server(address)\n    asyncio.create_task(server)\n \n    results = await run_async_client(address)\n    for number, outcome in results:\n        print(f'Client: {number} is {outcome}')\n \nasyncio.run(main_async())\n>>>\nGuess a number between 1 and 5! Shhhhh, it's 3.\nServer: 5 is Unsure\nServer: 4 is Warmer\nServer: 2 is Unsure\nServer: 1 is Colder\nServer: 3 is Correct\nGuess a number between 10 and 15! Shhhhh, it's 12.\nServer: 14 is Unsure\nServer: 10 is Unsure\nServer: 15 is Colder\nServer: 12 is Correct\nClient: 5 is Unsure\nClient: 4 is Warmer\nClient: 2 is Unsure\nClient: 1 is Colder\nClient: 3 is Correct\nClient: 14 is Unsure\nClient: 10 is Unsure\nClient: 15 is Colder\nClient: 12 is Correct\nThis works as expected. The coroutine version is easier to follow \nbecause all of the interactions with threads have been removed. The \nasyncio built-in module also provides many helper functions and \nshortens the amount of socket boilerplate required to write a server \nlike this.\nYour use case may be more complex and harder to port for a variety \nof reasons. The asyncio module has a vast number of I/O, synchro-\nnization, and task management features that could make adopting \n \nItem 61: Know How to Port Threaded I/O to asyncio \n281\n\n\n282 \nChapter 7 Concurrency and Parallelism\ncoroutines easier for you (see Item 62: “Mix Threads and Coroutines \nto Ease the Transition to asyncio” and Item 63: “Avoid Blocking the \nasyncio Event Loop to Maximize Responsiveness”). Be sure to check \nout the online documentation for the library (https://docs.python.\norg/3/library/asyncio.html) to understand its full potential.\nThings to Remember\n✦ Python provides asynchronous versions of for loops, with state-\nments, generators, comprehensions, and library helper functions \nthat can be used as drop-in replacements in coroutines.\n✦ The asyncio built-in module makes it straightforward to port exist-\ning code that uses threads and blocking I/O over to coroutines and \nasynchronous I/O.\nItem 62:  Mix Threads and Coroutines to Ease the \nTransition to asyncio\nIn the previous item (see Item 61: “Know How to Port Threaded I/O to \nasyncio”), I ported a TCP server that does blocking I/O with threads \nover to use asyncio with coroutines. The transition was big-bang: \nI moved all of the code to the new style in one go. But it’s rarely  feasible \nto port a large program this way. Instead, you usually need to incre-\nmentally migrate your codebase while also updating your tests as \nneeded and verifying that everything works at each step along the way.\nIn order to do that, your codebase needs to be able to use threads \nfor blocking I/O (see Item 53: “Use Threads for Blocking I/O, Avoid \nfor Parallelism”) and coroutines for asynchronous I/O (see Item 60: \n“Achieve Highly Concurrent I/O with Coroutines”) at the same time \nin a way that’s mutually compatible. Practically, this means that you \nneed threads to be able to run coroutines, and you need coroutines to \nbe able to start and wait on threads. Luckily, asyncio includes built-in \nfacilities for making this type of interoperability straightforward.\nFor example, say that I’m writing a program that merges log files into \none output stream to aid with debugging. Given a file handle for an \ninput log, I need a way to detect whether new data is available and \nreturn the next line of input. I can do this using the tell method of \nthe file handle to check whether the current read position matches the \nlength of the file. When no new data is present, an exception should \nbe raised (see Item 20: “Prefer Raising Exceptions to Returning None” \nfor background):\nclass NoNewData(Exception):\n    pass\n \n\n\n Item 62: Mix Threads and Coroutines to Ease the Transition to asyncio \n283\ndef readline(handle):\n    offset = handle.tell()\n    handle.seek(0, 2)\n    length = handle.tell()\n \n    if length == offset:\n        raise NoNewData\n \n    handle.seek(offset, 0)\n    return handle.readline()\nBy wrapping this function in a while loop, I can turn it into a worker \nthread. When a new line is available, I call a given callback function \nto write it to the output log (see Item 38: “Accept Functions Instead of \nClasses for Simple Interfaces” for why to use a function interface for \nthis instead of a class). When no data is available, the thread sleeps \nto reduce the amount of busy waiting caused by polling for new data. \nWhen the input file handle is closed, the worker thread exits:\nimport time\n \ndef tail_file(handle, interval, write_func):\n    while not handle.closed:\n        try:\n            line = readline(handle)\n        except NoNewData:\n            time.sleep(interval)\n        else:\n            write_func(line)\nNow, I can start one worker thread per input file and unify their out-\nput into a single output file. The write helper function below needs to \nuse a Lock instance (see Item 54: “Use Lock to Prevent Data Races in \nThreads”) in order to serialize writes to the output stream and make \nsure that there are no intra-line conflicts:\nfrom threading import Lock, Thread\n \ndef run_threads(handles, interval, output_path):\n    with open(output_path, 'wb') as output:\n        lock = Lock()\n        def write(data):\n            with lock:\n                output.write(data)\n \n\n\n284 \nChapter 7 Concurrency and Parallelism\n        threads = []\n        for handle in handles:\n            args = (handle, interval, write)\n            thread = Thread(target=tail_file, args=args)\n            thread.start()\n            threads.append(thread)\n \n        for thread in threads:\n            thread.join()\nAs long as an input file handle is still alive, its corresponding worker \nthread will also stay alive. That means it’s sufficient to wait for the \njoin method from each thread to complete in order to know that the \nwhole process is done.\nGiven a set of input paths and an output path, I can call run_threads \nand confirm that it works as expected. How the input file handles are \ncreated or separately closed isn’t important in order to demonstrate \nthis code’s behavior, nor is the output verification function—defined \nin confirm_merge that follows—which is why I’ve left them out here:\ndef confirm_merge(input_paths, output_path):\n    ...\n \ninput_paths = ...\nhandles = ...\noutput_path = ...\nrun_threads(handles, 0.1, output_path)\n \nconfirm_merge(input_paths, output_path)\nWith this threaded implementation as the starting point, how can \nI incrementally convert this code to use asyncio and coroutines \ninstead? There are two approaches: top-down and bottom-up.\nTop-down means starting at the highest parts of a codebase, like in \nthe main entry points, and working down to the individual functions \nand classes that are the leaves of the call hierarchy. This approach \ncan be useful when you maintain a lot of common modules that you \nuse across many different programs. By porting the entry points first, \nyou can wait to port the common modules until you’re already using \ncoroutines everywhere else.\nThe concrete steps are:\n 1. Change a top function to use async def instead of def.\n 2. Wrap all of its calls that do I/O—potentially blocking the event \nloop—to use asyncio.run_in_executor instead.\n\n\n 3. Ensure that the resources or callbacks used by run_in_executor \ninvocations are properly synchronized (i.e., using Lock or the \nasyncio.run_coroutine_threadsafe function).\n 4. Try to eliminate get_event_loop and run_in_executor calls by \nmoving downward through the call hierarchy and converting \nintermediate functions and methods to coroutines (following the \nfirst three steps).\nHere, I apply steps 1–3 to the run_threads function:\nimport asyncio\n \nasync def run_tasks_mixed(handles, interval, output_path):\n    loop = asyncio.get_event_loop()\n \n    with open(output_path, 'wb') as output:\n        async def write_async(data):\n            output.write(data)\n \n        def write(data):\n            coro = write_async(data)\n            future = asyncio.run_coroutine_threadsafe(\n                coro, loop)\n            future.result()\n \n        tasks = []\n        for handle in handles:\n            task = loop.run_in_executor(\n                None, tail_file, handle, interval, write)\n            tasks.append(task)\n \n        await asyncio.gather(*tasks)\nThe run_in_executor method instructs the event loop to run a given \nfunction—tail_file in this case—using a specific ThreadPoolExecutor \n(see Item 59: “Consider ThreadPoolExecutor When Threads Are Neces-\nsary for Concurrency”) or a default executor instance when the first \nparameter is None. By making multiple calls to run_in_executor with-\nout corresponding await expressions, the run_tasks_mixed coroutine \nfans out to have one concurrent line of execution for each input file. \nThen, the asyncio.gather function along with an await expression \nfans in the tail_file threads until they all complete (see Item 56: \n“Know How to Recognize When Concurrency Is Necessary” for more \nabout fan-out and fan-in).\n Item 62: Mix Threads and Coroutines to Ease the Transition to asyncio \n285\n\n\n286 \nChapter 7 Concurrency and Parallelism\nThis code eliminates the need for the Lock instance in the write helper \nby using asyncio.run_coroutine_threadsafe. This function allows \nplain old worker threads to call a coroutine—write_async in this \ncase—and have it execute in the event loop from the main thread (or \nfrom any other thread, if necessary). This effectively synchronizes the \nthreads together and ensures that all writes to the output file are only \ndone by the event loop in the main thread. Once the asyncio.gather \nawaitable is resolved, I can assume that all writes to the output file \nhave also completed, and thus I can close the output file handle in \nthe with statement without having to worry about race conditions.\nI can verify that this code works as expected. I use the asyncio.run \nfunction to start the coroutine and run the main event loop:\ninput_paths = ...\nhandles = ...\noutput_path = ...\nasyncio.run(run_tasks_mixed(handles, 0.1, output_path))\n \nconfirm_merge(input_paths, output_path)\nNow, I can apply step 4 to the run_tasks_mixed function by moving \ndown the call stack. I can redefine the tail_file dependent function \nto be an asynchronous coroutine instead of doing blocking I/O by fol-\nlowing steps 1–3:\nasync def tail_async(handle, interval, write_func):\n    loop = asyncio.get_event_loop()\n \n    while not handle.closed:\n        try:\n            line = await loop.run_in_executor(\n                None, readline, handle)\n        except NoNewData:\n            await asyncio.sleep(interval)\n        else:\n            await write_func(line)\nThis new implementation of tail_async allows me to push calls to \nget_event_loop and run_in_executor down the stack and out of the \nrun_tasks_mixed function entirely. What’s left is clean and much eas-\nier to follow:\nasync def run_tasks(handles, interval, output_path):\n    with open(output_path, 'wb') as output:\n        async def write_async(data):\n            output.write(data)\n \n\n\n        tasks = []\n        for handle in handles:\n            coro = tail_async(handle, interval, write_async)\n            task = asyncio.create_task(coro)\n            tasks.append(task)\n \n        await asyncio.gather(*tasks)\nI can verify that run_tasks works as expected, too:\ninput_paths = ...\nhandles = ...\noutput_path = ...\nasyncio.run(run_tasks(handles, 0.1, output_path))\n \nconfirm_merge(input_paths, output_path)\nIt’s possible to continue this iterative refactoring pattern and convert \nreadline into an asynchronous coroutine as well. However, that func-\ntion requires so many blocking file I/O operations that it doesn’t seem \nworth porting, given how much that would reduce the clarity of the \ncode and hurt performance. In some situations, it makes sense to \nmove everything to asyncio, and in others it doesn’t.\nThe bottom-up approach to adopting coroutines has four steps that \nare similar to the steps of the top-down style, but the process tra-\nverses the call hierarchy in the opposite direction: from leaves to \nentry points.\nThe concrete steps are:\n 1. Create a new asynchronous coroutine version of each leaf func-\ntion that you’re trying to port.\n 2. Change the existing synchronous functions so they call the \ncoroutine versions and run the event loop instead of implement-\ning any real behavior.\n 3. Move up a level of the call hierarchy, make another layer of corou-\ntines, and replace existing calls to synchronous functions with \ncalls to the coroutines defined in step 1.\n 4. Delete synchronous wrappers around coroutines created in step 2 \nas you stop requiring them to glue the pieces together.\nFor the example above, I would start with the tail_file function since \nI decided that the readline function should keep using blocking I/O. \nI can rewrite tail_file so it merely wraps the tail_async coroutine \nthat I defined above. To run that coroutine until it finishes, I need to \n Item 62: Mix Threads and Coroutines to Ease the Transition to asyncio \n287\n\n\n288 \nChapter 7 Concurrency and Parallelism\ncreate an event loop for each tail_file worker thread and then call \nits run_until_complete method. This method will block the current \nthread and drive the event loop until the tail_async coroutine exits, \nachieving the same behavior as the threaded, blocking I/O version of \ntail_file:\ndef tail_file(handle, interval, write_func):\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n \n    async def write_async(data):\n        write_func(data)\n \n    coro = tail_async(handle, interval, write_async)\n    loop.run_until_complete(coro)\nThis new tail_file function is a drop-in replacement for the old one. \nI can verify that everything works as expected by calling run_threads \nagain:\ninput_paths = ...\nhandles = ...\noutput_path = ...\nrun_threads(handles, 0.1, output_path)\n \nconfirm_merge(input_paths, output_path)\nAfter wrapping tail_async with tail_file, the next step is to convert \nthe run_threads function to a coroutine. This ends up being the same \nwork as step 4 of the top-down approach above, so at this point, the \nstyles converge.\nThis is a great start for adopting asyncio, but there’s even more \nthat you could do to increase the responsiveness of your program \n(see Item 63: “Avoid Blocking the asyncio Event Loop to Maximize \nResponsiveness”).\nThings to Remember\n✦ The awaitable run_in_executor method of the asyncio event \nloop enables coroutines to run synchronous functions in \nThreadPoolExecutor pools. This facilitates top-down migrations to \nasyncio.\n✦ The run_until_complete method of the asyncio event loop enables \nsynchronous code to run a coroutine until it finishes. The \nasyncio.run_coroutine_threadsafe function provides the same \nfunctionality across thread boundaries. Together these help with \nbottom-up migrations to asyncio.\n\n\n \nItem 63: Avoid Blocking the asyncio Event Loop \n289\nItem 63:  Avoid Blocking the asyncio Event Loop to \nMaximize Responsiveness\nIn the previous item I showed how to migrate to asyncio incrementally \n(see Item 62: “Mix Threads and Coroutines to Ease the Transition to \nasyncio” for background and the implementation of various functions \nbelow). The resulting coroutine properly tails input files and merges \nthem into a single output:\nimport asyncio\n \nasync def run_tasks(handles, interval, output_path):\n    with open(output_path, 'wb') as output:\n        async def write_async(data):\n            output.write(data)\n \n        tasks = []\n        for handle in handles:\n            coro = tail_async(handle, interval, write_async)\n            task = asyncio.create_task(coro)\n            tasks.append(task)\n \n        await asyncio.gather(*tasks)\nHowever, it still has one big problem: The open, close, and write calls \nfor the output file handle happen in the main event loop. These opera-\ntions all require making system calls to the program’s host operating \nsystem, which may block the event loop for significant amounts of \ntime and prevent other coroutines from making progress. This could \nhurt overall responsiveness and increase latency, especially for pro-\ngrams such as highly concurrent servers.\nI can detect when this problem happens by passing the debug=True \nparameter to the asyncio.run function. Here, I show how the file and \nline of a bad coroutine, presumably blocked on a slow system call, \ncan be identified:\nimport time\n \nasync def slow_coroutine():\n    time.sleep(0.5)  # Simulating slow I/O\n \nasyncio.run(slow_coroutine(), debug=True)\n>>>\nExecuting <Task finished name='Task-1' coro=<slow_coroutine() \n¯done, defined at example.py:29> result=None created \n¯at .../asyncio/base_events.py:487> took 0.503 seconds\n...\n\n\n290 \nChapter 7 Concurrency and Parallelism\nIf I want the most responsive program possible, I need to minimize \nthe potential system calls that are made from within the event loop. \nIn this case, I can create a new Thread subclass (see Item 53: “Use \nThreads for Blocking I/O, Avoid for Parallelism”) that encapsulates \neverything required to write to the output file using its own event \nloop:\nfrom threading import Thread\n \nclass WriteThread(Thread):\n    def __init__(self, output_path):\n        super().__init__()\n        self.output_path = output_path\n        self.output = None\n        self.loop = asyncio.new_event_loop()\n \n    def run(self):\n        asyncio.set_event_loop(self.loop)\n        with open(self.output_path, 'wb') as self.output:\n            self.loop.run_forever()\n \n        # Run one final round of callbacks so the await on\n        # stop() in another event loop will be resolved.\n        self.loop.run_until_complete(asyncio.sleep(0))\nCoroutines in other threads can directly call and await on the write \nmethod of this class, since it’s merely a thread-safe wrapper around \nthe real_write method that actually does the I/O. This eliminates \nthe need for a Lock (see Item 54: “Use Lock to Prevent Data Races in \nThreads”):\n    async def real_write(self, data):\n        self.output.write(data)\n \n    async def write(self, data):\n        coro = self.real_write(data)\n        future = asyncio.run_coroutine_threadsafe(\n            coro, self.loop)\n        await asyncio.wrap_future(future)\nOther coroutines can tell the worker thread when to stop in a thread-\nsafe manner, using similar boilerplate:\n    async def real_stop(self):\n        self.loop.stop()\n \n\n\n    async def stop(self):\n        coro = self.real_stop()\n        future = asyncio.run_coroutine_threadsafe(\n            coro, self.loop)\n        await asyncio.wrap_future(future)\nI can also define the __aenter__ and __aexit__ methods to allow this \nclass to be used in with statements (see Item 66: “Consider contextlib \nand with Statements for Reusable try/finally Behavior”). This \nensures that the worker thread starts and stops at the right times \nwithout slowing down the main event loop thread:\n    async def __aenter__(self):\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, self.start)\n        return self\n \n    async def __aexit__(self, *_):\n        await self.stop()\nWith this new WriteThread class, I can refactor run_tasks into a fully \nasynchronous version that’s easy to read and completely avoids run-\nning slow system calls in the main event loop thread:\ndef readline(handle):\n    ...\n \nasync def tail_async(handle, interval, write_func):\n    ...\n \nasync def run_fully_async(handles, interval, output_path):\n    async with WriteThread(output_path) as output:\n        tasks = []\n        for handle in handles:\n            coro = tail_async(handle, interval, output.write)\n            task = asyncio.create_task(coro)\n            tasks.append(task)\n \n        await asyncio.gather(*tasks)\nI can verify that this works as expected, given a set of input handles \nand an output file path:\ndef confirm_merge(input_paths, output_path):\n    ...\n \n \nItem 63: Avoid Blocking the asyncio Event Loop \n291\n\n\n292 \nChapter 7 Concurrency and Parallelism\ninput_paths = ...\nhandles = ...\noutput_path = ...\nasyncio.run(run_fully_async(handles, 0.1, output_path))\n \nconfirm_merge(input_paths, output_path)\nThings to Remember\n✦ Making system calls in coroutines—including blocking I/O and \nstarting threads—can reduce program responsiveness and increase \nthe perception of latency.\n✦ Pass the debug=True parameter to asyncio.run in order to detect \nwhen certain coroutines are preventing the event loop from reacting \nquickly.\nItem 64:  Consider concurrent.futures for True \nParallelism\nAt some point in writing Python programs, you may hit the perfor-\nmance wall. Even after optimizing your code (see Item 70: “Profile \nBefore Optimizing”), your program’s execution may still be too slow \nfor your needs. On modern computers that have an increasing num-\nber of CPU cores, it’s reasonable to assume that one solution would \nbe parallelism. What if you could split your code’s computation into \nindependent pieces of work that run simultaneously across multiple \nCPU cores?\nUnfortunately, Python’s global interpreter lock (GIL) prevents true \nparallelism in threads (see Item 53: “Use Threads for Blocking I/O, \nAvoid for Parallelism”), so that option is out. Another common sugges-\ntion is to rewrite your most performance-critical code as an extension \nmodule, using the C language. C gets you closer to the bare metal \nand can run faster than Python, eliminating the need for parallelism \nin some cases. C extensions can also start native threads indepen-\ndent of the Python interpreter that run in parallel and utilize multiple \nCPU cores with no concern for the GIL. Python’s API for C exten-\nsions is well documented and a good choice for an escape hatch. It’s \nalso worth checking out tools like SWIG (https://github.com/swig/\nswig) and CLIF (https://github.com/google/clif) to aid in extension \ndevelopment.\nBut rewriting your code in C has a high cost. Code that is short and \nunderstandable in Python can become verbose and complicated in C. \nSuch a port requires extensive testing to ensure that the functionality \n",
      "page_number": 299,
      "chapter_number": 29,
      "summary": "This chapter covers segment 29 (pages 299-314). Key topics include asyncio, thread, and function. First, I need to update my ConnectionBase class to provide coroutines \nfor send and receive instead of blocking I/O methods.",
      "keywords": [
        "async def",
        "async",
        "event loop",
        "Item",
        "Changed async def",
        "asyncio Event Loop",
        "threads",
        "changed",
        "loop",
        "run",
        "async def run",
        "output",
        "await",
        "event",
        "coroutines"
      ],
      "concepts": [
        "asyncio",
        "thread",
        "function",
        "functionality",
        "functions",
        "await",
        "changed",
        "changes",
        "async",
        "handling"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 40,
          "title": "Segment 40 (pages 798-818)",
          "relevance_score": 0.77,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 42,
          "title": "Segment 42 (pages 842-862)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 37,
          "title": "Segment 37 (pages 312-320)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 41,
          "title": "Segment 41 (pages 819-841)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 30,
          "title": "Segment 30 (pages 275-283)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 315-323)",
      "start_page": 315,
      "end_page": 323,
      "detection_method": "topic_boundary",
      "content": " \nItem 64: Consider concurrent.futures for True Parallelism \n293\nis equivalent to the original Python code and that no bugs have been \nintroduced. Sometimes it’s worth it, which explains the large ecosys-\ntem of C-extension modules in the Python community that speed up \nthings like text parsing, image compositing, and matrix math. There \nare even open source tools such as Cython (https://cython.org) and \nNumba (https://numba.pydata.org) that can ease the transition to C.\nThe problem is that moving one piece of your program to C isn’t suffi-\ncient most of the time. Optimized Python programs usually don’t have \none major source of slowness; rather, there are often many signifi-\ncant contributors. To get the benefits of C’s bare metal and threads, \nyou’d need to port large parts of your program, drastically increasing \ntesting needs and risk. There must be a better way to preserve your \ninvestment in Python to solve difficult computational problems.\nThe multiprocessing built-in module, which is easily accessed via the \nconcurrent.futures built-in module, may be exactly what you need \n(see Item 59: “Consider ThreadPoolExecutor When Threads Are Neces-\nsary for Concurrency” for a related example). It enables Python to uti-\nlize multiple CPU cores in parallel by running additional interpreters \nas child processes. These child processes are separate from the main \ninterpreter, so their global interpreter locks are also separate. Each \nchild can fully utilize one CPU core. Each child has a link to the main \nprocess where it receives instructions to do computation and returns \nresults.\nFor example, say that I want to do something computationally inten-\nsive with Python and utilize multiple CPU cores. I’ll use an implemen-\ntation of finding the greatest common divisor of two numbers as a \nproxy for a more computationally intense algorithm (like simulating \nfluid dynamics with the Navier–Stokes equation):\n# my_module.py\ndef gcd(pair):\n    a, b = pair\n    low = min(a, b)\n    for i in range(low, 0, -1):\n        if a % i == 0 and b % i == 0:\n            return i\n    assert False, 'Not reachable'\nRunning this function in serial takes a linearly increasing amount of \ntime because there is no parallelism:\n# run_serial.py\nimport my_module\nimport time\n \n\n\n294 \nChapter 7 Concurrency and Parallelism\nNUMBERS = [\n    (1963309, 2265973), (2030677, 3814172),\n    (1551645, 2229620), (2039045, 2020802),\n    (1823712, 1924928), (2293129, 1020491),\n    (1281238, 2273782), (3823812, 4237281),\n    (3812741, 4729139), (1292391, 2123811),\n]\n \ndef main():\n    start = time.time()\n    results = list(map(my_module.gcd, NUMBERS))\n    end = time.time()\n    delta = end - start\n    print(f'Took {delta:.3f} seconds')\n \nif __name__ == '__main__':\n    main()\n>>>\nTook 1.173 seconds\nRunning this code on multiple Python threads will yield no speed \nimprovement because the GIL prevents Python from using multiple \nCPU cores in parallel. Here, I do the same computation as above but \nusing the concurrent.futures module with its ThreadPoolExecutor \nclass and two worker threads (to match the number of CPU cores on \nmy computer):\n# run_threads.py\nimport my_module\nfrom concurrent.futures import ThreadPoolExecutor\nimport time\n \nNUMBERS = [\n    ...\n]\n \ndef main():\n    start = time.time()\n    pool = ThreadPoolExecutor(max_workers=2)\n    results = list(pool.map(my_module.gcd, NUMBERS))\n    end = time.time()\n    delta = end - start\n    print(f'Took {delta:.3f} seconds')\n \n\n\nif __name__ == '__main__':\n    main()\n>>>\nTook 1.436 seconds\nIt’s even slower this time because of the overhead of starting and com-\nmunicating with the pool of threads.\nNow for the surprising part: Changing a single line of code causes \nsomething magical to happen. If I replace the ThreadPoolExecutor \nwith the ProcessPoolExecutor from the concurrent.futures module, \neverything speeds up:\n# run_parallel.py\nimport my_module\nfrom concurrent.futures import ProcessPoolExecutor\nimport time\n \nNUMBERS = [\n    ...\n]\n \ndef main():\n    start = time.time()\n    pool = ProcessPoolExecutor(max_workers=2)  # The one change\n    results = list(pool.map(my_module.gcd, NUMBERS))\n    end = time.time()\n    delta = end - start\n    print(f'Took {delta:.3f} seconds')\n \nif __name__ == '__main__':\n    main()\n>>>\nTook 0.683 seconds\nRunning on my dual-core machine, this is significantly faster! How is \nthis possible? Here’s what the ProcessPoolExecutor class actually does \n(via the low-level constructs provided by the multiprocessing module):\n 1. It takes each item from the numbers input data to map.\n 2. It serializes the item into binary data by using the pickle module \n(see Item 68: “Make pickle Reliable with copyreg”).\n 3. It copies the serialized data from the main interpreter process to \na child interpreter process over a local socket.\n \nItem 64: Consider concurrent.futures for True Parallelism \n295\n\n\n296 \nChapter 7 Concurrency and Parallelism\n 4. It deserializes the data back into Python objects, using pickle in \nthe child process.\n 5. It imports the Python module containing the gcd function.\n 6. It runs the function on the input data in parallel with other child \nprocesses.\n 7. It serializes the result back into binary data.\n 8. It copies that binary data back through the socket.\n 9. It deserializes the binary data back into Python objects in the \nparent process.\n 10. It merges the results from multiple children into a single list to \nreturn.\nAlthough it looks simple to the programmer, the multiprocessing mod-\nule and ProcessPoolExecutor class do a huge amount of work to make \nparallelism possible. In most other languages, the only touch point \nyou need to coordinate two threads is a single lock or atomic operation \n(see Item 54: “Use Lock to Prevent Data Races in Threads” for an exam-\nple). The overhead of using multiprocessing via ProcessPoolExecutor is \nhigh because of all of the serialization and deserialization that must \nhappen between the parent and child processes.\nThis scheme is well suited to certain types of isolated, high-leverage \ntasks. By isolated, I mean functions that don’t need to share state \nwith other parts of the program. By high-leverage tasks, I mean sit-\nuations in which only a small amount of data must be transferred \nbetween the parent and child processes to enable a large amount of \ncomputation. The greatest common divisor algorithm is one example \nof this, but many other mathematical algorithms work similarly.\nIf your computation doesn’t have these characteristics, then the over-\nhead of ProcessPoolExecutor may prevent it from speeding up your \nprogram through parallelization. When that happens, multiprocessing \nprovides more advanced facilities for shared memory, cross-process \nlocks, queues, and proxies. But all of these features are very com-\nplex. It’s hard enough to reason about such tools in the memory space \nof a single process shared between Python threads. Extending that \ncomplexity to other processes and involving sockets makes this much \nmore difficult to understand.\nI suggest that you initially avoid all parts of the multiprocessing \nbuilt-in module. You can start by using the ThreadPoolExecutor \nclass to run isolated, high-leverage functions in threads. Later you \ncan move to the ProcessPoolExecutor to get a speedup. Finally, when \n\n\nyou’ve completely exhausted the other options, you can consider using \nthe multiprocessing module directly.\nThings to Remember\n✦ Moving CPU bottlenecks to C-extension modules can be an effective \nway to improve performance while maximizing your investment in \nPython code. However, doing so has a high cost and may introduce \nbugs.\n✦ The multiprocessing module provides powerful tools that can paral-\nlelize certain types of Python computation with minimal effort.\n✦ The power of multiprocessing is best accessed through the \nconcurrent.futures built-in module and its simple ProcessPoolExecutor \nclass.\n✦ Avoid the advanced (and complicated) parts of the multiprocessing \nmodule until you’ve exhausted all other options.\n \nItem 64: Consider concurrent.futures for True Parallelism \n297\n\n\nThis page intentionally left blank \n\n\n8\nRobustness and \nPerformance\nOnce you’ve written a useful Python program, the next step is to \n productionize your code so it’s bulletproof. Making programs depend-\nable when they encounter unexpected circumstances is just as \nimportant as making programs with correct functionality. Python has \nbuilt-in features and modules that aid in hardening your  programs so \nthey are robust in a wide variety of situations.\nOne dimension of robustness is scalability and performance. When \nyou’re implementing Python programs that handle a non-trivial \namount of data, you’ll often see slowdowns caused by the algorith-\nmic complexity of your code or other types of computational overhead. \nLuckily, Python includes many of the algorithms and data structures \nyou need to achieve high performance with minimal effort.\nItem 65:  Take Advantage of Each Block in try/except\n/else/finally\nThere are four distinct times when you might want to take action \nduring exception handling in Python. These are captured in the func-\ntionality of try, except, else, and finally blocks. Each block serves a \nunique purpose in the compound statement, and their various com-\nbinations are useful (see Item 87: “Define a Root Exception to Insulate \nCallers from APIs” for another example).\nfinally Blocks\nUse try/finally when you want exceptions to propagate up but also \nwant to run cleanup code even when exceptions occur. One common \nusage of try/finally is for reliably closing file handles (see Item 66: \n“Consider contextlib and with Statements for Reusable try/finally \nBehavior” for another—likely better—approach):\ndef try_finally_example(filename):\n    print('* Opening file')\n\n\n300 \nChapter 8 Robustness and Performance\n    handle = open(filename, encoding='utf-8') # Maybe OSError\n    try:\n        print('* Reading data')\n        return handle.read()  # Maybe UnicodeDecodeError\n    finally:\n        print('* Calling close()')\n        handle.close()        # Always runs after try block\nAny exception raised by the read method will always propagate up to \nthe calling code, but the close method of handle in the finally block \nwill run first:\nfilename = 'random_data.txt'\n \nwith open(filename, 'wb') as f:\n    f.write(b'\\xf1\\xf2\\xf3\\xf4\\xf5')  # Invalid utf-8\n \ndata = try_finally_example(filename)\n>>>\n* Opening file\n* Reading data\n* Calling close()\nTraceback ...\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in \n¯position 0: invalid continuation byte\nYou must call open before the try block because exceptions that occur \nwhen opening the file (like OSError if the file does not exist) should \nskip the finally block entirely:\ntry_finally_example('does_not_exist.txt')\n>>>\n* Opening file\nTraceback ...\nFileNotFoundError: [Errno 2] No such file or directory: \n¯'does_not_exist.txt'\nelse Blocks\nUse try/except/else to make it clear which exceptions will be han-\ndled by your code and which exceptions will propagate up. When \nthe try block doesn’t raise an exception, the else block runs. The \nelse block helps you minimize the amount of code in the try block, \nwhich is good for isolating potential exception causes and improves \n\n\n \nItem 65: Take Advantage of Each Block in try/except/else/finally \n301\nreadability. For example, say that I want to load JSON dictionary data \nfrom a string and return the value of a key it contains:\nimport json\n \ndef load_json_key(data, key):\n    try:\n        print('* Loading JSON data')\n        result_dict = json.loads(data)  # May raise ValueError\n    except ValueError as e:\n        print('* Handling ValueError')\n        raise KeyError(key) from e\n    else:\n        print('* Looking up key')\n        return result_dict[key]         # May raise KeyError\nIn the successful case, the JSON data is decoded in the try block, \nand then the key lookup occurs in the else block:\nassert load_json_key('{\"foo\": \"bar\"}', 'foo') == 'bar'\n>>>\n* Loading JSON data\n* Looking up key\nIf the input data isn’t valid JSON, then decoding with json.loads \nraises a ValueError. The exception is caught by the except block and \nhandled:\nload_json_key('{\"foo\": bad payload', 'foo')\n>>>\n* Loading JSON data\n* Handling ValueError\nTraceback ...\nJSONDecodeError: Expecting value: line 1 column 9 (char 8)\n \nThe above exception was the direct cause of the following \n¯exception:\n \nTraceback ...\nKeyError: 'foo'\nIf the key lookup raises any exceptions, they propagate up to the \ncaller because they are outside the try block. The else clause ensures \nthat what follows the try/except is visually distinguished from the \nexcept block. This makes the exception propagation behavior clear:\nload_json_key('{\"foo\": \"bar\"}', 'does not exist')\n",
      "page_number": 315,
      "chapter_number": 30,
      "summary": "Sometimes it’s worth it, which explains the large ecosys-\ntem of C-extension modules in the Python community that speed up \nthings like text parsing, image compositing, and matrix math Key topics include python, module, and item.",
      "keywords": [
        "Python",
        "True Parallelism",
        "multiple CPU cores",
        "CPU cores",
        "Parallelism",
        "Item",
        "module",
        "Python programs",
        "CPU",
        "multiple CPU",
        "data",
        "code",
        "threads",
        "Python threads",
        "Python code"
      ],
      "concepts": [
        "python",
        "module",
        "item",
        "data",
        "run",
        "running",
        "runs",
        "computers",
        "computationally",
        "program"
      ],
      "similar_chapters": [
        {
          "book": "Python Microservices Development",
          "chapter": 22,
          "title": "Segment 22 (pages 175-182)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Python Data Analysis 3rd",
          "chapter": 1,
          "title": "Preliminaries",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 26,
          "title": "Extending and Embedding Python",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 6,
          "title": "Segment 6 (pages 41-48)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 324-335)",
      "start_page": 324,
      "end_page": 335,
      "detection_method": "topic_boundary",
      "content": "302 \nChapter 8 Robustness and Performance\n>>>\n* Loading JSON data\n* Looking up key\nTraceback ...\nKeyError: 'does not exist'\nEverything Together\nUse try/except/else/finally when you want to do it all in one com-\npound statement. For example, say that I want to read a descrip-\ntion of work to do from a file, process it, and then update the file \nin-place. Here, the try block is used to read the file and process it; the \nexcept block is used to handle exceptions from the try block that are \nexpected; the else block is used to update the file in place and allow \nrelated exceptions to propagate up; and the finally block cleans up \nthe file handle:\nUNDEFINED = object()\n \ndef divide_json(path):\n    print('* Opening file')\n    handle = open(path, 'r+')   # May raise OSError\n    try:\n        print('* Reading data')\n        data = handle.read()    # May raise UnicodeDecodeError\n        print('* Loading JSON data')\n        op = json.loads(data)   # May raise ValueError\n        print('* Performing calculation')\n        value = (\n            op['numerator'] /\n            op['denominator'])  # May raise ZeroDivisionError\n    except ZeroDivisionError as e:\n        print('* Handling ZeroDivisionError')\n        return UNDEFINED\n    else:\n        print('* Writing calculation')\n        op['result'] = value\n        result = json.dumps(op)\n        handle.seek(0)          # May raise OSError\n        handle.write(result)    # May raise OSError\n        return value\n    finally:\n        print('* Calling close()')\n        handle.close()          # Always runs\n\n\nIn the successful case, the try, else, and finally blocks run:\ntemp_path = 'random_data.json'\n \nwith open(temp_path, 'w') as f:\n    f.write('{\"numerator\": 1, \"denominator\": 10}')\n \nassert divide_json(temp_path) == 0.1\n>>>\n* Opening file\n* Reading data\n* Loading JSON data\n* Performing calculation\n* Writing calculation\n* Calling close()\nIf the calculation is invalid, the try, except, and finally blocks run, \nbut the else block does not:\nwith open(temp_path, 'w') as f:\n    f.write('{\"numerator\": 1, \"denominator\": 0}')\n \nassert divide_json(temp_path) is UNDEFINED\n>>>\n* Opening file\n* Reading data\n* Loading JSON data\n* Performing calculation\n* Handling ZeroDivisionError\n* Calling close()\nIf the JSON data was invalid, the try block runs and raises an excep-\ntion, the finally block runs, and then the exception is propagated up \nto the caller. The except and else blocks do not run:\nwith open(temp_path, 'w') as f:\n    f.write('{\"numerator\": 1 bad data')\n \ndivide_json(temp_path)\n>>>\n* Opening file\n* Reading data\n* Loading JSON data\n* Calling close()\nTraceback ...\nJSONDecodeError: Expecting ',' delimiter: line 1 column 17 \n¯(char 16)\n \nItem 65: Take Advantage of Each Block in try/except/else/finally \n303\n\n\n304 \nChapter 8 Robustness and Performance\nThis layout is especially useful because all of the blocks work together \nin intuitive ways. For example, here I simulate this by running the \ndivide_json function at the same time that my hard drive runs out of \ndisk space:\nwith open(temp_path, 'w') as f:\n    f.write('{\"numerator\": 1, \"denominator\": 10}')\n \ndivide_json(temp_path)\n>>>\n* Opening file\n* Reading data\n* Loading JSON data\n* Performing calculation\n* Writing calculation\n* Calling close()\nTraceback ...\nOSError: [Errno 28] No space left on device\nWhen the exception was raised in the else block while rewriting the \nresult data, the finally block still ran and closed the file handle as \nexpected.\nThings to Remember\n✦ The try/finally compound statement lets you run cleanup code \nregardless of whether exceptions were raised in the try block.\n✦ The else block helps you minimize the amount of code in try blocks \nand visually distinguish the success case from the try/except \nblocks.\n✦ An else block can be used to perform additional actions after a suc-\ncessful try block but before common cleanup in a finally block.\nItem 66:  Consider contextlib and with Statements for \nReusable try/finally Behavior\nThe with statement in Python is used to indicate when code is run-\nning in a special context. For example, mutual-exclusion locks (see \nItem 54: “Use Lock to Prevent Data Races in Threads”) can be used \nin with statements to indicate that the indented code block runs only \nwhile the lock is held:\nfrom threading import Lock\n \nlock = Lock()\n\n\n \nItem 66: Consider contextlib and with Statements \n305\nwith lock:\n    # Do something while maintaining an invariant\n    ...\nThe example above is equivalent to this try/finally construction \nbecause the Lock class properly enables the with statement (see Item \n65: “Take Advantage of Each Block in try/except/else/finally” for \nmore about try/finally):\nlock.acquire()\ntry:\n    # Do something while maintaining an invariant\n    ...\nfinally:\n    lock.release()\nThe with statement version of this is better because it eliminates the \nneed to write the repetitive code of the try/finally construction, and \nit ensures that you don’t forget to have a corresponding release call \nfor every acquire call.\nIt’s easy to make your objects and functions work in with statements \nby using the contextlib built-in module. This module contains the \ncontextmanager decorator (see Item 26: “Define Function Decorators \nwith functools.wraps” for background), which lets a simple function be \nused in with statements. This is much easier than defining a new class \nwith the special methods __enter__ and __exit__ (the standard way).\nFor example, say that I want a region of code to have more debug \nlogging sometimes. Here, I define a function that does logging at two \nseverity levels:\nimport logging\n \ndef my_function():\n    logging.debug('Some debug data')\n    logging.error('Error log here')\n    logging.debug('More debug data')\nThe default log level for my program is WARNING, so only the error mes-\nsage will print to screen when I run the function:\nmy_function()\n>>>\nError log here\n\n\n306 \nChapter 8 Robustness and Performance\nI can elevate the log level of this function temporarily by defining a \ncontext manager. This helper function boosts the logging severity \nlevel before running the code in the with block and reduces the log-\nging severity level afterward:\nfrom contextlib import contextmanager\n \n@contextmanager\ndef debug_logging(level):\n    logger = logging.getLogger()\n    old_level = logger.getEffectiveLevel()\n    logger.setLevel(level)\n    try:\n        yield\n    finally:\n        logger.setLevel(old_level)\nThe yield expression is the point at which the with block’s contents \nwill execute (see Item 30: “Consider Generators Instead of Returning \nLists” for background). Any exceptions that happen in the with block \nwill be re-raised by the yield expression for you to catch in the helper \nfunction (see Item 35: “Avoid Causing State Transitions in Generators \nwith throw” for how that works).\nNow, I can call the same logging function again but in the \ndebug_logging context. This time, all of the debug messages are \nprinted to the screen during the with block. The same function run-\nning outside the with block won’t print debug messages:\nwith debug_logging(logging.DEBUG):\n    print('* Inside:')\n    my_function()\n \nprint('* After:')\nmy_function()\n>>>\n* Inside:\nSome debug data\nError log here\nMore debug data\n* After:\nError log here\nUsing with Targets\nThe context manager passed to a with statement may also return an \nobject. This object is assigned to a local variable in the as part of the \n\n\ncompound statement. This gives the code running in the with block \nthe ability to directly interact with its context. \nFor example, say I want to write a file and ensure that it’s always \nclosed correctly. I can do this by passing open to the with statement. \nopen returns a file handle for the as target of with, and it closes the \nhandle when the with block exits:\nwith open('my_output.txt', 'w') as handle:\n    handle.write('This is some data!')\nThis approach is more Pythonic than manually opening and closing \nthe file handle every time. It gives you confidence that the file is even-\ntually closed when execution leaves the with statement. By highlight-\ning the critical section, it also encourages you to reduce the amount \nof code that executes while the file handle is open, which is good \npractice in general.\nTo enable your own functions to supply values for as targets, all you \nneed to do is yield a value from your context manager. For example, \nhere I define a context manager to fetch a Logger instance, set its \nlevel, and then yield it as the target:\n@contextmanager\ndef log_level(level, name):\n    logger = logging.getLogger(name)\n    old_level = logger.getEffectiveLevel()\n    logger.setLevel(level)\n    try:\n        yield logger\n    finally:\n        logger.setLevel(old_level)\nCalling logging methods like debug on the as target produces output \nbecause the logging severity level is set low enough in the with block \non that specific Logger instance. Using the logging module directly \nwon’t print anything because the default logging severity level for the \ndefault program logger is WARNING:\nwith log_level(logging.DEBUG, 'my-log') as logger:\n    logger.debug(f'This is a message for {logger.name}!')\n    logging.debug('This will not print')\n>>>\nThis is a message for my-log!\nAfter the with statement exits, calling debug logging methods on the \nLogger named 'my-log' will not print anything because the default \n \nItem 66: Consider contextlib and with Statements \n307\n\n\n308 \nChapter 8 Robustness and Performance\nlogging severity level has been restored. Error log messages will \nalways print:\nlogger = logging.getLogger('my-log')\nlogger.debug('Debug will not print')\nlogger.error('Error will print')\n>>>\nError will print\nLater, I can change the name of the logger I want to use by simply \nupdating the with statement. This will point the Logger that’s the as \ntarget in the with block to a different instance, but I won’t have to \nupdate any of my other code to match:\nwith log_level(logging.DEBUG, 'other-log') as logger:\n    logger.debug(f'This is a message for {logger.name}!')\n    logging.debug('This will not print')\n>>>\nThis is a message for other-log!\nThis isolation of state and decoupling between creating a context and \nacting within that context is another benefit of the with statement.\nThings to Remember\n✦ The with statement allows you to reuse logic from try/finally blocks \nand reduce visual noise.\n✦ The contextlib built-in module provides a contextmanager decorator \nthat makes it easy to use your own functions in with statements.\n✦ The value yielded by context managers is supplied to the as part \nof the with statement. It’s useful for letting your code directly access \nthe cause of a special context.\nItem 67: Use datetime Instead of time for Local Clocks\nCoordinated Universal Time (UTC) is the standard, time-zone- \nindependent representation of time. UTC works great for computers \nthat represent time as seconds since the UNIX epoch. But UTC isn’t \nideal for humans. Humans reference time relative to where they’re \ncurrently located. People say “noon” or “8 am” instead of “UTC 15:00 \nminus 7 hours.” If your program handles time, you’ll probably find \nyourself converting time between UTC and local clocks for the sake of \nhuman understanding.\n\n\n \nItem 67: Use datetime Instead of time for Local Clocks \n309\nPython provides two ways of accomplishing time zone conversions. \nThe old way, using the time built-in module, is terribly error prone. \nThe new way, using the datetime built-in module, works great with \nsome help from the community-built package named pytz.\nYou should be acquainted with both time and datetime to thoroughly \nunderstand why datetime is the best choice and time should be \navoided.\nThe time Module\nThe localtime function from the time built-in module lets you convert \na UNIX timestamp (seconds since the UNIX epoch in UTC) to a local \ntime that matches the host computer’s time zone (Pacific Daylight \nTime in my case). This local time can be printed in human-readable \nformat using the strftime function:\nimport time\n \nnow = 1552774475\nlocal_tuple = time.localtime(now)\ntime_format = '%Y-%m-%d %H:%M:%S'\ntime_str = time.strftime(time_format, local_tuple)\nprint(time_str)\n>>>\n2019-03-16 15:14:35\nYou’ll often need to go the other way as well, starting with user input \nin human-readable local time and converting it to UTC time. You can \ndo this by using the strptime function to parse the time string, and \nthen calling mktime to convert local time to a UNIX timestamp:\ntime_tuple = time.strptime(time_str, time_format)\nutc_now = time.mktime(time_tuple)\nprint(utc_now)\n>>>\n1552774475.0\nHow do you convert local time in one time zone to local time in \nanother time zone? For example, say that I’m taking a flight between \nSan Francisco and New York, and I want to know what time it will be \nin San Francisco when I’ve arrived in New York.\nI might initially assume that I can directly manipulate the return val-\nues from the time, localtime, and strptime functions to do time zone \nconversions. But this is a very bad idea. Time zones change all the time \ndue to local laws. It’s too complicated to manage yourself, especially if \nyou want to handle every global city for flight departures and arrivals.\n\n\n310 \nChapter 8 Robustness and Performance\nMany operating systems have configuration files that keep up with \nthe time zone changes automatically. Python lets you use these time \nzones through the time module if your platform supports it. On other \nplatforms, such as Windows, some time zone functionality isn’t avail-\nable from time at all. For example, here I parse a departure time from \nthe San Francisco time zone, Pacific Daylight Time (PDT):\nimport os\n \nif os.name == 'nt':\n    print(\"This example doesn't work on Windows\")\nelse:\n    parse_format = '%Y-%m-%d %H:%M:%S %Z'\n    depart_sfo = '2019-03-16 15:45:16 PDT'\n    time_tuple = time.strptime(depart_sfo, parse_format)\n    time_str = time.strftime(time_format, time_tuple)\n    print(time_str)\n>>>\n2019-03-16 15:45:16\nAfter seeing that 'PDT' works with the strptime function, I might also \nassume that other time zones known to my computer will work. Unfor-\ntunately, this isn’t the case. strptime raises an exception when it sees \nEastern Daylight Time (EDT), which is the time zone for New York:\narrival_nyc = '2019-03-16 23:33:24 EDT'\ntime_tuple = time.strptime(arrival_nyc, time_format)\n>>>\nTraceback ...\nValueError: unconverted data remains:  EDT\nThe problem here is the platform-dependent nature of the time mod-\nule. Its behavior is determined by how the underlying C functions \nwork with the host operating system. This makes the functionality of \nthe time module unreliable in Python. The time module fails to consis-\ntently work properly for multiple local times. Thus, you should avoid \nusing the time module for this purpose. If you must use time, use it \nonly to convert between UTC and the host computer’s local time. For \nall other types of conversions, use the datetime module.\nThe datetime Module\nThe second option for representing times in Python is the datetime \nclass from the datetime built-in module. Like the time module, \ndatetime can be used to convert from the current time in UTC to local \ntime.\n\n\nHere, I convert the present time in UTC to my computer’s local time, \nPDT:\nfrom datetime import datetime, timezone\n \nnow = datetime(2019, 3, 16, 22, 14, 35)\nnow_utc = now.replace(tzinfo=timezone.utc)\nnow_local = now_utc.astimezone()\nprint(now_local)\n>>>\n2019-03-16 15:14:35-07:00\nThe datetime module can also easily convert a local time back to a \nUNIX timestamp in UTC:\ntime_str = '2019-03-16 15:14:35'\nnow = datetime.strptime(time_str, time_format)\ntime_tuple = now.timetuple()\nutc_now = time.mktime(time_tuple)\nprint(utc_now)\n>>>\n1552774475.0\nUnlike the time module, the datetime module has facilities for reli-\nably converting from one local time to another local time. However, \ndatetime only provides the machinery for time zone operations with \nits tzinfo class and related methods. The Python default installation \nis missing time zone definitions besides UTC.\nLuckily, the Python community has addressed this gap with the pytz \nmodule that’s available for download from the Python Package Index \n(see Item 82: “Know Where to Find Community-Built Modules” for \nhow to install it). pytz contains a full database of every time zone \ndefinition you might need.\nTo use pytz effectively, you should always convert local times to UTC \nfirst. Perform any datetime operations you need on the UTC values \n(such as offsetting). Then, convert to local times as a final step.\nFor example, here I convert a New York City flight arrival time to a \nUTC datetime. Although some of these calls seem redundant, all of \nthem are necessary when using pytz:\nimport pytz\n \narrival_nyc = '2019-03-16 23:33:24'\nnyc_dt_naive = datetime.strptime(arrival_nyc, time_format)\n \nItem 67: Use datetime Instead of time for Local Clocks \n311\n\n\n312 \nChapter 8 Robustness and Performance\neastern = pytz.timezone('US/Eastern')\nnyc_dt = eastern.localize(nyc_dt_naive)\nutc_dt = pytz.utc.normalize(nyc_dt.astimezone(pytz.utc))\nprint(utc_dt)\n>>>\n2019-03-17 03:33:24+00:00\nOnce I have a UTC datetime, I can convert it to San Francisco local \ntime:\npacific = pytz.timezone('US/Pacific')\nsf_dt = pacific.normalize(utc_dt.astimezone(pacific))\nprint(sf_dt)\n>>>\n2019-03-16 20:33:24-07:00\nJust as easily, I can convert it to the local time in Nepal:\nnepal = pytz.timezone('Asia/Katmandu')\nnepal_dt = nepal.normalize(utc_dt.astimezone(nepal))\nprint(nepal_dt)\n>>>\n2019-03-17 09:18:24+05:45\nWith datetime and pytz, these conversions are consistent across all \nenvironments, regardless of what operating system the host computer \nis running.\nThings to Remember\n✦ Avoid using the time module for translating between different time \nzones.\n✦ Use the datetime built-in module along with the pytz community \nmodule to reliably convert between times in different time zones.\n✦ Always represent time in UTC and do conversions to local time as \nthe very final step before presentation.\nItem 68: Make pickle Reliable with copyreg\nThe pickle built-in module can serialize Python objects into a stream \nof bytes and deserialize bytes back into objects. Pickled byte streams \nshouldn’t be used to communicate between untrusted parties. The \npurpose of pickle is to let you pass Python objects between programs \nthat you control over binary channels.\n\n\n \nItem 68: Make pickle Reliable with copyreg \n313\nNote\nThe pickle module’s serialization format is unsafe by design. The serialized data \ncontains what is essentially a program that describes how to reconstruct the \noriginal Python object. This means a malicious pickle payload could be used to \ncompromise any part of a Python program that attempts to  deserialize it.\nIn contrast, the json module is safe by design. Serialized JSON data contains \na simple description of an object hierarchy. Deserializing JSON data does \nnot expose a Python program to additional risk. Formats like JSON should \nbe used for communication between programs or people who don’t trust \neach other.\nFor example, say that I want to use a Python object to represent the \nstate of a player’s progress in a game. The game state includes the \nlevel the player is on and the number of lives they have remaining:\nclass GameState:\n    def __init__(self):\n        self.level = 0\n        self.lives = 4\nThe program modifies this object as the game runs:\nstate = GameState()\nstate.level += 1  # Player beat a level\nstate.lives -= 1  # Player had to try again\n \nprint(state.__dict__)\n>>>\n{'level': 1, 'lives': 3}\nWhen the user quits playing, the program can save the state of the \ngame to a file so it can be resumed at a later time. The pickle mod-\nule makes it easy to do this. Here, I use the dump function to write \nthe GameState object to a file:\nimport pickle\n \nstate_path = 'game_state.bin'\nwith open(state_path, 'wb') as f:\n    pickle.dump(state, f)\nLater, I can call the load function with the file and get back the \nGameState object as if it had never been serialized:\nwith open(state_path, 'rb') as f:\n    state_after = pickle.load(f)\n \nprint(state_after.__dict__)\n",
      "page_number": 324,
      "chapter_number": 31,
      "summary": "This chapter covers segment 31 (pages 324-335). Key topics include time, printed, and function. The exception is caught by the except block and \nhandled:\nload_json_key('{\"foo\": bad payload', 'foo')\n>>>\n* Loading JSON data\n* Handling ValueError\nTraceback.",
      "keywords": [
        "Loading JSON data",
        "time",
        "Loading JSON",
        "local time",
        "JSON data",
        "JSON",
        "Block",
        "UTC",
        "time zone",
        "data",
        "time Module",
        "local",
        "module",
        "Item",
        "finally"
      ],
      "concepts": [
        "time",
        "printed",
        "function",
        "functions",
        "functionality",
        "data",
        "utc",
        "logging",
        "log",
        "handling"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 7,
          "title": "Segment 7 (pages 51-58)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 15,
          "title": "[ 503 ]",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 21,
          "title": "Segment 21 (pages 167-174)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 36,
          "title": "Segment 36 (pages 718-737)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 9,
          "title": "Segment 9 (pages 76-97)",
          "relevance_score": 0.45,
          "method": "api"
        }
      ]
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 336-344)",
      "start_page": 336,
      "end_page": 344,
      "detection_method": "topic_boundary",
      "content": "314 \nChapter 8 Robustness and Performance\n>>>\n{'level': 1, 'lives': 3}\nThe problem with this approach is what happens as the game’s fea-\ntures expand over time. Imagine that I want the player to earn points \ntoward a high score. To track the player’s points, I’d add a new field to \nthe GameState class\nclass GameState:\n    def __init__(self):\n        self.level = 0\n        self.lives = 4\n        self.points = 0  # New field\nSerializing the new version of the GameState class using pickle will \nwork exactly as before. Here, I simulate the round-trip through a file \nby serializing to a string with dumps and back to an object with loads:\nstate = GameState()\nserialized = pickle.dumps(state)\nstate_after = pickle.loads(serialized)\nprint(state_after.__dict__)\n>>>\n{'level': 0, 'lives': 4, 'points': 0}\nBut what happens to older saved GameState objects that the user may \nwant to resume? Here, I unpickle an old game file by using a program \nwith the new definition of the GameState class:\nwith open(state_path, 'rb') as f:\n    state_after = pickle.load(f)\n \nprint(state_after.__dict__)\n>>>\n{'level': 1, 'lives': 3}\nThe points attribute is missing! This is especially confusing because \nthe returned object is an instance of the new GameState class:\nassert isinstance(state_after, GameState)\nThis behavior is a byproduct of the way the pickle module works. Its \nprimary use case is making object serialization easy. As soon as your \nuse of pickle moves beyond trivial usage, the module’s functionality \nstarts to break down in surprising ways.\nFixing these problems is straightforward using the copyreg built-in \nmodule. The copyreg module lets you register the functions responsible \n\n\nfor serializing and deserializing Python objects, allowing you to con-\ntrol the behavior of pickle and make it more reliable.\nDefault Attribute Values\nIn the simplest case, you can use a constructor with default  arguments \n(see Item 23: “Provide Optional Behavior with Keyword Arguments” \nfor background) to ensure that GameState objects will always have all \nattributes after unpickling. Here, I redefine the constructor this way:\nclass GameState:\n    def __init__(self, level=0, lives=4, points=0):\n        self.level = level\n        self.lives = lives\n        self.points = points\nTo use this constructor for pickling, I define a helper function that \ntakes a GameState object and turns it into a tuple of parameters for \nthe copyreg module. The returned tuple contains the function to use \nfor unpickling and the parameters to pass to the unpickling function:\ndef pickle_game_state(game_state):\n    kwargs = game_state.__dict__\n    return unpickle_game_state, (kwargs,)\nNow, I need to define the unpickle_game_state helper. This func-\ntion takes serialized data and parameters from pickle_game_state \nand returns the corresponding GameState object. It’s a tiny wrapper \naround the constructor:\ndef unpickle_game_state(kwargs):\n    return GameState(**kwargs)\nNow, I register these functions with the copyreg built-in module:\nimport copyreg\n \ncopyreg.pickle(GameState, pickle_game_state)\nAfter registration, serializing and deserializing works as before:\nstate = GameState()\nstate.points += 1000\nserialized = pickle.dumps(state)\nstate_after = pickle.loads(serialized)\nprint(state_after.__dict__)\n>>>\n{'level': 0, 'lives': 4, 'points': 1000}\n \nItem 68: Make pickle Reliable with copyreg \n315\n\n\n316 \nChapter 8 Robustness and Performance\nWith this registration done, now I’ll change the definition of GameState \nagain to give the player a count of magic spells to use. This change is \nsimilar to when I added the points field to GameState:\nclass GameState:\n    def __init__(self, level=0, lives=4, points=0, magic=5):\n        self.level = level\n        self.lives = lives\n        self.points = points\n        self.magic = magic  # New field\nBut unlike before, deserializing an old GameState object will result in \nvalid game data instead of missing attributes. This works because \nunpickle_game_state calls the GameState constructor directly instead \nof using the pickle module’s default behavior of saving and restor-\ning only the attributes that belong to an object. The GameState con-\nstructor’s keyword arguments have default values that will be used \nfor any parameters that are missing. This causes old game state \nfiles to receive the default value for the new magic field when they are \ndeserialized:\nprint('Before:', state.__dict__)\nstate_after = pickle.loads(serialized)\nprint('After: ', state_after.__dict__)\n>>>\nBefore: {'level': 0, 'lives': 4, 'points': 1000}\nAfter:  {'level': 0, 'lives': 4, 'points': 1000, 'magic': 5}\nVersioning Classes\nSometimes you need to make backward-incompatible changes to your \nPython objects by removing fields. Doing so prevents the default argu-\nment approach above from working.\nFor example, say I realize that a limited number of lives is a bad idea, \nand I want to remove the concept of lives from the game. Here, I rede-\nfine the GameState class to no longer have a lives field:\nclass GameState:\n    def __init__(self, level=0, points=0, magic=5):\n        self.level = level\n        self.points = points\n        self.magic = magic\nThe problem is that this breaks deserialization of old game data. \nAll fields from the old data, even ones removed from the class, will \nbe passed to the GameState constructor by the unpickle_game_state \nfunction:\npickle.loads(serialized)\n\n\n>>>\nTraceback ...\nTypeError: __init__() got an unexpected keyword argument \n¯'lives'\nI can fix this by adding a version parameter to the functions supplied \nto copyreg. New serialized data will have a version of 2 specified when \npickling a new GameState object:\ndef pickle_game_state(game_state):\n    kwargs = game_state.__dict__\n    kwargs['version'] = 2\n    return unpickle_game_state, (kwargs,)\nOld versions of the data will not have a version argument present, \nwhich means I can manipulate the arguments passed to the GameState \nconstructor accordingly:\ndef unpickle_game_state(kwargs):\n    version = kwargs.pop('version', 1)\n    if version == 1:\n        del kwargs['lives']\n    return GameState(**kwargs)\nNow, deserializing an old object works properly:\ncopyreg.pickle(GameState, pickle_game_state)\nprint('Before:', state.__dict__)\nstate_after = pickle.loads(serialized)\nprint('After: ', state_after.__dict__)\n>>>\nBefore: {'level': 0, 'lives': 4, 'points': 1000}\nAfter:  {'level': 0, 'points': 1000, 'magic': 5}\nI can continue using this approach to handle changes between \nfuture versions of the same class. Any logic I need to adapt an \nold version of the class to a new version of the class can go in the \nunpickle_game_state function.\nStable Import Paths\nOne other issue you may encounter with pickle is breakage from \nrenaming a class. Often over the life cycle of a program, you’ll refac-\ntor your code by renaming classes and moving them to other mod-\nules. Unfortunately, doing so breaks the pickle module unless you’re \ncareful.\n \nItem 68: Make pickle Reliable with copyreg \n317\n\n\n318 \nChapter 8 Robustness and Performance\nHere, I rename the GameState class to BetterGameState and remove \nthe old class from the program entirely:\nclass BetterGameState:\n    def __init__(self, level=0, points=0, magic=5):\n        self.level = level\n        self.points = points\n        self.magic = magic\nAttempting to deserialize an old GameState object now fails because \nthe class can’t be found:\npickle.loads(serialized)\n>>>\nTraceback ...\nAttributeError: Can't get attribute 'GameState' on <module \n¯'__main__' from 'my_code.py'>\nThe cause of this exception is that the import path of the serialized \nobject’s class is encoded in the pickled data:\nprint(serialized)\n>>>\nb'\\x80\\x04\\x95A\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08__main__\n¯\\x94\\x8c\\tGameState\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x05level\n¯\\x94K\\x00\\x8c\\x06points\\x94K\\x00\\x8c\\x05magic\\x94K\\x05ub.'\nThe solution is to use copyreg again. I can specify a stable identifier \nfor the function to use for unpickling an object. This allows me to \ntransition pickled data to different classes with different names when \nit’s deserialized. It gives me a level of indirection:\ncopyreg.pickle(BetterGameState, pickle_game_state)\nAfter I use copyreg, you can see that the import path to \nunpickle_game_state is encoded in the serialized data instead of \nBetterGameState:\nstate = BetterGameState()\nserialized = pickle.dumps(state)\nprint(serialized)\n>>>\nb'\\x80\\x04\\x95W\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08__main__\n¯\\x94\\x8c\\x13unpickle_game_state\\x94\\x93\\x94}\\x94(\\x8c\n¯\\x05level\\x94K\\x00\\x8c\\x06points\\x94K\\x00\\x8c\\x05magic\\x94K\n¯\\x05\\x8c\\x07version\\x94K\\x02u\\x85\\x94R\\x94.'\n\n\n \nItem 69: Use decimal When Precision Is Paramount \n319\nThe only gotcha is that I can’t change the path of the module in \nwhich the unpickle_game_state function is present. Once I serialize \ndata with a function, it must remain available on that import path for \ndeserialization in the future.\nThings to Remember\n✦ The pickle built-in module is useful only for serializing and deseri-\nalizing objects between trusted programs.\n✦ Deserializing previously pickled objects may break if the classes \ninvolved have changed over time (e.g., attributes have been added \nor removed).\n✦ Use the copyreg built-in module with pickle to ensure backward \ncompatibility for serialized objects.\nItem 69: Use decimal When Precision Is Paramount\nPython is an excellent language for writing code that interacts with \nnumerical data. Python’s integer type can represent values of any \npractical size. Its double-precision floating point type complies with \nthe IEEE 754 standard. The language also provides a standard com-\nplex number type for imaginary values. However, these aren’t enough \nfor every situation.\nFor example, say that I want to compute the amount to charge a cus-\ntomer for an international phone call. I know the time in minutes \nand seconds that the customer was on the phone (say, 3 minutes \n42  seconds). I also have a set rate for the cost of calling Antarctica \nfrom the United States ($1.45/minute). What should the charge be?\nWith floating point math, the computed charge seems reasonable\nrate = 1.45\nseconds = 3*60 + 42\ncost = rate * seconds / 60\nprint(cost)\n>>>\n5.364999999999999\nThe result is 0.0001 short of the correct value (5.365) due to how IEEE \n754 floating point numbers are represented. I might want to round up \nthis value to 5.37 to properly cover all costs incurred by the customer. \nHowever, due to floating point error, rounding to the nearest whole \ncent actually reduces the final charge (from 5.364 to 5.36) instead of \nincreasing it (from 5.365 to 5.37):\nprint(round(cost, 2))\n\n\n320 \nChapter 8 Robustness and Performance\n>>>\n5.36\nThe solution is to use the Decimal class from the decimal built-in mod-\nule. The Decimal class provides fixed point math of 28 decimal places \nby default. It can go even higher, if required. This works around the \nprecision issues in IEEE 754 floating point numbers. The class also \ngives you more control over rounding behaviors.\nFor example, redoing the Antarctica calculation with Decimal results \nin the exact expected charge instead of an approximation:\nfrom decimal import Decimal\n \nrate = Decimal('1.45')\nseconds = Decimal(3*60 + 42)\ncost = rate * seconds / Decimal(60)\nprint(cost)\n>>>\n5.365\nDecimal instances can be given starting values in two different ways. \nThe first way is by passing a str containing the number to the Decimal \nconstructor. This ensures that there is no loss of precision due to the \ninherent nature of Python floating point numbers. The second way \nis by directly passing a float or an int instance to the constructor. \nHere, you can see that the two construction methods result in differ-\nent behavior.\nprint(Decimal('1.45'))\nprint(Decimal(1.45))\n>>>\n1.45\n1.4499999999999999555910790149937383830547332763671875\nThe same problem doesn’t happen if I supply integers to the Decimal \nconstructor:\nprint('456')\nprint(456)\n>>>\n456\n456\nIf you care about exact answers, err on the side of caution and use \nthe str constructor for the Decimal type.\n\n\nGetting back to the phone call example, say that I also want to sup-\nport very short phone calls between places that are much cheaper \nto connect (like Toledo and Detroit). Here, I compute the charge for a \nphone call that was 5 seconds long with a rate of $0.05/minute:\nrate = Decimal('0.05')\nseconds = Decimal('5')\nsmall_cost = rate * seconds / Decimal(60)\nprint(small_cost)\n>>>\n0.004166666666666666666666666667\nThe result is so low that it is decreased to zero when I try to round it \nto the nearest whole cent. This won’t do!\nprint(round(small_cost, 2))\n>>>\n0.00\nLuckily, the Decimal class has a built-in function for rounding to \nexactly the decimal place needed with the desired rounding behavior. \nThis works for the higher cost case from earlier:\nfrom decimal import ROUND_UP\n \nrounded = cost.quantize(Decimal('0.01'), rounding=ROUND_UP)\nprint(f'Rounded {cost} to {rounded}')\n>>>\nRounded 5.365 to 5.37\nUsing the quantize method this way also properly handles the small \nusage case for short, cheap phone calls:.\nrounded = small_cost.quantize(Decimal('0.01'),\n                              rounding=ROUND_UP)\nprint(f'Rounded {small_cost} to {rounded}')\n>>>\nRounded 0.004166666666666666666666666667 to 0.01\nWhile Decimal works great for fixed point numbers, it still has limita-\ntions in its precision (e.g., 1/3 will be an approximation). For repre-\nsenting rational numbers with no limit to precision, consider using \nthe Fraction class from the fractions built-in module.\n \nItem 69: Use decimal When Precision Is Paramount \n321\n\n\n322 \nChapter 8 Robustness and Performance\nThings to Remember\n✦ Python has built-in types and classes in modules that can repre-\nsent practically every type of numerical value.\n✦ The Decimal class is ideal for situations that require high precision \nand control over rounding behavior, such as computations of mon-\netary values.\n✦ Pass str instances to the Decimal constructor instead of float \ninstances if it’s important to compute exact answers and not float-\ning point approximations.\nItem 70: Profile Before Optimizing\nThe dynamic nature of Python causes surprising behaviors in its run-\ntime performance. Operations you might assume would be slow are \nactually very fast (e.g., string manipulation, generators). Language \nfeatures you might assume would be fast are actually very slow (e.g., \nattribute accesses, function calls). The true source of slowdowns in a \nPython program can be obscure.\nThe best approach is to ignore your intuition and directly measure \nthe performance of a program before you try to optimize it. Python \nprovides a built-in profiler for determining which parts of a program \nare responsible for its execution time. This means you can focus your \noptimization efforts on the biggest sources of trouble and ignore parts \nof the program that don’t impact speed (i.e., follow Amdahl’s law).\nFor example, say that I want to determine why an algorithm in a pro-\ngram is slow. Here, I define a function that sorts a list of data using \nan insertion sort:\ndef insertion_sort(data):\n    result = []\n    for value in data:\n        insert_value(result, value)\n    return result\nThe core mechanism of the insertion sort is the function that finds \nthe insertion point for each piece of data. Here, I define an extremely \ninefficient version of the insert_value function that does a linear scan \nover the input array:\ndef insert_value(array, value):\n    for i, existing in enumerate(array):\n        if existing > value:\n            array.insert(i, value)\n",
      "page_number": 336,
      "chapter_number": 32,
      "summary": "This chapter covers segment 32 (pages 336-344). Key topics include pickle, pickling, and serializing. The serialized data \ncontains what is essentially a program that describes how to reconstruct the \noriginal Python object.",
      "keywords": [
        "state",
        "decimal",
        "GameState",
        "game",
        "GameState object",
        "pickle",
        "GameState class",
        "object",
        "points",
        "level",
        "serialized",
        "lives",
        "Decimal class",
        "module",
        "Python"
      ],
      "concepts": [
        "pickle",
        "pickling",
        "serializing",
        "serialize",
        "decimal",
        "points",
        "state",
        "classes",
        "object",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 20,
          "title": "Segment 20 (pages 168-176)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 26,
          "title": "Segment 26 (pages 238-247)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 345-356)",
      "start_page": 345,
      "end_page": 356,
      "detection_method": "topic_boundary",
      "content": " \nItem 70: Profile Before Optimizing \n323\n            return\n    array.append(value)\nTo profile insertion_sort and insert_value, I create a data set of ran-\ndom numbers and define a test function to pass to the profiler:\nfrom random import randint\n \nmax_size = 10**4\ndata = [randint(0, max_size) for _ in range(max_size)]\ntest = lambda: insertion_sort(data)\nPython provides two built-in profilers: one that is pure Python \n(profile) and another that is a C-extension module (cProfile). The \ncProfile built-in module is better because of its minimal impact on \nthe performance of your program while it’s being profiled. The pure- \nPython alternative imposes a high overhead that skews the results.\nNote\nWhen profiling a Python program, be sure that what you’re measuring is the \ncode itself and not external systems. Beware of functions that access the net-\nwork or resources on disk. These may appear to have a large impact on your \nprogram’s execution time because of the slowness of the underlying systems. \nIf your program uses a cache to mask the latency of slow resources like these, \nyou should ensure that it’s properly warmed up before you start profiling.\nHere, I instantiate a Profile object from the cProfile module and run \nthe test function through it using the runcall method:\nfrom cProfile import Profile\n \nprofiler = Profile()\nprofiler.runcall(test)\nWhen the test function has finished running, I can extract statistics \nabout its performance by using the pstats built-in module and its \nStats class. Various methods on a Stats object adjust how to select \nand sort the profiling information to show only the things I care \nabout:\nfrom pstats import Stats\n \nstats = Stats(profiler)\nstats.strip_dirs()\nstats.sort_stats('cumulative')\nstats.print_stats()\n\n\n324 \nChapter 8 Robustness and Performance\nThe output is a table of information organized by function. The data \nsample is taken only from the time the profiler was active, during the \nruncall method above:\n>>>\n         20003 function calls in 1.320 seconds\n \n   Ordered by: cumulative time\n \n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.000    0.000    1.320    1.320 main.py:35(<lambda>)\n        1    0.003    0.003    1.320    1.320 main.py:10(insertion_sort)\n    10000    1.306    0.000    1.317    0.000 main.py:20(insert_value)\n     9992    0.011    0.000    0.011    0.000 {method 'insert' of 'list' objects}\n        8    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\nHere’s a quick guide to what the profiler statistics columns mean:\n \n■ncalls: The number of calls to the function during the profiling \nperiod.\n \n■tottime: The number of seconds spent executing the function, \nexcluding time spent executing other functions it calls.\n \n■tottime percall: The average number of seconds spent in the \nfunction each time it is called, excluding time spent executing \nother functions it calls. This is tottime divided by ncalls.\n \n■cumtime: The cumulative number of seconds spent executing the \nfunction, including time spent in all other functions it calls.\n \n■cumtime percall: The average number of seconds spent in the \nfunction each time it is called, including time spent in all other \nfunctions it calls. This is cumtime divided by ncalls.\nLooking at the profiler statistics table above, I can see that the biggest \nuse of CPU in my test is the cumulative time spent in the insert_value \nfunction. Here, I redefine that function to use the bisect built-in mod-\nule (see Item 72: “Consider Searching Sorted Sequences with bisect”):\nfrom bisect import bisect_left\n \ndef insert_value(array, value):\n    i = bisect_left(array, value)\n    array.insert(i, value)\nI can run the profiler again and generate a new table of profiler sta-\ntistics. The new function is much faster, with a cumulative time spent \nthat is nearly 100 times smaller than with the previous insert_value \nfunction:\n\n\n \nItem 70: Profile Before Optimizing \n325\n>>>\n         30003 function calls in 0.017 seconds\n \n   Ordered by: cumulative time\n \n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.000    0.000    0.017    0.017 main.py:35(<lambda>)\n        1    0.002    0.002    0.017    0.017 main.py:10(insertion_sort)\n    10000    0.003    0.000    0.015    0.000 main.py:110(insert_value)\n    10000    0.008    0.000    0.008    0.000 {method 'insert' of 'list' objects}\n    10000    0.004    0.000    0.004    0.000 {built-in method _bisect.bisect_left}\nSometimes when you’re profiling an entire program, you might find \nthat a common utility function is responsible for the majority of exe-\ncution time. The default output from the profiler makes such a situ-\nation difficult to understand because it doesn’t show that the utility \nfunction is called by many different parts of your program.\nFor example, here the my_utility function is called repeatedly by two \ndifferent functions in the program:\ndef my_utility(a, b):\n    c = 1\n    for i in range(100):\n        c += a * b\n \ndef first_func():\n    for _ in range(1000):\n        my_utility(4, 5)\n \ndef second_func():\n    for _ in range(10):\n        my_utility(1, 3)\n \ndef my_program():\n    for _ in range(20):\n        first_func()\n        second_func()\nProfiling this code and using the default print_stats output gener-\nates statistics that are confusing:\n>>>\n         20242 function calls in 0.118 seconds\n \n   Ordered by: cumulative time\n \n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.000    0.000    0.118    0.118 main.py:176(my_program)\n       20    0.003    0.000    0.117    0.006 main.py:168(first_func)\n    20200    0.115    0.000    0.115    0.000 main.py:161(my_utility)\n       20    0.000    0.000    0.001    0.000 main.py:172(second_func)\n\n\n326 \nChapter 8 Robustness and Performance\nThe my_utility function is clearly the source of most execution time, \nbut it’s not immediately obvious why that function is called so much. \nIf you search through the program’s code, you’ll find multiple call \nsites for my_utility and still be confused.\nTo deal with this, the Python profiler provides the print_callers \nmethod to show which callers contributed to the profiling information \nof each function:\nstats.print_callers()\nThis profiler statistics table shows functions called on the left and \nwhich function was responsible for making the call on the right. Here, \nit’s clear that my_utility is most used by first_func:\n>>>\n   Ordered by: cumulative time\n \nFunction                                was called by...\n                                            ncalls  tottime  cumtime\nmain.py:176(my_program)                 <- \nmain.py:168(first_func)                 <-      20    0.003    0.117  main.py:176(my_program)\nmain.py:161(my_utility)                 <-   20000    0.114    0.114  main.py:168(first_func)\n                                               200    0.001    0.001  main.py:172(second_func)\nProfiling.md:172(second_func)           <-      20    0.000    0.001  main.py:176(my_program)\nThings to Remember\n✦ It’s important to profile Python programs before optimizing because \nthe sources of slowdowns are often obscure.\n✦ Use the cProfile module instead of the profile module because it \nprovides more accurate profiling information.\n✦ The Profile object’s runcall method provides everything you need \nto profile a tree of function calls in isolation.\n✦ The Stats object lets you select and print the subset of profil-\ning information you need to see to understand your program’s \nperformance.\nItem 71: Prefer deque for Producer–Consumer Queues\nA common need in writing programs is a first-in, first-out (FIFO) \nqueue, which is also known as a producer–consumer queue. A FIFO \nqueue is used when one function gathers values to process and \nanother function handles them in the order in which they were \nreceived. Often, programmers use Python’s built-in list type as a \nFIFO queue.\n\n\n \nItem 71: Prefer deque for Producer–Consumer Queues \n327\nFor example, say that I have a program that’s processing incoming \nemails for long-term archival, and it’s using a list for a producer–\nconsumer queue. Here, I define a class to represent the messages:\nclass Email:\n    def __init__(self, sender, receiver, message):\n        self.sender = sender\n        self.receiver = receiver\n        self.message = message\n    ...\nI also define a placeholder function for receiving a single email, pre-\nsumably from a socket, the file system, or some other type of I/O \nsystem. The implementation of this function doesn’t matter; what’s \nimportant is its interface: It will either return an Email instance or \nraise a NoEmailError exception:\nclass NoEmailError(Exception):\n    pass\n \ndef try_receive_email():\n    # Returns an Email instance or raises NoEmailError\n    ...\nThe producing function receives emails and enqueues them to be con-\nsumed at a later time. This function uses the append method on the \nlist to add new messages to the end of the queue so they are pro-\ncessed after all messages that were previously received:\ndef produce_emails(queue):\n    while True:\n        try:\n            email = try_receive_email()\n        except NoEmailError:\n            return\n        else:\n            queue.append(email)  # Producer\nThe consuming function does something useful with the emails. This \nfunction calls pop(0) on the queue, which removes the very first item \nfrom the list and returns it to the caller. By always processing items \nfrom the beginning of the queue, the consumer ensures that the items \nare processed in the order in which they were received:\ndef consume_one_email(queue):\n    if not queue:\n        return\n    email = queue.pop(0)  # Consumer\n\n\n328 \nChapter 8 Robustness and Performance\n    # Index the message for long-term archival\n    ...\nFinally, I need a looping function that connects the pieces together. \nThis function alternates between producing and consuming until the \nkeep_running function returns False (see Item 60: “Achieve Highly \nConcurrent I/O with Coroutines” on how to do this concurrently):\ndef loop(queue, keep_running):\n    while keep_running():\n        produce_emails(queue)\n        consume_one_email(queue)\n \ndef my_end_func():\n    ...\n \nloop([], my_end_func)\nWhy not process each Email message in produce_emails as it’s returned \nby try_receive_email? It comes down to the trade-off between latency \nand throughput. When using producer–consumer queues, you often \nwant to minimize the latency of accepting new items so they can be \ncollected as fast as possible. The consumer can then process through \nthe backlog of items at a consistent pace—one item per loop in this \ncase—which provides a stable performance profile and consistent \nthroughput at the cost of end-to-end latency (see Item 55: “Use Queue \nto Coordinate Work Between Threads” for related best practices).\nUsing a list for a producer–consumer queue like this works fine up \nto a point, but as the cardinality—the number of items in the list—\nincreases, the list type’s performance can degrade superlinearly. \nTo analyze the performance of using list as a FIFO queue, I can \nrun some micro-benchmarks using the timeit built-in module. Here, \nI define a benchmark for the performance of adding new items to the \nqueue using the append method of list (matching the producer func-\ntion’s usage):\nimport timeit\n \ndef print_results(count, tests):\n    avg_iteration = sum(tests) / len(tests)\n    print(f'Count {count:>5,} takes {avg_iteration:.6f}s')\n    return count, avg_iteration\n \ndef list_append_benchmark(count):\n    def run(queue):\n\n\n        for i in range(count):\n            queue.append(i)\n \n    tests = timeit.repeat(\n        setup='queue = []',\n        stmt='run(queue)',\n        globals=locals(),\n        repeat=1000,\n        number=1)\n \n    return print_results(count, tests)\nRunning this benchmark function with different levels of cardinality \nlets me compare its performance in relationship to data size:\ndef print_delta(before, after):\n    before_count, before_time = before\n    after_count, after_time = after\n    growth = 1 + (after_count - before_count) / before_count\n    slowdown = 1 + (after_time - before_time) / before_time\n    print(f'{growth:>4.1f}x data size, {slowdown:>4.1f}x time')\n \nbaseline = list_append_benchmark(500)\nfor count in (1_000, 2_000, 3_000, 4_000, 5_000):\n    comparison = list_append_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.000039s\n \nCount 1,000 takes 0.000073s\n 2.0x data size,  1.9x time\n \nCount 2,000 takes 0.000121s\n 4.0x data size,  3.1x time\n \nCount 3,000 takes 0.000172s\n 6.0x data size,  4.5x time\n \nCount 4,000 takes 0.000240s\n 8.0x data size,  6.2x time\n \nCount 5,000 takes 0.000304s\n10.0x data size,  7.9x time\n \nItem 71: Prefer deque for Producer–Consumer Queues \n329\n\n\n330 \nChapter 8 Robustness and Performance\nThis shows that the append method takes roughly constant time for \nthe list type, and the total time for enqueueing scales linearly as the \ndata size increases. There is overhead for the list type to increase its \ncapacity under the covers as new items are added, but it’s reasonably \nlow and is amortized across repeated calls to append.\nHere, I define a similar benchmark for the pop(0) call that removes \nitems from the beginning of the queue (matching the consumer func-\ntion’s usage):\ndef list_pop_benchmark(count):\n    def prepare():\n        return list(range(count))\n \n    def run(queue):\n        while queue:\n            queue.pop(0)\n \n    tests = timeit.repeat(\n        setup='queue = prepare()',\n        stmt='run(queue)',\n        globals=locals(),\n        repeat=1000,\n        number=1)\n \n    return print_results(count, tests)\nI can similarly run this benchmark for queues of different sizes to see \nhow performance is affected by cardinality:\nbaseline = list_pop_benchmark(500)\nfor count in (1_000, 2_000, 3_000, 4_000, 5_000):\n    comparison = list_pop_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.000050s\n \nCount 1,000 takes 0.000133s\n 2.0x data size,  2.7x time\n \nCount 2,000 takes 0.000347s\n 4.0x data size,  6.9x time\n \nCount 3,000 takes 0.000663s\n 6.0x data size, 13.2x time\n \n\n\nCount 4,000 takes 0.000943s\n 8.0x data size, 18.8x time\n \nCount 5,000 takes 0.001481s\n10.0x data size, 29.5x time\nSurprisingly, this shows that the total time for dequeuing items from \na list with pop(0) scales quadratically as the length of the queue \nincreases. The cause is that pop(0) needs to move every item in the \nlist back an index, effectively reassigning the entire list’s contents. \nI need to call pop(0) for every item in the list, and thus I end up \ndoing roughly len(queue) * len(queue) operations to consume the \nqueue. This doesn’t scale.\nPython provides the deque class from the collections built-in module \nto solve this problem. deque is a double-ended queue implementation. \nIt provides constant time operations for inserting or removing items \nfrom its beginning or end. This makes it ideal for FIFO queues.\nTo use the deque class, the call to append in produce_emails can \nstay the same as it was when using a list for the queue. The \nlist.pop method call in consume_one_email must change to call the \ndeque.popleft method with no arguments instead. And the loop \nmethod must be called with a deque instance instead of a list. Every-\nthing else stays the same. Here, I redefine the one function affected to \nuse the new method and run loop again:\nimport collections\n \ndef consume_one_email(queue):\n    if not queue:\n        return\n    email = queue.popleft()  # Consumer\n    # Process the email message\n    ...\n \ndef my_end_func():\n    ...\n \nloop(collections.deque(), my_end_func)\nI can run another version of the benchmark to verify that append \nperformance (matching the producer function’s usage) has stayed \nroughly the same (modulo a constant factor):\ndef deque_append_benchmark(count):\n    def prepare():\n        return collections.deque()\n \n \nItem 71: Prefer deque for Producer–Consumer Queues \n331\n\n\n332 \nChapter 8 Robustness and Performance\n    def run(queue):\n        for i in range(count):\n            queue.append(i)\n \n    tests = timeit.repeat(\n        setup='queue = prepare()',\n        stmt='run(queue)',\n        globals=locals(),\n        repeat=1000,\n        number=1)\n    return print_results(count, tests)\n \nbaseline = deque_append_benchmark(500)\nfor count in (1_000, 2_000, 3_000, 4_000, 5_000):\n    comparison = deque_append_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.000029s\n \nCount 1,000 takes 0.000059s\n 2.0x data size,  2.1x time\n \nCount 2,000 takes 0.000121s\n 4.0x data size,  4.2x time\n \nCount 3,000 takes 0.000171s\n 6.0x data size,  6.0x time\n \nCount 4,000 takes 0.000243s\n 8.0x data size,  8.5x time\n \nCount 5,000 takes 0.000295s\n10.0x data size, 10.3x time\nAnd I can benchmark the performance of calling popleft to mimic \nthe consumer function’s usage of deque:\ndef dequeue_popleft_benchmark(count):\n    def prepare():\n        return collections.deque(range(count))\n \n    def run(queue):\n        while queue:\n            queue.popleft()\n \n    tests = timeit.repeat(\n\n\n        setup='queue = prepare()',\n        stmt='run(queue)',\n        globals=locals(),\n        repeat=1000,\n        number=1)\n \n    return print_results(count, tests)\n \nbaseline = dequeue_popleft_benchmark(500)\nfor count in (1_000, 2_000, 3_000, 4_000, 5_000):\n    comparison = dequeue_popleft_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.000024s\n \nCount 1,000 takes 0.000050s\n 2.0x data size,  2.1x time\n \nCount 2,000 takes 0.000100s\n 4.0x data size,  4.2x time\n \nCount 3,000 takes 0.000152s\n 6.0x data size,  6.3x time\n \nCount 4,000 takes 0.000207s\n 8.0x data size,  8.6x time\n \nCount 5,000 takes 0.000265s\n10.0x data size, 11.0x time\nThe popleft usage scales linearly instead of displaying the super-\nlinear behavior of pop(0) that I measured before—hooray! If you \nknow that the performance of a program critically depends on the \nspeed of producer–consumer queues, then deque is a great choice. \nIf you’re not sure, then you should instrument your program to \nfind out (see Item 70: “Profile Before Optimizing”).\nThings to Remember\n✦ The list type can be used as a FIFO queue by having the producer \ncall append to add items and the consumer call pop(0) to receive \nitems. However, this may cause problems because the performance \nof pop(0) degrades superlinearly as the queue length increases.\n \nItem 71: Prefer deque for Producer–Consumer Queues \n333\n\n\n334 \nChapter 8 Robustness and Performance\n✦ The deque class from the collections built-in module takes constant \ntime—regardless of length—for append and popleft, making it ideal \nfor FIFO queues.\nItem 72:  Consider Searching Sorted Sequences \nwith bisect\nIt’s common to find yourself with a large amount of data in memory \nas a sorted list that you then want to search. For example, you may \nhave loaded an English language dictionary to use for spell check-\ning, or perhaps a list of dated financial transactions to audit for \ncorrectness.\nRegardless of the data your specific program needs to process, search-\ning for a specific value in a list takes linear time proportional to the \nlist’s length when you call the index method:\ndata = list(range(10**5))\nindex = data.index(91234)\nassert index == 91234\nIf you’re not sure whether the exact value you’re searching for is in the \nlist, then you may want to search for the closest index that is equal \nto or exceeds your goal value. The simplest way to do this is to lin-\nearly scan the list and compare each item to your goal value:\ndef find_closest(sequence, goal):\n    for index, value in enumerate(sequence):\n        if goal < value:\n            return index\n    raise ValueError(f'{goal} is out of bounds')\n \nindex = find_closest(data, 91234.56)\nassert index == 91235\nPython’s built-in bisect module provides better ways to accom-\nplish these types of searches through ordered lists. You can use the \nbisect_left function to do an efficient binary search through any \nsequence of sorted items. The index it returns will either be where the \nitem is already present in the list or where you’d want to insert the \nitem in the list to keep it in sorted order:\nfrom bisect import bisect_left\n \nindex = bisect_left(data, 91234)     # Exact match\nassert index == 91234\n \n",
      "page_number": 345,
      "chapter_number": 33,
      "summary": "Python \nprovides a built-in profiler for determining which parts of a program \nare responsible for its execution time Key topics include function, functions, and queues. Covers function.",
      "keywords": [
        "time Count",
        "count",
        "time",
        "function",
        "queue",
        "data size",
        "data",
        "list",
        "Consumer Queues",
        "Performance",
        "size",
        "Consumer",
        "Item",
        "program",
        "Python"
      ],
      "concepts": [
        "function",
        "functions",
        "queues",
        "times",
        "profile",
        "profiled",
        "item",
        "returns",
        "count",
        "emails"
      ],
      "similar_chapters": [
        {
          "book": "Python Microservices Development",
          "chapter": 22,
          "title": "Segment 22 (pages 175-182)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 14,
          "title": "[ 465 ]",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 11,
          "title": "Testing, Debugging, Profiling, and Tuning",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 11,
          "title": "Segment 11 (pages 90-99)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 20,
          "title": "Threads and Concurrency",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 357-368)",
      "start_page": 357,
      "end_page": 368,
      "detection_method": "topic_boundary",
      "content": " \nItem 72: Consider Searching Sorted Sequences with bisect \n335\nindex = bisect_left(data, 91234.56)  # Closest match\nassert index == 91235\nThe complexity of the binary search algorithm used by the bisect \nmodule is logarithmic. This means searching in a list of length \n1 million takes roughly the same amount of time with bisect as \nlinearly searching a list of length 20 using the list.index method \n(math.log2(10**6) == 19.93...). It’s way faster!\nI can verify this speed improvement for the example from above by \nusing the timeit built-in module to run a micro-benchmark:\nimport random\nimport timeit\n \nsize = 10**5\niterations = 1000\n \ndata = list(range(size))\nto_lookup = [random.randint(0, size)\n             for _ in range(iterations)]\n \ndef run_linear(data, to_lookup):\n    for index in to_lookup:\n        data.index(index)\n \ndef run_bisect(data, to_lookup):\n    for index in to_lookup:\n        bisect_left(data, index)\n \nbaseline = timeit.timeit(\n    stmt='run_linear(data, to_lookup)',\n    globals=globals(),\n    number=10)\nprint(f'Linear search takes {baseline:.6f}s')\n \ncomparison = timeit.timeit(\n    stmt='run_bisect(data, to_lookup)',\n    globals=globals(),\n    number=10)\nprint(f'Bisect search takes {comparison:.6f}s')\n \nslowdown = 1 + ((baseline - comparison) / comparison)\nprint(f'{slowdown:.1f}x time')\n\n\n336 \nChapter 8 Robustness and Performance\n>>>\nLinear search takes 5.370117s\nBisect search takes 0.005220s\n1028.7x time\nThe best part about bisect is that it’s not limited to the list type; \nyou can use it with any Python object that acts like a sequence (see \nItem 43: “Inherit from collections.abc for Custom Container Types” \nfor how to do that). The module also provides additional features for \nmore advanced situations (see help(bisect)).\nThings to Remember\n✦ Searching sorted data contained in a list takes linear time using \nthe index method or a for loop with simple comparisons.\n✦ The bisect built-in module’s bisect_left function takes logarith-\nmic time to search for values in sorted lists, which can be orders of \nmagnitude faster than other approaches.\nItem 73: Know How to Use heapq for Priority Queues\nOne of the limitations of Python’s other queue implementations (see \nItem 71: “Prefer deque for Producer–Consumer Queues” and Item 55: \n“Use Queue to Coordinate Work Between Threads”) is that they are \nfirst-in, first-out (FIFO) queues: Their contents are sorted by the order \nin which they were received. Often, you need a program to process \nitems in order of relative importance instead. To accomplish this, a \npriority queue is the right tool for the job.\nFor example, say that I’m writing a program to manage books bor-\nrowed from a library. There are people constantly borrowing new \nbooks. There are people returning their borrowed books on time. And \nthere are people who need to be reminded to return their overdue \nbooks. Here, I define a class to represent a book that’s been borrowed:\nclass Book:\n    def __init__(self, title, due_date):\n        self.title = title\n        self.due_date = due_date\nI need a system that will send reminder messages when each book \npasses its due date. Unfortunately, I can’t use a FIFO queue for this \nbecause the amount of time each book is allowed to be borrowed var-\nies based on its recency, popularity, and other factors. For example, a \nbook that is borrowed today may be due back later than a book that’s \n\n\n \nItem 73: Know How to Use heapq for Priority Queues \n337\nborrowed tomorrow. Here, I achieve this behavior by using a standard \nlist and sorting it by due_date each time a new Book is added:\ndef add_book(queue, book):\n    queue.append(book)\n    queue.sort(key=lambda x: x.due_date, reverse=True)\n \nqueue = []\nadd_book(queue, Book('Don Quixote', '2019-06-07'))\nadd_book(queue, Book('Frankenstein', '2019-06-05'))\nadd_book(queue, Book('Les Misérables', '2019-06-08'))\nadd_book(queue, Book('War and Peace', '2019-06-03'))\nIf I can assume that the queue of borrowed books is always in sorted \norder, then all I need to do to check for overdue books is to inspect the \nfinal element in the list. Here, I define a function to return the next \noverdue book, if any, and remove it from the queue:\nclass NoOverdueBooks(Exception):\n    pass\n \ndef next_overdue_book(queue, now):\n    if queue:\n        book = queue[-1]\n        if book.due_date < now:\n            queue.pop()\n            return book\n \n    raise NoOverdueBooks\nI can call this function repeatedly to get overdue books to remind peo-\nple about in the order of most overdue to least overdue:\nnow = '2019-06-10'\n \nfound = next_overdue_book(queue, now)\nprint(found.title)\n \nfound = next_overdue_book(queue, now)\nprint(found.title)\n>>>\nWar and Peace\nFrankenstein\n\n\n338 \nChapter 8 Robustness and Performance\nIf a book is returned before the due date, I can remove the scheduled \nreminder message by removing the Book from the list:\ndef return_book(queue, book):\n    queue.remove(book)\n \nqueue = []\nbook = Book('Treasure Island', '2019-06-04')\n \nadd_book(queue, book)\nprint('Before return:', [x.title for x in queue])\n \nreturn_book(queue, book)\nprint('After return: ', [x.title for x in queue])\n>>>\nBefore return: ['Treasure Island']\nAfter return:  []\nAnd I can confirm that when all books are returned, the return_book \nfunction will raise the right exception (see Item 20: “Prefer Raising \nExceptions to Returning None”):\ntry:\n    next_overdue_book(queue, now)\nexcept NoOverdueBooks:\n    pass          # Expected\nelse:\n    assert False  # Doesn't happen\nHowever, the computational complexity of this solution isn’t \nideal. Although checking for and removing an overdue book has \na constant cost, every time I add a book, I pay the cost of sorting \nthe whole list again. If I have len(queue) books to add, and the \ncost of sorting them is roughly len(queue) * math.log(len(queue)), \nthe \ntime \nit \ntakes \nto \nadd \nbooks \nwill \ngrow \nsuperlinearly \n(len(queue) * len(queue) * math.log(len(queue))).\nHere, I define a micro-benchmark to measure this performance \nbehavior experimentally by using the timeit built-in module (see Item \n71: “Prefer deque for Producer–Consumer Queues” for the implemen-\ntation of print_results and print_delta):\nimport random\nimport timeit\n \ndef print_results(count, tests):\n    ...\n \n\n\ndef print_delta(before, after):\n    ...\n \ndef list_overdue_benchmark(count):\n    def prepare():\n        to_add = list(range(count))\n        random.shuffle(to_add)\n        return [], to_add\n \n    def run(queue, to_add):\n        for i in to_add:\n            queue.append(i)\n            queue.sort(reverse=True)\n \n        while queue:\n            queue.pop()\n \n    tests = timeit.repeat(\n        setup='queue, to_add = prepare()',\n        stmt=f'run(queue, to_add)',\n        globals=locals(),\n        repeat=100,\n        number=1)\n \n    return print_results(count, tests)\nI can verify that the runtime of adding and removing books from the \nqueue scales superlinearly as the number of books being borrowed \nincreases:\nbaseline = list_overdue_benchmark(500)\nfor count in (1_000, 1_500, 2_000):\n    comparison = list_overdue_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.001138s\n \nCount 1,000 takes 0.003317s\n 2.0x data size,  2.9x time\n \nCount 1,500 takes 0.007744s\n 3.0x data size,  6.8x time\n \nCount 2,000 takes 0.014739s\n 4.0x data size, 13.0x time\n \nItem 73: Know How to Use heapq for Priority Queues \n339\n\n\n340 \nChapter 8 Robustness and Performance\nWhen a book is returned before the due date, I need to do a linear \nscan in order to find the book in the queue and remove it. Removing \na book causes all subsequent items in the list to be shifted back \nan index, which has a high cost that also scales superlinearly. Here, \nI define another micro-benchmark to test the performance of return-\ning a book using this function:\ndef list_return_benchmark(count):\n    def prepare():\n        queue = list(range(count))\n        random.shuffle(queue)\n \n        to_return = list(range(count))\n        random.shuffle(to_return)\n \n        return queue, to_return\n \n    def run(queue, to_return):\n        for i in to_return:\n            queue.remove(i)\n \n    tests = timeit.repeat(\n        setup='queue, to_return = prepare()',\n        stmt=f'run(queue, to_return)',\n        globals=locals(),\n        repeat=100,\n        number=1)\n \n    return print_results(count, tests)\nAnd again, I can verify that indeed the performance degrades super-\nlinearly as the number of books increases:\nbaseline = list_return_benchmark(500)\nfor count in (1_000, 1_500, 2_000):\n    comparison = list_return_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.000898s\n \nCount 1,000 takes 0.003331s\n 2.0x data size,  3.7x time\n \nCount 1,500 takes 0.007674s\n 3.0x data size,  8.5x time\n \n\n\nCount 2,000 takes 0.013721s\n 4.0x data size, 15.3x time\nUsing the methods of list may work for a tiny library, but it certainly \nwon’t scale to the size of the Great Library of Alexandria, as I want it to!\nFortunately, Python has the built-in heapq module that solves this \nproblem by implementing priority queues efficiently. A heap is a data \nstructure that allows for a list of items to be maintained where \nthe computational complexity of adding a new item or removing the \nsmallest item has logarithmic computational complexity (i.e., even \nbetter than linear scaling). In this library example, smallest means \nthe book with the earliest due date. The best part about this module \nis that you don’t have to understand how heaps are implemented in \norder to use its functions correctly.\nHere, I reimplement the add_book function using the heapq module. \nThe queue is still a plain list. The heappush function replaces the \nlist.append call from before. And I no longer have to call list.sort on \nthe queue:\nfrom heapq import heappush\n \ndef add_book(queue, book):\n    heappush(queue, book)\nIf I try to use this with the Book class as previously defined, I get this \nsomewhat cryptic error:\nqueue = []\nadd_book(queue, Book('Little Women', '2019-06-05'))\nadd_book(queue, Book('The Time Machine', '2019-05-30'))\n>>>\nTraceback ...\nTypeError: '<' not supported between instances of 'Book' and \n¯'Book'\nThe heapq module requires items in the priority queue to be compa-\nrable and have a natural sort order (see Item 14: “Sort by Complex \nCriteria Using the key Parameter” for details). You can quickly give \nthe Book class this behavior by using the total_ordering class dec-\norator from the functools built-in module (see Item 51: “Prefer Class \nDecorators Over Metaclasses for Composable Class Extensions” for \nbackground) and implementing the __lt__ special method (see Item \n43: “Inherit from collections.abc for Custom Container Types” for \n \nItem 73: Know How to Use heapq for Priority Queues \n341\n\n\n342 \nChapter 8 Robustness and Performance\nbackground). Here, I redefine the class with a less-than method that \nsimply compares the due_date fields between two Book instances:\nimport functools\n \n@functools.total_ordering\nclass Book:\n    def __init__(self, title, due_date):\n        self.title = title\n        self.due_date = due_date\n \n    def __lt__(self, other):\n        return self.due_date < other.due_date\nNow, I can add books to the priority queue by using the heapq.heappush \nfunction without issues:\nqueue = []\nadd_book(queue, Book('Pride and Prejudice', '2019-06-01'))\nadd_book(queue, Book('The Time Machine', '2019-05-30'))\nadd_book(queue, Book('Crime and Punishment', '2019-06-06'))\nadd_book(queue, Book('Wuthering Heights', '2019-06-12'))\nAlternatively, I can create a list with all of the books in any order and \nthen use the sort method of list to produce the heap:\nqueue = [\n    Book('Pride and Prejudice', '2019-06-01'),\n    Book('The Time Machine', '2019-05-30'),\n    Book('Crime and Punishment', '2019-06-06'),\n    Book('Wuthering Heights', '2019-06-12'),\n]\nqueue.sort()\nOr I can use the heapq.heapify function to create a heap in linear \ntime (as opposed to the sort method’s len(queue) * log(len(queue)) \ncomplexity):\nfrom heapq import heapify\n \nqueue = [\n    Book('Pride and Prejudice', '2019-06-01'),\n    Book('The Time Machine', '2019-05-30'),\n    Book('Crime and Punishment', '2019-06-06'),\n    Book('Wuthering Heights', '2019-06-12'),\n]\nheapify(queue)\n\n\nTo check for overdue books, I inspect the first element in the list \ninstead of the last, and then I use the heapq.heappop function instead \nof the list.pop function:\nfrom heapq import heappop\n \ndef next_overdue_book(queue, now):\n    if queue:\n        book = queue[0]           # Most overdue first\n        if book.due_date < now:\n            heappop(queue)        # Remove the overdue book\n            return book\n \n    raise NoOverdueBooks\nNow, I can find and remove overdue books in order until there are \nnone left for the current time:\nnow = '2019-06-02'\n \nbook = next_overdue_book(queue, now)\nprint(book.title)\n \nbook = next_overdue_book(queue, now)\nprint(book.title)\n \ntry:\n    next_overdue_book(queue, now)\nexcept NoOverdueBooks:\n    pass          # Expected\nelse:\n    assert False  # Doesn't happen\n>>>\nThe Time Machine\nPride and Prejudice\nI can write another micro-benchmark to test the performance of this \nimplementation that uses the heapq module:\ndef heap_overdue_benchmark(count):\n    def prepare():\n        to_add = list(range(count))\n        random.shuffle(to_add)\n        return [], to_add\n \n    def run(queue, to_add):\n        for i in to_add:\n \nItem 73: Know How to Use heapq for Priority Queues \n343\n\n\n344 \nChapter 8 Robustness and Performance\n            heappush(queue, i)\n        while queue:\n            heappop(queue)\n \n    tests = timeit.repeat(\n        setup='queue, to_add = prepare()',\n        stmt=f'run(queue, to_add)',\n        globals=locals(),\n        repeat=100,\n        number=1)\n \n    return print_results(count, tests)\nThis benchmark experimentally verifies that the heap-based \npriority \nqueue \nimplementation \nscales \nmuch \nbetter \n(roughly \nlen(queue) * math.log(len(queue))), without superlinearly degrading \nperformance:\nbaseline = heap_overdue_benchmark(500)\nfor count in (1_000, 1_500, 2_000):\n    comparison = heap_overdue_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.000150s\n \nCount 1,000 takes 0.000325s\n 2.0x data size,  2.2x time\n \nCount 1,500 takes 0.000528s\n 3.0x data size,  3.5x time\n \nCount 2,000 takes 0.000658s\n 4.0x data size,  4.4x time\nWith the heapq implementation, one question remains: How should \nI handle returns that are on time? The solution is to never remove a \nbook from the priority queue until its due date. At that time, it will \nbe the first item in the list, and I can simply ignore the book if it’s \nalready been returned. Here, I implement this behavior by adding a \nnew field to track the book’s return status:\n@functools.total_ordering\nclass Book:\n    def __init__(self, title, due_date):\n        self.title = title\n        self.due_date = due_date\n\n\n        self.returned = False  # New field\n \n    ...\nThen, I change the next_overdue_book function to repeatedly ignore \nany book that’s already been returned:\ndef next_overdue_book(queue, now):\n    while queue:\n        book = queue[0]\n        if book.returned:\n            heappop(queue)\n            continue\n \n        if book.due_date < now:\n            heappop(queue)\n            return book\n \n        break\n \n    raise NoOverdueBooks\nThis approach makes the return_book function extremely fast \nbecause it makes no modifications to the priority queue:\ndef return_book(queue, book):\n    book.returned = True\nThe downside of this solution for returns is that the priority queue \nmay grow to the maximum size it would have needed if all books from \nthe library were checked out and went overdue. Although the queue \noperations will be fast thanks to heapq, this storage overhead may \ntake significant memory (see Item 81: “Use tracemalloc to Understand \nMemory Usage and Leaks” for how to debug such usage).\nThat said, if you’re trying to build a robust system, you need to plan \nfor the worst-case scenario; thus, you should expect that it’s possible \nfor every library book to go overdue for some reason (e.g., a natural \ndisaster closes the road to the library). This memory cost is a design \nconsideration that you should have already planned for and mitigated \nthrough additional constraints (e.g., imposing a maximum number of \nsimultaneously lent books).\nBeyond the priority queue primitives that I’ve used in this example, \nthe heapq module provides additional functionality for advanced use \ncases (see help(heapq)). The module is a great choice when its function-\nality matches the problem you’re facing (see the queue.PriorityQueue \nclass for another thread-safe option).\n \nItem 73: Know How to Use heapq for Priority Queues \n345\n\n\n346 \nChapter 8 Robustness and Performance\nThings to Remember\n✦ Priority queues allow you to process items in order of importance \ninstead of in first-in, first-out order.\n✦ If you try to use list operations to implement a priority queue, your \nprogram’s performance will degrade superlinearly as the queue \ngrows.\n✦ The heapq built-in module provides all of the functions you need to \nimplement a priority queue that scales efficiently.\n✦ To use heapq, the items being prioritized must have a natural sort \norder, which requires special methods like __lt__ to be defined for \nclasses.\nItem 74:  Consider memoryview and bytearray for \nZero-Copy Interactions with bytes\nAlthough Python isn’t able to parallelize CPU-bound computation \nwithout extra effort (see Item 64: “Consider concurrent.futures for \nTrue Parallelism”), it is able to support high-throughput, parallel I/O \nin a variety of ways (see Item 53: “Use Threads for Blocking I/O, Avoid \nfor Parallelism” and Item 60: “Achieve Highly Concurrent I/O with \nCoroutines”). That said, it’s surprisingly easy to use these I/O tools \nthe wrong way and reach the conclusion that the language is too slow \nfor even I/O-bound workloads.\nFor example, say that I’m building a media server to stream television \nor movies over a network to users so they can watch without having \nto download the video data in advance. One of the key features of \nsuch a system is the ability for users to move forward or backward \nin the video playback so they can skip or repeat parts. In the client \nprogram, I can implement this by requesting a chunk of data from the \nserver corresponding to the new time index selected by the user:\ndef timecode_to_index(video_id, timecode):\n    ...\n    # Returns the byte offset in the video data\n \ndef request_chunk(video_id, byte_offset, size):\n    ...\n    # Returns size bytes of video_id's data from the offset\n \nvideo_id = ...\ntimecode = '01:09:14:28'\nbyte_offset = timecode_to_index(video_id, timecode)\n",
      "page_number": 357,
      "chapter_number": 34,
      "summary": "The module also provides additional features for \nmore advanced situations (see help(bisect)) Key topics include queues, books, and returns. Item 72:  Consider Searching Sorted Sequences \nwith bisect\nIt’s common to find yourself with a large amount of data in memory \nas a sorted list that you then want to search.",
      "keywords": [
        "book",
        "queue",
        "count",
        "list",
        "Item",
        "time",
        "priority queue",
        "add",
        "overdue",
        "time Count",
        "data",
        "overdue books",
        "index",
        "Priority",
        "date"
      ],
      "concepts": [
        "queues",
        "books",
        "returns",
        "item",
        "function",
        "functions",
        "functionality",
        "list",
        "index",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Data-Oriented Design",
          "chapter": 6,
          "title": "Searching",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 12,
          "title": "Segment 12 (pages 109-118)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 11,
          "title": "Segment 11 (pages 89-108)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 5,
          "title": "Segment 5 (pages 84-106)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 18,
          "title": "Segment 18 (pages 149-156)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 369-376)",
      "start_page": 369,
      "end_page": 376,
      "detection_method": "topic_boundary",
      "content": " \nItem 74: Consider memoryview for zero-copy interactions \n347\nsize = 20 * 1024 * 1024\nvideo_data = request_chunk(video_id, byte_offset, size)\nHow would you implement the server-side handler that receives the \nrequest_chunk request and returns the corresponding 20 MB chunk \nof video data? For the sake of this example, I assume that the com-\nmand and control parts of the server have already been hooked up \n(see Item 61: “Know How to Port Threaded I/O to asyncio” for what \nthat requires). I focus here on the last steps where the requested \nchunk is extracted from gigabytes of video data that’s cached in mem-\nory and is then sent over a socket back to the client. Here’s what the \nimplementation would look like:\nsocket = ...             # socket connection to client\nvideo_data = ...         # bytes containing data for video_id\nbyte_offset = ...        # Requested starting position\nsize = 20 * 1024 * 1024  # Requested chunk size\n \nchunk = video_data[byte_offset:byte_offset + size]\nsocket.send(chunk)\nThe latency and throughput of this code will come down to two fac-\ntors: how much time it takes to slice the 20 MB video chunk from \nvideo_data, and how much time the socket takes to transmit that \ndata to the client. If I assume that the socket is infinitely fast, I can \nrun a micro-benchmark by using the timeit built-in module to under-\nstand the performance characteristics of slicing bytes instances this \nway to create chunks (see Item 11: “Know How to Slice Sequences” for \nbackground):\nimport timeit\n \ndef run_test():\n    chunk = video_data[byte_offset:byte_offset + size]\n    # Call socket.send(chunk), but ignoring for benchmark\n \nresult = timeit.timeit(\n    stmt='run_test()',\n    globals=globals(),\n    number=100) / 100\n \nprint(f'{result:0.9f} seconds')\n>>>\n0.004925669 seconds\n\n\n348 \nChapter 8 Robustness and Performance\nIt took roughly 5 milliseconds to extract the 20 MB slice of data to \ntransmit to the client. That means the overall throughput of my \nserver is limited to a theoretical maximum of 20 MB / 5 milliseconds \n= 7.3 GB / second, since that’s the fastest I can extract the video \ndata from memory. My server will also be limited to 1 CPU-second / \n5  milliseconds = 200 clients requesting new chunks in parallel, which \nis tiny compared to the tens of thousands of simultaneous connec-\ntions that tools like the asyncio built-in module can support. The \nproblem is that slicing a bytes instance causes the underlying data to \nbe copied, which takes CPU time.\nA better way to write this code is by using Python’s built-in memoryview \ntype, which exposes CPython’s high-performance buffer protocol to \nprograms. The buffer protocol is a low-level C API that allows the \nPython runtime and C extensions to access the underlying data \nbuffers that are behind objects like bytes instances. The best part \nabout memoryview instances is that slicing them results in another \nmemoryview instance without copying the underlying data. Here, I cre-\nate a memoryview wrapping a bytes instance and inspect a slice of it:\ndata = b'shave and a haircut, two bits'\nview = memoryview(data)\nchunk = view[12:19]\nprint(chunk)\nprint('Size:           ', chunk.nbytes)\nprint('Data in view:   ', chunk.tobytes())\nprint('Underlying data:', chunk.obj)\n>>>\n<memory at 0x10951fb80>\nSize:            7\nData in view:    b'haircut'\nUnderlying data: b'shave and a haircut, two bits'\nBy enabling zero-copy operations, memoryview can provide enor-\nmous speedups for code that needs to quickly process large amounts \nof memory, such as numerical C extensions like NumPy and \nI/O-bound programs like this one. Here, I replace the simple bytes \nslicing from above with memoryview slicing instead and repeat the \nsame micro-benchmark:\nvideo_view = memoryview(video_data)\n \ndef run_test():\n    chunk = video_view[byte_offset:byte_offset + size]\n    # Call socket.send(chunk), but ignoring for benchmark\n \n\n\nresult = timeit.timeit(\n    stmt='run_test()',\n    globals=globals(),\n    number=100) / 100\n \nprint(f'{result:0.9f} seconds')\n>>>\n0.000000250 seconds\nThe result is 250 nanoseconds. Now the theoretical maximum through-\nput of my server is 20 MB / 250 nanoseconds = 164 TB / second. \nFor parallel clients, I can theoretically support up to 1 CPU- second / \n250 nanoseconds = 4 million. That’s more like it! This means that \nnow my program is entirely bound by the underlying performance of \nthe socket connection to the client, not by CPU constraints.\nNow, imagine that the data must flow in the other direction, where \nsome clients are sending live video streams to the server in order to \nbroadcast them to other users. In order to do this, I need to store the \nlatest video data from the user in a cache that other clients can read \nfrom. Here’s what the implementation of reading 1 MB of new data \nfrom the incoming client would look like:\nsocket = ...        # socket connection to the client\nvideo_cache = ...   # Cache of incoming video stream\nbyte_offset = ...   # Incoming buffer position\nsize = 1024 * 1024  # Incoming chunk size\n \nchunk = socket.recv(size)\nvideo_view = memoryview(video_cache)\nbefore = video_view[:byte_offset]\nafter = video_view[byte_offset + size:]\nnew_cache = b''.join([before, chunk, after])\nThe socket.recv method returns a bytes instance. I can splice the \nnew data with the existing cache at the current byte_offset by using \nsimple slicing operations and the bytes.join method. To understand \nthe performance of this, I can run another micro-benchmark. I’m \nusing a dummy socket, so the performance test is only for the mem-\nory operations, not the I/O interaction:\ndef run_test():\n    chunk = socket.recv(size)\n    before = video_view[:byte_offset]\n    after = video_view[byte_offset + size:]\n    new_cache = b''.join([before, chunk, after])\n \n \nItem 74: Consider memoryview for zero-copy interactions \n349\n\n\n350 \nChapter 8 Robustness and Performance\nresult = timeit.timeit(\n    stmt='run_test()',\n    globals=globals(),\n    number=100) / 100\n \nprint(f'{result:0.9f} seconds')\n>>>\n0.033520550 seconds\nIt takes 33 milliseconds to receive 1 MB and update the video cache. \nThis means my maximum receive throughput is 1 MB / 33  milliseconds \n= 31 MB / second, and I’m limited to 31 MB / 1 MB = 31 simultaneous \nclients streaming in video data this way. This doesn’t scale.\nA better way to write this code is to use Python’s built-in bytearray \ntype in conjunction with memoryview. One limitation with bytes \ninstances is that they are read-only and don’t allow for individual \nindexes to be updated:\nmy_bytes = b'hello'\nmy_bytes[0] = b'\\x79'\n>>>\nTraceback ...\nTypeError: 'bytes' object does not support item assignment\nThe bytearray type is like a mutable version of bytes that allows for \narbitrary positions to be overwritten. bytearray uses integers for its \nvalues instead of bytes:\nmy_array = bytearray(b'hello')\nmy_array[0] = 0x79\nprint(my_array)\n>>>\nbytearray(b'yello')\nA memoryview can also be used to wrap a bytearray. When you slice \nsuch a memoryview, the resulting object can be used to assign data to a \nparticular portion of the underlying buffer. This eliminates the copy-\ning costs from above that were required to splice the bytes instances \nback together after data was received from the client:\nmy_array = bytearray(b'row, row, row your boat')\nmy_view = memoryview(my_array)\nwrite_view = my_view[3:13]\nwrite_view[:] = b'-10 bytes-'\nprint(my_array)\n\n\n>>>\nbytearray(b'row-10 bytes- your boat')\nMany library methods in Python, such as socket.recv_into and \nRawIOBase.readinto, use the buffer protocol to receive or read data \nquickly. The benefit of these methods is that they avoid allocating \nmemory and creating another copy of the data; what’s received goes \nstraight into an existing buffer. Here, I use socket.recv_into along \nwith a memoryview slice to receive data into an underlying bytearray \nwithout the need for splicing:\nvideo_array = bytearray(video_cache)\nwrite_view = memoryview(video_array)\nchunk = write_view[byte_offset:byte_offset + size]\nsocket.recv_into(chunk)\nI can run another micro-benchmark to compare the performance of \nthis approach to the earlier example that used socket.recv:\ndef run_test():\n    chunk = write_view[byte_offset:byte_offset + size]\n    socket.recv_into(chunk)\n \nresult = timeit.timeit(\n    stmt='run_test()',\n    globals=globals(),\n    number=100) / 100\n \nprint(f'{result:0.9f} seconds')\n>>>\n0.000033925 seconds\nIt took 33 microseconds to receive a 1 MB video transmission. This \nmeans my server can support 1 MB / 33 microseconds = 31 GB / \nsecond of max throughput, and 31 GB / 1 MB = 31,000 parallel \nstreaming clients. That’s the type of scalability that I’m looking for!\nThings to Remember\n✦ The memoryview built-in type provides a zero-copy interface for \nreading and writing slices of objects that support Python’s high- \nperformance buffer protocol.\n✦ The bytearray built-in type provides a mutable bytes-like type \nthat can be used for zero-copy data reads with functions like \nsocket.recv_from.\n✦ A memoryview can wrap a bytearray, allowing for received data to be \nspliced into an arbitrary buffer location without copying costs.\n \nItem 74: Consider memoryview for zero-copy interactions \n351\n\n\nThis page intentionally left blank \n\n\n9\nTesting and \nDebugging\nPython doesn’t have compile-time static type checking. There’s \n nothing in the interpreter that will ensure that your program will \nwork correctly when you run it. Python does support optional type \nannotations that can be used in static analysis to detect many kinds \nof bugs (see Item 90: “Consider Static Analysis via typing to Obviate \nBugs” for details). However, it’s still fundamentally a dynamic lan-\nguage, and anything is possible. With Python, you ultimately don’t \nknow if the functions your program calls will be defined at runtime, \neven when their existence is evident in the source code. This dynamic \nbehavior is both a blessing and a curse.\nThe large numbers of Python programmers out there say it’s worth \ngoing without compile-time static type checking because of the pro-\nductivity gained from the resulting brevity and simplicity. But most \npeople using Python have at least one horror story about a program \nencountering a boneheaded error at runtime. One of the worst exam-\nples I’ve heard of involved a SyntaxError being raised in production as \na side effect of a dynamic import (see Item 88: “Know How to Break \nCircular Dependencies”), resulting in a crashed server process. The \nprogrammer I know who was hit by this surprising occurrence has \nsince ruled out using Python ever again.\nBut I have to wonder, why wasn’t the code more well tested before \nthe program was deployed to production? Compile-time static type \nsafety isn’t everything. You should always test your code, regardless \nof what language it’s written in. However, I’ll admit that in Python it \nmay be more important to write tests to verify correctness than in \nother languages. Luckily, the same dynamic features that create risks \nalso make it extremely easy to write tests for your code and to debug \nmalfunctioning programs. You can use Python’s dynamic nature and \neasily overridable behaviors to implement tests and ensure that your \nprograms work as expected.\n\n\n354 \nChapter 9 Testing and Debugging\nYou should think of tests as an insurance policy on your code. Good \ntests give you confidence that your code is correct. If you refactor or \nexpand your code, tests that verify behavior—not implementation—\nmake it easy to identify what’s changed. It sounds counterintuitive, \nbut having good tests actually makes it easier to modify Python code, \nnot harder.\nItem 75: Use repr Strings for Debugging Output\nWhen debugging a Python program, the print function and format \nstrings (see Item 4: “Prefer Interpolated F-Strings Over C-style Format \nStrings and str.format”), or output via the logging built-in module, \nwill get you surprisingly far. Python internals are often easy to access \nvia plain attributes (see Item 42: “Prefer Public Attributes Over Pri-\nvate Ones”). All you need to do is call print to see how the state of \nyour program changes while it runs and understand where it goes \nwrong.\nThe print function outputs a human-readable string version of \n whatever you supply it. For example, printing a basic string prints the \ncontents of the string without the surrounding quote characters:\nprint('foo bar')\n>>>\nfoo bar\nThis is equivalent to all of these alternatives:\n \n■Calling the str function before passing the value to print\n \n■Using the '%s' format string with the % operator\n \n■Default formatting of the value with an f-string\n \n■Calling the format built-in function\n \n■Explicitly calling the __format__ special method\n \n■Explicitly calling the __str__ special method\nHere, I verify this behavior:\nmy_value = 'foo bar'\nprint(str(my_value))\nprint('%s' % my_value)\nprint(f'{my_value}')\nprint(format(my_value))\nprint(my_value.__format__('s'))\nprint(my_value.__str__())\n",
      "page_number": 369,
      "chapter_number": 35,
      "summary": "✦ The heapq built-in module provides all of the functions you need to \nimplement a priority queue that scales efficiently Key topics include chunk, bytes, and memoryview.",
      "keywords": [
        "video",
        "data",
        "video data",
        "chunk",
        "byte",
        "Python",
        "memoryview",
        "size",
        "Item",
        "offset",
        "view",
        "underlying data",
        "bytearray",
        "Performance",
        "run"
      ],
      "concepts": [
        "chunk",
        "bytes",
        "memoryview",
        "size",
        "items",
        "type",
        "typing",
        "uses",
        "buffer",
        "program"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 21,
          "title": "Segment 21 (pages 193-203)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 20,
          "title": "Segment 20 (pages 185-192)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 38,
          "title": "Segment 38 (pages 351-358)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 10,
          "title": "Segment 10 (pages 86-98)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 377-390)",
      "start_page": 377,
      "end_page": 390,
      "detection_method": "topic_boundary",
      "content": " \nItem 75: Use repr Strings for Debugging Output \n355\n>>>\nfoo bar\nfoo bar\nfoo bar\nfoo bar\nfoo bar\nfoo bar\nThe problem is that the human-readable string for a value doesn’t \nmake it clear what the actual type and its specific composition are. \nFor example, notice how in the default output of print, you can’t dis-\ntinguish between the types of the number 5 and the string '5':\nprint(5)\nprint('5')\n \nint_value = 5\nstr_value = '5'\nprint(f'{int_value} == {str_value} ?')\n>>>\n5\n5\n5 == 5 ?\nIf you’re debugging a program with print, these type differences mat-\nter. What you almost always want while debugging is to see the repr \nversion of an object. The repr built-in function returns the printable \nrepresentation of an object, which should be its most clearly under-\nstandable string representation. For most built-in types, the string \nreturned by repr is a valid Python expression:\na = '\\x07'\nprint(repr(a))\n>>>\n'\\x07'\nPassing the value from repr to the eval built-in function should result \nin the same Python object that you started with (and, of course, in \npractice you should only use eval with extreme caution):\nb = eval(repr(a))\nassert a == b\nWhen you’re debugging with print, you should call repr on a value \nbefore printing to ensure that any difference in types is clear:\nprint(repr(5))\nprint(repr('5'))\n\n\n356 \nChapter 9 Testing and Debugging\n>>>\n5\n'5'\nThis is equivalent to using the '%r' format string with the % operator \nor an f-string with the !r type conversion:\nprint('%r' % 5)\nprint('%r' % '5')\n \nint_value = 5\nstr_value = '5'\nprint(f'{int_value!r} != {str_value!r}')\n>>>\n5\n'5'\n5 != '5'\nFor instances of Python classes, the default human-readable string \nvalue is the same as the repr value. This means that passing an \ninstance to print will do the right thing, and you don’t need to explic-\nitly call repr on it. Unfortunately, the default implementation of \nrepr for object subclasses isn’t especially helpful. For example, here \nI define a simple class and then print one of its instances:\nclass OpaqueClass:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n \nobj = OpaqueClass(1, 'foo')\nprint(obj)\n>>>\n<__main__.OpaqueClass object at 0x10963d6d0>\nThis output can’t be passed to the eval function, and it says nothing \nabout the instance fields of the object.\nThere are two solutions to this problem. If you have control of the \nclass, you can define your own __repr__ special method that returns \na string containing the Python expression that re-creates the object. \nHere, I define that function for the class above:\nclass BetterClass:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n \n\n\n \nItem 76: Verify Related Behaviors in TestCase Subclasses \n357\n    def __repr__(self):\n        return f'BetterClass({self.x!r}, {self.y!r})'\nNow the repr value is much more useful:\nobj = BetterClass(2, 'bar')\nprint(obj)\n>>>\nBetterClass(2, 'bar')\nWhen you don’t have control over the class definition, you can reach \ninto the object’s instance dictionary, which is stored in the __dict__ \nattribute. Here, I print out the contents of an OpaqueClass instance:\nobj = OpaqueClass(4, 'baz')\nprint(obj.__dict__)\n>>>\n{'x': 4, 'y': 'baz'}\nThings to Remember\n✦ Calling print on built-in Python types produces the human- \nreadable string version of a value, which hides type information.\n✦ Calling repr on built-in Python types produces the printable string \nversion of a value. These repr strings can often be passed to the \neval built-in function to get back the original value.\n✦ %s in format strings produces human-readable strings like str. %r \nproduces printable strings like repr. F-strings produce human- \nreadable strings for replacement text expressions unless you specify \nthe !r suffix.\n✦ You can define the __repr__ special method on a class to customize \nthe printable representation of instances and provide more detailed \ndebugging information.\nItem 76:  Verify Related Behaviors in TestCase \nSubclasses\nThe canonical way to write tests in Python is to use the unittest \nbuilt-in module. For example, say I have the following utility function \ndefined in utils.py that I would like to verify works correctly across a \nvariety of inputs:\n# utils.py\ndef to_str(data):\n\n\n358 \nChapter 9 Testing and Debugging\n    if isinstance(data, str):\n        return data\n    elif isinstance(data, bytes):\n        return data.decode('utf-8')\n    else:\n        raise TypeError('Must supply str or bytes, '\n                        'found: %r' % data)\nTo define tests, I create a second file named test_utils.py or \nutils_test.py—the naming scheme you prefer is a style choice—that \ncontains tests for each behavior that I expect:\n# utils_test.py\nfrom unittest import TestCase, main\nfrom utils import to_str\n \nclass UtilsTestCase(TestCase):\n    def test_to_str_bytes(self):\n        self.assertEqual('hello', to_str(b'hello'))\n \n    def test_to_str_str(self):\n        self.assertEqual('hello', to_str('hello'))\n \n    def test_failing(self):\n        self.assertEqual('incorrect', to_str('hello'))\n \nif __name__ == '__main__':\n    main()\nThen, I run the test file using the Python command line. In this case, \ntwo of the test methods pass and one fails, with a helpful error mes-\nsage about what went wrong:\n$ python3 utils_test.py\nF..\n===============================================================\nFAIL: test_failing (__main__.UtilsTestCase)\n---------------------------------------------------------------\nTraceback (most recent call last):\n  File \"utils_test.py\", line 15, in test_failing\n    self.assertEqual('incorrect', to_str('hello'))\nAssertionError: 'incorrect' != 'hello'\n- incorrect\n+ hello\n \n \n---------------------------------------------------------------\n\n\nRan 3 tests in 0.002s\n \nFAILED (failures=1)\nTests are organized into TestCase subclasses. Each test case is a \nmethod beginning with the word test. If a test method runs without \nraising any kind of Exception (including AssertionError from assert \nstatements), the test is considered to have passed successfully. If one \ntest fails, the TestCase subclass continues running the other test \nmethods so you can get a full picture of how all your tests are doing \ninstead of stopping at the first sign of trouble.\nIf you want to iterate quickly to fix or improve a specific test, you can \nrun only that test method by specifying its path within the test mod-\nule on the command line:\n$ python3 utils_test.py UtilsTestCase.test_to_str_bytes\n.\n---------------------------------------------------------------\nRan 1 test in 0.000s\n \nOK\nYou can also invoke the debugger from directly within test methods \nat specific breakpoints in order to dig more deeply into the cause of \nfailures (see Item 80: “Consider Interactive Debugging with pdb” for \nhow to do that).\nThe TestCase class provides helper methods for making assertions in \nyour tests, such as assertEqual for verifying equality, assertTrue for \nverifying Boolean expressions, and many more (see help(TestCase) \nfor the full list). These are better than the built-in assert state-\nment because they print out all of the inputs and outputs to help \nyou understand the exact reason the test is failing. For example, here \nI have the same test case written with and without using a helper \nassertion method:\n# assert_test.py\nfrom unittest import TestCase, main\nfrom utils import to_str\n \nclass AssertTestCase(TestCase):\n    def test_assert_helper(self):\n        expected = 12\n        found = 2 * 5\n        self.assertEqual(expected, found)\n \n \nItem 76: Verify Related Behaviors in TestCase Subclasses \n359\n\n\n360 \nChapter 9 Testing and Debugging\n    def test_assert_statement(self):\n        expected = 12\n        found = 2 * 5\n        assert expected == found\n \nif __name__ == '__main__':\n    main()\nWhich of these failure messages seems more helpful to you?\n$ python3 assert_test.py\nFF\n===============================================================\nFAIL: test_assert_helper (__main__.AssertTestCase)\n---------------------------------------------------------------\nTraceback (most recent call last):\n  File \"assert_test.py\", line 16, in test_assert_helper\n    self.assertEqual(expected, found)\nAssertionError: 12 != 10\n \n===============================================================\nFAIL: test_assert_statement (__main__.AssertTestCase)\n---------------------------------------------------------------\nTraceback (most recent call last):\n  File \"assert_test.py\", line 11, in test_assert_statement\n    assert expected == found\nAssertionError\n \n---------------------------------------------------------------\nRan 2 tests in 0.001s\n \nFAILED (failures=2)\nThere’s also an assertRaises helper method for verifying excep-\ntions that can be used as a context manager in with statements (see \nItem 66: “Consider contextlib and with Statements for Reusable \ntry/finally Behavior” for how that works). This appears similar to a \ntry/except statement and makes it abundantly clear where the excep-\ntion is expected to be raised:\n# utils_error_test.py\nfrom unittest import TestCase, main\nfrom utils import to_str\n \nclass UtilsErrorTestCase(TestCase):\n\n\n    def test_to_str_bad(self):\n        with self.assertRaises(TypeError):\n            to_str(object())\n \n    def test_to_str_bad_encoding(self):\n        with self.assertRaises(UnicodeDecodeError):\n            to_str(b'\\xfa\\xfa')\n \nif __name__ == '__main__':\n    main()\nYou can define your own helper methods with complex logic in \nTestCase subclasses to make your tests more readable. Just ensure \nthat your method names don’t begin with the word test, or they’ll \nbe run as if they’re test cases. In addition to calling TestCase asser-\ntion methods, these custom test helpers often use the fail method to \nclarify which assumption or invariant wasn’t met. For example, here \nI define a custom test helper method for verifying the behavior of a \ngenerator:\n# helper_test.py\nfrom unittest import TestCase, main\n \ndef sum_squares(values):\n    cumulative = 0\n    for value in values:\n        cumulative += value ** 2\n        yield cumulative\n \nclass HelperTestCase(TestCase):\n    def verify_complex_case(self, values, expected):\n        expect_it = iter(expected)\n        found_it = iter(sum_squares(values))\n        test_it = zip(expect_it, found_it)\n \n        for i, (expect, found) in enumerate(test_it):\n            self.assertEqual(\n                expect,\n                found,\n                f'Index {i} is wrong')\n \n        # Verify both generators are exhausted\n        try:\n            next(expect_it)\n        except StopIteration:\n            pass\n \nItem 76: Verify Related Behaviors in TestCase Subclasses \n361\n\n\n362 \nChapter 9 Testing and Debugging\n        else:\n            self.fail('Expected longer than found')\n \n        try:\n            next(found_it)\n        except StopIteration:\n            pass\n        else:\n            self.fail('Found longer than expected')\n \n    def test_wrong_lengths(self):\n        values = [1.1, 2.2, 3.3]\n        expected = [\n            1.1**2,\n        ]\n        self.verify_complex_case(values, expected)\n \n    def test_wrong_results(self):\n        values = [1.1, 2.2, 3.3]\n        expected = [\n            1.1**2,\n            1.1**2 + 2.2**2,\n            1.1**2 + 2.2**2 + 3.3**2 + 4.4**2,\n        ]\n        self.verify_complex_case(values, expected)\n \nif __name__ == '__main__':\n    main()\nThe helper method makes the test cases short and readable, and the \noutputted error messages are easy to understand:\n$ python3 helper_test.py\nFF\n===============================================================\nFAIL: test_wrong_lengths (__main__.HelperTestCase)\n---------------------------------------------------------------\nTraceback (most recent call last):\n  File \"helper_test.py\", line 43, in test_wrong_lengths\n    self.verify_complex_case(values, expected)\n  File \"helper_test.py\", line 34, in verify_complex_case\n    self.fail('Found longer than expected')\nAssertionError: Found longer than expected\n \n\n\n===============================================================\nFAIL: test_wrong_results (__main__.HelperTestCase)\n---------------------------------------------------------------\nTraceback (most recent call last):\n  File \"helper_test.py\", line 52, in test_wrong_results\n    self.verify_complex_case(values, expected)\n  File \"helper_test.py\", line 24, in verify_complex_case\n    f'Index {i} is wrong')\nAssertionError: 36.3 != 16.939999999999998 : Index 2 is wrong\n \n---------------------------------------------------------------\nRan 2 tests in 0.002s\n \nFAILED (failures=2)\nI usually define one TestCase subclass for each set of related tests. \nSometimes, I have one TestCase subclass for each function that has \nmany edge cases. Other times, a TestCase subclass spans all func-\ntions in a single module. I often create one TestCase subclass for test-\ning each basic class and all of its methods.\nThe TestCase class also provides a subTest helper method that enables \nyou to avoid boilerplate by defining multiple tests within a single test \nmethod. This is especially helpful for writing data-driven tests, and it \nallows the test method to continue testing other cases even after one \nof them fails (similar to the behavior of TestCase with its contained \ntest methods). To show this, here I define an example data-driven test:\n# data_driven_test.py\nfrom unittest import TestCase, main\nfrom utils import to_str\n \nclass DataDrivenTestCase(TestCase):\n    def test_good(self):\n        good_cases = [\n            (b'my bytes', 'my bytes'),\n            ('no error', b'no error'),  # This one will fail\n            ('other str', 'other str'),\n            ...\n        ]\n        for value, expected in good_cases:\n            with self.subTest(value):\n                self.assertEqual(expected, to_str(value))\n \n \nItem 76: Verify Related Behaviors in TestCase Subclasses \n363\n\n\n364 \nChapter 9 Testing and Debugging\n    def test_bad(self):\n        bad_cases = [\n            (object(), TypeError),\n            (b'\\xfa\\xfa', UnicodeDecodeError),\n            ...\n        ]\n        for value, exception in bad_cases:\n            with self.subTest(value):\n                with self.assertRaises(exception):\n                    to_str(value)\n \nif __name__ == '__main__':\n    main()\nThe 'no error' test case fails, printing a helpful error message, but \nall of the other cases are still tested and confirmed to pass:\n$ python3 data_driven_test.py\n.\n===============================================================\nFAIL: test_good (__main__.DataDrivenTestCase) [no error]\n---------------------------------------------------------------\nTraceback (most recent call last):\n  File \"testing/data_driven_test.py\", line 18, in test_good\n    self.assertEqual(expected, to_str(value))\nAssertionError: b'no error' != 'no error'\n \n---------------------------------------------------------------\nRan 2 tests in 0.001s\n \nFAILED (failures=1)\nNote\nDepending on your project’s complexity and testing requirements, the pytest \n(https://pytest.org) open source package and its large number of community \nplug-ins can be especially useful.\nThings to Remember\n✦ You can create tests by subclassing the TestCase class from the \nunittest built-in module and defining one method per behavior \nyou’d like to test. Test methods on TestCase classes must start with \nthe word test.\n✦ Use the various helper methods defined by the TestCase class, such \nas assertEqual, to confirm expected behaviors in your tests instead \nof using the built-in assert statement.\n\n\n \nItem 77: Isolate Tests from Each Other \n365\n✦ Consider writing data-driven tests using the subTest helper method \nin order to reduce boilerplate.\nItem 77:  Isolate Tests from Each Other with setUp, \ntearDown, setUpModule, and tearDownModule\nTestCase classes (see Item 76: “Verify Related Behaviors in TestCase \nSubclasses”) often need to have the test environment set up before \ntest methods can be run; this is sometimes called the test harness. \nTo do this, you can override the setUp and tearDown methods of a \nTestCase subclass. These methods are called before and after each \ntest method, respectively, so you can ensure that each test runs in \nisolation, which is an important best practice of proper testing.\nFor example, here I define a TestCase that creates a temporary direc-\ntory before each test and deletes its contents after each test finishes:\n# environment_test.py\nfrom pathlib import Path\nfrom tempfile import TemporaryDirectory\nfrom unittest import TestCase, main\n \nclass EnvironmentTest(TestCase):\n    def setUp(self):\n        self.test_dir = TemporaryDirectory()\n        self.test_path = Path(self.test_dir.name)\n \n    def tearDown(self):\n        self.test_dir.cleanup()\n \n    def test_modify_file(self):\n        with open(self.test_path / 'data.bin', 'w') as f:\n            ...\n \nif __name__ == '__main__':\n    main()\nWhen programs get complicated, you’ll want additional tests to ver-\nify the end-to-end interactions between your modules instead of only \ntesting code in isolation (using tools like mocks; see Item 78: “Use \nMocks to Test Code with Complex Dependencies”). This is the differ-\nence between unit tests and integration tests. In Python, it’s important \nto write both types of tests for exactly the same reason: You have no \nguarantee that your modules will actually work together unless you \nprove it.\n\n\n366 \nChapter 9 Testing and Debugging\nOne common problem is that setting up your test environment for \nintegration tests can be computationally expensive and may require \na lot of wall-clock time. For example, you might need to start a data-\nbase process and wait for it to finish loading indexes before you can \nrun your integration tests. This type of latency makes it impracti-\ncal to do test preparation and cleanup for every test in the TestCase \nclass’s setUp and tearDown methods.\nTo handle this situation, the unittest module also supports \n module-level test harness initialization. You can configure expensive \nresources a single time, and then have all TestCase classes and their \ntest methods run without repeating that initialization. Later, when all \ntests in the module are finished, the test harness can be torn down \na single time. Here, I take advantage of this behavior by defining \nsetUpModule and tearDownModule functions within the module con-\ntaining the TestCase classes:\n# integration_test.py\nfrom unittest import TestCase, main\n \ndef setUpModule():\n    print('* Module setup')\n \ndef tearDownModule():\n    print('* Module clean-up')\n \nclass IntegrationTest(TestCase):\n    def setUp(self):\n        print('* Test setup')\n \n    def tearDown(self):\n        print('* Test clean-up')\n \n    def test_end_to_end1(self):\n        print('* Test 1')\n \n    def test_end_to_end2(self):\n        print('* Test 2')\n \nif __name__ == '__main__':\n    main()\n$ python3 integration_test.py \n* Module setup\n* Test setup\n* Test 1\n\n\n \nItem 78: Use Mocks to Test Code with Complex Dependencies \n367\n* Test clean-up\n.* Test setup\n* Test 2\n* Test clean-up\n.* Module clean-up\n \n---------------------------------------------------------------\nRan 2 tests in 0.000s\n \nOK\nI can clearly see that setUpModule is run by unittest only once, \nand it happens before any setUp methods are called. Similarly, \ntearDownModule happens after the tearDown method is called.\nThings to Remember\n✦ It’s important to write both unit tests (for isolated functionality) and \nintegration tests (for modules that interact with each other).\n✦ Use the setUp and tearDown methods to make sure your tests are \nisolated from each other and have a clean test environment.\n✦ For integration tests, use the setUpModule and tearDownModule \n module-level functions to manage any test harnesses you need for \nthe entire lifetime of a test module and all of the TestCase classes \nthat it contains.\nItem 78:  Use Mocks to Test Code with Complex \nDependencies\nAnother common need when writing tests (see Item 76: “Verify Related \nBehaviors in TestCase Subclasses”) is to use mocked functions and \nclasses to simulate behaviors when it’s too difficult or slow to use the \nreal thing. For example, say that I need a program to maintain the \nfeeding schedule for animals at the zoo. Here, I define a function to \nquery a database for all of the animals of a certain species and return \nwhen they most recently ate:\nclass DatabaseConnection:\n    ...\n \ndef get_animals(database, species):\n    # Query the database\n    ...\n    # Return a list of (name, last_mealtime) tuples\n\n\n368 \nChapter 9 Testing and Debugging\nHow do I get a DatabaseConnection instance to use for testing this \nfunction? Here, I try to create one and pass it into the function being \ntested:\ndatabase = DatabaseConnection('localhost', '4444')\n \nget_animals(database, 'Meerkat')\n>>>\nTraceback ...\nDatabaseConnectionError: Not connected\nThere’s no database running, so of course this fails. One solution is \nto actually stand up a database server and connect to it in the test. \nHowever, it’s a lot of work to fully automate starting up a database, \nconfiguring its schema, populating it with data, and so on in order to \njust run a simple unit test. Further, it will probably take a lot of wall-\nclock time to set up a database server, which would slow down these \nunit tests and make them harder to maintain.\nA better approach is to mock out the database. A mock lets you provide \nexpected responses for dependent functions, given a set of expected \ncalls. It’s important not to confuse mocks with fakes. A fake would \nprovide most of the behavior of the DatabaseConnection class but with \na simpler implementation, such as a basic in-memory, single-threaded \ndatabase with no persistence.\nPython has the unittest.mock built-in module for creating mocks and \nusing them in tests. Here, I define a Mock instance that simulates the \nget_animals function without actually connecting to the database:\nfrom datetime import datetime\nfrom unittest.mock import Mock\n \nmock = Mock(spec=get_animals)\nexpected = [\n    ('Spot', datetime(2019, 6, 5, 11, 15)),\n    ('Fluffy', datetime(2019, 6, 5, 12, 30)),\n    ('Jojo', datetime(2019, 6, 5, 12, 45)),\n]\nmock.return_value = expected\nThe Mock class creates a mock function. The return_value attribute \nof the mock is the value to return when it is called. The spec argu-\nment indicates that the mock should act like the given object, which \nis a function in this case, and error if it’s used in the wrong way. \n",
      "page_number": 377,
      "chapter_number": 36,
      "summary": "This chapter covers segment 36 (pages 377-390). Key topics include tested, python, and printing. Good \ntests give you confidence that your code is correct.",
      "keywords": [
        "Verify Related Behaviors",
        "TestCase",
        "def test",
        "Item",
        "str",
        "main",
        "TestCase Subclasses",
        "test methods",
        "Debugging",
        "Related Behaviors",
        "Verify Related",
        "test.py",
        "Debugging def test",
        "Python",
        "method"
      ],
      "concepts": [
        "tested",
        "python",
        "printing",
        "classes",
        "value",
        "method",
        "data",
        "fails",
        "item",
        "strings"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 11,
          "title": "Segment 11 (pages 90-99)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 14,
          "title": "Testing, Debugging, and Exceptions",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 3,
          "title": "Segment 3 (pages 17-26)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 10,
          "title": "[ 329 ]",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 391-402)",
      "start_page": 391,
      "end_page": 402,
      "detection_method": "topic_boundary",
      "content": " \nItem 78: Use Mocks to Test Code with Complex Dependencies \n369\nFor example, here I try to treat the mock function as if it were a mock \nobject with attributes:\nmock.does_not_exist\n>>>\nTraceback ...\nAttributeError: Mock object has no attribute 'does_not_exist'\nOnce it’s created, I can call the mock, get its return value, and ver-\nify that what it returns matches expectations. I use a unique object \nvalue as the database argument because it won’t actually be used by \nthe mock to do anything; all I care about is that the database param-\neter was correctly plumbed through to any dependent functions that \nneeded a DatabaseConnection instance in order to work (see Item 55: \n“Use Queue to Coordinate Work Between Threads” for another example \nof using sentinel object instances):\ndatabase = object()\nresult = mock(database, 'Meerkat')\nassert result == expected\nThis verifies that the mock responded correctly, but how do I know if \nthe code that called the mock provided the correct arguments? For \nthis, the Mock class provides the assert_called_once_with method, \nwhich verifies that a single call with exactly the given parameters was \nmade:\nmock.assert_called_once_with(database, 'Meerkat')\nIf I supply the wrong parameters, an exception is raised, and any \nTestCase that the assertion is used in fails:\nmock.assert_called_once_with(database, 'Giraffe')\n>>>\nTraceback ...\nAssertionError: expected call not found.\nExpected: mock(<object object at 0x109038790>, 'Giraffe')\nActual: mock(<object object at 0x109038790>, 'Meerkat')\nIf I actually don’t care about some of the individual parameters, such \nas exactly which database object was used, then I can indicate that \nany value is okay for an argument by using the unittest.mock.ANY \nconstant. I can also use the assert_called_with method of Mock to \nverify that the most recent call to the mock—and there may have \nbeen multiple calls in this case—matches my expectations:\nfrom unittest.mock import ANY\n \n\n\n370 \nChapter 9 Testing and Debugging\nmock = Mock(spec=get_animals)\nmock('database 1', 'Rabbit')\nmock('database 2', 'Bison')\nmock('database 3', 'Meerkat')\n \nmock.assert_called_with(ANY, 'Meerkat')\nANY is useful in tests when a parameter is not core to the behavior that’s \nbeing tested. It’s often worth erring on the side of under- specifying \ntests by using ANY more liberally instead of over-specifying tests and \nhaving to plumb through various test parameter expectations.\nThe Mock class also makes it easy to mock exceptions being raised:\nclass MyError(Exception):\n    pass\n \nmock = Mock(spec=get_animals)\nmock.side_effect = MyError('Whoops! Big problem')\nresult = mock(database, 'Meerkat')\n>>>\nTraceback ...\nMyError: Whoops! Big problem\nThere are many more features available, so be sure to see \nhelp(unittest.mock.Mock) for the full range of options.\nNow that I’ve shown the mechanics of how a Mock works, I can apply \nit to an actual testing situation to show how to use it effectively in \nwriting unit tests. Here, I define a function to do the rounds of feeding \nanimals at the zoo, given a set of database-interacting functions:\ndef get_food_period(database, species):\n    # Query the database\n    ...\n    # Return a time delta\n \ndef feed_animal(database, name, when):\n    # Write to the database\n    ...\n \ndef do_rounds(database, species):\n    now = datetime.datetime.utcnow()\n    feeding_timedelta = get_food_period(database, species)\n    animals = get_animals(database, species)\n    fed = 0\n \n\n\n \nItem 78: Use Mocks to Test Code with Complex Dependencies \n371\n    for name, last_mealtime in animals:\n        if (now - last_mealtime) > feeding_timedelta:\n            feed_animal(database, name, now)\n            fed += 1\n \n    return fed\nThe goal of my test is to verify that when do_rounds is run, the right \nanimals got fed, the latest feeding time was recorded to the data-\nbase, and the total number of animals fed returned by the function \nmatches the correct total. In order to do all this, I need to mock out \ndatetime.utcnow so my tests have a stable time that isn’t affected by \ndaylight saving time and other ephemeral changes. I need to mock \nout get_food_period and get_animals to return values that would \nhave come from the database. And I need to mock out feed_animal to \naccept data that would have been written back to the database.\nThe question is: Even if I know how to create these mock functions \nand set expectations, how do I get the do_rounds function that’s \nbeing tested to use the mock dependent functions instead of the \nreal versions? One approach is to inject everything as keyword-only \narguments (see Item 25: “Enforce Clarity with Keyword-Only and \nPositional-Only Arguments”):\ndef do_rounds(database, species, *,\n              now_func=datetime.utcnow,\n              food_func=get_food_period,\n              animals_func=get_animals,\n              feed_func=feed_animal):\n    now = now_func()\n    feeding_timedelta = food_func(database, species)\n    animals = animals_func(database, species)\n    fed = 0\n \n    for name, last_mealtime in animals:\n        if (now - last_mealtime) > feeding_timedelta:\n            feed_func(database, name, now)\n            fed += 1\n \n    return fed\nTo test this function, I need to create all of the Mock instances upfront \nand set their expectations:\nfrom datetime import timedelta\n \n\n\n372 \nChapter 9 Testing and Debugging\nnow_func = Mock(spec=datetime.utcnow)\nnow_func.return_value = datetime(2019, 6, 5, 15, 45)\n \nfood_func = Mock(spec=get_food_period)\nfood_func.return_value = timedelta(hours=3)\n \nanimals_func = Mock(spec=get_animals)\nanimals_func.return_value = [\n    ('Spot', datetime(2019, 6, 5, 11, 15)),\n    ('Fluffy', datetime(2019, 6, 5, 12, 30)),\n    ('Jojo', datetime(2019, 6, 5, 12, 45)),\n]\n \nfeed_func = Mock(spec=feed_animal)\nThen, I can run the test by passing the mocks into the do_rounds \nfunction to override the defaults:\nresult = do_rounds(\n    database,\n    'Meerkat',\n    now_func=now_func,\n    food_func=food_func,\n    animals_func=animals_func,\n    feed_func=feed_func)\n \nassert result == 2\nFinally, I can verify that all of the calls to dependent functions \nmatched my expectations:\nfrom unittest.mock import call\n \nfood_func.assert_called_once_with(database, 'Meerkat')\n \nanimals_func.assert_called_once_with(database, 'Meerkat')\n \nfeed_func.assert_has_calls(\n    [\n        call(database, 'Spot', now_func.return_value),\n        call(database, 'Fluffy', now_func.return_value),\n    ],\n    any_order=True)\nI don’t verify the parameters to the datetime.utcnow mock or how many \ntimes it was called because that’s indirectly verified by the return value \nof the function. For get_food_period and get_animals, I verify a single \ncall with the specified parameters by using assert_called_once_with. \n\n\n \nItem 78: Use Mocks to Test Code with Complex Dependencies \n373\nFor the feed_animal function, I verify that two calls were made—\nand their order didn’t matter—to write to the database using the \nunittest.mock.call helper and the assert_has_calls method.\nThis approach of using keyword-only arguments for injecting mocks \nworks, but it’s quite verbose and requires changing every function \nyou want to test. The unittest.mock.patch family of functions makes \ninjecting mocks easier. It temporarily reassigns an attribute of a mod-\nule or class, such as the database-accessing functions that I defined \nabove. For example, here I override get_animals to be a mock using \npatch:\nfrom unittest.mock import patch\n \nprint('Outside patch:', get_animals)\n \nwith patch('__main__.get_animals'):\n    print('Inside patch: ', get_animals)\n \nprint('Outside again:', get_animals)\n>>>\nOutside patch: <function get_animals at 0x109217040>\nInside patch:  <MagicMock name='get_animals' id='4454622832'>\nOutside again: <function get_animals at 0x109217040>\npatch works for many modules, classes, and attributes. It can be used \nin with statements (see Item 66: “Consider contextlib and with State-\nments for Reusable try/finally Behavior”), as a function decorator \n(see Item 26: “Define Function Decorators with functools.wraps”), or \nin the setUp and tearDown methods of TestCase classes (see Item 76: \n“Verify Related Behaviors in TestCase Subclasses”). For the full range \nof options, see help(unittest.mock.patch).\nHowever, patch doesn’t work in all cases. For example, to test do_rounds \nI need to mock out the current time returned by the datetime.utcnow \nclass method. Python won’t let me do that because the datetime class is \ndefined in a C-extension module, which can’t be modified in this way:\nfake_now = datetime(2019, 6, 5, 15, 45)\n \nwith patch('datetime.datetime.utcnow'):\n    datetime.utcnow.return_value = fake_now\n>>>\nTraceback ...\nTypeError: can't set attributes of built-in/extension type \n¯'datetime.datetime'\n\n\n374 \nChapter 9 Testing and Debugging\nTo work around this, I can create another helper function to fetch \ntime that can be patched:\ndef get_do_rounds_time():\n    return datetime.datetime.utcnow()\n \ndef do_rounds(database, species):\n    now = get_do_rounds_time()\n    ...\n \nwith patch('__main__.get_do_rounds_time'):\n    ...\nAlternatively, \nI \ncan \nuse \na \nkeyword-only \nargument \nfor \nthe \ndatetime.utcnow mock and use patch for all of the other mocks:\ndef do_rounds(database, species, *, utcnow=datetime.utcnow):\n    now = utcnow()\n    feeding_timedelta = get_food_period(database, species)\n    animals = get_animals(database, species)\n    fed = 0\n \n    for name, last_mealtime in animals:\n        if (now - last_mealtime) > feeding_timedelta:\n            feed_func(database, name, now)\n            fed += 1\n \n    return fed\nI’m going to go with the latter approach. Now, I can use the \npatch.multiple function to create many mocks and set their \nexpectations:\nfrom unittest.mock import DEFAULT\n \nwith patch.multiple('__main__',\n                    autospec=True,\n                    get_food_period=DEFAULT,\n                    get_animals=DEFAULT,\n                    feed_animal=DEFAULT):\n    now_func = Mock(spec=datetime.utcnow)\n    now_func.return_value = datetime(2019, 6, 5, 15, 45)\n    get_food_period.return_value = timedelta(hours=3)\n    get_animals.return_value = [\n        ('Spot', datetime(2019, 6, 5, 11, 15)),\n        ('Fluffy', datetime(2019, 6, 5, 12, 30)),\n        ('Jojo', datetime(2019, 6, 5, 12, 45))\n    ]\n\n\n Item 79: Encapsulate Dependencies to Facilitate Mocking and Testing \n375\nWith the setup ready, I can run the test and verify that the calls were \ncorrect inside the with statement that used patch.multiple:\n    result = do_rounds(database, 'Meerkat', utcnow=now_func)\n    assert result == 2\n \n    food_func.assert_called_once_with(database, 'Meerkat')\n    animals_func.assert_called_once_with(database, 'Meerkat')\n    feed_func.assert_has_calls(\n        [\n            call(database, 'Spot', now_func.return_value),\n            call(database, 'Fluffy', now_func.return_value),\n        ],\n        any_order=True)\nThe keyword arguments to patch.multiple correspond to names in the \n__main__ module that I want to override during the test. The DEFAULT \nvalue indicates that I want a standard Mock instance to be created for \neach name. All of the generated mocks will adhere to the specification \nof the objects they are meant to simulate, thanks to the autospec=True \nparameter.\nThese mocks work as expected, but it’s important to realize that it’s \npossible to further improve the readability of these tests and reduce \nboilerplate by refactoring your code to be more testable (see Item 79: \n“Encapsulate Dependencies to Facilitate Mocking and Testing”).\nThings to Remember\n✦ The unittest.mock module provides a way to simulate the behavior \nof interfaces using the Mock class. Mocks are useful in tests when \nit’s difficult to set up the dependencies that are required by the code \nthat’s being tested.\n✦ When using mocks, it’s important to verify both the behavior of the \ncode being tested and how dependent functions were called by that \ncode, using the Mock.assert_called_once_with family of methods.\n✦ Keyword-only arguments and the unittest.mock.patch family of \nfunctions can be used to inject mocks into the code being tested.\nItem 79:  Encapsulate Dependencies to Facilitate \nMocking and Testing\nIn the previous item (see Item 78: “Use Mocks to Test Code with \nComplex Dependencies”), I showed how to use the facilities of the \nunittest.mock built-in module—including the Mock class and patch \n\n\n376 \nChapter 9 Testing and Debugging\nfamily of functions—to write tests that have complex dependencies, \nsuch as a database. However, the resulting test code requires a lot of \nboilerplate, which could make it more difficult for new readers of the \ncode to understand what the tests are trying to verify.\nOne way to improve these tests is to use a wrapper object to encapsu-\nlate the database’s interface instead of passing a DatabaseConnection \nobject to functions as an argument. It’s often worth refactoring your \ncode (see Item 89: “Consider warnings to Refactor and Migrate Usage” \nfor one approach) to use better abstractions because it facilitates cre-\nating mocks and writing tests. Here, I redefine the various database \nhelper functions from the previous item as methods on a class instead \nof as independent functions:\nclass ZooDatabase:\n    ...\n \n    def get_animals(self, species):\n        ...\n \n    def get_food_period(self, species):\n        ...\n \n    def feed_animal(self, name, when):\n        ...\nNow, I can redefine the do_rounds function to call methods on a \nZooDatabase object:\nfrom datetime import datetime\n \ndef do_rounds(database, species, *, utcnow=datetime.utcnow):\n    now = utcnow()\n    feeding_timedelta = database.get_food_period(species)\n    animals = database.get_animals(species)\n    fed = 0\n \n    for name, last_mealtime in animals:\n        if (now - last_mealtime) >= feeding_timedelta:\n            database.feed_animal(name, now)\n            fed += 1\n \n    return fed\nWriting a test for do_rounds is now a lot easier because I no longer \nneed to use unittest.mock.patch to inject the mock into the code \nbeing tested. Instead, I can create a Mock instance to represent \n\n\n Item 79: Encapsulate Dependencies to Facilitate Mocking and Testing \n377\na ZooDatabase and pass that in as the database parameter. The Mock \nclass returns a mock object for any attribute name that is accessed. \nThose attributes can be called like methods, which I can then use to \nset expectations and verify calls. This makes it easy to mock out all of \nthe methods of a class:\nfrom unittest.mock import Mock\n \ndatabase = Mock(spec=ZooDatabase)\nprint(database.feed_animal)\ndatabase.feed_animal()\ndatabase.feed_animal.assert_any_call()\n>>>\n<Mock name='mock.feed_animal' id='4384773408'>\nI can rewrite the Mock setup code by using the ZooDatabase \nencapsulation:\nfrom datetime import timedelta\nfrom unittest.mock import call\n \nnow_func = Mock(spec=datetime.utcnow)\nnow_func.return_value = datetime(2019, 6, 5, 15, 45)\n \ndatabase = Mock(spec=ZooDatabase)\ndatabase.get_food_period.return_value = timedelta(hours=3)\ndatabase.get_animals.return_value = [\n    ('Spot', datetime(2019, 6, 5, 11, 15)),\n    ('Fluffy', datetime(2019, 6, 5, 12, 30)),\n    ('Jojo', datetime(2019, 6, 5, 12, 55))\n]\nThen I can run the function being tested and verify that all depen-\ndent methods were called as expected:\nresult = do_rounds(database, 'Meerkat', utcnow=now_func)\nassert result == 2\n \ndatabase.get_food_period.assert_called_once_with('Meerkat')\ndatabase.get_animals.assert_called_once_with('Meerkat')\ndatabase.feed_animal.assert_has_calls(\n    [\n        call('Spot', now_func.return_value),\n        call('Fluffy', now_func.return_value),\n    ],\n    any_order=True)\n\n\n378 \nChapter 9 Testing and Debugging\nUsing the spec parameter to Mock is especially useful when mocking \nclasses because it ensures that the code under test doesn’t call a mis-\nspelled method name by accident. This allows you to avoid a common \npitfall where the same bug is present in both the code and the unit \ntest, masking a real error that will later reveal itself in production:\ndatabase.bad_method_name()\n>>>\nTraceback ...\nAttributeError: Mock object has no attribute 'bad_method_name'\nIf I want to test this program end-to-end with a mid-level integration \ntest (see Item 77: “Isolate Tests from Each Other with setUp, tearDown, \nsetUpModule, and tearDownModule”), I still need a way to inject a mock \nZooDatabase into the program. I can do this by creating a helper func-\ntion that acts as a seam for dependency injection. Here, I define such \na helper function that caches a ZooDatabase in module scope (see Item \n86: “Consider Module-Scoped Code to Configure Deployment Envi-\nronments”) by using a global statement:\nDATABASE = None\n \ndef get_database():\n    global DATABASE\n    if DATABASE is None:\n        DATABASE = ZooDatabase()\n    return DATABASE\n \ndef main(argv):\n    database = get_database()\n    species = argv[1]\n    count = do_rounds(database, species)\n    print(f'Fed {count} {species}(s)')\n    return 0\nNow, I can inject the mock ZooDatabase using patch, run the test, and \nverify the program’s output. I’m not using a mock datetime.utcnow \nhere; instead, I’m relying on the database records returned by the \nmock to be relative to the current time in order to produce similar \nbehavior to the unit test. This approach is more flaky than mocking \neverything, but it also tests more surface area:\nimport contextlib\nimport io\nfrom unittest.mock import patch\n \n\n\n \nItem 80: Consider Interactive Debugging with pdb \n379\nwith patch('__main__.DATABASE', spec=ZooDatabase):\n    now = datetime.utcnow()\n \n    DATABASE.get_food_period.return_value = timedelta(hours=3)\n    DATABASE.get_animals.return_value = [\n        ('Spot', now - timedelta(minutes=4.5)),\n        ('Fluffy', now - timedelta(hours=3.25)),\n        ('Jojo', now - timedelta(hours=3)),\n    ]\n \n    fake_stdout = io.StringIO()\n    with contextlib.redirect_stdout(fake_stdout):\n        main(['program name', 'Meerkat'])\n \n    found = fake_stdout.getvalue()\n    expected = 'Fed 2 Meerkat(s)\\n'\n \n    assert found == expected\nThe results match my expectations. Creating this integration test was \nstraightforward because I designed the implementation to make it \neasier to test.\nThings to Remember\n✦ When unit tests require a lot of repeated boilerplate to set up mocks, \none solution may be to encapsulate the functionality of dependen-\ncies into classes that are more easily mocked.\n✦ The Mock class of the unittest.mock built-in module simulates \nclasses by returning a new mock, which can act as a mock method, \nfor each attribute that is accessed.\n✦ For end-to-end tests, it’s valuable to refactor your code to have more \nhelper functions that can act as explicit seams to use for injecting \nmock dependencies in tests.\nItem 80: Consider Interactive Debugging with pdb\nEveryone encounters bugs in code while developing programs. Using \nthe print function can help you track down the sources of many \nissues (see Item 75: “Use repr Strings for Debugging Output”). \n Writing tests for specific cases that cause trouble is another great way \nto isolate problems (see Item 76: “Verify Related Behaviors in TestCase \nSubclasses”).\n\n\n380 \nChapter 9 Testing and Debugging\nBut these tools aren’t enough to find every root cause. When you need \nsomething more powerful, it’s time to try Python’s built-in interactive \ndebugger. The debugger lets you inspect program state, print local \nvariables, and step through a Python program one statement at a time.\nIn most other programming languages, you use a debugger by spec-\nifying what line of a source file you’d like to stop on, and then exe-\ncute the program. In contrast, with Python, the easiest way to use \nthe debugger is by modifying your program to directly initiate the \ndebugger just before you think you’ll have an issue worth investigat-\ning. This means that there is no difference between starting a Python \nprogram in order to run the debugger and starting it normally.\nTo initiate the debugger, all you have to do is call the breakpoint \nbuilt-in function. This is equivalent to importing the pdb built-in mod-\nule and running its set_trace function:\n# always_breakpoint.py\nimport math\n \ndef compute_rmse(observed, ideal):\n    total_err_2 = 0\n    count = 0\n    for got, wanted in zip(observed, ideal):\n        err_2 = (got - wanted) ** 2\n        breakpoint()  # Start the debugger here\n        total_err_2 += err_2\n        count += 1\n \n    mean_err = total_err_2 / count\n    rmse = math.sqrt(mean_err)\n    return rmse\n \nresult = compute_rmse(\n    [1.8, 1.7, 3.2, 6],\n    [2, 1.5, 3, 5])\nprint(result)\nAs soon as the breakpoint function runs, the program pauses its exe-\ncution before the line of code immediately following the breakpoint \ncall. The terminal that started the program turns into an interactive \nPython shell:\n$ python3 always_breakpoint.py \n> always_breakpoint.py(12)compute_rmse()\n-> total_err_2 += err_2\n(Pdb)\n",
      "page_number": 391,
      "chapter_number": 37,
      "summary": "This chapter covers segment 37 (pages 391-402). Key topics include mocks, animals, and function. Covers function. Here, I try to create one and pass it into the function being \ntested:\ndatabase = DatabaseConnection('localhost', '4444')\n \nget_animals(database, 'Meerkat')\n>>>\nTraceback.",
      "keywords": [
        "database",
        "mocks",
        "animals",
        "Meerkat",
        "Item",
        "Mock class",
        "function",
        "Code",
        "datetime",
        "mock object",
        "food",
        "species",
        "functions"
      ],
      "concepts": [
        "mocks",
        "animals",
        "function",
        "functions",
        "functionality",
        "patch",
        "item",
        "classes",
        "expected",
        "expectations"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 8",
          "chapter": 40,
          "title": "Segment 40 (pages 368-375)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 11,
          "title": "Segment 11 (pages 82-89)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 19,
          "title": "Segment 19 (pages 167-175)",
          "relevance_score": 0.43,
          "method": "api"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 14,
          "title": "Testing, Debugging, and Exceptions",
          "relevance_score": 0.43,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 14,
          "title": "Segment 14 (pages 116-123)",
          "relevance_score": 0.43,
          "method": "api"
        }
      ]
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 403-410)",
      "start_page": 403,
      "end_page": 410,
      "detection_method": "topic_boundary",
      "content": "At the (Pdb) prompt, you can type in the names of local variables to \nsee their values printed out (or use p <name>). You can see a list of all \nlocal variables by calling the locals built-in function. You can import \nmodules, inspect global state, construct new objects, run the help \nbuilt-in function, and even modify parts of the running program—\nwhatever you need to do to aid in your debugging. \nIn addition, the debugger has a variety of special commands to con-\ntrol and understand program execution; type help to see the full list.\nThree very useful commands make inspecting the running program \neasier:\n \n■where: Print the current execution call stack. This lets you figure \nout where you are in your program and how you arrived at the \nbreakpoint trigger.\n \n■up: Move your scope up the execution call stack to the caller of the \ncurrent function. This allows you to inspect the local variables in \nhigher levels of the program that led to the breakpoint.\n \n■down: Move your scope back down the execution call stack one \nlevel.\nWhen you’re done inspecting the current state, you can use these five \ndebugger commands to control the program’s execution in different \nways:\n \n■step: Run the program until the next line of execution in the pro-\ngram, and then return control back to the debugger prompt. If the \nnext line of execution includes calling a function, the debugger \nstops within the function that was called.\n \n■next: Run the program until the next line of execution in the \ncurrent function, and then return control back to the debugger \nprompt. If the next line of execution includes calling a function, \nthe debugger will not stop until the called function has returned.\n \n■return: Run the program until the current function returns, and \nthen return control back to the debugger prompt.\n \n■continue: Continue running the program until the next break-\npoint is hit (either through the breakpoint call or one added by a \ndebugger command).\n \n■quit: Exit the debugger and end the program. Run this command \nif you’ve found the problem, gone too far, or need to make program \nmodifications and try again.\n \nItem 80: Consider Interactive Debugging with pdb \n381\n\n\n382 \nChapter 9 Testing and Debugging\nThe breakpoint function can be called anywhere in a program. If you \nknow that the problem you’re trying to debug happens only under \nspecial circumstances, then you can just write plain old Python \ncode to call breakpoint after a specific condition is met. For example, \nhere I start the debugger only if the squared error for a datapoint is \nmore than 1:\n# conditional_breakpoint.py\ndef compute_rmse(observed, ideal):\n    ...\n    for got, wanted in zip(observed, ideal):\n        err_2 = (got - wanted) ** 2\n        if err_2 >= 1:  # Start the debugger if True\n            breakpoint()\n        total_err_2 += err_2\n        count += 1\n    ...\nresult = compute_rmse(\n    [1.8, 1.7, 3.2, 7],\n    [2, 1.5, 3, 5])\nprint(result)\nWhen I run the program and it enters the debugger, I can confirm \nthat the condition was true by inspecting local variables:\n$ python3 conditional_breakpoint.py \n> conditional_breakpoint.py(14)compute_rmse()\n-> total_err_2 += err_2\n(Pdb) wanted\n5\n(Pdb) got\n7\n(Pdb) err_2\n4\nAnother useful way to reach the debugger prompt is by using post- \nmortem debugging. This enables you to debug a program after it’s \nalready raised an exception and crashed. This is especially helpful \nwhen you’re not quite sure where to put the breakpoint function call.\nHere, I have a script that will crash due to the 7j complex number \nbeing present in one of the function’s arguments:\n# postmortem_breakpoint.py\nimport math\n \ndef compute_rmse(observed, ideal):\n    ...\n \n\n\nresult = compute_rmse(\n    [1.8, 1.7, 3.2, 7j],  # Bad input\n    [2, 1.5, 3, 5])\nprint(result)\nI use the command line python3 -m pdb -c continue <program path> \nto run the program under control of the pdb module. The continue \ncommand tells pdb to get the program started immediately. Once it’s \nrunning, the program hits a problem and automatically enters the \ninteractive debugger, at which point I can inspect the program state:\n$ python3 -m pdb -c continue postmortem_breakpoint.py \nTraceback (most recent call last):\n  File \".../pdb.py\", line 1697, in main\n    pdb._runscript(mainpyfile)\n  File \".../pdb.py\", line 1566, in _runscript\n    self.run(statement)\n  File \".../bdb.py\", line 585, in run\n    exec(cmd, globals, locals)\n  File \"<string>\", line 1, in <module>\n  File \"postmortem_breakpoint.py\", line 4, in <module>\n    import math\n  File \"postmortem_breakpoint.py\", line 16, in compute_rmse\n    rmse = math.sqrt(mean_err)\nTypeError: can't convert complex to float\nUncaught exception. Entering post mortem debugging\nRunning 'cont' or 'step' will restart the program\n> postmortem_breakpoint.py(16)compute_rmse()\n-> rmse = math.sqrt(mean_err)\n(Pdb) mean_err\n(-5.97-17.5j)\nYou can also use post-mortem debugging after hitting an uncaught \nexception in the interactive Python interpreter by calling the pm \nfunction of the pdb module (which is often done in a single line as \nimport pdb; pdb.pm()):\n$ python3\n>>> import my_module\n>>> my_module.compute_stddev([5])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"my_module.py\", line 17, in compute_stddev\n    variance = compute_variance(data)\n  File \"my_module.py\", line 13, in compute_variance\n    variance = err_2_sum / (len(data) - 1)\n \nItem 80: Consider Interactive Debugging with pdb \n383\n\n\n384 \nChapter 9 Testing and Debugging\nZeroDivisionError: float division by zero\n>>> import pdb; pdb.pm()\n> my_module.py(13)compute_variance()\n-> variance = err_2_sum / (len(data) - 1)\n(Pdb) err_2_sum\n0.0\n(Pdb) len(data)\n1\nThings to Remember\n✦ You can initiate the Python interactive debugger at a point of inter-\nest directly in your program by calling the breakpoint built-in \nfunction.\n✦ The Python debugger prompt is a full Python shell that lets you \ninspect and modify the state of a running program.\n✦ pdb shell commands let you precisely control program execution \nand allow you to alternate between inspecting program state and \nprogressing program execution.\n✦ The pdb module can be used for debug exceptions after they \nhappen in independent Python programs (using python -m pdb -c \ncontinue <program path>) or the interactive Python interpreter (using \nimport pdb; pdb.pm()).\nItem 81:  Use tracemalloc to Understand Memory \nUsage and Leaks\nMemory management in the default implementation of Python, \n CPython, uses reference counting. This ensures that as soon as all \nreferences to an object have expired, the referenced object is also \ncleared from memory, freeing up that space for other data. CPython \nalso has a built-in cycle detector to ensure that self-referencing objects \nare eventually garbage collected.\nIn theory, this means that most Python programmers don’t have to \nworry about allocating or deallocating memory in their programs. It’s \ntaken care of automatically by the language and the CPython run-\ntime. However, in practice, programs eventually do run out of mem-\nory due to no longer useful references still being held. Figuring out \nwhere a Python program is using or leaking memory proves to be a \nchallenge.\nThe first way to debug memory usage is to ask the gc built-in module \nto list every object currently known by the garbage collector. Although \n\n\n \nItem 81: Use tracemalloc to Understand Memory Usage and Leaks \n385\nit’s quite a blunt tool, this approach lets you quickly get a sense of \nwhere your program’s memory is being used.\nHere, I define a module that fills up memory by keeping references:\n# waste_memory.py\nimport os\n \nclass MyObject:\n    def __init__(self):\n        self.data = os.urandom(100)\n \ndef get_data():\n    values = []\n    for _ in range(100):\n        obj = MyObject()\n        values.append(obj)\n    return values\n \ndef run():\n    deep_values = []\n    for _ in range(100):\n        deep_values.append(get_data())\n    return deep_values\nThen, I run a program that uses the gc built-in module to print out \nhow many objects were created during execution, along with a small \nsample of allocated objects:\n# using_gc.py\nimport gc\n \nfound_objects = gc.get_objects()\nprint('Before:', len(found_objects))\n \nimport waste_memory\n \nhold_reference = waste_memory.run()\n \nfound_objects = gc.get_objects()\nprint('After: ', len(found_objects))\nfor obj in found_objects[:3]:\n    print(repr(obj)[:100])\n>>>\nBefore: 6207\nAfter:  16801\n\n\n386 \nChapter 9 Testing and Debugging\n<waste_memory.MyObject object at 0x10390aeb8>\n<waste_memory.MyObject object at 0x10390aef0>\n<waste_memory.MyObject object at 0x10390af28>\n...\nThe problem with gc.get_objects is that it doesn’t tell you anything \nabout how the objects were allocated. In complicated programs, \nobjects of a specific class could be allocated many different ways. \nKnowing the overall number of objects isn’t nearly as important as \nidentifying the code responsible for allocating the objects that are \nleaking memory.\nPython 3.4 introduced a new tracemalloc built-in module for solving \nthis problem. tracemalloc makes it possible to connect an object back \nto where it was allocated. You use it by taking before and after snap-\nshots of memory usage and comparing them to see what’s changed. \nHere, I use this approach to print out the top three memory usage \noffenders in a program:\n# top_n.py\nimport tracemalloc\n \ntracemalloc.start(10)                      # Set stack depth\ntime1 = tracemalloc.take_snapshot()        # Before snapshot\n \nimport waste_memory\n \nx = waste_memory.run()                     # Usage to debug\ntime2 = tracemalloc.take_snapshot()        # After snapshot\n \nstats = time2.compare_to(time1, 'lineno')  # Compare snapshots\nfor stat in stats[:3]:\n    print(stat)\n>>>\nwaste_memory.py:5: size=2392 KiB (+2392 KiB), count=29994 \n¯(+29994), average=82 B\nwaste_memory.py:10: size=547 KiB (+547 KiB), count=10001 \n¯(+10001), average=56 B\nwaste_memory.py:11: size=82.8 KiB (+82.8 KiB), count=100 \n¯(+100), average=848 B\nThe size and count labels in the output make it immediately clear \nwhich objects are dominating my program’s memory usage and where \nin the source code they were allocated.\n\n\nThe tracemalloc module can also print out the full stack trace of each \nallocation (up to the number of frames passed to the tracemalloc.start \nfunction). Here, I print out the stack trace of the biggest source of \nmemory usage in the program:\n# with_trace.py\nimport tracemalloc\n \ntracemalloc.start(10)\ntime1 = tracemalloc.take_snapshot()\n \nimport waste_memory\n \nx = waste_memory.run()\ntime2 = tracemalloc.take_snapshot()\n \nstats = time2.compare_to(time1, 'traceback')\ntop = stats[0]\nprint('Biggest offender is:')\nprint('\\n'.join(top.traceback.format()))\n>>>\nBiggest offender is:\n  File \"with_trace.py\", line 9\n    x = waste_memory.run()\n  File \"waste_memory.py\", line 17\n    deep_values.append(get_data())\n  File \"waste_memory.py\", line 10\n    obj = MyObject()\n  File \"waste_memory.py\", line 5\n    self.data = os.urandom(100)\nA stack trace like this is most valuable for figuring out which partic-\nular usage of a common function or class is responsible for memory \nconsumption in a program.\nThings to Remember\n✦ It can be difficult to understand how Python programs use and leak \nmemory.\n✦ The gc module can help you understand which objects exist, but it \nhas no information about how they were allocated.\n✦ The tracemalloc built-in module provides powerful tools for under-\nstanding the sources of memory usage.\n \nItem 81: Use tracemalloc to Understand Memory Usage and Leaks \n387\n\n\nThis page intentionally left blank \n",
      "page_number": 403,
      "chapter_number": 38,
      "summary": "This chapter covers segment 38 (pages 403-410). Key topics include program, programming, and python. Covers function. When you need \nsomething more powerful, it’s time to try Python’s built-in interactive \ndebugger.",
      "keywords": [
        "program",
        "Python",
        "pdb",
        "debugger",
        "Memory",
        "err",
        "Memory Usage",
        "line",
        "Python program",
        "function",
        "objects",
        "waste",
        "run",
        "compute",
        "Understand Memory Usage"
      ],
      "concepts": [
        "program",
        "programming",
        "python",
        "pdb",
        "run",
        "running",
        "runs",
        "file",
        "memory",
        "objects"
      ],
      "similar_chapters": [
        {
          "book": "Python Microservices Development",
          "chapter": 22,
          "title": "Segment 22 (pages 175-182)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 11,
          "title": "Segment 11 (pages 90-99)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "makinggames",
          "chapter": 8,
          "title": "Segment 8 (pages 60-68)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 16,
          "title": "Segment 16 (pages 308-330)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 13,
          "title": "Segment 13 (pages 250-271)",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 411-419)",
      "start_page": 411,
      "end_page": 419,
      "detection_method": "topic_boundary",
      "content": "10\nCollaboration\nPython has language features that help you construct well-defined \nAPIs with clear interface boundaries. The Python community has \nestablished best practices to maximize the maintainability of code \nover time. In addition, some standard tools that ship with Python \nenable large teams to work together across disparate environments.\nCollaborating with others on Python programs requires being \n deliberate in how you write your code. Even if you’re working on your \nown, chances are you’ll be using code written by someone else via the \nstandard library or open source packages. It’s important to under-\nstand the mechanisms that make it easy to collaborate with other \nPython programmers.\nItem 82:  Know Where to Find Community-Built \nModules\nPython has a central repository of modules (https://pypi.org) that you \ncan install and use in your programs. These modules are built and \nmaintained by people like you: the Python community. When you find \nyourself facing an unfamiliar challenge, the Python Package Index \n(PyPI) is a great place to look for code that will get you closer to your \ngoal.\nTo use the Package Index, you need to use the command-line tool pip \n(a recursive acronym for “pip installs packages”). pip can be run with \npython3 -m pip to ensure that packages are installed for the correct \nversion of Python on your system (see Item 1: “Know Which Version of \nPython You’re Using”). Using pip to install a new module is simple. For \nexample, here I install the pytz module that I use elsewhere in this \nbook (see Item 67: “Use datetime Instead of time for Local Clocks”):\n$ python3 -m pip install pytz\nCollecting pytz\n\n\n390 \nChapter 10 Collaboration\n  Downloading ... \nInstalling collected packages: pytz\nSuccessfully installed pytz-2018.9\npip is best used together with the built-in module venv to consistently \ntrack sets of packages to install for your projects (see Item 83: “Use \nVirtual Environments for Isolated and Reproducible Dependencies”). \nYou can also create your own PyPI packages to share with the Python \ncommunity or host your own private package repositories for use \nwith pip.\nEach module in the PyPI has its own software license. Most of the \npackages, especially the popular ones, have free or open source \nlicenses (see https://opensource.org for details). In most cases, these \nlicenses allow you to include a copy of the module with your program; \nwhen in doubt, talk to a lawyer.\nThings to Remember\n✦ The Python Package Index (PyPI) contains a wealth of common \npackages that are built and maintained by the Python community.\n✦ pip is the command-line tool you can use to install packages \nfrom PyPI.\n✦ The majority of PyPI modules are free and open source software.\nItem 83:  Use Virtual Environments for Isolated and \nReproducible Dependencies\nBuilding larger and more complex programs often leads you to rely on \nvarious packages from the Python community (see Item 82: “Know \nWhere to Find Community-Built Modules”). You’ll find yourself run-\nning the python3 -m pip command-line tool to install packages like \npytz, numpy, and many others.\nThe problem is that, by default, pip installs new packages in a global \nlocation. That causes all Python programs on your system to be \naffected by these installed modules. In theory, this shouldn’t be an \nissue. If you install a package and never import it, how could it affect \nyour programs?\nThe trouble comes from transitive dependencies: the packages \nthat the packages you install depend on. For example, you can see \nwhat the Sphinx package depends on after installing it by asking pip:\n$ python3 -m pip show Sphinx\nName: Sphinx\n\n\n \nItem 83: Use Virtual Environments for Isolated Dependencies \n391\nVersion: 2.1.2\nSummary: Python documentation generator\nLocation: /usr/local/lib/python3.8/site-packages\nRequires: alabaster, imagesize, requests, \n¯sphinxcontrib-applehelp, sphinxcontrib-qthelp, \n¯Jinja2, setuptools, sphinxcontrib-jsmath, \n¯sphinxcontrib-serializinghtml, Pygments, snowballstemmer, \n¯packaging, sphinxcontrib-devhelp, sphinxcontrib-htmlhelp, \n¯babel, docutils\nRequired-by:\nIf you install another package like flask, you can see that it, too, \ndepends on the Jinja2 package:\n$ python3 -m pip show flask\nName: Flask\nVersion: 1.0.3\nSummary: A simple framework for building complex web applications.\nLocation: /usr/local/lib/python3.8/site-packages\nRequires: itsdangerous, click, Jinja2, Werkzeug\nRequired-by:\nA dependency conflict can arise as Sphinx and flask diverge over \ntime. Perhaps right now they both require the same version of \nJinja2, and everything is fine. But six months or a year from now, \nJinja2 may release a new version that makes breaking changes \nto users of the library. If you update your global version of Jinja2 \nwith python3 -m pip install --upgrade Jinja2, you may find that Sphinx \nbreaks, while flask keeps working.\nThe cause of such breakage is that Python can have only a single \nglobal version of a module installed at a time. If one of your installed \npackages must use the new version and another package must use \nthe old version, your system isn’t going to work properly; this situa-\ntion is often called dependency hell.\nSuch breakage can even happen when package maintainers try their \nbest to preserve API compatibility between releases (see Item 85: \n“Use Packages to Organize Modules and Provide Stable APIs”). New \n versions of a library can subtly change behaviors that API-consuming \ncode relies on. Users on a system may upgrade one package to a new \nversion but not others, which could break dependencies. If you’re not \ncareful there’s a constant risk of the ground moving beneath your feet.\nThese difficulties are magnified when you collaborate with other \ndevelopers who do their work on separate computers. It’s best to \nassume the worst: that the versions of Python and global packages \n\n\n392 \nChapter 10 Collaboration\nthat they have installed on their machines will be slightly different \nfrom yours. This can cause frustrating situations such as a codebase \nworking perfectly on one programmer’s machine and being completely \nbroken on another’s.\nThe solution to all of these problems is using a tool called venv, which \nprovides virtual environments. Since Python 3.4, pip and the venv \nmodule have been available by default along with the Python installa-\ntion (accessible with python -m venv).\nvenv allows you to create isolated versions of the Python environment. \nUsing venv, you can have many different versions of the same package \ninstalled on the same system at the same time without conflicts. This \nmeans you can work on many different projects and use many differ-\nent tools on the same computer. venv does this by installing explicit \nversions of packages and their dependencies into completely separate \ndirectory structures. This makes it possible to reproduce a Python \nenvironment that you know will work with your code. It’s a reliable \nway to avoid surprising breakages.\nUsing venv on the Command Line\nHere’s a quick tutorial on how to use venv effectively. Before using \nthe tool, it’s important to note the meaning of the python3 command \nline on your system. On my computer, python3 is located in the \n/usr/local/bin directory and evaluates to version 3.8.0 (see Item 1: \n“Know Which Version of Python You’re Using”):\n$ which python3\n/usr/local/bin/python3\n$ python3 --version\nPython 3.8.0\nTo demonstrate the setup of my environment, I can test that running \na command to import the pytz module doesn’t cause an error. This \nworks because I already have the pytz package installed as a global \nmodule:\n$ python3 -c 'import pytz'\n$\nNow, I use venv to create a new virtual environment called myproject. \nEach virtual environment must live in its own unique directory. The \nresult of the command is a tree of directories and files that are used \nto manage the virtual environment:\n$ python3 -m venv myproject\n$ cd myproject\n\n\n$ ls\nbin     include     lib     pyvenv.cfg\nTo start using the virtual environment, I use the source command \nfrom my shell on the bin/activate script. activate modifies all of \nmy environment variables to match the virtual environment. It also \nupdates my command-line prompt to include the virtual environment \nname (“myproject”) to make it extremely clear what I’m working on:\n$ source bin/activate\n(myproject)$\nOn Windows the same script is available as:\nC:\\> myproject\\Scripts\\activate.bat\n(myproject) C:>\nOr with PowerShell as:\nPS C:\\> myproject\\Scripts\\activate.ps1\n(myproject) PS C:>\nAfter activation, the path to the python3  command-line tool has moved \nto within the virtual environment directory:\n(myproject)$ which python3\n/tmp/myproject/bin/python3\n(myproject)$ ls -l /tmp/myproject/bin/python3\n... -> /usr/local/bin/python3.8\nThis ensures that changes to the outside system will not affect the \nvirtual environment. Even if the outer system upgrades its default \npython3 to version 3.9, my virtual environment will still explicitly \npoint to version 3.8.\nThe virtual environment I created with venv starts with no packages \ninstalled except for pip and setuptools. Trying to use the pytz pack-\nage that was installed as a global module in the outside system will \nfail because it’s unknown to the virtual environment:\n(myproject)$ python3 -c 'import pytz'\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nModuleNotFoundError: No module named 'pytz'\nI can use the pip command-line tool to install the pytz module into \nmy virtual environment:\n(myproject)$ python3 -m pip install pytz\nCollecting pytz\n \nItem 83: Use Virtual Environments for Isolated Dependencies \n393\n\n\n394 \nChapter 10 Collaboration\n  Downloading ... \nInstalling collected packages: pytz\nSuccessfully installed pytz-2019.1\nOnce it’s installed, I can verify that it’s working by using the same \ntest import command:\n(myproject)$ python3 -c 'import pytz'\n(myproject)$\nWhen I’m done with a virtual environment and want to go back to my \ndefault system, I use the deactivate command. This restores my envi-\nronment to the system defaults, including the location of the python3 \ncommand-line tool:\n(myproject)$ which python3\n/tmp/myproject/bin/python3\n(myproject)$ deactivate\n$ which python3\n/usr/local/bin/python3\nIf I ever want to work in the myproject environment again, I can just \nrun source bin/activate in the directory as before.\nReproducing Dependencies\nOnce you are in a virtual environment, you can continue installing \npackages in it with pip as you need them. Eventually, you might want \nto copy your environment somewhere else. For example, say that I \nwant to reproduce the development environment from my workstation \non a server in a datacenter. Or maybe I want to clone someone else’s \nenvironment on my own machine so I can help debug their code.\nvenv makes such tasks easy. I can use the python3 -m pip freeze \ncommand to save all of my explicit package dependencies into a file \n(which, by convention, is named requirements.txt):\n(myproject)$ python3 -m pip freeze > requirements.txt\n(myproject)$ cat requirements.txt\ncertifi==2019.3.9\nchardet==3.0.4\nidna==2.8\nnumpy==1.16.2\npytz==2018.9\nrequests==2.21.0\nurllib3==1.24.1\n\n\nNow, imagine that I’d like to have another virtual environment that \nmatches the myproject environment. I can create a new directory as \nbefore by using venv and activate it:\n$ python3 -m venv otherproject\n$ cd otherproject\n$ source bin/activate\n(otherproject)$\nThe new environment will have no extra packages installed:\n(otherproject)$ python3 -m pip list\nPackage    Version\n---------- -------\npip        10.0.1 \nsetuptools 39.0.1\nI can install all of the packages from the first environment by  running \npython3 -m pip install on the requirements.txt that I generated with \nthe python3 -m pip freeze command:\n(otherproject)$ python3 -m pip install -r /tmp/myproject/\n¯requirements.txt\nThis command cranks along for a little while as it retrieves and \ninstalls all of the packages required to reproduce the first environ-\nment. When it’s done, I can list the set of installed packages in the \nsecond virtual environment and should see the same list of depen-\ndencies found in the first virtual environment:\n(otherproject)$ python3 -m pip list\nPackage    Version \n---------- --------\ncertifi    2019.3.9\nchardet    3.0.4   \nidna       2.8     \nnumpy      1.16.2  \npip        10.0.1  \npytz       2018.9  \nrequests   2.21.0  \nsetuptools 39.0.1  \nurllib3    1.24.1\nUsing a requirements.txt file is ideal for collaborating with others \nthrough a revision control system. You can commit changes to your \ncode at the same time you update your list of package dependencies, \nensuring that they move in lockstep. However, it’s important to note \nthat the specific version of Python you’re using is not included in the \nrequirements.txt file, so that must be managed separately.\n \nItem 83: Use Virtual Environments for Isolated Dependencies \n395\n\n\n396 \nChapter 10 Collaboration\nThe gotcha with virtual environments is that moving them breaks \neverything because all of the paths, like the python3 command-line \ntool, are hard-coded to the environment’s install directory. But ulti-\nmately this limitation doesn’t matter. The whole purpose of virtual \nenvironments is to make it easy to reproduce a setup. Instead of mov-\ning a virtual environment directory, just use python3 -m pip freeze \non the old one, create a new virtual environment somewhere else, and \nreinstall everything from the requirements.txt file.\nThings to Remember\n✦ Virtual environments allow you to use pip to install many differ-\nent versions of the same package on the same machine without \nconflicts.\n✦ Virtual environments are created with python -m venv, enabled with \nsource bin/activate, and disabled with deactivate.\n✦ You can dump all of the requirements of an environment with \npython3 -m pip freeze. You can reproduce an environment by  running \npython3 -m pip install -r requirements.txt.\nItem 84:  Write Docstrings for Every Function, \nClass, and Module\nDocumentation in Python is extremely important because of the \ndynamic nature of the language. Python provides built-in support \nfor attaching documentation to blocks of code. Unlike with many \nother languages, the documentation from a program’s source code is \ndirectly accessible as the program runs.\nFor example, you can add documentation by providing a docstring \nimmediately after the def statement of a function:\ndef palindrome(word):\n    \"\"\"Return True if the given word is a palindrome.\"\"\"\n    return word == word[::-1]\n \nassert palindrome('tacocat')\nassert not palindrome('banana')\nYou can retrieve the docstring from within the Python program by \naccessing the function’s __doc__ special attribute:\nprint(repr(palindrome.__doc__))\n>>>\n'Return True if the given word is a palindrome.'\n\n\n \nItem 84: Write Docstrings for Every Function, Class, and Module \n397\nYou can also use the built-in pydoc module from the command line \nto run a local web server that hosts all of the Python documentation \nthat’s accessible to your interpreter, including modules that you’ve \nwritten:\n$ python3 -m pydoc -p 1234\nServer ready at http://localhost:1234/\nServer commands: [b]rowser, [q]uit\nserver> b\nDocstrings can be attached to functions, classes, and modules. This \nconnection is part of the process of compiling and running a Python \nprogram. Support for docstrings and the __doc__ attribute has three \nconsequences:\n \n■The accessibility of documentation makes interactive develop-\nment easier. You can inspect functions, classes, and modules to \nsee their documentation by using the help built-in function. This \nmakes the Python interactive interpreter (the Python “shell”) and \ntools like IPython Notebook (https://ipython.org) a joy to use \nwhile you’re developing algorithms, testing APIs, and writing code \nsnippets.\n \n■A standard way of defining documentation makes it easy to build \ntools that convert the text into more appealing formats (like \nHTML). This has led to excellent documentation-generation tools \nfor the Python community, such as Sphinx (https://www.sphinx-\ndoc.org). It has also enabled community-funded sites like Read \nthe Docs (https://readthedocs.org) that provide free hosting of \nbeautiful-looking documentation for open source Python projects.\n \n■Python’s first-class, accessible, and good-looking documentation \nencourages people to write more documentation. The members \nof the Python community have a strong belief in the importance \nof documentation. There’s an assumption that “good code” also \nmeans well-documented code. This means that you can expect \nmost open source Python libraries to have decent documentation.\nTo participate in this excellent culture of documentation, you need to \nfollow a few guidelines when you write docstrings. The full details are \ndiscussed online in PEP 257 (https://www.python.org/dev/peps/pep-\n0257/). There are a few best practices you should be sure to follow.\nDocumenting Modules\nEach module should have a top-level docstring—a string literal that is \nthe first statement in a source file. It should use three double quotes \n(\"\"\"). The goal of this docstring is to introduce the module and its \ncontents.\n",
      "page_number": 411,
      "chapter_number": 39,
      "summary": "This chapter covers segment 39 (pages 411-419). Key topics include packages, packaging, and python. The Python community has \nestablished best practices to maximize the maintainability of code \nover time.",
      "keywords": [
        "virtual environment",
        "Python",
        "environment",
        "Virtual",
        "Python Package Index",
        "pip",
        "packages",
        "version",
        "myproject",
        "install",
        "Item",
        "Python environment",
        "Python community",
        "pip install",
        "Python Package"
      ],
      "concepts": [
        "packages",
        "packaging",
        "python",
        "version",
        "versions",
        "dependencies",
        "depend",
        "dependency",
        "install",
        "environments"
      ],
      "similar_chapters": [
        {
          "book": "Python Microservices Development",
          "chapter": 30,
          "title": "Segment 30 (pages 247-255)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 11,
          "title": "[ 383 ]",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 28,
          "title": "Segment 28 (pages 247-257)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.66,
          "method": "api"
        }
      ]
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 420-428)",
      "start_page": 420,
      "end_page": 428,
      "detection_method": "topic_boundary",
      "content": "398 \nChapter 10 Collaboration\nThe first line of the docstring should be a single sentence describing \nthe module’s purpose. The paragraphs that follow should contain the \ndetails that all users of the module should know about its operation. \nThe module docstring is also a jumping-off point where you can high-\nlight important classes and functions found in the module.\nHere’s an example of a module docstring:\n# words.py\n#!/usr/bin/env python3\n\"\"\"Library for finding linguistic patterns in words.\n \nTesting how words relate to each other can be tricky sometimes!\nThis module provides easy ways to determine when words you've\nfound have special properties.\n \nAvailable functions:\n- palindrome: Determine if a word is a palindrome.\n- check_anagram: Determine if two words are anagrams.\n...\n\"\"\"\n...\nIf the module is a command-line utility, the module docstring is also \na great place to put usage information for running the tool.\nDocumenting Classes\nEach class should have a class-level docstring. This largely follows \nthe same pattern as the module-level docstring. The first line is the \nsingle-sentence purpose of the class. Paragraphs that follow discuss \nimportant details of the class’s operation.\nImportant public attributes and methods of the class should be high-\nlighted in the class-level docstring. It should also provide guidance to \nsubclasses on how to properly interact with protected attributes (see \nItem 42: “Prefer Public Attributes Over Private Ones”) and the super-\nclass’s methods.\nHere’s an example of a class docstring:\nclass Player:\n    \"\"\"Represents a player of the game.\n \n    Subclasses may override the 'tick' method to provide\n    custom animations for the player's movement depending\n    on their power level, etc.\n \n\n\n \nItem 84: Write Docstrings for Every Function, Class, and Module \n399\n    Public attributes:\n    - power: Unused power-ups (float between 0 and 1).\n    - coins: Coins found during the level (integer).\n    \"\"\"\n    ...\nDocumenting Functions\nEach public function and method should have a docstring. This fol-\nlows the same pattern as the docstrings for modules and classes. The \nfirst line is a single-sentence description of what the function does. \nThe paragraphs that follow should describe any specific behaviors \nand the arguments for the function. Any return values should be \nmentioned. Any exceptions that callers must handle as part of the \nfunction’s interface should be explained (see Item 20: “Prefer Raising \nExceptions to Returning None” for how to document raised exceptions).\nHere’s an example of a function docstring:\ndef find_anagrams(word, dictionary):\n    \"\"\"Find all anagrams for a word.\n \n    This function only runs as fast as the test for\n    membership in the 'dictionary' container.\n \n    Args:\n        word: String of the target word.\n        dictionary: collections.abc.Container with all\n            strings that are known to be actual words.\n \n    Returns:\n        List of anagrams that were found. Empty if\n        none were found.\n    \"\"\"\n    ...\nThere are also some special cases in writing docstrings for functions \nthat are important to know:\n \n■If a function has no arguments and a simple return value, a \n single-sentence description is probably good enough.\n \n■If a function doesn’t return anything, it’s better to leave out any \nmention of the return value instead of saying “returns None.”\n \n■If a function’s interface includes raising exceptions (see Item 20: \n“Prefer Raising Exceptions to Returning None” for an example), its \ndocstring should describe each exception that’s raised and when \nit’s raised.\n\n\n400 \nChapter 10 Collaboration\n \n■If you don’t expect a function to raise an exception during normal \noperation, don’t mention that fact.\n \n■If a function accepts a variable number of arguments (see Item 22: \n“Reduce Visual Noise with Variable Positional Arguments”) or key-\nword arguments (see Item 23: “Provide Optional Behavior with \nKeyword Arguments”), use *args and **kwargs in the documented \nlist of arguments to describe their purpose.\n \n■If a function has arguments with default values, those defaults \nshould be mentioned (see Item 24: “Use None and Docstrings to \nSpecify Dynamic Default Arguments”).\n \n■If a function is a generator (see Item 30: “Consider Generators \nInstead of Returning Lists”), its docstring should describe what \nthe generator yields when it’s iterated.\n \n■If a function is an asynchronous coroutine (see Item 60: “Achieve \nHighly Concurrent I/O with Coroutines”), its docstring should \nexplain when it will stop execution.\nUsing Docstrings and Type Annotations\nPython now supports type annotations for a variety of purposes (see \nItem 90: “Consider Static Analysis via typing to Obviate Bugs” for how \nto use them). The information they contain may be redundant with \ntypical docstrings. For example, here is the function signature for \nfind_anagrams with type annotations applied:\nfrom typing import Container, List\n \ndef find_anagrams(word: str,\n                  dictionary: Container[str]) -> List[str]:\n    ...\nThere is no longer a need to specify in the docstring that the word \nargument is a string, since the type annotation has that infor-\nmation. The same goes for the dictionary argument being a \ncollections.abc.Container. There’s no reason to mention that the \nreturn type will be a list, since this fact is clearly annotated. And \nwhen no anagrams are found, the return value still must be a list, so \nit’s implied that it will be empty; that doesn’t need to be noted in the \ndocstring. Here, I write the same function signature from above along \nwith the docstring that has been shortened accordingly:\ndef find_anagrams(word: str,\n                  dictionary: Container[str]) -> List[str]:\n    \"\"\"Find all anagrams for a word.\n \n\n\n \nItem 85: Use Packages to Organize Modules and Provide Stable APIs \n401\n    This function only runs as fast as the test for\n    membership in the 'dictionary' container.\n \n    Args:\n        word: Target word.\n        dictionary: All known actual words.\n \n    Returns:\n        Anagrams that were found.\n    \"\"\"\n    ...\nThe redundancy between type annotations and docstrings should be \nsimilarly avoided for instance fields, class attributes, and methods. \nIt’s best to have type information in only one place so there’s less risk \nthat it will skew from the actual implementation.\nThings to Remember\n✦ Write documentation for every module, class, method, and function \nusing docstrings. Keep them up-to-date as your code changes.\n✦ For modules: Introduce the contents of a module and any important \nclasses or functions that all users should know about.\n✦ For classes: Document behavior, important attributes, and subclass \nbehavior in the docstring following the class statement.\n✦ For functions and methods: Document every argument, returned \nvalue, raised exception, and other behaviors in the docstring follow-\ning the def statement.\n✦ If you’re using type annotations, omit the information that’s already \npresent in type annotations from docstrings since it would be \nredundant to have it in both places.\nItem 85:  Use Packages to Organize Modules and \nProvide Stable APIs\nAs the size of a program’s codebase grows, it’s natural for you to reor-\nganize its structure. You’ll split larger functions into smaller func-\ntions. You’ll refactor data structures into helper classes (see Item 37: \n“Compose Classes Instead of Nesting Many Levels of Built-in Types” \nfor an example). You’ll separate functionality into various modules \nthat depend on each other.\nAt some point, you’ll find yourself with so many modules that you \nneed another layer in your program to make it understandable. For \n\n\n402 \nChapter 10 Collaboration\nthis purpose, Python provides packages. Packages are modules that \ncontain other modules.\nIn most cases, packages are defined by putting an empty file named \n__init__.py into a directory. Once __init__.py is present, any other \nPython files in that directory will be available for import, using a path \nrelative to the directory. For example, imagine that I have the follow-\ning directory structure in my program:\nmain.py\nmypackage/__init__.py\nmypackage/models.py\nmypackage/utils.py\nTo import the utils module, I use the absolute module name that \nincludes the package directory’s name:\n# main.py\nfrom mypackage import utils\nThis pattern continues when I have package directories present \nwithin other packages (like mypackage.foo.bar).\nThe functionality provided by packages has two primary purposes in \nPython programs.\nNamespaces\nThe first use of packages is to help divide your modules into separate \nnamespaces. They enable you to have many modules with the same \nfilename but different absolute paths that are unique. For example, \nhere’s a program that imports attributes from two modules with the \nsame filename, utils.py:\n# main.py\nfrom analysis.utils import log_base2_bucket\nfrom frontend.utils import stringify\n \nbucket = stringify(log_base2_bucket(33))\nThis approach breaks when the functions, classes, or submodules \ndefined in packages have the same names. For example, say that I \nwant to use the inspect function from both the analysis.utils and \nthe frontend.utils modules. Importing the attributes directly won’t \nwork because the second import statement will overwrite the value of \ninspect in the current scope:\n# main2.py\nfrom analysis.utils import inspect\nfrom frontend.utils import inspect  # Overwrites!\n\n\n \nItem 85: Use Packages to Organize Modules and Provide Stable APIs \n403\nThe solution is to use the as clause of the import statement to rename \nwhatever I’ve imported for the current scope:\n# main3.py\nfrom analysis.utils import inspect as analysis_inspect\nfrom frontend.utils import inspect as frontend_inspect\n \nvalue = 33\nif analysis_inspect(value) == frontend_inspect(value):\n    print('Inspection equal!')\nThe as clause can be used to rename anything retrieved with the \nimport statement, including entire modules. This facilitates accessing \nnamespaced code and makes its identity clear when you use it.\nAnother approach for avoiding imported name conflicts is to always \naccess names by their highest unique module name. For the exam-\nple above, this means I’d use basic import statements instead of \nimport from:\n# main4.py\nimport analysis.utils\nimport frontend.utils\n \nvalue = 33\nif (analysis.utils.inspect(value) ==\n    frontend.utils.inspect(value)):\n    print('Inspection equal!')\nThis approach allows you to avoid the as clause altogether. It also \nmakes it abundantly clear to new readers of the code where each of \nthe similarly named functions is defined.\nStable APIs\nThe second use of packages in Python is to provide strict, stable APIs \nfor external consumers.\nWhen you’re writing an API for wider consumption, such as an open \nsource package (see Item 82: “Know Where to Find Community-Built \nModules” for examples), you’ll want to provide stable functionality that \ndoesn’t change between releases. To ensure that happens, it’s import-\nant to hide your internal code organization from external users. This \nway, you can refactor and improve your package’s internal modules \nwithout breaking existing users.\nPython can limit the surface area exposed to API consumers by using \nthe __all__ special attribute of a module or package. The value of \n__all__ is a list of every name to export from the module as part of \nits public API. When consuming code executes from foo import *, \n\n\n404 \nChapter 10 Collaboration\nonly the attributes in foo.__all__ will be imported from foo. If __all__ \nisn’t present in foo, then only public attributes—those without a lead-\ning underscore—are imported (see Item 42: “Prefer Public Attributes \nOver Private Ones” for details about that convention).\nFor example, say that I want to provide a package for calculating col-\nlisions between moving projectiles. Here, I define the models module of \nmypackage to contain the representation of projectiles:\n# models.py\n__all__ = ['Projectile']\n \nclass Projectile:\n    def __init__(self, mass, velocity):\n        self.mass = mass\n        self.velocity = velocity\nI also define a utils module in mypackage to perform operations on the \nProjectile instances, such as simulating collisions between them:\n# utils.py\nfrom . models import Projectile\n \n__all__ = ['simulate_collision']\n \ndef _dot_product(a, b):\n    ...\n \ndef simulate_collision(a, b):\n    ...\nNow, I’d like to provide all of the public parts of this API as a set of \nattributes that are available on the mypackage module. This will allow \ndownstream consumers to always import directly from mypackage \ninstead of importing from mypackage.models or mypackage.utils. This \nensures that the API consumer’s code will continue to work even if the \ninternal organization of mypackage changes (e.g., models.py is deleted).\nTo do this with Python packages, you need to modify the __init__.py \nfile in the mypackage directory. This file is what actually becomes the \ncontents of the mypackage module when it’s imported. Thus, you can \nspecify an explicit API for mypackage by limiting what you import into \n__init__.py. Since all of my internal modules already specify __all__, \nI can expose the public interface of mypackage by simply import-\ning everything from the internal modules and updating __all__ \naccordingly:\n# __init__.py\n__all__ = []\nfrom . models import *\n\n\n__all__ += models.__all__\nfrom . utils import *\n__all__ += utils.__all__\nHere’s a consumer of the API that directly imports from mypackage \ninstead of accessing the inner modules:\n# api_consumer.py\nfrom mypackage import *\n \na = Projectile(1.5, 3)\nb = Projectile(4, 1.7)\nafter_a, after_b = simulate_collision(a, b)\nNotably, internal-only functions like mypackage.utils._dot_product \nwill not be available to the API consumer on mypackage because they \nweren’t present in __all__. Being omitted from __all__ also means \nthat they weren’t imported by the from mypackage import * state-\nment. The internal-only names are effectively hidden.\nThis whole approach works great when it’s important to provide an \nexplicit, stable API. However, if you’re building an API for use between \nyour own modules, the functionality of __all__ is probably unneces-\nsary and should be avoided. The namespacing provided by packages \nis usually enough for a team of programmers to collaborate on large \namounts of code they control while maintaining reasonable interface \nboundaries.\n \nItem 85: Use Packages to Organize Modules and Provide Stable APIs \n405\nBeware of import *\nImport statements like from x import y are clear because the \nsource of y is explicitly the x package or module. Wildcard imports \nlike from foo import * can also be useful, especially in interac-\ntive Python sessions. However, wildcards make code more diffi-\ncult to understand:\n \n■from foo import * hides the source of names from new read-\ners of the code. If a module has multiple import * statements, \nyou’ll need to check all of the referenced modules to figure out \nwhere a name was defined.\n \n■Names from import * statements will overwrite any conflicting \nnames within the containing module. This can lead to strange \nbugs caused by accidental interactions between your code and \noverlapping names from multiple import * statements.\nThe safest approach is to avoid import * in your code and explicitly \nimport names with the from x import y style.\n\n\n406 \nChapter 10 Collaboration\nThings to Remember\n✦ Packages in Python are modules that contain other modules. Pack-\nages allow you to organize your code into separate, non-conflicting \nnamespaces with unique absolute module names.\n✦ Simple packages are defined by adding an __init__.py file to a \ndirectory that contains other source files. These files become the \nchild modules of the directory’s package. Package directories may \nalso contain other packages.\n✦ You can provide an explicit API for a module by listing its publicly \nvisible names in its __all__ special attribute.\n✦ You can hide a package’s internal implementation by only import-\ning public names in the package’s __init__.py file or by naming \n internal-only members with a leading underscore.\n✦ When collaborating within a single team or on a single codebase, \nusing __all__ for explicit APIs is probably unnecessary.\nItem 86:  Consider Module-Scoped Code to Configure \nDeployment Environments\nA deployment environment is a configuration in which a program \nruns. Every program has at least one deployment environment: the \nproduction environment. The goal of writing a program in the first \nplace is to put it to work in the production environment and achieve \nsome kind of outcome.\nWriting or modifying a program requires being able to run it on the \ncomputer you use for developing. The configuration of your develop-\nment environment may be very different from that of your production \nenvironment. For example, you may be using a tiny single-board \ncomputer to develop a program that’s meant to run on enormous \nsupercomputers.\nTools like venv (see Item 83: “Use Virtual Environments for Isolated \nand Reproducible Dependencies”) make it easy to ensure that all envi-\nronments have the same Python packages installed. The trouble is \nthat production environments often require many external assump-\ntions that are hard to reproduce in development environments.\nFor example, say that I want to run a program in a web server con-\ntainer and give it access to a database. Every time I want to modify \nmy program’s code, I need to run a server container, the database \nschema must be set up properly, and my program needs the password \nfor access. This is a very high cost if all I’m trying to do is verify that \na one-line change to my program works correctly.\n",
      "page_number": 420,
      "chapter_number": 40,
      "summary": "This chapter covers segment 40 (pages 420-428). Key topics include importance, important, and imports. This \nconnection is part of the process of compiling and running a Python \nprogram.",
      "keywords": [
        "Module",
        "Item",
        "Provide Stable APIs",
        "Python",
        "Function",
        "docstring",
        "Stable APIs",
        "module docstring",
        "functions",
        "API",
        "Packages",
        "provide",
        "mypackage",
        "Organize Modules",
        "Python provides packages"
      ],
      "concepts": [
        "importance",
        "important",
        "imports",
        "module",
        "documenting",
        "classes",
        "words",
        "function",
        "functions",
        "functionality"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.83,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 28,
          "title": "Segment 28 (pages 247-257)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 25,
          "title": "Miscellaneous Library Modules",
          "relevance_score": 0.71,
          "method": "api"
        }
      ]
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 429-441)",
      "start_page": 429,
      "end_page": 441,
      "detection_method": "topic_boundary",
      "content": " \nItem 86: Consider Module-Scoped Code to Configure Enviornments \n407\nThe best way to work around such issues is to override parts of a pro-\ngram at startup time to provide different functionality depending on \nthe deployment environment. For example, I could have two different \n__main__ files—one for production and one for development:\n# dev_main.py\nTESTING = True\n \nimport db_connection\n \ndb = db_connection.Database()\n# prod_main.py\nTESTING = False\n \nimport db_connection\n \ndb = db_connection.Database()\nThe only difference between the two files is the value of the TESTING \nconstant. Other modules in my program can then import the __main__ \nmodule and use the value of TESTING to decide how they define their \nown attributes:\n# db_connection.py\nimport __main__\n \nclass TestingDatabase:\n    ...\n \nclass RealDatabase:\n    ...\n \nif __main__.TESTING:\n    Database = TestingDatabase\nelse:\n    Database = RealDatabase\nThe key behavior to notice here is that code running in module \nscope—not inside a function or method—is just normal Python code. \nYou can use an if statement at the module level to decide how the \nmodule will define names. This makes it easy to tailor modules to \nyour various deployment environments. You can avoid having to \nreproduce costly assumptions like database configurations when \nthey aren’t needed. You can inject local or fake implementations that \nease interactive development, or you can use mocks for writing tests \n(see Item 78: “Use Mocks to Test Code with Complex Dependencies”).\n\n\n408 \nChapter 10 Collaboration\nNote\nWhen your deployment environment configuration gets really complicated, \nyou should consider moving it out of Python constants (like TESTING) and into \ndedicated configuration files. Tools like the configparser built-in module let \nyou maintain production configurations separately from code, a distinction \nthat’s crucial for collaborating with an operations team.\nThis approach can be used for more than working around external \nassumptions. For example, if I know that my program must work dif-\nferently depending on its host platform, I can inspect the sys module \nbefore defining top-level constructs in a module:\n# db_connection.py\nimport sys\n \nclass Win32Database:\n    ...\n \nclass PosixDatabase:\n    ...\n \nif sys.platform.startswith('win32'):\n    Database = Win32Database\nelse:\n    Database = PosixDatabase\nSimilarly, I could use environment variables from os.environ to guide \nmy module definitions.\nThings to Remember\n✦ Programs often need to run in multiple deployment environments \nthat each have unique assumptions and configurations.\n✦ You can tailor a module’s contents to different deployment environ-\nments by using normal Python statements in module scope.\n✦ Module contents can be the product of any external condition, \nincluding host introspection through the sys and os modules.\nItem 87:  Define a Root Exception to Insulate Callers \nfrom APIs\nWhen you’re defining a module’s API, the exceptions you raise are \njust as much a part of your interface as the functions and classes you \ndefine (see Item 20: “Prefer Raising Exceptions to Returning None” for \nan example).\n\n\nPython has a built-in hierarchy of exceptions for the language and \nstandard library. There’s a draw to using the built-in exception types \nfor reporting errors instead of defining your own new types. For exam-\nple, I could raise a ValueError exception whenever an invalid parame-\nter is passed to a function in one of my modules:\n# my_module.py\ndef determine_weight(volume, density):\n    if density <= 0:\n        raise ValueError('Density must be positive')\n    ...\nIn some cases, using ValueError makes sense, but for APIs, it’s much \nmore powerful to define a new hierarchy of exceptions. I can do this \nby providing a root Exception in my module and having all other \nexceptions raised by that module inherit from the root exception:\n# my_module.py\nclass Error(Exception):\n    \"\"\"Base-class for all exceptions raised by this module.\"\"\"\n \nclass InvalidDensityError(Error):\n    \"\"\"There was a problem with a provided density value.\"\"\"\n \nclass InvalidVolumeError(Error):\n    \"\"\"There was a problem with the provided weight value.\"\"\"\n \ndef determine_weight(volume, density):\n    if density < 0:\n        raise InvalidDensityError('Density must be positive')\n    if volume < 0:\n        raise InvalidVolumeError('Volume must be positive')\n    if volume == 0:\n        density / volume\nHaving a root exception in a module makes it easy for consumers of \nan API to catch all of the exceptions that were raised deliberately. For \nexample, here a consumer of my API makes a function call with a \ntry/except statement that catches my root exception:\ntry:\n    weight = my_module.determine_weight(1, -1)\nexcept my_module.Error:\n    logging.exception('Unexpected error')\n>>>\nUnexpected error\n \nItem 87: Define a Root Exception to Insulate Callers from APIs \n409\n\n\n410 \nChapter 10 Collaboration\nTraceback (most recent call last):\n  File \".../example.py\", line 3, in <module>\n    weight = my_module.determine_weight(1, -1)\n  File \".../my_module.py\", line 10, in determine_weight\n    raise InvalidDensityError('Density must be positive')\nInvalidDensityError: Density must be positive\nHere, the logging.exception function prints the full stack trace of the \ncaught exception so it’s easier to debug in this situation. The try/\nexcept also prevents my API’s exceptions from propagating too far \nupward and breaking the calling program. It insulates the calling \ncode from my API. This insulation has three helpful effects.\nFirst, root exceptions let callers understand when there’s a problem \nwith their usage of an API. If callers are using my API properly, they \nshould catch the various exceptions that I deliberately raise. If they \ndon’t handle such an exception, it will propagate all the way up to \nthe insulating except block that catches my module’s root exception. \nThat block can bring the exception to the attention of the API con-\nsumer, providing an opportunity for them to add proper handling of \nthe missed exception type:\ntry:\n    weight = my_module.determine_weight(-1, 1)\nexcept my_module.InvalidDensityError:\n    weight = 0\nexcept my_module.Error:\n    logging.exception('Bug in the calling code')\n>>>\nBug in the calling code\nTraceback (most recent call last):\n  File \".../example.py\", line 3, in <module>\n    weight = my_module.determine_weight(-1, 1)\n  File \".../my_module.py\", line 12, in determine_weight\n    raise InvalidVolumeError('Volume must be positive')\nInvalidVolumeError: Volume must be positive\nThe second advantage of using root exceptions is that they can help \nfind bugs in an API module’s code. If my code only deliberately raises \nexceptions that I define within my module’s hierarchy, then all other \ntypes of exceptions raised by my module must be the ones that I didn’t \nintend to raise. These are bugs in my API’s code.\nUsing the try/except statement above will not insulate API consum-\ners from bugs in my API module’s code. To do that, the caller needs to \nadd another except block that catches Python’s base Exception class. \n\n\nThis allows the API consumer to detect when there’s a bug in the API \nmodule’s implementation that needs to be fixed. The output for this \nexample includes both the logging.exception message and the default \ninterpreter output for the exception since it was re-raised:\ntry:\n    weight = my_module.determine_weight(0, 1)\nexcept my_module.InvalidDensityError:\n    weight = 0\nexcept my_module.Error:\n    logging.exception('Bug in the calling code')\nexcept Exception:\n    logging.exception('Bug in the API code!')\n    raise  # Re-raise exception to the caller\n>>>\nBug in the API code!\nTraceback (most recent call last):\n  File \".../example.py\", line 3, in <module>\n    weight = my_module.determine_weight(0, 1)\n  File \".../my_module.py\", line 14, in determine_weight\n    density / volume\nZeroDivisionError: division by zero\nTraceback ...\nZeroDivisionError: division by zero\nThe third impact of using root exceptions is future-proofing an API. \nOver time, I might want to expand my API to provide more spe-\ncific exceptions in certain situations. For example, I could add an \nException subclass that indicates the error condition of supplying \nnegative densities:\n# my_module.py\n...\n \nclass NegativeDensityError(InvalidDensityError):\n    \"\"\"A provided density value was negative.\"\"\"\n \n...\n \ndef determine_weight(volume, density):\n    if density < 0:\n        raise NegativeDensityError('Density must be positive')\n    ...\n \nItem 87: Define a Root Exception to Insulate Callers from APIs \n411\n\n\n412 \nChapter 10 Collaboration\nThe calling code will continue to work exactly as before because it \nalready catches InvalidDensityError exceptions (the parent class \nof NegativeDensityError). In the future, the caller could decide to \n special-case the new type of exception and change the handling \nbehavior accordingly:\ntry:\n    weight = my_module.determine_weight(1, -1)\nexcept my_module.NegativeDensityError:\n    raise ValueError('Must supply non-negative density')\nexcept my_module.InvalidDensityError:\n    weight = 0\nexcept my_module.Error:\n    logging.exception('Bug in the calling code')\nexcept Exception:\n    logging.exception('Bug in the API code!')\n    raise\n>>>\nTraceback ...\nNegativeDensityError: Density must be positive\n \nThe above exception was the direct cause of the following \n¯exception:\n \nTraceback ...\nValueError: Must supply non-negative density\nI can take API future-proofing further by providing a broader set of \nexceptions directly below the root exception. For example, imagine \nthat I have one set of errors related to calculating weights, another \nrelated to calculating volume, and a third related to calculating \ndensity:\n# my_module.py\nclass Error(Exception):\n    \"\"\"Base-class for all exceptions raised by this module.\"\"\"\n \nclass WeightError(Error):\n    \"\"\"Base-class for weight calculation errors.\"\"\"\n \nclass VolumeError(Error):\n    \"\"\"Base-class for volume calculation errors.\"\"\"\n \nclass DensityError(Error):\n    \"\"\"Base-class for density calculation errors.\"\"\"\n...\n\n\n \nItem 88: Know How to Break Circular Dependencies \n413\nSpecific exceptions would inherit from these general exceptions. Each \nintermediate exception acts as its own kind of root exception. This \nmakes it easier to insulate layers of calling code from API code based \non broad functionality. This is much better than having all callers \ncatch a long list of very specific Exception subclasses.\nThings to Remember\n✦ Defining root exceptions for modules allows API consumers to \n insulate themselves from an API.\n✦ Catching root exceptions can help you find bugs in code that \n consumes an API.\n✦ Catching the Python Exception base class can help you find bugs in \nAPI implementations.\n✦ Intermediate root exceptions let you add more specific types of \nexceptions in the future without breaking your API consumers.\nItem 88: Know How to Break Circular Dependencies\nInevitably, while you’re collaborating with others, you’ll find a mutual \ninterdependence between modules. It can even happen while you work \nby yourself on the various parts of a single program.\nFor example, say that I want my GUI application to show a dialog box \nfor choosing where to save a document. The data displayed by the dia-\nlog could be specified through arguments to my event handlers. But \nthe dialog also needs to read global state, such as user preferences, to \nknow how to render properly.\nHere, I define a dialog that retrieves the default document save loca-\ntion from global preferences:\n# dialog.py\nimport app\n \nclass Dialog:\n    def __init__(self, save_dir):\n        self.save_dir = save_dir\n    ...\n \nsave_dialog = Dialog(app.prefs.get('save_dir'))\n \ndef show():\n    ...\n\n\n414 \nChapter 10 Collaboration\nThe problem is that the app module that contains the prefs object \nalso imports the dialog class in order to show the same dialog on pro-\ngram start:\n# app.py\nimport dialog\n \nclass Prefs:\n    ...\n    def get(self, name):\n        ...\n \nprefs = Prefs()\ndialog.show()\nIt’s a circular dependency. If I try to import the app module from my \nmain program like this:\n# main.py\nimport app\nI get an exception:\n>>>\n$ python3 main.py \nTraceback (most recent call last):\n  File \".../main.py\", line 17, in <module>\n    import app\n  File \".../app.py\", line 17, in <module>\n    import dialog\n  File \".../dialog.py\", line 23, in <module>\n    save_dialog = Dialog(app.prefs.get('save_dir'))\nAttributeError: partially initialized module 'app' has no \n¯attribute 'prefs' (most likely due to a circular import)\nTo understand what’s happening here, you need to know how Python’s \nimport machinery works in general (see the importlib built-in package \nfor the full details). When a module is imported, here’s what Python \nactually does, in depth-first order:\n1. Searches for a module in locations from sys.path\n2. Loads the code from the module and ensures that it compiles\n3. Creates a corresponding empty module object\n4. Inserts the module into sys.modules\n5. Runs the code in the module object to define its contents\n\n\n \nItem 88: Know How to Break Circular Dependencies \n415\nThe problem with a circular dependency is that the attributes of a \nmodule aren’t defined until the code for those attributes has executed \n(after step 5). But the module can be loaded with the import state-\nment immediately after it’s inserted into sys.modules (after step 4).\nIn the example above, the app module imports dialog before defin-\ning anything. Then, the dialog module imports app. Since app still \nhasn’t finished running—it’s currently importing dialog—the app \nmodule is empty (from step 4). The AttributeError is raised (during \nstep 5 for dialog) because the code that defines prefs hasn’t run yet \n(step 5 for app isn’t complete).\nThe best solution to this problem is to refactor the code so that the \nprefs data structure is at the bottom of the dependency tree. Then, \nboth app and dialog can import the same utility module and avoid \nany circular dependencies. But such a clear division isn’t always pos-\nsible or could require too much refactoring to be worth the effort.\nThere are three other ways to break circular dependencies.\nReordering Imports\nThe first approach is to change the order of imports. For example, if I \nimport the dialog module toward the bottom of the app module, after \nthe app module’s other contents have run, the AttributeError goes \naway:\n# app.py\nclass Prefs:\n    ...\n \nprefs = Prefs()\n \nimport dialog  # Moved\ndialog.show()\nThis works because, when the dialog module is loaded late, its recur-\nsive import of app finds that app.prefs has already been defined (step \n5 is mostly done for app).\nAlthough this avoids the AttributeError, it goes against the PEP 8 \nstyle guide (see Item 2: “Follow the PEP 8 Style Guide”). The style \nguide suggests that you always put imports at the top of your Python \nfiles. This makes your module’s dependencies clear to new readers of \nthe code. It also ensures that any module you depend on is in scope \nand available to all the code in your module.\n\n\n416 \nChapter 10 Collaboration\nHaving imports later in a file can be brittle and can cause small \nchanges in the ordering of your code to break the module entirely. \nI suggest not using import reordering to solve your circular depen-\ndency issues.\nImport, Configure, Run\nA second solution to the circular imports problem is to have mod-\nules minimize side effects at import time. I can have my modules only \ndefine functions, classes, and constants. I avoid actually running \nany functions at import time. Then, I have each module provide a \nconfigure function that I call once all other modules have finished \nimporting. The purpose of configure is to prepare each module’s state \nby accessing the attributes of other modules. I run configure after \nall modules have been imported (step 5 is complete), so all attributes \nmust be defined.\nHere, I redefine the dialog module to only access the prefs object \nwhen configure is called:\n# dialog.py\nimport app\n \nclass Dialog:\n    ...\n \nsave_dialog = Dialog()\n \ndef show():\n    ...\n \ndef configure():\n    save_dialog.save_dir = app.prefs.get('save_dir')\nI also redefine the app module to not run activities on import:\n# app.py\nimport dialog\n \nclass Prefs:\n    ...\n \nprefs = Prefs()\n \ndef configure():\n    ...\n\n\nFinally, the main module has three distinct phases of execution—\nimport everything, configure everything, and run the first activity:\n# main.py\nimport app\nimport dialog\n \napp.configure()\ndialog.configure()\n \ndialog.show()\nThis works well in many situations and enables patterns like depen-\ndency injection. But sometimes it can be difficult to structure your \ncode so that an explicit configure step is possible. Having two dis-\ntinct phases within a module can also make your code harder to read \nbecause it separates the definition of objects from their configuration.\nDynamic Import\nThe third—and often simplest—solution to the circular imports prob-\nlem is to use an import statement within a function or method. This \nis called a dynamic import because the module import happens while \nthe program is running, not while the program is first starting up \nand initializing its modules.\nHere, I redefine the dialog module to use a dynamic import. The \ndialog.show function imports the app module at runtime instead of \nthe dialog module importing app at initialization time:\n# dialog.py\nclass Dialog:\n    ...\n \nsave_dialog = Dialog()\n \ndef show():\n    import app  # Dynamic import\n    save_dialog.save_dir = app.prefs.get('save_dir')\n    ...\nThe app module can now be the same as it was in the original exam-\nple. It imports dialog at the top and calls dialog.show at the bottom:\n# app.py\nimport dialog\n \n \nItem 88: Know How to Break Circular Dependencies \n417\n\n\n418 \nChapter 10 Collaboration\nclass Prefs:\n    ...\n \nprefs = Prefs()\ndialog.show()\nThis approach has a similar effect to the import, configure, and run \nsteps from before. The difference is that it requires no structural \nchanges to the way the modules are defined and imported. I’m simply \ndelaying the circular import until the moment I must access the other \nmodule. At that point, I can be pretty sure that all other modules \nhave already been initialized (step 5 is complete for everything).\nIn general, it’s good to avoid dynamic imports like this. The cost of the \nimport statement is not negligible and can be especially bad in tight \nloops. By delaying execution, dynamic imports also set you up for \nsurprising failures at runtime, such as SyntaxError exceptions long \nafter your program has started running (see Item 76: “Verify Related \nBehaviors in TestCase Subclasses” for how to avoid that). However, \nthese downsides are often better than the alternative of restructuring \nyour entire program.\nThings to Remember\n✦ Circular dependencies happen when two modules must call into \neach other at import time. They can cause your program to crash at \nstartup.\n✦ The best way to break a circular dependency is by refactoring \nmutual dependencies into a separate module at the bottom of the \ndependency tree.\n✦ Dynamic imports are the simplest solution for breaking a circular \ndependency between modules while minimizing refactoring and \ncomplexity.\nItem 89:  Consider warnings to Refactor and \nMigrate Usage\nIt’s natural for APIs to change in order to satisfy new requirements \nthat meet formerly unanticipated needs. When an API is small and has \nfew upstream or downstream dependencies, making such changes is \nstraightforward. One programmer can often update a small API and \nall of its callers in a single commit.\n\n\nHowever, as a codebase grows, the number of callers of an API can be \nso large or fragmented across source repositories that it’s infeasible \nor impractical to make API changes in lockstep with updating callers \nto match. Instead, you need a way to notify and encourage the people \nthat you collaborate with to refactor their code and migrate their API \nusage to the latest forms.\nFor example, say that I want to provide a module for calculating how \nfar a car will travel at a given average speed and duration. Here, \nI define such a function and assume that speed is in miles per hour \nand duration is in hours:\ndef print_distance(speed, duration):\n    distance = speed * duration\n    print(f'{distance} miles')\n \nprint_distance(5, 2.5)\n>>>\n12.5 miles\nImagine that this works so well that I quickly gather a large number \nof dependencies on this function. Other programmers that I collabo-\nrate with need to calculate and print distances like this all across our \nshared codebase.\nDespite its success, this implementation is error prone because the \nunits for the arguments are implicit. For example, if I wanted to see \nhow far a bullet travels in 3 seconds at 1000 meters per second, I \nwould get the wrong result:\nprint_distance(1000, 3)\n>>>\n3000 miles\nI can address this problem by expanding the API of print_distance to \ninclude optional keyword arguments (see Item 23: “Provide Optional \nBehavior with Keyword Arguments” and Item 25: “Enforce Clarity \nwith Keyword-Only and Positional-Only Arguments”) for the units of \nspeed, duration, and the computed distance to print out:\nCONVERSIONS = {\n    'mph': 1.60934 / 3600 * 1000,   # m/s\n    'hours': 3600,                  # seconds\n    'miles': 1.60934 * 1000,        # m\n    'meters': 1,                    # m\n    'm/s': 1,                       # m\n    'seconds': 1,                   # s\n}\n \n \nItem 89: Consider warnings to Refactor and Migrate Usage \n419\n",
      "page_number": 429,
      "chapter_number": 41,
      "summary": "This chapter covers segment 41 (pages 429-441). Key topics include modules, imports, and exception. Pack-\nages allow you to organize your code into separate, non-conflicting \nnamespaces with unique absolute module names.",
      "keywords": [
        "module",
        "API",
        "Exception",
        "exceptions",
        "dialog",
        "Root Exception",
        "code",
        "API code",
        "dialog module",
        "API module",
        "app module",
        "import dialog",
        "density",
        "Break Circular Dependencies",
        "weight"
      ],
      "concepts": [
        "modules",
        "imports",
        "exception",
        "exceptions",
        "classes",
        "configure",
        "configuration",
        "configurations",
        "dependencies",
        "dependency"
      ],
      "similar_chapters": [
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 7,
          "title": "Segment 7 (pages 51-58)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 11,
          "title": "[ 383 ]",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 38,
          "title": "Segment 38 (pages 345-352)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 14,
          "title": "Testing, Debugging, and Exceptions",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 27,
          "title": "Segment 27 (pages 238-246)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 442-451)",
      "start_page": 442,
      "end_page": 451,
      "detection_method": "topic_boundary",
      "content": "420 \nChapter 10 Collaboration\ndef convert(value, units):\n    rate = CONVERSIONS[units]\n    return rate * value\n \ndef localize(value, units):\n    rate = CONVERSIONS[units]\n    return value / rate\n \ndef print_distance(speed, duration, *,\n                   speed_units='mph',\n                   time_units='hours',\n                   distance_units='miles'):\n    norm_speed = convert(speed, speed_units)\n    norm_duration = convert(duration, time_units)\n    norm_distance = norm_speed * norm_duration\n    distance = localize(norm_distance, distance_units)\n    print(f'{distance} {distance_units}')\nNow, I can modify the speeding bullet call to produce an accurate \nresult with a unit conversion to miles:\nprint_distance(1000, 3,\n               speed_units='meters',\n               time_units='seconds')\n>>>\n1.8641182099494205 miles\nIt seems like requiring units to be specified for this function is a much \nbetter way to go. Making them explicit reduces the likelihood of errors \nand is easier for new readers of the code to understand. But how can I \nmigrate all callers of the API over to always specifying units? How do \nI minimize breakage of any code that’s dependent on print_distance \nwhile also encouraging callers to adopt the new units arguments as \nsoon as possible?\nFor this purpose, Python provides the built-in warnings module. \nUsing warnings is a programmatic way to inform other programmers \nthat their code needs to be modified due to a change to an underly-\ning library that they depend on. While exceptions are primarily for \nautomated error handling by machines (see Item 87: “Define a Root \nException to Insulate Callers from APIs”), warnings are all about \ncommunication between humans about what to expect in their col-\nlaboration with each other.\n\n\nI can modify print_distance to issue warnings when the optional \nkeyword arguments for specifying units are not supplied. This way, \nthe arguments can continue being optional temporarily (see Item 24: \n“Use None and Docstrings to Specify Dynamic Default Arguments” \nfor background), while providing an explicit notice to people running \ndependent programs that they should expect breakage in the future if \nthey fail to take action:\nimport warnings\n \ndef print_distance(speed, duration, *,\n                   speed_units=None,\n                   time_units=None,\n                   distance_units=None):\n    if speed_units is None:\n        warnings.warn(\n            'speed_units required', DeprecationWarning)\n        speed_units = 'mph'\n \n    if time_units is None:\n        warnings.warn(\n            'time_units required', DeprecationWarning)\n        time_units = 'hours'\n \n    if distance_units is None:\n        warnings.warn(\n            'distance_units required', DeprecationWarning)\n        distance_units = 'miles'\n \n    norm_speed = convert(speed, speed_units)\n    norm_duration = convert(duration, time_units)\n    norm_distance = norm_speed * norm_duration\n    distance = localize(norm_distance, distance_units)\n    print(f'{distance} {distance_units}')\nI can verify that this code issues a warning by calling the function \nwith the same arguments as before and capturing the sys.stderr out-\nput from the warnings module:\nimport contextlib\nimport io\n \nfake_stderr = io.StringIO()\nwith contextlib.redirect_stderr(fake_stderr):\n \nItem 89: Consider warnings to Refactor and Migrate Usage \n421\n\n\n422 \nChapter 10 Collaboration\n    print_distance(1000, 3,\n                   speed_units='meters',\n                   time_units='seconds')\n \nprint(fake_stderr.getvalue())\n>>>\n1.8641182099494205 miles\n.../example.py:97: DeprecationWarning: distance_units required\n  warnings.warn(\nAdding warnings to this function required quite a lot of repetitive boil-\nerplate that’s hard to read and maintain. Also, the warning message \nindicates the line where warning.warn was called, but what I really \nwant to point out is where the call to print_distance was made with-\nout soon-to-be-required keyword arguments.\nLuckily, the warnings.warn function supports the stacklevel param-\neter, which makes it possible to report the correct place in the stack \nas the cause of the warning. stacklevel also makes it easy to write \nfunctions that can issue warnings on behalf of other code, reducing \nboilerplate. Here, I define a helper function that warns if an optional \nargument wasn’t supplied and then provides a default value for it:\ndef require(name, value, default):\n    if value is not None:\n        return value\n    warnings.warn(\n        f'{name} will be required soon, update your code',\n        DeprecationWarning,\n        stacklevel=3)\n    return default\n \ndef print_distance(speed, duration, *,\n                   speed_units=None,\n                   time_units=None,\n                   distance_units=None):\n    speed_units = require('speed_units', speed_units, 'mph')\n    time_units = require('time_units', time_units, 'hours')\n    distance_units = require(\n        'distance_units', distance_units, 'miles')\n \n    norm_speed = convert(speed, speed_units)\n    norm_duration = convert(duration, time_units)\n    norm_distance = norm_speed * norm_duration\n    distance = localize(norm_distance, distance_units)\n    print(f'{distance} {distance_units}')\n\n\nI can verify that this propagates the proper offending line by inspect-\ning the captured output:\nimport contextlib\nimport io\n \nfake_stderr = io.StringIO()\nwith contextlib.redirect_stderr(fake_stderr):\n    print_distance(1000, 3,\n                   speed_units='meters',\n                   time_units='seconds')\n \nprint(fake_stderr.getvalue())\n>>>\n1.8641182099494205 miles\n.../example.py:174: DeprecationWarning: distance_units will be \n¯required soon, update your code\n  print_distance(1000, 3,\nThe warnings module also lets me configure what should happen \nwhen a warning is encountered. One option is to make all warnings \nbecome errors, which raises the warning as an exception instead of \nprinting it out to sys.stderr:\nwarnings.simplefilter('error')\ntry:\n    warnings.warn('This usage is deprecated',\n                  DeprecationWarning)\nexcept DeprecationWarning:\n    pass  # Expected\nThis exception-raising behavior is especially useful for automated \ntests in order to detect changes in upstream dependencies and fail \ntests accordingly. Using such test failures is a great way to make it \nclear to the people you collaborate with that they will need to update \ntheir code. You can use the -W error command-line argument to the \nPython interpreter or the PYTHONWARNINGS environment variable to \napply this policy:\n$ python -W error example_test.py \nTraceback (most recent call last):\n  File \".../example_test.py\", line 6, in <module>\n    warnings.warn('This might raise an exception!')\nUserWarning: This might raise an exception!\n \nItem 89: Consider warnings to Refactor and Migrate Usage \n423\n\n\n424 \nChapter 10 Collaboration\nOnce the people responsible for code that depends on a deprecated \nAPI are aware that they’ll need to do a migration, they can tell the \nwarnings module to ignore the error by using the simplefilter and \nfilterwarnings functions (see https://docs.python.org/3/library/\nwarnings for all the details):\nwarnings.simplefilter('ignore')\nwarnings.warn('This will not be printed to stderr')\nAfter a program is deployed into production, it doesn’t make sense for \nwarnings to cause errors because they might crash the program at a \ncritical time. Instead, a better approach is to replicate warnings into \nthe logging built-in module. Here, I accomplish this by calling the \nlogging.captureWarnings function and configuring the corresponding \n'py.warnings' logger:\nimport logging\n \nfake_stderr = io.StringIO()\nhandler = logging.StreamHandler(fake_stderr)\nformatter = logging.Formatter(\n    '%(asctime)-15s WARNING] %(message)s')\nhandler.setFormatter(formatter)\n \nlogging.captureWarnings(True)\nlogger = logging.getLogger('py.warnings')\nlogger.addHandler(handler)\nlogger.setLevel(logging.DEBUG)\n \nwarnings.resetwarnings()\nwarnings.simplefilter('default')\nwarnings.warn('This will go to the logs output')\n \nprint(fake_stderr.getvalue())\n>>>\n2019-06-11 19:48:19,132 WARNING] .../example.py:227: \n¯UserWarning: This will go to the logs output\n  warnings.warn('This will go to the logs output')\nUsing logging to capture warnings ensures that any error reporting \nsystems that my program already has in place will also receive notice \nof important warnings in production. This can be especially useful if \nmy tests don’t cover every edge case that I might see when the pro-\ngram is undergoing real usage.\n\n\nAPI library maintainers should also write unit tests to verify that \nwarnings are generated under the correct circumstances with clear \nand actionable messages (see Item 76: “Verify Related Behaviors in \nTestCase Subclasses”). Here, I use the warnings.catch_warnings func-\ntion as a context manager (see Item 66: “Consider contextlib and \nwith Statements for Reusable try/finally Behavior” for background) \nto wrap a call to the require function that I defined above:\nwith warnings.catch_warnings(record=True) as found_warnings:\n    found = require('my_arg', None, 'fake units')\n    expected = 'fake units'\n    assert found == expected\nOnce I’ve collected the warning messages, I can verify that their num-\nber, detail messages, and categories match my expectations:\nassert len(found_warnings) == 1\nsingle_warning = found_warnings[0]\nassert str(single_warning.message) == (\n    'my_arg will be required soon, update your code')\nassert single_warning.category == DeprecationWarning\nThings to Remember\n✦ The warnings module can be used to notify callers of your API about \ndeprecated usage. Warning messages encourage such callers to fix \ntheir code before later changes break their programs.\n✦ Raise warnings as errors by using the -W error command-line argu-\nment to the Python interpreter. This is especially useful in auto-\nmated tests to catch potential regressions of dependencies.\n✦ In production, you can replicate warnings into the logging module \nto ensure that your existing error reporting systems will capture \nwarnings at runtime.\n✦ It’s useful to write tests for the warnings that your code generates to \nmake sure that they’ll be triggered at the right time in any of your \ndownstream dependencies.\nItem 90:  Consider Static Analysis via typing to \nObviate Bugs\nProviding documentation is a great way to help users of an API under-\nstand how to use it properly (see Item 84: “Write Docstrings for Every \nFunction, Class, and Module”), but often it’s not enough, and incor-\nrect usage still causes bugs. Ideally, there would be a programmatic \n \nItem 90: Consider Static Analysis via typing to Obviate Bugs \n425\n\n\n426 \nChapter 10 Collaboration\nmechanism to verify that callers are using your APIs the right way, \nand that you are using your downstream dependencies correctly. \nMany programming languages address part of this need with com-\npile-time type checking, which can identify and eliminate some cate-\ngories of bugs.\nHistorically Python has focused on dynamic features and has not \nprovided compile-time type safety of any kind. However, more recently \nPython has introduced special syntax and the built-in typing mod-\nule, which allow you to annotate variables, class fields, functions, \nand methods with type information. These type hints allow for grad-\nual typing, where a codebase can be incrementally updated to specify \ntypes as desired.\nThe benefit of adding type information to a Python program is that \nyou can run static analysis tools to ingest a program’s source code \nand identify where bugs are most likely to occur. The typing built-in \nmodule doesn’t actually implement any of the type checking function-\nality itself. It merely provides a common library for defining types, \nincluding generics, that can be applied to Python code and consumed \nby separate tools.\nMuch as there are multiple distinct implementations of the Python \ninterpreter (e.g., CPython, PyPy), there are multiple implementa-\ntions of static analysis tools for Python that use typing. As of the time \nof this writing, the most popular tools are mypy (https://github.com/\npython/mypy), \npytype \n(https://github.com/google/pytype), \npyright \n(https://github.com/microsoft/pyright), and pyre (https://pyre-check.\norg). For the typing examples in this book, I’ve used mypy with the \n--strict flag, which enables all of the various warnings supported by the \ntool. Here’s an example of what running the command line looks like:\n$ python3 -m mypy --strict example.py\nThese tools can be used to detect a large number of common errors \nbefore a program is ever run, which can provide an added layer of \nsafety in addition to having good unit tests (see Item 76: “Verify \nRelated Behaviors in TestCase Subclasses”). For example, can you \nfind the bug in this simple function that causes it to compile fine but \nthrow an exception at runtime?\ndef subtract(a, b):\n    return a - b\n \nsubtract(10, '5')\n>>>\nTraceback ...\nTypeError: unsupported operand type(s) for -: 'int' and 'str'\n\n\nParameter and variable type annotations are delineated with a colon \n(such as name: type). Return value types are specified with -> type \nfollowing the argument list. Using such type annotations and mypy, \nI can easily spot the bug:\ndef subtract(a: int, b: int) -> int:  # Function annotation\n    return a - b\n \nsubtract(10, '5')  # Oops: passed string value\n$ python3 -m mypy --strict example.py\n.../example.py:4: error: Argument 2 to \"subtract\" has \nincompatible type \"str\"; expected \"int\"\nAnother common mistake, especially for programmers who have \nrecently moved from Python 2 to Python 3, is mixing bytes and str \ninstances together (see Item 3: “Know the Differences Between bytes \nand str”). Do you see the problem in this example that causes a run-\ntime error?\ndef concat(a, b):\n    return a + b\n \nconcat('first', b'second')\n>>>\nTraceback ...\nTypeError: can only concatenate str (not \"bytes\") to str\nUsing type hints and mypy, this issue can be detected statically before \nthe program runs:\ndef concat(a: str, b: str) -> str:\n    return a + b\n \nconcat('first', b'second')  # Oops: passed bytes value\n$ python3 -m mypy --strict example.py\n.../example.py:4: error: Argument 2 to \"concat\" has \n¯incompatible type \"bytes\"; expected \"str\"\nType annotations can also be applied to classes. For example, this \nclass has two bugs in it that will raise exceptions when the program \nis run:\nclass Counter:\n    def __init__(self):\n        self.value = 0\n \n \nItem 90: Consider Static Analysis via typing to Obviate Bugs \n427\n\n\n428 \nChapter 10 Collaboration\n    def add(self, offset):\n        value += offset\n \n    def get(self) -> int:\n        self.value\nThe first one happens when I call the add method:\ncounter = Counter()\ncounter.add(5)\n>>>\nTraceback ...\nUnboundLocalError: local variable 'value' referenced before \n¯assignment\nThe second bug happens when I call get:\ncounter = Counter()\nfound = counter.get()\nassert found == 0, found\n>>>\nTraceback ...\nAssertionError: None\nBoth of these problems are easily found by mypy:\nclass Counter:\n    def __init__(self) -> None:\n        self.value: int = 0  # Field / variable annotation\n \n    def add(self, offset: int) -> None:\n        value += offset      # Oops: forgot \"self.\"\n \n    def get(self) -> int:\n        self.value           # Oops: forgot \"return\"\n \ncounter = Counter()\ncounter.add(5)\ncounter.add(3)\nassert counter.get() == 8\n$ python3 -m mypy --strict example.py\n.../example.py:6: error: Name 'value' is not defined\n.../example.py:8: error: Missing return statement\n\n\nOne of the strengths of Python’s dynamism is the ability to write \ngeneric functionality that operates on duck types (see Item 15: “Be \nCautious When Relying on dict Insertion Ordering” and Item 43: \n“Inherit from collections.abc for Custom Container Types”). This \nallows one implementation to accept a wide range of types, saving a \nlot of duplicative effort and simplifying testing. Here, I’ve defined such \na generic function for combining values from a list. Do you under-\nstand why the last assertion fails?\ndef combine(func, values):\n    assert len(values) > 0\n \n    result = values[0]\n    for next_value in values[1:]:\n        result = func(result, next_value)\n \n    return result\n \ndef add(x, y):\n    return x + y\n \ninputs = [1, 2, 3, 4j]\nresult = combine(add, inputs)\nassert result == 10, result  # Fails\n>>>\nTraceback ...\nAssertionError: (6+4j)\nI can use the typing module’s support for generics to annotate this \nfunction and detect the problem statically:\nfrom typing import Callable, List, TypeVar\n \nValue = TypeVar('Value')\nFunc = Callable[[Value, Value], Value]\n \ndef combine(func: Func[Value], values: List[Value]) -> Value:\n    assert len(values) > 0\n \n    result = values[0]\n    for next_value in values[1:]:\n        result = func(result, next_value)\n \n    return result\n \n \nItem 90: Consider Static Analysis via typing to Obviate Bugs \n429\n",
      "page_number": 442,
      "chapter_number": 42,
      "summary": "For this purpose, Python provides the built-in warnings module Key topics include warnings, typing, and type. Instead, you need a way to notify and encourage the people \nthat you collaborate with to refactor their code and migrate their API \nusage to the latest forms.",
      "keywords": [
        "units",
        "distance",
        "warnings",
        "speed",
        "Item",
        "Python",
        "duration",
        "norm",
        "time",
        "API",
        "code",
        "function",
        "arguments",
        "type",
        "error"
      ],
      "concepts": [
        "warnings",
        "typing",
        "type",
        "error",
        "value",
        "item",
        "function",
        "functions",
        "units",
        "logging"
      ],
      "similar_chapters": [
        {
          "book": "Python Cookbook 3rd",
          "chapter": 14,
          "title": "Testing, Debugging, and Exceptions",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 21,
          "title": "Segment 21 (pages 167-174)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 12,
          "title": "[ 427 ]",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 11,
          "title": "[ 383 ]",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 3,
          "title": "Lexical Conventions and Syntax",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 452-461)",
      "start_page": 452,
      "end_page": 461,
      "detection_method": "topic_boundary",
      "content": "430 \nChapter 10 Collaboration\nReal = TypeVar('Real', int, float)\n \ndef add(x: Real, y: Real) -> Real:\n    return x + y\n \ninputs = [1, 2, 3, 4j]  # Oops: included a complex number\nresult = combine(add, inputs)\nassert result == 10\n$ python3 -m mypy --strict example.py\n.../example.py:21: error: Argument 1 to \"combine\" has \n¯incompatible type \"Callable[[Real, Real], Real]\"; expected \n¯\"Callable[[complex, complex], complex]\"\nAnother extremely common error is to encounter a None value when \nyou thought you’d have a valid object (see Item 20: “Prefer Raising \nExceptions to Returning None”). This problem can affect seemingly \nsimple code. Do you see the issue here?\ndef get_or_default(value, default):\n    if value is not None:\n        return value\n    return value\n \nfound = get_or_default(3, 5)\nassert found == 3\n \nfound = get_or_default(None, 5)\nassert found == 5, found  # Fails\n>>>\nTraceback ...\nAssertionError: None\nThe typing module supports option types, which ensure that pro-\ngrams only interact with values after proper null checks have been \nperformed. This allows mypy to infer that there’s a bug in this code: \nThe type used in the return statement must be None, and that doesn’t \nmatch the int type required by the function signature:\nfrom typing import Optional\n \ndef get_or_default(value: Optional[int],\n                   default: int) -> int:\n    if value is not None:\n        return value\n    return value  # Oops: should have returned \"default\"\n\n\n$ python3 -m mypy --strict example.py\n.../example.py:7: error: Incompatible return value type (got \n¯\"None\", expected \"int\")\nA wide variety of other options are available in the typing module. \nSee https://docs.python.org/3.8/library/typing for all of the details. \nNotably, exceptions are not included. Unlike Java, which has checked \nexceptions that are enforced at the API boundary of every method, \nPython’s type annotations are more similar to C#’s: Exceptions are \nnot considered part of an interface’s definition. Thus, if you want to \nverify that you’re raising and catching exceptions properly, you need \nto write tests.\nOne common gotcha in using the typing module occurs when you \nneed to deal with forward references (see Item 88: “Know How to \nBreak Circular Dependencies” for a similar problem). For example, \nimagine that I have two classes and one holds a reference to the other:\nclass FirstClass:\n    def __init__(self, value):\n        self.value = value\n \nclass SecondClass:\n    def __init__(self, value):\n        self.value = value\n \nsecond = SecondClass(5)\nfirst = FirstClass(second)\nIf I apply type hints to this program and run mypy it will say that \nthere are no issues:\nclass FirstClass:\n    def __init__(self, value: SecondClass) -> None:\n        self.value = value\n \nclass SecondClass:\n    def __init__(self, value: int) -> None:\n        self.value = value\n \nsecond = SecondClass(5)\nfirst = FirstClass(second)\n$ python3 -m mypy --strict example.py\n \nItem 90: Consider Static Analysis via typing to Obviate Bugs \n431\n\n\n432 \nChapter 10 Collaboration\nHowever, if you actually try to run this code, it will fail because \nSecondClass \nis \nreferenced \nby \nthe \ntype \nannotation \nin \nthe \nFirstClass.__init__ method’s parameters before it’s actually defined:\nclass FirstClass:\n    def __init__(self, value: SecondClass) -> None:  # Breaks\n        self.value = value\n \nclass SecondClass:\n    def __init__(self, value: int) -> None:\n        self.value = value\n \nsecond = SecondClass(5)\nfirst = FirstClass(second)\n>>>\nTraceback ...\nNameError: name 'SecondClass' is not defined\nOne workaround supported by these static analysis tools is to use \na string as the type annotation that contains the forward reference. \nThe string value is later parsed and evaluated to extract the type \ninformation to check:\nclass FirstClass:\n    def __init__(self, value: 'SecondClass') -> None:  # OK\n        self.value = value\n \nclass SecondClass:\n    def __init__(self, value: int) -> None:\n        self.value = value\n \nsecond = SecondClass(5)\nfirst = FirstClass(second)\nA better approach is to use from __future__ import annotations, \nwhich is available in Python 3.7 and will become the default in \nPython 4. This instructs the Python interpreter to completely ignore \nthe values supplied in type annotations when the program is being \nrun. This resolves the forward reference problem and provides a per-\nformance improvement at program start time:\nfrom __future__ import annotations\n \nclass FirstClass:\n    def __init__(self, value: SecondClass) -> None:  # OK\n        self.value = value\n \n\n\n \nItem 90: Consider Static Analysis via typing to Obviate Bugs \n433\nclass SecondClass:\n    def __init__(self, value: int) -> None:\n        self.value = value\n \nsecond = SecondClass(5)\nfirst = FirstClass(second)\nNow that you’ve seen how to use type hints and their potential bene-\nfits, it’s important to be thoughtful about when to use them. Here are \nsome of the best practices to keep in mind:\n \n■It’s going to slow you down if you try to use type annotations from \nthe start when writing a new piece of code. A general strategy is \nto write a first version without annotations, then write tests, and \nthen add type information where it’s most valuable.\n \n■Type hints are most important at the boundaries of a codebase, \nsuch as an API you provide that many callers (and thus other \npeople) depend on. Type hints complement integration tests (see \nItem 77: “Isolate Tests from Each Other with setUp, tearDown, \nsetUpModule, and tearDownModule”) and warnings (see Item 89: \n“Consider warnings to Refactor and Migrate Usage”) to ensure that \nyour API callers aren’t surprised or broken by your changes.\n \n■It can be useful to apply type hints to the most complex and error-\nprone parts of your codebase that aren’t part of an API. However, \nit may not be worth striving for 100% coverage in your type anno-\ntations because you’ll quickly encounter diminishing returns.\n \n■If possible, you should include static analysis as part of your \nautomated build and test system to ensure that every commit to \nyour codebase is vetted for errors. In addition, the configuration \nused for type checking should be maintained in the repository to \nensure that all of the people you collaborate with are using the \nsame rules.\n \n■As you add type information to your code, it’s important to run \nthe type checker as you go. Otherwise, you may nearly finish \nsprinkling type hints everywhere and then be hit by a huge wall \nof errors from the type checking tool, which can be disheartening \nand make you want to abandon type hints altogether.\nFinally, it’s important to acknowledge that in many situations, you \nmay not need or want to use type annotations at all. For small pro-\ngrams, ad-hoc code, legacy codebases, and prototypes, type hints \nmay require far more effort than they’re worth.\n\n\n434 \nChapter 10 Collaboration\nThings to Remember\n✦ Python has special syntax and the typing built-in module for \nannotating variables, fields, functions, and methods with type \ninformation.\n✦ Static type checkers can leverage type information to help you avoid \nmany common bugs that would otherwise happen at runtime.\n✦ There are a variety of best practices for adopting types in your pro-\ngrams, using them in APIs, and making sure they don’t get in the \nway of your productivity.\n\n\nIndex\nSymbols\n* (asterisk) operator\nkeyword-only arguments, 98\nvariable positional arguments, \n87–88\n@ (at) symbol, decorators, 101\n** (double asterisk) operator, \nkeyword arguments, 90–91\n/ (forward slash) operator, \npositional-only arguments, 99\n% (percent) operator\nbytes versus str instances, 8–9\nformatting strings, 11\n+ (plus) operator, bytes versus str \ninstances, 7\n_ (underscore) variable name, 149\n:= (walrus) operator\nassignment expression, 35–41\nin comprehensions, 112–114\n__call__ method, 154–155\n@classmethod, 155–160\n__format__method, 16\n__getattr__ method, 195–201\n__getattribute__ method, 195–201\n__init__ method, 160–164\n__init_subclass__ method\nregistering classes, 208–213\nvalidating subclasses, 201–208\n__iter__ method, 119, 244–245\n__missing__ method (dictionary \nsubclasses), 73–75\n@property decorator\ndescriptors versus, 190–195\nrefactoring attributes with, \n186–189\nsetter attribute, 182–185\n__set_name__ method, annotating \nattributes, 214–218\n__setattr__ method, 195–201\nCPython, 230\nA\nAPIs\nmigrating usage, 418–425\nroot exceptions for, 408–413\nstability, 403–405\narguments\ndynamic default values, 93–96\niterating over, 116–121\nkeyword, 89–92\nkeyword-only, 96–101\npositional-only, 96–101\nvariable positional, 86–89\nassertions in TestCase subclasses, \n359\nassignment expressions\nin comprehensions, 110–114\nscope and, 85\nwalrus (:=) operator, 35–41\nassociative arrays, 43\nasterisk (*) operator\nkeyword-only arguments, 98\nvariable positional arguments, \n87–88\nasyncio built-in module\navoiding blocking, 289–292\ncombining threads and \ncoroutines, 282–288\nporting threaded I/O to, 271–282\nat (@) symbol, decorators, 101\n\n\n436 \nIndex\nattributes\nannotating, 214–218\ndynamic, 181\ngetter and setter methods versus, \n181–185\nlazy, 195–201\npublic versus private, 169–174\nrefactoring, 186–189\nB\nbinary data, converting to Unicode, \n6–7\nbinary operators, bytes versus str \ninstances, 8\nbisect built-in module, 334–336\nblocking asyncio event loop, \navoiding, 289–292\nblocking I/O (input/output) with \nthreads, 230–235\nbreaking circular dependencies, \n413–418\nbreakpoint built-in function, \n379–384\nbuffer protocol, 348\nbuilt-in types, classes versus, \n145–148\nbytearray built-in type, 346–351\nbytecode, 230\nbytes instances, str instances \nversus, 5–10\nC\nC extensions, 292–293\nC3 linearization, 162\ncallables, 154\ncatch-all unpacking, slicing versus, \n48–52\ncharacter data, bytes versus str \ninstances, 5–10\nchecked exceptions, 82\nchild processes, managing, \n226–230\ncircular dependencies, breaking, \n413–418\nclasses, 145\nattributes. See attributes\nbuilt-in types versus, 145–148\ndecorators, 218–224\ndocumentation, 398–399\nfunction interfaces versus, \n151–155\ninitializing parent classes, \n160–164\nmetaclasses. See metaclasses\nmix-in classes, 164–169\npolymorphism, 155–160\npublic versus private attributes, \n169–174\nrefactoring to, 148–151\nregistering, 208–213\nserializing, 168–169\nvalidating subclasses, 201–208\nversioning, 316–317\nclosures, variable scope and, 83–86\ncollaboration\nbreaking circular dependencies, \n413–418\ndynamic import, 417–418\nimport/configure/run, \n415–416\nreordering imports, 415–416\ncommunity-built modules, \n389–390\ndocumentation, 396–401\nmigrating API usage, 418–425\norganizing modules into \npackages, 401–406\nroot exceptions for APIs, 408–413\nstatic analysis, 425–434\nvirtual environments, 390–396\ncollections.abc module, inheritance \nfrom, 174–178\ncombining iterator items, 139–142\ncommands for interactive debugger, \n381\ncommunity-built modules, 389–390\ncompile-time static type checking, \n353\ncomplex sort criteria with key \nparameter, 52–58\ncomprehensions, 107\nassignment expressions in, \n110–114\ngenerator expressions for, \n121–122\nmap and filter functions versus, \n107–109\n\n\n \nIndex \n437\nmultiple subexpressions in, \n109–110\nconcurrency, 225\navoiding threads for fan-out, \n252–256\nfan-in, 252\nfan-out, 252\nhighly concurrent I/O (input/\noutput), 266–271\nparallelism versus, 225\nwith pipelines, 238–247\npreventing data races, 235–238\nusing Queue class for, 257–263\nusing ThreadPoolExecutor for, \n264–266\nwith threads, 230–235\nwhen to use, 248–252\nconcurrent.futures built-in module, \n292–297\nconfiguring deployment \nenvironments, 406–408\nconflicts with dependencies, \n390–396\ncontainers\ninheritance from collections.abc \nmodule, 174–178\niterator protocol, 119–121\ncontextlib built-in module, 304–308\nCoordinated Universal Time (UTC), \n308\ncopyreg built-in module, 312–319\ncoroutines, 266–271\ncombining with threads, \n282–288\nC-style strings, f-strings versus, \n11–21\ncustom container types, inheritance \nfrom collections.abc module, \n174–178\nD\ndata races, preventing, 235–238\ndatetime built-in module, 308–312\ndebugging\nwith interactive debugger, \n379–384\nmemory usage, 384–387\nwith repr strings, 354–357\nwith static analysis, 425–434\nDecimal class, rounding numbers, \n319–322\ndecorators\nclass decorators, 218–224\nfunction decorators, 101–104\ndefault arguments\ndynamic, 93–96\nwith pickle built-in module, \n315–316\ndefault values in dictionaries\n__missing__ method, 73–75\ndefaultdict versus setdefault \nmethods, 70–72\nget method versus in \nexpressions, 65–70\ndefaultdict class, setdefault method \nversus, 70–72\ndependencies\nbreaking circular, 413–418\nconflicts, 390–396\nencapsulating, 375–379\ninjecting, 378–379\nreproducing, 394–396\ntesting with mocks, 367–375\ndependency hell, 391\ndeployment environments, \nconfiguring, 406–408\ndeque class, 326–334\ndescriptor protocol, 191\ndescriptors versus @property \ndecorator, 190–195\ndeserializing with pickle built-in \nmodule, 312–319\ndevelopment environment, 406–407\ndiamond inheritance, 161–162, \n207–208\ndictionaries, 43\ninsertion ordering, 58–65\nmissing keys\n__missing__ method, 73–75\ndefaultdict versus setdefault \nmethods, 70–72\nget method versus in \nexpressions, 65–70\nnesting, 145–148\ntuples versus in format strings, \n13–15\n\n\n438 \nIndex\ndictionary comprehensions, \n108–109\ndocstrings\nfor dynamic default arguments, \n93–96\nwriting, 396–401\nfor classes, 398–399\nfor functions, 399–400\nfor modules, 397–398\ntype annotations and, \n400–401\ndocumentation. See docstrings\ndouble asterisk (**) operator, \nkeyword arguments, 90–91\ndouble-ended queues, 331\nduck typing, 61, 429\ndynamic attributes, 181\ndynamic default arguments, 93–96\ndynamic import, 417–418\nE\nelse blocks\nfor statements, 32–35\nexception handling, 299–304\nencapsulating dependencies, \n375–379\nenumerate built-in function, range \nbuilt-in function versus, 28–30\nexcept blocks, exception handling, \n299–304\nexception handling with try/except/\nelse/finally blocks, 299–304\nexceptions\nraising, None return value \nversus, 80–82\nroot exceptions for APIs, 408–413\nexpressions\nhelper functions versus, 21–24\nPEP 8 style guide, 4\nF\nfakes, mocks versus, 368\nfan-in, 252\nwith Queue class, 257–263\nwith ThreadPoolExecutor class, \n264–265\nfan-out, 252\navoiding threads for, 252–256\nwith Queue class, 257–263\nwith ThreadPoolExecutor class, \n264–265\nFIFO (first-in, first-out) queues, \n326–334\nfile operations, bytes versus str \ninstances, 9–10\nfilter built-in function, \ncomprehensions versus, \n107–109\nfinally blocks\nexception handling, 299–304\nwith statements versus, 304–308\nfirst-class functions, 152\nfirst-in, first-out (FIFO) queues, \n326–334\nfor loops, avoiding else blocks, \n32–35\nformat built-in function, 15–19\nformat strings\nbytes versus str instances, 8–9\nC-style strings versus f-strings, \n11–21\nformat built-in function, \n15–19\nf-strings explained, 19–21\ninterpolated format strings, \n19–21\nproblems with C-style strings, \n11–15\nstr.format method, 15–19\nforward slash (/) operator, \npositional-only arguments, 99\nf-strings\nC-style strings versus, 11–21\nstr.format method versus, 15–19\nexplained, 19–21\nfunctions, 77. See also generators\nclosures, variable scope and, \n83–86\ndecorators, 101–104\ndocumentation, 399–400\ndynamic default arguments, \n93–96\nas hooks, 151–155\nkeyword arguments, 89–92\nkeyword-only arguments, 96–101\nNone return value, raising \nexceptions versus, 80–82\n\n\n \nIndex \n439\nin pipelines, 238–247\npositional-only arguments, \n96–101\nmultiple return values, 77–80\nvariable positional arguments, \n86–89\nfunctools.wraps method, 101–104\nG\ngc built-in module, 384–386\ngenerator expressions, 121–122\ngenerators, 107\nyield from for composing, \n123–126\ninjecting data into, 126–131\nitertools module with, 136–142\nreturning lists versus, 114–116\nsend method, 126–131\nthrow method, 132–136\ngeneric object construction, \n155–160\nget method for missing dictionary \nkeys, 65–70\ngetter methods, attributes versus, \n181–185\nGIL (global interpreter lock), 230–\n235, 292\ngradual typing, 426\nH\nhasattr built-in function, 198–199\nhash tables, 43\nheapq built-in module, 336–346\nheaps, 341\nhelper functions, expressions \nversus, 21–24\nhighly concurrent I/O, 266–271\nhooks, functions as, 151–155\nI\nif/else conditional expressions, 23\nimport paths, stabilizing, 317–319\nimporting modules, 5, 414–415\nin expressions for missing \ndictionary keys, 65–70\nindexing\nslicing and, 44\nunpacking versus, 24–28\ninheritance\nfrom collections.abc module, \n174–178\ndiamond inheritance, 161–162, \n207–208\ninitializing parent classes, 160–164\ninjecting\ndata into generators, 126–131\ndependencies, 378–379\nmocks, 371–375\ninput/output (I/O). See I/O (input/\noutput)\ninsertion ordering, dictionaries, \n58–65\ninstalling modules, 389–390\nintegration tests, unit tests versus, \n365\ninteractive debugging, 379–384\ninterfaces, 145\nsimple functions for, 151–155\ninterpolated format strings. See \nf-strings\nI/O (input/output)\navoiding blocking asyncio event \nloop, 289–292\nusing threads for, 230–235\nhighly concurrent, 266–271\nporting threaded I/O to asyncio \nbuilt-in module, 271–282\nzero-copy interactions, 346–351\nisolating tests, 365–367\niterator protocol, 119–121\niterators. See also loops\ncombining items, 139–142\nfiltering items, 138–139\ngenerator expressions and, \n121–122\ngenerator functions and, 115–116\nlinking, 136–138\nas function arguments, 116–121\nStopIteration exception, 117\nitertools module, 136–142\nitertools.accumulate method, \n139–140\nitertools.chain method, 136\nitertools.combinations method, 141\nitertools.combinations_with_\nreplacement method, 141–142\nitertools.cycle method, 137\n",
      "page_number": 452,
      "chapter_number": 43,
      "summary": "This chapter covers segment 43 (pages 452-461). Key topics include types, typing, and classes. Covers method. Here, I’ve defined such \na generic function for combining values from a list.",
      "keywords": [
        "Cautious When Relying",
        "type",
        "versus",
        "type hints",
        "built-in module",
        "method",
        "built-in",
        "Static Analysis",
        "type annotations",
        "versus str instances",
        "module",
        "method versus",
        "built-in types versus",
        "bytes versus str",
        "arguments"
      ],
      "concepts": [
        "types",
        "typing",
        "classes",
        "value",
        "method",
        "module",
        "built",
        "functionality",
        "function",
        "functions"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 29,
          "title": "Segment 29 (pages 576-595)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 26,
          "title": "Segment 26 (pages 815-849)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 71,
          "title": "Segment 71 (pages 2281-2311)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 14,
          "title": "Segment 14 (pages 272-289)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 49,
          "title": "Segment 49 (pages 993-1010)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 462-467)",
      "start_page": 462,
      "end_page": 467,
      "detection_method": "topic_boundary",
      "content": "440 \nIndex\nitertools.dropwhile method, 139\nitertools.filterfalse method, 139\nitertools.islice method, 138\nitertools.permutations method, \n140–141\nitertools.product method, 140\nitertools.repeat method, 136\nitertools.takewhile method, 138\nitertools.tee method, 137\nitertools.zip_longest method, 31–32, \n137–138\nJ\njson built-in module, 313\nK\nkey parameter, sorting lists, 52–58\nKeyError exceptions for missing \ndictionary keys, 65–70\nkeys\nhandling in dictionaries\n__missing__ method, 73–75\ndefaultdict versus setdefault \nmethods, 70–72\nget method versus in \nexpressions, 65–70\nkeyword arguments, 89–92\nkeyword-only arguments, 96–101\nL\nlazy attributes, 195–201\nleaks (memory), debugging, \n384–387\nlinking iterators, 136–138\nlist comprehensions, 107–108\ngenerator expressions versus, \n121–122\nlists, 43. See also comprehensions\nas FIFO queues, 326–331\nas return values, generators \nversus, 114–116\nslicing, 43–46\ncatch-all unpacking versus, \n48–52\nstriding with, 46–48\nsorting\nwith key parameter, 52–58\nsearching sorted lists, \n334–336\nlocal time, 308–312\nLock class, preventing data races, \n235–238\nloops. See also comprehensions\nelse blocks, avoiding, 32–35\nrange versus enumerate built-in \nfunctions, 28–30\nzip built-in function, 30–32\nM\nmap built-in function, \ncomprehensions versus, \n107–109\nmemory usage, debugging, 384–387\nmemoryview built-in type, 346–351\nmetaclasses, 181\nannotating attributes, 214–218\nclass decorators versus, 218–224\nregistering classes, 208–213\nvalidating subclasses, 201–208\nmigrating API usage, 418–425\nmissing dictionary keys\n__missing__ method, 73–75\ndefaultdict versus setdefault \nmethods, 70–72\nget method versus in \nexpressions, 65–70\nmix-in classes, 164–169\nmocks\nencapsulating dependencies for, \n375–379\ntesting with, 367–375\nmodules\ndocumentation, 397–398\nimporting, 5, 414–415\ndynamic import, 417–418\nimport/configure/run, \n415–416\nreordering imports, 415–416\ninstalling, 389–390\norganizing into packages, \n401–406\nmodule-scoped code, 406–408\nmultiple assignment. See tuples\nmultiple return values, unpacking, \n77–80\nmultiple generators, composing with \nyield from expression, 123–126\n\n\n \nIndex \n441\nmultiprocessing built-in module, \n292–297\nmulti-threaded program, converting \nfrom single-threaded to, \n248–252\nmutexes (mutual-exclusion locks), \npreventing data races, \n235–238\nN\nnamedtuple type, 149–150\nnamespaces, 402–403\nnaming conventions, 3–4\nnegative numbers for slicing, 44\nnested built-in types, classes \nversus, 145–148\nNone\nfor dynamic default arguments, \n93–96\nraising exceptions versus \nreturning, 80–82\nnonlocal statement, 85–86\nO\nobjects, generic construction, \n155–160\noptimizing, profiling before, \n322–326\noption types, 430\noptional arguments, extending \nfunctions with, 92\nOrderedDict class, 61\norganizing modules into packages, \n401–406\nP\npackages\ninstalling, 389–390\norganizing modules into, \n401–406\nparallel iteration, zip built-in \nfunction, 30–32\nparallelism, 225\navoiding threads, 230–235\nconcurrency versus, 225\nwith concurrent.futures built-in \nmodule, 292–297\nmanaging child processes, \n226–230\nparent classes, initializing, 160–164\npdb built-in module, 379–384\nPEP 8 style guide, 2–5\npercent (%) operator\nbytes versus str instances, 8–9\ndictionaries versus tuples with, \n13–15\nformatting strings, 11\nperformance, 299\nfirst-in, first-out (FIFO) queues, \n326–334\npriority queues, 336–346\nprofiling before optimizing, \n322–326\nsearching sorted lists, 334–336\nzero-copy interactions, 346–351\npickle built-in module, 312–319\npip command-line tool, 389–390\npipelines\ncoordinating threads with, \n238–247\nparallel processes, chains of, \n228–229\nrefactoring to use Queue for, \n257–263\nplus (+) operator, bytes versus str \ninstances, 7\npolymorphism, 155–160\nporting threaded I/O to asyncio \nbuilt-in module, 271–282\npositional arguments, variable, \n86–89\npositional-only arguments, 96–101\npost-mortem debugging, 382–384\nprint function, debugging with, \n354–357\npriority queues, 336–346\nprivate attributes, public attributes \nversus, 169–174\nprocesses, managing child \nprocesses, 226–230\nProcessPoolExecutor class, 295–297\nproducer-consumer queues, \n326–334\nproduction environment, 406\nprofiling before optimizing, 322–326\npublic attributes, private attributes \nversus, 169–174\nPylint, 5\n\n\n442 \nIndex\nPyPI (Python Package Index), \n389–390\nPython\ndetermining version used, 1–2\nstyle guide. See PEP 8 style guide\nPython 2, 1–2\nPython 3, 1–2\nPython Enhancement Proposal #8. \nSee PEP 8 style guide\nPython Package Index (PyPI), \n389–390\nPythonic style, 1\npytz module, 311–312\nQ\nQueue class\ncoordinating threads with, \n238–247\nrefactoring to use for \nconcurrency, 257–263\nR\nraising exceptions, None return \nvalue versus, 80–82\nrange built-in function, enumerate \nbuilt-in function versus,\n28–30\nrefactoring\nattributes, 186–189\nto break circular dependencies, \n415\nto classes, 148–151\nto use Queue class for \nconcurrency, 257–263\nregistering classes, 208–213\nreordering imports, 415–416\nrepetitive code, avoiding, 35–41\nrepr strings, debugging with, \n354–357\nreproducing dependencies, 394–396\nreturn values\ngenerators versus lists as, \n114–116\nNone return value, raising \nexceptions versus, 80–82\nunpacking multiple, 77–80\nreusable @property methods, \n190–195\nreusable try/finally blocks, \n304–308\nrobustness, 299\nexception handling with try/\nexcept/else/finally blocks, \n299–304\nreusable try/finally blocks, \n304–308\nrounding numbers, 319–322\nserialization/deserialization with \npickle, 312–319\ntime zone conversion, 308–312\nroot exceptions for APIs, 408–413\nrounding numbers with Decimal \nclass, 319–322\nrule of least surprise, 181\nS\nscope, closures and, 83–86\nscoping bug, 85\nsearching sorted lists, 334–336\nsend method in generators, 126–131\nsequences\nsearching sorted, 334–336\nslicing, 43–46\ncatch-all unpacking versus, \n48–52\nstriding, 46–48\nserializing\nclasses, 168–169\nwith pickle built-in module, \n312–319\nset comprehensions, 108–109\nsetdefault method (dictionaries), \n68–70\ndefaultdict method versus, 70–72\nsetter methods, attributes versus, \n181–185\nsetUp method (TestCase class), \n365–367\nsetUpModule function, 365–367\nsingle-threaded program, \nconverting to multi-threaded, \n248–252\nslicing\nmemoryview instances, 348\nsequences, 43–46\n\n\n \nIndex \n443\ncatch-all unpacking versus, \n48–52\nstriding, 46–48\nsoftware licensing, 390\nsorting\ndictionaries, insertion ordering, \n58–65\nlists\nwith key parameter, 52–58\nsearching sorted lists, \n334–336\nspeedup, 225\nstabilizing import paths, 317–319\nstable APIs, 403–405\nstable sorting, 56–57\nstar args, 86–89\nstarred expressions, 49–52\nstatements, PEP 8 style guide, 4\nstatic analysis, 425–434\nStopIteration exception, 117\nstr instances, bytes instances \nversus, 5–10\nstr.format method, 15–19\nstriding, 46–48\nstrings, C-style versus f-strings, \n11–21\nformat built-in function, 15–19\ninterpolated format strings, \n19–21\nproblems with C-style strings, \n11–15\nstr.format method, 15–19\nsubclasses, validating, 201–208\nsubexpressions in comprehensions, \n109–110\nsubprocess built-in module, \n226–230\nsuper built-in function, 160–164\nT\ntearDown method (TestCase class), \n365–367\ntearDownModule function, 365–367\nternary expressions, 23\ntest harness, 365\nTestCase subclasses\nisolating tests, 365–367\nverifying related behaviors, \n357–365\ntesting\nencapsulating dependencies for, \n375–379\nimportance of, 353–354\nisolating tests, 365–367\nwith mocks, 367–375\nwith TestCase subclasses, \n357–365\nunit versus integration tests, 365\nwith unittest built-in module, \n357\nThreadPoolExecutor class, 264–266\nthreads\navoiding for fan-out, 252–256\ncombining with coroutines, \n282–288\nconverting from single- to multi-\nthreaded program, 248–252\ncoordinating between, 238–247\nporting threaded I/O to asyncio \nbuilt-in module, 271–282\npreventing data races, 235–238\nrefactoring to use Queue class for \nconcurrency, 257–263\nThreadPoolExecutor class, \n264–266\nwhen to use, 230–235\nthrow method in generators, \n132–136\ntime built-in module, 308–312\ntime zone conversion, 308–312\ntimeout parameter for \nsubprocesses, 229–230\ntracemalloc built-in module, \n384–387\ntry blocks\nexception handling, 299–304\nversus with statements, 304–308\ntuples\ndictionaries versus with format \nstrings, 13–15\nindexing versus unpacking, \n24–28\nnamedtuple type, 149–150\nsorting with multiple criteria, \n55–56\n\n\n444 \nIndex\nunderscore (_) variable name in, \n149\ntype annotations, 82\ndocstrings and, 400–401\nwith static analysis, 425–434\ntype hints, 426\ntyping built-in module, 425–434\nU\nunderscore (_) variable name, 149\nUnicode data, converting to binary, \n6–7\nunit tests, integration tests versus, \n365\nunittest built-in module, 357\nunpacking\nindexing versus, 24–28\nmultiple return values, 77–80\nslicing versus, 48–52\nUTC (Coordinated Universal Time), \n308\nV\nvalidating subclasses, 201–208\nvariable positional arguments \n(varargs), 86–89\nvariable scope, closures and, 83–86\nvenv built-in module, 392–394\nversioning classes, 316–317\nversions of Python, determining \nversion used, 1–2\nvirtual environments, 390–396\nW\nwalrus (:=) operator\nassignment expression, 35–41\nin comprehensions, 112–114\nwarnings built-in module, 418–425\nweakref built-in module, 194\nwhile loops, avoiding else blocks, \n32–35\nwhitespace, 3\nwith statements for reusable try/\nfinally blocks, 304–308\nwith as targets, 306–308\nwriting docstrings, 396–401\nfor classes, 398–399\nfor functions, 399–400\nfor modules, 397–398\ntype annotations and, 400–401\nY\nyield from expressions, composing \nmultiple generators, 123–126\nZ\nzero-copy interactions, 346–351\nzip built-in function, 30–32\n",
      "page_number": 462,
      "chapter_number": 44,
      "summary": "This chapter covers segment 44 (pages 462-467). Key topics include method, module, and classes. Covers method. See I/O (input/\noutput)\ninsertion ordering, dictionaries, \n58–65\ninstalling modules, 389–390\nintegration tests, unit tests versus, \n365\ninteractive debugging, 379–384\ninterfaces, 145\nsimple functions for, 151–155\ninterpolated format strings.",
      "keywords": [
        "built-in module",
        "versus",
        "built-in",
        "method",
        "built-in function",
        "module",
        "asyncio built-in module",
        "method versus",
        "built-in function versus",
        "versus setdefault methods",
        "function",
        "zip built-in function",
        "attributes versus",
        "pickle built-in module",
        "unpacking versus"
      ],
      "concepts": [
        "method",
        "module",
        "classes",
        "built",
        "function",
        "functions",
        "index",
        "arguments",
        "typing",
        "type"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 38,
          "title": "Segment 38 (pages 345-352)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 5,
          "title": "Segment 5 (pages 36-43)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 32,
          "title": "Segment 32 (pages 320-329)",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "Praise for Effective Python\n“I have been recommending this book enthusiastically since the first edition \nappeared in 2015. This new edition, updated and expanded for Python 3, is a \ntreasure trove of practical Python programming wisdom that can benefit pro-\ngrammers of all experience levels.”\n—Wes McKinney, Creator of Python Pandas project, Director of Ursa Labs\n“If you’re coming from another language, this is your definitive guide to taking \nfull advantage of the unique features Python has to offer. I’ve been working with \nPython for nearly twenty years and I still learned a bunch of useful tricks, espe-\ncially around newer features introduced by Python 3. Effective Python is crammed \nwith actionable advice, and really helps define what our community means when \nthey talk about Pythonic code.”\n—Simon Willison, Co-creator of Django\n“I’ve been programming in Python for years and thought I knew it pretty well. \nThanks to this treasure trove of tips and techniques, I’ve discovered many ways \nto improve my Python code to make it faster (e.g., using bisect to search sorted \nlists), easier to read (e.g., enforcing keyword-only arguments), less prone to error \n(e.g., unpacking with starred expressions), and more Pythonic (e.g., using zip to \niterate over lists in parallel). Plus, the second edition is a great way to quickly get \nup to speed on Python 3 features, such as the walrus operator, f-strings, and the \ntyping module.”\n—Pamela Fox, Creator of Khan Academy programming courses\n“Now that Python 3 has finally become the standard version of Python, it’s \nalready gone through eight minor releases and a lot of new features have been \nadded throughout. Brett Slatkin returns with a second edition of Effective Python \nwith a huge new list of Python idioms and straightforward recommendations, \ncatching up with everything that’s introduced in version 3 all the way through \n3.8 that we’ll all want to use as we finally leave Python 2 behind. Early sections \nlay out an enormous list of tips regarding new Python 3 syntaxes and concepts \nlike string and byte objects, f-strings, assignment expressions (and their special \nnickname you might not know), and catch-all unpacking of tuples. Later sec-\ntions take on bigger subjects, all of which are packed with things I either didn’t \nknow or which I’m always trying to teach to others, including ‘Metaclasses and \nAttributes’ (good advice includes ‘Prefer Class Decorators over Metaclasses’ and \nalso introduces a new magic method ‘__init_subclass__()’ I wasn’t familiar with), \n‘Concurrency’ (favorite advice: ‘Use Threads for Blocking I/O, but not Parallel-\nism,’ but it also covers asyncio and coroutines correctly) and ‘Robustness and \nPerformance’ (advice given: ‘Profile before Optimizing’). It’s a joy to go through \neach section as everything I read is terrific best practice information smartly \nstated, and I’m considering quoting from this book in the future as it has such \ngreat advice all throughout. This is the definite winner for the ‘if you only read \none Python book this year...’ contest.\n—Mike Bayer, Creator of SQLAlchemy\n",
      "content_length": 3112,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "“This is a great book for both novice and experienced programmers. The code \nexamples and explanations are well thought out and explained concisely and \nthoroughly. The second edition updates the advice for Python 3, and it’s fantastic! \nI’ve been using Python for almost 20 years, and I learned something new every \nfew pages. The advice given in this book will serve anyone well.”\n—Titus Brown, Associate Professor at UC Davis\n“Once again, Brett Slatkin has managed to condense a wide range of solid prac-\ntices from the community into a single volume. From exotic topics like metaclasses \nand concurrency to crucial basics like robustness, testing, and collaboration, the \nupdated Effective Python makes a consensus view of what’s ‘Pythonic’ available to \na wide audience.”\n—Brandon Rhodes, Author of python-patterns.guide\n",
      "content_length": 826,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "Effective Python\nSecond Edition\n",
      "content_length": 32,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": "Effective Python\n90 SPECIFIC WAYS TO WRITE BETTER PYTHON\nSecond Edition\nBrett Slatkin\n",
      "content_length": 86,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "Many of the designations used by manufacturers and sellers to distinguish their \nproducts are claimed as trademarks. Where those designations appear in this \nbook, and the publisher was aware of a trademark claim, the designations have \nbeen printed with initial capital letters or in all capitals.\nThe author and publisher have taken care in the preparation of this book, but \nmake no expressed or implied warranty of any kind and assume no responsibility \nfor errors or omissions. No liability is assumed for incidental or consequential \ndamages in connection with or arising out of the use of the information or \nprograms contained herein.\nFor information about buying this title in bulk quantities, or for special \nsales opportunities (which may include electronic versions; custom cover \ndesigns; and content particular to your business, training goals, marketing \nfocus, or branding interests), please contact our corporate sales department \nat corpsales@pearsoned.com or (800) 382-3419.\nFor government sales inquiries, please contact governmentsales@pearsoned.com. \nFor questions about sales outside the U.S., please contact intlcs@pearson.com. \nVisit us on the Web: informit.com/aw\nLibrary of Congress Control Number: On file\nCopyright © 2020 Pearson Education, Inc.\nAll rights reserved. This publication is protected by copyright, and permission \nmust be obtained from the publisher prior to any prohibited reproduction, storage \nin a retrieval system, or transmission in any form or by any means, electronic, \nmechanical, photocopying, recording, or likewise. For information regarding \npermissions, request forms and the appropriate contacts within the Pearson \nEducation Global Rights & Permissions Department, please visit www.pearson.\ncom/permissions/.\nISBN-13: 978-0-13-485398-7\nISBN-10: 0-13-485398-9\nScoutAutomatedPrintCode\nEditor-in-Chief\nMark L. Taub\nExecutive Editor\nDeborah Williams\nDevelopment Editor\nChris Zahn\nManaging Editor\nSandra Schroeder\nSenior Project Editor\nLori Lyons\nProduction Manager\nAswini Kumar/codeMantra\nCopy Editor\nCatherine D. Wilson\nIndexer\nCheryl Lenser\nProofreader\nGill Editorial Services\nCover Designer\nChuti Prasertsith\nCompositor\ncodeMantra\n",
      "content_length": 2188,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "To our family\n",
      "content_length": 14,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "Contents at a Glance\nPreface \nxvii\nAcknowledgments \nxxi\nAbout the Author \nxxiii\nChapter 1: Pythonic Thinking \n1\nChapter 2: Lists and Dictionaries \n43\nChapter 3: Functions \n77\nChapter 4: Comprehensions and Generators \n107\nChapter 5: Classes and Interfaces \n145\nChapter 6: Metaclasses and Attributes \n181\nChapter 7: Concurrency and Parallelism \n225\nChapter 8: Robustness and Performance \n299\nChapter 9: Testing and Debugging \n353\nChapter 10: Collaboration \n389\nIndex \n435\n",
      "content_length": 470,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "Contents\nPreface \nxvii\nAcknowledgments \nxxi\nAbout the Author \nxxiii\nChapter 1 Pythonic Thinking \n1\nItem 1: Know Which Version of Python You’re Using \n1\nItem 2: Follow the PEP 8 Style Guide \n2\nItem 3: Know the Differences Between bytes and str \n5\nItem 4:  Prefer Interpolated F-Strings Over C-style \nFormat Strings and str.format \n11\nItem 5:  Write Helper Functions Instead of \nComplex Expressions \n21\nItem 6:  Prefer Multiple Assignment Unpacking \nOver Indexing \n24\nItem 7: Prefer enumerate Over range \n28\nItem 8: Use zip to Process Iterators in Parallel \n30\nItem 9: Avoid else Blocks After for and while Loops \n32\nItem 10:  Prevent Repetition with Assignment Expressions \n35\nChapter 2 Lists and Dictionaries \n43\nItem 11: Know How to Slice Sequences \n43\nItem 12:  Avoid Striding and Slicing in a Single Expression \n46\nItem 13: Prefer Catch-All Unpacking Over Slicing \n48\nItem 14:  Sort by Complex Criteria Using the key Parameter \n52\n",
      "content_length": 934,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "xii \nContents\nItem 15:  Be Cautious When Relying on dict \nInsertion Ordering \n58\nItem 16:  Prefer get Over in and KeyError to \nHandle Missing Dictionary Keys \n65\nItem 17:  Prefer defaultdict Over setdefault to \nHandle Missing Items in Internal State \n70\nItem 18:  Know How to Construct Key-Dependent \nDefault Values with __missing__ \n73\nChapter 3 Functions \n77\nItem 19:  Never Unpack More Than Three Variables \nWhen Functions Return Multiple Values \n77\nItem 20: Prefer Raising Exceptions to Returning None \n80\nItem 21:  Know How Closures Interact with Variable Scope \n83\nItem 22:  Reduce Visual Noise with Variable \nPositional Arguments \n87\nItem 23:  Provide Optional Behavior with Keyword Arguments \n90\nItem 24:  Use None and Docstrings to Specify \nDynamic Default Arguments \n94\nItem 25:  Enforce Clarity with Keyword-Only and \nPositional-Only Arguments \n97\nItem 26:  Define Function Decorators with functools.wraps \n102\nChapter 4 Comprehensions and Generators \n107\nItem 27:  Use Comprehensions Instead of map and filter \n107\nItem 28:  Avoid More Than Two Control Subexpressions in \nComprehensions \n109\nItem 29:  Avoid Repeated Work in Comprehensions by Using \nAssignment Expressions \n111\nItem 30:  Consider Generators Instead of Returning Lists \n114\nItem 31: Be Defensive When Iterating Over Arguments \n117\nItem 32:  Consider Generator Expressions for Large List \nComprehensions \n122\nItem 33: Compose Multiple Generators with yield from \n124\nItem 34:  Avoid Injecting Data into Generators with send \n127\nItem 35:  Avoid Causing State Transitions in \nGenerators with throw \n133\n",
      "content_length": 1579,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": " \nContents \nxiii\nItem 36:  Consider itertools for Working with Iterators \nand Generators \n138\nChapter 5 Classes and Interfaces \n145\nItem 37:  Compose Classes Instead of Nesting \nMany Levels of Built-in Types \n145\nItem 38:  Accept Functions Instead of Classes for \nSimple Interfaces \n152\nItem 39:  Use @classmethod Polymorphism to \nConstruct Objects Generically \n155\nItem 40: Initialize Parent Classes with super \n160\nItem 41:  Consider Composing Functionality \nwith Mix-in Classes \n165\nItem 42: Prefer Public Attributes Over Private Ones \n170\nItem 43:  Inherit from collections.abc for \nCustom Container Types \n175\nChapter 6 Metaclasses and Attributes \n181\nItem 44:  Use Plain Attributes Instead of Setter and \nGetter Methods \n181\nItem 45:  Consider @property Instead of \nRefactoring Attributes \n186\nItem 46:  Use Descriptors for Reusable @property Methods \n190\nItem 47:  Use __getattr__, __getattribute__, and \n__setattr__ for Lazy Attributes \n195\nItem 48: Validate Subclasses with __init_subclass__ \n201\nItem 49:  Register Class Existence with __init_subclass__ \n208\nItem 50: Annotate Class Attributes with __set_name__ \n214\nItem 51:  Prefer Class Decorators Over Metaclasses for \nComposable Class Extensions \n218\nChapter 7 Concurrency and Parallelism \n225\nItem 52: Use subprocess to Manage Child Processes \n226\nItem 53:  Use Threads for Blocking I/O, Avoid for Parallelism \n230\nItem 54: Use Lock to Prevent Data Races in Threads \n235\nItem 55:  Use Queue to Coordinate Work Between Threads \n238\nItem 56:  Know How to Recognize When Concurrency \nIs Necessary \n248\n",
      "content_length": 1565,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "xiv \nContents\nItem 57:  Avoid Creating New Thread Instances for \nOn-demand Fan-out \n252\nItem 58:  Understand How Using Queue for \nConcurrency Requires Refactoring \n257\nItem 59:  Consider ThreadPoolExecutor When Threads \nAre Necessary for Concurrency \n264\nItem 60:  Achieve Highly Concurrent I/O with Coroutines \n266\nItem 61: Know How to Port Threaded I/O to asyncio \n271\nItem 62:  Mix Threads and Coroutines to Ease the \nTransition to asyncio \n282\nItem 63:  Avoid Blocking the asyncio Event Loop to \nMaximize Responsiveness \n289\nItem 64:  Consider concurrent.futures for True Parallelism \n292\nChapter 8 Robustness and Performance \n299\nItem 65:  Take Advantage of Each Block in try/except \n/else/finally \n299\nItem 66:  Consider contextlib and with Statements \nfor Reusable try/finally Behavior \n304\nItem 67: Use datetime Instead of time for Local Clocks \n308\nItem 68: Make pickle Reliable with copyreg \n312\nItem 69: Use decimal When Precision Is Paramount \n319\nItem 70: Profile Before Optimizing \n322\nItem 71: Prefer deque for Producer–Consumer Queues \n326\nItem 72:  Consider Searching Sorted Sequences with bisect \n334\nItem 73: Know How to Use heapq for Priority Queues \n336\nItem 74:  Consider memoryview and bytearray for \nZero-Copy Interactions with bytes \n346\nChapter 9 Testing and Debugging \n353\nItem 75: Use repr Strings for Debugging Output \n354\nItem 76:  Verify Related Behaviors in TestCase Subclasses \n357\nItem 77:  Isolate Tests from Each Other with setUp, \ntearDown, setUpModule, and tearDownModule \n365\nItem 78:  Use Mocks to Test Code with \nComplex Dependencies \n367\n",
      "content_length": 1580,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": " \nContents \nxv\nItem 79:  Encapsulate Dependencies to Facilitate \nMocking and Testing \n375\nItem 80: Consider Interactive Debugging with pdb \n379\nItem 81:  Use tracemalloc to Understand Memory \nUsage and Leaks \n384\nChapter 10 Collaboration \n389\nItem 82: Know Where to Find Community-Built Modules \n389\nItem 83:  Use Virtual Environments for Isolated and \nReproducible Dependencies \n390\nItem 84:  Write Docstrings for Every Function, \nClass, and Module \n396\nItem 85:  Use Packages to Organize Modules and \nProvide Stable APIs \n401\nItem 86:  Consider Module-Scoped Code to \nConfigure Deployment Environments \n406\nItem 87: Define a Root Exception to Insulate \nCallers from APIs \n408\nItem 88:  Know How to Break Circular Dependencies \n413\nItem 89:  Consider warnings to Refactor and Migrate Usage \n418\nItem 90:  Consider Static Analysis via typing to Obviate Bugs 425\nIndex \n435\n",
      "content_length": 873,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "Preface\nThe Python programming language has unique strengths and charms \nthat can be hard to grasp. Many programmers familiar with other \nlanguages often approach Python from a limited mindset instead of \nembracing its full expressivity. Some programmers go too far in the \nother direction, overusing Python features that can cause big prob-\nlems later.\nThis book provides insight into the Pythonic way of writing programs: \nthe best way to use Python. It builds on a fundamental understanding \nof the language that I assume you already have. Novice programmers \nwill learn the best practices of Python’s capabilities. Experienced pro-\ngrammers will learn how to embrace the strangeness of a new tool \nwith confidence.\nMy goal is to prepare you to make a big impact with Python.\nWhat This Book Covers\nEach chapter in this book contains a broad but related set of items. \nFeel free to jump between items and follow your interest. Each item \ncontains concise and specific guidance explaining how you can write \nPython programs more effectively. Items include advice on what to \ndo, what to avoid, how to strike the right balance, and why this is the \nbest choice. Items reference each other to make it easier to fill in the \ngaps as you read.\nThis second edition of this book is focused exclusively on Python 3 \n(see Item 1: “Know Which Version of Python You’re Using”), up to and \nincluding version 3.8. Most of the original items from the first edi-\ntion have been revised and included, but many have undergone sub-\nstantial updates. For some items, my advice has completely changed \nbetween the two editions of the book due to best practices evolving as \nPython has matured. If you’re still primarily using Python 2, despite \n",
      "content_length": 1727,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "xviii \nPreface\nits end-of-life on January 1st, 2020, the previous edition of the book \nmay be more useful to you.\nPython takes a “batteries included” approach to its standard library, \nin comparison to many other languages that ship with a small \n number of common packages and require you to look elsewhere \nfor important functionality. Many of these built-in packages are so \nclosely intertwined with idiomatic Python that they may as well be \npart of the language specification. The full set of standard modules \nis too large to cover in this book, but I’ve included the ones that I feel \nare critical to be aware of and use.\nChapter 1: Pythonic Thinking\nThe Python community has come to use the adjective Pythonic to \ndescribe code that follows a particular style. The idioms of Python \nhave emerged over time through experience using the language and \nworking with others. This chapter covers the best way to do the most \ncommon things in Python.\nChapter 2: Lists and Dictionaries\nIn Python, the most common way to organize information is in a \nsequence of values stored in a list. A list’s natural complement is the \ndict that stores lookup keys mapped to corresponding values. This \nchapter covers how to build programs with these versatile building \nblocks.\nChapter 3: Functions\nFunctions in Python have a variety of extra features that make a \n programmer’s life easier. Some are similar to capabilities in other pro-\ngramming languages, but many are unique to Python. This chapter \ncovers how to use functions to clarify intention, promote reuse, and \nreduce bugs.\nChapter 4: Comprehensions and Generators\nPython has special syntax for quickly iterating through lists, dictio-\nnaries, and sets to generate derivative data structures. It also allows \nfor a stream of iterable values to be incrementally returned by a \nfunction. This chapter covers how these features can provide better \n performance, reduced memory usage, and improved readability.\nChapter 5: Classes and Interfaces\nPython is an object-oriented language. Getting things done in Python \noften requires writing new classes and defining how they interact \n",
      "content_length": 2129,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": " \nPreface \nxix\nthrough their interfaces and hierarchies. This chapter covers how to \nuse classes to express your intended behaviors with objects.\nChapter 6: Metaclasses and Attributes\nMetaclasses and dynamic attributes are powerful Python features. \nHowever, they also enable you to implement extremely bizarre and \nunexpected behaviors. This chapter covers the common idioms for \nusing these mechanisms to ensure that you follow the rule of least \nsurprise.\nChapter 7: Concurrency and Parallelism\nPython makes it easy to write concurrent programs that do many \ndifferent things seemingly at the same time. Python can also be used \nto do parallel work through system calls, subprocesses, and C exten-\nsions. This chapter covers how to best utilize Python in these subtly \ndifferent situations.\nChapter 8: Robustness and Performance\nPython has built-in features and modules that aid in hardening your \nprograms so they are dependable. Python also includes tools to help \nyou achieve higher performance with minimal effort. This chapter \ncovers how to use Python to optimize your programs to maximize \ntheir reliability and efficiency in production.\nChapter 9: Testing and Debugging\nYou should always test your code, regardless of what language it’s \nwritten in. However, Python’s dynamic features can increase the risk \nof runtime errors in unique ways. Luckily, they also make it easier to \nwrite tests and diagnose malfunctioning programs. This chapter cov-\ners Python’s built-in tools for testing and debugging.\nChapter 10: Collaboration\nCollaborating on Python programs requires you to be deliberate about \nhow you write your code. Even if you’re working alone, you’ll want to \nunderstand how to use modules written by others. This chapter cov-\ners the standard tools and best practices that enable people to work \ntogether on Python programs.\nConventions Used in This Book\nPython code snippets in this book are in monospace font and have \nsyntax highlighting. When lines are long, I use ¯ characters to show \n",
      "content_length": 2013,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "xx \nPreface\nwhen they wrap. I truncate some snippets with ellipses (...) to indi-\ncate regions where code exists that isn’t essential for expressing the \npoint. You’ll need to download the full example code (see below on \nwhere to get it) to get these truncated snippets to run correctly on \nyour computer.\nI take some artistic license with the Python style guide in order to \nmake the code examples better fit the format of a book, or to highlight \nthe most important parts. I’ve also left out embedded documentation \nto reduce the size of code examples. I strongly suggest that you don’t \nemulate this in your projects; instead, you should follow the style \nguide (see Item 2: “Follow the PEP 8 Style Guide”) and write documen-\ntation (see Item 84: “Write Docstrings for Every Function, Class, and \nModule”).\nMost code snippets in this book are accompanied by the correspond-\ning output from running the code. When I say “output,” I mean console \nor terminal output: what you see when running the Python program \nin an interactive interpreter. Output sections are in monospace font \nand are preceded by a >>> line (the Python interactive prompt). The \nidea is that you could type the code snippets into a Python shell and \nreproduce the expected output.\nFinally, there are some other sections in monospace font that are not \npreceded by a >>> line. These represent the output of running pro-\ngrams besides the normal Python interpreter. These examples often \nbegin with $ characters to indicate that I’m running programs from a \n command-line shell like Bash. If you’re running these commands on \nWindows or another type of system, you may need to adjust the pro-\ngram names and arguments accordingly.\nWhere to Get the Code and Errata\nIt’s useful to view some of the examples in this book as whole \n programs without interleaved prose. This also gives you a chance to \ntinker with the code yourself and understand why the program works \nas described. You can find the source code for all code snippets in \nthis book on the book’s website at https://effectivepython.com. The \n website also includes any corrections to the book, as well as how to \nreport errors.\n",
      "content_length": 2163,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "Acknowledgments\nThis book would not have been possible without the guidance, \n support, and encouragement from many people in my life.\nThanks to Scott Meyers for the Effective Software Development series. \nI first read Effective C++ when I was 15 years old and fell in love with \nprogramming. There’s no doubt that Scott’s books led to my academic \nexperience and first job. I’m thrilled to have had the opportunity to \nwrite this book.\nThanks to my technical reviewers for the depth and thoroughness \nof their feedback for the second edition of this book: Andy Chu, Nick \nCohron, Andrew Dolan, Asher Mancinelli, and Alex Martelli. Thanks \nto my colleagues at Google for their review and input. Without all of \nyour help, this book would have been inscrutable.\nThanks to everyone at Pearson involved in making this second edi-\ntion a reality. Thanks to my executive editor Debra Williams for being \nsupportive throughout the process. Thanks to the team who were \ninstrumental: development editor Chris Zahn, marketing manager \nStephane Nakib, copy editor Catherine Wilson, senior project editor \nLori Lyons, and cover designer Chuti Prasertsith.\nThanks to everyone who supported me in creating the first edition of \nthis book: Trina MacDonald, Brett Cannon, Tavis Rudd, Mike  Taylor, \nLeah Culver, Adrian Holovaty, Michael Levine, Marzia  Niccolai, Ade \nOshineye, Katrina Sostek, Tom Cirtin, Chris Zahn, Olivia  Basegio, \nStephane Nakib, Stephanie Geels, Julie Nahil, and Toshiaki \n Kurokawa. Thanks to all of the readers who reported errors and room \nfor improvement. Thanks to all of the translators who made the book \navailable in other languages around the world.\nThanks to the wonderful Python programmers I’ve known and \nworked with: Anthony Baxter, Brett Cannon, Wesley Chun, Jeremy \nHylton, Alex Martelli, Neal Norwitz, Guido van Rossum, Andy Smith, \n",
      "content_length": 1859,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "xxii \nAcknowledgments\nGreg Stein, and Ka-Ping Yee. I appreciate your tutelage and leader-\nship. Python has an excellent community, and I feel lucky to be a part \nof it.\nThanks to my teammates over the years for letting me be the worst \nplayer in the band. Thanks to Kevin Gibbs for helping me take risks. \nThanks to Ken Ashcraft, Ryan Barrett, and Jon McAlister for showing \nme how it’s done. Thanks to Brad Fitzpatrick for taking it to the next \nlevel. Thanks to Paul McDonald for being an amazing co-founder. \nThanks to Jeremy Ginsberg, Jack Hebert, John Skidgel, Evan  Martin, \nTony Chang, Troy Trimble, Tessa Pupius, and Dylan Lorimer for help-\ning me learn. Thanks to Sagnik Nandy and Waleed Ojeil for your \nmentorship.\nThanks to the inspiring programming and engineering teachers \nthat I’ve had: Ben Chelf, Glenn Cowan, Vince Hugo, Russ Lewin, Jon \nStemmle, Derek Thomson, and Daniel Wang. Without your instruc-\ntion, I would never have pursued our craft or gained the perspective \nrequired to teach others.\nThanks to my mother for giving me a sense of purpose and \n encouraging me to become a programmer. Thanks to my brother, my \ngrandparents, and the rest of my family and childhood friends for \nbeing role models as I grew up and found my passion.\nFinally, thanks to my wife, Colleen, for her love, support, and  laughter \nthrough the journey of life.\n",
      "content_length": 1362,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "About the Author\nBrett Slatkin is a principal software engineer at Google. He is \nthe technical co-founder of Google Surveys, the co-creator of the \n PubSubHubbub protocol, and he launched Google’s first cloud com-\nputing product (App Engine). Fourteen years ago, he cut his teeth \nusing Python to manage Google’s enormous fleet of servers.\nOutside of his day job, he likes to play piano and surf (both poorly). He \nalso enjoys writing about programming-related topics on his  personal \nwebsite (https://onebigfluke.com). He earned his B.S. in computer \nengineering from Columbia University in the City of New York. He \nlives in San Francisco.\n",
      "content_length": 644,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "1\nPythonic Thinking\nThe idioms of a programming language are defined by its users. \nOver the years, the Python community has come to use the adjective \nPythonic to describe code that follows a particular style. The Pythonic \nstyle isn’t regimented or enforced by the compiler. It has emerged \nover time through experience using the language and working with \n others. Python programmers prefer to be explicit, to choose simple \nover  complex, and to maximize readability. (Type import this into \nyour interpreter to read The Zen of Python.)\nProgrammers familiar with other languages may try to write Python \nas if it’s C++, Java, or whatever they know best. New programmers \nmay still be getting comfortable with the vast range of concepts \nthat can be expressed in Python. It’s important for you to know the \nbest—the Pythonic—way to do the most common things in Python. \nThese patterns will affect every program you write.\nItem 1: Know Which Version of Python You’re Using\nThroughout this book, the majority of example code is in the syntax \nof Python 3.7 (released in June 2018). This book also provides some \nexamples in the syntax of Python 3.8 (released in October 2019) to \nhighlight new features that will be more widely available soon. This \nbook does not cover Python 2.\nMany computers come with multiple versions of the standard CPython \nruntime preinstalled. However, the default meaning of python on the \ncommand line may not be clear. python is usually an alias for python2.7, \nbut it can sometimes be an alias for even older versions, like python2.6 \nor python2.5. To find out exactly which version of Python you’re using, \nyou can use the --version flag:\n$ python --version\nPython 2.7.10\n",
      "content_length": 1704,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "2 \nChapter 1 Pythonic Thinking\nPython 3 is usually available under the name python3:\n$ python3 --version\nPython 3.8.0\nYou can also figure out the version of Python you’re using at runtime \nby inspecting values in the sys built-in module:\nimport sys\nprint(sys.version_info)\nprint(sys.version)\n>>>\nsys.version_info(major=3, minor=8, micro=0, \n¯releaselevel='final', serial=0)\n3.8.0 (default, Oct 21 2019, 12:51:32) \n[Clang 6.0 (clang-600.0.57)]\nPython 3 is actively maintained by the Python core developers and \ncommunity, and it is constantly being improved. Python 3 includes \na variety of powerful new features that are covered in this book. The \nmajority of Python’s most common open source libraries are compat-\nible with and focused on Python 3. I strongly encourage you to use \nPython 3 for all your Python projects.\nPython 2 is scheduled for end of life after January 1, 2020, at which \npoint all forms of bug fixes, security patches, and backports of fea-\ntures will cease. Using Python 2 after that date is a liability because it \nwill no longer be officially maintained. If you’re still stuck working in \na Python 2 codebase, you should consider using helpful tools like 2to3 \n(preinstalled with Python) and six (available as a community pack-\nage; see Item 82: “Know Where to Find Community-Built  Modules”) to \nhelp you make the transition to Python 3.\nThings to Remember\n✦ Python 3 is the most up-to-date and well-supported version of \nPython, and you should use it for your projects.\n✦ Be sure that the command-line executable for running Python on \nyour system is the version you expect it to be.\n✦ Avoid Python 2 because it will no longer be maintained after January 1, \n2020.\nItem 2: Follow the PEP 8 Style Guide\nPython Enhancement Proposal #8, otherwise known as PEP 8, is \nthe style guide for how to format Python code. You are welcome to \n",
      "content_length": 1858,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": " \nItem 2: Follow the PEP 8 Style Guide \n3\nwrite Python code any way you want, as long as it has valid syntax. \n However, using a consistent style makes your code more approach-\nable and easier to read. Sharing a common style with other Python \n programmers in the larger community facilitates collaboration on \nprojects. But even if you are the only one who will ever read your \ncode, following the style guide will make it easier for you to change \nthings later, and can help you avoid many common errors.\nPEP 8 provides a wealth of details about how to write clear Python \ncode. It continues to be updated as the Python language evolves. \nIt’s worth reading the whole guide online (https://www.python.org/\ndev/peps/pep-0008/). Here are a few rules you should be sure to \nfollow.\nWhitespace\nIn Python, whitespace is syntactically significant. Python program-\nmers are especially sensitive to the effects of whitespace on code \n clarity. Follow these guidelines related to whitespace:\n \n■Use spaces instead of tabs for indentation.\n \n■Use four spaces for each level of syntactically significant indenting.\n \n■Lines should be 79 characters in length or less.\n \n■Continuations of long expressions onto additional lines should \nbe indented by four extra spaces from their normal indentation \nlevel.\n \n■In a file, functions and classes should be separated by two blank \nlines.\n \n■In a class, methods should be separated by one blank line.\n \n■In a dictionary, put no whitespace between each key and colon, \nand put a single space before the corresponding value if it fits on \nthe same line.\n \n■Put one—and only one—space before and after the = operator in a \nvariable assignment.\n \n■For type annotations, ensure that there is no separation between \nthe variable name and the colon, and use a space before the type \ninformation.\nNaming\nPEP 8 suggests unique styles of naming for different parts in the \nlanguage. These conventions make it easy to distinguish which type \n",
      "content_length": 1965,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "4 \nChapter 1 Pythonic Thinking\ncorresponds to each name when reading code. Follow these guidelines \nrelated to naming:\n \n■Functions, variables, and attributes should be in lowercase_\nunderscore format.\n \n■Protected instance attributes should be in _leading_underscore \nformat.\n \n■Private instance attributes should be in __double_leading_\nunderscore format.\n \n■Classes (including exceptions) should be in CapitalizedWord \nformat.\n \n■Module-level constants should be in ALL_CAPS format.\n \n■Instance methods in classes should use self, which refers to the \nobject, as the name of the first parameter.\n \n■Class methods should use cls, which refers to the class, as the \nname of the first parameter.\nExpressions and Statements\nThe Zen of Python states: “There should be one—and preferably only \none—obvious way to do it.” PEP 8 attempts to codify this style in its \nguidance for expressions and statements:\n \n■Use inline negation (if a is not b) instead of negation of positive \nexpressions (if not a is b).\n \n■Don’t check for empty containers or sequences (like [] or '') \nby comparing the length to zero (if len(somelist) == 0). Use \nif not somelist and assume that empty values will implicitly \nevaluate to False.\n \n■The same thing goes for non-empty containers or sequences (like \n[1] or 'hi'). The statement if somelist is implicitly True for non-\nempty values.\n \n■Avoid single-line if statements, for and while loops, and except \ncompound statements. Spread these over multiple lines for \nclarity.\n \n■If you can’t fit an expression on one line, surround it with paren-\ntheses and add line breaks and indentation to make it easier to \nread.\n \n■Prefer surrounding multiline expressions with parentheses over \nusing the \\ line continuation character.\n",
      "content_length": 1750,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": " \nItem 3: Know the Differences Between bytes and str \n5\nImports\nPEP 8 suggests some guidelines for how to import modules and use \nthem in your code:\n \n■Always put import statements (including from x import y) at the \ntop of a file.\n \n■Always use absolute names for modules when importing them, not \nnames relative to the current module’s own path. For example, to \nimport the foo module from within the bar package, you should \nuse from bar import foo, not just import foo.\n \n■If you must do relative imports, use the explicit syntax \nfrom . import foo.\n \n■Imports should be in sections in the following order: standard \nlibrary modules, third-party modules, your own modules. Each \nsubsection should have imports in alphabetical order.\nNote\nThe Pylint tool (https://www.pylint.org) is a popular static analyzer for Python \nsource code. Pylint provides automated enforcement of the PEP 8 style guide and \ndetects many other types of common errors in Python programs. Many IDEs and \neditors also include linting tools or support similar plug-ins.\nThings to Remember\n✦ Always follow the Python Enhancement Proposal #8 (PEP 8) style \nguide when writing Python code.\n✦ Sharing a common style with the larger Python community facili-\ntates collaboration with others.\n✦ Using a consistent style makes it easier to modify your own code later.\nItem 3: Know the Differences Between bytes and str\nIn Python, there are two types that represent sequences of character \ndata: bytes and str. Instances of bytes contain raw, unsigned 8-bit \nvalues (often displayed in the ASCII encoding):\na = b'h\\x65llo'\nprint(list(a))\nprint(a)\n>>>\n[104, 101, 108, 108, 111]\nb'hello'\n",
      "content_length": 1653,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "6 \nChapter 1 Pythonic Thinking\nInstances of str contain Unicode code points that represent textual \ncharacters from human languages:\na = 'a\\u0300 propos'\nprint(list(a))\nprint(a)\n>>>\n['a', '`', ' ', 'p', 'r', 'o', 'p', 'o', 's']\nà propos\nImportantly, str instances do not have an associated binary encod-\ning, and bytes instances do not have an associated text encoding. To \nconvert Unicode data to binary data, you must call the encode method \nof str. To convert binary data to Unicode data, you must call the \ndecode method of bytes. You can explicitly specify the encoding you \nwant to use for these methods, or accept the system default, which is \ncommonly UTF-8 (but not always—see more on that below).\nWhen you’re writing Python programs, it’s important to do encoding \nand decoding of Unicode data at the furthest boundary of your inter-\nfaces; this approach is often called the Unicode sandwich. The core \nof your program should use the str type containing Unicode data \nand should not assume anything about character encodings. This \napproach allows you to be very accepting of alternative text encodings \n(such as Latin-1, Shift JIS, and Big5) while being strict about your \n output text encoding (ideally, UTF-8).\nThe split between character types leads to two common situations in \nPython code:\n \n■You want to operate on raw 8-bit sequences that contain \nUTF-8-encoded strings (or some other encoding).\n \n■You want to operate on Unicode strings that have no specific \nencoding.\nYou’ll often need two helper functions to convert between these cases \nand to ensure that the type of input values matches your code’s \nexpectations.\nThe first function takes a bytes or str instance and always returns \na str:\ndef to_str(bytes_or_str):\n    if isinstance(bytes_or_str, bytes):\n        value = bytes_or_str.decode('utf-8')\n    else:\n        value = bytes_or_str\n    return value  # Instance of str\n",
      "content_length": 1901,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": " \nItem 3: Know the Differences Between bytes and str \n7\nprint(repr(to_str(b'foo')))\nprint(repr(to_str('bar')))\n>>>\n'foo'\n'bar'\nThe second function takes a bytes or str instance and always returns \na bytes:\ndef to_bytes(bytes_or_str):\n    if isinstance(bytes_or_str, str):\n        value = bytes_or_str.encode('utf-8')\n    else:\n        value = bytes_or_str\n    return value  # Instance of bytes\nprint(repr(to_bytes(b'foo')))\nprint(repr(to_bytes('bar')))\nThere are two big gotchas when dealing with raw 8-bit values and \nUnicode strings in Python.\nThe first issue is that bytes and str seem to work the same way, but \ntheir instances are not compatible with each other, so you must be \ndeliberate about the types of character sequences that you’re passing \naround.\nBy using the + operator, you can add bytes to bytes and str to str, \nrespectively:\nprint(b'one' + b'two')\nprint('one' + 'two')\n>>>\nb'onetwo'\nonetwo\nBut you can’t add str instances to bytes instances:\nb'one' + 'two'\n>>>\nTraceback ...\nTypeError: can't concat str to bytes\nNor can you add bytes instances to str instances:\n'one' + b'two'\n>>>\nTraceback ...\nTypeError: can only concatenate str (not \"bytes\") to str\n",
      "content_length": 1173,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "8 \nChapter 1 Pythonic Thinking\nBy using binary operators, you can compare bytes to bytes and str to \nstr, respectively:\nassert b'red' > b'blue'\nassert 'red' > 'blue'\nBut you can’t compare a str instance to a bytes instance:\nassert 'red' > b'blue'\n>>>\nTraceback ...\nTypeError: '>' not supported between instances of 'str' and \n¯'bytes'\nNor can you compare a bytes instance to a str instance:\nassert b'blue' < 'red'\n>>>\nTraceback ...\nTypeError: '<' not supported between instances of 'bytes' \n¯and 'str'\nComparing bytes and str instances for equality will always evaluate \nto False, even when they contain exactly the same characters (in this \ncase, ASCII-encoded “foo”):\nprint(b'foo' == 'foo')\n>>>\nFalse\nThe % operator works with format strings for each type, respectively:\nprint(b'red %s' % b'blue')\nprint('red %s' % 'blue')\n>>>\nb'red blue'\nred blue\nBut you can’t pass a str instance to a bytes format string because \nPython doesn’t know what binary text encoding to use:\nprint(b'red %s' % 'blue')\n>>>\nTraceback ...\nTypeError: %b requires a bytes-like object, or an object that \n¯implements __bytes__, not 'str'\n",
      "content_length": 1112,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": " \nItem 3: Know the Differences Between bytes and str \n9\nYou can pass a bytes instance to a str format string using the \n%  operator, but it doesn’t do what you’d expect:\nprint('red %s' % b'blue')\n>>>\nred b'blue'\nThis code actually invokes the __repr__ method (see Item 75: “Use \nrepr Strings for Debugging Output”) on the bytes instance and sub-\nstitutes that in place of the %s, which is why b'blue' remains escaped \nin the output.\nThe second issue is that operations involving file handles (returned by \nthe open built-in function) default to requiring Unicode strings instead \nof raw bytes. This can cause surprising failures, especially for pro-\ngrammers accustomed to Python 2. For example, say that I want to \nwrite some binary data to a file. This seemingly simple code breaks:\nwith open('data.bin', 'w') as f:\n    f.write(b'\\xf1\\xf2\\xf3\\xf4\\xf5')\n>>>\nTraceback ...\nTypeError: write() argument must be str, not bytes\nThe cause of the exception is that the file was opened in write text \nmode ('w') instead of write binary mode ('wb'). When a file is in text \nmode, write operations expect str instances containing Unicode data \ninstead of bytes instances containing binary data. Here, I fix this by \nchanging the open mode to 'wb':\nwith open('data.bin', 'wb') as f:\n    f.write(b'\\xf1\\xf2\\xf3\\xf4\\xf5')\nA similar problem also exists for reading data from files. For example, \nhere I try to read the binary file that was written above:\nwith open('data.bin', 'r') as f:\n    data = f.read()\n>>>\nTraceback ...\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in \n¯position 0: invalid continuation byte\nThis fails because the file was opened in read text mode ('r') \ninstead of read binary mode ('rb'). When a handle is in text mode, \nit uses the system’s default text encoding to interpret binary data \n",
      "content_length": 1814,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "10 \nChapter 1 Pythonic Thinking\nusing the bytes.encode (for writing) and str.decode (for reading) \n methods. On most systems, the default encoding is UTF-8, which \ncan’t accept the binary data b'\\xf1\\xf2\\xf3\\xf4\\xf5', thus causing \nthe error above. Here, I solve this problem by changing the open \nmode to 'rb':\nwith open('data.bin', 'rb') as f:\n    data = f.read()\nassert data == b'\\xf1\\xf2\\xf3\\xf4\\xf5'\nAlternatively, I can explicitly specify the encoding parameter to \nthe open function to make sure that I’m not surprised by any \n platform-specific behavior. For example, here I assume that the \nbinary data in the file was actually meant to be a string encoded as \n'cp1252' (a legacy  Windows encoding):\nwith open('data.bin', 'r', encoding='cp1252') as f:\n    data = f.read()\nassert data == 'ñòóôõ'\nThe exception is gone, and the string interpretation of the file’s con-\ntents is very different from what was returned when reading raw \nbytes. The lesson here is that you should check the default encod-\ning on your system (using python3 -c 'import locale; print(locale.\ngetpreferredencoding())') to understand how it differs from your \nexpectations. When in doubt, you should explicitly pass the encoding \nparameter to open.\nThings to Remember\n✦ bytes contains sequences of 8-bit values, and str contains \nsequences of Unicode code points.\n✦ Use helper functions to ensure that the inputs you operate on \nare the type of character sequence that you expect (8-bit values, \nUTF-8-encoded strings, Unicode code points, etc).\n✦ bytes and str instances can’t be used together with operators (like \n>, ==, +, and %).\n✦ If you want to read or write binary data to/from a file, always open \nthe file using a binary mode (like 'rb' or 'wb').\n✦ If you want to read or write Unicode data to/from a file, be care-\nful about your system’s default text encoding. Explicitly pass the \nencoding parameter to open if you want to avoid surprises.\n",
      "content_length": 1934,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": " \nItem 4: Prefer Interpolated F-Strings \n11\nItem 4:  Prefer Interpolated F-Strings Over C-style \nFormat Strings and str.format\nStrings are present throughout Python codebases. They’re used for \nrendering messages in user interfaces and command-line utilities. \nThey’re used for writing data to files and sockets. They’re used for \nspecifying what’s gone wrong in Exception details (see Item 27: “Use \nComprehensions Instead of map and filter”). They’re used in debug-\nging (see Item 80: “Consider Interactive Debugging with pdb” and Item \n75: “Use repr Strings for Debugging Output”).\nFormatting is the process of combining predefined text with data val-\nues into a single human-readable message that’s stored as a string. \nPython has four different ways of formatting strings that are built \ninto the language and standard library. All but one of them, which is \ncovered last in this item, have serious shortcomings that you should \nunderstand and avoid.\nThe most common way to format a string in Python is by using the \n% formatting operator. The predefined text template is provided on the \nleft side of the operator in a format string. The values to insert into \nthe template are provided as a single value or tuple of multiple values \non the right side of the format operator. For example, here I use the \n% operator to convert difficult-to-read binary and hexadecimal values \nto integer strings:\na = 0b10111011\nb = 0xc5f\nprint('Binary is %d, hex is %d' % (a, b))\n>>>\nBinary is 187, hex is 3167\nThe format string uses format specifiers (like %d) as placeholders that \nwill be replaced by values from the right side of the formatting expres-\nsion. The syntax for format specifiers comes from C’s printf function, \nwhich has been inherited by Python (as well as by other programming \nlanguages). Python supports all the usual options you’d expect from \nprintf, such as %s, %x, and %f format specifiers, as well as control over \ndecimal places, padding, fill, and alignment. Many programmers who \nare new to Python start with C-style format strings because they’re \nfamiliar and simple to use.\nThere are four problems with C-style format strings in Python.\nThe first problem is that if you change the type or order of data val-\nues in the tuple on the right side of a formatting expression, you can \n",
      "content_length": 2302,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "12 \nChapter 1 Pythonic Thinking\nget errors due to type conversion incompatibility. For example, this \n simple formatting expression works:\nkey = 'my_var'\nvalue = 1.234\nformatted = '%-10s = %.2f' % (key, value)\nprint(formatted)\n>>>\nmy_var     = 1.23\nBut if you swap key and value, you get an exception at runtime:\nreordered_tuple = '%-10s = %.2f' % (value, key)\n>>>\nTraceback ...\nTypeError: must be real number, not str\nSimilarly, leaving the right side parameters in the original order but \nchanging the format string results in the same error:\nreordered_string = '%.2f = %-10s' % (key, value)\n>>>\nTraceback ...\nTypeError: must be real number, not str\nTo avoid this gotcha, you need to constantly check that the two sides \nof the % operator are in sync; this process is error prone because it \nmust be done manually for every change.\nThe second problem with C-style formatting expressions is that they \nbecome difficult to read when you need to make small modifications to \nvalues before formatting them into a string—and this is an extremely \ncommon need. Here, I list the contents of my kitchen pantry without \nmaking inline changes:\npantry = [\n    ('avocados', 1.25),\n    ('bananas', 2.5),\n    ('cherries', 15),\n]\nfor i, (item, count) in enumerate(pantry):\n    print('#%d: %-10s = %.2f' % (i, item, count))\n>>>\n#0: avocados   = 1.25\n#1: bananas    = 2.50\n#2: cherries   = 15.00\n",
      "content_length": 1381,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": " \nItem 4: Prefer Interpolated F-Strings \n13\nNow, I make a few modifications to the values that I’m formatting \nto make the printed message more useful. This causes the tuple in \nthe formatting expression to become so long that it needs to be split \nacross multiple lines, which hurts readability:\nfor i, (item, count) in enumerate(pantry):\n    print('#%d: %-10s = %d' % (\n        i + 1,\n        item.title(),\n        round(count)))\n>>>\n#1: Avocados   = 1\n#2: Bananas    = 2\n#3: Cherries   = 15\nThe third problem with formatting expressions is that if you want \nto use the same value in a format string multiple times, you have to \nrepeat it in the right side tuple:\ntemplate = '%s loves food. See %s cook.'\nname = 'Max'\nformatted = template % (name, name)\nprint(formatted)\n>>>\nMax loves food. See Max cook.\nThis is especially annoying and error prone if you have to repeat \nsmall modifications to the values being formatted. For example, here \nI remembered to call the title() method multiple times, but I could \nhave easily added the method call to one reference to name and not the \nother, which would cause mismatched output:\nname = 'brad'\nformatted = template % (name.title(), name.title())\nprint(formatted)\n>>>\nBrad loves food. See Brad cook.\nTo help solve some of these problems, the % operator in Python has \nthe ability to also do formatting with a dictionary instead of a tuple. The \nkeys from the dictionary are matched with format specifiers with the \ncorresponding name, such as %(key)s. Here, I use this functionality to \nchange the order of values on the right side of the formatting expres-\nsion with no effect on the output, thus solving problem #1 from above:\nkey = 'my_var'\nvalue = 1.234\n",
      "content_length": 1706,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "14 \nChapter 1 Pythonic Thinking\nold_way = '%-10s = %.2f' % (key, value)\nnew_way = '%(key)-10s = %(value).2f' % {\n    'key': key, 'value': value}  # Original\nreordered = '%(key)-10s = %(value).2f' % {\n    'value': value, 'key': key}  # Swapped\nassert old_way == new_way == reordered\nUsing dictionaries in formatting expressions also solves problem #3 \nfrom above by allowing multiple format specifiers to reference the same \nvalue, thus making it unnecessary to supply that value more than once:\nname = 'Max'\ntemplate = '%s loves food. See %s cook.'\nbefore = template % (name, name)   # Tuple\ntemplate = '%(name)s loves food. See %(name)s cook.'\nafter = template % {'name': name}  # Dictionary\nassert before == after\nHowever, dictionary format strings introduce and exacerbate other \nissues. For problem #2 above, regarding small modifications to values \nbefore formatting them, formatting expressions become longer and \nmore visually noisy because of the presence of the dictionary key and \ncolon operator on the right side. Here, I render the same string with \nand without dictionaries to show this problem:\nfor i, (item, count) in enumerate(pantry):\n    before = '#%d: %-10s = %d' % (\n        i + 1,\n        item.title(),\n        round(count))\n    after = '#%(loop)d: %(item)-10s = %(count)d' % {\n        'loop': i + 1,\n        'item': item.title(),\n        'count': round(count),\n    }\n    assert before == after\n",
      "content_length": 1416,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": " \nItem 4: Prefer Interpolated F-Strings \n15\nUsing dictionaries in formatting expressions also increases verbosity, \nwhich is problem #4 with C-style formatting expressions in Python. \nEach key must be specified at least twice—once in the format speci-\nfier, once in the dictionary as a key, and potentially once more for the \nvariable name that contains the dictionary value:\nsoup = 'lentil'\nformatted = 'Today\\'s soup is %(soup)s.' % {'soup': soup}\nprint(formatted)\n>>>\nToday's soup is lentil.\nBesides the duplicative characters, this redundancy causes format-\nting expressions that use dictionaries to be long. These expressions \noften must span multiple lines, with the format strings being concat-\nenated across multiple lines and the dictionary assignments having \none line per value to use in formatting:\nmenu = {\n    'soup': 'lentil',\n    'oyster': 'kumamoto',\n    'special': 'schnitzel',\n}\ntemplate = ('Today\\'s soup is %(soup)s, '\n            'buy one get two %(oyster)s oysters, '\n            'and our special entrée is %(special)s.')\nformatted = template % menu\nprint(formatted)\n>>>\nToday's soup is lentil, buy one get two kumamoto oysters, and \n¯our special entrée is schnitzel.\nTo understand what this formatting expression is going to produce, \nyour eyes have to keep going back and forth between the lines of the \nformat string and the lines of the dictionary. This disconnect makes \nit hard to spot bugs, and readability gets even worse if you need to \nmake small modifications to any of the values before formatting.\nThere must be a better way.\nThe format Built-in and str.format\nPython 3 added support for advanced string formatting that is more \nexpressive than the old C-style format strings that use the % operator. \nFor individual Python values, this new functionality can be accessed \nthrough the format built-in function. For example, here I use some of \n",
      "content_length": 1879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "16 \nChapter 1 Pythonic Thinking\nthe new options (, for thousands separators and ^ for centering) to \nformat values:\na = 1234.5678\nformatted = format(a, ',.2f')\nprint(formatted)\nb = 'my string'\nformatted = format(b, '^20s')\nprint('*', formatted, '*')\n>>>\n1,234.57\n*      my string       *\nYou can use this functionality to format multiple values together \nby calling the new format method of the str type. Instead of using \nC-style format specifiers like %d, you can specify placeholders with {}. \nBy default the placeholders in the format string are replaced by the \ncorresponding positional arguments passed to the format method in \nthe order in which they appear:\nkey = 'my_var'\nvalue = 1.234\nformatted = '{} = {}'.format(key, value)\nprint(formatted)\n>>>\nmy_var = 1.234\nWithin each placeholder you can optionally provide a colon char-\nacter followed by format specifiers to customize how values will be \nconverted into strings (see help('FORMATTING') for the full range of \noptions):\nformatted = '{:<10} = {:.2f}'.format(key, value)\nprint(formatted)\n>>>\nmy_var     = 1.23\nThe way to think about how this works is that the format specifiers \nwill be passed to the format built-in function along with the value \n(format(value, '.2f') in the example above). The result of that func-\ntion call is what replaces the placeholder in the overall formatted \nstring. The formatting behavior can be customized per class using \nthe __format__ special method.\n",
      "content_length": 1449,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": " \nItem 4: Prefer Interpolated F-Strings \n17\nWith C-style format strings, you need to escape the % character (by \ndoubling it) so it’s not interpreted as a placeholder accidentally. With \nthe str.format method you need to similarly escape braces:\nprint('%.2f%%' % 12.5)\nprint('{} replaces {{}}'.format(1.23))\n>>>\n12.50%\n1.23 replaces {}\nWithin the braces you may also specify the positional index of an \nargument passed to the format method to use for replacing the place-\nholder. This allows the format string to be updated to reorder the \noutput without requiring you to also change the right side of the for-\nmatting expression, thus addressing problem #1 from above:\nformatted = '{1} = {0}'.format(key, value)\nprint(formatted)\n>>>\n1.234 = my_var\nThe same positional index may also be referenced multiple times in \nthe format string without the need to pass the value to the format \nmethod more than once, which solves problem #3 from above:\nformatted = '{0} loves food. See {0} cook.'.format(name)\nprint(formatted)\n>>>\nMax loves food. See Max cook.\nUnfortunately, the new format method does nothing to address prob-\nlem #2 from above, leaving your code difficult to read when you need \nto make small modifications to values before formatting them. There’s \nlittle difference in readability between the old and new options, which \nare similarly noisy:\nfor i, (item, count) in enumerate(pantry):\n    old_style = '#%d: %-10s = %d' % (\n        i + 1,\n        item.title(),\n        round(count))\n    new_style = '#{}: {:<10s} = {}'.format(\n        i + 1,\n        item.title(),\n        round(count))\n    assert old_style == new_style\n",
      "content_length": 1631,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "18 \nChapter 1 Pythonic Thinking\nThere are even more advanced options for the specifiers used with \nthe str.format method, such as using combinations of dictionary keys \nand list indexes in placeholders, and coercing values to Unicode and \nrepr strings:\nformatted = 'First letter is {menu[oyster][0]!r}'.format(\n    menu=menu)\nprint(formatted)\n>>>\nFirst letter is 'k'\nBut these features don’t help reduce the redundancy of repeated keys \nfrom problem #4 above. For example, here I compare the verbosity of \nusing dictionaries in C-style formatting expressions to the new style \nof passing keyword arguments to the format method:\nold_template = (\n    'Today\\'s soup is %(soup)s, '\n    'buy one get two %(oyster)s oysters, '\n    'and our special entrée is %(special)s.')\nold_formatted = template % {\n    'soup': 'lentil',\n    'oyster': 'kumamoto',\n    'special': 'schnitzel',\n}\nnew_template = (\n    'Today\\'s soup is {soup}, '\n    'buy one get two {oyster} oysters, '\n    'and our special entrée is {special}.')\nnew_formatted = new_template.format(\n    soup='lentil',\n    oyster='kumamoto',\n    special='schnitzel',\n)\nassert old_formatted == new_formatted\nThis style is slightly less noisy because it eliminates some quotes in \nthe dictionary and a few characters in the format specifiers, but it’s \nhardly compelling. Further, the advanced features of using dictionary \nkeys and indexes within placeholders only provides a tiny subset of \nPython’s expression functionality. This lack of expressiveness is so \nlimiting that it undermines the value of the format method from str \noverall.\n",
      "content_length": 1585,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": " \nItem 4: Prefer Interpolated F-Strings \n19\nGiven these shortcomings and the problems from C-style formatting \nexpressions that remain (problems #2 and #4 from above), I suggest \nthat you avoid the str.format method in general. It’s important to \nknow about the new mini language used in format specifiers (every-\nthing after the colon) and how to use the format built-in function. But \nthe rest of the str.format method should be treated as a historical \nartifact to help you understand how Python’s new f-strings work and \nwhy they’re so great.\nInterpolated Format Strings\nPython 3.6 added interpolated format strings—f-strings for short—to \nsolve these issues once and for all. This new language syntax requires \nyou to prefix format strings with an f character, which is similar to \nhow byte strings are prefixed with a b character and raw (unescaped) \nstrings are prefixed with an r character.\nF-strings take the expressiveness of format strings to the extreme, \nsolving problem #4 from above by completely eliminating the redun-\ndancy of providing keys and values to be formatted. They achieve \nthis pithiness by allowing you to reference all names in the current \nPython scope as part of a formatting expression:\nkey = 'my_var'\nvalue = 1.234\nformatted = f'{key} = {value}'\nprint(formatted)\n>>>\nmy_var = 1.234\nAll of the same options from the new format built-in mini language \nare available after the colon in the placeholders within an f-string, as \nis the ability to coerce values to Unicode and repr strings similar to \nthe str.format method:\nformatted = f'{key!r:<10} = {value:.2f}'\nprint(formatted)\n>>>\n'my_var'   = 1.23\nFormatting with f-strings is shorter than using C-style format strings \nwith the % operator and the str.format method in all cases. Here, \nI show all these options together in order of shortest to longest, and \n",
      "content_length": 1844,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "20 \nChapter 1 Pythonic Thinking\nline up the left side of the assignment so you can easily compare \nthem:\nf_string = f'{key:<10} = {value:.2f}'\nc_tuple  = '%-10s = %.2f' % (key, value)\nstr_args = '{:<10} = {:.2f}'.format(key, value)\nstr_kw   = '{key:<10} = {value:.2f}'.format(key=key,\n  \n \n \n \n \n \n    value=value)\nc_dict   = '%(key)-10s = %(value).2f' % {'key': key,\n \n \n \n \n \n \n 'value': value}\nassert c_tuple == c_dict == f_string\nassert str_args == str_kw == f_string\nF-strings also enable you to put a full Python expression within the \nplaceholder braces, solving problem #2 from above by allowing small \nmodifications to the values being formatted with concise syntax. \nWhat took multiple lines with C-style formatting and the str.format \nmethod now easily fits on a single line:\nfor i, (item, count) in enumerate(pantry):\n    old_style = '#%d: %-10s = %d' % (\n        i + 1,\n        item.title(),\n        round(count))\n    new_style = '#{}: {:<10s} = {}'.format(\n        i + 1,\n        item.title(),\n        round(count))\n    f_string = f'#{i+1}: {item.title():<10s} = {round(count)}'\n    assert old_style == new_style == f_string\nOr, if it’s clearer, you can split an f-string over multiple lines by rely-\ning on adjacent-string concatenation (similar to C). Even though this \nis longer than the single-line version, it’s still much clearer than any \nof the other multiline approaches:\nfor i, (item, count) in enumerate(pantry):\n    print(f'#{i+1}: '\n",
      "content_length": 1460,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": " \nItem 5: Write Helper Functions Instead of Complex Expressions \n21\n          f'{item.title():<10s} = '\n          f'{round(count)}')\n>>>\n#1: Avocados   = 1\n#2: Bananas    = 2\n#3: Cherries   = 15\nPython expressions may also appear within the format specifier \noptions. For example, here I parameterize the number of digits to print \nby using a variable instead of hard-coding it in the format string:\nplaces = 3\nnumber = 1.23456\nprint(f'My number is {number:.{places}f}')\n>>>\nMy number is 1.235\nThe combination of expressiveness, terseness, and clarity provided \nby f-strings makes them the best built-in option for Python pro-\ngrammers. Any time you find yourself needing to format values into \nstrings, choose f-strings over the alternatives.\nThings to Remember\n✦ C-style format strings that use the % operator suffer from a variety \nof gotchas and verbosity problems.\n✦ The str.format method introduces some useful concepts in its for-\nmatting specifiers mini language, but it otherwise repeats the mis-\ntakes of C-style format strings and should be avoided.\n✦ F-strings are a new syntax for formatting values into strings that \nsolves the biggest problems with C-style format strings.\n✦ F-strings are succinct yet powerful because they allow for arbi-\ntrary Python expressions to be directly embedded within format \nspecifiers.\nItem 5:  Write Helper Functions Instead of Complex \nExpressions\nPython’s pithy syntax makes it easy to write single-line expressions \nthat implement a lot of logic. For example, say that I want to decode \nthe query string from a URL. Here, each query string parameter rep-\nresents an integer value:\nfrom urllib.parse import parse_qs\n",
      "content_length": 1664,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "22 \nChapter 1 Pythonic Thinking\nmy_values = parse_qs('red=5&blue=0&green=',\n                     keep_blank_values=True)\nprint(repr(my_values))\n>>>\n{'red': ['5'], 'blue': ['0'], 'green': ['']}\nSome query string parameters may have multiple values, some may \nhave single values, some may be present but have blank values, and \nsome may be missing entirely. Using the get method on the result dic-\ntionary will return different values in each circumstance:\nprint('Red:     ', my_values.get('red'))\nprint('Green:   ', my_values.get('green'))\nprint('Opacity: ', my_values.get('opacity'))\n>>>\nRed:      ['5']\nGreen:    ['']\nOpacity:  None\nIt’d be nice if a default value of 0 were assigned when a parameter isn’t \nsupplied or is blank. I might choose to do this with Boolean expres-\nsions because it feels like this logic doesn’t merit a whole if statement \nor helper function quite yet.\nPython’s syntax makes this choice all too easy. The trick here is that \nthe empty string, the empty list, and zero all evaluate to False implic-\nitly. Thus, the expressions below will evaluate to the subexpression \nafter the or operator when the first subexpression is False:\n# For query string 'red=5&blue=0&green='\nred = my_values.get('red', [''])[0] or 0\ngreen = my_values.get('green', [''])[0] or 0\nopacity = my_values.get('opacity', [''])[0] or 0\nprint(f'Red:     {red!r}')\nprint(f'Green:   {green!r}')\nprint(f'Opacity: {opacity!r}')\n>>>\nRed:     '5'\nGreen:   0\nOpacity: 0\nThe red case works because the key is present in the my_values dictio-\nnary. The value is a list with one member: the string '5'. This string \nimplicitly evaluates to True, so red is assigned to the first part of the \nor expression.\n",
      "content_length": 1694,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": " \nItem 5: Write Helper Functions Instead of Complex Expressions \n23\nThe green case works because the value in the my_values dictionary is \na list with one member: an empty string. The empty string implicitly \nevaluates to False, causing the or expression to evaluate to 0.\nThe opacity case works because the value in the my_values dictionary \nis missing altogether. The behavior of the get method is to return its \nsecond argument if the key doesn’t exist in the dictionary (see Item 16: \n“Prefer get Over in and KeyError to Handle Missing Dictionary Keys”). \nThe default value in this case is a list with one member: an empty \nstring. When opacity isn’t found in the dictionary, this code does \nexactly the same thing as the green case.\nHowever, this expression is difficult to read, and it still doesn’t do \neverything I need. I’d also want to ensure that all the parameter val-\nues are converted to integers so I can immediately use them in math-\nematical expressions. To do that, I’d wrap each expression with the \nint built-in function to parse the string as an integer:\nred = int(my_values.get('red', [''])[0] or 0)\nThis is now extremely hard to read. There’s so much visual noise. The \ncode isn’t approachable. A new reader of the code would have to spend \ntoo much time picking apart the expression to figure out what it actu-\nally does. Even though it’s nice to keep things short, it’s not worth \ntrying to fit this all on one line.\nPython has if/else conditional—or ternary—expressions to make \ncases like this clearer while keeping the code short:\nred_str = my_values.get('red', [''])\nred = int(red_str[0]) if red_str[0] else 0\nThis is better. For less complicated situations, if/else conditional \nexpressions can make things very clear. But the example above is \nstill not as clear as the alternative of a full if/else statement over \nmultiple lines. Seeing all of the logic spread out like this makes the \ndense version seem even more complex:\ngreen_str = my_values.get('green', [''])\nif green_str[0]:\n    green = int(green_str[0])\nelse:\n    green = 0\nIf you need to reuse this logic repeatedly—even just two or three times, \nas in this example—then writing a helper function is the way to go:\ndef get_first_int(values, key, default=0):\n    found = values.get(key, [''])\n",
      "content_length": 2284,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "24 \nChapter 1 Pythonic Thinking\n    if found[0]:\n        return int(found[0])\n    return default\nThe calling code is much clearer than the complex expression using \nor and the two-line version using the if/else expression:\ngreen = get_first_int(my_values, 'green')\nAs soon as expressions get complicated, it’s time to consider split-\nting them into smaller pieces and moving logic into helper functions. \nWhat you gain in readability always outweighs what brevity may have \nafforded you. Avoid letting Python’s pithy syntax for complex expres-\nsions from getting you into a mess like this. Follow the DRY principle: \nDon’t repeat yourself.\nThings to Remember\n✦ Python’s syntax makes it easy to write single-line expressions that \nare overly complicated and difficult to read.\n✦ Move complex expressions into helper functions, especially if you \nneed to use the same logic repeatedly.\n✦ An if/else expression provides a more readable alternative to using \nthe Boolean operators or and and in expressions.\nItem 6:  Prefer Multiple Assignment Unpacking Over \nIndexing\nPython has a built-in tuple type that can be used to create immutable, \nordered sequences of values. In the simplest case, a tuple is a pair of \ntwo values, such as keys and values from a dictionary:\nsnack_calories = {\n    'chips': 140,\n    'popcorn': 80,\n    'nuts': 190,\n}\nitems = tuple(snack_calories.items())\nprint(items)\n>>>\n(('chips', 140), ('popcorn', 80), ('nuts', 190))\nThe values in tuples can be accessed through numerical indexes:\nitem = ('Peanut butter', 'Jelly')\nfirst = item[0]\nsecond = item[1]\nprint(first, 'and', second)\n",
      "content_length": 1603,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": " \nItem 6: Prefer Multiple Assignment Unpacking Over Indexing \n25\n>>>\nPeanut butter and Jelly\nOnce a tuple is created, you can’t modify it by assigning a new value \nto an index:\npair = ('Chocolate', 'Peanut butter')\npair[0] = 'Honey'\n>>>\nTraceback ...\nTypeError: 'tuple' object does not support item assignment\nPython also has syntax for unpacking, which allows for assigning \nmultiple values in a single statement. The patterns that you specify in \nunpacking assignments look a lot like trying to mutate tuples—which \nisn’t allowed—but they actually work quite differently. For example, if \nyou know that a tuple is a pair, instead of using indexes to access its \nvalues, you can assign it to a tuple of two variable names:\nitem = ('Peanut butter', 'Jelly')\nfirst, second = item  # Unpacking\nprint(first, 'and', second)\n>>>\nPeanut butter and Jelly\nUnpacking has less visual noise than accessing the tuple’s indexes, \nand it often requires fewer lines. The same pattern matching syntax \nof unpacking works when assigning to lists, sequences, and multiple \nlevels of arbitrary iterables within iterables. I don’t recommend doing \nthe following in your code, but it’s important to know that it’s possible \nand how it works:\nfavorite_snacks = {\n    'salty': ('pretzels', 100),\n    'sweet': ('cookies', 180),\n    'veggie': ('carrots', 20),\n}\n((type1, (name1, cals1)),\n (type2, (name2, cals2)),\n (type3, (name3, cals3))) = favorite_snacks.items()\n",
      "content_length": 1441,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "26 \nChapter 1 Pythonic Thinking\nprint(f'Favorite {type1} is {name1} with {cals1} calories')\nprint(f'Favorite {type2} is {name2} with {cals2} calories')\nprint(f'Favorite {type3} is {name3} with {cals3} calories')\n>>>\nFavorite salty is pretzels with 100 calories\nFavorite sweet is cookies with 180 calories\nFavorite veggie is carrots with 20 calories\nNewcomers to Python may be surprised to learn that unpacking can \neven be used to swap values in place without the need to create tem-\nporary variables. Here, I use typical syntax with indexes to swap the \nvalues between two positions in a list as part of an ascending order \nsorting algorithm:\ndef bubble_sort(a):\n    for _ in range(len(a)):\n        for i in range(1, len(a)):\n            if a[i] < a[i-1]:\n                temp = a[i]\n                a[i] = a[i-1]\n                a[i-1] = temp\nnames = ['pretzels', 'carrots', 'arugula', 'bacon']\nbubble_sort(names)\nprint(names)\n>>>\n['arugula', 'bacon', 'carrots', 'pretzels']\nHowever, with unpacking syntax, it’s possible to swap indexes in a \nsingle line:\ndef bubble_sort(a):\n    for _ in range(len(a)):\n        for i in range(1, len(a)):\n            if a[i] < a[i-1]:\n                a[i-1], a[i] = a[i], a[i-1]  # Swap\nnames = ['pretzels', 'carrots', 'arugula', 'bacon']\nbubble_sort(names)\nprint(names)\n>>>\n['arugula', 'bacon', 'carrots', 'pretzels']\nThe way this swap works is that the right side of the assignment \n(a[i], a[i-1]) is evaluated first, and its values are put into a new tem-\nporary, unnamed tuple (such as ('carrots', 'pretzels') on the first \n",
      "content_length": 1564,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": " \nItem 6: Prefer Multiple Assignment Unpacking Over Indexing \n27\niteration of the loops). Then, the unpacking pattern from the left side \nof the assignment (a[i-1], a[i]) is used to receive that tuple value \nand assign it to the variable names a[i-1] and a[i], respectively. \nThis replaces 'pretzels' with 'carrots' at index 0 and 'carrots' with \n'pretzels' at index 1. Finally, the temporary unnamed tuple silently \ngoes away.\nAnother valuable application of unpacking is in the target list of for \nloops and similar constructs, such as comprehensions and generator \nexpressions (see Item 27: “Use Comprehensions Instead of map and \nfilter” for those). As an example for contrast, here I iterate over a \nlist of snacks without using unpacking:\nsnacks = [('bacon', 350), ('donut', 240), ('muffin', 190)]\nfor i in range(len(snacks)):\n    item = snacks[i]\n    name = item[0]\n    calories = item[1]\n    print(f'#{i+1}: {name} has {calories} calories')\n>>>\n#1: bacon has 350 calories\n#2: donut has 240 calories\n#3: muffin has 190 calories\nThis works, but it’s noisy. There are a lot of extra characters required \nin order to index into the various levels of the snacks structure. \nHere, I achieve the same output by using unpacking along with the \nenumerate built-in function (see Item 7: “Prefer enumerate Over range”):\nfor rank, (name, calories) in enumerate(snacks, 1):\n    print(f'#{rank}: {name} has {calories} calories')\n>>>\n#1: bacon has 350 calories\n#2: donut has 240 calories\n#3: muffin has 190 calories\nThis is the Pythonic way to write this type of loop; it’s short and easy to \nunderstand. There’s usually no need to access anything using indexes.\nPython provides additional unpacking functionality for list con-\nstruction (see Item 13: “Prefer Catch-All Unpacking Over Slicing”), \nfunction arguments (see Item 22: “Reduce Visual Noise with Variable \nPositional Arguments”), keyword arguments (see Item 23: “Provide \nOptional Behavior with Keyword Arguments”), multiple return val-\nues (see Item 19: “Never Unpack More Than Three Variables When \n Functions Return Multiple Values”), and more.\nUsing unpacking wisely will enable you to avoid indexing when possi-\nble, resulting in clearer and more Pythonic code.\n",
      "content_length": 2220,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "28 \nChapter 1 Pythonic Thinking\nThings to Remember\n✦ Python has special syntax called unpacking for assigning multiple \nvalues in a single statement.\n✦ Unpacking is generalized in Python and can be applied to any \n iterable, including many levels of iterables within iterables.\n✦ Reduce visual noise and increase code clarity by using unpacking \nto avoid explicitly indexing into sequences.\nItem 7: Prefer enumerate Over range\nThe range built-in function is useful for loops that iterate over a set of \nintegers:\nfrom random import randint\nrandom_bits = 0\nfor i in range(32):\n    if randint(0, 1):\n        random_bits |= 1 << i\nprint(bin(random_bits))\n>>>\n0b11101000100100000111000010000001\nWhen you have a data structure to iterate over, like a list of strings, \nyou can loop directly over the sequence:\nflavor_list = ['vanilla', 'chocolate', 'pecan', 'strawberry']\nfor flavor in flavor_list:\n    print(f'{flavor} is delicious')\n>>>\nvanilla is delicious\nchocolate is delicious\npecan is delicious\nstrawberry is delicious\nOften, you’ll want to iterate over a list and also know the index of \nthe current item in the list. For example, say that I want to print the \nranking of my favorite ice cream flavors. One way to do it is by using \nrange:\nfor i in range(len(flavor_list)):\n    flavor = flavor_list[i]\n    print(f'{i + 1}: {flavor}')\n",
      "content_length": 1337,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": " \nItem 7: Prefer enumerate Over range \n29\n>>>\n1: vanilla\n2: chocolate\n3: pecan\n4: strawberry\nThis looks clumsy compared with the other examples of iterating over \nflavor_list or range. I have to get the length of the list. I have to \nindex into the array. The multiple steps make it harder to read.\nPython provides the enumerate built-in function to address this situa-\ntion. enumerate wraps any iterator with a lazy generator (see Item 30: \n“Consider Generators Instead of Returning Lists”). enumerate yields \npairs of the loop index and the next value from the given iterator. \nHere, I manually advance the returned iterator with the next built-in \nfunction to demonstrate what it does:\nit = enumerate(flavor_list)\nprint(next(it))\nprint(next(it))\n>>>\n(0, 'vanilla')\n(1, 'chocolate')\nEach pair yielded by enumerate can be succinctly unpacked in a for \nstatement (see Item 6: “Prefer Multiple Assignment Unpacking Over \nIndexing” for how that works). The resulting code is much clearer:\nfor i, flavor in enumerate(flavor_list):\n    print(f'{i + 1}: {flavor}')\n>>>\n1: vanilla\n2: chocolate\n3: pecan\n4: strawberry\nI can make this even shorter by specifying the number from which \nenumerate should begin counting (1 in this case) as the second \nparameter:\nfor i, flavor in enumerate(flavor_list, 1):\n    print(f'{i}: {flavor}')\nThings to Remember\n✦ enumerate provides concise syntax for looping over an iterator and \ngetting the index of each item from the iterator as you go.\n",
      "content_length": 1473,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "30 \nChapter 1 Pythonic Thinking\n✦ Prefer enumerate instead of looping over a range and indexing into a \nsequence.\n✦ You can supply a second parameter to enumerate to specify the \nnumber from which to begin counting (zero is the default).\nItem 8: Use zip to Process Iterators in Parallel\nOften in Python you find yourself with many lists of related objects. \nList comprehensions make it easy to take a source list and get a \nderived list by applying an expression (see Item 27: “Use Comprehen-\nsions Instead of map and filter”):\nnames = ['Cecilia', 'Lise', 'Marie']\ncounts = [len(n) for n in names]\nprint(counts)\n>>>\n[7, 4, 5]\nThe items in the derived list are related to the items in the source \nlist by their indexes. To iterate over both lists in parallel, I can iterate \nover the length of the names source list:\nlongest_name = None\nmax_count = 0\nfor i in range(len(names)):\n    count = counts[i]\n    if count > max_count:\n        longest_name = names[i]\n        max_count = count\nprint(longest_name)\n>>>\nCecilia\nThe problem is that this whole loop statement is visually noisy. The \nindexes into names and counts make the code hard to read. Indexing \ninto the arrays by the loop index i happens twice. Using enumerate \n(see Item 7: “Prefer enumerate Over range”) improves this slightly, but \nit’s still not ideal:\nfor i, name in enumerate(names):\n    count = counts[i]\n    if count > max_count:\n        longest_name = name\n        max_count = count\n",
      "content_length": 1452,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": " \nItem 8: Use zip to Process Iterators in Parallel \n31\nTo make this code clearer, Python provides the zip built-in function. \nzip wraps two or more iterators with a lazy generator. The zip gener-\nator yields tuples containing the next value from each iterator. These \ntuples can be unpacked directly within a for statement (see Item 6: \n“Prefer Multiple Assignment Unpacking Over Indexing”). The resulting \ncode is much cleaner than the code for indexing into multiple lists:\nfor name, count in zip(names, counts):\n    if count > max_count:\n        longest_name = name\n        max_count = count\nzip consumes the iterators it wraps one item at a time, which means \nit can be used with infinitely long inputs without risk of a program \nusing too much memory and crashing.\nHowever, beware of zip’s behavior when the input iterators are of \ndifferent lengths. For example, say that I add another item to names \nabove but forget to update counts. Running zip on the two input lists \nwill have an unexpected result:\nnames.append('Rosalind')\nfor name, count in zip(names, counts):\n    print(name)\n>>>\nCecilia\nLise\nMarie\nThe new item for 'Rosalind' isn’t there. Why not? This is just how \nzip works. It keeps yielding tuples until any one of the wrapped iter-\nators is exhausted. Its output is as long as its shortest input. This \napproach works fine when you know that the iterators are of the \nsame length, which is often the case for derived lists created by list \ncomprehensions.\nBut in many other cases, the truncating behavior of zip is surprising \nand bad. If you don’t expect the lengths of the lists passed to zip to \nbe equal, consider using the zip_longest function from the itertools \nbuilt-in module instead:\nimport itertools\nfor name, count in itertools.zip_longest(names, counts):\n    print(f'{name}: {count}')\n",
      "content_length": 1818,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "32 \nChapter 1 Pythonic Thinking\n>>>\nCecilia: 7\nLise: 4\nMarie: 5\nRosalind: None\nzip_longest replaces missing values—the length of the string \n'Rosalind' in this case—with whatever fillvalue is passed to it, which \ndefaults to None.\nThings to Remember\n✦ The zip built-in function can be used to iterate over multiple itera-\ntors in parallel.\n✦ zip creates a lazy generator that produces tuples, so it can be used \non infinitely long inputs.\n✦ zip truncates its output silently to the shortest iterator if you supply \nit with iterators of different lengths.\n✦ Use the zip_longest function from the itertools built-in mod-\nule if you want to use zip on iterators of unequal lengths without \ntruncation.\nItem 9: Avoid else Blocks After for and while Loops\nPython loops have an extra feature that is not available in most other \nprogramming languages: You can put an else block immediately after \na loop’s repeated interior block:\nfor i in range(3):\n    print('Loop', i)\nelse:\n    print('Else block!')\n>>>\nLoop 0\nLoop 1\nLoop 2\nElse block!\nSurprisingly, the else block runs immediately after the loop finishes. \nWhy is the clause called “else”? Why not “and”? In an if/else state-\nment, else means “Do this if the block before this doesn’t happen.” In \na try/except statement, except has the same definition: “Do this if \ntrying the block before this failed.”\n",
      "content_length": 1353,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": " \nItem 9: Avoid else Blocks After for and while Loops \n33\nSimilarly, else from try/except/else follows this pattern (see Item 65: \n“Take Advantage of Each Block in try/except/else/finally”) because it \nmeans “Do this if there was no exception to handle.” try/finally is also \nintuitive because it means “Always do this after trying the block before.”\nGiven all the uses of else, except, and finally in Python, a new pro-\ngrammer might assume that the else part of for/else means “Do this \nif the loop wasn’t completed.” In reality, it does exactly the opposite. \nUsing a break statement in a loop actually skips the else block:\nfor i in range(3):\n    print('Loop', i)\n    if i == 1:\n        break\nelse:\n    print('Else block!')\n>>>\nLoop 0\nLoop 1\nAnother surprise is that the else block runs immediately if you loop \nover an empty sequence:\nfor x in []:\n    print('Never runs')\nelse:\n    print('For Else block!')\n>>>\nFor Else block!\nThe else block also runs when while loops are initially False:\nwhile False:\n    print('Never runs')\nelse:\n    print('While Else block!')\n>>>\nWhile Else block!\nThe rationale for these behaviors is that else blocks after loops are \nuseful when using loops to search for something. For example, say \nthat I want to determine whether two numbers are coprime (that is, \ntheir only common divisor is 1). Here, I iterate through every pos-\nsible common divisor and test the numbers. After every option has \n",
      "content_length": 1432,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "34 \nChapter 1 Pythonic Thinking\nbeen tried, the loop ends. The else block runs when the numbers are \ncoprime because the loop doesn’t encounter a break:\na = 4\nb = 9\nfor i in range(2, min(a, b) + 1):\n    print('Testing', i)\n    if a % i == 0 and b % i == 0:\n        print('Not coprime')\n        break\nelse:\n    print('Coprime')\n>>>\nTesting 2\nTesting 3\nTesting 4\nCoprime\nIn practice, I wouldn’t write the code this way. Instead, I’d write a \nhelper function to do the calculation. Such a helper function is writ-\nten in two common styles.\nThe first approach is to return early when I find the condition I’m look-\ning for. I return the default outcome if I fall through the loop:\ndef coprime(a, b):\n    for i in range(2, min(a, b) + 1):\n        if a % i == 0 and b % i == 0:\n            return False\n    return True\nassert coprime(4, 9)\nassert not coprime(3, 6)\nThe second way is to have a result variable that indicates whether I’ve \nfound what I’m looking for in the loop. I break out of the loop as soon \nas I find something:\ndef coprime_alternate(a, b):\n    is_coprime = True\n    for i in range(2, min(a, b) + 1):\n        if a % i == 0 and b % i == 0:\n            is_coprime = False\n            break\n    return is_coprime\n",
      "content_length": 1224,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": " \nItem 10: Prevent Repetition with Assignment Expressions \n35\nassert coprime_alternate(4, 9)\nassert not coprime_alternate(3, 6)\nBoth approaches are much clearer to readers of unfamiliar code. \nDepending on the situation, either may be a good choice. However, the \nexpressivity you gain from the else block doesn’t outweigh the burden \nyou put on people (including yourself) who want to understand your \ncode in the future. Simple constructs like loops should be self-evident \nin Python. You should avoid using else blocks after loops entirely.\nThings to Remember\n✦ Python has special syntax that allows else blocks to immediately \nfollow for and while loop interior blocks.\n✦ The else block after a loop runs only if the loop body did not encoun-\nter a break statement.\n✦ Avoid using else blocks after loops because their behavior isn’t \n intuitive and can be confusing.\nItem 10:  Prevent Repetition with Assignment \nExpressions\nAn assignment expression—also known as the walrus operator—is a \nnew syntax introduced in Python 3.8 to solve a long-standing problem \nwith the language that can cause code duplication. Whereas normal \nassignment statements are written a = b and pronounced “a equals b,” \nthese assignments are written a := b and pronounced “a walrus b” \n(because := looks like a pair of eyeballs and tusks).\nAssignment expressions are useful because they enable you to assign \nvariables in places where assignment statements are disallowed, such \nas in the conditional expression of an if statement. An assignment \nexpression’s value evaluates to whatever was assigned to the identi-\nfier on the left side of the walrus operator.\nFor example, say that I have a basket of fresh fruit that I’m trying to \nmanage for a juice bar. Here, I define the contents of the basket:\nfresh_fruit = {\n    'apple': 10,\n    'banana': 8,\n    'lemon': 5,\n}\n",
      "content_length": 1851,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "36 \nChapter 1 Pythonic Thinking\nWhen a customer comes to the counter to order some lemonade, \nI need to make sure there is at least one lemon in the basket to \nsqueeze. Here, I do this by retrieving the count of lemons and then \nusing an if statement to check for a non-zero value:\ndef make_lemonade(count):\n    ...\ndef out_of_stock():\n    ...\ncount = fresh_fruit.get('lemon', 0)\nif count:\n    make_lemonade(count)\nelse:\n    out_of_stock()\nThe problem with this seemingly simple code is that it’s noisier than \nit needs to be. The count variable is used only within the first block \nof the if statement. Defining count above the if statement causes it \nto appear to be more important than it really is, as if all code that fol-\nlows, including the else block, will need to access the count variable, \nwhen in fact that is not the case.\nThis pattern of fetching a value, checking to see if it’s non-zero, and \nthen using it is extremely common in Python. Many programmers \ntry to work around the multiple references to count with a variety \nof tricks that hurt readability (see Item 5: “Write Helper Functions \nInstead of Complex Expressions” for an example). Luckily, assign-\nment expressions were added to the language to streamline exactly \nthis type of code. Here, I rewrite this example using the walrus \noperator:\nif count := fresh_fruit.get('lemon', 0):\n    make_lemonade(count)\nelse:\n    out_of_stock()\nThough this is only one line shorter, it’s a lot more readable because \nit’s now clear that count is only relevant to the first block of the if \nstatement. The assignment expression is first assigning a value to the \ncount variable, and then evaluating that value in the context of the if \nstatement to determine how to proceed with flow control. This two-\nstep behavior—assign and then evaluate—is the fundamental nature \nof the walrus operator.\nLemons are quite potent, so only one is needed for my lemonade rec-\nipe, which means a non-zero check is good enough. If a customer \n",
      "content_length": 1990,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": " \nItem 10: Prevent Repetition with Assignment Expressions \n37\norders a cider, though, I need to make sure that I have at least four \napples. Here, I do this by fetching the count from the fruit_basket \ndictionary, and then using a comparison in the if statement condi-\ntional expression:\ndef make_cider(count):\n    ...\ncount = fresh_fruit.get('apple', 0)\nif count >= 4:\n    make_cider(count)\nelse:\n    out_of_stock()\nThis has the same problem as the lemonade example, where the \nassignment of count puts distracting emphasis on that variable. \nHere, I improve the clarity of this code by also using the walrus \noperator:\nif (count := fresh_fruit.get('apple', 0)) >= 4:\n    make_cider(count)\nelse:\n    out_of_stock()\nThis works as expected and makes the code one line shorter. It’s \nimportant to note how I needed to surround the assignment expres-\nsion with parentheses to compare it with 4 in the if statement. In \nthe lemonade example, no surrounding parentheses were required \nbecause the assignment expression stood on its own as a non-zero \ncheck; it wasn’t a subexpression of a larger expression. As with other \nexpressions, you should avoid surrounding assignment expressions \nwith parentheses when possible.\nAnother common variation of this repetitive pattern occurs when I \nneed to assign a variable in the enclosing scope depending on some \ncondition, and then reference that variable shortly afterward in a \nfunction call. For example, say that a customer orders some banana \nsmoothies. In order to make them, I need to have at least two bananas’ \nworth of slices, or else an OutOfBananas exception will be raised. Here, \nI implement this logic in a typical way:\ndef slice_bananas(count):\n    ...\nclass OutOfBananas(Exception):\n    pass\n",
      "content_length": 1748,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "38 \nChapter 1 Pythonic Thinking\ndef make_smoothies(count):\n    ...\npieces = 0\ncount = fresh_fruit.get('banana', 0)\nif count >= 2:\n    pieces = slice_bananas(count)\ntry:\n    smoothies = make_smoothies(pieces)\nexcept OutOfBananas:\n    out_of_stock()\nThe other common way to do this is to put the pieces = 0 assignment \nin the else block:\ncount = fresh_fruit.get('banana', 0)\nif count >= 2:\n    pieces = slice_bananas(count)\nelse:\n    pieces = 0\ntry:\n    smoothies = make_smoothies(pieces)\nexcept OutOfBananas:\n    out_of_stock()\nThis second approach can feel odd because it means that the \npieces variable has two different locations—in each block of the if \nstatement—where it can be initially defined. This split definition tech-\nnically works because of Python’s scoping rules (see Item 21: “Know \nHow Closures Interact with Variable Scope”), but it isn’t easy to read \nor discover, which is why many people prefer the construct above, \nwhere the pieces = 0 assignment is first.\nThe walrus operator can again be used to shorten this example by \none line of code. This small change removes any emphasis on the \ncount variable. Now, it’s clearer that pieces will be important beyond \nthe if statement:\npieces = 0\nif (count := fresh_fruit.get('banana', 0)) >= 2:\n    pieces = slice_bananas(count)\ntry:\n    smoothies = make_smoothies(pieces)\nexcept OutOfBananas:\n    out_of_stock()\n",
      "content_length": 1379,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": " \nItem 10: Prevent Repetition with Assignment Expressions \n39\nUsing the walrus operator also improves the readability of splitting \nthe definition of pieces across both parts of the if statement. It’s eas-\nier to trace the pieces variable when the count definition no longer \nprecedes the if statement:\nif (count := fresh_fruit.get('banana', 0)) >= 2:\n    pieces = slice_bananas(count)\nelse:\n    pieces = 0\ntry:\n    smoothies = make_smoothies(pieces)\nexcept OutOfBananas:\n    out_of_stock()\nOne frustration that programmers who are new to Python often have \nis the lack of a flexible switch/case statement. The general style for \napproximating this type of functionality is to have a deep nesting of \nmultiple if, elif, and else statements.\nFor example, imagine that I want to implement a system of precedence \nso that each customer automatically gets the best juice available and \ndoesn’t have to order. Here, I define logic to make it so banana smooth-\nies are served first, followed by apple cider, and then finally lemonade:\ncount = fresh_fruit.get('banana', 0)\nif count >= 2:\n    pieces = slice_bananas(count)\n    to_enjoy = make_smoothies(pieces)\nelse:\n    count = fresh_fruit.get('apple', 0)\n    if count >= 4:\n        to_enjoy = make_cider(count)\n    else:\n        count = fresh_fruit.get('lemon', 0)\n        if count:\n            to_enjoy = make_lemonade(count)\n        else:\n            to_enjoy‘= 'Nothing'\nUgly constructs like this are surprisingly common in Python code. \nLuckily, the walrus operator provides an elegant solution that can feel \nnearly as versatile as dedicated syntax for switch/case statements:\nif (count := fresh_fruit.get('banana', 0)) >= 2:\n    pieces = slice_bananas(count)\n    to_enjoy = make_smoothies(pieces)\n",
      "content_length": 1747,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "40 \nChapter 1 Pythonic Thinking\nelif (count := fresh_fruit.get('apple', 0)) >= 4:\n    to_enjoy = make_cider(count)\nelif count := fresh_fruit.get('lemon', 0):\n    to_enjoy = make_lemonade(count)\nelse:\n    to_enjoy = 'Nothing'\nThe version that uses assignment expressions is only five lines shorter \nthan the original, but the improvement in readability is vast due to \nthe reduction in nesting and indentation. If you ever see such ugly \nconstructs emerge in your code, I suggest that you move them over to \nusing the walrus operator if possible.\nAnother common frustration of new Python programmers is the lack \nof a do/while loop construct. For example, say that I want to bottle \njuice as new fruit is delivered until there’s no fruit remaining. Here, \nI implement this logic with a while loop:\ndef pick_fruit():\n    ...\ndef make_juice(fruit, count):\n    ...\nbottles = []\nfresh_fruit = pick_fruit()\nwhile fresh_fruit:\n    for fruit, count in fresh_fruit.items():\n        batch = make_juice(fruit, count)\n        bottles.extend(batch)\n    fresh_fruit = pick_fruit()\nThis is repetitive because it requires two separate fresh_fruit = \npick_fruit() calls: one before the loop to set initial conditions, and \nanother at the end of the loop to replenish the list of delivered fruit.\nA strategy for improving code reuse in this situation is to use the \nloop-and-a-half idiom. This eliminates the redundant lines, but it \nalso undermines the while loop’s contribution by making it a dumb \ninfinite loop. Now, all of the flow control of the loop depends on the \nconditional break statement:\nbottles = []\nwhile True:                     # Loop\n    fresh_fruit = pick_fruit()\n    if not fresh_fruit:         # And a half\n        break\n",
      "content_length": 1726,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": " \nItem 10: Prevent Repetition with Assignment Expressions \n41\n    for fruit, count in fresh_fruit.items():\n        batch = make_juice(fruit, count)\n        bottles.extend(batch)\nThe walrus operator obviates the need for the loop-and-a-half idiom \nby allowing the fresh_fruit variable to be reassigned and then con-\nditionally evaluated each time through the while loop. This solution \nis short and easy to read, and it should be the preferred approach in \nyour code:\nbottles = []\nwhile fresh_fruit := pick_fruit():\n    for fruit, count in fresh_fruit.items():\n        batch = make_juice(fruit, count)\n        bottles.extend(batch)\nThere are many other situations where assignment expressions can \nbe used to eliminate redundancy (see Item 29: “Avoid Repeated Work \nin Comprehensions by Using Assignment Expressions” for another). \nIn general, when you find yourself repeating the same expression or \nassignment multiple times within a grouping of lines, it’s time to con-\nsider using assignment expressions in order to improve readability.\nThings to Remember\n✦ Assignment expressions use the walrus operator (:=) to both assign \nand evaluate variable names in a single expression, thus reducing \nrepetition.\n✦ When an assignment expression is a subexpression of a larger \nexpression, it must be surrounded with parentheses.\n✦ Although switch/case statements and do/while loops are not avail-\nable in Python, their functionality can be emulated much more \nclearly by using assignment expressions.\n",
      "content_length": 1496,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "2\nLists and \nDictionaries\nMany programs are written to automate repetitive tasks that are \n better suited to machines than to humans. In Python, the most \n common way to organize this kind of work is by using a sequence of \nvalues stored in a list type. Lists are extremely versatile and can be \nused to solve a variety of problems.\nA natural complement to lists is the dict type, which stores lookup \nkeys mapped to corresponding values (in what is often called an \nassociative array or a hash table). Dictionaries provide constant time \n(amortized) performance for assignments and accesses, which means \nthey are ideal for bookkeeping dynamic information.\nPython has special syntax and built-in modules that enhance read-\nability and extend the capabilities of lists and dictionaries beyond \nwhat you might expect from simple array, vector, and hash table types \nin other languages.\nItem 11: Know How to Slice Sequences\nPython includes syntax for slicing sequences into pieces. Slicing \nallows you to access a subset of a sequence’s items with minimal \neffort. The simplest uses for slicing are the built-in types list, str, and \nbytes. Slicing can be extended to any Python class that implements \nthe __getitem__ and __setitem__ special methods (see Item 43: \n“Inherit from collections.abc for Custom Container Types”).\nThe basic form of the slicing syntax is somelist[start:end], where \nstart is inclusive and end is exclusive:\na = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nprint('Middle two:  ', a[3:5])\nprint('All but ends:', a[1:7])\n>>>\nMiddle two:   ['d', 'e']\nAll but ends: ['b', 'c', 'd', 'e', 'f', 'g']\n",
      "content_length": 1613,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "44 \nChapter 2 Lists and Dictionaries\nWhen slicing from the start of a list, you should leave out the zero \nindex to reduce visual noise:\nassert a[:5] == a[0:5]\nWhen slicing to the end of a list, you should leave out the final index \nbecause it’s redundant:\nassert a[5:] == a[5:len(a)]\nUsing negative numbers for slicing is helpful for doing offsets relative \nto the end of a list. All of these forms of slicing would be clear to \na new reader of your code:\na[:]      # ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\na[:5]     # ['a', 'b', 'c', 'd', 'e']\na[:-1]    # ['a', 'b', 'c', 'd', 'e', 'f', 'g']\na[4:]     #                     ['e', 'f', 'g', 'h']\na[-3:]    #                          ['f', 'g', 'h']\na[2:5]    #           ['c', 'd', 'e']\na[2:-1]   #           ['c', 'd', 'e', 'f', 'g']\na[-3:-1]  #                          ['f', 'g']\nThere are no surprises here, and I encourage you to use these \nvariations.\nSlicing deals properly with start and end indexes that are beyond the \nboundaries of a list by silently omitting missing items. This behav-\nior makes it easy for your code to establish a maximum length to \n consider for an input sequence:\nfirst_twenty_items = a[:20]\nlast_twenty_items = a[-20:]\nIn contrast, accessing the same index directly causes an exception:\na[20]\n>>>\nTraceback ...\nIndexError: list index out of range\nNote\nBeware that indexing a list by a negated variable is one of the few situ-\nations in which you can get surprising results from slicing. For example, \nthe expression somelist[-n:] will work fine when n is greater than one \n(e.g., somelist[-3:]). However, when n is zero, the expression somelist[-0:] \nis equivalent to somelist[:] and will result in a copy of the original list.\n",
      "content_length": 1715,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": " \nItem 11: Know How to Slice Sequences \n45\nThe result of slicing a list is a whole new list. References to the \nobjects from the original list are maintained. Modifying the result of \nslicing won’t affect the original list:\nb = a[3:]\nprint('Before:   ', b)\nb[1] = 99\nprint('After:    ', b)\nprint('No change:', a)\n>>>\nBefore:    ['d', 'e', 'f', 'g', 'h']\nAfter:     ['d', 99, 'f', 'g', 'h']\nNo change: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nWhen used in assignments, slices replace the specified range \nin the original list. Unlike unpacking assignments (such as \na, b = c[:2]; see Item 6: “Prefer Multiple Assignment Unpacking \nOver Indexing”), the lengths of slice assignments don’t need to be the \nsame. The values before and after the assigned slice will be preserved. \nHere, the list shrinks because the replacement list is shorter than \nthe specified slice:\nprint('Before ', a)\na[2:7] = [99, 22, 14]\nprint('After  ', a)\n>>>\nBefore  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nAfter   ['a', 'b', 99, 22, 14, 'h']\nAnd here the list grows because the assigned list is longer than the \nspecific slice:\nprint('Before ', a)\na[2:3] = [47, 11]\nprint('After  ', a)\n>>>\nBefore  ['a', 'b', 99, 22, 14, 'h']\nAfter   ['a', 'b', 47, 11, 22, 14, 'h']\nIf you leave out both the start and the end indexes when slicing, you \nend up with a copy of the original list:\nb = a[:]\nassert b == a and b is not a\n",
      "content_length": 1390,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "46 \nChapter 2 Lists and Dictionaries\nIf you assign to a slice with no start or end indexes, you replace the \nentire contents of the list with a copy of what’s referenced (instead of \nallocating a new list):\nb = a\nprint('Before a', a)\nprint('Before b', b)\na[:] = [101, 102, 103]\nassert a is b             # Still the same list object\nprint('After a ', a)      # Now has different contents\nprint('After b ', b)      # Same list, so same contents as a\n>>>\nBefore a ['a', 'b', 47, 11, 22, 14, 'h']\nBefore b ['a', 'b', 47, 11, 22, 14, 'h']\nAfter a  [101, 102, 103]\nAfter b  [101, 102, 103]\nThings to Remember\n✦ Avoid being verbose when slicing: Don’t supply 0 for the start index \nor the length of the sequence for the end index.\n✦ Slicing is forgiving of start or end indexes that are out of bounds, \nwhich means it’s easy to express slices on the front or back bound-\naries of a sequence (like a[:20] or a[-20:]).\n✦ Assigning to a list slice replaces that range in the original sequence \nwith what’s referenced even if the lengths are different.\nItem 12:  Avoid Striding and Slicing in \na Single Expression\nIn addition to basic slicing (see Item 11: “Know How to Slice \nSequences”), Python has special syntax for the stride of a slice in \nthe form somelist[start:end:stride]. This lets you take every nth item \nwhen slicing a sequence. For example, the stride makes it easy to \ngroup by even and odd indexes in a list:\nx = ['red', 'orange', 'yellow', 'green', 'blue', 'purple']\nodds = x[::2]\nevens = x[1::2]\nprint(odds)\nprint(evens)\n>>>\n['red', 'yellow', 'blue']\n['orange', 'green', 'purple']\n",
      "content_length": 1590,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": " \nItem 12: Avoid Striding and Slicing in a Single Expression \n47\nThe problem is that the stride syntax often causes unexpected behav-\nior that can introduce bugs. For example, a common Python trick for \nreversing a byte string is to slice the string with a stride of -1:\nx = b'mongoose'\ny = x[::-1]\nprint(y)\n>>>\nb'esoognom'\nThis also works correctly for Unicode strings (see Item 3: “Know the \nDifferences Between bytes and str”):\nx = 'ᇓৌ'\ny = x[::-1]\nprint(y)\n>>>\nৌᇓ\nBut it will break when Unicode data is encoded as a UTF-8 byte string:\nw = 'ᇓৌ'\nx = w.encode('utf-8')\ny = x[::-1]\nz = y.decode('utf-8')\n>>>\nTraceback ...\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xb8 in \nposition 0: invalid start byte\nAre negative strides besides -1 useful? Consider the following \nexamples:\nx = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nx[::2]   # ['a', 'c', 'e', 'g']\nx[::-2]  # ['h', 'f', 'd', 'b']\nHere, ::2 means “Select every second item starting at the beginning.” \nTrickier, ::-2 means “Select every second item starting at the end and \nmoving backward.”\nWhat do you think 2::2 means? What about -2::-2 vs. -2:2:-2 vs. \n2:2:-2?\nx[2::2]     # ['c', 'e', 'g']\nx[-2::-2]   # ['g', 'e', 'c', 'a']\nx[-2:2:-2]  # ['g', 'e']\nx[2:2:-2]   # []\n",
      "content_length": 1238,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "48 \nChapter 2 Lists and Dictionaries\nThe point is that the stride part of the slicing syntax can be extremely \nconfusing. Having three numbers within the brackets is hard enough \nto read because of its density. Then, it’s not obvious when the start \nand end indexes come into effect relative to the stride value, espe-\ncially when the stride is negative.\nTo prevent problems, I suggest you avoid using a stride along with \nstart and end indexes. If you must use a stride, prefer making it a \npositive value and omit start and end indexes. If you must use a stride \nwith start or end indexes, consider using one assignment for striding \nand another for slicing:\ny = x[::2]   # ['a', 'c', 'e', 'g']\nz = y[1:-1]  # ['c', 'e']\nStriding and then slicing creates an extra shallow copy of the data. \nThe first operation should try to reduce the size of the resulting slice \nby as much as possible. If your program can’t afford the time or mem-\nory required for two steps, consider using the itertools built-in mod-\nule’s islice method (see Item 36: “Consider itertools for Working with \nIterators and Generators”), which is clearer to read and doesn’t permit \nnegative values for start, end, or stride.\nThings to Remember\n✦ Specifying start, end, and stride in a slice can be extremely \nconfusing.\n✦ Prefer using positive stride values in slices without start or end \nindexes. Avoid negative stride values if possible.\n✦ Avoid using start, end, and stride together in a single slice. If you \nneed all three parameters, consider doing two assignments (one \nto stride and another to slice) or using islice from the itertools \nbuilt-in module.\nItem 13: Prefer Catch-All Unpacking Over Slicing\nOne limitation of basic unpacking (see Item 6: “Prefer Multiple Assign-\nment Unpacking Over Indexing”) is that you must know the length of \nthe sequences you’re unpacking in advance. For example, here I have \na list of the ages of cars that are being traded in at a dealership. \nWhen I try to take the first two items of the list with basic unpack-\ning, an exception is raised at runtime:\ncar_ages = [0, 9, 4, 8, 7, 20, 19, 1, 6, 15]\ncar_ages_descending = sorted(car_ages, reverse=True)\noldest, second_oldest = car_ages_descending\n",
      "content_length": 2214,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": " \nItem 13: Prefer Catch-All Unpacking Over Slicing \n49\n>>>\nTraceback ...\nValueError: too many values to unpack (expected 2)\nNewcomers to Python often rely on indexing and slicing (see Item 11: \n“Know How to Slice Sequences”) for this situation. For example, here \nI extract the oldest, second oldest, and other car ages from a list of at \nleast two items:\noldest = car_ages_descending[0]\nsecond_oldest = car_ages_descending[1]\nothers = car_ages_descending[2:]\nprint(oldest, second_oldest, others)\n>>>\n20 19 [15, 9, 8, 7, 6, 4, 1, 0]\nThis works, but all of the indexing and slicing is visually noisy. In \npractice, it’s also error prone to divide the members of a sequence into \nvarious subsets this way because you’re much more likely to make \noff-by-one errors; for example, you might change boundaries on one \nline and forget to update the others.\nTo better handle this situation, Python also supports catch-all \nunpacking through a starred expression. This syntax allows one part \nof the unpacking assignment to receive all values that didn’t match \nany other part of the unpacking pattern. Here, I use a starred expres-\nsion to achieve the same result as above without indexing or slicing:\noldest, second_oldest, *others = car_ages_descending\nprint(oldest, second_oldest, others)\n>>>\n20 19 [15, 9, 8, 7, 6, 4, 1, 0]\nThis code is shorter, easier to read, and no longer has the error-prone \nbrittleness of boundary indexes that must be kept in sync between \nlines.\nA starred expression may appear in any position, so you can get the \nbenefits of catch-all unpacking anytime you need to extract one slice:\noldest, *others, youngest = car_ages_descending\nprint(oldest, youngest, others)\n*others, second_youngest, youngest = car_ages_descending\nprint(youngest, second_youngest, others)\n>>>\n20 0 [19, 15, 9, 8, 7, 6, 4, 1]\n0 1 [20, 19, 15, 9, 8, 7, 6, 4]\n",
      "content_length": 1853,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "50 \nChapter 2 Lists and Dictionaries\nHowever, to unpack assignments that contain a starred expres-\nsion, you must have at least one required part, or else you’ll get a \nSyntaxError. You can’t use a catch-all expression on its own:\n*others = car_ages_descending\n>>>\nTraceback ...\nSyntaxError: starred assignment target must be in a list or \n¯tuple\nYou also can’t use multiple catch-all expressions in a single-level \nunpacking pattern:\nfirst, *middle, *second_middle, last = [1, 2, 3, 4]\n>>>\nTraceback ...\nSyntaxError: two starred expressions in assignment\nBut it is possible to use multiple starred expressions in an unpacking \nassignment statement, as long as they’re catch-alls for different parts \nof the multilevel structure being unpacked. I don’t recommend doing \nthe following (see Item 19: “Never Unpack More Than Three Variables \nWhen Functions Return Multiple Values” for related guidance), but \nunderstanding it should help you develop an intuition for how starred \nexpressions can be used in unpacking assignments:\ncar_inventory = {\n    'Downtown': ('Silver Shadow', 'Pinto', 'DMC'),\n    'Airport': ('Skyline', 'Viper', 'Gremlin', 'Nova'),\n}\n((loc1, (best1, *rest1)),\n (loc2, (best2, *rest2))) = car_inventory.items()\nprint(f'Best at {loc1} is {best1}, {len(rest1)} others')\nprint(f'Best at {loc2} is {best2}, {len(rest2)} others')\n>>>\nBest at Downtown is Silver Shadow, 2 others\nBest at Airport is Skyline, 3 others\nStarred expressions become list instances in all cases. If there are \nno leftover items from the sequence being unpacked, the catch-all \npart will be an empty list. This is especially useful when you’re pro-\ncessing a sequence that you know in advance has at least N elements:\nshort_list = [1, 2]\nfirst, second, *rest = short_list\nprint(first, second, rest)\n",
      "content_length": 1787,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": " \nItem 13: Prefer Catch-All Unpacking Over Slicing \n51\n>>>\n1 2 []\nYou can also unpack arbitrary iterators with the unpacking  syntax. \nThis isn’t worth much with a basic multiple-assignment statement. \nFor example, here I unpack the values from iterating over a range \nof length 2. This doesn’t seem useful because it would be easier \nto just assign to a static list that matches the unpacking pattern \n(e.g., [1, 2]):\nit = iter(range(1, 3))\nfirst, second = it\nprint(f'{first} and {second}')\n>>>\n1 and 2\nBut with the addition of starred expressions, the value of unpack-\ning iterators becomes clear. For example, here I have a generator \nthat yields the rows of a CSV file containing all car orders from the \n dealership this week:\ndef generate_csv():\n    yield ('Date', 'Make' , 'Model', 'Year', 'Price')\n    ...\nProcessing the results of this generator using indexes and slices is \nfine, but it requires multiple lines and is visually noisy:\nall_csv_rows = list(generate_csv())\nheader = all_csv_rows[0]\nrows = all_csv_rows[1:]\nprint('CSV Header:', header)\nprint('Row count: ', len(rows))\n>>>\nCSV Header: ('Date', 'Make', 'Model', 'Year', 'Price')\nRow count:  200\nUnpacking with a starred expression makes it easy to process the first \nrow—the header—separately from the rest of the iterator’s  contents. \nThis is much clearer:\nit = generate_csv()\nheader, *rows = it\nprint('CSV Header:', header)\nprint('Row count: ', len(rows))\n>>>\nCSV Header: ('Date', 'Make', 'Model', 'Year', 'Price')\nRow count:  200\n",
      "content_length": 1504,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "52 \nChapter 2 Lists and Dictionaries\nKeep in mind, however, that because a starred expression is always \nturned into a list, unpacking an iterator also risks the potential of \nusing up all of the memory on your computer and causing your pro-\ngram to crash. So you should only use catch-all unpacking on itera-\ntors when you have good reason to believe that the result data will all \nfit in memory (see Item 31: “Be Defensive When Iterating Over Argu-\nments” for another approach).\nThings to Remember\n✦ Unpacking assignments may use a starred expression to catch all \nvalues that weren’t assigned to the other parts of the unpacking \npattern into a list.\n✦ Starred expressions may appear in any position, and they will \nalways become a list containing the zero or more values they \nreceive.\n✦ When dividing a list into non-overlapping pieces, catch-all unpack-\ning is much less error prone than slicing and indexing.\nItem 14:  Sort by Complex Criteria Using the key \nParameter\nThe list built-in type provides a sort method for ordering the items \nin a list instance based on a variety of criteria. By default, sort will \norder a list’s contents by the natural ascending order of the items. \nFor example, here I sort a list of integers from smallest to largest:\nnumbers = [93, 86, 11, 68, 70]\nnumbers.sort()\nprint(numbers)\n>>>\n[11, 68, 70, 86, 93]\nThe sort method works for nearly all built-in types (strings, floats, \netc.) that have a natural ordering to them. What does sort do with \nobjects? For example, here I define a class—including a __repr__ \nmethod so instances are printable; see Item 75: “Use repr Strings for \nDebugging Output”—to represent various tools you may need to use \non a construction site:\nclass Tool:\n    def __init__(self, name, weight):\n        self.name = name\n        self.weight = weight\n    def __repr__(self):\n        return f'Tool({self.name!r}, {self.weight})'\n",
      "content_length": 1893,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": " \nItem 14: Sort by Complex Criteria Using the key Parameter \n53\ntools = [\n    Tool('level', 3.5),\n    Tool('hammer', 1.25),\n    Tool('screwdriver', 0.5),\n    Tool('chisel', 0.25),\n]\nSorting objects of this type doesn’t work because the sort method \ntries to call comparison special methods that aren’t defined by the \nclass:\ntools.sort()\n>>>\nTraceback ...\nTypeError: '<' not supported between instances of 'Tool' and \n'Tool'\nIf your class should have a natural ordering like integers do, then you \ncan define the necessary special methods (see Item 73: “Know How \nto Use heapq for Priority Queues” for an example) to make sort work \nwithout extra parameters. But the more common case is that your \nobjects may need to support multiple orderings, in which case defin-\ning a natural ordering really doesn’t make sense.\nOften there’s an attribute on the object that you’d like to use for sort-\ning. To support this use case, the sort method accepts a key param-\neter that’s expected to be a function. The key function is passed a \nsingle argument, which is an item from the list that is being sorted. \nThe return value of the key function should be a comparable value \n(i.e., with a natural ordering) to use in place of an item for sorting \npurposes.\nHere, I use the lambda keyword to define a function for the key param-\neter that enables me to sort the list of Tool objects alphabetically by \ntheir name:\nprint('Unsorted:', repr(tools))\ntools.sort(key=lambda x: x.name)\nprint('\\nSorted:  ', tools)\n>>>\nUnsorted: [Tool('level',       3.5),\n           Tool('hammer',      1.25),\n           Tool('screwdriver', 0.5),\n           Tool('chisel',      0.25)]\n",
      "content_length": 1651,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "54 \nChapter 2 Lists and Dictionaries\nSorted:   [Tool('chisel',      0.25),\n           Tool('hammer',      1.25),\n           Tool('level',       3.5),\n           Tool('screwdriver', 0.5)]\nI can just as easily define another lambda function to sort by weight \nand pass it as the key parameter to the sort method:\ntools.sort(key=lambda x: x.weight)\nprint('By weight:', tools)\n>>>\nBy weight: [Tool('chisel',      0.25),\n            Tool('screwdriver', 0.5),\n            Tool('hammer',      1.25),\n            Tool('level',       3.5)]\nWithin the lambda function passed as the key parameter you can \naccess attributes of items as I’ve done here, index into items (for \nsequences, tuples, and dictionaries), or use any other valid expression.\nFor basic types like strings, you may even want to use the key func-\ntion to do transformations on the values before sorting. For example, \nhere I apply the lower method to each item in a list of place names to \nensure that they’re in alphabetical order, ignoring any capitalization \n(since in the natural lexical ordering of strings, capital letters come \nbefore lowercase letters):\nplaces = ['home', 'work', 'New York', 'Paris']\nplaces.sort()\nprint('Case sensitive:  ', places)\nplaces.sort(key=lambda x: x.lower())\nprint('Case insensitive:', places)\n>>>\nCase sensitive:   ['New York', 'Paris',    'home',  'work']\nCase insensitive: ['home',     'New York', 'Paris', 'work']\nSometimes you may need to use multiple criteria for sorting. For \nexample, say that I have a list of power tools and I want to sort them \nfirst by weight and then by name. How can I accomplish this?\npower_tools = [\n    Tool('drill', 4),\n    Tool('circular saw', 5),\n    Tool('jackhammer', 40),\n    Tool('sander', 4),\n]\n",
      "content_length": 1732,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": " \nItem 14: Sort by Complex Criteria Using the key Parameter \n55\nThe simplest solution in Python is to use the tuple type. Tuples are \nimmutable sequences of arbitrary Python values. Tuples are compara-\nble by default and have a natural ordering, meaning that they imple-\nment all of the special methods, such as __lt__, that are required by \nthe sort method. Tuples implement these special method comparators \nby iterating over each position in the tuple and comparing the cor-\nresponding values one index at a time. Here, I show how this works \nwhen one tool is heavier than another:\nsaw = (5, 'circular saw')\njackhammer = (40, 'jackhammer')\nassert not (jackhammer < saw)  # Matches expectations\nIf the first position in the tuples being compared are equal—weight \nin this case—then the tuple comparison will move on to the second \nposition, and so on:\ndrill = (4, 'drill')\nsander = (4, 'sander')\nassert drill[0] == sander[0]  # Same weight\nassert drill[1] < sander[1]   # Alphabetically less\nassert drill < sander         # Thus, drill comes first\nYou can take advantage of this tuple comparison behavior in order \nto sort the list of power tools first by weight and then by name. Here, \nI define a key function that returns a tuple containing the two attri-\nbutes that I want to sort on in order of priority:\npower_tools.sort(key=lambda x: (x.weight, x.name))\nprint(power_tools)\n>>>\n[Tool('drill',        4),\n Tool('sander',       4),\n Tool('circular saw', 5),\n Tool('jackhammer',   40)]\nOne limitation of having the key function return a tuple is that the \ndirection of sorting for all criteria must be the same (either all in \nascending order, or all in descending order). If I provide the reverse \nparameter to the sort method, it will affect both criteria in the tuple \nthe same way (note how 'sander' now comes before 'drill' instead of \nafter):\npower_tools.sort(key=lambda x: (x.weight, x.name),\n                 reverse=True)  # Makes all criteria descending\nprint(power_tools)\n",
      "content_length": 1988,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "56 \nChapter 2 Lists and Dictionaries\n>>>\n[Tool('jackhammer',   40),\n Tool('circular saw', 5),\n Tool('sander',       4),\n Tool('drill',        4)]\nFor numerical values it’s possible to mix sorting directions by using \nthe unary minus operator in the key function. This negates one of \nthe values in the returned tuple, effectively reversing its sort order \nwhile leaving the others intact. Here, I use this approach to sort by \nweight descending, and then by name ascending (note how 'sander' \nnow comes after 'drill' instead of before):\npower_tools.sort(key=lambda x: (-x.weight, x.name))\nprint(power_tools)\n>>>\n[Tool('jackhammer',   40),\n Tool('circular saw', 5),\n Tool('drill',        4),\n Tool('sander',       4)]\nUnfortunately, unary negation isn’t possible for all types. Here, I try \nto achieve the same outcome by using the reverse argument to sort \nby weight descending and then negating name to put it in ascending \norder:\npower_tools.sort(key=lambda x: (x.weight, -x.name),\n                 reverse=True)\n>>>\nTraceback ...\nTypeError: bad operand type for unary -: 'str'\nFor situations like this, Python provides a stable sorting algorithm. \nThe sort method of the list type will preserve the order of the input \nlist when the key function returns values that are equal to each \nother. This means that I can call sort multiple times on the same \nlist to combine different criteria together. Here, I produce the same \nsort ordering of weight descending and name ascending as I did above \nbut by using two separate calls to sort:\npower_tools.sort(key=lambda x: x.name)   # Name ascending\npower_tools.sort(key=lambda x: x.weight, # Weight descending\n                 reverse=True)\nprint(power_tools)\n",
      "content_length": 1706,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": " \nItem 14: Sort by Complex Criteria Using the key Parameter \n57\n>>>\n[Tool('jackhammer',   40),\n Tool('circular saw', 5),\n Tool('drill',        4),\n Tool('sander',       4)]\nTo understand why this works, note how the first call to sort puts the \nnames in alphabetical order:\npower_tools.sort(key=lambda x: x.name)\nprint(power_tools)\n>>>\n[Tool('circular saw', 5),\n Tool('drill',        4),\n Tool('jackhammer',   40),\n Tool('sander',       4)]\nWhen the second sort call by weight descending is made, it sees that \nboth 'sander' and 'drill' have a weight of 4. This causes the sort \nmethod to put both items into the final result list in the same order \nthat they appeared in the original list, thus preserving their relative \nordering by name ascending:\npower_tools.sort(key=lambda x: x.weight,\n                 reverse=True)\nprint(power_tools)\n>>>\n[Tool('jackhammer',   40),\n Tool('circular saw', 5),\n Tool('drill',        4),\n Tool('sander',       4)]\nThis same approach can be used to combine as many different types \nof sorting criteria as you’d like in any direction, respectively. You just \nneed to make sure that you execute the sorts in the opposite sequence \nof what you want the final list to contain. In this example, I wanted \nthe sort order to be by weight descending and then by name ascend-\ning, so I had to do the name sort first, followed by the weight sort.\nThat said, the approach of having the key function return a tuple, \nand using unary negation to mix sort orders, is simpler to read and \nrequires less code. I recommend only using multiple calls to sort if \nit’s absolutely necessary.\n",
      "content_length": 1607,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "58 \nChapter 2 Lists and Dictionaries\nThings to Remember\n✦ The sort method of the list type can be used to rearrange a list’s \ncontents by the natural ordering of built-in types like strings, inte-\ngers, tuples, and so on.\n✦ The sort method doesn’t work for objects unless they define a natu-\nral ordering using special methods, which is uncommon.\n✦ The key parameter of the sort method can be used to supply a \nhelper function that returns the value to use for sorting in place of \neach item from the list.\n✦ Returning a tuple from the key function allows you to combine mul-\ntiple sorting criteria together. The unary minus operator can be \nused to reverse individual sort orders for types that allow it.\n✦ For types that can’t be negated, you can combine many sorting cri-\nteria together by calling the sort method multiple times using dif-\nferent key functions and reverse values, in the order of lowest rank \nsort call to highest rank sort call.\nItem 15:  Be Cautious When Relying on dict Insertion \nOrdering\nIn Python 3.5 and before, iterating over a dict would return keys in \narbitrary order. The order of iteration would not match the order in \nwhich the items were inserted. For example, here I create a dictionary \nmapping animal names to their corresponding baby names and then \nprint it out (see Item 75: “Use repr Strings for Debugging Output” for \nhow this works):\n# Python 3.5\nbaby_names = {\n    'cat': 'kitten',\n    'dog': 'puppy',\n}\nprint(baby_names)\n>>>\n{'dog': 'puppy', 'cat': 'kitten'}\nWhen I created the dictionary the keys were in the order 'cat', 'dog', \nbut when I printed it the keys were in the reverse order 'dog', 'cat'. \nThis behavior is surprising, makes it harder to reproduce test cases, \nincreases the difficulty of debugging, and is especially confusing to \nnewcomers to Python.\n",
      "content_length": 1813,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": " \nItem 15: Be Cautious When Relying on dict Insertion Ordering \n59\nThis happened because the dictionary type previously implemented \nits hash table algorithm with a combination of the hash built-in func-\ntion and a random seed that was assigned when the Python inter-\npreter started. Together, these behaviors caused dictionary orderings \nto not match insertion order and to randomly shuffle between pro-\ngram executions.\nStarting with Python 3.6, and officially part of the Python specifica-\ntion in version 3.7, dictionaries will preserve insertion order. Now, this \ncode will always print the dictionary in the same way it was originally \ncreated by the programmer:\nbaby_names = {\n    'cat': 'kitten',\n    'dog': 'puppy',\n}\nprint(baby_names)\n>>>\n{'cat': 'kitten', 'dog': 'puppy'}\nWith Python 3.5 and earlier, all methods provided by dict that relied \non iteration order, including keys, values, items, and popitem, would \nsimilarly demonstrate this random-looking behavior:\n# Python 3.5\nprint(list(baby_names.keys()))\nprint(list(baby_names.values()))\nprint(list(baby_names.items()))\nprint(baby_names.popitem())  # Randomly chooses an item\n>>>\n['dog', 'cat']\n['puppy', 'kitten']\n[('dog', 'puppy'), ('cat', 'kitten')]\n('dog', 'puppy')\nThese methods now provide consistent insertion ordering that you \ncan rely on when you write your programs:\nprint(list(baby_names.keys()))\nprint(list(baby_names.values()))\nprint(list(baby_names.items()))\nprint(baby_names.popitem())  # Last item inserted\n>>>\n['cat', 'dog']\n['kitten', 'puppy']\n[('cat', 'kitten'), ('dog', 'puppy')]\n('dog', 'puppy')\n",
      "content_length": 1584,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "60 \nChapter 2 Lists and Dictionaries\nThere are many repercussions of this change on other Python features \nthat are dependent on the dict type and its specific implementation.\nKeyword arguments to functions—including the **kwargs catch-all \nparameter; see Item 23: “Provide Optional Behavior with Keyword \nArguments”—previously would come through in seemingly random \norder, which can make it harder to debug function calls:\n# Python 3.5\ndef my_func(**kwargs):\n    for key, value in kwargs.items():\n        print('%s = %s' % (key, value))\nmy_func(goose='gosling', kangaroo='joey')\n>>>\nkangaroo = joey\ngoose = gosling\nNow, the order of keyword arguments is always preserved to match \nhow the programmer originally called the function:\ndef my_func(**kwargs):\n    for key, value in kwargs.items():\n        print(f'{key} = {value}')\nmy_func(goose='gosling', kangaroo='joey')\n>>>\ngoose = gosling\nkangaroo = joey\nClasses also use the dict type for their instance dictionaries. In pre-\nvious versions of Python, object fields would show the randomizing \nbehavior:\n# Python 3.5\nclass MyClass:\n    def __init__(self):\n        self.alligator = 'hatchling'\n        self.elephant = 'calf'\na = MyClass()\nfor key, value in a.__dict__.items():\n    print('%s = %s' % (key, value))\n>>>\nelephant = calf\nalligator = hatchling\n",
      "content_length": 1307,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": " \nItem 15: Be Cautious When Relying on dict Insertion Ordering \n61\nAgain, you can now assume that the order of assignment for these \ninstance fields will be reflected in __dict__:\nclass MyClass:\n    def __init__(self):\n        self.alligator = 'hatchling'\n        self.elephant = 'calf'\na = MyClass()\nfor key, value in a.__dict__.items():\n    print(f'{key} = {value}')\n>>>\nalligator = hatchling\nelephant = calf\nThe way that dictionaries preserve insertion ordering is now part of \nthe Python language specification. For the language features above, \nyou can rely on this behavior and even make it part of the APIs you \ndesign for your classes and functions.\nNote\nFor a long time the collections built-in module has had an OrderedDict \nclass that preserves insertion ordering. Although this class’s behavior is similar \nto that of the standard dict type (since Python 3.7), the performance charac-\nteristics of OrderedDict are quite different. If you need to handle a high rate \nof key insertions and popitem calls (e.g., to implement a least-recently-used \ncache), OrderedDict may be a better fit than the standard Python dict type \n(see Item 70: “Profile Before Optimizing” on how to make sure you need this).\nHowever, you shouldn’t always assume that insertion ordering behav-\nior will be present when you’re handling dictionaries. Python makes \nit easy for programmers to define their own custom container types \nthat emulate the standard protocols matching list, dict, and other \ntypes (see Item 43: “Inherit from collections.abc for Custom Con-\ntainer Types”). Python is not statically typed, so most code relies on \nduck typing—where an object’s behavior is its de facto type—instead \nof rigid class hierarchies. This can result in surprising gotchas.\nFor example, say that I’m writing a program to show the results of a \ncontest for the cutest baby animal. Here, I start with a dictionary con-\ntaining the total vote count for each one:\nvotes = {\n    'otter': 1281,\n    'polar bear': 587,\n    'fox': 863,\n}\n",
      "content_length": 2014,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "62 \nChapter 2 Lists and Dictionaries\nI define a function to process this voting data and save the rank of \neach animal name into a provided empty dictionary. In this case, the \ndictionary could be the data model that powers a UI element:\ndef populate_ranks(votes, ranks):\n    names = list(votes.keys())\n    names.sort(key=votes.get, reverse=True)\n    for i, name in enumerate(names, 1):\n        ranks[name] = i\nI also need a function that will tell me which animal won the contest. \nThis function works by assuming that populate_ranks will assign the \ncontents of the ranks dictionary in ascending order, meaning that the \nfirst key must be the winner:\ndef get_winner(ranks):\n    return next(iter(ranks))\nHere, I can confirm that these functions work as designed and deliver \nthe result that I expected:\nranks = {}\npopulate_ranks(votes, ranks)\nprint(ranks)\nwinner = get_winner(ranks)\nprint(winner)\n>>>\n{'otter': 1, 'fox': 2, 'polar bear': 3}\notter\nNow, imagine that the requirements of this program have changed. \nThe UI element that shows the results should be in alphabet-\nical order instead of rank order. To accomplish this, I can use the \ncollections.abc built-in module to define a new dictionary-like class \nthat iterates its contents in alphabetical order:\nfrom collections.abc import MutableMapping\nclass SortedDict(MutableMapping):\n    def __init__(self):\n        self.data = {}\n    def __getitem__(self, key):\n        return self.data[key]\n    def __setitem__(self, key, value):\n        self.data[key] = value\n",
      "content_length": 1521,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": " \nItem 15: Be Cautious When Relying on dict Insertion Ordering \n63\n    def __delitem__(self, key):\n        del self.data[key]\n    def __iter__(self):\n        keys = list(self.data.keys())\n        keys.sort()\n        for key in keys:\n            yield key\n    def __len__(self):\n        return len(self.data)\nI can use a SortedDict instance in place of a standard dict with the \nfunctions from before and no errors will be raised since this class \nconforms to the protocol of a standard dictionary. However, the result \nis incorrect:\nsorted_ranks = SortedDict()\npopulate_ranks(votes, sorted_ranks)\nprint(sorted_ranks.data)\nwinner = get_winner(sorted_ranks)\nprint(winner)\n>>>\n{'otter': 1, 'fox': 2, 'polar bear': 3}\nfox\nThe problem here is that the implementation of get_winner assumes \nthat the dictionary’s iteration is in insertion order to match \npopulate_ranks. This code is using SortedDict instead of dict, so that \nassumption is no longer true. Thus, the value returned for the winner \nis 'fox', which is alphabetically first.\nThere are three ways to mitigate this problem. First, I can reimple-\nment the get_winner function to no longer assume that the ranks dic-\ntionary has a specific iteration order. This is the most conservative \nand robust solution:\ndef get_winner(ranks):\n    for name, rank in ranks.items():\n        if rank == 1:\n            return name\nwinner = get_winner(sorted_ranks)\nprint(winner)\n>>>\notter\n",
      "content_length": 1427,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "64 \nChapter 2 Lists and Dictionaries\nThe second approach is to add an explicit check to the top of the func-\ntion to ensure that the type of ranks matches my expectations, and \nto raise an exception if not. This solution likely has better runtime \nperformance than the more conservative approach:\ndef get_winner(ranks):\n    if not isinstance(ranks, dict):\n        raise TypeError('must provide a dict instance')\n    return next(iter(ranks))\nget_winner(sorted_ranks)\n>>>\nTraceback ...\nTypeError: must provide a dict instance\nThe third alternative is to use type annotations to enforce that the \nvalue passed to get_winner is a dict instance and not a MutableMapping \nwith dictionary-like behavior (see Item 90: “Consider Static Analysis \nvia typing to Obviate Bugs”). Here, I run the mypy tool in strict mode \non an annotated version of the code above:\nfrom typing import Dict, MutableMapping\ndef populate_ranks(votes: Dict[str, int],\n                   ranks: Dict[str, int]) -> None:\n    names = list(votes.keys())\n    names.sort(key=votes.get, reverse=True)\n    for i, name in enumerate(names, 1):\n        ranks[name] = i\ndef get_winner(ranks: Dict[str, int]) -> str:\n    return next(iter(ranks))\nclass SortedDict(MutableMapping[str, int]):\n    ...\nvotes = {\n    'otter': 1281,\n    'polar bear': 587,\n    'fox': 863,\n}\nsorted_ranks = SortedDict()\npopulate_ranks(votes, sorted_ranks)\nprint(sorted_ranks.data)\nwinner = get_winner(sorted_ranks)\nprint(winner)\n",
      "content_length": 1458,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": " Item 16: Prefer get Over in and KeyError to Handle Missing Dictionary Keys 65\n$ python3 -m mypy --strict example.py\n.../example.py:48: error: Argument 2 to \"populate_ranks\" has \n¯incompatible type \"SortedDict\"; expected \"Dict[str, int]\"\n.../example.py:50: error: Argument 1 to \"get_winner\" has \n¯incompatible type \"SortedDict\"; expected \"Dict[str, int]\"\nThis correctly detects the mismatch between the dict and \nMutableMapping types and flags the incorrect usage as an error. This \nsolution provides the best mix of static type safety and runtime \nperformance.\nThings to Remember\n✦ Since Python 3.7, you can rely on the fact that iterating a dict \ninstance’s contents will occur in the same order in which the keys \nwere initially added.\n✦ Python makes it easy to define objects that act like dictionaries but \nthat aren’t dict instances. For these types, you can’t assume that \ninsertion ordering will be preserved.\n✦ There are three ways to be careful about dictionary-like classes: \nWrite code that doesn’t rely on insertion ordering, explicitly check \nfor the dict type at runtime, or require dict values using type anno-\ntations and static analysis.\nItem 16:  Prefer get Over in and KeyError to Handle \nMissing Dictionary Keys\nThe three fundamental operations for interacting with dictionar-\nies are accessing, assigning, and deleting keys and their associated \nvalues. The contents of dictionaries are dynamic, and thus it’s entirely \npossible—even likely—that when you try to access or delete a key, \nit won’t already be present.\nFor example, say that I’m trying to determine people’s favorite type of \nbread to devise the menu for a sandwich shop. Here, I define a dictio-\nnary of counters with the current votes for each style:\ncounters = {\n    'pumpernickel': 2,\n    'sourdough': 1,\n}\nTo increment the counter for a new vote, I need to see if the key exists, \ninsert the key with a default counter value of zero if it’s missing, and \nthen increment the counter’s value. This requires accessing the key \ntwo times and assigning it once. Here, I accomplish this task using \n",
      "content_length": 2083,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "66 \nChapter 2 Lists and Dictionaries\nan if statement with an in expression that returns True when the key \nis present:\nkey = 'wheat'\nif key in counters:\n    count = counters[key]\nelse:\n    count = 0\ncounters[key] = count + 1\nAnother way to accomplish the same behavior is by relying on how \ndictionaries raise a KeyError exception when you try to get the value \nfor a key that doesn’t exist. This approach is more efficient because it \nrequires only one access and one assignment:\ntry:\n    count = counters[key]\nexcept KeyError:\n    count = 0\ncounters[key] = count + 1\nThis flow of fetching a key that exists or returning a default value \nis so common that the dict built-in type provides the get method to \naccomplish this task. The second parameter to get is the default value \nto return in the case that the key—the first parameter—isn’t present. \nThis also requires only one access and one assignment, but it’s much \nshorter than the KeyError example:\ncount = counters.get(key, 0)\ncounters[key] = count + 1\nIt’s possible to shorten the in expression and KeyError approaches in \nvarious ways, but all of these alternatives suffer from requiring code \nduplication for the assignments, which makes them less readable and \nworth avoiding:\nif key not in counters:\n    counters[key] = 0\ncounters[key] += 1\nif key in counters:\n    counters[key] += 1\nelse:\n    counters[key] = 1\n",
      "content_length": 1375,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": " Item 16: Prefer get Over in and KeyError to Handle Missing Dictionary Keys 67\ntry:\n    counters[key] += 1\nexcept KeyError:\n    counters[key] = 1\nThus, for a dictionary with simple types, using the get method is the \nshortest and clearest option.\nNote\nIf you’re maintaining dictionaries of counters like this, it’s worth considering \nthe Counter class from the collections built-in module, which provides \nmost of the facilities you are likely to need.\nWhat if the values of the dictionary are a more complex type, like a \nlist? For example, say that instead of only counting votes, I also want \nto know who voted for each type of bread. Here, I do this by associat-\ning a list of names with each key:\nvotes = {\n    'baguette': ['Bob', 'Alice'],\n    'ciabatta': ['Coco', 'Deb'],\n}\nkey = 'brioche'\nwho = 'Elmer'\nif key in votes:\n    names = votes[key]\nelse:\n    votes[key] = names = []\nnames.append(who)\nprint(votes)\n>>>\n{'baguette': ['Bob', 'Alice'],\n 'ciabatta': ['Coco', 'Deb'],\n 'brioche': ['Elmer']}\nRelying on the in expression requires two accesses if the key is pres-\nent, or one access and one assignment if the key is missing. This \nexample is different from the counters example above because the \nvalue for each key can be assigned blindly to the default value of an \nempty list if the key doesn’t already exist. The triple assignment \nstatement (votes[key] = names = []) populates the key in one line \ninstead of two. Once the default value has been inserted into the dic-\ntionary, I don’t need to assign it again because the list is modified by \nreference in the later call to append.\n",
      "content_length": 1598,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "68 \nChapter 2 Lists and Dictionaries\nIt’s also possible to rely on the KeyError exception being raised when \nthe dictionary value is a list. This approach requires one key access \nif the key is present, or one key access and one assignment if it’s \nmissing, which makes it more efficient than the in condition:\ntry:\n    names = votes[key]\nexcept KeyError:\n    votes[key] = names = []\nnames.append(who)\nSimilarly, you can use the get method to fetch a list value when the \nkey is present, or do one fetch and one assignment if the key isn’t \npresent:\nnames = votes.get(key)\nif names is None:\n    votes[key] = names = []\nnames.append(who)\nThe approach that involves using get to fetch list values can \n further be shortened by one line if you use an assignment expres-\nsion ( introduced in Python 3.8; see Item 10: “Prevent Repetition \nwith Assignment Expressions”) in the if statement, which improves \nreadability:\nif (names := votes.get(key)) is None:\n    votes[key] = names = []\nnames.append(who)\nThe dict type also provides the setdefault method to help shorten \nthis pattern even further. setdefault tries to fetch the value of a key \nin the dictionary. If the key isn’t present, the method assigns that key \nto the default value provided. And then the method returns the value \nfor that key: either the originally present value or the newly inserted \ndefault value. Here, I use setdefault to implement the same logic as in \nthe get example above:\nnames = votes.setdefault(key, [])\nnames.append(who)\nThis works as expected, and it is shorter than using get with an \nassignment expression. However, the readability of this approach \nisn’t ideal. The method name setdefault doesn’t make its purpose \n",
      "content_length": 1701,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": " Item 16: Prefer get Over in and KeyError to Handle Missing Dictionary Keys 69\nimmediately obvious. Why is it set when what it’s doing is getting \na value? Why not call it get_or_set? I’m arguing about the color of \nthe bike shed here, but the point is that if you were a new reader of \nthe code and not completely familiar with Python, you might have \ntrouble understanding what this code is trying to accomplish because \nsetdefault isn’t self-explanatory.\nThere’s also one important gotcha: The default value passed to \nsetdefault is assigned directly into the dictionary when the key is \nmissing instead of being copied. Here, I demonstrate the effect of this \nwhen the value is a list:\ndata = {}\nkey = 'foo'\nvalue = []\ndata.setdefault(key, value)\nprint('Before:', data)\nvalue.append('hello')\nprint('After: ', data)\n>>>\nBefore: {'foo': []}\nAfter:  {'foo': ['hello']}\nThis means that I need to make sure that I’m always construct-\ning a new default value for each key I access with setdefault. This \nleads to a significant performance overhead in this example because \nI have to allocate a list instance for each call. If I reuse an object \nfor the default value—which I might try to do to increase efficiency \nor  readability—I might introduce strange behavior and bugs (see \nItem 24: “Use None and Docstrings to Specify Dynamic Default \n Arguments” for another example of this problem).\nGoing back to the earlier example that used counters for dictionary \nvalues instead of lists of who voted: Why not also use the setdefault \nmethod in that case? Here, I reimplement the same example using \nthis approach:\ncount = counters.setdefault(key, 0)\ncounters[key] = count + 1\nThe problem here is that the call to setdefault is superfluous. You \nalways need to assign the key in the dictionary to a new value \nafter you increment the counter, so the extra assignment done by \nsetdefault is unnecessary. The earlier approach of using get for \ncounter updates requires only one access and one assignment, \nwhereas using setdefault requires one access and two assignments.\n",
      "content_length": 2066,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "70 \nChapter 2 Lists and Dictionaries\nThere are only a few circumstances in which using setdefault is the \nshortest way to handle missing dictionary keys, such as when the \ndefault values are cheap to construct, mutable, and there’s no poten-\ntial for raising exceptions (e.g., list instances). In these very spe-\ncific cases, it may seem worth accepting the confusing method name \nsetdefault instead of having to write more characters and lines to \nuse get. However, often what you really should do in these situations \nis to use defaultdict instead (see Item 17: “Prefer defaultdict Over \nsetdefault to Handle Missing Items in Internal State”).\nThings to Remember\n✦ There are four common ways to detect and handle missing keys \nin dictionaries: using in expressions, KeyError exceptions, the get \nmethod, and the setdefault method.\n✦ The get method is best for dictionaries that contain basic types \nlike counters, and it is preferable along with assignment expres-\nsions when creating dictionary values has a high cost or may raise \nexceptions.\n✦ When the setdefault method of dict seems like the best fit for your \nproblem, you should consider using defaultdict instead.\nItem 17:  Prefer defaultdict Over setdefault to \nHandle Missing Items in Internal State\nWhen working with a dictionary that you didn’t create, there are a \nvariety of ways to handle missing keys (see Item 16: “Prefer get Over \nin and KeyError to Handle Missing Dictionary Keys”). Although using \nthe get method is a better approach than using in expressions and \nKeyError exceptions, for some use cases setdefault appears to be the \nshortest option.\nFor example, say that I want to keep track of the cities I’ve visited in \ncountries around the world. Here, I do this by using a dictionary that \nmaps country names to a set instance containing corresponding city \nnames:\nvisits = {\n    'Mexico': {'Tulum', 'Puerto Vallarta'},\n    'Japan': {'Hakone'},\n}\nI can use the setdefault method to add new cities to the sets, whether \nthe country name is already present in the dictionary or not. This \napproach is much shorter than achieving the same behavior with the \n",
      "content_length": 2135,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": " \nItem 17: Prefer defaultdict Over setdefault \n71\nget method and an assignment expression (which is available as of \nPython 3.8):\nvisits.setdefault('France', set()).add('Arles')  # Short\nif (japan := visits.get('Japan')) is None:       # Long\n    visits['Japan'] = japan = set()\njapan.add('Kyoto')\nprint(visits)\n>>>\n{'Mexico': {'Tulum', 'Puerto Vallarta'},\n 'Japan': {'Kyoto', 'Hakone'},\n 'France': {'Arles'}}\nWhat about the situation when you do control creation of the dictio-\nnary being accessed? This is generally the case when you’re using a \ndictionary instance to keep track of the internal state of a class, for \nexample. Here, I wrap the example above in a class with helper meth-\nods to access the dynamic inner state stored in a dictionary:\nclass Visits:\n    def __init__(self):\n        self.data = {}\n    def add(self, country, city):\n        city_set = self.data.setdefault(country, set())\n        city_set.add(city)\nThis new class hides the complexity of calling setdefault correctly, \nand it provides a nicer interface for the programmer:\nvisits = Visits()\nvisits.add('Russia', 'Yekaterinburg')\nvisits.add('Tanzania', 'Zanzibar')\nprint(visits.data)\n>>>\n{'Russia': {'Yekaterinburg'}, 'Tanzania': {'Zanzibar'}}\nHowever, the implementation of the Visits.add method still isn’t ideal. \nThe setdefault method is still confusingly named, which makes it \nmore difficult for a new reader of the code to immediately understand \nwhat’s happening. And the implementation isn’t efficient because it \nconstructs a new set instance on every call, regardless of whether the \ngiven country was already present in the data dictionary.\n",
      "content_length": 1633,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "72 \nChapter 2 Lists and Dictionaries\nLuckily, the defaultdict class from the collections built-in module \nsimplifies this common use case by automatically storing a default \nvalue when a key doesn’t exist. All you have to do is provide a  function \nthat will return the default value to use each time a key is missing \n(an example of Item 38: “Accept Functions Instead of Classes for Sim-\nple Interfaces”). Here, I rewrite the Visits class to use defaultdict:\nfrom collections import defaultdict\nclass Visits:\n    def __init__(self):\n        self.data = defaultdict(set)\n    def add(self, country, city):\n        self.data[country].add(city)\nvisits = Visits()\nvisits.add('England', 'Bath')\nvisits.add('England', 'London')\nprint(visits.data)\n>>>\ndefaultdict(<class 'set'>, {'England': {'London', 'Bath'}})\nNow, the implementation of add is short and simple. The code can \nassume that accessing any key in the data dictionary will always \nresult in an existing set instance. No superfluous set instances will \nbe allocated, which could be costly if the add method is called a large \nnumber of times.\nUsing defaultdict is much better than using setdefault for this type \nof situation (see Item 37: “Compose Classes Instead of Nesting Many \nLevels of Built-in Types” for another example). There are still cases in \nwhich defaultdict will fall short of solving your problems, but there \nare even more tools available in Python to work around those limita-\ntions (see Item 18: “Know How to Construct Key-Dependent Default \nValues with __missing__,” Item 43: “Inherit from collections.abc for \nCustom Container Types,” and the collections.Counter built-in class).\nThings to Remember\n✦ If you’re creating a dictionary to manage an arbitrary set of poten-\ntial keys, then you should prefer using a defaultdict instance from \nthe collections built-in module if it suits your problem. \n✦ If a dictionary of arbitrary keys is passed to you, and you don’t con-\ntrol its creation, then you should prefer the get method to access its \nitems. However, it’s worth considering using the setdefault method \nfor the few situations in which it leads to shorter code.\n",
      "content_length": 2146,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": " \nItem 18: Know How to Construct Key-Dependent Default Values \n73\nItem 18:  Know How to Construct Key-Dependent \nDefault Values with __missing__\nThe built-in dict type’s setdefault method results in shorter code \nwhen handling missing keys in some specific circumstances (see Item \n16: “Prefer get Over in and KeyError to Handle Missing Dictionary \nKeys” for examples). For many of those situations, the better tool for \nthe job is the defaultdict type from the collections built-in module \n(see Item 17: “Prefer defaultdict Over setdefault to Handle Missing \nItems in Internal State” for why). However, there are times when nei-\nther setdefault nor defaultdict is the right fit.\nFor example, say that I’m writing a program to manage social network \nprofile pictures on the filesystem. I need a dictionary to map profile \npicture pathnames to open file handles so I can read and write those \nimages as needed. Here, I do this by using a normal dict instance \nand checking for the presence of keys using the get method and an \nassignment expression (introduced in Python 3.8; see Item 10: “Pre-\nvent Repetition with Assignment Expressions”):\npictures = {}\npath = 'profile_1234.png'\nif (handle := pictures.get(path)) is None:\n    try:\n        handle = open(path, 'a+b')\n    except OSError:\n        print(f'Failed to open path {path}')\n        raise\n    else:\n        pictures[path] = handle\nhandle.seek(0)\nimage_data = handle.read()\nWhen the file handle already exists in the dictionary, this code makes \nonly a single dictionary access. In the case that the file handle doesn’t \nexist, the dictionary is accessed once by get, and then it is assigned \nin the else clause of the try/except block. (This approach also \nworks with finally; see Item 65: “Take Advantage of Each Block in \ntry/except/else/finally.”) The call to the read method stands clearly \nseparate from the code that calls open and handles exceptions.\nAlthough it’s possible to use the in expression or KeyError approaches \nto implement this same logic, those options require more dictionary \naccesses and levels of nesting. Given that these other options work, \nyou might also assume that the setdefault method would work, too:\n",
      "content_length": 2193,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "74 \nChapter 2 Lists and Dictionaries\ntry:\n    handle = pictures.setdefault(path, open(path, 'a+b'))\nexcept OSError:\n    print(f'Failed to open path {path}')\n    raise\nelse:\n    handle.seek(0)\n    image_data = handle.read()\nThis code has many problems. The open built-in function to create \nthe file handle is always called, even when the path is already pres-\nent in the dictionary. This results in an additional file handle that \nmay conflict with existing open handles in the same program. Excep-\ntions may be raised by the open call and need to be handled, but it \nmay not be possible to differentiate them from exceptions that may \nbe raised by the setdefault call on the same line (which is possible \nfor other  dictionary-like implementations; see Item 43: “Inherit from \ncollections.abc for Custom Container Types”).\nIf you’re trying to manage internal state, another assumption you \nmight make is that a defaultdict could be used for keeping track of \nthese profile pictures. Here, I attempt to implement the same logic as \nbefore but now using a helper function and the defaultdict class:\nfrom collections import defaultdict\ndef open_picture(profile_path):\n    try:\n        return open(profile_path, 'a+b')\n    except OSError:\n        print(f'Failed to open path {profile_path}')\n        raise\npictures = defaultdict(open_picture)\nhandle = pictures[path]\nhandle.seek(0)\nimage_data = handle.read()\n>>>\nTraceback ...\nTypeError: open_picture() missing 1 required positional \nargument: 'profile_path'\nThe problem is that defaultdict expects that the function passed to \nits constructor doesn’t require any arguments. This means that the \nhelper function that defaultdict calls doesn’t know which specific key \n",
      "content_length": 1715,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": " \nItem 18: Know How to Construct Key-Dependent Default Values \n75\nis being accessed, which eliminates my ability to call open. In this \nsituation, both setdefault and defaultdict fall short of what I need.\nFortunately, this situation is common enough that Python has \nanother built-in solution. You can subclass the dict type and imple-\nment the __missing__ special method to add custom logic for han-\ndling missing keys. Here, I do this by defining a new class that takes \nadvantage of the same open_picture helper method defined above:\nclass Pictures(dict):\n    def __missing__(self, key):\n        value = open_picture(key)\n        self[key] = value\n        return value\npictures = Pictures()\nhandle = pictures[path]\nhandle.seek(0)\nimage_data = handle.read()\nWhen the pictures[path] dictionary access finds that the path key \nisn’t present in the dictionary, the __missing__ method is called. This \nmethod must create the new default value for the key, insert it into \nthe dictionary, and return it to the caller. Subsequent accesses of \nthe same path will not call __missing__ since the corresponding item \nis already present (similar to the behavior of __getattr__; see Item \n47: “Use __getattr__, __getattribute__, and __setattr__ for Lazy \nAttributes”).\nThings to Remember\n✦ The setdefault method of dict is a bad fit when creating the default \nvalue has high computational cost or may raise exceptions.\n✦ The function passed to defaultdict must not require any argu-\nments, which makes it impossible to have the default value depend \non the key being accessed.\n✦ You can define your own dict subclass with a __missing__ method \nin order to construct default values that must know which key was \nbeing accessed.\n",
      "content_length": 1718,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "3\nFunctions\nThe first organizational tool programmers use in Python is the \n function. As in other programming languages, functions enable you \nto break large programs into smaller, simpler pieces with names to \nrepresent their intent. They improve readability and make code more \napproachable. They allow for reuse and refactoring.\nFunctions in Python have a variety of extra features that make a \nprogrammer’s life easier. Some are similar to capabilities in other \nprogramming languages, but many are unique to Python. These \nextras can make a function’s purpose more obvious. They can elimi-\nnate noise and clarify the intention of callers. They can significantly \nreduce subtle bugs that are difficult to find.\nItem 19:  Never Unpack More Than Three Variables \nWhen Functions Return Multiple Values\nOne effect of the unpacking syntax (see Item 6: “Prefer Multiple \nAssignment Unpacking Over Indexing”) is that it allows Python func-\ntions to seemingly return more than one value. For example, say \nthat I’m trying to determine various statistics for a population of \nalligators. Given a list of lengths, I need to calculate the minimum \nand  maximum lengths in the population. Here, I do this in a single \n function that appears to return two values:\ndef get_stats(numbers):\n    minimum = min(numbers)\n    maximum = max(numbers)\n    return minimum, maximum\n \nlengths = [63, 73, 72, 60, 67, 66, 71, 61, 72, 70]\n \nminimum, maximum = get_stats(lengths)  # Two return values\n \nprint(f'Min: {minimum}, Max: {maximum}')\n",
      "content_length": 1519,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "78 \nChapter 3 Functions\n>>>\nMin: 60, Max: 73\nThe way this works is that multiple values are returned together in a \ntwo-item tuple. The calling code then unpacks the returned tuple by \nassigning two variables. Here, I use an even simpler example to show \nhow an unpacking statement and multiple-return function work the \nsame way:\nfirst, second = 1, 2\nassert first == 1\nassert second == 2\n \ndef my_function():\n    return 1, 2\n \nfirst, second = my_function()\nassert first == 1\nassert second == 2\nMultiple return values can also be received by starred expressions for \ncatch-all unpacking (see Item 13: “Prefer Catch-All Unpacking Over \nSlicing”). For example, say I need another function that calculates \nhow big each alligator is relative to the population average. This func-\ntion returns a list of ratios, but I can receive the longest and shortest \nitems individually by using a starred expression for the middle por-\ntion of the list:\ndef get_avg_ratio(numbers):\n    average = sum(numbers) / len(numbers)\n    scaled = [x / average for x in numbers]\n    scaled.sort(reverse=True)\n    return scaled\n \nlongest, *middle, shortest = get_avg_ratio(lengths)\n \nprint(f'Longest:  {longest:>4.0%}')\nprint(f'Shortest: {shortest:>4.0%}')\n>>>\nLongest:  108%\nShortest:  89%\nNow, imagine that the program’s requirements change, and I need to \nalso determine the average length, median length, and total popula-\ntion size of the alligators. I can do this by expanding the get_stats \n",
      "content_length": 1471,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": " \nItem 19: Never Unpack More Than Three Return Values \n79\nfunction to also calculate these statistics and return them in the \nresult tuple that is unpacked by the caller:\ndef get_stats(numbers):\n    minimum = min(numbers)\n    maximum = max(numbers)\n    count = len(numbers)\n    average = sum(numbers) / count\n \n    sorted_numbers = sorted(numbers)\n    middle = count // 2\n    if count % 2 == 0:\n        lower = sorted_numbers[middle - 1]\n        upper = sorted_numbers[middle]\n        median = (lower + upper) / 2\n    else:\n        median = sorted_numbers[middle]\n \n    return minimum, maximum, average, median, count\n \nminimum, maximum, average, median, count = get_stats(lengths)\n \nprint(f'Min: {minimum}, Max: {maximum}')\nprint(f'Average: {average}, Median: {median}, Count {count}')\n>>>\nMin: 60, Max: 73\nAverage: 67.5, Median: 68.5, Count 10\nThere are two problems with this code. First, all the return values \nare numeric, so it is all too easy to reorder them accidentally (e.g., \nswapping average and median), which can cause bugs that are hard \nto spot later. Using a large number of return values is extremely error \nprone:\n# Correct:\nminimum, maximum, average, median, count = get_stats(lengths)\n \n# Oops! Median and average swapped:\nminimum, maximum, median, average, count = get_stats(lengths)\nSecond, the line that calls the function and unpacks the values is \nlong, and it likely will need to be wrapped in one of a variety of ways \n(due to PEP8 style; see Item 2: “Follow the PEP 8 Style Guide”), which \nhurts readability:\nminimum, maximum, average, median, count = get_stats(\n    lengths)\n \n",
      "content_length": 1607,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "80 \nChapter 3 Functions\nminimum, maximum, average, median, count = \\\n    get_stats(lengths)\n \n(minimum, maximum, average,\n median, count) = get_stats(lengths)\n \n(minimum, maximum, average, median, count\n    ) = get_stats(lengths)\nTo avoid these problems, you should never use more than three vari-\nables when unpacking the multiple return values from a function. \nThese could be individual values from a three-tuple, two variables \nand one catch-all starred expression, or anything shorter. If you \nneed to unpack more return values than that, you’re better off defin-\ning a lightweight class or namedtuple (see Item 37: “Compose Classes \nInstead of Nesting Many Levels of Built-in Types”) and having your \nfunction return an instance of that instead.\nThings to Remember\n✦ You can have functions return multiple values by putting them in a \ntuple and having the caller take advantage of Python’s unpacking \nsyntax.\n✦ Multiple return values from a function can also be unpacked by \ncatch-all starred expressions.\n✦ Unpacking into four or more variables is error prone and should be \navoided; instead, return a small class or namedtuple instance.\nItem 20: Prefer Raising Exceptions to Returning None\nWhen writing utility functions, there’s a draw for Python program-\nmers to give special meaning to the return value of None. It seems to \nmake sense in some cases. For example, say I want a helper function \nthat divides one number by another. In the case of dividing by zero, \nreturning None seems natural because the result is undefined:\ndef careful_divide(a, b):\n    try:\n        return a / b\n    except ZeroDivisionError:\n        return None\nCode using this function can interpret the return value accordingly:\nx, y = 1, 0\nresult = careful_divide(x, y)\nif result is None:\n    print('Invalid inputs')\n",
      "content_length": 1801,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": " \nItem 20: Prefer Raising Exceptions to Returning None \n81\nWhat happens with the careful_divide function when the numerator \nis zero? If the denominator is not zero, the function returns zero. The \nproblem is that a zero return value can cause issues when you evalu-\nate the result in a condition like an if statement. You might acciden-\ntally look for any False-equivalent value to indicate errors instead of \nonly looking for None (see Item 5: “Write Helper Functions Instead of \nComplex Expressions” for a similar situation):\nx, y = 0, 5\nresult = careful_divide(x, y)\nif not result:\n    print('Invalid inputs')  # This runs! But shouldn't\n>>>\nInvalid inputs\nThis misinterpretation of a False-equivalent return value is a common \nmistake in Python code when None has special meaning. This is why \nreturning None from a function like careful_divide is error prone. \nThere are two ways to reduce the chance of such errors.\nThe first way is to split the return value into a two-tuple (see Item 19: \n“Never Unpack More Than Three Variables When Functions Return \nMultiple Values” for background). The first part of the tuple indicates \nthat the operation was a success or failure. The second part is the \nactual result that was computed:\ndef careful_divide(a, b):\n    try:\n        return True, a / b\n    except ZeroDivisionError:\n        return False, None\nCallers of this function have to unpack the tuple. That forces them \nto consider the status part of the tuple instead of just looking at the \nresult of division:\nsuccess, result = careful_divide(x, y)\nif not success:\n    print('Invalid inputs')\nThe problem is that callers can easily ignore the first part of the tuple \n(using the underscore variable name, a Python convention for unused \nvariables). The resulting code doesn’t look wrong at first glance, but \nthis can be just as error prone as returning None:\n_, result = careful_divide(x, y)\nif not result:\n    print('Invalid inputs')\n",
      "content_length": 1943,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "82 \nChapter 3 Functions\nThe second, better way to reduce these errors is to never return \nNone for special cases. Instead, raise an Exception up to the caller \nand have the caller deal with it. Here, I turn a ZeroDivisionError into \na ValueError to indicate to the caller that the input values are bad \n(see Item 87: “Define a Root Exception to Insulate Callers from APIs” \non when you should use Exception subclasses):\ndef careful_divide(a, b):\n    try:\n        return a / b\n    except ZeroDivisionError as e:\n        raise ValueError('Invalid inputs')\nThe caller no longer requires a condition on the return value of the \nfunction. Instead, it can assume that the return value is always \nvalid and use the results immediately in the else block after try \n(see Item 65: “Take Advantage of Each Block in try/except/else/\nfinally” for details):\nx, y = 5, 2\ntry:\n    result = careful_divide(x, y)\nexcept ValueError:\n    print('Invalid inputs')\nelse:\n    print('Result is %.1f' % result)\n>>>\nResult is 2.5\nThis approach can be extended to code using type annotations \n(see Item 90: “Consider Static Analysis via typing to Obviate Bugs” \nfor background). You can specify that a function’s return value will \nalways be a float and thus will never be None. However, Python’s \ngradual typing purposefully doesn’t provide a way to indicate when \nexceptions are part of a function’s interface (also known as checked \nexceptions). Instead, you have to document the exception-raising \nbehavior and expect callers to rely on that in order to know which \nExceptions they should plan to catch (see Item 84: “Write Docstrings \nfor Every Function, Class, and Module”).\nPulling it all together, here’s what this function should look like when \nusing type annotations and docstrings:\n",
      "content_length": 1766,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": " \nItem 21: Know How Closures Interact with Variable Scope \n83\ndef careful_divide(a: float, b: float) -> float:\n    \"\"\"Divides a by b.\n \n    Raises:\n        ValueError: When the inputs cannot be divided.\n    \"\"\"\n    try:\n        return a / b\n    except ZeroDivisionError as e:\n        raise ValueError('Invalid inputs')\nNow the inputs, outputs, and exceptional behavior is clear, and the \nchance of a caller doing the wrong thing is extremely low.\nThings to Remember\n✦ Functions that return None to indicate special meaning are error \nprone because None and other values (e.g., zero, the empty string) \nall evaluate to False in conditional expressions.\n✦ Raise exceptions to indicate special situations instead of returning \nNone. Expect the calling code to handle exceptions properly when \nthey’re documented.\n✦ Type annotations can be used to make it clear that a function will \nnever return the value None, even in special situations.\nItem 21:  Know How Closures Interact with \nVariable Scope\nSay that I want to sort a list of numbers but prioritize one group of \nnumbers to come first. This pattern is useful when you’re rendering a \nuser interface and want important messages or exceptional events to \nbe displayed before everything else.\nA common way to do this is to pass a helper function as the key argu-\nment to a list’s sort method (see Item 14: “Sort by Complex Criteria \nUsing the key Parameter” for details). The helper’s return value will \nbe used as the value for sorting each item in the list. The helper can \ncheck whether the given item is in the important group and can vary \nthe sorting value accordingly:\ndef sort_priority(values, group):\n    def helper(x):\n        if x in group:\n            return (0, x)\n        return (1, x)\n    values.sort(key=helper)\n",
      "content_length": 1778,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "84 \nChapter 3 Functions\nThis function works for simple inputs:\nnumbers = [8, 3, 1, 2, 5, 4, 7, 6]\ngroup = {2, 3, 5, 7}\nsort_priority(numbers, group)\nprint(numbers)\n>>>\n[2, 3, 5, 7, 1, 4, 6, 8]\nThere are three reasons this function operates as expected:\n \n■Python supports closures—that is, functions that refer to variables \nfrom the scope in which they were defined. This is why the helper \nfunction is able to access the group argument for sort_priority.\n \n■Functions are first-class objects in Python, which means you can \nrefer to them directly, assign them to variables, pass them as \narguments to other functions, compare them in expressions and \nif statements, and so on. This is how the sort method can accept \na closure function as the key argument.\n \n■Python has specific rules for comparing sequences (including \ntuples). It first compares items at index zero; then, if those are \nequal, it compares items at index one; if they are still equal, it \ncompares items at index two, and so on. This is why the return \nvalue from the helper closure causes the sort order to have two \ndistinct groups.\nIt’d be nice if this function returned whether higher-priority items \nwere seen at all so the user interface code can act accordingly. Add-\ning such behavior seems straightforward. There’s already a closure \nfunction for deciding which group each number is in. Why not also \nuse the closure to flip a flag when high-priority items are seen? Then, \nthe function can return the flag value after it’s been modified by the \nclosure.\nHere, I try to do that in a seemingly obvious way:\ndef sort_priority2(numbers, group):\n    found = False\n    def helper(x):\n        if x in group:\n            found = True  # Seems simple\n            return (0, x)\n        return (1, x)\n    numbers.sort(key=helper)\n    return found\n",
      "content_length": 1817,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": " \nItem 21: Know How Closures Interact with Variable Scope \n85\nI can run the function on the same inputs as before:\nfound = sort_priority2(numbers, group)\nprint('Found:', found)\nprint(numbers)\n>>>\nFound: False\n[2, 3, 5, 7, 1, 4, 6, 8]\nThe sorted results are correct, which means items from group were \ndefinitely found in numbers. Yet the found result returned by the func-\ntion is False when it should be True. How could this happen?\nWhen you reference a variable in an expression, the Python  interpreter \ntraverses the scope to resolve the reference in this order:\n 1. The current function’s scope.\n 2. Any enclosing scopes (such as other containing functions).\n 3. The scope of the module that contains the code (also called the \nglobal scope).\n 4. The built-in scope (that contains functions like len and str).\nIf none of these places has defined a variable with the referenced \nname, then a NameError exception is raised:\nfoo = does_not_exist * 5\n>>>\nTraceback ...\nNameError: name 'does_not_exist' is not defined\nAssigning a value to a variable works differently. If the variable is \nalready defined in the current scope, it will just take on the new \nvalue. If the variable doesn’t exist in the current scope, Python treats \nthe assignment as a variable definition. Critically, the scope of the \nnewly defined variable is the function that contains the assignment.\nThis assignment behavior explains the wrong return value of the \nsort_priority2 function. The found variable is assigned to True in the \nhelper closure. The closure’s assignment is treated as a new variable \ndefinition within helper, not as an assignment within sort_priority2:\ndef sort_priority2(numbers, group):\n    found = False         # Scope: 'sort_priority2'\n    def helper(x):\n        if x in group:\n            found = True  # Scope: 'helper' -- Bad!\n            return (0, x)\n        return (1, x)\n",
      "content_length": 1879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "86 \nChapter 3 Functions\n    numbers.sort(key=helper)\n    return found\nThis problem is sometimes called the scoping bug because it can be \nso surprising to newbies. But this behavior is the intended result: It \nprevents local variables in a function from polluting the containing \nmodule. Otherwise, every assignment within a function would put \ngarbage into the global module scope. Not only would that be noise, \nbut the interplay of the resulting global variables could cause obscure \nbugs.\nIn Python, there is special syntax for getting data out of a closure. \nThe nonlocal statement is used to indicate that scope traversal should \nhappen upon assignment for a specific variable name. The only limit \nis that nonlocal won’t traverse up to the module-level scope (to avoid \npolluting globals).\nHere, I define the same function again, now using nonlocal:\ndef sort_priority3(numbers, group):\n    found = False\n    def helper(x):\n        nonlocal found  # Added\n        if x in group:\n            found = True\n            return (0, x)\n        return (1, x)\n    numbers.sort(key=helper)\n    return found\nThe nonlocal statement makes it clear when data is being assigned \nout of a closure and into another scope. It’s complementary to the \nglobal statement, which indicates that a variable’s assignment should \ngo directly into the module scope.\nHowever, much as with the anti-pattern of global variables, I’d cau-\ntion against using nonlocal for anything beyond simple functions. \nThe side effects of nonlocal can be hard to follow. It’s especially hard \nto understand in long functions where the nonlocal statements and \nassignments to associated variables are far apart.\nWhen your usage of nonlocal starts getting complicated, it’s better to \nwrap your state in a helper class. Here, I define a class that achieves \nthe same result as the nonlocal approach; it’s a little longer but much \neasier to read (see Item 38: “Accept Functions Instead of Classes for \nSimple Interfaces” for details on the __call__ special method):\nclass Sorter:\n    def __init__(self, group):\n",
      "content_length": 2071,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": " \nItem 22: Reduce Visual Noise with Variable Positional Arguments \n87\n        self.group = group\n        self.found = False\n \n    def __call__(self, x):\n        if x in self.group:\n            self.found = True\n            return (0, x)\n        return (1, x)\n \nsorter = Sorter(group)\nnumbers.sort(key=sorter)\nassert sorter.found is True\nThings to Remember\n✦ Closure functions can refer to variables from any of the scopes in \nwhich they were defined.\n✦ By default, closures can’t affect enclosing scopes by assigning \nvariables.\n✦ Use the nonlocal statement to indicate when a closure can modify a \nvariable in its enclosing scopes.\n✦ Avoid using nonlocal statements for anything beyond simple \nfunctions.\nItem 22:  Reduce Visual Noise with Variable Positional \nArguments\nAccepting a variable number of positional arguments can make a \nfunction call clearer and reduce visual noise. (These positional argu-\nments are often called varargs for short, or star args, in reference to \nthe conventional name for the parameter *args.) For example, say \nthat I want to log some debugging information. With a fixed number \nof arguments, I would need a function that takes a message and a \nlist of values:\ndef log(message, values):\n    if not values:\n        print(message)\n    else:\n        values_str = ', '.join(str(x) for x in values)\n        print(f'{message}: {values_str}')\n \nlog('My numbers are', [1, 2])\nlog('Hi there', [])\n",
      "content_length": 1423,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "88 \nChapter 3 Functions\n>>>\nMy numbers are: 1, 2\nHi there\nHaving to pass an empty list when I have no values to log is cum-\nbersome and noisy. It’d be better to leave out the second argument \nentirely. I can do this in Python by prefixing the last positional \nparameter name with *. The first parameter for the log message is \nrequired, whereas any number of subsequent positional arguments \nare optional. The function body doesn’t need to change; only the call-\ners do:\ndef log(message, *values):  # The only difference\n    if not values:\n        print(message)\n    else:\n        values_str = ', '.join(str(x) for x in values)\n        print(f'{message}: {values_str}')\n \nlog('My numbers are', 1, 2)\nlog('Hi there')  # Much better\n>>>\nMy numbers are: 1, 2\nHi there\nYou might notice that this syntax works very similarly to the starred \nexpressions used in unpacking assignment statements (see Item 13: \n“Prefer Catch-All Unpacking Over Slicing”).\nIf I already have a sequence (like a list) and want to call a variadic \nfunction like log, I can do this by using the * operator. This instructs \nPython to pass items from the sequence as positional arguments to \nthe function:\nfavorites = [7, 33, 99]\nlog('Favorite colors', *favorites)\n>>>\nFavorite colors: 7, 33, 99\nThere are two problems with accepting a variable number of posi-\ntional arguments.\nThe first issue is that these optional positional arguments are always \nturned into a tuple before they are passed to a function. This means \nthat if the caller of a function uses the * operator on a generator, it \nwill be iterated until it’s exhausted (see Item 30: “Consider Genera-\ntors Instead of Returning Lists” for background). The resulting tuple \n",
      "content_length": 1703,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": " \nItem 22: Reduce Visual Noise with Variable Positional Arguments \n89\nincludes every value from the generator, which could consume a lot of \nmemory and cause the program to crash:\ndef my_generator():\n    for i in range(10):\n        yield i\n \ndef my_func(*args):\n    print(args)\n \nit = my_generator()\nmy_func(*it)\n>>>\n(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\nFunctions that accept *args are best for situations where you know \nthe number of inputs in the argument list will be reasonably small. \n*args is ideal for function calls that pass many literals or variable \nnames together. It’s primarily for the convenience of the programmer \nand the readability of the code.\nThe second issue with *args is that you can’t add new positional \narguments to a function in the future without migrating every caller. \nIf you try to add a positional argument in the front of the argument \nlist, existing callers will subtly break if they aren’t updated:\ndef log(sequence, message, *values):\n    if not values:\n        print(f'{sequence} - {message}')\n    else:\n        values_str = ', '.join(str(x) for x in values)\n        print(f'{sequence} - {message}: {values_str}')\n \nlog(1, 'Favorites', 7, 33)      # New with *args OK\nlog(1, 'Hi there')              # New message only OK\nlog('Favorite numbers', 7, 33)  # Old usage breaks\n>>>\n1 - Favorites: 7, 33\n1 - Hi there\nFavorite numbers - 7: 33\nThe problem here is that the third call to log used 7 as the message \nparameter because a sequence argument wasn’t given. Bugs like \nthis are hard to track down because the code still runs without \nraising exceptions. To avoid this possibility entirely, you should use \n keyword-only arguments when you want to extend functions that \n",
      "content_length": 1704,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "90 \nChapter 3 Functions\naccept *args (see Item 25: “Enforce Clarity with Keyword-Only and \nPositional-Only Arguments”). To be even more defensive, you could \nalso consider using type annotations (see Item 90: “Consider Static \nAnalysis via typing to Obviate Bugs”).\nThings to Remember\n✦ Functions can accept a variable number of positional arguments by \nusing *args in the def statement.\n✦ You can use the items from a sequence as the positional arguments \nfor a function with the * operator.\n✦ Using the * operator with a generator may cause a program to run \nout of memory and crash.\n✦ Adding new positional parameters to functions that accept *args \ncan introduce hard-to-detect bugs.\nItem 23:  Provide Optional Behavior with \nKeyword Arguments\nAs in most other programming languages, in Python you may pass \narguments by position when calling a function:\ndef remainder(number, divisor):\n    return number % divisor\n \nassert remainder(20, 7) == 6\nAll normal arguments to Python functions can also be passed by \nkeyword, where the name of the argument is used in an assignment \nwithin the parentheses of a function call. The keyword arguments \ncan be passed in any order as long as all of the required positional \narguments are specified. You can mix and match keyword and posi-\ntional arguments. These calls are equivalent:\nremainder(20, 7)\nremainder(20, divisor=7)\nremainder(number=20, divisor=7)\nremainder(divisor=7, number=20)\nPositional arguments must be specified before keyword arguments:\nremainder(number=20, 7)\n>>>\nTraceback ...\nSyntaxError: positional argument follows keyword argument\n",
      "content_length": 1598,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": " \nItem 23: Provide Optional Behavior with Keyword Arguments \n91\nEach argument can be specified only once:\nremainder(20, number=7)\n>>>\nTraceback ...\nTypeError: remainder() got multiple values for argument \n¯'number'\nIf you already have a dictionary, and you want to use its contents to \ncall a function like remainder, you can do this by using the ** opera-\ntor. This instructs Python to pass the values from the dictionary as \nthe corresponding keyword arguments of the function:\nmy_kwargs = {\n    'number': 20,\n    'divisor': 7,\n}\nassert remainder(**my_kwargs) == 6\nYou can mix the ** operator with positional arguments or keyword \narguments in the function call, as long as no argument is repeated:\nmy_kwargs = {\n    'divisor': 7,\n}\nassert remainder(number=20, **my_kwargs) == 6\nYou can also use the ** operator multiple times if you know that the \ndictionaries don’t contain overlapping keys:\nmy_kwargs = {\n    'number': 20,\n}\nother_kwargs = {\n    'divisor': 7,\n}\nassert remainder(**my_kwargs, **other_kwargs) == 6\nAnd if you’d like for a function to receive any named keyword argu-\nment, you can use the **kwargs catch-all parameter to collect those \narguments into a dict that you can then process (see Item 26: “Define \nFunction Decorators with functools.wraps” for when this is especially \nuseful):\ndef print_parameters(**kwargs):\n    for key, value in kwargs.items():\n        print(f'{key} = {value}')\n \nprint_parameters(alpha=1.5, beta=9, gamma=4)\n",
      "content_length": 1457,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "92 \nChapter 3 Functions\n>>>\nalpha = 1.5\nbeta = 9\ngamma = 4\nThe flexibility of keyword arguments provides three significant \nbenefits.\nThe first benefit is that keyword arguments make the function call \nclearer to new readers of the code. With the call remainder(20, 7), it’s \nnot evident which argument is number and which is divisor unless \nyou look at the implementation of the remainder method. In the call \nwith keyword arguments, number=20 and divisor=7 make it immedi-\nately obvious which parameter is being used for each purpose.\nThe second benefit of keyword arguments is that they can have \ndefault values specified in the function definition. This allows a func-\ntion to provide additional capabilities when you need them, but you \ncan accept the default behavior most of the time. This eliminates \nrepetitive code and reduces noise.\nFor example, say that I want to compute the rate of fluid flowing into \na vat. If the vat is also on a scale, then I could use the difference \nbetween two weight measurements at two different times to deter-\nmine the flow rate:\ndef flow_rate(weight_diff, time_diff):\n    return weight_diff / time_diff\n \nweight_diff = 0.5\ntime_diff = 3\nflow = flow_rate(weight_diff, time_diff)\nprint(f'{flow:.3} kg per second')\n>>>\n0.167 kg per second\nIn the typical case, it’s useful to know the flow rate in kilograms per \nsecond. Other times, it’d be helpful to use the last sensor measure-\nments to approximate larger time scales, like hours or days. I can \nprovide this behavior in the same function by adding an argument for \nthe time period scaling factor:\ndef flow_rate(weight_diff, time_diff, period):\n    return (weight_diff / time_diff) * period\nThe problem is that now I need to specify the period argument every \ntime I call the function, even in the common case of flow rate per sec-\nond (where the period is 1):\nflow_per_second = flow_rate(weight_diff, time_diff, 1)\n",
      "content_length": 1909,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": " \nItem 23: Provide Optional Behavior with Keyword Arguments \n93\nTo make this less noisy, I can give the period argument a default \nvalue:\ndef flow_rate(weight_diff, time_diff, period=1):\n    return (weight_diff / time_diff) * period\nThe period argument is now optional:\nflow_per_second = flow_rate(weight_diff, time_diff)\nflow_per_hour = flow_rate(weight_diff, time_diff, period=3600)\nThis works well for simple default values; it gets tricky for complex \ndefault values (see Item 24: “Use None and Docstrings to Specify \nDynamic Default Arguments” for details).\nThe third reason to use keyword arguments is that they provide a \npowerful way to extend a function’s parameters while remaining \nbackward compatible with existing callers. This means you can pro-\nvide additional functionality without having to migrate a lot of exist-\ning code, which reduces the chance of introducing bugs.\nFor example, say that I want to extend the flow_rate function above \nto calculate flow rates in weight units besides kilograms. I can do this \nby adding a new optional parameter that provides a conversion rate to \nalternative measurement units:\ndef flow_rate(weight_diff, time_diff,\n              period=1, units_per_kg=1):\n    return ((weight_diff * units_per_kg) / time_diff) * period\nThe default argument value for units_per_kg is 1, which makes the \nreturned weight units remain kilograms. This means that all existing \ncallers will see no change in behavior. New callers to flow_rate can \nspecify the new keyword argument to see the new behavior:\npounds_per_hour = flow_rate(weight_diff, time_diff,\n                            period=3600, units_per_kg=2.2)\nProviding backward compatibility using optional keyword arguments \nlike this is also crucial for functions that accept *args (see Item 22: \n“Reduce Visual Noise with Variable Positional Arguments”).\nThe only problem with this approach is that optional keyword argu-\nments like period and units_per_kg may still be specified as posi-\ntional arguments:\npounds_per_hour = flow_rate(weight_diff, time_diff, 3600, 2.2)\nSupplying optional arguments positionally can be confusing because \nit isn’t clear what the values 3600 and 2.2 correspond to. The best \npractice is to always specify optional arguments using the keyword \n",
      "content_length": 2270,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "94 \nChapter 3 Functions\nnames and never pass them as positional arguments. As a function \nauthor, you can also require that all callers use this more explicit \nkeyword style to minimize potential errors (see Item 25: “Enforce \nClarity with Keyword-Only and Positional-Only Arguments”).\nThings to Remember\n✦ Function arguments can be specified by position or by keyword.\n✦ Keywords make it clear what the purpose of each argument is when \nit would be confusing with only positional arguments.\n✦ Keyword arguments with default values make it easy to add new \nbehaviors to a function without needing to migrate all existing \ncallers.\n✦ Optional keyword arguments should always be passed by keyword \ninstead of by position.\nItem 24:  Use None and Docstrings to Specify Dynamic \nDefault Arguments\nSometimes you need to use a non-static type as a keyword  argument’s \ndefault value. For example, say I want to print logging messages that \nare marked with the time of the logged event. In the default case, \nI want the message to include the time when the function was \ncalled. I might try the following approach, assuming that the default \n arguments are reevaluated each time the function is called:\nfrom time import sleep\nfrom datetime import datetime\n \ndef log(message, when=datetime.now()):\n    print(f'{when}: {message}')\n \nlog('Hi there!')\nsleep(0.1)\nlog('Hello again!')\n>>>\n2019-07-06 14:06:15.120124: Hi there!\n2019-07-06 14:06:15.120124: Hello again!\nThis doesn’t work as expected. The timestamps are the same because \ndatetime.now is executed only a single time: when the function is \ndefined. A default argument value is evaluated only once per module \n",
      "content_length": 1658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": " \nItem 24: Specify Dynamic Default Arguments in Docstrings \n95\nload, which usually happens when a program starts up. After the \nmodule containing this code is loaded, the datetime.now() default \nargument will never be evaluated again.\nThe convention for achieving the desired result in Python is to provide \na default value of None and to document the actual behavior in the \ndocstring (see Item 84: “Write Docstrings for Every Function, Class, \nand Module” for background). When your code sees the argument \nvalue None, you allocate the default value accordingly:\ndef log(message, when=None):\n    \"\"\"Log a message with a timestamp.\n \n    Args:\n        message: Message to print.\n        when: datetime of when the message occurred.\n            Defaults to the present time.\n    \"\"\"\n    if when is None:\n        when = datetime.now()\n    print(f'{when}: {message}')\nNow the timestamps will be different:\nlog('Hi there!')\nsleep(0.1)\nlog('Hello again!')\n>>>\n2019-07-06 14:06:15.222419: Hi there!\n2019-07-06 14:06:15.322555: Hello again!\nUsing None for default argument values is especially important when \nthe arguments are mutable. For example, say that I want to load a \nvalue encoded as JSON data; if decoding the data fails, I want an \nempty dictionary to be returned by default:\nimport json\n \ndef decode(data, default={}):\n    try:\n        return json.loads(data)\n    except ValueError:\n        return default\nThe problem here is the same as in the datetime.now example above. \nThe dictionary specified for default will be shared by all calls to \n",
      "content_length": 1550,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "96 \nChapter 3 Functions\ndecode because default argument values are evaluated only once (at \nmodule load time). This can cause extremely surprising behavior:\nfoo = decode('bad data')\nfoo['stuff'] = 5\nbar = decode('also bad')\nbar['meep'] = 1\nprint('Foo:', foo)\nprint('Bar:', bar)\n>>>\nFoo: {'stuff': 5, 'meep': 1}\nBar: {'stuff': 5, 'meep': 1}\nYou might expect two different dictionaries, each with a single key \nand value. But modifying one seems to also modify the other. The cul-\nprit is that foo and bar are both equal to the default parameter. They \nare the same dictionary object:\nassert foo is bar\nThe fix is to set the keyword argument default value to None and then \ndocument the behavior in the function’s docstring:\ndef decode(data, default=None):\n    \"\"\"Load JSON data from a string.\n \n    Args:\n        data: JSON data to decode.\n        default: Value to return if decoding fails.\n            Defaults to an empty dictionary.\n    \"\"\"\n    try:\n        return json.loads(data)\n    except ValueError:\n        if default is None:\n            default = {}\n        return default\nNow, running the same test code as before produces the expected \nresult:\nfoo = decode('bad data')\nfoo['stuff'] = 5\nbar = decode('also bad')\nbar['meep'] = 1\nprint('Foo:', foo)\nprint('Bar:', bar)\nassert foo is not bar\n",
      "content_length": 1300,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": " \nItem 25: Clarity with Keyword-Only and Positional-Only Arguments \n97\n>>>\nFoo: {'stuff': 5}\nBar: {'meep': 1}\nThis approach also works with type annotations (see Item 90: “Con-\nsider Static Analysis via typing to Obviate Bugs”). Here, the when \nargument is marked as having an Optional value that is a datetime. \nThus, the only two valid choices for when are None or a datetime object:\nfrom typing import Optional\n \ndef log_typed(message: str,\n              when: Optional[datetime]=None) -> None:\n    \"\"\"Log a message with a timestamp.\n \n    Args:\n        message: Message to print.\n        when: datetime of when the message occurred.\n            Defaults to the present time.\n    \"\"\"\n    if when is None:\n        when = datetime.now()\n    print(f'{when}: {message}')\nThings to Remember\n✦ A default argument value is evaluated only once: during function \ndefinition at module load time. This can cause odd behaviors for \ndynamic values (like {}, [], or datetime.now()).\n✦ Use None as the default value for any keyword argument that has a \ndynamic value. Document the actual default behavior in the func-\ntion’s docstring.\n✦ Using None to represent keyword argument default values also \nworks correctly with type annotations.\nItem 25:  Enforce Clarity with Keyword-Only and \nPositional-Only Arguments\nPassing arguments by keyword is a powerful feature of Python func-\ntions (see Item 23: “Provide Optional Behavior with Keyword Argu-\nments”). The flexibility of keyword arguments enables you to write \nfunctions that will be clear to new readers of your code for many use \ncases.\nFor example, say I want to divide one number by another but know \nthat I need to be very careful about special cases. Sometimes, I want \n",
      "content_length": 1718,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "98 \nChapter 3 Functions\nto ignore ZeroDivisionError exceptions and return infinity instead. \nOther times, I want to ignore OverflowError exceptions and return \nzero instead:\ndef safe_division(number, divisor,\n                  ignore_overflow,\n                  ignore_zero_division):\n    try:\n        return number / divisor\n    except OverflowError:\n        if ignore_overflow:\n            return 0\n        else:\n            raise\n    except ZeroDivisionError:\n        if ignore_zero_division:\n            return float('inf')\n        else:\n            raise\nUsing this function is straightforward. This call ignores the float \noverflow from division and returns zero:\nresult = safe_division(1.0, 10**500, True, False)\nprint(result)\n>>>\n0\nThis call ignores the error from dividing by zero and returns infinity:\nresult = safe_division(1.0, 0, False, True)\nprint(result)\n>>>\ninf\nThe problem is that it’s easy to confuse the position of the two Bool-\nean arguments that control the exception-ignoring behavior. This can \neasily cause bugs that are hard to track down. One way to improve the \nreadability of this code is to use keyword arguments. By default, the \nfunction can be overly cautious and can always re-raise exceptions:\ndef safe_division_b(number, divisor,\n                    ignore_overflow=False,        # Changed\n                    ignore_zero_division=False):  # Changed\n    ...\n",
      "content_length": 1394,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "Then, callers can use keyword arguments to specify which of the \nignore flags they want to set for specific operations, overriding the \ndefault behavior:\nresult = safe_division_b(1.0, 10**500, ignore_overflow=True)\nprint(result)\n \nresult = safe_division_b(1.0, 0, ignore_zero_division=True)\nprint(result)\n>>>\n0\ninf\nThe problem is, since these keyword arguments are optional behavior, \nthere’s nothing forcing callers to use keyword arguments for clarity. \nEven with the new definition of safe_division_b, you can still call it \nthe old way with positional arguments:\nassert safe_division_b(1.0, 10**500, True, False) == 0\nWith complex functions like this, it’s better to require that callers are \nclear about their intentions by defining functions with  keyword-only \narguments. These arguments can only be supplied by keyword, never \nby position.\nHere, I redefine the safe_division function to accept keyword-only \narguments. The * symbol in the argument list indicates the end \nof positional arguments and the beginning of keyword-only \narguments:\ndef safe_division_c(number, divisor, *,  # Changed\n                    ignore_overflow=False,\n                    ignore_zero_division=False):\n    ...\nNow, calling the function with positional arguments for the keyword \narguments won’t work:\nsafe_division_c(1.0, 10**500, True, False)\n>>>\nTraceback ...\nTypeError: safe_division_c() takes 2 positional arguments but 4 \n¯were given\nBut keyword arguments and their default values will work as expected \n(ignoring an exception in one case and raising it in another):\nresult = safe_division_c(1.0, 0, ignore_zero_division=True)\nassert result == float('inf')\n \n \nItem 25: Clarity with Keyword-Only and Positional-Only Arguments \n99\n",
      "content_length": 1726,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "100 \nChapter 3 Functions\ntry:\n    result = safe_division_c(1.0, 0)\nexcept ZeroDivisionError:\n    pass  # Expected\nHowever, a problem still remains with the safe_division_c version of \nthis function: Callers may specify the first two required arguments \n(number and divisor) with a mix of positions and keywords:\nassert safe_division_c(number=2, divisor=5) == 0.4\nassert safe_division_c(divisor=5, number=2) == 0.4\nassert safe_division_c(2, divisor=5) == 0.4\nLater, I may decide to change the names of these first two arguments \nbecause of expanding needs or even just because my style preferences \nchange:\ndef safe_division_c(numerator, denominator, *,  # Changed\n                    ignore_overflow=False,\n                    ignore_zero_division=False):\n    ...\nUnfortunately, this seemingly superficial change breaks all the exist-\ning callers that specified the number or divisor arguments using \nkeywords:\nsafe_division_c(number=2, divisor=5)\n>>>\nTraceback ...\nTypeError: safe_division_c() got an unexpected keyword argument \n¯'number'\nThis is especially problematic because I never intended for number and \ndivisor to be part of an explicit interface for this function. These were \njust convenient parameter names that I chose for the implementation, \nand I didn’t expect anyone to rely on them explicitly.\nPython 3.8 introduces a solution to this problem, called positional-only \narguments. These arguments can be supplied only by position and \nnever by keyword (the opposite of the keyword-only arguments \ndemonstrated above).\nHere, I redefine the safe_division function to use positional-only \narguments for the first two required parameters. The / symbol in the \nargument list indicates where positional-only arguments end:\ndef safe_division_d(numerator, denominator, /, *,  # Changed\n                    ignore_overflow=False,\n                    ignore_zero_division=False):\n    ...\n",
      "content_length": 1895,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "I can verify that this function works when the required arguments \nare provided positionally:\nassert safe_division_d(2, 5) == 0.4\nBut an exception is raised if keywords are used for the positional-only \nparameters:\nsafe_division_d(numerator=2, denominator=5)\n>>>\nTraceback ...\nTypeError: safe_division_d() got some positional-only arguments \n¯passed as keyword arguments: 'numerator, denominator'\nNow, I can be sure that the first two required positional arguments \nin the definition of the safe_division_d function are decoupled from \ncallers. I won’t break anyone if I change the parameters’ names again.\nOne notable consequence of keyword- and positional-only arguments \nis that any parameter name between the / and * symbols in the argu-\nment list may be passed either by position or by keyword (which is \nthe default for all function arguments in Python). Depending on your \nAPI’s style and needs, allowing both argument passing styles can \nincrease readability and reduce noise. For example, here I’ve added \nanother optional parameter to safe_division that allows callers to \nspecify how many digits to use in rounding the result:\ndef safe_division_e(numerator, denominator, /,\n                    ndigits=10, *,                # Changed\n                    ignore_overflow=False,\n                    ignore_zero_division=False):\n    try:\n        fraction = numerator / denominator        # Changed\n        return round(fraction, ndigits)           # Changed\n    except OverflowError:\n        if ignore_overflow:\n            return 0\n        else:\n            raise\n    except ZeroDivisionError:\n        if ignore_zero_division:\n            return float('inf')\n        else:\n            raise\n \nItem 25: Clarity with Keyword-Only and Positional-Only Arguments \n101\n",
      "content_length": 1772,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "102 \nChapter 3 Functions\nNow, I can call this new version of the function in all these differ-\nent ways, since ndigits is an optional parameter that may be passed \neither by position or by keyword:\nresult = safe_division_e(22, 7)\nprint(result)\nresult = safe_division_e(22, 7, 5)\nprint(result)\nresult = safe_division_e(22, 7, ndigits=2)\nprint(result)\n>>>\n3.1428571429\n3.14286\n3.14\nThings to Remember\n✦ Keyword-only arguments force callers to supply certain arguments \nby keyword (instead of by position), which makes the intention of a \nfunction call clearer. Keyword-only arguments are defined after a \nsingle * in the argument list.\n✦ Positional-only arguments ensure that callers can’t supply \n certain parameters using keywords, which helps reduce coupling. \n Positional-only arguments are defined before a single / in the argu-\nment list.\n✦ Parameters between the / and * characters in the argument list \nmay be supplied by position or keyword, which is the default for \nPython parameters.\nItem 26:  Define Function Decorators with \nfunctools.wraps\nPython has special syntax for decorators that can be applied to \nfunctions. A decorator has the ability to run additional code before \nand after each call to a function it wraps. This means decorators \ncan access and modify input arguments, return values, and raised \nexceptions. This functionality can be useful for enforcing semantics, \ndebugging, registering functions, and more.\nFor example, say that I want to print the arguments and return value \nof a function call. This can be especially helpful when debugging \n",
      "content_length": 1573,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": " \nItem 26: Define Function Decorators with functools.wraps \n103\nthe stack of nested function calls from a recursive function. Here, \nI define such a decorator by using *args and **kwargs (see Item 22: \n“Reduce Visual Noise with Variable Positional Arguments” and Item \n23:  “Provide Optional Behavior with Keyword Arguments”) to pass \nthrough all parameters to the wrapped function:\ndef trace(func):\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        print(f'{func.__name__}({args!r}, {kwargs!r}) '\n              f'-> {result!r}')\n        return result\n    return wrapper\nI can apply this decorator to a function by using the @ symbol:\n@trace\ndef fibonacci(n):\n    \"\"\"Return the n-th Fibonacci number\"\"\"\n    if n in (0, 1):\n        return n\n    return (fibonacci(n - 2) + fibonacci(n - 1))\nUsing the @ symbol is equivalent to calling the decorator on the func-\ntion it wraps and assigning the return value to the original name in \nthe same scope:\nfibonacci = trace(fibonacci)\nThe decorated function runs the wrapper code before and after \nfibonacci runs. It prints the arguments and return value at each \nlevel in the recursive stack:\nfibonacci(4)\n>>>\nfibonacci((0,), {}) -> 0\nfibonacci((1,), {}) -> 1\nfibonacci((2,), {}) -> 1\nfibonacci((1,), {}) -> 1\nfibonacci((0,), {}) -> 0\nfibonacci((1,), {}) -> 1\nfibonacci((2,), {}) -> 1\nfibonacci((3,), {}) -> 2\nfibonacci((4,), {}) -> 3\n",
      "content_length": 1407,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "104 \nChapter 3 Functions\nThis works well, but it has an unintended side effect. The value \nreturned by the decorator—the function that’s called above—doesn’t \nthink it’s named fibonacci:\nprint(fibonacci)\n>>>\n<function trace.<locals>.wrapper at 0x108955dc0>\nThe cause of this isn’t hard to see. The trace function returns the \nwrapper defined within its body. The wrapper function is what’s \nassigned to the fibonacci name in the containing module because \nof the decorator. This behavior is problematic because it undermines \ntools that do introspection, such as debuggers (see Item 80: “Consider \nInteractive Debugging with pdb”).\nFor example, the help built-in function is useless when called on the \ndecorated fibonacci function. It should instead print out the doc-\nstring defined above ('Return the n-th Fibonacci number'):\nhelp(fibonacci)\n>>>\nHelp on function wrapper in module __main__:\n \nwrapper(*args, **kwargs)\nObject serializers (see Item 68: “Make pickle Reliable with copyreg”) \nbreak because they can’t determine the location of the original func-\ntion that was decorated:\nimport pickle\n \npickle.dumps(fibonacci)\n>>>\nTraceback ...\nAttributeError: Can't pickle local object 'trace.<locals>.\n¯wrapper'\nThe solution is to use the wraps helper function from the functools \nbuilt-in module. This is a decorator that helps you write decorators. \nWhen you apply it to the wrapper function, it copies all of the import-\nant metadata about the inner function to the outer function:\nfrom functools import wraps\n \n",
      "content_length": 1517,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": " \nItem 26: Define Function Decorators with functools.wraps \n105\ndef trace(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        ...\n    return wrapper\n \n@trace\ndef fibonacci(n):\n    ...\nNow, running the help function produces the expected result, even \nthough the function is decorated:\nhelp(fibonacci)\n>>>\nHelp on function fibonacci in module __main__:\n \nfibonacci(n)\n    Return the n-th Fibonacci number\nThe pickle object serializer also works:\nprint(pickle.dumps(fibonacci))\n>>>\nb'\\x80\\x04\\x95\\x1a\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08__main__\\\n¯x94\\x8c\\tfibonacci\\x94\\x93\\x94.'\nBeyond these examples, Python functions have many other standard \nattributes (e.g., __name__, __module__, __annotations__) that must \nbe preserved to maintain the interface of functions in the language. \nUsing wraps ensures that you’ll always get the correct behavior.\nThings to Remember\n✦ Decorators in Python are syntax to allow one function to modify \nanother function at runtime.\n✦ Using decorators can cause strange behaviors in tools that do intro-\nspection, such as debuggers.\n✦ Use the wraps decorator from the functools built-in module when \nyou define your own decorators to avoid issues.\n",
      "content_length": 1191,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "4\nComprehensions \nand Generators\nMany programs are built around processing lists, dictionary \nkey/value pairs, and sets. Python provides a special syntax, called \ncomprehensions, for succinctly iterating through these types and cre-\nating derivative data structures. Comprehensions can significantly \nincrease the readability of code performing these common tasks and \nprovide a number of other benefits.\nThis style of processing is extended to functions with generators, \nwhich enable a stream of values to be incrementally returned by a \nfunction. The result of a call to a generator function can be used any-\nwhere an iterator is appropriate (e.g., for loops, starred expressions). \nGenerators can improve performance, reduce memory usage, and \nincrease readability.\nItem 27:  Use Comprehensions Instead of map \nand filter\nPython provides compact syntax for deriving a new list from another \nsequence or iterable. These expressions are called list comprehensions. \nFor example, say that I want to compute the square of each number \nin a list. Here, I do this by using a simple for loop:\na = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nsquares = []\nfor x in a:\n    squares.append(x**2)\nprint(squares)\n>>>\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
      "content_length": 1234,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "108 \nChapter 4 Comprehensions and Generators\nWith a list comprehension, I can achieve the same outcome by specify-\ning the expression for my computation along with the input sequence \nto loop over:\nsquares = [x**2 for x in a]  # List comprehension\nprint(squares)\n>>>\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\nUnless you’re applying a single-argument function, list comprehen-\nsions are also clearer than the map built-in function for simple cases. \nmap requires the creation of a lambda function for the computation, \nwhich is visually noisy:\nalt = map(lambda x: x ** 2, a)\nUnlike map, list comprehensions let you easily filter items from the \ninput list, removing corresponding outputs from the result. For \nexample, say I want to compute the squares of the numbers that are \ndivisible by 2. Here, I do this by adding a conditional expression to \nthe list comprehension after the loop:\neven_squares = [x**2 for x in a if x % 2 == 0]\nprint(even_squares)\n>>>\n[4, 16, 36, 64, 100]\nThe filter built-in function can be used along with map to achieve the \nsame outcome, but it is much harder to read:\nalt = map(lambda x: x**2, filter(lambda x: x % 2 == 0, a))\nassert even_squares == list(alt)\nDictionaries and sets have their own equivalents of list comprehen-\nsions (called dictionary comprehensions and set comprehensions, \nrespectively). These make it easy to create other types of derivative \ndata structures when writing algorithms:\neven_squares_dict = {x: x**2 for x in a if x % 2 == 0}\nthrees_cubed_set = {x**3 for x in a if x % 3 == 0}\nprint(even_squares_dict)\nprint(threes_cubed_set)\n>>>\n{2: 4, 4: 16, 6: 36, 8: 64, 10: 100}\n{216, 729, 27}\nAchieving the same outcome is possible with map and filter if you \nwrap each call with a corresponding constructor. These statements \n",
      "content_length": 1775,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": " \nItem 28: Control Subexpressions in Comprehensions \n109\nget so long that you have to break them up across multiple lines, \nwhich is even noisier and should be avoided:\nalt_dict = dict(map(lambda x: (x, x**2),\n                filter(lambda x: x % 2 == 0, a)))\nalt_set = set(map(lambda x: x**3,\n              filter(lambda x: x % 3 == 0, a)))\nThings to Remember\n✦ List comprehensions are clearer than the map and filter built-in \nfunctions because they don’t require lambda expressions.\n✦ List comprehensions allow you to easily skip items from the input \nlist, a behavior that map doesn’t support without help from filter.\n✦ Dictionaries and sets may also be created using comprehensions.\nItem 28:  Avoid More Than Two Control \nSubexpressions in Comprehensions\nBeyond basic usage (see Item 27: “Use Comprehensions Instead of map \nand filter”), comprehensions support multiple levels of looping. For \nexample, say that I want to simplify a matrix (a list containing other \nlist instances) into one flat list of all cells. Here, I do this with a list \ncomprehension by including two for subexpressions. These subex-\npressions run in the order provided, from left to right:\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nflat = [x for row in matrix for x in row]\nprint(flat)\n>>>\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\nThis example is simple, readable, and a reasonable usage of multiple \nloops in a comprehension. Another reasonable usage of multiple loops \ninvolves replicating the two-level-deep layout of the input list. For \nexample, say that I want to square the value in each cell of a two- \ndimensional matrix. This comprehension is noisier because of the \nextra [] characters, but it’s still relatively easy to read:\nsquared = [[x**2 for x in row] for row in matrix]\nprint(squared)\n>>>\n[[1, 4, 9], [16, 25, 36], [49, 64, 81]]\n",
      "content_length": 1816,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "110 \nChapter 4 Comprehensions and Generators\nIf this comprehension included another loop, it would get so long that \nI’d have to split it over multiple lines:\nmy_lists = [\n    [[1, 2, 3], [4, 5, 6]],\n    ...\n]\nflat = [x for sublist1 in my_lists\n        for sublist2 in sublist1\n        for x in sublist2]\nAt this point, the multiline comprehension isn’t much shorter than \nthe alternative. Here, I produce the same result using normal loop \nstatements. The indentation of this version makes the looping clearer \nthan the three-level-list comprehension:\nflat = []\nfor sublist1 in my_lists:\n    for sublist2 in sublist1:\n        flat.extend(sublist2)\nComprehensions support multiple if conditions. Multiple conditions \nat the same loop level have an implicit and expression. For example, \nsay that I want to filter a list of numbers to only even values greater \nthan 4. These two list comprehensions are equivalent:\na = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nb = [x for x in a if x > 4 if x % 2 == 0]\nc = [x for x in a if x > 4 and x % 2 == 0]\nConditions can be specified at each level of looping after the for sub-\nexpression. For example, say I want to filter a matrix so the only cells \nremaining are those divisible by 3 in rows that sum to 10 or higher. \nExpressing this with a list comprehension does not require a lot of \ncode, but it is extremely difficult to read:\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nfiltered = [[x for x in row if x % 3 == 0]\n            for row in matrix if sum(row) >= 10]\nprint(filtered)\n>>>\n[[6], [9]]\nAlthough this example is a bit convoluted, in practice you’ll see \n situations arise where such comprehensions seem like a good fit. \nI strongly encourage you to avoid using list, dict, or set comprehen-\nsions that look like this. The resulting code is very difficult for new \nreaders to understand. The potential for confusion is even worse for \n",
      "content_length": 1878,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": " \nItem 29: Control Subexpressions in Comprehensions \n111\ndict comprehensions since they already need an extra parameter to \nrepresent both the key and the value for each item.\nThe rule of thumb is to avoid using more than two control subexpres-\nsions in a comprehension. This could be two conditions, two loops, \nor one condition and one loop. As soon as it gets more complicated \nthan that, you should use normal if and for statements and write a \nhelper function (see Item 30: “Consider Generators Instead of Return-\ning Lists”).\nThings to Remember\n✦ Comprehensions support multiple levels of loops and multiple con-\nditions per loop level.\n✦ Comprehensions with more than two control subexpressions are \nvery difficult to read and should be avoided.\nItem 29:  Avoid Repeated Work in Comprehensions by \nUsing Assignment Expressions\nA common pattern with comprehensions—including list, dict, and \nset variants—is the need to reference the same computation in mul-\ntiple places. For example, say that I’m writing a program to manage \norders for a fastener company. As new orders come in from customers, \nI need to be able to tell them whether I can fulfill their orders. I need \nto verify that a request is sufficiently in stock and above the mini-\nmum threshold for shipping (in batches of 8):\nstock = {\n    'nails': 125,\n    'screws': 35,\n    'wingnuts': 8,\n    'washers': 24,\n}\n \norder = ['screws', 'wingnuts', 'clips']\ndef get_batches(count, size):\n    return count // size\nresult = {}\nfor name in order:\n  count = stock.get(name, 0)\n  batches = get_batches(count, 8)\n",
      "content_length": 1572,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "112 \nChapter 4 Comprehensions and Generators\n  if batches:\n    result[name] = batches\nprint(result)\n>>>\n{'screws': 4, 'wingnuts': 1}\nHere, I implement this looping logic more succinctly using a dictio-\nnary comprehension (see Item 27: “Use Comprehensions Instead of \nmap and filter” for best practices):\nfound = {name: get_batches(stock.get(name, 0), 8)\n         for name in order\n         if get_batches(stock.get(name, 0), 8)}\nprint(found)\n>>>\n{'screws': 4, 'wingnuts': 1}\nAlthough this code is more compact, the problem with it is that the \nget_batches(stock.get(name, 0), 8) expression is repeated. This \nhurts readability by adding visual noise that’s technically unneces-\nsary. It also increases the likelihood of introducing a bug if the two \nexpressions aren’t kept in sync. For example, here I’ve changed the \nfirst get_batches call to have 4 as its second parameter instead of 8, \nwhich causes the results to be different:\nhas_bug = {name: get_batches(stock.get(name, 0), 4)\n           for name in order\n           if get_batches(stock.get(name, 0), 8)}\nprint('Expected:', found)\nprint('Found:   ', has_bug)\n>>>\nExpected: {'screws': 4, 'wingnuts': 1}\nFound:    {'screws': 8, 'wingnuts': 2}\nAn easy solution to these problems is to use the walrus operator (:=), \nwhich was introduced in Python 3.8, to form an assignment expres-\nsion as part of the comprehension (see Item 10: “Prevent Repetition \nwith Assignment Expressions” for background):\nfound = {name: batches for name in order\n         if (batches := get_batches(stock.get(name, 0), 8))}\nThe assignment expression (batches := get_batches(...)) allows me \nto look up the value for each order key in the stock dictionary a single \n",
      "content_length": 1696,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "time, call get_batches once, and then store its corresponding value in \nthe batches variable. I can then reference that variable elsewhere in \nthe comprehension to construct the dict’s contents instead of having \nto call get_batches a second time. Eliminating the redundant calls \nto get and get_batches may also improve performance by avoiding \nunnecessary computations for each item in the order list.\nIt’s valid syntax to define an assignment expression in the value \nexpression for a comprehension. But if you try to reference the vari-\nable it defines in other parts of the comprehension, you might get an \nexception at runtime because of the order in which comprehensions \nare evaluated:\nresult = {name: (tenth := count // 10)\n          for name, count in stock.items() if tenth > 0}\n>>>\nTraceback ...\nNameError: name 'tenth' is not defined\nI can fix this example by moving the assignment expression into the \ncondition and then referencing the variable name it defined in the \ncomprehension’s value expression:\nresult = {name: tenth for name, count in stock.items()\n          if (tenth := count // 10) > 0}\nprint(result)\n>>>\n{'nails': 12, 'screws': 3, 'washers': 2}\nIf a comprehension uses the walrus operator in the value part of the \ncomprehension and doesn’t have a condition, it’ll leak the loop vari-\nable into the containing scope (see Item 21: “Know How Closures \nInteract with Variable Scope” for background):\nhalf = [(last := count // 2) for count in stock.values()]\nprint(f'Last item of {half} is {last}')\n>>>\nLast item of [62, 17, 4, 12] is 12\nThis leakage of the loop variable is similar to what happens with a \nnormal for loop:\nfor count in stock.values():  # Leaks loop variable\n    pass\nprint(f'Last item of {list(stock.values())} is {count}')\n \nItem 29: Control Subexpressions in Comprehensions \n113\n",
      "content_length": 1823,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "114 \nChapter 4 Comprehensions and Generators\n>>>\nLast item of [125, 35, 8, 24] is 24\nHowever, similar leakage doesn’t happen for the loop variables from \ncomprehensions:\nhalf = [count // 2 for count in stock.values()]\nprint(half)   # Works\nprint(count)  # Exception because loop variable didn't leak\n>>>\n[62, 17, 4, 12]\nTraceback ...\nNameError: name 'count' is not defined\nIt’s better not to leak loop variables, so I recommend using assign-\nment expressions only in the condition part of a comprehension.\nUsing an assignment expression also works the same way in gener-\nator expressions (see Item 32: “Consider Generator Expressions for \nLarge List Comprehensions”). Here, I create an iterator of pairs con-\ntaining the item name and the current count in stock instead of a \ndict instance:\nfound = ((name, batches) for name in order\n         if (batches := get_batches(stock.get(name, 0), 8)))\nprint(next(found))\nprint(next(found))\n>>>\n('screws', 4)\n('wingnuts', 1)\nThings to Remember\n✦ Assignment expressions make it possible for comprehensions and \ngenerator expressions to reuse the value from one condition else-\nwhere in the same comprehension, which can improve readability \nand performance.\n✦ Although it’s possible to use an assignment expression outside of \na comprehension or generator expression’s condition, you should \navoid doing so.\nItem 30:  Consider Generators Instead of Returning \nLists\nThe simplest choice for a function that produces a sequence of results \nis to return a list of items. For example, say that I want to find the \n",
      "content_length": 1551,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": " \nItem 30: Consider Generators Instead of Returning Lists \n115\nindex of every word in a string. Here, I accumulate results in a list \nusing the append method and return it at the end of the function:\ndef index_words(text):\n    result = []\n    if text:\n        result.append(0)\n    for index, letter in enumerate(text):\n        if letter == ' ':\n            result.append(index + 1)\n    return result\nThis works as expected for some sample input:\naddress = 'Four score and seven years ago...'\nresult = index_words(address)\nprint(result[:10])\n>>>\n[0, 5, 11, 15, 21, 27, 31, 35, 43, 51]\nThere are two problems with the index_words function.\nThe first problem is that the code is a bit dense and noisy. Each time \na new result is found, I call the append method. The method call’s \nbulk (result.append) deemphasizes the value being added to the list \n(index + 1). There is one line for creating the result list and another \nfor returning it. While the function body contains ~130 characters \n(without whitespace), only ~75 characters are important.\nA better way to write this function is by using a generator. Generators \nare produced by functions that use yield expressions. Here, I define a \ngenerator function that produces the same results as before:\ndef index_words_iter(text):\n    if text:\n        yield 0\n    for index, letter in enumerate(text):\n        if letter == ' ':\n            yield index + 1\nWhen called, a generator function does not actually run but instead \nimmediately returns an iterator. With each call to the next built-in \nfunction, the iterator advances the generator to its next yield expres-\nsion. Each value passed to yield by the generator is returned by the \niterator to the caller:\nit = index_words_iter(address)\nprint(next(it))\nprint(next(it))\n",
      "content_length": 1772,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "116 \nChapter 4 Comprehensions and Generators\n>>>\n0\n5\nThe index_words_iter function is significantly easier to read because \nall interactions with the result list have been eliminated. Results are \npassed to yield expressions instead. You can easily convert the itera-\ntor returned by the generator to a list by passing it to the list built-in \nfunction if necessary (see Item 32: “Consider Generator Expressions \nfor Large List Comprehensions” for how this works):\nresult = list(index_words_iter(address))\nprint(result[:10])\n>>>\n[0, 5, 11, 15, 21, 27, 31, 35, 43, 51]\nThe second problem with index_words is that it requires all results to \nbe stored in the list before being returned. For huge inputs, this can \ncause a program to run out of memory and crash.\nIn contrast, a generator version of this function can easily be adapted \nto take inputs of arbitrary length due to its bounded memory require-\nments. For example, here I define a generator that streams input from \na file one line at a time and yields outputs one word at a time:\ndef index_file(handle):\n    offset = 0\n    for line in handle:\n        if line:\n            yield offset\n        for letter in line:\n            offset += 1\n            if letter == ' ':\n                yield offset\nThe working memory for this function is limited to the maximum \nlength of one line of input. Running the generator produces the same \nresults (see Item 36: “Consider itertools for Working with Iterators \nand Generators” for more about the islice function):\nwith open('address.txt', 'r') as f:\n    it = index_file(f)\n    results = itertools.islice(it, 0, 10)\n    print(list(results))\n>>>\n[0, 5, 11, 15, 21, 27, 31, 35, 43, 51]\n",
      "content_length": 1681,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": " \nItem 31: Be Defensive When Iterating Over Arguments \n117\nThe only gotcha with defining generators like this is that the callers \nmust be aware that the iterators returned are stateful and can’t be \nreused (see Item 31: “Be Defensive When Iterating Over Arguments”).\nThings to Remember\n✦ Using generators can be clearer than the alternative of having a \nfunction return a list of accumulated results.\n✦ The iterator returned by a generator produces the set of values \npassed to yield expressions within the generator function’s body.\n✦ Generators can produce a sequence of outputs for arbitrarily large \ninputs because their working memory doesn’t include all inputs and \noutputs.\nItem 31: Be Defensive When Iterating Over Arguments\nWhen a function takes a list of objects as a parameter, it’s often \nimportant to iterate over that list multiple times. For example, say \nthat I want to analyze tourism numbers for the U.S. state of Texas. \nImagine that the data set is the number of visitors to each city (in mil-\nlions per year). I’d like to figure out what percentage of overall tourism \neach city receives.\nTo do this, I need a normalization function that sums the inputs to \ndetermine the total number of tourists per year and then divides each \ncity’s individual visitor count by the total to find that city’s contribu-\ntion to the whole:\ndef normalize(numbers):\n    total = sum(numbers)\n    result = []\n    for value in numbers:\n        percent = 100 * value / total\n        result.append(percent)\n    return result\nThis function works as expected when given a list of visits:\nvisits = [15, 35, 80]\npercentages = normalize(visits)\nprint(percentages)\nassert sum(percentages) == 100.0\n>>>\n[11.538461538461538, 26.923076923076923, 61.53846153846154]\n",
      "content_length": 1754,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "118 \nChapter 4 Comprehensions and Generators\nTo scale this up, I need to read the data from a file that contains every \ncity in all of Texas. I define a generator to do this because then I can \nreuse the same function later, when I want to compute tourism num-\nbers for the whole world—a much larger data set with higher memory \nrequirements (see Item 30: “Consider Generators Instead of Returning \nLists” for background):\ndef read_visits(data_path):\n    with open(data_path) as f:\n        for line in f:\n            yield int(line)\nSurprisingly, calling normalize on the read_visits generator’s return \nvalue produces no results:\nit = read_visits('my_numbers.txt')\npercentages = normalize(it)\nprint(percentages)\n>>>\n[]\nThis behavior occurs because an iterator produces its results only \na single time. If you iterate over an iterator or a generator that has \nalready raised a StopIteration exception, you won’t get any results \nthe second time around:\nit = read_visits('my_numbers.txt')\nprint(list(it))\nprint(list(it))  # Already exhausted\n>>>\n[15, 35, 80]\n[]\nConfusingly, you also won’t get errors when you iterate over an \nalready exhausted iterator. for loops, the list constructor, and many \nother functions throughout the Python standard library expect the \nStopIteration exception to be raised during normal operation. These \nfunctions can’t tell the difference between an iterator that has no out-\nput and an iterator that had output and is now exhausted.\nTo solve this problem, you can explicitly exhaust an input iterator and \nkeep a copy of its entire contents in a list. You can then iterate over \nthe list version of the data as many times as you need to. Here’s the \nsame function as before, but it defensively copies the input iterator:\ndef normalize_copy(numbers):\n    numbers_copy = list(numbers)  # Copy the iterator\n",
      "content_length": 1835,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": " \nItem 31: Be Defensive When Iterating Over Arguments \n119\n    total = sum(numbers_copy)\n    result = []\n    for value in numbers_copy:\n        percent = 100 * value / total\n        result.append(percent)\n    return result\nNow the function works correctly on the read_visits generator’s \nreturn value:\nit = read_visits('my_numbers.txt')\npercentages = normalize_copy(it)\nprint(percentages)\nassert sum(percentages) == 100.0\n>>>\n[11.538461538461538, 26.923076923076923, 61.53846153846154]\nThe problem with this approach is that the copy of the input iterator’s \ncontents could be extremely large. Copying the iterator could cause \nthe program to run out of memory and crash. This potential for scal-\nability issues undermines the reason that I wrote read_visits as a \ngenerator in the first place. One way around this is to accept a func-\ntion that returns a new iterator each time it’s called:\ndef normalize_func(get_iter):\n    total = sum(get_iter())   # New iterator\n    result = []\n    for value in get_iter():  # New iterator\n        percent = 100 * value / total\n        result.append(percent)\n    return result\nTo use normalize_func, I can pass in a lambda expression that calls \nthe generator and produces a new iterator each time:\npath = 'my_numbers.txt'\npercentages = normalize_func(lambda: read_visits(path))\nprint(percentages)\nassert sum(percentages) == 100.0\n>>>\n[11.538461538461538, 26.923076923076923, 61.53846153846154]\nAlthough this works, having to pass a lambda function like this is \nclumsy. A better way to achieve the same result is to provide a new \ncontainer class that implements the iterator protocol.\n",
      "content_length": 1625,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "120 \nChapter 4 Comprehensions and Generators\nThe iterator protocol is how Python for loops and related expressions \ntraverse the contents of a container type. When Python sees a state-\nment like for x in foo, it actually calls iter(foo). The iter built-in \nfunction calls the foo.__iter__ special method in turn. The __iter__ \nmethod must return an iterator object (which itself implements the \n__next__ special method). Then, the for loop repeatedly calls the \nnext built-in function on the iterator object until it’s exhausted (indi-\ncated by raising a StopIteration exception).\nIt sounds complicated, but practically speaking, you can achieve all of \nthis behavior for your classes by implementing the __iter__ method \nas a generator. Here, I define an iterable container class that reads \nthe file containing tourism data:\nclass ReadVisits:\n    def __init__(self, data_path):\n        self.data_path = data_path\n \n    def __iter__(self):\n        with open(self.data_path) as f:\n            for line in f:\n                yield int(line)\nThis new container type works correctly when passed to the original \nfunction without modifications:\nvisits = ReadVisits(path)\npercentages = normalize(visits)\nprint(percentages)\nassert sum(percentages) == 100.0\n>>>\n[11.538461538461538, 26.923076923076923, 61.53846153846154]\nThis \nworks \nbecause \nthe \nsum \nmethod \nin \nnormalize \ncalls \nReadVisits.__iter__ to allocate a new iterator object. The for loop to \nnormalize the numbers also calls __iter__ to allocate a second iter-\nator object. Each of those iterators will be advanced and exhausted \nindependently, ensuring that each unique iteration sees all of the \ninput data values. The only downside of this approach is that it reads \nthe input data multiple times.\nNow that you know how containers like ReadVisits work, you can \nwrite your functions and methods to ensure that parameters aren’t \njust iterators. The protocol states that when an iterator is passed \nto the iter built-in function, iter returns the iterator itself. In con-\ntrast, when a container type is passed to iter, a new iterator object is \n",
      "content_length": 2105,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": " \nItem 31: Be Defensive When Iterating Over Arguments \n121\nreturned each time. Thus, you can test an input value for this behav-\nior and raise a TypeError to reject arguments that can’t be repeatedly \niterated over:\ndef normalize_defensive(numbers):\n    if iter(numbers) is numbers:  # An iterator -- bad!\n        raise TypeError('Must supply a container')\n    total = sum(numbers)\n    result = []\n    for value in numbers:\n        percent = 100 * value / total\n        result.append(percent)\n    return result\nAlternatively, the collections.abc built-in module defines an Iterator \nclass that can be used in an isinstance test to recognize the potential \nproblem (see Item 43: “Inherit from collections.abc for Custom Con-\ntainer Types”):\nfrom collections.abc import Iterator\ndef normalize_defensive(numbers):\n    if isinstance(numbers, Iterator):  # Another way to check\n        raise TypeError('Must supply a container')\n    total = sum(numbers)\n    result = []\n    for value in numbers:\n        percent = 100 * value / total\n        result.append(percent)\n    return result\nThe approach of using a container is ideal if you don’t want to copy \nthe full input iterator, as with the normalize_copy function above, but \nyou also need to iterate over the input data multiple times. This func-\ntion works as expected for list and ReadVisits inputs because they \nare iterable containers that follow the iterator protocol:\nvisits = [15, 35, 80]\npercentages = normalize_defensive(visits)\nassert sum(percentages) == 100.0\nvisits = ReadVisits(path)\npercentages = normalize_defensive(visits)\nassert sum(percentages) == 100.0\n",
      "content_length": 1618,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "122 \nChapter 4 Comprehensions and Generators\nThe function raises an exception if the input is an iterator rather than \na container:\nvisits = [15, 35, 80]\nit = iter(visits)\nnormalize_defensive(it)\n>>>\nTraceback ...\nTypeError: Must supply a container\nThe same approach can also be used for asynchronous iterators (see \nItem 61: “Know How to Port Threaded I/O to asyncio” for an example).\nThings to Remember\n✦ Beware of functions and methods that iterate over input argu-\nments multiple times. If these arguments are iterators, you may see \nstrange behavior and missing values.\n✦ Python’s iterator protocol defines how containers and iterators inter-\nact with the iter and next built-in functions, for loops, and related \nexpressions.\n✦ You can easily define your own iterable container type by imple-\nmenting the __iter__ method as a generator.\n✦ You can detect that a value is an iterator (instead of a container) \nif calling iter on it produces the same value as what you passed \nin. Alternatively, you can use the isinstance built-in function along \nwith the collections.abc.Iterator class.\nItem 32:  Consider Generator Expressions for Large \nList Comprehensions\nThe problem with list comprehensions (see Item 27: “Use Comprehen-\nsions Instead of map and filter”) is that they may create new list \ninstances containing one item for each value in input sequences. This \nis fine for small inputs, but for large inputs, this behavior could con-\nsume significant amounts of memory and cause a program to crash.\nFor example, say that I want to read a file and return the number of \ncharacters on each line. Doing this with a list comprehension would \nrequire holding the length of every line of the file in memory. If the \nfile is enormous or perhaps a never-ending network socket, using list \ncomprehensions would be problematic. Here, I use a list comprehen-\nsion in a way that can only handle small input values:\nvalue = [len(x) for x in open('my_file.txt')]\nprint(value)\n",
      "content_length": 1971,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": " Item 32: Consider Generator Expressions for Large List Comprehensions \n123\n>>>\n[100, 57, 15, 1, 12, 75, 5, 86, 89, 11]\nTo solve this issue, Python provides generator expressions, which are \na generalization of list comprehensions and generators. Generator \nexpressions don’t materialize the whole output sequence when they’re \nrun. Instead, generator expressions evaluate to an iterator that yields \none item at a time from the expression.\nYou create a generator expression by putting list-comprehension-like \nsyntax between () characters. Here, I use a generator expression \nthat is equivalent to the code above. However, the generator expres-\nsion immediately evaluates to an iterator and doesn’t make forward \nprogress:\nit = (len(x) for x in open('my_file.txt'))\nprint(it)\n>>>\n<generator object <genexpr> at 0x108993dd0>\nThe returned iterator can be advanced one step at a time to produce \nthe next output from the generator expression, as needed (using \nthe next built-in function). I can consume as much of the generator \nexpression as I want without risking a blowup in memory usage:\nprint(next(it))\nprint(next(it))\n>>>\n100\n57\nAnother powerful outcome of generator expressions is that they can \nbe composed together. Here, I take the iterator returned by the gen-\nerator expression above and use it as the input for another generator \nexpression:\nroots = ((x, x**0.5) for x in it)\nEach time I advance this iterator, it also advances the interior itera-\ntor, creating a domino effect of looping, evaluating conditional expres-\nsions, and passing around inputs and outputs, all while being as \nmemory efficient as possible:\nprint(next(roots))\n>>>\n(15, 3.872983346207417)\n",
      "content_length": 1676,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "124 \nChapter 4 Comprehensions and Generators\nChaining generators together like this executes very quickly in \nPython. When you’re looking for a way to compose functionality that’s \noperating on a large stream of input, generator expressions are a \ngreat choice. The only gotcha is that the iterators returned by gener-\nator expressions are stateful, so you must be careful not to use these \niterators more than once (see Item 31: “Be Defensive When Iterating \nOver Arguments”).\nThings to Remember\n✦ List comprehensions can cause problems for large inputs by using \ntoo much memory.\n✦ Generator expressions avoid memory issues by producing outputs \none at a time as iterators.\n✦ Generator expressions can be composed by passing the iterator from \none generator expression into the for subexpression of another.\n✦ Generator expressions execute very quickly when chained together \nand are memory efficient.\nItem 33: Compose Multiple Generators with yield from\nGenerators provide a variety of benefits (see Item 30: “Consider Gen-\nerators Instead of Returning Lists”) and solutions to common prob-\nlems (see Item 31: “Be Defensive When Iterating Over Arguments”). \nGenerators are so useful that many programs start to look like layers \nof generators strung together.\nFor example, say that I have a graphical program that’s using gener-\nators to animate the movement of images onscreen. To get the visual \neffect I’m looking for, I need the images to move quickly at first, pause \ntemporarily, and then continue moving at a slower pace. Here, I define \ntwo generators that yield the expected onscreen deltas for each part of \nthis animation:\ndef move(period, speed):\n    for _ in range(period):\n        yield speed\n \ndef pause(delay):\n    for _ in range(delay):\n        yield 0\nTo create the final animation, I need to combine move and pause \ntogether to produce a single sequence of onscreen deltas. Here, I do \n",
      "content_length": 1908,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": " \nItem 33: Compose Multiple Generators with yield from \n125\nthis by calling a generator for each step of the animation, iterating \nover each generator in turn, and then yielding the deltas from all of \nthem in sequence:\ndef animate():\n    for delta in move(4, 5.0):\n        yield delta\n    for delta in pause(3):\n        yield delta\n    for delta in move(2, 3.0):\n        yield delta\nNow, I can render those deltas onscreen as they’re produced by the \nsingle animation generator:\ndef render(delta):\n    print(f'Delta: {delta:.1f}')\n    # Move the images onscreen\n    ...\ndef run(func):\n    for delta in func():\n        render(delta)\nrun(animate)\n>>>\nDelta: 5.0\nDelta: 5.0\nDelta: 5.0\nDelta: 5.0\nDelta: 0.0\nDelta: 0.0\nDelta: 0.0\nDelta: 3.0\nDelta: 3.0\nThe problem with this code is the repetitive nature of the animate \nfunction. The redundancy of the for statements and yield expres-\nsions for each generator adds noise and reduces readability. This \nexample includes only three nested generators and it’s already hurt-\ning clarity; a complex animation with a dozen phases or more would \nbe extremely difficult to follow.\nThe solution to this problem is to use the yield from expression. \nThis advanced generator feature allows you to yield all values from \n",
      "content_length": 1256,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "126 \nChapter 4 Comprehensions and Generators\na nested generator before returning control to the parent generator. \nHere, I reimplement the animation function by using yield from:\ndef animate_composed():\n    yield from move(4, 5.0)\n    yield from pause(3)\n    yield from move(2, 3.0)\n \nrun(animate_composed)\n>>>\nDelta: 5.0\nDelta: 5.0\nDelta: 5.0\nDelta: 5.0\nDelta: 0.0\nDelta: 0.0\nDelta: 0.0\nDelta: 3.0\nDelta: 3.0\nThe result is the same as before, but now the code is clearer and more \nintuitive. yield from essentially causes the Python interpreter to han-\ndle the nested for loop and yield expression boilerplate for you, which \nresults in better performance. Here, I verify the speedup by using the \ntimeit built-in module to run a micro-benchmark:\nimport timeit\n \ndef child():\n    for i in range(1_000_000):\n        yield i\n \ndef slow():\n    for i in child():\n        yield i\n \ndef fast():\n    yield from child()\n \nbaseline = timeit.timeit(\n    stmt='for _ in slow(): pass',\n    globals=globals(),\n    number=50)\nprint(f'Manual nesting {baseline:.2f}s')\n \n",
      "content_length": 1056,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": " \nItem 34: Avoid Injecting Data into Generators with send \n127\ncomparison = timeit.timeit(\n    stmt='for _ in fast(): pass',\n    globals=globals(),\n    number=50)\nprint(f'Composed nesting {comparison:.2f}s')\n \nreduction = -(comparison - baseline) / baseline\nprint(f'{reduction:.1%} less time')\n>>>\nManual nesting 4.02s\nComposed nesting 3.47s\n13.5% less time\nIf you find yourself composing generators, I strongly encourage you to \nuse yield from when possible.\nThings to Remember\n✦ The yield from expression allows you to compose multiple nested \ngenerators together into a single combined generator.\n✦ yield from provides better performance than manually iterating \nnested generators and yielding their outputs.\nItem 34:  Avoid Injecting Data into Generators \nwith send\nyield expressions provide generator functions with a simple way to \nproduce an iterable series of output values (see Item 30: “Consider \nGenerators Instead of Returning Lists”). However, this channel \nappears to be unidirectional: There’s no immediately obvious way to \nsimultaneously stream data in and out of a generator as it runs. Hav-\ning such bidirectional communication could be valuable for a variety \nof use cases.\nFor example, say that I’m writing a program to transmit signals using \na software-defined radio. Here, I use a function to generate an approx-\nimation of a sine wave with a given number of points:\nimport math\n \ndef wave(amplitude, steps):\n    step_size = 2 * math.pi / steps\n    for step in range(steps):\n        radians = step * step_size\n        fraction = math.sin(radians)\n",
      "content_length": 1571,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "128 \nChapter 4 Comprehensions and Generators\n        output = amplitude * fraction\n        yield output\nNow, I can transmit the wave signal at a single specified amplitude by \niterating over the wave generator:\ndef transmit(output):\n    if output is None:\n        print(f'Output is None')\n    else:\n        print(f'Output: {output:>5.1f}')\n \ndef run(it):\n    for output in it:\n        transmit(output)\n \nrun(wave(3.0, 8))\n>>>\nOutput:   0.0\nOutput:   2.1\nOutput:   3.0\nOutput:   2.1\nOutput:   0.0\nOutput:  -2.1\nOutput:  -3.0\nOutput:  -2.1\nThis works fine for producing basic waveforms, but it can’t be used to \nconstantly vary the amplitude of the wave based on a separate input \n(i.e., as required to broadcast AM radio signals). I need a way to mod-\nulate the amplitude on each iteration of the generator.\nPython generators support the send method, which upgrades yield \nexpressions into a two-way channel. The send method can be used to \nprovide streaming inputs to a generator at the same time it’s yielding \noutputs. Normally, when iterating a generator, the value of the yield \nexpression is None:\ndef my_generator():\n    received = yield 1\n    print(f'received = {received}')\n \nit = iter(my_generator())\noutput = next(it)       # Get first generator output\nprint(f'output = {output}')\n \n",
      "content_length": 1293,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": " \nItem 34: Avoid Injecting Data into Generators with send \n129\ntry:\n    next(it)            # Run generator until it exits\nexcept StopIteration:\n    pass\n>>>\noutput = 1\nreceived = None\nWhen I call the send method instead of iterating the generator with a \nfor loop or the next built-in function, the supplied parameter becomes \nthe value of the yield expression when the generator is resumed. How-\never, when the generator first starts, a yield expression has not been \nencountered yet, so the only valid value for calling send initially is \nNone (any other argument would raise an exception at runtime):\nit = iter(my_generator())\noutput = it.send(None)  # Get first generator output\nprint(f'output = {output}')\n \ntry:\n    it.send('hello!')   # Send value into the generator\nexcept StopIteration:\n    pass\n>>>\noutput = 1\nreceived = hello!\nI can take advantage of this behavior in order to modulate the ampli-\ntude of the sine wave based on an input signal. First, I need to change \nthe wave generator to save the amplitude returned by the yield expres-\nsion and use it to calculate the next generated output:\ndef wave_modulating(steps):\n    step_size = 2 * math.pi / steps\n    amplitude = yield             # Receive initial amplitude\n    for step in range(steps):\n        radians = step * step_size\n        fraction = math.sin(radians)\n        output = amplitude * fraction\n        amplitude = yield output  # Receive next amplitude\nThen, I need to update the run function to stream the modulating \namplitude into the wave_modulating generator on each iteration. The \n",
      "content_length": 1569,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "130 \nChapter 4 Comprehensions and Generators\nfirst input to send must be None, since a yield expression would not \nhave occurred within the generator yet:\ndef run_modulating(it):\n    amplitudes = [\n        None, 7, 7, 7, 2, 2, 2, 2, 10, 10, 10, 10, 10]\n    for amplitude in amplitudes:\n        output = it.send(amplitude)\n        transmit(output)\n \nrun_modulating(wave_modulating(12))\n>>>\nOutput is None\nOutput:   0.0\nOutput:   3.5\nOutput:   6.1\nOutput:   2.0\nOutput:   1.7\nOutput:   1.0\nOutput:   0.0\nOutput:  -5.0\nOutput:  -8.7\nOutput: -10.0\nOutput:  -8.7\nOutput:  -5.0\nThis works; it properly varies the output amplitude based on the input \nsignal. The first output is None, as expected, because a value for the \namplitude wasn’t received by the generator until after the initial yield \nexpression.\nOne problem with this code is that it’s difficult for new readers to \nunderstand: Using yield on the right side of an assignment statement \nisn’t intuitive, and it’s hard to see the connection between yield and \nsend without already knowing the details of this advanced generator \nfeature.\nNow, imagine that the program’s requirements get more complicated. \nInstead of using a simple sine wave as my carrier, I need to use a \ncomplex waveform consisting of multiple signals in sequence. One \nway to implement this behavior is by composing multiple generators \ntogether by using the yield from expression (see Item 33: “Compose \nMultiple Generators with yield from”). Here, I confirm that this works \nas expected in the simpler case where the amplitude is fixed:\ndef complex_wave():\n    yield from wave(7.0, 3)\n",
      "content_length": 1612,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": " \nItem 34: Avoid Injecting Data into Generators with send \n131\n    yield from wave(2.0, 4)\n    yield from wave(10.0, 5)\n \nrun(complex_wave())\n>>>\nOutput:   0.0\nOutput:   6.1\nOutput:  -6.1\nOutput:   0.0\nOutput:   2.0\nOutput:   0.0\nOutput:  -2.0\nOutput:   0.0\nOutput:   9.5\nOutput:   5.9\nOutput:  -5.9\nOutput:  -9.5\nGiven that the yield from expression handles the simpler case, you \nmay expect it to also work properly along with the generator send \nmethod. Here, I try to use it this way by composing multiple calls to \nthe wave_modulating generator together:\ndef complex_wave_modulating():\n    yield from wave_modulating(3)\n    yield from wave_modulating(4)\n    yield from wave_modulating(5)\n \nrun_modulating(complex_wave_modulating())\n>>>\nOutput is None\nOutput:   0.0\nOutput:   6.1\nOutput:  -6.1\nOutput is None\nOutput:   0.0\nOutput:   2.0\nOutput:   0.0\nOutput: -10.0\nOutput is None\nOutput:   0.0\nOutput:   9.5\nOutput:   5.9\n",
      "content_length": 926,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "132 \nChapter 4 Comprehensions and Generators\nThis works to some extent, but the result contains a big surprise: \nThere are many None values in the output! Why does this happen? \nWhen each yield from expression finishes iterating over a nested gen-\nerator, it moves on to the next one. Each nested generator starts with \na bare yield expression—one without a value—in order to receive the \ninitial amplitude from a generator send method call. This causes the \nparent generator to output a None value when it transitions between \nchild generators.\nThis means that assumptions about how the yield from and send \nfeatures behave individually will be broken if you try to use them \ntogether. Although it’s possible to work around this None problem \nby increasing the complexity of the run_modulating function, it’s not \nworth the trouble. It’s already difficult for new readers of the code to \nunderstand how send works. This surprising gotcha with yield from \nmakes it even worse. My advice is to avoid the send method entirely \nand go with a simpler approach.\nThe easiest solution is to pass an iterator into the wave function. The \niterator should return an input amplitude each time the next built-in \nfunction is called on it. This arrangement ensures that each genera-\ntor is progressed in a cascade as inputs and outputs are processed \n(see Item 32: “Consider Generator Expressions for Large List Compre-\nhensions” for another example):\ndef wave_cascading(amplitude_it, steps):\n    step_size = 2 * math.pi / steps\n    for step in range(steps):\n        radians = step * step_size\n        fraction = math.sin(radians)\n        amplitude = next(amplitude_it)  # Get next input\n        output = amplitude * fraction\n        yield output\nI can pass the same iterator into each of the generator functions that \nI’m trying to compose together. Iterators are stateful (see Item 31: “Be \nDefensive When Iterating Over Arguments”), and thus each of the \nnested generators picks up where the previous generator left off:\ndef complex_wave_cascading(amplitude_it):\n    yield from wave_cascading(amplitude_it, 3)\n    yield from wave_cascading(amplitude_it, 4)\n    yield from wave_cascading(amplitude_it, 5)\nNow, I can run the composed generator by simply passing in an itera-\ntor from the amplitudes list:\ndef run_cascading():\n    amplitudes = [7, 7, 7, 2, 2, 2, 2, 10, 10, 10, 10, 10]\n",
      "content_length": 2373,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": " \nItem 35: Avoid Causing State Transitions in Generators with throw \n133\n    it = complex_wave_cascading(iter(amplitudes))\n    for amplitude in amplitudes:\n        output = next(it)\n        transmit(output)\n \nrun_cascading()\n>>>\nOutput:   0.0\nOutput:   6.1\nOutput:  -6.1\nOutput:   0.0\nOutput:   2.0\nOutput:   0.0\nOutput:  -2.0\nOutput:   0.0\nOutput:   9.5\nOutput:   5.9\nOutput:  -5.9\nOutput:  -9.5\nThe best part about this approach is that the iterator can come from \nanywhere and could be completely dynamic (e.g., implemented using \na generator function). The only downside is that this code assumes \nthat the input generator is completely thread safe, which may not be \nthe case. If you need to cross thread boundaries, async functions may \nbe a better fit (see Item 62: “Mix Threads and Coroutines to Ease the \nTransition to asyncio”).\nThings to Remember\n✦ The send method can be used to inject data into a generator by giv-\ning the yield expression a value that can be assigned to a variable.\n✦ Using send with yield from expressions may cause surprising \nbehavior, such as None values appearing at unexpected times in the \ngenerator output.\n✦ Providing an input iterator to a set of composed generators is a bet-\nter approach than using the send method, which should be avoided.\nItem 35:  Avoid Causing State Transitions in \nGenerators with throw\nIn addition to yield from expressions (see Item 33: “Compose Multi-\nple Generators with yield from”) and the send method (see Item 34: \n“Avoid Injecting Data into Generators with send”), another advanced \n",
      "content_length": 1557,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "134 \nChapter 4 Comprehensions and Generators\ngenerator feature is the throw method for re-raising Exception \ninstances within generator functions. The way throw works is simple: \nWhen the method is called, the next occurrence of a yield expression \nre-raises the provided Exception instance after its output is received \ninstead of continuing normally. Here, I show a simple example of this \nbehavior in action:\nclass MyError(Exception):\n    pass\n \ndef my_generator():\n    yield 1\n    yield 2\n    yield 3\n \nit = my_generator()\nprint(next(it))  # Yield 1\nprint(next(it))  # Yield 2\nprint(it.throw(MyError('test error')))\n>>>\n1\n2\nTraceback ...\nMyError: test error\nWhen you call throw, the generator function may catch the injected \nexception with a standard try/except compound statement that sur-\nrounds the last yield expression that was executed (see Item 65: \n“Take Advantage of Each Block in try/except/else/finally” for more \nabout exception handling):\ndef my_generator():\n    yield 1\n \n    try:\n        yield 2\n    except MyError:\n        print('Got MyError!')\n    else:\n        yield 3\n \n    yield 4\n \nit = my_generator()\nprint(next(it))  # Yield 1\n",
      "content_length": 1155,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": " \nItem 35: Avoid Causing State Transitions in Generators with throw \n135\nprint(next(it))  # Yield 2\nprint(it.throw(MyError('test error')))\n>>>\n1\n2\nGot MyError!\n4\nThis functionality provides a two-way communication channel \nbetween a generator and its caller that can be useful in certain situ-\nations (see Item 34: “Avoid Injecting Data into Generators with send” \nfor another one). For example, imagine that I’m trying to write a pro-\ngram with a timer that supports sporadic resets. Here, I implement \nthis behavior by defining a generator that relies on the throw method:\nclass Reset(Exception):\n    pass\n \ndef timer(period):\n    current = period\n    while current:\n        current -= 1\n        try:\n            yield current\n        except Reset:\n            current = period\nIn this code, whenever the Reset exception is raised by the yield \nexpression, the counter resets itself to its original period.\nI can connect this counter reset event to an external input that’s \npolled every second. Then, I can define a run function to drive the \ntimer generator, which injects exceptions with throw to cause resets, \nor calls announce for each generator output:\ndef check_for_reset():\n    # Poll for external event\n    ...\n \ndef announce(remaining):\n    print(f'{remaining} ticks remaining')\n \ndef run():\n    it = timer(4)\n    while True:\n",
      "content_length": 1339,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "136 \nChapter 4 Comprehensions and Generators\n        try:\n            if check_for_reset():\n                current = it.throw(Reset())\n            else:\n                current = next(it)\n        except StopIteration:\n            break\n        else:\n            announce(current)\n \nrun()\n>>>\n3 ticks remaining\n2 ticks remaining\n1 ticks remaining\n3 ticks remaining\n2 ticks remaining\n3 ticks remaining\n2 ticks remaining\n1 ticks remaining\n0 ticks remaining\nThis code works as expected, but it’s much harder to read than nec-\nessary. The various levels of nesting required to catch StopIteration \nexceptions or decide to throw, call next, or announce make the code \nnoisy.\nA simpler approach to implementing this functionality is to define a \nstateful closure (see Item 38: “Accept Functions Instead of Classes for \nSimple Interfaces”) using an iterable container object (see Item 31: “Be \nDefensive When Iterating Over Arguments”). Here, I redefine the timer \ngenerator by using such a class:\nclass Timer:\n    def __init__(self, period):\n        self.current = period\n        self.period = period\n \n    def reset(self):\n        self.current = self.period\n \n    def __iter__(self):\n        while self.current:\n            self.current -= 1\n            yield self.current\n",
      "content_length": 1268,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": " \nItem 35: Avoid Causing State Transitions in Generators with throw \n137\nNow, the run method can do a much simpler iteration by using a for \nstatement, and the code is much easier to follow because of the reduc-\ntion in the levels of nesting:\ndef run():\n    timer = Timer(4)\n    for current in timer:\n        if check_for_reset():\n            timer.reset()\n        announce(current)\n \nrun()\n>>>\n3 ticks remaining\n2 ticks remaining\n1 ticks remaining\n3 ticks remaining\n2 ticks remaining\n3 ticks remaining\n2 ticks remaining\n1 ticks remaining\n0 ticks remaining\nThe output matches the earlier version using throw, but this imple-\nmentation is much easier to understand, especially for new readers \nof the code. Often, what you’re trying to accomplish by mixing gen-\nerators and exceptions is better achieved with asynchronous fea-\ntures (see Item 60: “Achieve Highly Concurrent I/O with Coroutines”). \nThus, I suggest that you avoid using throw entirely and instead use \nan iterable class if you need this type of exceptional behavior.\nThings to Remember\n✦ The throw method can be used to re-raise exceptions within \n generators at the position of the most recently executed yield \nexpression.\n✦ Using throw harms readability because it requires additional nest-\ning and boilerplate in order to raise and catch exceptions.\n✦ A better way to provide exceptional behavior in generators is to use \na class that implements the __iter__ method along with methods to \ncause exceptional state transitions. \n",
      "content_length": 1495,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "138 \nChapter 4 Comprehensions and Generators\nItem 36:  Consider itertools for Working with Iterators \nand Generators\nThe itertools built-in module contains a large number of functions \nthat are useful for organizing and interacting with iterators (see Item \n30: “Consider Generators Instead of Returning Lists” and Item 31: “Be \nDefensive When Iterating Over Arguments” for background):\nimport itertools\nWhenever you find yourself dealing with tricky iteration code, it’s \nworth looking at the itertools documentation again to see if there’s \nanything in there for you to use (see help(itertools)). The following \nsections describe the most important functions that you should know \nin three primary categories.\nLinking Iterators Together\nThe itertools built-in module includes a number of functions for \nlinking iterators together.\nchain\nUse chain to combine multiple iterators into a single sequential \niterator:\nit = itertools.chain([1, 2, 3], [4, 5, 6])\nprint(list(it))\n>>>\n[1, 2, 3, 4, 5, 6]\nrepeat\nUse repeat to output a single value forever, or use the second param-\neter to specify a maximum number of times:\nit = itertools.repeat('hello', 3)\nprint(list(it))\n>>>\n['hello', 'hello', 'hello']\ncycle\nUse cycle to repeat an iterator’s items forever:\nit = itertools.cycle([1, 2])\nresult = [next(it) for _ in range (10)]\nprint(result)\n",
      "content_length": 1337,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": " \nItem 36: Consider itertools for Working with Iterators and Generators \n139\n>>>\n[1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\ntee\nUse tee to split a single iterator into the number of parallel iterators \nspecified by the second parameter. The memory usage of this func-\ntion will grow if the iterators don’t progress at the same speed since \nbuffering will be required to enqueue the pending items:\nit1, it2, it3 = itertools.tee(['first', 'second'], 3)\nprint(list(it1))\nprint(list(it2))\nprint(list(it3))\n>>>\n['first', 'second']\n['first', 'second']\n['first', 'second']\nzip_longest\nThis variant of the zip built-in function (see Item 8: “Use zip to \n Process Iterators in Parallel”) returns a placeholder value when an \niterator is exhausted, which may happen if iterators have different \nlengths:\nkeys = ['one', 'two', 'three']\nvalues = [1, 2]\n \nnormal = list(zip(keys, values))\nprint('zip:        ', normal)\n \nit = itertools.zip_longest(keys, values, fillvalue='nope')\nlongest = list(it)\nprint('zip_longest:', longest)\n>>>\nzip:         [('one', 1), ('two', 2)]\nzip_longest: [('one', 1), ('two', 2), ('three', 'nope')]\nFiltering Items from an Iterator\nThe itertools built-in module includes a number of functions for fil-\ntering items from an iterator.\n",
      "content_length": 1238,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "140 \nChapter 4 Comprehensions and Generators\nislice\nUse islice to slice an iterator by numerical indexes without copying. \nYou can specify the end, start and end, or start, end, and step sizes, \nand the behavior is similar to that of standard sequence slicing and \nstriding (see Item 11: “Know How to Slice Sequences” and Item 12: \n“Avoid Striding and Slicing in a Single Expression”):\nvalues = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n \nfirst_five = itertools.islice(values, 5)\nprint('First five: ', list(first_five))\n \nmiddle_odds = itertools.islice(values, 2, 8, 2)\nprint('Middle odds:', list(middle_odds))\n>>>\nFirst five:  [1, 2, 3, 4, 5]\nMiddle odds: [3, 5, 7]\ntakewhile\ntakewhile returns items from an iterator until a predicate function \nreturns False for an item:\nvalues = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nless_than_seven = lambda x: x < 7\nit = itertools.takewhile(less_than_seven, values)\nprint(list(it))\n>>>\n[1, 2, 3, 4, 5, 6]\ndropwhile\ndropwhile, which is the opposite of takewhile, skips items from an \niterator until the predicate function returns True for the first time:\nvalues = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nless_than_seven = lambda x: x < 7\nit = itertools.dropwhile(less_than_seven, values)\nprint(list(it))\n>>>\n[7, 8, 9, 10]\n",
      "content_length": 1235,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": " \nItem 36: Consider itertools for Working with Iterators and Generators \n141\nfilterfalse\nfilterfalse, which is the opposite of the filter built-in function, \nreturns all items from an iterator where a predicate function returns \nFalse:\nvalues = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nevens = lambda x: x % 2 == 0\n \nfilter_result = filter(evens, values)\nprint('Filter:      ', list(filter_result))\n \nfilter_false_result = itertools.filterfalse(evens, values)\nprint('Filter false:', list(filter_false_result))\n>>>\nFilter:       [2, 4, 6, 8, 10]\nFilter false: [1, 3, 5, 7, 9]\nProducing Combinations of Items from Iterators\nThe itertools built-in module includes a number of functions for \n producing combinations of items from iterators.\naccumulate\naccumulate folds an item from the iterator into a running value by \napplying a function that takes two parameters. It outputs the current \naccumulated result for each input value:\nvalues = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nsum_reduce = itertools.accumulate(values)\nprint('Sum:   ', list(sum_reduce))\n \ndef sum_modulo_20(first, second):\n    output = first + second\n    return output % 20\n \nmodulo_reduce = itertools.accumulate(values, sum_modulo_20)\nprint('Modulo:', list(modulo_reduce))\n>>>\nSum:    [1, 3, 6, 10, 15, 21, 28, 36, 45, 55]\nModulo: [1, 3, 6, 10, 15, 1, 8, 16, 5, 15]\nThis is essentially the same as the reduce function from the functools \nbuilt-in module, but with outputs yielded one step at a time. By default \nit sums the inputs if no binary function is specified.\n",
      "content_length": 1518,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "142 \nChapter 4 Comprehensions and Generators\nproduct\nproduct returns the Cartesian product of items from one or more iter-\nators, which is a nice alternative to using deeply nested list compre-\nhensions (see Item 28: “Avoid More Than Two Control Subexpressions \nin Comprehensions” for why to avoid those):\nsingle = itertools.product([1, 2], repeat=2)\nprint('Single:  ', list(single))\n \nmultiple = itertools.product([1, 2], ['a', 'b'])\nprint('Multiple:', list(multiple))\n>>>\nSingle:   [(1, 1), (1, 2), (2, 1), (2, 2)]\nMultiple: [(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]\npermutations\npermutations returns the unique ordered permutations of length N \nwith items from an iterator:\nit = itertools.permutations([1, 2, 3, 4], 2)\nprint(list(it))\n>>>\n[(1, 2),\n (1, 3),\n (1, 4),\n (2, 1),\n (2, 3),\n (2, 4),\n (3, 1),\n (3, 2),\n (3, 4),\n (4, 1),\n (4, 2),\n (4, 3)]\ncombinations\ncombinations returns the unordered combinations of length N with \nunrepeated items from an iterator:\nit = itertools.combinations([1, 2, 3, 4], 2)\nprint(list(it))\n>>>\n[(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
      "content_length": 1077,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": " \nItem 36: Consider itertools for Working with Iterators and Generators \n143\ncombinations_with_replacement\ncombinations_with_replacement is the same as combinations, but \nrepeated values are allowed:\nit = itertools.combinations_with_replacement([1, 2, 3, 4], 2)\nprint(list(it))\n>>>\n[(1, 1),\n (1, 2),\n (1, 3),\n (1, 4),\n (2, 2),\n (2, 3),\n (2, 4),\n (3, 3),\n (3, 4),\n (4, 4)]\nThings to Remember\n✦ The itertools functions fall into three main categories for work-\ning with iterators and generators: linking iterators together, filtering \nitems they output, and producing combinations of items.\n✦ There are more advanced functions, additional parameters, and \nuseful recipes available in the documentation at help(itertools).\n",
      "content_length": 720,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "5\nClasses and \nInterfaces\nAs an object-oriented programming language, Python supports a full \nrange of features, such as inheritance, polymorphism, and encap-\nsulation. Getting things done in Python often requires writing new \nclasses and defining how they interact through their interfaces and \nhierarchies. \nPython’s classes and inheritance make it easy to express a program’s \nintended behaviors with objects. They allow you to improve and \nexpand functionality over time. They provide flexibility in an envi-\nronment of changing requirements. Knowing how to use them well \nenables you to write maintainable code.\nItem 37:  Compose Classes Instead of Nesting Many \nLevels of Built-in Types\nPython’s built-in dictionary type is wonderful for maintaining \ndynamic internal state over the lifetime of an object. By dynamic, \nI mean situations in which you need to do bookkeeping for an unex-\npected set of identifiers. For example, say that I want to record the \ngrades of a set of students whose names aren’t known in advance. \nI can define a class to store the names in a dictionary instead of using \na predefined attribute for each student:\nclass SimpleGradebook:\n    def __init__(self):\n        self._grades = {}\n \n    def add_student(self, name):\n        self._grades[name] = []\n \n    def report_grade(self, name, score):\n        self._grades[name].append(score)\n \n",
      "content_length": 1370,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "146 \nChapter 5 Classes and Interfaces\n    def average_grade(self, name):\n        grades = self._grades[name]\n        return sum(grades) / len(grades)\nUsing the class is simple:\nbook = SimpleGradebook()\nbook.add_student('Isaac Newton')\nbook.report_grade('Isaac Newton', 90)\nbook.report_grade('Isaac Newton', 95)\nbook.report_grade('Isaac Newton', 85)\n \nprint(book.average_grade('Isaac Newton'))\n>>>\n90.0\nDictionaries and their related built-in types are so easy to use that \nthere’s a danger of overextending them to write brittle code. For \nexample, say that I want to extend the SimpleGradebook class to keep \na list of grades by subject, not just overall. I can do this by changing \nthe _grades dictionary to map student names (its keys) to yet another \ndictionary (its values). The innermost dictionary will map subjects \n(its keys) to a list of grades (its values). Here, I do this by using a \ndefaultdict instance for the inner dictionary to handle missing sub-\njects (see Item 17: “Prefer defaultdict Over setdefault to Handle Miss-\ning Items in Internal State” for background):\nfrom collections import defaultdict\n \nclass BySubjectGradebook:\n    def __init__(self):\n        self._grades = {}                       # Outer dict\n \n    def add_student(self, name):\n        self._grades[name] = defaultdict(list)  # Inner dict\nThis \nseems \nstraightforward \nenough. \nThe \nreport_grade \nand \naverage_grade methods gain quite a bit of complexity to deal with the \nmultilevel dictionary, but it’s seemingly manageable:\n    def report_grade(self, name, subject, grade):\n        by_subject = self._grades[name]\n        grade_list = by_subject[subject]\n        grade_list.append(grade)\n \n    def average_grade(self, name):\n        by_subject = self._grades[name]\n",
      "content_length": 1758,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": " \nItem 37: Compose Classes Instead of Nesting Built-in Types \n147\n        total, count = 0, 0\n        for grades in by_subject.values():\n            total += sum(grades)\n            count += len(grades)\n        return total / count\nUsing the class remains simple:\nbook = BySubjectGradebook()\nbook.add_student('Albert Einstein')\nbook.report_grade('Albert Einstein', 'Math', 75)\nbook.report_grade('Albert Einstein', 'Math', 65)\nbook.report_grade('Albert Einstein', 'Gym', 90)\nbook.report_grade('Albert Einstein', 'Gym', 95)\nprint(book.average_grade('Albert Einstein'))\n>>>\n81.25\nNow, imagine that the requirements change again. I also want to \ntrack the weight of each score toward the overall grade in the class \nso that midterm and final exams are more important than pop quiz-\nzes. One way to implement this feature is to change the innermost \n dictionary; instead of mapping subjects (its keys) to a list of grades \n (its values), I can use the tuple of (score, weight) in the values list:\nclass WeightedGradebook:\n    def __init__(self):\n        self._grades = {}\n \n    def add_student(self, name):\n        self._grades[name] = defaultdict(list)\n \n    def report_grade(self, name, subject, score, weight):\n        by_subject = self._grades[name]\n        grade_list = by_subject[subject]\n        grade_list.append((score, weight))\nAlthough the changes to report_grade seem simple—just make the \ngrade list store tuple instances—the average_grade method now has a \nloop within a loop and is difficult to read:\n    def average_grade(self, name):\n        by_subject = self._grades[name]\n \n        score_sum, score_count = 0, 0\n        for subject, scores in by_subject.items():\n            subject_avg, total_weight = 0, 0\n",
      "content_length": 1722,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "148 \nChapter 5 Classes and Interfaces\n            for score, weight in scores:\n                subject_avg += score * weight\n                total_weight += weight\n \n            score_sum += subject_avg / total_weight\n            score_count += 1\n \n        return score_sum / score_count\nUsing the class has also gotten more difficult. It’s unclear what all of \nthe numbers in the positional arguments mean:\nbook = WeightedGradebook()\nbook.add_student('Albert Einstein')\nbook.report_grade('Albert Einstein', 'Math', 75, 0.05)\nbook.report_grade('Albert Einstein', 'Math', 65, 0.15)\nbook.report_grade('Albert Einstein', 'Math', 70, 0.80)\nbook.report_grade('Albert Einstein', 'Gym', 100, 0.40)\nbook.report_grade('Albert Einstein', 'Gym', 85, 0.60)\nprint(book.average_grade('Albert Einstein'))\n>>>\n80.25\nWhen you see complexity like this, it’s time to make the leap from \nbuilt-in types like dictionaries, tuples, sets, and lists to a hierarchy of \nclasses.\nIn the grades example, at first I didn’t know I’d need to support \nweighted grades, so the complexity of creating classes seemed unwar-\nranted. Python’s built-in dictionary and tuple types made it easy to \nkeep going, adding layer after layer to the internal bookkeeping. But \nyou should avoid doing this for more than one level of nesting; using \ndictionaries that contain dictionaries makes your code hard to read \nby other programmers and sets you up for a maintenance nightmare.\nAs soon as you realize that your bookkeeping is getting complicated, \nbreak it all out into classes. You can then provide well-defined inter-\nfaces that better encapsulate your data. This approach also enables \nyou to create a layer of abstraction between your interfaces and your \nconcrete implementations.\nRefactoring to Classes\nThere are many approaches to refactoring (see Item 89: “Consider \nwarnings to Refactor and Migrate Usage” for another). In this case, \n",
      "content_length": 1903,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "I can start moving to classes at the bottom of the dependency tree: \na single grade. A class seems too heavyweight for such simple infor-\nmation. A tuple, though, seems appropriate because grades are \nimmutable. Here, I use the tuple of (score, weight) to track grades in \na list:\ngrades = []\ngrades.append((95, 0.45))\ngrades.append((85, 0.55))\ntotal = sum(score * weight for score, weight in grades)\ntotal_weight = sum(weight for _, weight in grades)\naverage_grade = total / total_weight\nI used _ (the underscore variable name, a Python convention for \nunused variables) to capture the first entry in each grade’s tuple and \nignore it when calculating the total_weight.\nThe problem with this code is that tuple instances are positional. For \nexample, if I want to associate more information with a grade, such \nas a set of notes from the teacher, I need to rewrite every usage of the \ntwo-tuple to be aware that there are now three items present instead \nof two, which means I need to use _ further to ignore certain indexes:\ngrades = []\ngrades.append((95, 0.45, 'Great job'))\ngrades.append((85, 0.55, 'Better next time'))\ntotal = sum(score * weight for score, weight, _ in grades)\ntotal_weight = sum(weight for _, weight, _ in grades)\naverage_grade = total / total_weight\nThis pattern of extending tuples longer and longer is similar to deep-\nening layers of dictionaries. As soon as you find yourself going longer \nthan a two-tuple, it’s time to consider another approach.\nThe namedtuple type in the collections built-in module does exactly \nwhat I need in this case: It lets me easily define tiny, immutable data \nclasses:\nfrom collections import namedtuple\n \nGrade = namedtuple('Grade', ('score', 'weight'))\nThese classes can be constructed with positional or keyword argu-\nments. The fields are accessible with named attributes. Having named \nattributes makes it easy to move from a namedtuple to a class later if \nthe requirements change again and I need to, say, support mutability \nor behaviors in the simple data containers.\n \nItem 37: Compose Classes Instead of Nesting Built-in Types \n149\n",
      "content_length": 2101,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "150 \nChapter 5 Classes and Interfaces\nLimitations of namedtuple\nAlthough namedtuple is useful in many circumstances, it’s import-\nant to understand when it can do more harm than good:\n \n■You can’t specify default argument values for namedtuple \nclasses. This makes them unwieldy when your data may have \nmany optional properties. If you find yourself using more than \na handful of attributes, using the built-in dataclasses module \nmay be a better choice.\n \n■The attribute values of namedtuple instances are still accessi-\nble using numerical indexes and iteration. Especially in exter-\nnalized APIs, this can lead to unintentional usage that makes \nit harder to move to a real class later. If you’re not in control \nof all of the usage of your namedtuple instances, it’s better to \nexplicitly define a new class.\nNext, I can write a class to represent a single subject that contains a \nset of grades:\nclass Subject:\n    def __init__(self):\n        self._grades = []\n \n    def report_grade(self, score, weight):\n        self._grades.append(Grade(score, weight))\n \n    def average_grade(self):\n        total, total_weight = 0, 0\n        for grade in self._grades:\n            total += grade.score * grade.weight\n            total_weight += grade.weight\n        return total / total_weight\nThen, I write a class to represent a set of subjects that are being stud-\nied by a single student:\nclass Student:\n    def __init__(self):\n        self._subjects = defaultdict(Subject)\n \n    def get_subject(self, name):\n        return self._subjects[name]\n \n",
      "content_length": 1545,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": " \nItem 37: Compose Classes Instead of Nesting Built-in Types \n151\n    def average_grade(self):\n        total, count = 0, 0\n        for subject in self._subjects.values():\n            total += subject.average_grade()\n            count += 1\n        return total / count\nFinally, I’d write a container for all of the students, keyed dynamically \nby their names:\nclass Gradebook:\n    def __init__(self):\n        self._students = defaultdict(Student)\n \n    def get_student(self, name):\n        return self._students[name]\nThe line count of these classes is almost double the previous imple-\nmentation’s size. But this code is much easier to read. The example \ndriving the classes is also more clear and extensible:\nbook = Gradebook()\nalbert = book.get_student('Albert Einstein')\nmath = albert.get_subject('Math')\nmath.report_grade(75, 0.05)\nmath.report_grade(65, 0.15)\nmath.report_grade(70, 0.80)\ngym = albert.get_subject('Gym')\ngym.report_grade(100, 0.40)\ngym.report_grade(85, 0.60)\nprint(albert.average_grade())\n>>>\n80.25\nIt would also be possible to write backward-compatible methods to \nhelp migrate usage of the old API style to the new hierarchy of objects.\nThings to Remember\n✦ Avoid making dictionaries with values that are dictionaries, long \ntuples, or complex nestings of other built-in types.\n✦ Use namedtuple for lightweight, immutable data containers before \nyou need the flexibility of a full class.\n✦ Move your bookkeeping code to using multiple classes when your \ninternal state dictionaries get complicated.\n",
      "content_length": 1521,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "152 \nChapter 5 Classes and Interfaces\nItem 38:  Accept Functions Instead of Classes for \nSimple Interfaces\nMany of Python’s built-in APIs allow you to customize behavior by \npassing in a function. These hooks are used by APIs to call back your \ncode while they execute. For example, the list type’s sort method \ntakes an optional key argument that’s used to determine each index’s \nvalue for sorting (see Item 14: “Sort by Complex Criteria Using the key \nParameter” for details). Here, I sort a list of names based on their \nlengths by providing the len built-in function as the key hook:\nnames = ['Socrates', 'Archimedes', 'Plato', 'Aristotle']\nnames.sort(key=len)\nprint(names)\n>>>\n['Plato', 'Socrates', 'Aristotle', 'Archimedes']\nIn other languages, you might expect hooks to be defined by an \nabstract class. In Python, many hooks are just stateless functions \nwith well-defined arguments and return values. Functions are ideal \nfor hooks because they are easier to describe and simpler to define \nthan classes. Functions work as hooks because Python has first-class \nfunctions: Functions and methods can be passed around and refer-\nenced like any other value in the language.\nFor example, say that I want to customize the behavior of the \ndefaultdict class (see Item 17: “Prefer defaultdict Over setdefault to \nHandle Missing Items in Internal State” for background). This data \nstructure allows you to supply a function that will be called with no \narguments each time a missing key is accessed. The function must \nreturn the default value that the missing key should have in the dic-\ntionary. Here, I define a hook that logs each time a key is missing and \nreturns 0 for the default value:\ndef log_missing():\n    print('Key added')\n    return 0\nGiven an initial dictionary and a set of desired increments, I can \ncause the log_missing function to run and print twice (for 'red' and \n'orange'):\nfrom collections import defaultdict\ncurrent = {'green': 12, 'blue': 3}\nincrements = [\n",
      "content_length": 1986,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": " \nItem 38: Accept Functions Instead of Classes for Simple Interfaces \n153\n    ('red', 5),\n    ('blue', 17),\n    ('orange', 9),\n]\nresult = defaultdict(log_missing, current)\nprint('Before:', dict(result))\nfor key, amount in increments:\n    result[key] += amount\nprint('After: ', dict(result))\n>>>\nBefore: {'green': 12, 'blue': 3}\nKey added\nKey added\nAfter:  {'green': 12, 'blue': 20, 'red': 5, 'orange': 9}\nSupplying functions like log_missing makes APIs easy to build and \ntest because it separates side effects from deterministic behavior. For \nexample, say I now want the default value hook passed to defaultdict \nto count the total number of keys that were missing. One way to \nachieve this is by using a stateful closure (see Item 21: “Know How \nClosures Interact with Variable Scope” for details). Here, I define a \nhelper function that uses such a closure as the default value hook:\ndef increment_with_report(current, increments):\n    added_count = 0\n \n    def missing():\n        nonlocal added_count  # Stateful closure\n        added_count += 1\n        return 0\n \n    result = defaultdict(missing, current)\n    for key, amount in increments:\n        result[key] += amount\n \n    return result, added_count\nRunning this function produces the expected result (2), even though \nthe defaultdict has no idea that the missing hook maintains state. \nAnother benefit of accepting simple functions for interfaces is that it’s \neasy to add functionality later by hiding state in a closure:\nresult, count = increment_with_report(current, increments)\nassert count == 2\n",
      "content_length": 1562,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "154 \nChapter 5 Classes and Interfaces\nThe problem with defining a closure for stateful hooks is that it’s \nharder to read than the stateless function example. Another approach \nis to define a small class that encapsulates the state you want to \ntrack:\nclass CountMissing:\n    def __init__(self):\n        self.added = 0\n \n    def missing(self):\n        self.added += 1\n        return 0\nIn other languages, you might expect that now defaultdict would \nhave to be modified to accommodate the interface of CountMissing. \nBut in Python, thanks to first-class functions, you can reference \nthe CountMissing.missing method directly on an object and pass it to \ndefaultdict as the default value hook. It’s trivial to have an object \ninstance’s method satisfy a function interface:\ncounter = CountMissing()\nresult = defaultdict(counter.missing, current)  # Method ref\nfor key, amount in increments:\n    result[key] += amount\nassert counter.added == 2\nUsing a helper class like this to provide the behavior of a stateful \nclosure is clearer than using the increment_with_report function, as \nabove. However, in isolation, it’s still not immediately obvious what the \npurpose of the CountMissing class is. Who constructs a CountMissing \nobject? Who calls the missing method? Will the class need other pub-\nlic methods to be added in the future? Until you see its usage with \ndefaultdict, the class is a mystery.\nTo clarify this situation, Python allows classes to define the __call__ \nspecial method. __call__ allows an object to be called just like a func-\ntion. It also causes the callable built-in function to return True for \nsuch an instance, just like a normal function or method. All objects \nthat can be executed in this manner are referred to as callables:\nclass BetterCountMissing:\n    def __init__(self):\n        self.added = 0\n \n    def __call__(self):\n        self.added += 1\n        return 0\n \n",
      "content_length": 1897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": " \nItem 39: Use @classmethod Polymorphism to Construct Objects \n155\ncounter = BetterCountMissing()\nassert counter() == 0\nassert callable(counter)\nHere, I use a BetterCountMissing instance as the default value hook for \na defaultdict to track the number of missing keys that were added:\ncounter = BetterCountMissing()\nresult = defaultdict(counter, current)  # Relies on __call__\nfor key, amount in increments:\n    result[key] += amount\nassert counter.added == 2\nThis is much clearer than the CountMissing.missing example. The \n__call__ method indicates that a class’s instances will be used some-\nwhere a function argument would also be suitable (like API hooks). It \ndirects new readers of the code to the entry point that’s responsible \nfor the class’s primary behavior. It provides a strong hint that the goal \nof the class is to act as a stateful closure.\nBest of all, defaultdict still has no view into what’s going on when \nyou use __call__. All that defaultdict requires is a function for the \ndefault value hook. Python provides many different ways to satisfy a \nsimple function interface, and you can choose the one that works best \nfor what you need to accomplish.\nThings to Remember\n✦ Instead of defining and instantiating classes, you can often simply \nuse functions for simple interfaces between components in Python.\n✦ References to functions and methods in Python are first class, \nmeaning they can be used in expressions (like any other type).\n✦ The __call__ special method enables instances of a class to be \ncalled like plain Python functions.\n✦ When you need a function to maintain state, consider defining a \nclass that provides the __call__ method instead of defining a state-\nful closure.\nItem 39:  Use @classmethod Polymorphism to Construct \nObjects Generically\nIn Python, not only do objects support polymorphism, but classes do \nas well. What does that mean, and what is it good for?\nPolymorphism enables multiple classes in a hierarchy to implement \ntheir own unique versions of a method. This means that many classes \n",
      "content_length": 2043,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "156 \nChapter 5 Classes and Interfaces\ncan fulfill the same interface or abstract base class while providing \ndifferent functionality (see Item 43: “Inherit from collections.abc for \nCustom Container Types”).\nFor example, say that I’m writing a MapReduce implementation, and \nI want a common class to represent the input data. Here, I define \nsuch a class with a read method that must be defined by subclasses:\nclass InputData:\n    def read(self):\n        raise NotImplementedError\nI also have a concrete subclass of InputData that reads data from a \nfile on disk:\nclass PathInputData(InputData):\n    def __init__(self, path):\n        super().__init__()\n        self.path = path\n \n    def read(self):\n        with open(self.path) as f:\n            return f.read()\nI could have any number of InputData subclasses, like PathInputData, \nand each of them could implement the standard interface for read to \nreturn the data to process. Other InputData subclasses could read \nfrom the network, decompress data transparently, and so on.\nI’d want a similar abstract interface for the MapReduce worker that \nconsumes the input data in a standard way:\nclass Worker:\n    def __init__(self, input_data):\n        self.input_data = input_data\n        self.result = None\n \n    def map(self):\n        raise NotImplementedError\n \n    def reduce(self, other):\n        raise NotImplementedError\nHere, I define a concrete subclass of Worker to implement the specific \nMapReduce function I want to apply—a simple newline counter:\nclass LineCountWorker(Worker):\n    def map(self):\n        data = self.input_data.read()\n        self.result = data.count('\\n')\n \n",
      "content_length": 1637,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": " \nItem 39: Use @classmethod Polymorphism to Construct Objects \n157\n    def reduce(self, other):\n        self.result += other.result\nIt may look like this implementation is going great, but I’ve reached the \nbiggest hurdle in all of this. What connects all of these pieces? I have \na nice set of classes with reasonable interfaces and abstractions, but \nthat’s only useful once the objects are constructed. What’s responsi-\nble for building the objects and orchestrating the MapReduce?\nThe simplest approach is to manually build and connect the objects \nwith some helper functions. Here, I list the contents of a directory and \nconstruct a PathInputData instance for each file it contains:\nimport os\n \ndef generate_inputs(data_dir):\n    for name in os.listdir(data_dir):\n        yield PathInputData(os.path.join(data_dir, name))\nNext, I create the LineCountWorker instances by using the InputData \ninstances returned by generate_inputs:\ndef create_workers(input_list):\n    workers = []\n    for input_data in input_list:\n        workers.append(LineCountWorker(input_data))\n    return workers\nI execute these Worker instances by fanning out the map step to multi-\nple threads (see Item 53: “Use Threads for Blocking I/O, Avoid for Par-\nallelism” for background). Then, I call reduce repeatedly to combine \nthe results into one final value:\nfrom threading import Thread\n \ndef execute(workers):\n    threads = [Thread(target=w.map) for w in workers]\n    for thread in threads: thread.start()\n    for thread in threads: thread.join()\n \n    first, *rest = workers\n    for worker in rest:\n        first.reduce(worker)\n    return first.result\n",
      "content_length": 1633,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "158 \nChapter 5 Classes and Interfaces\nFinally, I connect all the pieces together in a function to run each \nstep:\ndef mapreduce(data_dir):\n    inputs = generate_inputs(data_dir)\n    workers = create_workers(inputs)\n    return execute(workers)\nRunning this function on a set of test input files works great:\nimport os\nimport random\n \ndef write_test_files(tmpdir):\n    os.makedirs(tmpdir)\n    for i in range(100):\n        with open(os.path.join(tmpdir, str(i)), 'w') as f:\n            f.write('\\n' * random.randint(0, 100))\n \ntmpdir = 'test_inputs'\nwrite_test_files(tmpdir)\n \nresult = mapreduce(tmpdir)\nprint(f'There are {result} lines')\n>>>\nThere are 4360 lines\nWhat’s the problem? The huge issue is that the mapreduce func-\ntion is not generic at all. If I wanted to write another InputData or \nWorker subclass, I would also have to rewrite the generate_inputs, \ncreate_workers, and mapreduce functions to match.\nThis problem boils down to needing a generic way to construct objects. \nIn other languages, you’d solve this problem with constructor poly-\nmorphism, requiring that each InputData subclass provides a spe-\ncial constructor that can be used generically by the helper methods \nthat orchestrate the MapReduce (similar to the factory pattern). The \ntrouble is that Python only allows for the single constructor method \n__init__. It’s unreasonable to require every InputData subclass to \nhave a compatible constructor.\nThe best way to solve this problem is with class method polymor-\nphism. This is exactly like the instance method polymorphism I used \nfor InputData.read, except that it’s for whole classes instead of their \nconstructed objects.\n",
      "content_length": 1654,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": " \nItem 39: Use @classmethod Polymorphism to Construct Objects \n159\nLet me apply this idea to the MapReduce classes. Here, I extend the \nInputData class with a generic @classmethod that’s responsible for cre-\nating new InputData instances using a common interface:\nclass GenericInputData:\n    def read(self):\n        raise NotImplementedError\n \n    @classmethod\n    def generate_inputs(cls, config):\n        raise NotImplementedError\nI have generate_inputs take a dictionary with a set of configuration \nparameters that the GenericInputData concrete subclass needs to inter-\npret. Here, I use the config to find the directory to list for input files:\nclass PathInputData(GenericInputData):\n    ...\n \n    @classmethod\n    def generate_inputs(cls, config):\n        data_dir = config['data_dir']\n        for name in os.listdir(data_dir):\n            yield cls(os.path.join(data_dir, name))\nSimilarly, I can make the create_workers helper part of the \nGenericWorker class. Here, I use the input_class parameter, which \nmust be a subclass of GenericInputData, to generate the necessary \ninputs. I construct instances of the GenericWorker concrete subclass \nby using cls() as a generic constructor:\nclass GenericWorker:\n    def __init__(self, input_data):\n        self.input_data = input_data\n        self.result = None\n \n    def map(self):\n        raise NotImplementedError\n \n    def reduce(self, other):\n        raise NotImplementedError\n \n    @classmethod\n    def create_workers(cls, input_class, config):\n        workers = []\n        for input_data in input_class.generate_inputs(config):\n            workers.append(cls(input_data))\n        return workers\n",
      "content_length": 1653,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "160 \nChapter 5 Classes and Interfaces\nNote that the call to input_class.generate_inputs above is the \nclass polymorphism that I’m trying to show. You can also see how \ncreate_workers calling cls() provides an alternative way to construct \nGenericWorker objects besides using the __init__ method directly.\nThe effect on my concrete GenericWorker subclass is nothing more \nthan changing its parent class:\nclass LineCountWorker(GenericWorker):\n    ...\nFinally, I can rewrite the mapreduce function to be completely generic \nby calling create_workers:\ndef mapreduce(worker_class, input_class, config):\n    workers = worker_class.create_workers(input_class, config)\n    return execute(workers)\nRunning the new worker on a set of test files produces the same \nresult as the old implementation. The difference is that the mapreduce \nfunction requires more parameters so that it can operate generically:\nconfig = {'data_dir': tmpdir}\nresult = mapreduce(LineCountWorker, PathInputData, config)\nprint(f'There are {result} lines')\n>>>\nThere are 4360 lines\nNow, I can write other GenericInputData and GenericWorker sub-\nclasses as I wish, without having to rewrite any of the glue code.\nThings to Remember\n✦ Python only supports a single constructor per class: the __init__ \nmethod.\n✦ Use @classmethod to define alternative constructors for your classes.\n✦ Use class method polymorphism to provide generic ways to build \nand connect many concrete subclasses.\nItem 40: Initialize Parent Classes with super\nThe old, simple way to initialize a parent class from a child class \nis to directly call the parent class’s __init__ method with the child \ninstance:\nclass MyBaseClass:\n    def __init__(self, value):\n        self.value = value\n \n",
      "content_length": 1722,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": " \nItem 40: Initialize Parent Classes with super \n161\nclass MyChildClass(MyBaseClass):\n    def __init__(self):\n        MyBaseClass.__init__(self, 5)\nThis approach works fine for basic class hierarchies but breaks in \nmany cases.\nIf a class is affected by multiple inheritance (something to avoid in \ngeneral; see Item 41: “Consider Composing Functionality with Mix-in \nClasses”), calling the superclasses’ __init__ methods directly can \nlead to unpredictable behavior.\nOne problem is that the __init__ call order isn’t specified across all \nsubclasses. For example, here I define two parent classes that operate \non the instance’s value field:\nclass TimesTwo:\n    def __init__(self):\n        self.value *= 2\n \nclass PlusFive:\n    def __init__(self):\n        self.value += 5\nThis class defines its parent classes in one ordering:\nclass OneWay(MyBaseClass, TimesTwo, PlusFive):\n    def __init__(self, value):\n        MyBaseClass.__init__(self, value)\n        TimesTwo.__init__(self)\n        PlusFive.__init__(self)\nAnd constructing it produces a result that matches the parent class \nordering:\nfoo = OneWay(5)\nprint('First ordering value is (5 * 2) + 5 =', foo.value)\n>>>\nFirst ordering value is (5 * 2) + 5 = 15\nHere’s another class that defines the same parent classes but in a \ndifferent ordering (PlusFive followed by TimesTwo instead of the other \nway around):\nclass AnotherWay(MyBaseClass, PlusFive, TimesTwo):\n    def __init__(self, value):\n        MyBaseClass.__init__(self, value)\n        TimesTwo.__init__(self)\n        PlusFive.__init__(self)\n",
      "content_length": 1551,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "162 \nChapter 5 Classes and Interfaces\nHowever, I left the calls to the parent class constructors— \nPlusFive.__init__ and TimesTwo.__init__—in the same order as before, \nwhich means this class’s behavior doesn’t match the order of the par-\nent classes in its definition. The conflict here between the inheritance \nbase classes and the __init__ calls is hard to spot, which makes this \nespecially difficult for new readers of the code to understand:\nbar = AnotherWay(5)\nprint('Second ordering value is', bar.value)\n>>>\nSecond ordering value is 15\nAnother problem occurs with diamond inheritance. Diamond inher-\nitance happens when a subclass inherits from two separate classes \nthat have the same superclass somewhere in the hierarchy. Diamond \ninheritance causes the common superclass’s __init__ method to \nrun multiple times, causing unexpected behavior. For example, here \nI define two child classes that inherit from MyBaseClass:\nclass TimesSeven(MyBaseClass):\n    def __init__(self, value):\n        MyBaseClass.__init__(self, value)\n        self.value *= 7\n \nclass PlusNine(MyBaseClass):\n    def __init__(self, value):\n        MyBaseClass.__init__(self, value)\n        self.value += 9\nThen, I define a child class that inherits from both of these classes, \nmaking MyBaseClass the top of the diamond:\nclass ThisWay(TimesSeven, PlusNine):\n    def __init__(self, value):\n        TimesSeven.__init__(self, value)\n        PlusNine.__init__(self, value)\n \nfoo = ThisWay(5)\nprint('Should be (5 * 7) + 9 = 44 but is', foo.value)\n>>>\nShould be (5 * 7) + 9 = 44 but is 14\nThe call to the second parent class’s constructor, PlusNine.__init__, \ncauses self.value to be reset back to 5 when MyBaseClass.__init__ gets \ncalled a second time. That results in the calculation of self.value to be \n5 + 9 = 14, completely ignoring the effect of the TimesSeven.__init__ \n",
      "content_length": 1854,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": " \nItem 40: Initialize Parent Classes with super \n163\nconstructor. This behavior is surprising and can be very difficult to \ndebug in more complex cases.\nTo solve these problems, Python has the super built-in function and \nstandard method resolution order (MRO). super ensures that common \nsuperclasses in diamond hierarchies are run only once (for another \nexample, see Item 48: “Validate Subclasses with __init_subclass__”). \nThe MRO defines the ordering in which superclasses are initialized, \nfollowing an algorithm called C3 linearization.\nHere, I create a diamond-shaped class hierarchy again, but this time \nI use super to initialize the parent class:\nclass TimesSevenCorrect(MyBaseClass):\n    def __init__(self, value):\n        super().__init__(value)\n        self.value *= 7\n \nclass PlusNineCorrect(MyBaseClass):\n    def __init__(self, value):\n        super().__init__(value)\n        self.value += 9\nNow, the top part of the diamond, MyBaseClass.__init__, is run only a \nsingle time. The other parent classes are run in the order specified in \nthe class statement:\nclass GoodWay(TimesSevenCorrect, PlusNineCorrect):\n    def __init__(self, value):\n        super().__init__(value)\n \nfoo = GoodWay(5)\nprint('Should be 7 * (5 + 9) = 98 and is', foo.value)\n>>>\nShould be 7 * (5 + 9) = 98 and is 98\nThis \norder \nmay \nseem \nbackward \nat \nfirst. \nShouldn’t \nTimesSevenCorrect.__init__ have run first? Shouldn’t the result be \n(5 * 7) + 9 = 44? The answer is no. This ordering matches what the \nMRO defines for this class. The MRO ordering is available on a class \nmethod called mro:\nmro_str = '\\n'.join(repr(cls) for cls in GoodWay.mro())\nprint(mro_str)\n>>>\n<class '__main__.GoodWay'>\n<class '__main__.TimesSevenCorrect'>\n",
      "content_length": 1722,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "164 \nChapter 5 Classes and Interfaces\n<class '__main__.PlusNineCorrect'>\n<class '__main__.MyBaseClass'>\n<class 'object'>\nWhen I call GoodWay(5), it in turn calls TimesSevenCorrect.__init__, \nwhich calls PlusNineCorrect.__init__, which calls MyBaseClass.__\ninit__. Once this reaches the top of the diamond, all of the initializa-\ntion methods actually do their work in the opposite order from how \ntheir __init__ functions were called. MyBaseClass.__init__ assigns \nvalue to 5. PlusNineCorrect.__init__ adds 9 to make value equal 14. \nTimesSevenCorrect.__init__ multiplies it by 7 to make value equal 98.\nBesides making multiple inheritance robust, the call to super().\n__init__ \nis \nalso \nmuch \nmore \nmaintainable \nthan \ncalling \nMyBaseClass.__init__ directly from within the subclasses. I could \nlater rename MyBaseClass to something else or have TimesSevenCorrect \nand PlusNineCorrect inherit from another superclass without having \nto update their __init__ methods to match.\nThe super function can also be called with two parameters: first the \ntype of the class whose MRO parent view you’re trying to access, and \nthen the instance on which to access that view. Using these optional \nparameters within the constructor looks like this:\nclass ExplicitTrisect(MyBaseClass):\n    def __init__(self, value):\n        super(ExplicitTrisect, self).__init__(value)\n        self.value /= 3\nHowever, these parameters are not required for object instance ini-\ntialization. Python’s compiler automatically provides the correct \nparameters (__class__ and self) for you when super is called with \nzero arguments within a class definition. This means all three of \nthese usages are equivalent:\nclass AutomaticTrisect(MyBaseClass):\n    def __init__(self, value):\n        super(__class__, self).__init__(value)\n        self.value /= 3\n \nclass ImplicitTrisect(MyBaseClass):\n    def __init__(self, value):\n        super().__init__(value)\n        self.value /= 3\n \nassert ExplicitTrisect(9).value == 3\nassert AutomaticTrisect(9).value == 3\nassert ImplicitTrisect(9).value == 3\n",
      "content_length": 2059,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": " \nItem 41: Consider Composing Functionality with Mix-in Classes \n165\nThe only time you should provide parameters to super is in situa-\ntions where you need to access the specific functionality of a super-\nclass’s implementation from a child class (e.g., to wrap or reuse \nfunctionality).\nThings to Remember\n✦ Python’s standard method resolution order (MRO) solves the prob-\nlems of superclass initialization order and diamond inheritance.\n✦ Use the super built-in function with zero arguments to initialize \nparent classes.\nItem 41:  Consider Composing Functionality with \nMix-in Classes\nPython is an object-oriented language with built-in facilities for mak-\ning multiple inheritance tractable (see Item 40: “Initialize Parent \nClasses with super”). However, it’s better to avoid multiple inheritance \naltogether.\nIf you find yourself desiring the convenience and encapsulation that \ncome with multiple inheritance, but want to avoid the potential head-\naches, consider writing a mix-in instead. A mix-in is a class that \ndefines only a small set of additional methods for its child classes to \nprovide. Mix-in classes don’t define their own instance attributes nor \nrequire their __init__ constructor to be called.\nWriting mix-ins is easy because Python makes it trivial to inspect the \ncurrent state of any object, regardless of its type. Dynamic inspection \nmeans you can write generic functionality just once, in a mix-in, and \nit can then be applied to many other classes. Mix-ins can be com-\nposed and layered to minimize repetitive code and maximize reuse.\nFor example, say I want the ability to convert a Python object from its \nin-memory representation to a dictionary that’s ready for serializa-\ntion. Why not write this functionality generically so I can use it with \nall my classes?\nHere, I define an example mix-in that accomplishes this with a new \npublic method that’s added to any class that inherits from it:\nclass ToDictMixin:\n    def to_dict(self):\n        return self._traverse_dict(self.__dict__)\n",
      "content_length": 2019,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "166 \nChapter 5 Classes and Interfaces\nThe implementation details are straightforward and rely on dynamic \nattribute access using hasattr, dynamic type inspection with \nisinstance, and accessing the instance dictionary __dict__:\n    def _traverse_dict(self, instance_dict):\n        output = {}\n        for key, value in instance_dict.items():\n            output[key] = self._traverse(key, value)\n        return output\n \n    def _traverse(self, key, value):\n        if isinstance(value, ToDictMixin):\n            return value.to_dict()\n        elif isinstance(value, dict):\n            return self._traverse_dict(value)\n        elif isinstance(value, list):\n            return [self._traverse(key, i) for i in value]\n        elif hasattr(value, '__dict__'):\n            return self._traverse_dict(value.__dict__)\n        else:\n            return value\nHere, I define an example class that uses the mix-in to make a dictio-\nnary representation of a binary tree:\nclass BinaryTree(ToDictMixin):\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\nTranslating a large number of related Python objects into a dictionary \nbecomes easy:\ntree = BinaryTree(10,\n    left=BinaryTree(7, right=BinaryTree(9)),\n    right=BinaryTree(13, left=BinaryTree(11)))\nprint(tree.to_dict())\n>>>\n{'value': 10,\n 'left': {'value': 7,\n          'left': None,\n          'right': {'value': 9, 'left': None, 'right': None}},\n 'right': {'value': 13,\n           'left': {'value': 11, 'left': None, 'right': None},\n           'right': None}}\n",
      "content_length": 1586,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": " \nItem 41: Consider Composing Functionality with Mix-in Classes \n167\nThe best part about mix-ins is that you can make their generic func-\ntionality pluggable so behaviors can be overridden when required. For \nexample, here I define a subclass of BinaryTree that holds a reference \nto its parent. This circular reference would cause the default imple-\nmentation of ToDictMixin.to_dict to loop forever:\nclass BinaryTreeWithParent(BinaryTree):\n    def __init__(self, value, left=None,\n                 right=None, parent=None):\n        super().__init__(value, left=left, right=right)\n        self.parent = parent\nThe solution is to override the BinaryTreeWithParent._traverse method \nto only process values that matter, preventing cycles encountered by \nthe mix-in. Here, the _traverse override inserts the parent’s numeri-\ncal value and otherwise defers to the mix-in’s default implementation \nby using the super built-in function:\n    def _traverse(self, key, value):\n        if (isinstance(value, BinaryTreeWithParent) and\n                key == 'parent'):\n            return value.value  # Prevent cycles\n        else:\n            return super()._traverse(key, value)\nCalling BinaryTreeWithParent.to_dict works without issue because \nthe circular referencing properties aren’t followed:\nroot = BinaryTreeWithParent(10)\nroot.left = BinaryTreeWithParent(7, parent=root)\nroot.left.right = BinaryTreeWithParent(9, parent=root.left)\nprint(root.to_dict())\n>>>\n{'value': 10,\n 'left': {'value': 7,\n          'left': None,\n          'right': {'value': 9,\n                    'left': None,\n                    'right': None,\n                    'parent': 7},\n          'parent': 10},\n 'right': None,\n 'parent': None}\n",
      "content_length": 1708,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "168 \nChapter 5 Classes and Interfaces\nBy defining BinaryTreeWithParent._traverse, I’ve also enabled any \nclass that has an attribute of type BinaryTreeWithParent to automati-\ncally work with the ToDictMixin:\nclass NamedSubTree(ToDictMixin):\n    def __init__(self, name, tree_with_parent):\n        self.name = name\n        self.tree_with_parent = tree_with_parent\n \nmy_tree = NamedSubTree('foobar', root.left.right)\nprint(my_tree.to_dict())  # No infinite loop\n>>>\n{'name': 'foobar',\n 'tree_with_parent': {'value': 9,\n                      'left': None,\n                      'right': None,\n                      'parent': 7}}\nMix-ins can also be composed together. For example, say I want a \nmix-in that provides generic JSON serialization for any class. I can do \nthis by assuming that a class provides a to_dict method (which may \nor may not be provided by the ToDictMixin class):\nimport json\n \nclass JsonMixin:\n    @classmethod\n    def from_json(cls, data):\n        kwargs = json.loads(data)\n        return cls(**kwargs)\n \n    def to_json(self):\n        return json.dumps(self.to_dict())\nNote how the JsonMixin class defines both instance methods and class \nmethods. Mix-ins let you add either kind of behavior to  subclasses. \nIn this example, the only requirements of a JsonMixin subclass are \nproviding a to_dict method and taking keyword arguments for \nthe __init__ method (see Item 23: “Provide Optional Behavior with \n Keyword Arguments” for background).\nThis mix-in makes it simple to create hierarchies of utility classes \nthat can be serialized to and from JSON with little boilerplate. For \nexample, here I have a hierarchy of data classes representing parts of \na datacenter topology:\n",
      "content_length": 1699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": " \nItem 41: Consider Composing Functionality with Mix-in Classes \n169\nclass DatacenterRack(ToDictMixin, JsonMixin):\n    def __init__(self, switch=None, machines=None):\n        self.switch = Switch(**switch)\n        self.machines = [\n            Machine(**kwargs) for kwargs in machines]\n \nclass Switch(ToDictMixin, JsonMixin):\n    def __init__(self, ports=None, speed=None):\n        self.ports = ports\n        self.speed = speed\n \nclass Machine(ToDictMixin, JsonMixin):\n    def __init__(self, cores=None, ram=None, disk=None):\n        self.cores = cores\n        self.ram = ram\n        self.disk = disk\nSerializing these classes to and from JSON is simple. Here, I verify \nthat the data is able to be sent round-trip through serializing and \ndeserializing:\nserialized = \"\"\"{\n    \"switch\": {\"ports\": 5, \"speed\": 1e9},\n    \"machines\": [\n        {\"cores\": 8, \"ram\": 32e9, \"disk\": 5e12},\n        {\"cores\": 4, \"ram\": 16e9, \"disk\": 1e12},\n        {\"cores\": 2, \"ram\": 4e9, \"disk\": 500e9}\n    ]\n}\"\"\"\n \ndeserialized = DatacenterRack.from_json(serialized)\nroundtrip = deserialized.to_json()\nassert json.loads(serialized) == json.loads(roundtrip)\nWhen you use mix-ins like this, it’s fine if the class you apply \nJsonMixin to already inherits from JsonMixin higher up in the class \nhierarchy. The resulting class will behave the same way, thanks to \nthe behavior of super.\nThings to Remember\n✦ Avoid using multiple inheritance with instance attributes and \n__init__ if mix-in classes can achieve the same outcome.\n✦ Use pluggable behaviors at the instance level to provide per-class \ncustomization when mix-in classes may require it.\n",
      "content_length": 1621,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "170 \nChapter 5 Classes and Interfaces\n✦ Mix-ins can include instance methods or class methods, depending \non your needs.\n✦ Compose mix-ins to create complex functionality from simple \nbehaviors.\nItem 42: Prefer Public Attributes Over Private Ones\nIn Python, there are only two types of visibility for a class’s attributes: \npublic and private:\nclass MyObject:\n    def __init__(self):\n        self.public_field = 5\n        self.__private_field = 10\n \n    def get_private_field(self):\n        return self.__private_field\nPublic attributes can be accessed by anyone using the dot operator on \nthe object:\nfoo = MyObject()\nassert foo.public_field == 5\nPrivate fields are specified by prefixing an attribute’s name with a \ndouble underscore. They can be accessed directly by methods of the \ncontaining class:\nassert foo.get_private_field() == 10\nHowever, directly accessing private fields from outside the class raises \nan exception:\nfoo.__private_field\n>>>\nTraceback ...\nAttributeError: 'MyObject' object has no attribute \n¯'__private_field'\nClass methods also have access to private attributes because they are \ndeclared within the surrounding class block:\nclass MyOtherObject:\n    def __init__(self):\n        self.__private_field = 71\n \n    @classmethod\n    def get_private_field_of_instance(cls, instance):\n        return instance.__private_field\n \n",
      "content_length": 1348,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": " \nItem 42: Prefer Public Attributes Over Private Ones \n171\nbar = MyOtherObject()\nassert MyOtherObject.get_private_field_of_instance(bar) == 71\nAs you’d expect with private fields, a subclass can’t access its parent \nclass’s private fields:\nclass MyParentObject:\n    def __init__(self):\n        self.__private_field = 71\n \nclass MyChildObject(MyParentObject):\n    def get_private_field(self):\n        return self.__private_field\n \nbaz = MyChildObject()\nbaz.get_private_field()\n>>>\nTraceback ...\nAttributeError: 'MyChildObject' object has no attribute \n¯'_MyChildObject__private_field'\nThe private attribute behavior is implemented with a sim-\nple transformation of the attribute name. When the Python \ncompiler \nsees \nprivate \nattribute \naccess \nin \nmethods \nlike \nMyChildObject.get_private_field, it translates the __private_field \nattribute access to use the name _MyChildObject__private_field \ninstead. In the example above, __private_field is only defined in \nMyParentObject.__init__, which means the private attribute’s real \nname is _MyParentObject__private_field. Accessing the parent’s pri-\nvate attribute from the child class fails simply because the trans-\nformed attribute name doesn’t exist (_MyChildObject__private_field \ninstead of _MyParentObject__private_field).\nKnowing this scheme, you can easily access the private attributes \nof any class—from a subclass or externally—without asking for \npermission:\nassert baz._MyParentObject__private_field == 71\nIf you look in the object’s attribute dictionary, you can see that private \nattributes are actually stored with the names as they appear after the \ntransformation:\nprint(baz.__dict__)\n>>>\n{'_MyParentObject__private_field': 71}\n",
      "content_length": 1695,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "172 \nChapter 5 Classes and Interfaces\nWhy doesn’t the syntax for private attributes actually enforce strict \nvisibility? The simplest answer is one often-quoted motto of Python: \n“We are all consenting adults here.” What this means is that we don’t \nneed the language to prevent us from doing what we want to do. It’s \nour individual choice to extend functionality as we wish and to take \nresponsibility for the consequences of such a risk. Python program-\nmers believe that the benefits of being open—permitting unplanned \nextension of classes by default—outweigh the downsides.\nBeyond that, having the ability to hook language features like attri-\nbute access (see Item 47: “Use __getattr__, __getattribute__, and \n__setattr__ for Lazy Attributes”) enables you to mess around with the \ninternals of objects whenever you wish. If you can do that, what is the \nvalue of Python trying to prevent private attribute access otherwise?\nTo minimize damage from accessing internals unknowingly, Python \nprogrammers follow a naming convention defined in the style guide \n(see Item 2: “Follow the PEP 8 Style Guide”). Fields prefixed by a sin-\ngle underscore (like _protected_field) are protected by convention, \nmeaning external users of the class should proceed with caution.\nHowever, many programmers who are new to Python use private fields \nto indicate an internal API that shouldn’t be accessed by subclasses \nor externally:\nclass MyStringClass:\n    def __init__(self, value):\n        self.__value = value\n \n    def get_value(self):\n        return str(self.__value)\n \nfoo = MyStringClass(5)\nassert foo.get_value() == '5'\nThis is the wrong approach. Inevitably someone—maybe even \nyou—will want to subclass your class to add new behavior or to \nwork around deficiencies in existing methods (e.g., the way that \nMyStringClass.get_value always returns a string). By choosing pri-\nvate attributes, you’re only making subclass overrides and extensions \ncumbersome and brittle. Your potential subclassers will still access \nthe private fields when they absolutely need to do so:\nclass MyIntegerSubclass(MyStringClass):\n    def get_value(self):\n        return int(self._MyStringClass__value)\n \n",
      "content_length": 2184,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": " \nItem 42: Prefer Public Attributes Over Private Ones \n173\nfoo = MyIntegerSubclass('5')\nassert foo.get_value() == 5\nBut if the class hierarchy changes beneath you, these classes will \nbreak because the private attribute references are no longer valid. \nHere, the MyIntegerSubclass class’s immediate parent, MyStringClass, \nhas had another parent class added, called MyBaseClass:\nclass MyBaseClass:\n    def __init__(self, value):\n        self.__value = value\n \n    def get_value(self):\n        return self.__value\n \nclass MyStringClass(MyBaseClass):\n    def get_value(self):\n        return str(super().get_value())         # Updated\n \nclass MyIntegerSubclass(MyStringClass):\n    def get_value(self):\n        return int(self._MyStringClass__value)  # Not updated\nThe __value attribute is now assigned in the MyBaseClass parent class, \nnot the MyStringClass parent. This causes the private variable refer-\nence self._MyStringClass__value to break in MyIntegerSubclass:\nfoo = MyIntegerSubclass(5)\nfoo.get_value()\n>>>\nTraceback ...\nAttributeError: 'MyIntegerSubclass' object has no attribute \n¯'_MyStringClass__value'\nIn general, it’s better to err on the side of allowing subclasses to do \nmore by using protected attributes. Document each protected field \nand explain which fields are internal APIs available to subclasses and \nwhich should be left alone entirely. This is as much advice to other \nprogrammers as it is guidance for your future self on how to extend \nyour own code safely:\nclass MyStringClass:\n    def __init__(self, value):\n        # This stores the user-supplied value for the object.\n        # It should be coercible to a string. Once assigned in\n        # the object it should be treated as immutable.\n",
      "content_length": 1719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "174 \nChapter 5 Classes and Interfaces\n        self._value = value\n \n    ...\nThe only time to seriously consider using private attributes is when \nyou’re worried about naming conflicts with subclasses. This problem \noccurs when a child class unwittingly defines an attribute that was \nalready defined by its parent class:\nclass ApiClass:\n    def __init__(self):\n        self._value = 5\n \n    def get(self):\n        return self._value\n \nclass Child(ApiClass):\n    def __init__(self):\n        super().__init__()\n        self._value = 'hello'  # Conflicts\n \na = Child()\nprint(f'{a.get()} and {a._value} should be different')\n>>>\nhello and hello should be different\nThis is primarily a concern with classes that are part of a public \nAPI; the subclasses are out of your control, so you can’t refactor to \nfix the problem. Such a conflict is especially possible with attribute \nnames that are very common (like value). To reduce the risk of this \nissue occurring, you can use a private attribute in the parent class \nto ensure that there are no attribute names that overlap with child \nclasses:\nclass ApiClass:\n    def __init__(self):\n        self.__value = 5       # Double underscore\n \n    def get(self):\n        return self.__value    # Double underscore\n \nclass Child(ApiClass):\n    def __init__(self):\n        super().__init__()\n        self._value = 'hello'  # OK!\n \n",
      "content_length": 1367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": " \nItem 43: Inherit from collections.abc for Custom Container Types \n175\na = Child()\nprint(f'{a.get()} and {a._value} are different')\n>>>\n5 and hello are different\nThings to Remember\n✦ Private attributes aren’t rigorously enforced by the Python compiler.\n✦ Plan from the beginning to allow subclasses to do more with your \ninternal APIs and attributes instead of choosing to lock them out.\n✦ Use documentation of protected fields to guide subclasses instead of \ntrying to force access control with private attributes.\n✦ Only consider using private attributes to avoid naming conflicts \nwith subclasses that are out of your control.\nItem 43:  Inherit from collections.abc for Custom \nContainer Types\nMuch of programming in Python is defining classes that contain data \nand describing how such objects relate to each other. Every Python \nclass is a container of some kind, encapsulating attributes and func-\ntionality together. Python also provides built-in container types for \nmanaging data: lists, tuples, sets, and dictionaries.\nWhen you’re designing classes for simple use cases like sequences, \nit’s natural to want to subclass Python’s built-in list type directly. \nFor example, say I want to create my own custom list type that has \nadditional methods for counting the frequency of its members:\nclass FrequencyList(list):\n    def __init__(self, members):\n        super().__init__(members)\n \n    def frequency(self):\n        counts = {}\n        for item in self:\n            counts[item] = counts.get(item, 0) + 1\n        return counts\nBy subclassing list, I get all of list’s standard functionality and pre-\nserve the semantics familiar to all Python programmers. I can define \nadditional methods to provide any custom behaviors that I need:\nfoo = FrequencyList(['a', 'b', 'a', 'c', 'b', 'a', 'd'])\nprint('Length is', len(foo))\n",
      "content_length": 1833,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "176 \nChapter 5 Classes and Interfaces\nfoo.pop()\nprint('After pop:', repr(foo))\nprint('Frequency:', foo.frequency())\n>>>\nLength is 7\nAfter pop: ['a', 'b', 'a', 'c', 'b', 'a']\nFrequency: {'a': 3, 'b': 2, 'c': 1}\nNow, imagine that I want to provide an object that feels like a list \nand allows indexing but isn’t a list subclass. For example, say that \nI want to provide sequence semantics (like list or tuple) for a binary \ntree class:\nclass BinaryNode:\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\nHow do you make this class act like a sequence type? Python imple-\nments its container behaviors with instance methods that have spe-\ncial names. When you access a sequence item by index:\nbar = [1, 2, 3]\nbar[0]\nit will be interpreted as:\nbar.__getitem__(0)\nTo make the BinaryNode class act like a sequence, you can provide \na custom implementation of __getitem__ (often pronounced “dunder \ngetitem” as an abbreviation for “double underscore getitem”) that tra-\nverses the object tree depth first:\nclass IndexableNode(BinaryNode):\n    def _traverse(self):\n        if self.left is not None:\n            yield from self.left._traverse()\n        yield self\n        if self.right is not None:\n            yield from self.right._traverse()\n \n    def __getitem__(self, index):\n        for i, item in enumerate(self._traverse()):\n            if i == index:\n                return item.value\n        raise IndexError(f'Index {index} is out of range')\n",
      "content_length": 1527,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": " \nItem 43: Inherit from collections.abc for Custom Container Types \n177\nYou can construct your binary tree as usual:\ntree = IndexableNode(\n    10,\n    left=IndexableNode(\n        5,\n        left=IndexableNode(2),\n        right=IndexableNode(\n            6,\n            right=IndexableNode(7))),\n    right=IndexableNode(\n        15,\n        left=IndexableNode(11)))\nBut you can also access it like a list in addition to being able to tra-\nverse the tree with the left and right attributes:\nprint('LRR is', tree.left.right.right.value)\nprint('Index 0 is', tree[0])\nprint('Index 1 is', tree[1])\nprint('11 in the tree?', 11 in tree)\nprint('17 in the tree?', 17 in tree)\nprint('Tree is', list(tree))\n>>>\nLRR is 7\nIndex 0 is 2\nIndex 1 is 5\n11 in the tree? True\n17 in the tree? False\nTree is [2, 5, 6, 7, 10, 11, 15]\nThe problem is that implementing __getitem__ isn’t enough to provide \nall of the sequence semantics you’d expect from a list instance:\nlen(tree)\n>>>\nTraceback ...\nTypeError: object of type 'IndexableNode' has no len()\nThe len built-in function requires another special method, named \n__len__, that must have an implementation for a custom sequence \ntype:\nclass SequenceNode(IndexableNode):\n    def __len__(self):\n        for count, _ in enumerate(self._traverse(), 1):\n            pass\n        return count\n",
      "content_length": 1317,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "178 \nChapter 5 Classes and Interfaces\ntree = SequenceNode(\n    10,\n    left=SequenceNode(\n        5,\n        left=SequenceNode(2),\n        right=SequenceNode(\n            6,\n            right=SequenceNode(7))),\n    right=SequenceNode(\n        15,\n        left=SequenceNode(11))\n)\n \nprint('Tree length is', len(tree))\n>>>\nTree length is 7\nUnfortunately, this still isn’t enough for the class to fully be a valid \nsequence. Also missing are the count and index methods that a \nPython programmer would expect to see on a sequence like list or \ntuple. It turns out that defining your own container types is much \nharder than it seems.\nTo avoid this difficulty throughout the Python universe, the built-in \ncollections.abc module defines a set of abstract base classes that \nprovide all of the typical methods for each container type. When you \nsubclass from these abstract base classes and forget to implement \nrequired methods, the module tells you something is wrong:\nfrom collections.abc import Sequence\n \nclass BadType(Sequence):\n    pass\n \nfoo = BadType()\n>>>\nTraceback ...\nTypeError: Can't instantiate abstract class BadType with \n¯abstract methods __getitem__, __len__\nWhen you do implement all the methods required by an abstract base \nclass from collections.abc, as I did above with SequenceNode, it pro-\nvides all of the additional methods, like index and count, for free:\nclass BetterNode(SequenceNode, Sequence):\n    pass\n \n",
      "content_length": 1432,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": " \nItem 43: Inherit from collections.abc for Custom Container Types \n179\ntree = BetterNode(\n    10,\n    left=BetterNode(\n        5,\n        left=BetterNode(2),\n        right=BetterNode(\n            6,\n            right=BetterNode(7))),\n    right=BetterNode(\n        15,\n        left=BetterNode(11))\n)\n \nprint('Index of 7 is', tree.index(7))\nprint('Count of 10 is', tree.count(10))\n>>>\nIndex of 7 is 3\nCount of 10 is 1\nThe benefit of using these abstract base classes is even greater for \nmore complex container types such as Set and MutableMapping, which \nhave a large number of special methods that need to be implemented \nto match Python conventions.\nBeyond the collections.abc module, Python uses a variety of special \nmethods for object comparisons and sorting, which may be provided \nby container classes and non-container classes alike (see Item 73: \n“Know How to Use heapq for Priority Queues” for an example).\nThings to Remember\n✦ Inherit directly from Python’s container types (like list or dict) for \nsimple use cases.\n✦ Beware of the large number of methods required to implement cus-\ntom container types correctly.\n✦ Have your custom container types inherit from the interfaces \ndefined in collections.abc to ensure that your classes match \nrequired interfaces and behaviors.\n",
      "content_length": 1287,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "6\nMetaclasses and \nAttributes\nMetaclasses are often mentioned in lists of Python’s features, but \nfew understand what they accomplish in practice. The name meta-\nclass vaguely implies a concept above and beyond a class. Simply put, \nmetaclasses let you intercept Python’s class statement and provide \nspecial behavior each time a class is defined.\nSimilarly mysterious and powerful are Python’s built-in features for \ndynamically customizing attribute accesses. Along with Python’s \nobject-oriented constructs, these facilities provide wonderful tools to \nease the transition from simple classes to complex ones.\nHowever, with these powers come many pitfalls. Dynamic attributes \nenable you to override objects and cause unexpected side effects. \nMetaclasses can create extremely bizarre behaviors that are unap-\nproachable to newcomers. It’s important that you follow the rule of \nleast surprise and only use these mechanisms to implement well- \nunderstood idioms.\nItem 44:  Use Plain Attributes Instead of Setter and \nGetter Methods\nProgrammers coming to Python from other languages may naturally \ntry to implement explicit getter and setter methods in their classes:\nclass OldResistor:\n    def __init__(self, ohms):\n        self._ohms = ohms\n \n    def get_ohms(self):\n        return self._ohms\n \n    def set_ohms(self, ohms):\n        self._ohms = ohms\n",
      "content_length": 1355,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "182 \nChapter 6 Metaclasses and Attributes\nUsing these setters and getters is simple, but it’s not Pythonic:\nr0 = OldResistor(50e3)\nprint('Before:', r0.get_ohms())\nr0.set_ohms(10e3)\nprint('After: ', r0.get_ohms())\n>>>\nBefore: 50000.0\nAfter:  10000.0\nSuch methods are especially clumsy for operations like incrementing \nin place:\nr0.set_ohms(r0.get_ohms() - 4e3)\nassert r0.get_ohms() == 6e3\nThese utility methods do, however, help define the interface for \na class, making it easier to encapsulate functionality, validate usage, \nand define boundaries. Those are important goals when designing a \nclass to ensure that you don’t break callers as the class evolves over \ntime.\nIn Python, however, you never need to implement explicit setter or \ngetter methods. Instead, you should always start your implementa-\ntions with simple public attributes, as I do here:\nclass Resistor:\n    def __init__(self, ohms):\n        self.ohms = ohms\n        self.voltage = 0\n        self.current = 0\n \nr1 = Resistor(50e3)\nr1.ohms = 10e3\nThese attributes make operations like incrementing in place natural \nand clear:\nr1.ohms += 5e3\nLater, if I decide I need special behavior when an attribute is set, I \ncan migrate to the @property decorator (see Item 26: “Define Function \nDecorators with functools.wraps” for background) and its correspond-\ning setter attribute. Here, I define a new subclass of Resistor that \nlets me vary the current by assigning the voltage property. Note that \nin order for this code to work properly, the names of both the setter \nand the getter methods must match the intended property name:\nclass VoltageResistance(Resistor):\n    def __init__(self, ohms):\n",
      "content_length": 1662,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": " \nItem 44: Use Plain Attributes Instead of Setter and Getter Methods \n183\n        super().__init__(ohms)\n        self._voltage = 0\n \n    @property\n    def voltage(self):\n        return self._voltage\n \n    @voltage.setter\n    def voltage(self, voltage):\n        self._voltage = voltage\n        self.current = self._voltage / self.ohms\nNow, assigning the voltage property will run the voltage setter \nmethod, which in turn will update the current attribute of the object \nto match:\nr2 = VoltageResistance(1e3)\nprint(f'Before: {r2.current:.2f} amps')\nr2.voltage = 10\nprint(f'After:  {r2.current:.2f} amps')\n>>>\nBefore: 0.00 amps\nAfter:  0.01 amps\nSpecifying a setter on a property also enables me to perform type \nchecking and validation on values passed to the class. Here, I define a \nclass that ensures all resistance values are above zero ohms:\nclass BoundedResistance(Resistor):\n    def __init__(self, ohms):\n        super().__init__(ohms)\n \n    @property\n    def ohms(self):\n        return self._ohms\n \n    @ohms.setter\n    def ohms(self, ohms):\n        if ohms <= 0:\n            raise ValueError(f'ohms must be > 0; got {ohms}')\n        self._ohms = ohms\nAssigning an invalid resistance to the attribute now raises an \nexception:\nr3 = BoundedResistance(1e3)\nr3.ohms = 0\n",
      "content_length": 1274,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "184 \nChapter 6 Metaclasses and Attributes\n>>>\nTraceback ...\nValueError: ohms must be > 0; got 0\nAn exception is also raised if I pass an invalid value to the constructor:\nBoundedResistance(-5)\n>>>\nTraceback ...\nValueError: ohms must be > 0; got -5\nThis \nhappens \nbecause \nBoundedResistance.__init__ \ncalls \nResistor.__init__, which assigns self.ohms = -5. That assignment \ncauses the @ohms.setter method from BoundedResistance to be called, \nand it immediately runs the validation code before object construc-\ntion has completed.\nI can even use @property to make attributes from parent classes \nimmutable:\nclass FixedResistance(Resistor):\n    def __init__(self, ohms):\n        super().__init__(ohms)\n \n    @property\n    def ohms(self):\n        return self._ohms\n \n    @ohms.setter\n    def ohms(self, ohms):\n        if hasattr(self, '_ohms'):\n            raise AttributeError(\"Ohms is immutable\")\n        self._ohms = ohms\nTrying to assign to the property after construction raises an exception:\nr4 = FixedResistance(1e3)\nr4.ohms = 2e3\n>>>\nTraceback ...\nAttributeError: Ohms is immutable\nWhen you use @property methods to implement setters and getters, \nbe sure that the behavior you implement is not surprising. For exam-\nple, don’t set other attributes in getter property methods:\nclass MysteriousResistor(Resistor):\n    @property\n    def ohms(self):\n",
      "content_length": 1352,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": " \nItem 44: Use Plain Attributes Instead of Setter and Getter Methods \n185\n        self.voltage = self._ohms * self.current\n        return self._ohms\n \n    @ohms.setter\n    def ohms(self, ohms):\n        self._ohms = ohms\nSetting other attributes in getter property methods leads to extremely \nbizarre behavior:\nr7 = MysteriousResistor(10)\nr7.current = 0.01\nprint(f'Before: {r7.voltage:.2f}')\nr7.ohms\nprint(f'After:  {r7.voltage:.2f}')\n>>>\nBefore: 0.00\nAfter:  0.10\nThe best policy is to modify only related object state in @property.setter \nmethods. Be sure to also avoid any other side effects that the caller \nmay not expect beyond the object, such as importing modules dynam-\nically, running slow helper functions, doing I/O, or making expensive \ndatabase queries. Users of a class will expect its attributes to be like \nany other Python object: quick and easy. Use normal methods to do \nanything more complex or slow.\nThe biggest shortcoming of @property is that the methods for an attri-\nbute can only be shared by subclasses. Unrelated classes can’t share \nthe same implementation. However, Python also supports descriptors \n(see Item 46: “Use Descriptors for Reusable @property Methods”) that \nenable reusable property logic and many other use cases.\nThings to Remember\n✦ Define new class interfaces using simple public attributes and avoid \ndefining setter and getter methods.\n✦ Use @property to define special behavior when attributes are \naccessed on your objects, if necessary.\n✦ Follow the rule of least surprise and avoid odd side effects in your \n@property methods.\n✦ Ensure that @property methods are fast; for slow or complex work—\nespecially involving I/O or causing side effects—use normal meth-\nods instead.\n",
      "content_length": 1726,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "186 \nChapter 6 Metaclasses and Attributes\nItem 45:  Consider @property Instead of Refactoring \nAttributes\nThe built-in @property decorator makes it easy for simple accesses \nof an instance’s attributes to act smarter (see Item 44: “Use Plain \nAttributes Instead of Setter and Getter Methods”). One advanced but \ncommon use of @property is transitioning what was once a simple \nnumerical attribute into an on-the-fly calculation. This is extremely \nhelpful because it lets you migrate all existing usage of a class to have \nnew behaviors without requiring any of the call sites to be rewritten \n(which is especially important if there’s calling code that you don’t \ncontrol). @property also provides an important stopgap for improving \ninterfaces over time.\nFor example, say that I want to implement a leaky bucket quota using \nplain Python objects. Here, the Bucket class represents how much \nquota remains and the duration for which the quota will be available:\nfrom datetime import datetime, timedelta\n \nclass Bucket:\n    def __init__(self, period):\n        self.period_delta = timedelta(seconds=period)\n        self.reset_time = datetime.now()\n        self.quota = 0\n \n    def __repr__(self):\n        return f'Bucket(quota={self.quota})'\nThe leaky bucket algorithm works by ensuring that, whenever the \nbucket is filled, the amount of quota does not carry over from one \nperiod to the next:\ndef fill(bucket, amount):\n    now = datetime.now()\n    if (now - bucket.reset_time) > bucket.period_delta:\n        bucket.quota = 0\n        bucket.reset_time = now\n    bucket.quota += amount\nEach time a quota consumer wants to do something, it must first \nensure that it can deduct the amount of quota it needs to use:\ndef deduct(bucket, amount):\n    now = datetime.now()\n    if (now - bucket.reset_time) > bucket.period_delta:\n        return False  # Bucket hasn't been filled this period\n    if bucket.quota - amount < 0:\n        return False  # Bucket was filled, but not enough\n",
      "content_length": 1976,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": " \nItem 45: Consider @property Instead of Refactoring Attributes \n187\n    bucket.quota -= amount\n    return True       # Bucket had enough, quota consumed\nTo use this class, first I fill the bucket up:\nbucket = Bucket(60)\nfill(bucket, 100)\nprint(bucket)\n>>>\nBucket(quota=100)\nThen, I deduct the quota that I need:\nif deduct(bucket, 99):\n    print('Had 99 quota')\nelse:\n    print('Not enough for 99 quota')\nprint(bucket)\n>>>\nHad 99 quota\nBucket(quota=1)\nEventually, I’m prevented from making progress because I try to \ndeduct more quota than is available. In this case, the bucket’s quota \nlevel remains unchanged:\nif deduct(bucket, 3):\n    print('Had 3 quota')\nelse:\n    print('Not enough for 3 quota')\nprint(bucket)\n>>>\nNot enough for 3 quota\nBucket(quota=1)\nThe problem with this implementation is that I never know what \nquota level the bucket started with. The quota is deducted over the \ncourse of the period until it reaches zero. At that point, deduct will \nalways return False until the bucket is refilled. When that happens, it \nwould be useful to know whether callers to deduct are being blocked \nbecause the Bucket ran out of quota or because the Bucket never had \nquota during this period in the first place.\nTo fix this, I can change the class to keep track of the max_quota \nissued in the period and the quota_consumed in the period:\nclass NewBucket:\n    def __init__(self, period):\n        self.period_delta = timedelta(seconds=period)\n",
      "content_length": 1450,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "188 \nChapter 6 Metaclasses and Attributes\n        self.reset_time = datetime.now()\n        self.max_quota = 0\n        self.quota_consumed = 0\n \n    def __repr__(self):\n        return (f'NewBucket(max_quota={self.max_quota}, '\n                f'quota_consumed={self.quota_consumed})')\nTo match the previous interface of the original Bucket class, I use a \n@property method to compute the current level of quota on-the-fly \nusing these new attributes:\n    @property\n    def quota(self):\n        return self.max_quota - self.quota_consumed\nWhen the quota attribute is assigned, I take special action to be com-\npatible with the current usage of the class by the fill and deduct \nfunctions:\n    @quota.setter\n    def quota(self, amount):\n        delta = self.max_quota - amount\n        if amount == 0:\n            # Quota being reset for a new period\n            self.quota_consumed = 0\n            self.max_quota = 0\n        elif delta < 0:\n            # Quota being filled for the new period\n            assert self.quota_consumed == 0\n            self.max_quota = amount\n        else:\n            # Quota being consumed during the period\n            assert self.max_quota >= self.quota_consumed\n            self.quota_consumed += delta\nRerunning the demo code from above produces the same results:\nbucket = NewBucket(60)\nprint('Initial', bucket)\nfill(bucket, 100)\nprint('Filled', bucket)\n \nif deduct(bucket, 99):\n    print('Had 99 quota')\nelse:\n    print('Not enough for 99 quota')\n \n",
      "content_length": 1483,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": " \nItem 45: Consider @property Instead of Refactoring Attributes \n189\nprint('Now', bucket)\n \nif deduct(bucket, 3):\n    print('Had 3 quota')\nelse:\n    print('Not enough for 3 quota')\n \nprint('Still', bucket)\n>>>\nInitial NewBucket(max_quota=0, quota_consumed=0)\nFilled NewBucket(max_quota=100, quota_consumed=0)\nHad 99 quota\nNow NewBucket(max_quota=100, quota_consumed=99)\nNot enough for 3 quota\nStill NewBucket(max_quota=100, quota_consumed=99)\nThe best part is that the code using Bucket.quota doesn’t have to \nchange or know that the class has changed. New usage of Bucket can \ndo the right thing and access max_quota and quota_consumed directly.\nI especially like @property because it lets you make incremental prog-\nress toward a better data model over time. Reading the Bucket exam-\nple above, you may have thought that fill and deduct should have \nbeen implemented as instance methods in the first place. Although \nyou’re probably right (see Item 37: “Compose Classes Instead of \n Nesting Many Levels of Built-in Types”), in practice there are many \nsituations in which objects start with poorly defined interfaces or act \nas dumb data containers. This happens when code grows over time, \nscope increases, multiple authors contribute without anyone consid-\nering long-term hygiene, and so on.\n@property is a tool to help you address problems you’ll come across in \nreal-world code. Don’t overuse it. When you find yourself repeatedly \nextending @property methods, it’s probably time to refactor your class \ninstead of further paving over your code’s poor design.\nThings to Remember\n✦ Use @property to give existing instance attributes new functionality.\n✦ Make incremental progress toward better data models by using \n@property.\n✦ Consider refactoring a class and all call sites when you find yourself \nusing @property too heavily.\n",
      "content_length": 1836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "190 \nChapter 6 Metaclasses and Attributes\nItem 46:  Use Descriptors for Reusable @property \nMethods\nThe big problem with the @property built-in (see Item 44: “Use \nPlain Attributes Instead of Setter and Getter Methods” and Item 45: \n “Consider @property Instead of Refactoring Attributes”) is reuse. The \nmethods it decorates can’t be reused for multiple attributes of the \nsame class. They also can’t be reused by unrelated classes.\nFor example, say I want a class to validate that the grade received by \na student on a homework assignment is a percentage:\nclass Homework:\n    def __init__(self):\n        self._grade = 0\n \n    @property\n    def grade(self):\n        return self._grade\n \n    @grade.setter\n    def grade(self, value):\n        if not (0 <= value <= 100):\n            raise ValueError(\n                'Grade must be between 0 and 100')\n        self._grade = value\nUsing @property makes this class easy to use:\ngalileo = Homework()\ngalileo.grade = 95\nSay that I also want to give the student a grade for an exam, where \nthe exam has multiple subjects, each with a separate grade:\nclass Exam:\n    def __init__(self):\n        self._writing_grade = 0\n        self._math_grade = 0\n \n    @staticmethod\n    def _check_grade(value):\n        if not (0 <= value <= 100):\n            raise ValueError(\n                'Grade must be between 0 and 100')\n",
      "content_length": 1357,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": " \nItem 46: Use Descriptors for Reusable @property Methods \n191\nThis quickly gets tedious. For each section of the exam I need to add a \nnew @property and related validation:\n    @property\n    def writing_grade(self):\n        return self._writing_grade\n \n    @writing_grade.setter\n    def writing_grade(self, value):\n        self._check_grade(value)\n        self._writing_grade = value\n \n    @property\n    def math_grade(self):\n        return self._math_grade\n \n    @math_grade.setter\n    def math_grade(self, value):\n        self._check_grade(value)\n        self._math_grade = value\nAlso, this approach is not general. If I want to reuse this percentage \nvalidation in other classes beyond homework and exams, I’ll need to \nwrite the @property boilerplate and _check_grade method over and \nover again.\nThe better way to do this in Python is to use a descriptor. The descrip-\ntor protocol defines how attribute access is interpreted by the lan-\nguage. A descriptor class can provide __get__ and __set__ methods \nthat let you reuse the grade validation behavior without boilerplate. \nFor this purpose, descriptors are also better than mix-ins (see Item \n41: “Consider Composing Functionality with Mix-in Classes”) because \nthey let you reuse the same logic for many different attributes in a \nsingle class.\nHere, I define a new class called Exam with class attributes that are \nGrade instances. The Grade class implements the descriptor protocol:\nclass Grade:\n    def __get__(self, instance, instance_type):\n        ...\n \n    def __set__(self, instance, value):\n        ...\n \n",
      "content_length": 1574,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "192 \nChapter 6 Metaclasses and Attributes\nclass Exam:\n    # Class attributes\n    math_grade = Grade()\n    writing_grade = Grade()\n    science_grade = Grade()\nBefore I explain how the Grade class works, it’s important to under-\nstand what Python will do when such descriptor attributes are \naccessed on an Exam instance. When I assign a property:\nexam = Exam()\nexam.writing_grade = 40\nit is interpreted as:\nExam.__dict__['writing_grade'].__set__(exam, 40)\nWhen I retrieve a property:\nexam.writing_grade\nit is interpreted as:\nExam.__dict__['writing_grade'].__get__(exam, Exam)\nWhat drives this behavior is the __getattribute__ method of object \n(see Item 47: “Use __getattr__, __getattribute__, and __setattr__ \nfor Lazy Attributes”). In short, when an Exam instance doesn’t have an \nattribute named writing_grade, Python falls back to the Exam class’s \nattribute instead. If this class attribute is an object that has __get__ \nand __set__ methods, Python assumes that you want to follow the \ndescriptor protocol.\nKnowing this behavior and how I used @property for grade validation \nin the Homework class, here’s a reasonable first attempt at implement-\ning the Grade descriptor:\nclass Grade:\n    def __init__(self):\n        self._value = 0\n \n    def __get__(self, instance, instance_type):\n        return self._value\n \n    def __set__(self, instance, value):\n        if not (0 <= value <= 100):\n            raise ValueError(\n                'Grade must be between 0 and 100')\n        self._value = value\n",
      "content_length": 1503,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": " \nItem 46: Use Descriptors for Reusable @property Methods \n193\nUnfortunately, this is wrong and results in broken behavior. Access-\ning multiple attributes on a single Exam instance works as expected:\nclass Exam:\n    math_grade = Grade()\n    writing_grade = Grade()\n    science_grade = Grade()\n \nfirst_exam = Exam()\nfirst_exam.writing_grade = 82\nfirst_exam.science_grade = 99\nprint('Writing', first_exam.writing_grade)\nprint('Science', first_exam.science_grade)\n>>>\nWriting 82\nScience 99\nBut accessing these attributes on multiple Exam instances causes \nunexpected behavior:\nsecond_exam = Exam()\nsecond_exam.writing_grade = 75\nprint(f'Second {second_exam.writing_grade} is right')\nprint(f'First  {first_exam.writing_grade} is wrong; '\n      f'should be 82')\n>>>\nSecond 75 is right\nFirst  75 is wrong; should be 82\nThe problem is that a single Grade instance is shared across all Exam \ninstances for the class attribute writing_grade. The Grade instance for \nthis attribute is constructed once in the program lifetime, when the \nExam class is first defined, not each time an Exam instance is created.\nTo solve this, I need the Grade class to keep track of its value for each \nunique Exam instance. I can do this by saving the per-instance state \nin a dictionary:\nclass Grade:\n    def __init__(self):\n        self._values = {}\n \n    def __get__(self, instance, instance_type):\n        if instance is None:\n            return self\n        return self._values.get(instance, 0)\n \n",
      "content_length": 1475,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "194 \nChapter 6 Metaclasses and Attributes\n    def __set__(self, instance, value):\n        if not (0 <= value <= 100):\n            raise ValueError(\n                'Grade must be between 0 and 100')\n        self._values[instance] = value\nThis implementation is simple and works well, but there’s still one \ngotcha: It leaks memory. The _values dictionary holds a reference to \nevery instance of Exam ever passed to __set__ over the lifetime of the \nprogram. This causes instances to never have their reference count \ngo to zero, preventing cleanup by the garbage collector (see Item 81: \n“Use tracemalloc to Understand Memory Usage and Leaks” for how to \ndetect this type of problem).\nTo fix this, I can use Python’s weakref built-in module. This module \nprovides a special class called WeakKeyDictionary that can take the \nplace of the simple dictionary used for _values. The unique behavior \nof WeakKeyDictionary is that it removes Exam instances from its set of \nitems when the Python runtime knows it’s holding the instance’s last \nremaining reference in the program. Python does the bookkeeping for \nme and ensures that the _values dictionary will be empty when all \nExam instances are no longer in use:\nfrom weakref import WeakKeyDictionary\n \nclass Grade:\n    def __init__(self):\n        self._values = WeakKeyDictionary()\n \n    def __get__(self, instance, instance_type):\n        ...\n \n    def __set__(self, instance, value):\n        ...\nUsing this implementation of the Grade descriptor, everything works \nas expected:\nclass Exam:\n    math_grade = Grade()\n    writing_grade = Grade()\n    science_grade = Grade()\n \nfirst_exam = Exam()\nfirst_exam.writing_grade = 82\nsecond_exam = Exam()\nsecond_exam.writing_grade = 75\nprint(f'First  {first_exam.writing_grade} is right')\nprint(f'Second {second_exam.writing_grade} is right')\n",
      "content_length": 1831,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": " \nItem 47: Use __getattr__, etc for Lazy Attributes \n195\n>>>\nFirst  82 is right\nSecond 75 is right\nThings to Remember\n✦ Reuse the behavior and validation of @property methods by defining \nyour own descriptor classes.\n✦ Use WeakKeyDictionary to ensure that your descriptor classes don’t \ncause memory leaks.\n✦ Don’t get bogged down trying to understand exactly how \n__getattribute__ uses the descriptor protocol for getting and set-\nting attributes.\nItem 47:  Use __getattr__, __getattribute__, \nand __setattr__ for Lazy Attributes\nPython’s object hooks make it easy to write generic code for glu-\ning systems together. For example, say that I want to represent the \nrecords in a database as Python objects. The database has its schema \nset already. My code that uses objects corresponding to those records \nmust also know what the database looks like. However, in Python, \nthe code that connects Python objects to the database doesn’t need to \nexplicitly specify the schema of the records; it can be generic.\nHow is that possible? Plain instance attributes, @property methods, \nand descriptors can’t do this because they all need to be defined in \nadvance. Python makes this dynamic behavior possible with the \n__getattr__ special method. If a class defines __getattr__, that \nmethod is called every time an attribute can’t be found in an object’s \ninstance dictionary:\nclass LazyRecord:\n    def __init__(self):\n        self.exists = 5\n \n    def __getattr__(self, name):\n        value = f'Value for {name}'\n        setattr(self, name, value)\n        return value\nHere, I access the missing property foo. This causes Python to call \nthe __getattr__ method above, which mutates the instance dictio-\nnary __dict__:\ndata = LazyRecord()\nprint('Before:', data.__dict__)\n",
      "content_length": 1764,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "196 \nChapter 6 Metaclasses and Attributes\nprint('foo:   ', data.foo)\nprint('After: ', data.__dict__)\n>>>\nBefore: {'exists': 5}\nfoo:    Value for foo\nAfter:  {'exists': 5, 'foo': 'Value for foo'}\nHere, I add logging to LazyRecord to show when __getattr__ is actu-\nally called. Note how I call super().__getattr__() to use the super-\nclass’s implementation of __getattr__ in order to fetch the real \nproperty value and avoid infinite recursion (see Item 40: “Initialize \nParent Classes with super” for background):\nclass LoggingLazyRecord(LazyRecord):\n    def __getattr__(self, name):\n        print(f'* Called __getattr__({name!r}), '\n              f'populating instance dictionary')\n        result = super().__getattr__(name)\n        print(f'* Returning {result!r}')\n        return result\n \ndata = LoggingLazyRecord()\nprint('exists:     ', data.exists)\nprint('First foo:  ', data.foo)\nprint('Second foo: ', data.foo)\n>>>\nexists:      5\n* Called __getattr__('foo'), populating instance dictionary\n* Returning 'Value for foo'\nFirst foo:   Value for foo\nSecond foo:  Value for foo\nThe exists attribute is present in the instance dictionary, so \n__getattr__ is never called for it. The foo attribute is not in the \ninstance dictionary initially, so __getattr__ is called the first time. \nBut the call to __getattr__ for foo also does a setattr, which pop-\nulates foo in the instance dictionary. This is why the second time I \naccess foo, it doesn’t log a call to __getattr__.\nThis behavior is especially helpful for use cases like lazily accessing \nschemaless data. __getattr__ runs once to do the hard work of load-\ning a property; all subsequent accesses retrieve the existing result.\nSay that I also want transactions in this database system. The next \ntime the user accesses a property, I want to know whether the cor-\nresponding record in the database is still valid and whether the \n",
      "content_length": 1884,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "transaction is still open. The __getattr__ hook won’t let me do this \nreliably because it will use the object’s instance dictionary as the fast \npath for existing attributes.\nTo enable this more advanced use case, Python has another object \nhook called __getattribute__. This special method is called every \ntime an attribute is accessed on an object, even in cases where it does \nexist in the attribute dictionary. This enables me to do things like \ncheck global transaction state on every property access. It’s import-\nant to note that such an operation can incur significant overhead \nand negatively impact performance, but sometimes it’s worth it. Here, \nI define ValidatingRecord to log each time __getattribute__ is called:\nclass ValidatingRecord:\n    def __init__(self):\n        self.exists = 5\n \n    def __getattribute__(self, name):\n        print(f'* Called __getattribute__({name!r})')\n        try:\n            value = super().__getattribute__(name)\n            print(f'* Found {name!r}, returning {value!r}')\n            return value\n        except AttributeError:\n            value = f'Value for {name}'\n            print(f'* Setting {name!r} to {value!r}')\n            setattr(self, name, value)\n            return value\n \ndata = ValidatingRecord()\nprint('exists:     ', data.exists)\nprint('First foo:  ', data.foo)\nprint('Second foo: ', data.foo)\n>>>\n* Called __getattribute__('exists')\n* Found 'exists', returning 5\nexists:      5\n* Called __getattribute__('foo')\n* Setting 'foo' to 'Value for foo'\nFirst foo:   Value for foo\n* Called __getattribute__('foo')\n* Found 'foo', returning 'Value for foo'\nSecond foo:  Value for foo\n \nItem 47: Use __getattr__, etc for Lazy Attributes \n197\n",
      "content_length": 1699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "198 \nChapter 6 Metaclasses and Attributes\nIn the event that a dynamically accessed property shouldn’t exist, \nI can raise an AttributeError to cause Python’s standard missing \nproperty behavior for both __getattr__ and __getattribute__:\nclass MissingPropertyRecord:\n    def __getattr__(self, name):\n        if name == 'bad_name':\n            raise AttributeError(f'{name} is missing')\n        ...\n \ndata = MissingPropertyRecord()\ndata.bad_name\n>>>\nTraceback ...\nAttributeError: bad_name is missing\nPython code implementing generic functionality often relies on the \nhasattr built-in function to determine when properties exist, and the \ngetattr built-in function to retrieve property values. These functions \nalso look in the instance dictionary for an attribute name before call-\ning __getattr__:\ndata = LoggingLazyRecord()  # Implements __getattr__\nprint('Before:         ', data.__dict__)\nprint('Has first foo:  ', hasattr(data, 'foo'))\nprint('After:          ', data.__dict__)\nprint('Has second foo: ', hasattr(data, 'foo'))\n>>>\nBefore:          {'exists': 5}\n* Called __getattr__('foo'), populating instance dictionary\n* Returning 'Value for foo'\nHas first foo:   True\nAfter:           {'exists': 5, 'foo': 'Value for foo'}\nHas second foo:  True\nIn the example above, __getattr__ is called only once. In contrast, \nclasses that implement __getattribute__ have that method called \neach time hasattr or getattr is used with an instance:\ndata = ValidatingRecord()  # Implements __getattribute__\nprint('Has first foo:  ', hasattr(data, 'foo'))\nprint('Has second foo: ', hasattr(data, 'foo'))\n>>>\n* Called __getattribute__('foo')\n* Setting 'foo' to 'Value for foo'\nHas first foo:   True\n",
      "content_length": 1687,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "* Called __getattribute__('foo')\n* Found 'foo', returning 'Value for foo'\nHas second foo:  True\nNow, say that I want to lazily push data back to the database \nwhen values are assigned to my Python object. I can do this with \n__setattr__, a similar object hook that lets you intercept arbitrary \nattribute assignments. Unlike when retrieving an attribute with \n__getattr__ and __getattribute__, there’s no need for two separate \nmethods. The __setattr__ method is always called every time an \nattribute is assigned on an instance (either directly or through the \nsetattr built-in function):\nclass SavingRecord:\n    def __setattr__(self, name, value):\n        # Save some data for the record\n        ...\n        super().__setattr__(name, value)\nHere, I define a logging subclass of SavingRecord. Its __setattr__ \nmethod is always called on each attribute assignment:\nclass LoggingSavingRecord(SavingRecord):\n    def __setattr__(self, name, value):\n        print(f'* Called __setattr__({name!r}, {value!r})')\n        super().__setattr__(name, value)\n \ndata = LoggingSavingRecord()\nprint('Before: ', data.__dict__)\ndata.foo = 5\nprint('After:  ', data.__dict__)\ndata.foo = 7\nprint('Finally:', data.__dict__)\n>>>\nBefore:  {}\n* Called __setattr__('foo', 5)\nAfter:   {'foo': 5}\n* Called __setattr__('foo', 7)\nFinally: {'foo': 7}\nThe problem with __getattribute__ and __setattr__ is that they’re \ncalled on every attribute access for an object, even when you may not \nwant that to happen. For example, say that I want attribute accesses \non my object to actually look up keys in an associated dictionary:\nclass BrokenDictionaryRecord:\n    def __init__(self, data):\n        self._data = {}\n \n \nItem 47: Use __getattr__, etc for Lazy Attributes \n199\n",
      "content_length": 1739,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "200 \nChapter 6 Metaclasses and Attributes\n    def __getattribute__(self, name):\n        print(f'* Called __getattribute__({name!r})')\n        return self._data[name]\nThis requires accessing self._data from the __getattribute__ \nmethod. However, if I actually try to do that, Python will recurse until \nit reaches its stack limit, and then it’ll die:\ndata = BrokenDictionaryRecord({'foo': 3})\ndata.foo\n>>>\n* Called __getattribute__('foo')\n* Called __getattribute__('_data')\n* Called __getattribute__('_data')\n* Called __getattribute__('_data')\n...\nTraceback ...\nRecursionError: maximum recursion depth exceeded while calling \n¯a Python object\nThe problem is that __getattribute__ accesses self._data, which \ncauses __getattribute__ to run again, which accesses self._data \nagain, and so on. The solution is to use the super().__getattribute__ \nmethod to fetch values from the instance attribute dictionary. This \navoids the recursion:\nclass DictionaryRecord:\n    def __init__(self, data):\n        self._data = data\n \n    def __getattribute__(self, name):\n        print(f'* Called __getattribute__({name!r})')\n        data_dict = super().__getattribute__('_data')\n        return data_dict[name]\n \ndata = DictionaryRecord({'foo': 3})\nprint('foo: ', data.foo)\n>>>\n* Called __getattribute__('foo')\nfoo:  3\n__setattr__ methods that modify attributes on an object also need to \nuse super().__setattr__ accordingly.\nThings to Remember\n✦ Use __getattr__ and __setattr__ to lazily load and save attributes \nfor an object.\n",
      "content_length": 1512,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": " \nItem 48: Validate Subclasses with __init_subclass__ \n201\n✦ Understand that __getattr__ only gets called when accessing a \nmissing attribute, whereas __getattribute__ gets called every time \nany attribute is accessed.\n✦ Avoid infinite recursion in __getattribute__ and __setattr__ \nby using methods from super() (i.e., the object class) to access \ninstance attributes.\nItem 48: Validate Subclasses with __init_subclass__\nOne of the simplest applications of metaclasses is verifying that a \nclass was defined correctly. When you’re building a complex class \nhierarchy, you may want to enforce style, require overriding meth-\nods, or have strict relationships between class attributes. Metaclasses \nenable these use cases by providing a reliable way to run your valida-\ntion code each time a new subclass is defined.\nOften a class’s validation code runs in the __init__ method, when an \nobject of the class’s type is constructed at runtime (see Item 44: “Use \nPlain Attributes Instead of Setter and Getter Methods” for an exam-\nple). Using metaclasses for validation can raise errors much earlier, \nsuch as when the module containing the class is first imported at \nprogram startup.\nBefore I get into how to define a metaclass for validating subclasses, \nit’s important to understand the metaclass action for standard \nobjects. A metaclass is defined by inheriting from type. In the default \ncase, a metaclass receives the contents of associated class statements \nin its __new__ method. Here, I can inspect and modify the class infor-\nmation before the type is actually constructed:\nclass Meta(type):\n    def __new__(meta, name, bases, class_dict):\n        print(f'* Running {meta}.__new__ for {name}')\n        print('Bases:', bases)\n        print(class_dict)\n        return type.__new__(meta, name, bases, class_dict)\n \nclass MyClass(metaclass=Meta):\n    stuff = 123\n \n    def foo(self):\n        pass\n \nclass MySubclass(MyClass):\n    other = 567\n \n    def bar(self):\n        pass\n",
      "content_length": 1980,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "202 \nChapter 6 Metaclasses and Attributes\nThe metaclass has access to the name of the class, the parent classes \nit inherits from (bases), and all the class attributes that were defined \nin the class’s body. All classes inherit from object, so it’s not explicitly \nlisted in the tuple of base classes:\n>>>\n* Running <class '__main__.Meta'>.__new__ for MyClass\nBases: ()\n{'__module__': '__main__',\n '__qualname__': 'MyClass',\n 'stuff': 123,\n 'foo': <function MyClass.foo at 0x105a05280>}\n* Running <class '__main__.Meta'>.__new__ for MySubclass\nBases: (<class '__main__.MyClass'>,)\n{'__module__': '__main__',\n '__qualname__': 'MySubclass',\n 'other': 567,\n 'bar': <function MySubclass.bar at 0x105a05310>}\nI can add functionality to the Meta.__new__ method in order to vali-\ndate all of the parameters of an associated class before it’s defined. \nFor example, say that I want to represent any type of multisided \npolygon. I can do this by defining a special validating metaclass and \nusing it in the base class of my polygon class hierarchy. Note that it’s \nimportant not to apply the same validation to the base class:\nclass ValidatePolygon(type):\n    def __new__(meta, name, bases, class_dict):\n        # Only validate subclasses of the Polygon class\n        if bases:\n            if class_dict['sides'] < 3:\n                raise ValueError('Polygons need 3+ sides')\n        return type.__new__(meta, name, bases, class_dict)\n \nclass Polygon(metaclass=ValidatePolygon):\n    sides = None  # Must be specified by subclasses\n \n    @classmethod\n    def interior_angles(cls):\n        return (cls.sides - 2) * 180\n \nclass Triangle(Polygon):\n    sides = 3\n \n",
      "content_length": 1652,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "class Rectangle(Polygon):\n    sides = 4\n \nclass Nonagon(Polygon):\n    sides = 9\n \nassert Triangle.interior_angles() == 180\nassert Rectangle.interior_angles() == 360\nassert Nonagon.interior_angles() == 1260\nIf I try to define a polygon with fewer than three sides, the valida-\ntion will cause the class statement to fail immediately after the class \nstatement body. This means the program will not even be able to start \nrunning when I define such a class (unless it’s defined in a dynam-\nically imported module; see Item 88: “Know How to Break Circular \nDependencies” for how this can happen):\nprint('Before class')\n \nclass Line(Polygon):\n    print('Before sides')\n    sides = 2\n    print('After sides')\n \nprint('After class')\n>>>\nBefore class\nBefore sides\nAfter sides\nTraceback ...\nValueError: Polygons need 3+ sides\nThis seems like quite a lot of machinery in order to get Python to \naccomplish such a basic task. Luckily, Python 3.6 introduced simpli-\nfied syntax—the __init_subclass__ special class method—for achiev-\ning the same behavior while avoiding metaclasses entirely. Here, I use \nthis mechanism to provide the same level of validation as before:\nclass BetterPolygon:\n    sides = None  # Must be specified by subclasses\n \n    def __init_subclass__(cls):\n        super().__init_subclass__()\n        if cls.sides < 3:\n            raise ValueError('Polygons need 3+ sides')\n \n \nItem 48: Validate Subclasses with __init_subclass__ \n203\n",
      "content_length": 1445,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "204 \nChapter 6 Metaclasses and Attributes\n    @classmethod\n    def interior_angles(cls):\n        return (cls.sides - 2) * 180\n \nclass Hexagon(BetterPolygon):\n    sides = 6\n \nassert Hexagon.interior_angles() == 720\nThe code is much shorter now, and the ValidatePolygon metaclass is \ngone entirely. It’s also easier to follow since I can access the sides \nattribute directly on the cls instance in __init_subclass__ instead of \nhaving to go into the class’s dictionary with class_dict['sides']. If \nI define an invalid subclass of BetterPolygon, the same exception is \nraised:\nprint('Before class')\n \nclass Point(BetterPolygon):\n    sides = 1\n \nprint('After class')\n>>>\nBefore class\nTraceback ...\nValueError: Polygons need 3+ sides\nAnother problem with the standard Python metaclass machinery \nis that you can only specify a single metaclass per class definition. \nHere, I define a second metaclass that I’d like to use for validating the \nfill color used for a region (not necessarily just polygons):\nclass ValidateFilled(type):\n    def __new__(meta, name, bases, class_dict):\n        # Only validate subclasses of the Filled class\n        if bases:\n            if class_dict['color'] not in ('red', 'green'):\n                raise ValueError('Fill color must be supported')\n        return type.__new__(meta, name, bases, class_dict)\n \nclass Filled(metaclass=ValidateFilled):\n    color = None  # Must be specified by subclasses\n",
      "content_length": 1427,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "When I try to use the Polygon metaclass and Filled metaclass \ntogether, I get a cryptic error message:\nclass RedPentagon(Filled, Polygon):\n    color = 'red'\n    sides = 5\n>>>\nTraceback ...\nTypeError: metaclass conflict: the metaclass of a derived \n¯class must be a (non-strict) subclass of the metaclasses \n¯of all its bases\nIt’s possible to fix this by creating a complex hierarchy of metaclass \ntype definitions to layer validation:\nclass ValidatePolygon(type):\n    def __new__(meta, name, bases, class_dict):\n        # Only validate non-root classes\n        if not class_dict.get('is_root'):\n            if class_dict['sides'] < 3:\n                raise ValueError('Polygons need 3+ sides')\n        return type.__new__(meta, name, bases, class_dict)\n \nclass Polygon(metaclass=ValidatePolygon):\n    is_root = True\n    sides = None  # Must be specified by subclasses\n \nclass ValidateFilledPolygon(ValidatePolygon):\n    def __new__(meta, name, bases, class_dict):\n        # Only validate non-root classes\n        if not class_dict.get('is_root'):\n            if class_dict['color'] not in ('red', 'green'):\n                raise ValueError('Fill color must be supported')\n        return super().__new__(meta, name, bases, class_dict)\n \nclass FilledPolygon(Polygon, metaclass=ValidateFilledPolygon):\n    is_root = True\n    color = None  # Must be specified by subclasses\nThis requires every FilledPolygon instance to be a Polygon instance:\nclass GreenPentagon(FilledPolygon):\n    color = 'green'\n    sides = 5\n \ngreenie = GreenPentagon()\nassert isinstance(greenie, Polygon)\n \nItem 48: Validate Subclasses with __init_subclass__ \n205\n",
      "content_length": 1632,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "206 \nChapter 6 Metaclasses and Attributes\nValidation works for colors:\nclass OrangePentagon(FilledPolygon):\n    color = 'orange'\n    sides = 5\n>>>\nTraceback ...\nValueError: Fill color must be supported\nValidation also works for number of sides:\nclass RedLine(FilledPolygon):\n    color = 'red'\n    sides = 2\n>>>\nTraceback ...\nValueError: Polygons need 3+ sides\nHowever, this approach ruins composability, which is often the pur-\npose of class validation like this (similar to mix-ins; see Item 41: \n“Consider Composing Functionality with Mix-in Classes”). If I want to \napply the color validation logic from ValidateFilledPolygon to another \nhierarchy of classes, I’ll have to duplicate all of the logic again, which \nreduces code reuse and increases boilerplate.\nThe __init_subclass__ special class method can also be used to \nsolve this problem. It can be defined by multiple levels of a class \nhierarchy as long as the super built-in function is used to call any \nparent or sibling __init_subclass__ definitions (see Item 40: “Initial-\nize Parent Classes with super” for a similar example). It’s even com-\npatible with multiple inheritance. Here, I define a class to represent \nregion fill color that can be composed with the BetterPolygon class \nfrom before:\nclass Filled:\n    color = None  # Must be specified by subclasses\n \n    def __init_subclass__(cls):\n        super().__init_subclass__()\n        if cls.color not in ('red', 'green', 'blue'):\n            raise ValueError('Fills need a valid color')\nI can inherit from both classes to define a new class. Both classes call \nsuper().__init_subclass__(), causing their corresponding validation \nlogic to run when the subclass is created:\nclass RedTriangle(Filled, Polygon):\n    color = 'red'\n    sides = 3\n \n",
      "content_length": 1765,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "ruddy = RedTriangle()\nassert isinstance(ruddy, Filled)\nassert isinstance(ruddy, Polygon)\nIf I specify the number of sides incorrectly, I get a validation error:\nprint('Before class')\n \nclass BlueLine(Filled, Polygon):\n    color = 'blue'\n    sides = 2\n \nprint('After class')\n>>>\nBefore class\nTraceback ...\nValueError: Polygons need 3+ sides\nIf I specify the color incorrectly, I also get a validation error:\nprint('Before class')\n \nclass BeigeSquare(Filled, Polygon):\n    color = 'beige'\n    sides = 4\n \nprint('After class')\n>>>\nBefore class\nTraceback ...\nValueError: Fills need a valid color\nYou can even use __init_subclass__ in complex cases like diamond \ninheritance (see Item 40: “Initialize Parent Classes with super”). Here, \nI define a basic diamond hierarchy to show this in action:\nclass Top:\n    def __init_subclass__(cls):\n        super().__init_subclass__()\n        print(f'Top for {cls}')\n \nclass Left(Top):\n    def __init_subclass__(cls):\n        super().__init_subclass__()\n        print(f'Left for {cls}')\n \nclass Right(Top):\n    def __init_subclass__(cls):\n \nItem 48: Validate Subclasses with __init_subclass__ \n207\n",
      "content_length": 1133,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "208 \nChapter 6 Metaclasses and Attributes\n        super().__init_subclass__()\n        print(f'Right for {cls}')\n \nclass Bottom(Left, Right):\n    def __init_subclass__(cls):\n        super().__init_subclass__()\n        print(f'Bottom for {cls}')\n>>>\nTop for <class '__main__.Left'>\nTop for <class '__main__.Right'>\nTop for <class '__main__.Bottom'>\nRight for <class '__main__.Bottom'>\nLeft for <class '__main__.Bottom'>\nAs expected, Top.__init_subclass__ is called only a single time for \neach class, even though there are two paths to it for the Bottom class \nthrough its Left and Right parent classes.\nThings to Remember\n✦ The __new__ method of metaclasses is run after the class state-\nment’s entire body has been processed.\n✦ Metaclasses can be used to inspect or modify a class after it’s \ndefined but before it’s created, but they’re often more heavyweight \nthan what you need.\n✦ Use __init_subclass__ to ensure that subclasses are well formed \nat the time they are defined, before objects of their type are \nconstructed.\n✦ Be sure to call super().__init_subclass__ from within your class’s \n__init_subclass__ definition to enable validation in multiple layers \nof classes and multiple inheritance.\nItem 49:  Register Class Existence with \n__init_subclass__\nAnother common use of metaclasses is to automatically register types \nin a program. Registration is useful for doing reverse lookups, where \nyou need to map a simple identifier back to a corresponding class.\nFor example, say that I want to implement my own serialized repre-\nsentation of a Python object using JSON. I need a way to turn an \nobject into a JSON string. Here, I do this generically by defining a \n",
      "content_length": 1673,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": " \nItem 49: Register Class Existence with __init_subclass__ \n209\nbase class that records the constructor parameters and turns them \ninto a JSON dictionary:\nimport json\n \nclass Serializable:\n    def __init__(self, *args):\n        self.args = args\n \n    def serialize(self):\n        return json.dumps({'args': self.args})\nThis class makes it easy to serialize simple, immutable data struc-\ntures like Point2D to a string:\nclass Point2D(Serializable):\n    def __init__(self, x, y):\n        super().__init__(x, y)\n        self.x = x\n        self.y = y\n \n    def __repr__(self):\n        return f'Point2D({self.x}, {self.y})'\n \npoint = Point2D(5, 3)\nprint('Object:    ', point)\nprint('Serialized:', point.serialize())\n>>>\nObject:     Point2D(5, 3)\nSerialized: {\"args\": [5, 3]}\nNow, I need to deserialize this JSON string and construct the Point2D \nobject it represents. Here, I define another class that can deserialize \nthe data from its Serializable parent class:\nclass Deserializable(Serializable):\n    @classmethod\n    def deserialize(cls, json_data):\n        params = json.loads(json_data)\n        return cls(*params['args'])\nUsing Deserializable makes it easy to serialize and deserialize sim-\nple, immutable objects in a generic way:\nclass BetterPoint2D(Deserializable):\n    ...\n \n",
      "content_length": 1281,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "210 \nChapter 6 Metaclasses and Attributes\nbefore = BetterPoint2D(5, 3)\nprint('Before:    ', before)\ndata = before.serialize()\nprint('Serialized:', data)\nafter = BetterPoint2D.deserialize(data)\nprint('After:     ', after)\n>>>\nBefore:     Point2D(5, 3)\nSerialized: {\"args\": [5, 3]}\nAfter:      Point2D(5, 3)\nThe problem with this approach is that it works only if you know \nthe intended type of the serialized data ahead of time (e.g., Point2D, \nBetterPoint2D). Ideally, you’d have a large number of classes serializ-\ning to JSON and one common function that could deserialize any of \nthem back to a corresponding Python object.\nTo do this, I can include the serialized object’s class name in the \nJSON data:\nclass BetterSerializable:\n    def __init__(self, *args):\n        self.args = args\n \n    def serialize(self):\n        return json.dumps({\n            'class': self.__class__.__name__,\n            'args': self.args,\n        })\n \n    def __repr__(self):\n        name = self.__class__.__name__\n        args_str = ', '.join(str(x) for x in self.args)\n        return f'{name}({args_str})'\nThen, I can maintain a mapping of class names back to construc-\ntors for those objects. The general deserialize function works for any \nclasses passed to register_class:\nregistry = {}\n \ndef register_class(target_class):\n    registry[target_class.__name__] = target_class\n \ndef deserialize(data):\n    params = json.loads(data)\n",
      "content_length": 1416,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "    name = params['class']\n    target_class = registry[name]\n    return target_class(*params['args'])\nTo ensure that deserialize always works properly, I must call \nregister_class for every class I may want to deserialize in the future:\nclass EvenBetterPoint2D(BetterSerializable):\n    def __init__(self, x, y):\n        super().__init__(x, y)\n        self.x = x\n        self.y = y\n \nregister_class(EvenBetterPoint2D)\nNow, I can deserialize an arbitrary JSON string without having to \nknow which class it contains:\nbefore = EvenBetterPoint2D(5, 3)\nprint('Before:    ', before)\ndata = before.serialize()\nprint('Serialized:', data)\nafter = deserialize(data)\nprint('After:     ', after)\n>>>\nBefore:     EvenBetterPoint2D(5, 3)\nSerialized: {\"class\": \"EvenBetterPoint2D\", \"args\": [5, 3]}\nAfter:      EvenBetterPoint2D(5, 3)\nThe problem with this approach is that it’s possible to forget to call \nregister_class:\nclass Point3D(BetterSerializable):\n    def __init__(self, x, y, z):\n        super().__init__(x, y, z)\n        self.x = x\n        self.y = y\n        self.z = z\n \n# Forgot to call register_class! Whoops!\nThis causes the code to break at runtime, when I finally try to deseri-\nalize an instance of a class I forgot to register:\npoint = Point3D(5, 9, -4)\ndata = point.serialize()\ndeserialize(data)\n \nItem 49: Register Class Existence with __init_subclass__ \n211\n",
      "content_length": 1364,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "212 \nChapter 6 Metaclasses and Attributes\n>>>\nTraceback ...\nKeyError: 'Point3D'\nEven though I chose to subclass BetterSerializable, I don’t actually \nget all of its features if I forget to call register_class after the class \nstatement body. This approach is error prone and especially chal-\nlenging for beginners. The same omission can happen with class dec-\norators (see Item 51: “Prefer Class Decorators Over Metaclasses for \nComposable Class Extensions” for when those are appropriate).\nWhat if I could somehow act on the programmer’s intent to use \nBetterSerializable and ensure that register_class is called in all \ncases? Metaclasses enable this by intercepting the class statement \nwhen subclasses are defined (see Item 48: “Validate Subclasses with \n__init_subclass__” for details on the machinery). Here, I use a meta-\nclass to register the new type immediately after the class’s body:\nclass Meta(type):\n    def __new__(meta, name, bases, class_dict):\n        cls = type.__new__(meta, name, bases, class_dict)\n        register_class(cls)\n        return cls\n \nclass RegisteredSerializable(BetterSerializable,\n                             metaclass=Meta):\n    pass\nWhen I define a subclass of RegisteredSerializable, I can be confident \nthat the call to register_class happened and deserialize will always \nwork as expected:\nclass Vector3D(RegisteredSerializable):\n    def __init__(self, x, y, z):\n        super().__init__(x, y, z)\n        self.x, self.y, self.z = x, y, z\n \nbefore = Vector3D(10, -7, 3)\nprint('Before:    ', before)\ndata = before.serialize()\nprint('Serialized:', data)\nprint('After:     ', deserialize(data))\n>>>\nBefore:     Vector3D(10, -7, 3)\nSerialized: {\"class\": \"Vector3D\", \"args\": [10, -7, 3]}\nAfter:      Vector3D(10, -7, 3)\n",
      "content_length": 1757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "An even better approach is to use the __init_subclass__ special class \nmethod. This simplified syntax, introduced in Python 3.6, reduces \nthe visual noise of applying custom logic when a class is defined. It \nalso makes it more approachable to beginners who may be confused \nby the complexity of metaclass syntax:\nclass BetterRegisteredSerializable(BetterSerializable):\n    def __init_subclass__(cls):\n        super().__init_subclass__()\n        register_class(cls)\n \nclass Vector1D(BetterRegisteredSerializable):\n    def __init__(self, magnitude):\n        super().__init__(magnitude)\n        self.magnitude = magnitude\n \nbefore = Vector1D(6)\nprint('Before:    ', before)\ndata = before.serialize()\nprint('Serialized:', data)\nprint('After:     ', deserialize(data))\n>>>\nBefore:     Vector1D(6)\nSerialized: {\"class\": \"Vector1D\", \"args\": [6]}\nAfter:      Vector1D(6)\nBy using __init_subclass__ (or metaclasses) for class registration, \nyou can ensure that you’ll never miss registering a class as long as \nthe inheritance tree is right. This works well for serialization, as \nI’ve shown, and also applies to database object-relational mappings \n(ORMs), extensible plug-in systems, and callback hooks.\nThings to Remember\n✦ Class registration is a helpful pattern for building modular Python \nprograms.\n✦ Metaclasses let you run registration code automatically each time a \nbase class is subclassed in a program.\n✦ Using metaclasses for class registration helps you avoid errors by \nensuring that you never miss a registration call.\n✦ Prefer __init_subclass__ over standard metaclass machinery \nbecause it’s clearer and easier for beginners to understand.\n \nItem 49: Register Class Existence with __init_subclass__ \n213\n",
      "content_length": 1715,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "214 \nChapter 6 Metaclasses and Attributes\nItem 50: Annotate Class Attributes with __set_name__\nOne more useful feature enabled by metaclasses is the ability to mod-\nify or annotate properties after a class is defined but before the class \nis actually used. This approach is commonly used with descriptors \n(see Item 46: “Use Descriptors for Reusable @property Methods”) to \ngive them more introspection into how they’re being used within their \ncontaining class.\nFor example, say that I want to define a new class that represents a \nrow in a customer database. I’d like to have a corresponding property \non the class for each column in the database table. Here, I define a \ndescriptor class to connect attributes to column names:\nclass Field:\n    def __init__(self, name):\n        self.name = name\n        self.internal_name = '_' + self.name\n \n    def __get__(self, instance, instance_type):\n        if instance is None:\n            return self\n        return getattr(instance, self.internal_name, '')\n \n    def __set__(self, instance, value):\n        setattr(instance, self.internal_name, value)\nWith the column name stored in the Field descriptor, I can save all of \nthe per-instance state directly in the instance dictionary as protected \nfields by using the setattr built-in function, and later I can load state \nwith getattr. At first, this seems to be much more convenient than \nbuilding descriptors with the weakref built-in module to avoid mem-\nory leaks.\nDefining the class representing a row requires supplying the data-\nbase table’s column name for each class attribute:\nclass Customer:\n    # Class attributes\n    first_name = Field('first_name')\n    last_name = Field('last_name')\n    prefix = Field('prefix')\n    suffix = Field('suffix')\nUsing the class is simple. Here, you can see how the Field descriptors \nmodify the instance dictionary __dict__ as expected:\ncust = Customer()\nprint(f'Before: {cust.first_name!r} {cust.__dict__}')\n",
      "content_length": 1949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": " \nItem 50: Annotate Class Attributes with __set_name__ \n215\ncust.first_name = 'Euclid'\nprint(f'After:  {cust.first_name!r} {cust.__dict__}')\n>>>\nBefore: '' {}\nAfter:  'Euclid' {'_first_name': 'Euclid'}\nBut the class definition seems redundant. I already declared the \nname of the field for the class on the left ('field_name ='). Why do \nI also have to pass a string containing the same information to the \nField constructor (Field('first_name')) on the right?\nclass Customer:\n    # Left side is redundant with right side\n    first_name = Field('first_name')\n    ...\nThe problem is that the order of operations in the Customer class defi-\nnition is the opposite of how it reads from left to right. First, the Field \nconstructor is called as Field('first_name'). Then, the return value \nof that is assigned to Customer.field_name. There’s no way for a Field \ninstance to know upfront which class attribute it will be assigned to.\nTo eliminate this redundancy, I can use a metaclass. Metaclasses \nlet you hook the class statement directly and take action as soon \nas a class body is finished (see Item 48: “Validate Subclasses with \n__init_subclass__” for details on how they work). In this case, I can \nuse the metaclass to assign Field.name and Field.internal_name on \nthe descriptor automatically instead of manually specifying the field \nname multiple times:\nclass Meta(type):\n    def __new__(meta, name, bases, class_dict):\n        for key, value in class_dict.items():\n            if isinstance(value, Field):\n                value.name = key\n                value.internal_name = '_' + key\n        cls = type.__new__(meta, name, bases, class_dict)\n        return cls\nHere, I define a base class that uses the metaclass. All classes repre-\nsenting database rows should inherit from this class to ensure that \nthey use the metaclass:\nclass DatabaseRow(metaclass=Meta):\n    pass\nTo work with the metaclass, the Field descriptor is largely unchanged. \nThe only difference is that it no longer requires arguments to be passed \n",
      "content_length": 2027,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "216 \nChapter 6 Metaclasses and Attributes\nto its constructor. Instead, its attributes are set by the Meta.__new__ \nmethod above:\nclass Field:\n    def __init__(self):\n        # These will be assigned by the metaclass.\n        self.name = None\n        self.internal_name = None\n \n    def __get__(self, instance, instance_type):\n        if instance is None:\n            return self\n        return getattr(instance, self.internal_name, '')\n \n    def __set__(self, instance, value):\n        setattr(instance, self.internal_name, value)\nBy using the metaclass, the new DatabaseRow base class, and the new \nField descriptor, the class definition for a database row no longer has \nthe redundancy from before:\nclass BetterCustomer(DatabaseRow):\n    first_name = Field()\n    last_name = Field()\n    prefix = Field()\n    suffix = Field()\nThe behavior of the new class is identical to the behavior of the old \none:\ncust = BetterCustomer()\nprint(f'Before: {cust.first_name!r} {cust.__dict__}')\ncust.first_name = 'Euler'\nprint(f'After:  {cust.first_name!r} {cust.__dict__}')\n>>>\nBefore: '' {}\nAfter:  'Euler' {'_first_name': 'Euler'}\nThe trouble with this approach is that you can’t use the Field class for \nproperties unless you also inherit from DatabaseRow. If you somehow \nforget to subclass DatabaseRow, or if you don’t want to due to other \nstructural requirements of the class hierarchy, the code will break:\nclass BrokenCustomer:\n    first_name = Field()\n    last_name = Field()\n    prefix = Field()\n    suffix = Field()\n \n",
      "content_length": 1517,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "cust = BrokenCustomer()\ncust.first_name = 'Mersenne'\n>>>\nTraceback ...\nTypeError: attribute name must be string, not 'NoneType'\nThe solution to this problem is to use the __set_name__ special method \nfor descriptors. This method, introduced in Python 3.6, is called on \nevery descriptor instance when its containing class is defined. It \nreceives as parameters the owning class that contains the descriptor \ninstance and the attribute name to which the descriptor instance was \nassigned. Here, I avoid defining a metaclass entirely and move what \nthe Meta.__new__ method from above was doing into __set_name__:\nclass Field:\n    def __init__(self):\n        self.name = None\n        self.internal_name = None\n \n    def __set_name__(self, owner, name):\n        # Called on class creation for each descriptor\n        self.name = name\n        self.internal_name = '_' + name\n \n    def __get__(self, instance, instance_type):\n        if instance is None:\n            return self\n        return getattr(instance, self.internal_name, '')\n \n    def __set__(self, instance, value):\n        setattr(instance, self.internal_name, value)\nNow, I can get the benefits of the Field descriptor without having to \ninherit from a specific parent class or having to use a metaclass:\nclass FixedCustomer:\n    first_name = Field()\n    last_name = Field()\n    prefix = Field()\n    suffix = Field()\n \ncust = FixedCustomer()\nprint(f'Before: {cust.first_name!r} {cust.__dict__}')\ncust.first_name = 'Mersenne'\nprint(f'After:  {cust.first_name!r} {cust.__dict__}')\n \nItem 50: Annotate Class Attributes with __set_name__ \n217\n",
      "content_length": 1597,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "218 \nChapter 6 Metaclasses and Attributes\n>>>\nBefore: '' {}\nAfter:  'Mersenne' {'_first_name': 'Mersenne'}\nThings to Remember\n✦ Metaclasses enable you to modify a class’s attributes before the \nclass is fully defined.\n✦ Descriptors and metaclasses make a powerful combination for \ndeclarative behavior and runtime introspection.\n✦ Define __set_name__ on your descriptor classes to allow them to \ntake into account their surrounding class and its property names.\n✦ Avoid memory leaks and the weakref built-in module by having \ndescriptors store data they manipulate directly within a class’s \ninstance dictionary.\nItem 51:  Prefer Class Decorators Over Metaclasses for \nComposable Class Extensions\nAlthough metaclasses allow you to customize class creation in multi-\nple ways (see Item 48: “Validate Subclasses with __init_subclass__” \nand Item 49: “Register Class Existence with __init_subclass__”), they \nstill fall short of handling every situation that may arise.\nFor example, say that I want to decorate all of the methods of a class \nwith a helper that prints arguments, return values, and exceptions \nraised. Here, I define the debugging decorator (see Item 26: “Define \nFunction Decorators with functools.wraps” for background):\nfrom functools import wraps\n \ndef trace_func(func):\n    if hasattr(func, 'tracing'):  # Only decorate once\n        return func\n \n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        result = None\n        try:\n            result = func(*args, **kwargs)\n            return result\n        except Exception as e:\n            result = e\n            raise\n",
      "content_length": 1592,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": " \nItem 51: Prefer Class Decorators Over Metaclasses \n219\n        finally:\n            print(f'{func.__name__}({args!r}, {kwargs!r}) -> '\n                  f'{result!r}')\n \n    wrapper.tracing = True\n    return wrapper\nI can apply this decorator to various special methods in my new dict \nsubclass (see Item 43: “Inherit from collections.abc for Custom Con-\ntainer Types” for background):\nclass TraceDict(dict):\n    @trace_func\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n \n    @trace_func\n    def __setitem__(self, *args, **kwargs):\n        return super().__setitem__(*args, **kwargs)\n \n    @trace_func\n    def __getitem__(self, *args, **kwargs):\n        return super().__getitem__(*args, **kwargs)\n \n    ...\nAnd I can verify that these methods are decorated by interacting with \nan instance of the class:\ntrace_dict = TraceDict([('hi', 1)])\ntrace_dict['there'] = 2\ntrace_dict['hi']\ntry:\n    trace_dict['does not exist']\nexcept KeyError:\n    pass  # Expected\n>>>\n__init__(({'hi': 1}, [('hi', 1)]), {}) -> None\n__setitem__(({'hi': 1, 'there': 2}, 'there', 2), {}) -> None\n__getitem__(({'hi': 1, 'there': 2}, 'hi'), {}) -> 1\n__getitem__(({'hi': 1, 'there': 2}, 'does not exist'), \n¯{}) -> KeyError('does not exist')\nThe problem with this code is that I had to redefine all of the methods \nthat I wanted to decorate with @trace_func. This is redundant boiler-\nplate that’s hard to read and error prone. Further, if a new method is \n",
      "content_length": 1468,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "220 \nChapter 6 Metaclasses and Attributes\nlater added to the dict superclass, it won’t be decorated unless I also \ndefine it in TraceDict.\nOne way to solve this problem is to use a metaclass to automati-\ncally decorate all methods of a class. Here, I implement this behav-\nior by wrapping each function or method in the new type with the \ntrace_func decorator:\nimport types\n \ntrace_types = (\n    types.MethodType,\n    types.FunctionType,\n    types.BuiltinFunctionType,\n    types.BuiltinMethodType,\n    types.MethodDescriptorType,\n    types.ClassMethodDescriptorType)\n \nclass TraceMeta(type):\n    def __new__(meta, name, bases, class_dict):\n        klass = super().__new__(meta, name, bases, class_dict)\n \n        for key in dir(klass):\n            value = getattr(klass, key)\n            if isinstance(value, trace_types):\n                wrapped = trace_func(value)\n                setattr(klass, key, wrapped)\n \n        return klass\nNow, I can declare my dict subclass by using the TraceMeta metaclass \nand verify that it works as expected:\nclass TraceDict(dict, metaclass=TraceMeta):\n    pass\n \ntrace_dict = TraceDict([('hi', 1)])\ntrace_dict['there'] = 2\ntrace_dict['hi']\ntry:\n    trace_dict['does not exist']\nexcept KeyError:\n    pass  # Expected\n>>>\n__new__((<class '__main__.TraceDict'>, [('hi', 1)]), {}) -> {}\n__getitem__(({'hi': 1, 'there': 2}, 'hi'), {}) -> 1\n",
      "content_length": 1370,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "__getitem__(({'hi': 1, 'there': 2}, 'does not exist'), \n¯{}) -> KeyError('does not exist')\nThis works, and it even prints out a call to __new__ that was miss-\ning from my earlier implementation. What happens if I try to use \nTraceMeta when a superclass already has specified a metaclass?\nclass OtherMeta(type):\n    pass\n \nclass SimpleDict(dict, metaclass=OtherMeta):\n    pass\n \nclass TraceDict(SimpleDict, metaclass=TraceMeta):\n    pass\n>>>\nTraceback ...\nTypeError: metaclass conflict: the metaclass of a derived \n¯class must be a (non-strict) subclass of the metaclasses \n¯of all its bases\nThis fails because TraceMeta does not inherit from OtherMeta. In the-\nory, I can use metaclass inheritance to solve this problem by having \nOtherMeta inherit from TraceMeta:\nclass TraceMeta(type):\n    ...\n \nclass OtherMeta(TraceMeta):\n    pass\n \nclass SimpleDict(dict, metaclass=OtherMeta):\n    pass\n \nclass TraceDict(SimpleDict, metaclass=TraceMeta):\n    pass\n \ntrace_dict = TraceDict([('hi', 1)])\ntrace_dict['there'] = 2\ntrace_dict['hi']\ntry:\n    trace_dict['does not exist']\nexcept KeyError:\n    pass  # Expected\n \nItem 51: Prefer Class Decorators Over Metaclasses \n221\n",
      "content_length": 1164,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "222 \nChapter 6 Metaclasses and Attributes\n>>>\n__init_subclass__((), {}) -> None\n__new__((<class '__main__.TraceDict'>, [('hi', 1)]), {}) -> {}\n__getitem__(({'hi': 1, 'there': 2}, 'hi'), {}) -> 1\n__getitem__(({'hi': 1, 'there': 2}, 'does not exist'), \n¯{}) -> KeyError('does not exist')\nBut this won’t work if the metaclass is from a library that I can’t mod-\nify, or if I want to use multiple utility metaclasses like TraceMeta at \nthe same time. The metaclass approach puts too many constraints on \nthe class that’s being modified.\nTo solve this problem, Python supports class decorators. Class \n decorators work just like function decorators: They’re applied with the \n@ symbol prefixing a function before the class declaration. The func-\ntion is expected to modify or re-create the class accordingly and then \nreturn it:\ndef my_class_decorator(klass):\n    klass.extra_param = 'hello'\n    return klass\n \n@my_class_decorator\nclass MyClass:\n    pass\n \nprint(MyClass)\nprint(MyClass.extra_param)\n>>>\n<class '__main__.MyClass'>\nhello\nI can implement a class decorator to apply trace_func to all methods \nand functions of a class by moving the core of the TraceMeta.__new__ \nmethod above into a stand-alone function. This implementation is \nmuch shorter than the metaclass version:\ndef trace(klass):\n    for key in dir(klass):\n        value = getattr(klass, key)\n        if isinstance(value, trace_types):\n            wrapped = trace_func(value)\n            setattr(klass, key, wrapped)\n    return klass\n",
      "content_length": 1500,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "I can apply this decorator to my dict subclass to get the same behav-\nior as I get by using the metaclass approach above:\n@trace\nclass TraceDict(dict):\n    pass\n \ntrace_dict = TraceDict([('hi', 1)])\ntrace_dict['there'] = 2\ntrace_dict['hi']\ntry:\n    trace_dict['does not exist']\nexcept KeyError:\n    pass  # Expected\n>>>\n__new__((<class '__main__.TraceDict'>, [('hi', 1)]), {}) -> {}\n__getitem__(({'hi': 1, 'there': 2}, 'hi'), {}) -> 1\n__getitem__(({'hi': 1, 'there': 2}, 'does not exist'), \n¯{}) -> KeyError('does not exist')\nClass decorators also work when the class being decorated already \nhas a metaclass:\nclass OtherMeta(type):\n    pass\n \n@trace\nclass TraceDict(dict, metaclass=OtherMeta):\n    pass\n \ntrace_dict = TraceDict([('hi', 1)])\ntrace_dict['there'] = 2\ntrace_dict['hi']\ntry:\n    trace_dict['does not exist']\nexcept KeyError:\n    pass  # Expected\n>>>\n__new__((<class '__main__.TraceDict'>, [('hi', 1)]), {}) -> {}\n__getitem__(({'hi': 1, 'there': 2}, 'hi'), {}) -> 1\n__getitem__(({'hi': 1, 'there': 2}, 'does not exist'), \n¯{}) -> KeyError('does not exist')\nWhen you’re looking for composable ways to extend classes, class \ndecorators are the best tool for the job. (See Item 73: “Know How \n \nItem 51: Prefer Class Decorators Over Metaclasses \n223\n",
      "content_length": 1259,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "224 \nChapter 6 Metaclasses and Attributes\nto Use heapq for Priority Queues” for a useful class decorator called \nfunctools.total_ordering.)\nThings to Remember\n✦ A class decorator is a simple function that receives a class instance \nas a parameter and returns either a new class or a modified version \nof the original class.\n✦ Class decorators are useful when you want to modify every method \nor attribute of a class with minimal boilerplate.\n✦ Metaclasses can’t be composed together easily, while many class \ndecorators can be used to extend the same class without conflicts.\n",
      "content_length": 576,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "7\nConcurrency and \nParallelism\nConcurrency enables a computer to do many different things  seemingly \nat the same time. For example, on a computer with one CPU core, the \noperating system rapidly changes which program is running on the \nsingle processor. In doing so, it interleaves execution of the programs, \nproviding the illusion that the programs are running simultaneously.\nParallelism, in contrast, involves actually doing many different things \nat the same time. A computer with multiple CPU cores can execute \nmultiple programs simultaneously. Each CPU core runs the instruc-\ntions of a separate program, allowing each program to make forward \nprogress during the same instant.\nWithin a single program, concurrency is a tool that makes it easier \nfor programmers to solve certain types of problems. Concurrent pro-\ngrams enable many distinct paths of execution, including separate \nstreams of I/O, to make forward progress in a way that seems to be \nboth simultaneous and independent.\nThe key difference between parallelism and concurrency is speedup. \nWhen two distinct paths of execution in a program make forward \nprogress in parallel, the time it takes to do the total work is cut in \nhalf; the speed of execution is faster by a factor of two. In contrast, \nconcurrent programs may run thousands of separate paths of execu-\ntion seemingly in parallel but provide no speedup for the total work.\nPython makes it easy to write concurrent programs in a variety of \nstyles. Threads support a relatively small amount of concurrency, \nwhile coroutines enable vast numbers of concurrent functions. \nPython can also be used to do parallel work through system calls, \nsubprocesses, and C extensions. But it can be very difficult to make \nconcurrent Python code truly run in parallel. It’s important to under-\nstand how to best utilize Python in these different situations.\n",
      "content_length": 1876,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "226 \nChapter 7 Concurrency and Parallelism\nItem 52: Use subprocess to Manage Child Processes\nPython has battle-hardened libraries for running and managing child \nprocesses. This makes it a great language for gluing together other \ntools, such as command-line utilities. When existing shell scripts get \ncomplicated, as they often do over time, graduating them to a rewrite \nin Python for the sake of readability and maintainability is a natural \nchoice.\nChild processes started by Python are able to run in parallel, enabling \nyou to use Python to consume all of the CPU cores of a machine and \nmaximize the throughput of programs. Although Python itself may \nbe CPU bound (see Item 53: “Use Threads for Blocking I/O, Avoid for \nParallelism”), it’s easy to use Python to drive and coordinate CPU- \nintensive workloads.\nPython has many ways to run subprocesses (e.g., os.popen, os.exec*), \nbut the best choice for managing child processes is to use the \nsubprocess built-in module. Running a child process with subprocess \nis simple. Here, I use the module’s run convenience function to start a \nprocess, read its output, and verify that it terminated cleanly:\nimport subprocess\n \nresult = subprocess.run(\n    ['echo', 'Hello from the child!'],\n    capture_output=True,\n    encoding='utf-8')\n \nresult.check_returncode()  # No exception means clean exit\nprint(result.stdout)\n>>>\nHello from the child!\nNote\nThe examples in this item assume that your system has the echo, sleep, and \nopenssl commands available. On Windows, this may not be the case. Please \nrefer to the full example code for this item to see specific directions on how to \nrun these snippets on Windows.\nChild processes run independently from their parent process, the \nPython interpreter. If I create a subprocess using the Popen class \ninstead of the run function, I can poll child process status periodically \nwhile Python does other work:\nproc = subprocess.Popen(['sleep', '1'])\nwhile proc.poll() is None:\n    print('Working...')\n",
      "content_length": 1998,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": " \nItem 52: Use subprocess to Manage Child Processes \n227\n    # Some time-consuming work here\n    ...\n \nprint('Exit status', proc.poll())\n>>>\nWorking...\nWorking...\nWorking...\nWorking...\nExit status 0\nDecoupling the child process from the parent frees up the parent \n process to run many child processes in parallel. Here, I do this by \nstarting all the child processes together with Popen upfront:\nimport time\n \nstart = time.time()\nsleep_procs = []\nfor _ in range(10):\n    proc = subprocess.Popen(['sleep', '1'])\n    sleep_procs.append(proc)\nLater, I wait for them to finish their I/O and terminate with the \ncommunicate method:\nfor proc in sleep_procs:\n    proc.communicate()\n \nend = time.time()\ndelta = end - start\nprint(f'Finished in {delta:.3} seconds')\n>>>\nFinished in 1.05 seconds\nIf these processes ran in sequence, the total delay would be 10  seconds \nor more rather than the ~1 second that I measured.\nYou can also pipe data from a Python program into a subprocess and \nretrieve its output. This allows you to utilize many other programs to \ndo work in parallel. For example, say that I want to use the openssl \ncommand-line tool to encrypt some data. Starting the child process \nwith command-line arguments and I/O pipes is easy:\nimport os\ndef run_encrypt(data):\n    env = os.environ.copy()\n",
      "content_length": 1301,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "228 \nChapter 7 Concurrency and Parallelism\n    env['password'] = 'zf7ShyBhZOraQDdE/FiZpm/m/8f9X+M1'\n    proc = subprocess.Popen(\n        ['openssl', 'enc', '-des3', '-pass', 'env:password'],\n        env=env,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE)\n    proc.stdin.write(data)\n    proc.stdin.flush()  # Ensure that the child gets input\n    return proc\nHere, I pipe random bytes into the encryption function, but in prac-\ntice this input pipe would be fed data from user input, a file handle, a \nnetwork socket, and so on:\nprocs = []\nfor _ in range(3):\n    data = os.urandom(10)\n    proc = run_encrypt(data)\n    procs.append(proc)\nThe child processes run in parallel and consume their input. Here, \nI wait for them to finish and then retrieve their final output. The \n output is random encrypted bytes as expected:\nfor proc in procs:\n    out, _ = proc.communicate()\n    print(out[-10:])\n>>>\nb'\\x8c(\\xed\\xc7m1\\xf0F4\\xe6'\nb'\\x0eD\\x97\\xe9>\\x10h{\\xbd\\xf0'\nb'g\\x93)\\x14U\\xa9\\xdc\\xdd\\x04\\xd2'\nIt’s also possible to create chains of parallel processes, just like \nUNIX pipelines, connecting the output of one child process to the \ninput of another, and so on. Here’s a function that starts the openssl \n command-line tool as a subprocess to generate a Whirlpool hash of \nthe input stream:\ndef run_hash(input_stdin):\n    return subprocess.Popen(\n        ['openssl', 'dgst', '-whirlpool', '-binary'],\n        stdin=input_stdin,\n        stdout=subprocess.PIPE)\nNow, I can kick off one set of processes to encrypt some data and \nanother set of processes to subsequently hash their encrypted output. \nNote that I have to be careful with how the stdout instance of the \n",
      "content_length": 1677,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "upstream process is retained by the Python interpreter process that’s \nstarting this pipeline of child processes:\nencrypt_procs = []\nhash_procs = []\nfor _ in range(3):\n    data = os.urandom(100)\n \n    encrypt_proc = run_encrypt(data)\n    encrypt_procs.append(encrypt_proc)\n \n    hash_proc = run_hash(encrypt_proc.stdout)\n    hash_procs.append(hash_proc)\n \n    # Ensure that the child consumes the input stream and\n    # the communicate() method doesn't inadvertently steal\n    # input from the child. Also lets SIGPIPE propagate to\n    # the upstream process if the downstream process dies.\n    encrypt_proc.stdout.close()\n    encrypt_proc.stdout = None\nThe I/O between the child processes happens automatically once they \nare started. All I need to do is wait for them to finish and print the \nfinal output:\nfor proc in encrypt_procs:\n    proc.communicate()\n    assert proc.returncode == 0\n \nfor proc in hash_procs:\n    out, _ = proc.communicate()\n    print(out[-10:])\n    assert proc.returncode == 0\n>>>\nb'\\xe2j\\x98h\\xfd\\xec\\xe7T\\xd84'\nb'\\xf3.i\\x01\\xd74|\\xf2\\x94E'\nb'5_n\\xc3-\\xe6j\\xeb[i'\nIf I’m worried about the child processes never finishing or somehow \nblocking on input or output pipes, I can pass the timeout parameter \nto the communicate method. This causes an exception to be raised if \nthe child process hasn’t finished within the time period, giving me a \nchance to terminate the misbehaving subprocess:\nproc = subprocess.Popen(['sleep', '10'])\ntry:\n    proc.communicate(timeout=0.1)\n \nItem 52: Use subprocess to Manage Child Processes \n229\n",
      "content_length": 1553,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "230 \nChapter 7 Concurrency and Parallelism\nexcept subprocess.TimeoutExpired:\n    proc.terminate()\n    proc.wait()\n \nprint('Exit status', proc.poll())\n>>>\nExit status -15\nThings to Remember\n✦ Use the subprocess module to run child processes and manage their \ninput and output streams.\n✦ Child processes run in parallel with the Python interpreter, enabling \nyou to maximize your usage of CPU cores.\n✦ Use the run convenience function for simple usage, and the Popen \nclass for advanced usage like UNIX-style pipelines.\n✦ Use the timeout parameter of the communicate method to avoid dead-\nlocks and hanging child processes.\nItem 53:  Use Threads for Blocking I/O, Avoid for \nParallelism\nThe standard implementation of Python is called CPython. CPython \nruns a Python program in two steps. First, it parses and compiles the \nsource text into bytecode, which is a low-level representation of the \nprogram as 8-bit instructions. (As of Python 3.6, however, it’s tech-\nnically wordcode with 16-bit instructions, but the idea is the same.) \nThen, CPython runs the bytecode using a stack-based interpreter. The \nbytecode interpreter has state that must be maintained and coherent \nwhile the Python program executes. CPython enforces coherence with \na mechanism called the global interpreter lock (GIL).\nEssentially, the GIL is a mutual-exclusion lock (mutex) that prevents \nCPython from being affected by preemptive multithreading, where \none thread takes control of a program by interrupting another thread. \nSuch an interruption could corrupt the interpreter state (e.g., garbage \ncollection reference counts) if it comes at an unexpected time. The \nGIL prevents these interruptions and ensures that every bytecode \ninstruction works correctly with the CPython implementation and its \nC-extension modules.\nThe GIL has an important negative side effect. With programs written \nin languages like C++ or Java, having multiple threads of execution \n",
      "content_length": 1939,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": " \nItem 53: Use Threads for Blocking I/O, Avoid for Parallelism \n231\nmeans that a program could utilize multiple CPU cores at the same \ntime. Although Python supports multiple threads of execution, the GIL \ncauses only one of them to ever make forward progress at a time. This \nmeans that when you reach for threads to do parallel computation \nand speed up your Python programs, you will be sorely disappointed.\nFor example, say that I want to do something computationally inten-\nsive with Python. Here, I use a naive number factorization algorithm \nas a proxy:\ndef factorize(number):\n    for i in range(1, number + 1):\n        if number % i == 0:\n            yield i\nFactoring a set of numbers in serial takes quite a long time:\nimport time\n \nnumbers = [2139079, 1214759, 1516637, 1852285]\nstart = time.time()\n \nfor number in numbers:\n    list(factorize(number))\n \nend = time.time()\ndelta = end - start\nprint(f'Took {delta:.3f} seconds')\n>>>\nTook 0.399 seconds\nUsing multiple threads to do this computation would make sense in \nother languages because I could take advantage of all the CPU cores \nof my computer. Let me try that in Python. Here, I define a Python \nthread for doing the same computation as before:\nfrom threading import Thread\n \nclass FactorizeThread(Thread):\n    def __init__(self, number):\n        super().__init__()\n        self.number = number\n \n    def run(self):\n        self.factors = list(factorize(self.number))\n",
      "content_length": 1437,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "232 \nChapter 7 Concurrency and Parallelism\nThen, I start a thread for each number to factorize in parallel:\nstart = time.time()\n \nthreads = []\nfor number in numbers:\n    thread = FactorizeThread(number)\n    thread.start()\n    threads.append(thread)\nFinally, I wait for all of the threads to finish:\nfor thread in threads:\n    thread.join()\n \nend = time.time()\ndelta = end - start\nprint(f'Took {delta:.3f} seconds')\n>>>\nTook 0.446 seconds\nSurprisingly, this takes even longer than running factorize in serial. \nWith one thread per number, you might expect less than a 4x speedup \nin other languages due to the overhead of creating threads and coor-\ndinating with them. You might expect only a 2x speedup on the dual-\ncore machine I used to run this code. But you wouldn’t expect the \nperformance of these threads to be worse when there are multiple \nCPUs to utilize. This demonstrates the effect of the GIL (e.g., lock con-\ntention and scheduling overhead) on programs running in the stan-\ndard CPython interpreter.\nThere are ways to get CPython to utilize multiple cores, but they \ndon’t work with the standard Thread class (see Item 64: “Consider \nconcurrent.futures for True Parallelism”), and they can require sub-\nstantial effort. Given these limitations, why does Python support \nthreads at all? There are two good reasons.\nFirst, multiple threads make it easy for a program to seem like it’s \ndoing multiple things at the same time. Managing the juggling act \nof simultaneous tasks is difficult to implement yourself (see Item 56: \n“Know How to Recognize When Concurrency Is Necessary” for an \nexample). With threads, you can leave it to Python to run your func-\ntions concurrently. This works because CPython ensures a level of \nfairness between Python threads of execution, even though only one \nof them makes forward progress at a time due to the GIL.\nThe second reason Python supports threads is to deal with blocking \nI/O, which happens when Python does certain types of system calls. \n",
      "content_length": 1997,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": " \nItem 53: Use Threads for Blocking I/O, Avoid for Parallelism \n233\nA Python program uses system calls to ask the computer’s  operating \nsystem to interact with the external environment on its behalf. Block-\ning I/O includes things like reading and writing files, interacting \nwith networks, communicating with devices like displays, and so on. \nThreads help handle blocking I/O by insulating a program from the \ntime it takes for the operating system to respond to requests.\nFor example, say that I want to send a signal to a remote-controlled \nhelicopter through a serial port. I’ll use a slow system call (select) as \na proxy for this activity. This function asks the operating system to \nblock for 0.1 seconds and then return control to my program, which is \nsimilar to what would happen when using a synchronous serial port:\nimport select\nimport socket\n \ndef slow_systemcall():\n    select.select([socket.socket()], [], [], 0.1)\nRunning this system call in serial requires a linearly increasing \namount of time:\nstart = time.time()\n \nfor _ in range(5):\n    slow_systemcall()\n \nend = time.time()\ndelta = end - start\nprint(f'Took {delta:.3f} seconds')\n>>>\nTook 0.510 seconds\nThe problem is that while the slow_systemcall function is running, my \nprogram can’t make any other progress. My program’s main thread of \nexecution is blocked on the select system call. This situation is awful \nin practice. You need to be able to compute your helicopter’s next move \nwhile you’re sending it a signal; otherwise, it’ll crash. When you find \nyourself needing to do blocking I/O and computation simultaneously, \nit’s time to consider moving your system calls to threads.\nHere, I run multiple invocations of the slow_systemcall function in \nseparate threads. This would allow me to communicate with multiple \nserial ports (and helicopters) at the same time while leaving the main \nthread to do whatever computation is required:\nstart = time.time()\n \n",
      "content_length": 1941,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "234 \nChapter 7 Concurrency and Parallelism\nthreads = []\nfor _ in range(5):\n    thread = Thread(target=slow_systemcall)\n    thread.start()\n    threads.append(thread)\nWith the threads started, here I do some work to calculate the next \nhelicopter move before waiting for the system call threads to finish:\ndef compute_helicopter_location(index):\n    ...\n \nfor i in range(5):\n    compute_helicopter_location(i)\n \nfor thread in threads:\n    thread.join()\n \nend = time.time()\ndelta = end - start\nprint(f'Took {delta:.3f} seconds')\n>>>\nTook 0.108 seconds\nThe parallel time is ~5x less than the serial time. This shows that \nall the system calls will run in parallel from multiple Python threads \neven though they’re limited by the GIL. The GIL prevents my Python \ncode from running in parallel, but it doesn’t have an effect on system \ncalls. This works because Python threads release the GIL just before \nthey make system calls, and they reacquire the GIL as soon as the \nsystem calls are done.\nThere are many other ways to deal with blocking I/O besides using \nthreads, such as the asyncio built-in module, and these alternatives \nhave important benefits. But those options might require extra work \nin refactoring your code to fit a different model of execution (see Item \n60: “Achieve Highly Concurrent I/O with Coroutines” and Item 62: \n“Mix Threads and Coroutines to Ease the Transition to asyncio”). \nUsing threads is the simplest way to do blocking I/O in parallel with \nminimal changes to your program.\nThings to Remember\n✦ Python threads can’t run in parallel on multiple CPU cores because \nof the global interpreter lock (GIL).\n",
      "content_length": 1633,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": " \nItem 54: Use Lock to Prevent Data Races in Threads \n235\n✦ Python threads are still useful despite the GIL because they provide \nan easy way to do multiple things seemingly at the same time.\n✦ Use Python threads to make multiple system calls in parallel. This \nallows you to do blocking I/O at the same time as computation.\nItem 54: Use Lock to Prevent Data Races in Threads\nAfter learning about the global interpreter lock (GIL) (see Item 53: \n“Use Threads for Blocking I/O, Avoid for Parallelism”), many new \nPython programmers assume they can forgo using mutual- exclusion \nlocks (also called mutexes) in their code altogether. If the GIL is \nalready  preventing Python threads from running on multiple CPU \ncores in parallel, it must also act as a lock for a program’s data struc-\ntures, right? Some testing on types like lists and dictionaries may \neven show that this assumption appears to hold.\nBut beware, this is not truly the case. The GIL will not protect you. \nAlthough only one Python thread runs at a time, a thread’s opera-\ntions on data structures can be interrupted between any two byte-\ncode instructions in the Python interpreter. This is dangerous if you \naccess the same objects from multiple threads simultaneously. The \ninvariants of your data structures could be violated at practically any \ntime because of these interruptions, leaving your program in a cor-\nrupted state.\nFor example, say that I want to write a program that counts many \nthings in parallel, like sampling light levels from a whole network of \nsensors. If I want to determine the total number of light samples over \ntime, I can aggregate them with a new class:\nclass Counter:\n    def __init__(self):\n        self.count = 0\n \n    def increment(self, offset):\n        self.count += offset\nImagine that each sensor has its own worker thread because reading \nfrom the sensor requires blocking I/O. After each sensor measure-\nment, the worker thread increments the counter up to a maximum \nnumber of desired readings:\ndef worker(sensor_index, how_many, counter):\n    for _ in range(how_many):\n        # Read from the sensor\n        ...\n        counter.increment(1)\n",
      "content_length": 2153,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "236 \nChapter 7 Concurrency and Parallelism\nHere, I run one worker thread for each sensor in parallel and wait for \nthem all to finish their readings:\nfrom threading import Thread\n \nhow_many = 10**5\ncounter = Counter()\n \nthreads = []\nfor i in range(5):\n    thread = Thread(target=worker,\n                    args=(i, how_many, counter))\n    threads.append(thread)\n    thread.start()\n \nfor thread in threads:\n    thread.join()\n \nexpected = how_many * 5\nfound = counter.count\nprint(f'Counter should be {expected}, got {found}')\n>>>\nCounter should be 500000, got 246760\nThis seemed straightforward, and the outcome should have been \nobvious, but the result is way off! What happened here? How could \nsomething so simple go so wrong, especially since only one Python \ninterpreter thread can run at a time?\nThe Python interpreter enforces fairness between all of the threads \nthat are executing to ensure they get roughly equal processing time. \nTo do this, Python suspends a thread as it’s running and resumes \nanother thread in turn. The problem is that you don’t know exactly \nwhen Python will suspend your threads. A thread can even be paused \nseemingly halfway through what looks like an atomic operation. \nThat’s what happened in this case.\nThe body of the Counter object’s increment method looks simple, and is \nequivalent to this statement from the perspective of the worker thread:\ncounter.count += 1\nBut the += operator used on an object attribute actually instructs \nPython to do three separate operations behind the scenes. The state-\nment above is equivalent to this:\nvalue = getattr(counter, 'count')\nresult = value + 1\nsetattr(counter, 'count', result)\n",
      "content_length": 1662,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": " \nItem 54: Use Lock to Prevent Data Races in Threads \n237\nPython threads incrementing the counter can be suspended between \nany two of these operations. This is problematic if the way the oper-\nations interleave causes old versions of value to be assigned to the \ncounter. Here’s an example of bad interaction between two threads, \nA and B:\n# Running in Thread A\nvalue_a = getattr(counter, 'count')\n# Context switch to Thread B\nvalue_b = getattr(counter, 'count')\nresult_b = value_b + 1\nsetattr(counter, 'count', result_b)\n# Context switch back to Thread A\nresult_a = value_a + 1\nsetattr(counter, 'count', result_a)\nThread B interrupted thread A before it had completely finished. \nThread B ran and finished, but then thread A resumed mid-execution, \noverwriting all of thread B’s progress in incrementing the counter. \nThis is exactly what happened in the light sensor example above.\nTo prevent data races like these, and other forms of data structure \ncorruption, Python includes a robust set of tools in the threading \nbuilt-in module. The simplest and most useful of them is the Lock \nclass, a mutual-exclusion lock (mutex).\nBy using a lock, I can have the Counter class protect its current \nvalue against simultaneous accesses from multiple threads. Only one \nthread will be able to acquire the lock at a time. Here, I use a with \nstatement to acquire and release the lock; this makes it easier to see \nwhich code is executing while the lock is held (see Item 66: “Consider \ncontextlib and with Statements for Reusable try/finally Behavior” \nfor background):\nfrom threading import Lock\n \nclass LockingCounter:\n    def __init__(self):\n        self.lock = Lock()\n        self.count = 0\n \n    def increment(self, offset):\n        with self.lock:\n            self.count += offset\n",
      "content_length": 1781,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "238 \nChapter 7 Concurrency and Parallelism\nNow, I run the worker threads as before but use a LockingCounter \ninstead:\ncounter = LockingCounter()\n \nfor i in range(5):\n    thread = Thread(target=worker,\n                    args=(i, how_many, counter))\n    threads.append(thread)\n    thread.start()\n \nfor thread in threads:\n    thread.join()\n \nexpected = how_many * 5\nfound = counter.count\nprint(f'Counter should be {expected}, got {found}')\n>>>\nCounter should be 500000, got 500000\nThe result is exactly what I expect. Lock solved the problem.\nThings to Remember\n✦ Even though Python has a global interpreter lock, you’re still \nresponsible for protecting against data races between the threads in \nyour programs.\n✦ Your programs will corrupt their data structures if you allow mul-\ntiple threads to modify the same objects without mutual-exclusion \nlocks (mutexes).\n✦ Use the Lock class from the threading built-in module to enforce \nyour program’s invariants between multiple threads.\nItem 55:  Use Queue to Coordinate Work Between \nThreads\nPython programs that do many things concurrently often need to \ncoordinate their work. One of the most useful arrangements for con-\ncurrent work is a pipeline of functions.\nA pipeline works like an assembly line used in manufacturing. Pipe-\nlines have many phases in serial, with a specific function for each \nphase. New pieces of work are constantly being added to the begin-\nning of the pipeline. The functions can operate concurrently, each \n",
      "content_length": 1486,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": " \nItem 55: Use Queue to Coordinate Work Between Threads \n239\nworking on the piece of work in its phase. The work moves forward \nas each function completes until there are no phases remaining. This \napproach is especially good for work that includes blocking I/O or \nsubprocesses—activities that can easily be parallelized using Python \n(see Item 53: “Use Threads for Blocking I/O, Avoid for Parallelism”).\nFor example, say I want to build a system that will take a constant \nstream of images from my digital camera, resize them, and then add \nthem to a photo gallery online. Such a program could be split into \nthree phases of a pipeline. New images are retrieved in the first phase. \nThe downloaded images are passed through the resize function in the \nsecond phase. The resized images are consumed by the upload func-\ntion in the final phase.\nImagine that I’ve already written Python functions that execute the \nphases: download, resize, upload. How do I assemble a pipeline to do \nthe work concurrently?\ndef download(item):\n    ...\n \ndef resize(item):\n    ...\n \ndef upload(item):\n    ...\nThe first thing I need is a way to hand off work between the pipeline \nphases. This can be modeled as a thread-safe producer–consumer \nqueue (see Item 54: “Use Lock to Prevent Data Races in Threads” to \nunderstand the importance of thread safety in Python; see Item 71: \n“Prefer deque for Producer–Consumer Queues” to understand queue \nperformance):\nfrom collections import deque\nfrom threading import Lock\n \nclass MyQueue:\n    def __init__(self):\n        self.items = deque()\n        self.lock = Lock()\nThe producer, my digital camera, adds new images to the end of the \ndeque of pending items:\n    def put(self, item):\n        with self.lock:\n            self.items.append(item)\n",
      "content_length": 1772,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "240 \nChapter 7 Concurrency and Parallelism\nThe consumer, the first phase of the processing pipeline, removes \nimages from the front of the deque of pending items:\n    def get(self):\n        with self.lock:\n            return self.items.popleft()\nHere, I represent each phase of the pipeline as a Python thread that \ntakes work from one queue like this, runs a function on it, and puts \nthe result on another queue. I also track how many times the worker \nhas checked for new input and how much work it’s completed:\nfrom threading import Thread\nimport time\n \nclass Worker(Thread):\n    def __init__(self, func, in_queue, out_queue):\n        super().__init__()\n        self.func = func\n        self.in_queue = in_queue\n        self.out_queue = out_queue\n        self.polled_count = 0\n        self.work_done = 0\nThe trickiest part is that the worker thread must properly han-\ndle the case where the input queue is empty because the previous \nphase hasn’t completed its work yet. This happens where I catch the \nIndexError exception below. You can think of this as a holdup in the \nassembly line:\n    def run(self):\n        while True:\n            self.polled_count += 1\n            try:\n                item = self.in_queue.get()\n            except IndexError:\n                time.sleep(0.01)  # No work to do\n            else:\n                result = self.func(item)\n                self.out_queue.put(result)\n                self.work_done += 1\nNow, I can connect the three phases together by creating the queues \nfor their coordination points and the corresponding worker threads:\ndownload_queue = MyQueue()\nresize_queue = MyQueue()\nupload_queue = MyQueue()\n",
      "content_length": 1659,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": " \nItem 55: Use Queue to Coordinate Work Between Threads \n241\ndone_queue = MyQueue()\nthreads = [\n    Worker(download, download_queue, resize_queue),\n    Worker(resize, resize_queue, upload_queue),\n    Worker(upload, upload_queue, done_queue),\n]\nI can start the threads and then inject a bunch of work into the first \nphase of the pipeline. Here, I use a plain object instance as a proxy \nfor the real data required by the download function:\nfor thread in threads:\n    thread.start()\n \nfor _ in range(1000):\n    download_queue.put(object())\nNow, I wait for all of the items to be processed by the pipeline and end \nup in the done_queue:\nwhile len(done_queue.items) < 1000:\n    # Do something useful while waiting\n    ...\nThis runs properly, but there’s an interesting side effect caused by \nthe threads polling their input queues for new work. The tricky part, \nwhere I catch IndexError exceptions in the run method, executes a \nlarge number of times:\nprocessed = len(done_queue.items)\npolled = sum(t.polled_count for t in threads)\nprint(f'Processed {processed} items after '\n      f'polling {polled} times')\n>>>\nProcessed 1000 items after polling 3035 times\nWhen the worker functions vary in their respective speeds, an ear-\nlier phase can prevent progress in later phases, backing up the pipe-\nline. This causes later phases to starve and constantly check their \ninput queues for new work in a tight loop. The outcome is that worker \nthreads waste CPU time doing nothing useful; they’re constantly rais-\ning and catching IndexError exceptions.\nBut that’s just the beginning of what’s wrong with this implementa-\ntion. There are three more problems that you should also avoid. First, \ndetermining that all of the input work is complete requires yet another \nbusy wait on the done_queue. Second, in Worker, the run method will \nexecute forever in its busy loop. There’s no obvious way to signal to a \nworker thread that it’s time to exit.\n",
      "content_length": 1937,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "242 \nChapter 7 Concurrency and Parallelism\nThird, and worst of all, a backup in the pipeline can cause the \n program to crash arbitrarily. If the first phase makes rapid progress \nbut the second phase makes slow progress, then the queue connecting \nthe first phase to the second phase will constantly increase in size. \nThe second phase won’t be able to keep up. Given enough time and \ninput data, the program will eventually run out of memory and die.\nThe lesson here isn’t that pipelines are bad; it’s that it’s hard to build \na good producer–consumer queue yourself. So why even try?\nQueue to the Rescue\nThe Queue class from the queue built-in module provides all of the \nfunctionality you need to solve the problems outlined above.\nQueue eliminates the busy waiting in the worker by making the get \nmethod block until new data is available. For example, here I start a \nthread that waits for some input data on a queue:\nfrom queue import Queue\n \nmy_queue = Queue()\n \ndef consumer():\n    print('Consumer waiting')\n    my_queue.get()              # Runs after put() below\n    print('Consumer done')\n \nthread = Thread(target=consumer)\nthread.start()\nEven though the thread is running first, it won’t finish until an item \nis put on the Queue instance and the get method has something to \nreturn:\nprint('Producer putting')\nmy_queue.put(object())          # Runs before get() above\nprint('Producer done')\nthread.join()\n>>>\nConsumer waiting\nProducer putting\nProducer done\nConsumer done\nTo solve the pipeline backup issue, the Queue class lets you specify \nthe maximum amount of pending work to allow between two phases. \n",
      "content_length": 1619,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": " \nItem 55: Use Queue to Coordinate Work Between Threads \n243\nThis buffer size causes calls to put to block when the queue is already \nfull. For example, here I define a thread that waits for a while before \nconsuming a queue:\nmy_queue = Queue(1)             # Buffer size of 1\n \ndef consumer():\n    time.sleep(0.1)             # Wait\n    my_queue.get()              # Runs second\n    print('Consumer got 1')\n    my_queue.get()              # Runs fourth\n    print('Consumer got 2')\n    print('Consumer done')\n \nthread = Thread(target=consumer)\nthread.start()\nThe wait should allow the producer thread to put both objects on the \nqueue before the consumer thread ever calls get. But the Queue size \nis one. This means the producer adding items to the queue will have \nto wait for the consumer thread to call get at least once before the \nsecond call to put will stop blocking and add the second item to the \nqueue:\nmy_queue.put(object())          # Runs first\nprint('Producer put 1')\nmy_queue.put(object())          # Runs third\nprint('Producer put 2')\nprint('Producer done')\nthread.join()\n>>>\nProducer put 1\nConsumer got 1\nProducer put 2\nProducer done\nConsumer got 2\nConsumer done\nThe Queue class can also track the progress of work using the \ntask_done method. This lets you wait for a phase’s input queue to \ndrain and eliminates the need to poll the last phase of a pipeline (as \nwith the done_queue above). For example, here I define a consumer \nthread that calls task_done when it finishes working on an item:\nin_queue = Queue()\n \n",
      "content_length": 1536,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "244 \nChapter 7 Concurrency and Parallelism\ndef consumer():\n    print('Consumer waiting')\n    work = in_queue.get()       # Runs second\n    print('Consumer working')\n    # Doing work\n    ...\n    print('Consumer done')\n    in_queue.task_done()        # Runs third\n \nthread = Thread(target=consumer)\nthread.start()\nNow, the producer code doesn’t have to join the consumer thread or \npoll. The producer can just wait for the in_queue to finish by calling \njoin on the Queue instance. Even once it’s empty, the in_queue won’t \nbe joinable until after task_done is called for every item that was ever \nenqueued:\nprint('Producer putting')\nin_queue.put(object())         # Runs first\nprint('Producer waiting')\nin_queue.join()                # Runs fourth\nprint('Producer done')\nthread.join()\n>>>\nConsumer waiting\nProducer putting\nProducer waiting\nConsumer working\nConsumer done\nProducer done\nI can put all these behaviors together into a Queue subclass that also \ntells the worker thread when it should stop processing. Here, I define \na close method that adds a special sentinel item to the queue that \nindicates there will be no more input items after it:\nclass ClosableQueue(Queue):\n    SENTINEL = object()\n \n    def close(self):\n        self.put(self.SENTINEL)\nThen, I define an iterator for the queue that looks for this special \nobject and stops iteration when it’s found. This __iter__ method also \ncalls task_done at appropriate times, letting me track the progress of \n",
      "content_length": 1470,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": " \nItem 55: Use Queue to Coordinate Work Between Threads \n245\nwork on the queue (see Item 31: “Be Defensive When Iterating Over \nArguments” for details about __iter__):\n    def __iter__(self):\n        while True:\n            item = self.get()\n            try:\n                if item is self.SENTINEL:\n                    return  # Cause the thread to exit\n                yield item\n            finally:\n                self.task_done()\nNow, I can redefine my worker thread to rely on the behavior of \nthe ClosableQueue class. The thread will exit when the for loop is \nexhausted:\nclass StoppableWorker(Thread):\n    def __init__(self, func, in_queue, out_queue):\n        super().__init__()\n        self.func = func\n        self.in_queue = in_queue\n        self.out_queue = out_queue\n \n    def run(self):\n        for item in self.in_queue:\n            result = self.func(item)\n            self.out_queue.put(result)\nI re-create the set of worker threads using the new worker class:\ndownload_queue = ClosableQueue()\nresize_queue = ClosableQueue()\nupload_queue = ClosableQueue()\ndone_queue = ClosableQueue()\nthreads = [\n    StoppableWorker(download, download_queue, resize_queue),\n    StoppableWorker(resize, resize_queue, upload_queue),\n    StoppableWorker(upload, upload_queue, done_queue),\n]\nAfter running the worker threads as before, I also send the stop sig-\nnal after all the input work has been injected by closing the input \nqueue of the first phase:\nfor thread in threads:\n    thread.start()\n \n",
      "content_length": 1501,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "246 \nChapter 7 Concurrency and Parallelism\nfor _ in range(1000):\n    download_queue.put(object())\n \ndownload_queue.close()\nFinally, I wait for the work to finish by joining the queues that con-\nnect the phases. Each time one phase is done, I signal the next phase \nto stop by closing its input queue. At the end, the done_queue contains \nall of the output objects, as expected:\ndownload_queue.join()\nresize_queue.close()\nresize_queue.join()\nupload_queue.close()\nupload_queue.join()\nprint(done_queue.qsize(), 'items finished')\n \nfor thread in threads:\n    thread.join()\n>>>\n1000 items finished\nThis approach can be extended to use multiple worker threads per \nphase, which can increase I/O parallelism and speed up this type of \nprogram significantly. To do this, first I define some helper functions \nthat start and stop multiple threads. The way stop_threads works \nis by calling close on each input queue once per consuming thread, \nwhich ensures that all of the workers exit cleanly:\ndef start_threads(count, *args):\n    threads = [StoppableWorker(*args) for _ in range(count)]\n    for thread in threads:\n        thread.start()\n    return threads\n \ndef stop_threads(closable_queue, threads):\n    for _ in threads:\n        closable_queue.close()\n \n    closable_queue.join()\n \n    for thread in threads:\n        thread.join()\n",
      "content_length": 1327,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": " \nItem 55: Use Queue to Coordinate Work Between Threads \n247\nThen, I connect the pieces together as before, putting objects to pro-\ncess into the top of the pipeline, joining queues and threads along the \nway, and finally consuming the results:\ndownload_queue = ClosableQueue()\nresize_queue = ClosableQueue()\nupload_queue = ClosableQueue()\ndone_queue = ClosableQueue()\n \ndownload_threads = start_threads(\n    3, download, download_queue, resize_queue)\nresize_threads = start_threads(\n    4, resize, resize_queue, upload_queue)\nupload_threads = start_threads(\n    5, upload, upload_queue, done_queue)\n \nfor _ in range(1000):\n    download_queue.put(object())\n \nstop_threads(download_queue, download_threads)\nstop_threads(resize_queue, resize_threads)\nstop_threads(upload_queue, upload_threads)\n \nprint(done_queue.qsize(), 'items finished')\n>>>\n1000 items finished\nAlthough Queue works well in this case of a linear pipeline, there \nare many other situations for which there are better tools that you \nshould consider (see Item 60: “Achieve Highly Concurrent I/O with \nCoroutines”).\nThings to Remember\n✦ Pipelines are a great way to organize sequences of work—especially \nI/O-bound programs—that run concurrently using multiple Python \nthreads.\n✦ Be aware of the many problems in building concurrent pipelines: \nbusy waiting, how to tell workers to stop, and potential memory \nexplosion.\n✦ The Queue class has all the facilities you need to build robust \n pipelines: blocking operations, buffer sizes, and joining.\n",
      "content_length": 1512,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "248 \nChapter 7 Concurrency and Parallelism\nItem 56:  Know How to Recognize When Concurrency \nIs Necessary\nInevitably, as the scope of a program grows, it also becomes more \ncomplicated. Dealing with expanding requirements in a way that \nmaintains clarity, testability, and efficiency is one of the most difficult \nparts of programming. Perhaps the hardest type of change to handle \nis moving from a single-threaded program to one that needs multiple \nconcurrent lines of execution.\nLet me demonstrate how you might encounter this problem with an \nexample. Say that I want to implement Conway’s Game of Life, a clas-\nsic illustration of finite state automata. The rules of the game are sim-\nple: You have a two-dimensional grid of an arbitrary size. Each cell in \nthe grid can either be alive or empty:\nALIVE = '*'\nEMPTY = '-'\nThe game progresses one tick of the clock at a time. Every tick, each \ncell counts how many of its neighboring eight cells are still alive. \nBased on its neighbor count, a cell decides if it will keep living, die, \nor regenerate. (I’ll explain the specific rules further below.) Here’s an \nexample of a 5 × 5 Game of Life grid after four generations with time \ngoing to the right:\n  0   |   1   |   2   |   3   |   4  \n----- | ----- | ----- | ----- | -----\n-*--- | --*-- | --**- | --*-- | -----\n--**- | --**- | -*--- | -*--- | -**--\n---*- | --**- | --**- | --*-- | -----\n----- | ----- | ----- | ----- | -----\nI can represent the state of each cell with a simple container class. \nThe class must have methods that allow me to get and set the value \nof any coordinate. Coordinates that are out of bounds should wrap \naround, making the grid act like an infinite looping space:\nclass Grid:\n    def __init__(self, height, width):\n        self.height = height\n        self.width = width\n        self.rows = []\n        for _ in range(self.height):\n            self.rows.append([EMPTY] * self.width)\n \n",
      "content_length": 1921,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": " \nItem 56: Know How to Recognize When Concurrency Is Necessary \n249\n    def get(self, y, x):\n        return self.rows[y % self.height][x % self.width]\n \n    def set(self, y, x, state):\n        self.rows[y % self.height][x % self.width] = state\n \n    def __str__(self):\n        ...\nTo see this class in action, I can create a Grid instance and set its ini-\ntial state to a classic shape called a glider:\ngrid = Grid(5, 9)\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\ngrid.set(2, 4, ALIVE)\nprint(grid)\n>>>\n---*-----\n----*----\n--***----\n---------\n---------\nNow, I need a way to retrieve the status of neighboring cells. I can \ndo this with a helper function that queries the grid and returns the \ncount of living neighbors. I use a simple function for the get param-\neter instead of passing in a whole Grid instance in order to reduce \ncoupling (see Item 38: “Accept Functions Instead of Classes for Simple \nInterfaces” for more about this approach):\ndef count_neighbors(y, x, get):\n    n_ = get(y - 1, x + 0)  # North\n    ne = get(y - 1, x + 1)  # Northeast\n    e_ = get(y + 0, x + 1)  # East\n    se = get(y + 1, x + 1)  # Southeast\n    s_ = get(y + 1, x + 0)  # South\n    sw = get(y + 1, x - 1)  # Southwest\n    w_ = get(y + 0, x - 1)  # West\n    nw = get(y - 1, x - 1)  # Northwest\n    neighbor_states = [n_, ne, e_, se, s_, sw, w_, nw]\n    count = 0\n",
      "content_length": 1394,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "250 \nChapter 7 Concurrency and Parallelism\n    for state in neighbor_states:\n        if state == ALIVE:\n            count += 1\n    return count\nNow, I define the simple logic for Conway’s Game of Life, based on the \ngame’s three rules: Die if a cell has fewer than two neighbors, die if a \ncell has more than three neighbors, or become alive if an empty cell \nhas exactly three neighbors:\ndef game_logic(state, neighbors):\n    if state == ALIVE:\n        if neighbors < 2:\n            return EMPTY     # Die: Too few\n        elif neighbors > 3:\n            return EMPTY     # Die: Too many\n    else:\n        if neighbors == 3:\n            return ALIVE     # Regenerate\n    return state\nI can connect count_neighbors and game_logic together in another \nfunction that transitions the state of a cell. This function will be \ncalled each generation to figure out a cell’s current state, inspect the \nneighboring cells around it, determine what its next state should be, \nand update the resulting grid accordingly. Again, I use a function \ninterface for set instead of passing in the Grid instance to make this \ncode more decoupled:\ndef step_cell(y, x, get, set):\n    state = get(y, x)\n    neighbors = count_neighbors(y, x, get)\n    next_state = game_logic(state, neighbors)\n    set(y, x, next_state)\nFinally, I can define a function that progresses the whole grid of cells \nforward by a single step and then returns a new grid containing \nthe state for the next generation. The important detail here is that \nI need all dependent functions to call the get method on the previ-\nous  generation’s Grid instance, and to call the set method on the \nnext generation’s Grid instance. This is how I ensure that all of \nthe cells move in lockstep, which is an essential part of how the game \nworks. This is easy to achieve because I used function interfaces for \nget and set instead of passing Grid instances:\ndef simulate(grid):\n    next_grid = Grid(grid.height, grid.width)\n",
      "content_length": 1963,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": " \nItem 56: Know How to Recognize When Concurrency Is Necessary \n251\n    for y in range(grid.height):\n        for x in range(grid.width):\n            step_cell(y, x, grid.get, next_grid.set)\n    return next_grid\nNow, I can progress the grid forward one generation at a time. You \ncan see how the glider moves down and to the right on the grid based \non the simple rules from the game_logic function:\nclass ColumnPrinter:\n    ...\n \ncolumns = ColumnPrinter()\nfor i in range(5):\n    columns.append(str(grid))\n    grid = simulate(grid)\n \nprint(columns)\n>>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --*-*---- | ----*---- | ---*----- | ----*----\n--***---- | ---**---- | --*-*---- | ----**--- | -----*---\n--------- | ---*----- | ---**---- | ---**---- | ---***---\n--------- | --------- | --------- | --------- | ---------\nThis works great for a program that can run in one thread on a sin-\ngle machine. But imagine that the program’s requirements have \nchanged—as I alluded to above—and now I need to do some I/O (e.g., \nwith a socket) from within the game_logic function. For example, this \nmight be required if I’m trying to build a massively multiplayer online \ngame where the state transitions are determined by a combination of \nthe grid state and communication with other players over the Internet.\nHow can I extend this implementation to support such functional-\nity? The simplest thing to do is to add blocking I/O directly into the \ngame_logic function:\ndef game_logic(state, neighbors):\n    ...\n    # Do some blocking input/output in here:\n    data = my_socket.recv(100)\n    ...\nThe problem with this approach is that it’s going to slow down the \nwhole program. If the latency of the I/O required is 100 millisec-\nonds (i.e., a reasonably good cross-country, round-trip latency on the \n",
      "content_length": 1874,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "252 \nChapter 7 Concurrency and Parallelism\nInternet), and there are 45 cells in the grid, then each generation will \ntake a minimum of 4.5 seconds to evaluate because each cell is pro-\ncessed serially in the simulate function. That’s far too slow and will \nmake the game unplayable. It also scales poorly: If I later wanted to \nexpand the grid to 10,000 cells, I would need over 15 minutes to eval-\nuate each generation.\nThe solution is to do the I/O in parallel so each generation takes \nroughly 100 milliseconds, regardless of how big the grid is. The \nprocess of spawning a concurrent line of execution for each unit of \nwork—a cell in this case—is called fan-out. Waiting for all of those \nconcurrent units of work to finish before moving on to the next phase \nin a coordinated process—a generation in this case—is called fan-in.\nPython provides many built-in tools for achieving fan-out and fan-in \nwith various trade-offs. You should understand the pros and cons \nof each approach and choose the best tool for the job, depending \non the situation. See the items that follow for details based on this \nGame of Life example program (Item 57: “Avoid Creating New Thread \nInstances for On-demand Fan-out,” Item 58: “Understand How Using \nQueue for Concurrency Requires Refactoring,” Item 59: “Consider \nThreadPoolExecutor When Threads Are Necessary for Concurrency,” \nand Item 60: “Achieve Highly Concurrent I/O with Coroutines”).\nThings to Remember\n✦ A program often grows to require multiple concurrent lines of exe-\ncution as its scope and complexity increases.\n✦ The most common types of concurrency coordination are fan-out \n(generating new units of concurrency) and fan-in (waiting for exist-\ning units of concurrency to complete).\n✦ Python has many different ways of achieving fan-out and fan-in.\nItem 57:  Avoid Creating New Thread Instances for \nOn-demand Fan-out\nThreads are the natural first tool to reach for in order to do parallel \nI/O in Python (see Item 53: “Use Threads for Blocking I/O, Avoid for \nParallelism”). However, they have significant downsides when you try \nto use them for fanning out to many concurrent lines of execution.\nTo demonstrate this, I’ll continue with the Game of Life example from \nbefore (see Item 56: “Know How to Recognize When Concurrency Is \nNecessary” for background and the implementations of various func-\ntions and classes below). I’ll use threads to solve the latency problem \n",
      "content_length": 2431,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": " \nItem 57: Avoid Creating New Thread Instances for On-demand Fan-out \n253\ncaused by doing I/O in the game_logic function. To begin, threads \nrequire coordination using locks to ensure that assumptions within \ndata structures are maintained properly. I can create a subclass of \nthe Grid class that adds locking behavior so an instance can be used \nby multiple threads simultaneously:\nfrom threading import Lock\n \nALIVE = '*'\nEMPTY = '-'\n \nclass Grid:\n    ...\n \nclass LockingGrid(Grid):\n    def __init__(self, height, width):\n        super().__init__(height, width)\n        self.lock = Lock()\n \n    def __str__(self):\n        with self.lock:\n            return super().__str__()\n \n    def get(self, y, x):\n        with self.lock:\n            return super().get(y, x)\n \n    def set(self, y, x, state):\n        with self.lock:\n            return super().set(y, x, state)\nThen, I can reimplement the simulate function to fan out by creating a \nthread for each call to step_cell. The threads will run in parallel and \nwon’t have to wait on each other’s I/O. I can then fan in by waiting for \nall of the threads to complete before moving on to the next generation:\nfrom threading import Thread\n \ndef count_neighbors(y, x, get):\n    ...\n \ndef game_logic(state, neighbors):\n    ...\n    # Do some blocking input/output in here:\n    data = my_socket.recv(100)\n    ...\n \n",
      "content_length": 1360,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "254 \nChapter 7 Concurrency and Parallelism\ndef step_cell(y, x, get, set):\n    state = get(y, x)\n    neighbors = count_neighbors(y, x, get)\n    next_state = game_logic(state, neighbors)\n    set(y, x, next_state)\n \ndef simulate_threaded(grid):\n    next_grid = LockingGrid(grid.height, grid.width)\n \n    threads = []\n    for y in range(grid.height):\n        for x in range(grid.width):\n            args = (y, x, grid.get, next_grid.set)\n            thread = Thread(target=step_cell, args=args)\n            thread.start()  # Fan out\n            threads.append(thread)\n \n    for thread in threads:\n        thread.join()       # Fan in\n \n    return next_grid\nI can run this code using the same implementation of step_cell and \nthe same driving code as before with only two lines changed to use \nthe LockingGrid and simulate_threaded implementations:\nclass ColumnPrinter:\n    ...\n \ngrid = LockingGrid(5, 9)            # Changed\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\ngrid.set(2, 4, ALIVE)\n \ncolumns = ColumnPrinter()\nfor i in range(5):\n    columns.append(str(grid))\n    grid = simulate_threaded(grid)  # Changed\n \nprint(columns)\n",
      "content_length": 1171,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": ">>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --*-*---- | ----*---- | ---*----- | ----*----\n--***---- | ---**---- | --*-*---- | ----**--- | -----*---\n--------- | ---*----- | ---**---- | ---**---- | ---***---\n--------- | --------- | --------- | --------- | ---------\nThis works as expected, and the I/O is now parallelized between the \nthreads. However, this code has three big problems:\n \n■The Thread instances require special tools to coordinate with \neach other safely (see Item 54: “Use Lock to Prevent Data Races in \nThreads”). This makes the code that uses threads harder to rea-\nson about than the procedural, single-threaded code from before. \nThis complexity makes threaded code more difficult to extend and \nmaintain over time.\n \n■Threads require a lot of memory—about 8 MB per executing \nthread. On many computers, that amount of memory doesn’t mat-\nter for the 45 threads I’d need in this example. But if the game \ngrid had to grow to 10,000 cells, I would need to create that many \nthreads, which couldn’t even fit in the memory of my machine. \nRunning a thread per concurrent activity just won’t work.\n \n■Starting a thread is costly, and threads have a negative perfor-\nmance impact when they run due to context switching between \nthem. In this case, all of the threads are started and stopped each \ngeneration of the game, which has high overhead and will increase \nlatency beyond the expected I/O time of 100 milliseconds.\nThis code would also be very difficult to debug if something went wrong. \nFor example, imagine that the game_logic function raises an exception, \nwhich is highly likely due to the generally flaky nature of I/O:\ndef game_logic(state, neighbors):\n    ...\n    raise OSError('Problem with I/O')\n    ...\nI can test what this would do by running a Thread instance pointed at \nthis function and redirecting the sys.stderr output from the program \nto an in-memory StringIO buffer:\nimport contextlib\nimport io\n \n \nItem 57: Avoid Creating New Thread Instances for On-demand Fan-out \n255\n",
      "content_length": 2101,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "256 \nChapter 7 Concurrency and Parallelism\nfake_stderr = io.StringIO()\nwith contextlib.redirect_stderr(fake_stderr):\n    thread = Thread(target=game_logic, args=(ALIVE, 3))\n    thread.start()\n    thread.join()\n \nprint(fake_stderr.getvalue())\n>>>\nException in thread Thread-226:\nTraceback (most recent call last):\n  File \"threading.py\", line 917, in _bootstrap_inner\n    self.run()\n  File \"threading.py\", line 865, in run\n    self._target(*self._args, **self._kwargs)\n  File \"example.py\", line 193, in game_logic\n    raise OSError('Problem with I/O')\nOSError: Problem with I/O\nAn OSError exception is raised as expected, but somehow the code \nthat created the Thread and called join on it is unaffected. How can \nthis be? The reason is that the Thread class will independently catch \nany exceptions that are raised by the target function and then write \ntheir traceback to sys.stderr. Such exceptions are never re-raised to \nthe caller that started the thread in the first place.\nGiven all of these issues, it’s clear that threads are not the solution if \nyou need to constantly create and finish new concurrent functions. \nPython provides other solutions that are a better fit (see Item 58: \n“Understand How Using Queue for Concurrency Requires Refactoring,” \nItem 59: “Consider ThreadPoolExecutor When Threads Are Necessary \nfor Concurrency”, and Item 60: “Achieve Highly Concurrent I/O with \nCoroutines”).\nThings to Remember\n✦ Threads have many downsides: They’re costly to start and run \nif you need a lot of them, they each require a significant amount \nof memory, and they require special tools like Lock instances for \ncoordination.\n✦ Threads do not provide a built-in way to raise exceptions back in \nthe code that started a thread or that is waiting for one to finish, \nwhich makes them difficult to debug.\n",
      "content_length": 1815,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": " \nItem 58: Using Queue for Concurrency Requires Refactoring \n257\nItem 58:  Understand How Using Queue for Concurrency \nRequires Refactoring\nIn the previous item (see Item 57: “Avoid Creating New Thread \nInstances for On-demand Fan-out”) I covered the downsides of using \nThread to solve the parallel I/O problem in the Game of Life example \nfrom earlier (see Item 56: “Know How to Recognize When Concur-\nrency Is Necessary” for background and the implementations of vari-\nous functions and classes below).\nThe next approach to try is to implement a threaded pipeline using \nthe Queue class from the queue built-in module (see Item 55: “Use \nQueue to Coordinate Work Between Threads” for background; I rely on \nthe implementations of ClosableQueue and StoppableWorker from that \nitem in the example code below).\nHere’s the general approach: Instead of creating one thread per cell \nper generation of the Game of Life, I can create a fixed number of \nworker threads upfront and have them do parallelized I/O as needed. \nThis will keep my resource usage under control and eliminate the \noverhead of frequently starting new threads.\nTo do this, I need two ClosableQueue instances to use for communi-\ncating to and from the worker threads that execute the game_logic \nfunction:\nfrom queue import Queue\n \nclass ClosableQueue(Queue):\n    ...\n \nin_queue = ClosableQueue()\nout_queue = ClosableQueue()\nI can start multiple threads that will consume items from the \nin_queue, process them by calling game_logic, and put the results on \nout_queue. These threads will run concurrently, allowing for parallel \nI/O and reduced latency for each generation:\nfrom threading import Thread\n \nclass StoppableWorker(Thread):\n    ...\n \ndef game_logic(state, neighbors):\n    ...\n",
      "content_length": 1755,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "258 \nChapter 7 Concurrency and Parallelism\n    # Do some blocking input/output in here:\n    data = my_socket.recv(100)\n    ...\n \ndef game_logic_thread(item):\n    y, x, state, neighbors = item\n    try:\n        next_state = game_logic(state, neighbors)\n    except Exception as e:\n        next_state = e\n    return (y, x, next_state)\n \n# Start the threads upfront\nthreads = []\nfor _ in range(5):\n    thread = StoppableWorker(\n        game_logic_thread, in_queue, out_queue)\n    thread.start()\n    threads.append(thread)\nNow, I can redefine the simulate function to interact with these \nqueues to request state transition decisions and receive correspond-\ning responses. Adding items to in_queue causes fan-out, and consum-\ning items from out_queue until it’s empty causes fan-in:\nALIVE = '*'\nEMPTY = '-'\n \nclass SimulationError(Exception):\n    pass\n \nclass Grid:\n    ...\n \ndef count_neighbors(y, x, get):\n    ...\n \ndef simulate_pipeline(grid, in_queue, out_queue):\n    for y in range(grid.height):\n        for x in range(grid.width):\n            state = grid.get(y, x)\n            neighbors = count_neighbors(y, x, grid.get)\n            in_queue.put((y, x, state, neighbors))  # Fan out\n \n    in_queue.join()\n    out_queue.close()\n \n",
      "content_length": 1230,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "    next_grid = Grid(grid.height, grid.width)\n    for item in out_queue:                          # Fan in\n        y, x, next_state = item\n        if isinstance(next_state, Exception):\n            raise SimulationError(y, x) from next_state\n        next_grid.set(y, x, next_state)\n \n    return next_grid\nThe calls to Grid.get and Grid.set both happen within this new \nsimulate_pipeline function, which means I can use the  single-threaded \nimplementation of Grid instead of the implementation that requires \nLock instances for synchronization.\nThis code is also easier to debug than the Thread approach used \nin the previous item. If an exception occurs while doing I/O in the \ngame_logic function, it will be caught, propagated to the out_queue, \nand then re-raised in the main thread:\ndef game_logic(state, neighbors):\n    ...\n    raise OSError('Problem with I/O in game_logic')\n    ...\n \nsimulate_pipeline(Grid(1, 1), in_queue, out_queue)\n>>>\nTraceback ...\nOSError: Problem with I/O in game_logic\n \nThe above exception was the direct cause of the following \n¯exception:\n \nTraceback ...\nSimulationError: (0, 0)\nI can drive this multithreaded pipeline for repeated generations by \ncalling simulate_pipeline in a loop:\nclass ColumnPrinter:\n    ...\n \ngrid = Grid(5, 9)\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\n \nItem 58: Using Queue for Concurrency Requires Refactoring \n259\n",
      "content_length": 1421,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "260 \nChapter 7 Concurrency and Parallelism\ngrid.set(2, 4, ALIVE)\n \ncolumns = ColumnPrinter()\nfor i in range(5):\n    columns.append(str(grid))\n    grid = simulate_pipeline(grid, in_queue, out_queue)\n \nprint(columns)\n \nfor thread in threads:\n    in_queue.close()\nfor thread in threads:\n    thread.join()\n>>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --------- | --*-*---- | --------- | ----*----\n--***---- | --------- | ---**---- | --------- | --*-*----\n--------- | --------- | ---*----- | --------- | ---**----\n--------- | --------- | --------- | --------- | ---------\nThe results are the same as before. Although I’ve addressed the mem-\nory explosion problem, startup costs, and debugging issues of using \nthreads on their own, many issues remain:\n \n■The simulate_pipeline function is even harder to follow than the \nsimulate_threaded approach from the previous item.\n \n■Extra support classes were required for ClosableQueue and \nStoppableWorker in order to make the code easier to read, at the \nexpense of increased complexity.\n \n■I have to specify the amount of potential parallelism—the num-\nber of threads running game_logic_thread—upfront based on my \nexpectations of the workload instead of having the system auto-\nmatically scale up parallelism as needed.\n \n■In order to enable debugging, I have to manually catch exceptions \nin worker threads, propagate them on a Queue, and then re-raise \nthem in the main thread.\nHowever, the biggest problem with this code is apparent if the require-\nments change again. Imagine that later I needed to do I/O within \nthe count_neighbors function in addition to the I/O that was needed \nwithin game_logic:\ndef count_neighbors(y, x, get):\n    ...\n",
      "content_length": 1775,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "    # Do some blocking input/output in here:\n    data = my_socket.recv(100)\n    ...\nIn order to make this parallelizable, I need to add another stage to the \npipeline that runs count_neighbors in a thread. I need to make sure \nthat exceptions propagate correctly between the worker threads and \nthe main thread. And I need to use a Lock for the Grid class in order \nto ensure safe synchronization between the worker threads (see Item \n54: “Use Lock to Prevent Data Races in Threads” for background and \nItem 57: “Avoid Creating New Thread Instances for On-demand Fan-\nout” for the implementation of LockingGrid):\ndef count_neighbors_thread(item):\n    y, x, state, get = item\n    try:\n        neighbors = count_neighbors(y, x, get)\n    except Exception as e:\n        neighbors = e\n    return (y, x, state, neighbors)\n \ndef game_logic_thread(item):\n    y, x, state, neighbors = item\n    if isinstance(neighbors, Exception):\n        next_state = neighbors\n    else:\n        try:\n            next_state = game_logic(state, neighbors)\n        except Exception as e:\n            next_state = e\n    return (y, x, next_state)\n \nclass LockingGrid(Grid):\n    ...\nI \nhave \nto \ncreate \nanother \nset \nof \nQueue \ninstances \nfor \nthe \ncount_neighbors_thread workers and the corresponding Thread \ninstances:\nin_queue = ClosableQueue()\nlogic_queue = ClosableQueue()\nout_queue = ClosableQueue()\n \nthreads = []\n \n \nItem 58: Using Queue for Concurrency Requires Refactoring \n261\n",
      "content_length": 1459,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "262 \nChapter 7 Concurrency and Parallelism\nfor _ in range(5):\n    thread = StoppableWorker(\n        count_neighbors_thread, in_queue, logic_queue)\n    thread.start()\n    threads.append(thread)\n \nfor _ in range(5):\n    thread = StoppableWorker(\n        game_logic_thread, logic_queue, out_queue)\n    thread.start()\n    threads.append(thread)\nFinally, I need to update simulate_pipeline to coordinate the multiple \nphases in the pipeline and ensure that work fans out and back in \ncorrectly:\ndef simulate_phased_pipeline(\n        grid, in_queue, logic_queue, out_queue):\n    for y in range(grid.height):\n        for x in range(grid.width):\n            state = grid.get(y, x)\n            item = (y, x, state, grid.get)\n            in_queue.put(item)          # Fan out\n \n    in_queue.join()\n    logic_queue.join()                  # Pipeline sequencing\n    out_queue.close()\n \n    next_grid = LockingGrid(grid.height, grid.width)\n    for item in out_queue:              # Fan in\n        y, x, next_state = item\n        if isinstance(next_state, Exception):\n            raise SimulationError(y, x) from next_state\n        next_grid.set(y, x, next_state)\n \n    return next_grid\nWith these updated implementations, now I can run the multiphase \npipeline end-to-end:\ngrid = LockingGrid(5, 9)\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\ngrid.set(2, 4, ALIVE)\n \n",
      "content_length": 1397,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "columns = ColumnPrinter()\nfor i in range(5):\n    columns.append(str(grid))\n    grid = simulate_phased_pipeline(\n        grid, in_queue, logic_queue, out_queue)\n \nprint(columns)\n \nfor thread in threads:\n    in_queue.close()\nfor thread in threads:\n    logic_queue.close()\nfor thread in threads:\n    thread.join()\n>>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --*-*---- | ----*---- | ---*----- | ----*----\n--***---- | ---**---- | --*-*---- | ----**--- | -----*---\n--------- | ---*----- | ---**---- | ---**---- | ---***---\n--------- | --------- | --------- | --------- | ---------\nAgain, this works as expected, but it required a lot of changes and \nboilerplate. The point here is that Queue does make it possible to solve \nfan-out and fan-in problems, but the overhead is very high. Although \nusing Queue is a better approach than using Thread instances on their \nown, it’s still not nearly as good as some of the other tools provided \nby Python (see Item 59: “Consider ThreadPoolExecutor When Threads \nAre Necessary for Concurrency” and Item 60: “Achieve Highly Con-\ncurrent I/O with Coroutines”).\nThings to Remember\n✦ Using Queue instances with a fixed number of worker threads \nimproves the scalability of fan-out and fan-in using threads.\n✦ It takes a significant amount of work to refactor existing code to use \nQueue, especially when multiple stages of a pipeline are required.\n✦ Using Queue fundamentally limits the total amount of I/O paral-\nlelism a program can leverage compared to alternative approaches \nprovided by other built-in Python features and modules.\n \nItem 58: Using Queue for Concurrency Requires Refactoring \n263\n",
      "content_length": 1720,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "264 \nChapter 7 Concurrency and Parallelism\nItem 59:  Consider ThreadPoolExecutor When Threads \nAre Necessary for Concurrency\nPython includes the concurrent.futures built-in module, which pro-\nvides the ThreadPoolExecutor class. It combines the best of the Thread \n(see Item 57: “Avoid Creating New Thread Instances for On-demand \nFan-out”) and Queue (see Item 58: “Understand How Using Queue for \nConcurrency Requires Refactoring”) approaches to solving the par-\nallel I/O problem from the Game of Life example (see Item 56: “Know \nHow to Recognize When Concurrency Is Necessary” for background \nand the implementations of various functions and classes below):\nALIVE = '*'\nEMPTY = '-'\n \nclass Grid:\n    ...\n \nclass LockingGrid(Grid):\n    ...\n \ndef count_neighbors(y, x, get):\n    ...\n \ndef game_logic(state, neighbors):\n    ...\n    # Do some blocking input/output in here:\n    data = my_socket.recv(100)\n    ...\n \ndef step_cell(y, x, get, set):\n    state = get(y, x)\n    neighbors = count_neighbors(y, x, get)\n    next_state = game_logic(state, neighbors)\n    set(y, x, next_state)\nInstead of starting a new Thread instance for each Grid square, I can \nfan out by submitting a function to an executor that will be run in a \nseparate thread. Later, I can wait for the result of all tasks in order to \nfan in:\nfrom concurrent.futures import ThreadPoolExecutor\n \ndef simulate_pool(pool, grid):\n    next_grid = LockingGrid(grid.height, grid.width)\n \n",
      "content_length": 1446,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": " \nItem 59: Consider ThreadPoolExecutor When Threads Are Necessary \n265\n    futures = []\n    for y in range(grid.height):\n        for x in range(grid.width):\n            args = (y, x, grid.get, next_grid.set)\n            future = pool.submit(step_cell, *args)  # Fan out\n            futures.append(future)\n \n    for future in futures:\n        future.result()                             # Fan in\n \n    return next_grid\nThe threads used for the executor can be allocated in advance, which \nmeans I don’t have to pay the startup cost on each execution of \nsimulate_pool. I can also specify the maximum number of threads \nto use for the pool—using the max_workers parameter—to prevent the \nmemory blow-up issues associated with the naive Thread solution to \nthe parallel I/O problem:\nclass ColumnPrinter:\n    ...\n \ngrid = LockingGrid(5, 9)\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\ngrid.set(2, 4, ALIVE)\n \ncolumns = ColumnPrinter()\nwith ThreadPoolExecutor(max_workers=10) as pool:\n    for i in range(5):\n        columns.append(str(grid))\n        grid = simulate_pool(pool, grid)\n \nprint(columns)\n>>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --*-*---- | ----*---- | ---*----- | ----*----\n--***---- | ---**---- | --*-*---- | ----**--- | -----*---\n--------- | ---*----- | ---**---- | ---**---- | ---***---\n--------- | --------- | --------- | --------- | ---------\n",
      "content_length": 1490,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 289,
      "content": "266 \nChapter 7 Concurrency and Parallelism\nThe best part about the ThreadPoolExecutor class is that it automati-\ncally propagates exceptions back to the caller when the result method \nis called on the Future instance returned by the submit method:\ndef game_logic(state, neighbors):\n    ...\n    raise OSError('Problem with I/O')\n    ...\n \nwith ThreadPoolExecutor(max_workers=10) as pool:\n    task = pool.submit(game_logic, ALIVE, 3)\n    task.result()\n>>>\nTraceback ...\nOSError: Problem with I/O\nIf I needed to provide I/O parallelism for the count_neighbors func-\ntion in addition to game_logic, no modifications to the program would \nbe required since ThreadPoolExecutor already runs these functions \nconcurrently as part of step_cell. It’s even possible to achieve CPU \nparallelism by using the same interface if necessary (see Item 64: \n“Consider concurrent.futures for True Parallelism”).\nHowever, the big problem that remains is the limited amount of I/O par-\nallelism that ThreadPoolExecutor provides. Even if I use a max_workers \nparameter of 100, this solution still won’t scale if I need 10,000+ cells \nin the grid that require simultaneous I/O. ThreadPoolExecutor is a \ngood choice for situations where there is no asynchronous solution \n(e.g., file I/O), but there are better ways to maximize I/O parallel-\nism in many cases (see Item 60: “Achieve Highly Concurrent I/O with \nCoroutines”).\nThings to Remember\n✦ ThreadPoolExecutor enables simple I/O parallelism with limited \nrefactoring, easily avoiding the cost of thread startup each time fan-\nout concurrency is required.\n✦ Although ThreadPoolExecutor eliminates the potential memory \nblow-up issues of using threads directly, it also limits I/O parallel-\nism by requiring max_workers to be specified upfront.\nItem 60:  Achieve Highly Concurrent I/O with \nCoroutines\nThe previous items have tried to solve the parallel I/O problem for \nthe Game of Life example with varying degrees of success. (See Item \n56: “Know How to Recognize When Concurrency Is Necessary” for \n",
      "content_length": 2031,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": " \nItem 60: Achieve Highly Concurrent I/O with Coroutines \n267\nbackground and the implementations of various functions and classes \nbelow.) All of the other approaches fall short in their ability to han-\ndle thousands of simultaneously concurrent functions (see Item 57: \n“Avoid Creating New Thread Instances for On-demand Fan-out,” Item \n58: “Understand How Using Queue for Concurrency Requires Refactor-\ning,” and Item 59: “Consider ThreadPoolExecutor When Threads Are \nNecessary for Concurrency”).\nPython addresses the need for highly concurrent I/O with coroutines. \nCoroutines let you have a very large number of seemingly simultane-\nous functions in your Python programs. They’re implemented using \nthe async and await keywords along with the same infrastructure \nthat powers generators (see Item 30: “Consider Generators Instead of \nReturning Lists,” Item 34: “Avoid Injecting Data into Generators with \nsend,” and Item 35: “Avoid Causing State Transitions in Generators \nwith throw”). \nThe cost of starting a coroutine is a function call. Once a coroutine \nis active, it uses less than 1 KB of memory until it’s exhausted. Like \nthreads, coroutines are independent functions that can consume \ninputs from their environment and produce resulting outputs. The \ndifference is that coroutines pause at each await expression and \nresume executing an async function after the pending awaitable is \nresolved (similar to how yield behaves in generators).\nMany separate async functions advanced in lockstep all seem to \nrun simultaneously, mimicking the concurrent behavior of Python \nthreads. However, coroutines do this without the memory overhead, \nstartup and context switching costs, or complex locking and synchro-\nnization code that’s required for threads. The magical mechanism \npowering coroutines is the event loop, which can do highly concurrent \nI/O efficiently, while rapidly interleaving execution between appropri-\nately written functions.\nI can use coroutines to implement the Game of Life. My goal is to \nallow for I/O to occur within the game_logic function while overcom-\ning the problems from the Thread and Queue approaches in the previ-\nous items. To do this, first I indicate that the game_logic function is a \ncoroutine by defining it using async def instead of def. This will allow \nme to use the await syntax for I/O, such as an asynchronous read \nfrom a socket:\nALIVE = '*'\nEMPTY = '-'\n \n",
      "content_length": 2413,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "268 \nChapter 7 Concurrency and Parallelism\nclass Grid:\n    ...\n \ndef count_neighbors(y, x, get):\n    ...\n \nasync def game_logic(state, neighbors):\n    ...\n    # Do some input/output in here:\n    data = await my_socket.read(50)\n    ...\nSimilarly, I can turn step_cell into a coroutine by adding async to its \ndefinition and using await for the call to the game_logic function:\nasync def step_cell(y, x, get, set):\n    state = get(y, x)\n    neighbors = count_neighbors(y, x, get)\n    next_state = await game_logic(state, neighbors)\n    set(y, x, next_state)\nThe simulate function also needs to become a coroutine:\nimport asyncio\n \nasync def simulate(grid):\n    next_grid = Grid(grid.height, grid.width)\n \n    tasks = []\n    for y in range(grid.height):\n        for x in range(grid.width):\n            task = step_cell(\n                y, x, grid.get, next_grid.set)      # Fan out\n            tasks.append(task)\n \n    await asyncio.gather(*tasks)                    # Fan in\n \n    return next_grid\nThe coroutine version of the simulate function requires some \nexplanation:\n \n■Calling step_cell doesn’t immediately run that function. Instead, \nit returns a coroutine instance that can be used with an await \nexpression at a later time. This is similar to how generator func-\ntions that use yield return a generator instance when they’re \ncalled instead of executing immediately. Deferring execution like \nthis is the mechanism that causes fan-out.\n",
      "content_length": 1445,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": " \nItem 60: Achieve Highly Concurrent I/O with Coroutines \n269\n \n■The gather function from the asyncio built-in library causes \nfan-in. The await expression on gather instructs the event loop to \nrun the step_cell coroutines concurrently and resume execution \nof the simulate coroutine when all of them have been completed.\n \n■No locks are required for the Grid instance since all execution \noccurs within a single thread. The I/O becomes parallelized as \npart of the event loop that’s provided by asyncio.\nFinally, I can drive this code with a one-line change to the origi-\nnal example. This relies on the asyncio.run function to execute the \nsimulate coroutine in an event loop and carry out its dependent I/O:\nclass ColumnPrinter:\n    ...\n \ngrid = Grid(5, 9)\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\ngrid.set(2, 4, ALIVE)\n \ncolumns = ColumnPrinter()\nfor i in range(5):\n    columns.append(str(grid))\n    grid = asyncio.run(simulate(grid))   # Run the event loop\n \nprint(columns)\n>>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --*-*---- | ----*---- | ---*----- | ----*----\n--***---- | ---**---- | --*-*---- | ----**--- | -----*---\n--------- | ---*----- | ---**---- | ---**---- | ---***---\n--------- | --------- | --------- | --------- | ---------\nThe result is the same as before. All of the overhead associ-\nated with threads has been eliminated. Whereas the Queue and \nThreadPoolExecutor approaches are limited in their exception \n handling—merely re-raising exceptions across thread boundaries—\nwith coroutines I can actually use the interactive debugger to step \nthrough the code line by line (see Item 80: “Consider Interactive \nDebugging with pdb”):\nasync def game_logic(state, neighbors):\n    ...\n",
      "content_length": 1836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "270 \nChapter 7 Concurrency and Parallelism\n    raise OSError('Problem with I/O')\n    ...\n \nasyncio.run(game_logic(ALIVE, 3))\n>>>\nTraceback ...\nOSError: Problem with I/O\nLater, if my requirements change and I also need to do I/O from \nwithin count_neighbors, I can easily accomplish this by adding async \nand await keywords to the existing functions and call sites instead of \nhaving to restructure everything as I would have had to do if I were \nusing Thread or Queue instances (see Item 61: “Know How to Port \nThreaded I/O to asyncio” for another example):\nasync def count_neighbors(y, x, get):\n    ...\n \nasync def step_cell(y, x, get, set):\n    state = get(y, x)\n    neighbors = await count_neighbors(y, x, get)\n    next_state = await game_logic(state, neighbors)\n    set(y, x, next_state)\n \ngrid = Grid(5, 9)\ngrid.set(0, 3, ALIVE)\ngrid.set(1, 4, ALIVE)\ngrid.set(2, 2, ALIVE)\ngrid.set(2, 3, ALIVE)\ngrid.set(2, 4, ALIVE)\n \ncolumns = ColumnPrinter()\nfor i in range(5):\n    columns.append(str(grid))\n    grid = asyncio.run(simulate(grid))\n \nprint(columns)\n>>>\n    0     |     1     |     2     |     3     |     4    \n---*----- | --------- | --------- | --------- | ---------\n----*---- | --*-*---- | ----*---- | ---*----- | ----*----\n--***---- | ---**---- | --*-*---- | ----**--- | -----*---\n--------- | ---*----- | ---**---- | ---**---- | ---***---\n--------- | --------- | --------- | --------- | ---------\n",
      "content_length": 1407,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": " \nItem 61: Know How to Port Threaded I/O to asyncio \n271\nThe beauty of coroutines is that they decouple your code’s instruc-\ntions for the external environment (i.e., I/O) from the implementation \nthat carries out your wishes (i.e., the event loop). They let you focus \non the logic of what you’re trying to do instead of wasting time trying \nto figure out how you’re going to accomplish your goals concurrently.\nThings to Remember\n✦ Functions that are defined using the async keyword are called \ncoroutines. A caller can receive the result of a dependent coroutine \nby using the await keyword.\n✦ Coroutines provide an efficient way to run tens of thousands of \nfunctions seemingly at the same time.\n✦ Coroutines can use fan-out and fan-in in order to parallelize I/O, \nwhile also overcoming all of the problems associated with doing I/O \nin threads.\nItem 61: Know How to Port Threaded I/O to asyncio\nOnce you understand the advantage of coroutines (see Item 60: \n“Achieve Highly Concurrent I/O with Coroutines”), it may seem daunt-\ning to port an existing codebase to use them. Luckily, Python’s sup-\nport for asynchronous execution is well integrated into the language. \nThis makes it straightforward to move code that does threaded, \nblocking I/O over to coroutines and asynchronous I/O.\nFor example, say that I have a TCP-based server for playing a game \ninvolving guessing a number. The server takes lower and upper \nparameters that determine the range of numbers to consider. Then, \nthe server returns guesses for integer values in that range as they are \nrequested by the client. Finally, the server collects reports from the \nclient on whether each of those numbers was closer (warmer) or fur-\nther away (colder) from the client’s secret number.\nThe most common way to build this type of client/server system is by \nusing blocking I/O and threads (see Item 53: “Use Threads for Block-\ning I/O, Avoid for Parallelism”). To do this, I need a helper class that \ncan manage sending and receiving of messages. For my purposes, \neach line sent or received represents a command to be processed:\nclass EOFError(Exception):\n    pass\n \nclass ConnectionBase:\n    def __init__(self, connection):\n",
      "content_length": 2192,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 295,
      "content": "272 \nChapter 7 Concurrency and Parallelism\n        self.connection = connection\n        self.file = connection.makefile('rb')\n \n    def send(self, command):\n        line = command + '\\n'\n        data = line.encode()\n        self.connection.send(data)\n \n    def receive(self):\n        line = self.file.readline()\n        if not line:\n            raise EOFError('Connection closed')\n        return line[:-1].decode()\nThe server is implemented as a class that handles one connection at a \ntime and maintains the client’s session state: \nimport random\n \nWARMER = 'Warmer'\nCOLDER = 'Colder'\nUNSURE = 'Unsure'\nCORRECT = 'Correct'\n \nclass UnknownCommandError(Exception):\n    pass\n \nclass Session(ConnectionBase):\n    def __init__(self, *args):\n        super().__init__(*args)\n        self._clear_state(None, None)\n \n    def _clear_state(self, lower, upper):\n        self.lower = lower\n        self.upper = upper\n        self.secret = None\n        self.guesses = []\nIt has one primary method that handles incoming commands from \nthe client and dispatches them to methods as needed. Note that here \nI’m using an assignment expression (introduced in Python 3.8; see \nItem 10: “Prevent Repetition with Assignment Expressions”) to keep \nthe code short:\n    def loop(self):\n        while command := self.receive():\n",
      "content_length": 1302,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "            parts = command.split(' ')\n            if parts[0] == 'PARAMS':\n                self.set_params(parts)\n            elif parts[0] == 'NUMBER':\n                self.send_number()\n            elif parts[0] == 'REPORT':\n                self.receive_report(parts)\n            else:\n                raise UnknownCommandError(command)\nThe first command sets the lower and upper bounds for the numbers \nthat the server is trying to guess:\n    def set_params(self, parts):\n        assert len(parts) == 3\n        lower = int(parts[1])\n        upper = int(parts[2])\n        self._clear_state(lower, upper)\nThe second command makes a new guess based on the previous state \nthat’s stored in the client’s Session instance. Specifically, this code \nensures that the server will never try to guess the same number more \nthan once per parameter assignment:\n    def next_guess(self):\n        if self.secret is not None:\n            return self.secret\n \n        while True:\n            guess = random.randint(self.lower, self.upper)\n            if guess not in self.guesses:\n                return guess\n \n    def send_number(self):\n        guess = self.next_guess()\n        self.guesses.append(guess)\n        self.send(format(guess))\nThe third command receives the decision from the client of whether \nthe guess was warmer or colder, and it updates the Session state \naccordingly:\n    def receive_report(self, parts):\n        assert len(parts) == 2\n        decision = parts[1]\n \n        last = self.guesses[-1]\n \nItem 61: Know How to Port Threaded I/O to asyncio \n273\n",
      "content_length": 1562,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "274 \nChapter 7 Concurrency and Parallelism\n        if decision == CORRECT:\n            self.secret = last\n \n        print(f'Server: {last} is {decision}')\nThe client is also implemented using a stateful class:\nimport contextlib\nimport math\n \nclass Client(ConnectionBase):\n    def __init__(self, *args):\n        super().__init__(*args)\n        self._clear_state()\n \n    def _clear_state(self):\n        self.secret = None\n        self.last_distance = None\nThe parameters of each guessing game are set using a with state-\nment to ensure that state is correctly managed on the server side (see \nItem 66: “Consider contextlib and with Statements for Reusable try/\nfinally Behavior” for background and Item 63: “Avoid Blocking the \nasyncio Event Loop to Maximize Responsiveness” for another exam-\nple). This method sends the first command to the server:\n    @contextlib.contextmanager\n    def session(self, lower, upper, secret):\n        print(f'Guess a number between {lower} and {upper}!'\n              f' Shhhhh, it\\'s {secret}.')\n        self.secret = secret\n        self.send(f'PARAMS {lower} {upper}')\n        try:\n            yield\n        finally:\n            self._clear_state()\n            self.send('PARAMS 0 -1')\nNew guesses are requested from the server, using another method \nthat implements the second command:\n    def request_numbers(self, count):\n        for _ in range(count):\n            self.send('NUMBER')\n            data = self.receive()\n            yield int(data)\n            if self.last_distance == 0:\n                return\n",
      "content_length": 1546,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "Whether each guess from the server was warmer or colder than the \nlast is reported using the third command in the final method:\n    def report_outcome(self, number):\n        new_distance = math.fabs(number - self.secret)\n        decision = UNSURE\n \n        if new_distance == 0:\n            decision = CORRECT\n        elif self.last_distance is None:\n            pass\n        elif new_distance < self.last_distance:\n            decision = WARMER\n        elif new_distance > self.last_distance:\n            decision = COLDER\n \n        self.last_distance = new_distance\n \n        self.send(f'REPORT {decision}')\n        return decision\nI can run the server by having one thread listen on a socket and \nspawn additional threads to handle the new connections:\nimport socket\nfrom threading import Thread\n \ndef handle_connection(connection):\n    with connection:\n        session = Session(connection)\n        try:\n            session.loop()\n        except EOFError:\n            pass\n \ndef run_server(address):\n    with socket.socket() as listener:\n        listener.bind(address)\n        listener.listen()\n        while True:\n            connection, _ = listener.accept()\n            thread = Thread(target=handle_connection,\n                            args=(connection,),\n                            daemon=True)\n            thread.start()\n \nItem 61: Know How to Port Threaded I/O to asyncio \n275\n",
      "content_length": 1392,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "276 \nChapter 7 Concurrency and Parallelism\nThe client runs in the main thread and returns the results of the \nguessing game to the caller. This code explicitly exercises a variety \nof Python language features (for loops, with statements, generators, \ncomprehensions) so that below I can show what it takes to port these \nover to using coroutines:\ndef run_client(address):\n    with socket.create_connection(address) as connection:\n        client = Client(connection)\n \n        with client.session(1, 5, 3):\n            results = [(x, client.report_outcome(x))\n                       for x in client.request_numbers(5)]\n \n        with client.session(10, 15, 12):\n            for number in client.request_numbers(5):\n                outcome = client.report_outcome(number)\n                results.append((number, outcome))\n \n    return results\nFinally, I can glue all of this together and confirm that it works as \nexpected:\ndef main():\n    address = ('127.0.0.1', 1234)\n    server_thread = Thread(\n        target=run_server, args=(address,), daemon=True)\n    server_thread.start()\n \n    results = run_client(address)\n    for number, outcome in results:\n        print(f'Client: {number} is {outcome}')\n \nmain()\n>>>\nGuess a number between 1 and 5! Shhhhh, it's 3.\nServer: 4 is Unsure\nServer: 1 is Colder\nServer: 5 is Unsure\nServer: 3 is Correct\nGuess a number between 10 and 15! Shhhhh, it's 12.\nServer: 11 is Unsure\nServer: 10 is Colder\nServer: 12 is Correct\n",
      "content_length": 1456,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "Client: 4 is Unsure\nClient: 1 is Colder\nClient: 5 is Unsure\nClient: 3 is Correct\nClient: 11 is Unsure\nClient: 10 is Colder\nClient: 12 is Correct\nHow much effort is needed to convert this example to using async, \nawait, and the asyncio built-in module? \nFirst, I need to update my ConnectionBase class to provide coroutines \nfor send and receive instead of blocking I/O methods. I’ve marked \neach line that’s changed with a # Changed comment to make it clear \nwhat the delta is between this new example and the code above:\nclass AsyncConnectionBase:\n    def __init__(self, reader, writer):             # Changed\n        self.reader = reader                        # Changed\n        self.writer = writer                        # Changed\n \n    async def send(self, command):\n        line = command + '\\n'\n        data = line.encode()\n        self.writer.write(data)                     # Changed\n        await self.writer.drain()                   # Changed\n \n    async def receive(self):\n        line = await self.reader.readline()         # Changed\n        if not line:\n            raise EOFError('Connection closed')\n        return line[:-1].decode()\nI can create another stateful class to represent the session state for \na single connection. The only changes here are the class’s name and \ninheriting from AsyncConnectionBase instead of ConnectionBase:\nclass AsyncSession(AsyncConnectionBase):            # Changed\n    def __init__(self, *args):\n        ...\n \n    def _clear_values(self, lower, upper):\n        ...\nThe primary entry point for the server’s command processing loop \nrequires only minimal changes to become a coroutine:\n    async def loop(self):                           # Changed\n \nItem 61: Know How to Port Threaded I/O to asyncio \n277\n",
      "content_length": 1755,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "278 \nChapter 7 Concurrency and Parallelism\n        while command := await self.receive():      # Changed\n            parts = command.split(' ')\n            if parts[0] == 'PARAMS':\n                self.set_params(parts)\n            elif parts[0] == 'NUMBER':\n                await self.send_number()            # Changed\n            elif parts[0] == 'REPORT':\n                self.receive_report(parts)\n            else:\n                raise UnknownCommandError(command)\nNo changes are required for handling the first command:\n    def set_params(self, parts):\n        ...\nThe only change required for the second command is allowing asyn-\nchronous I/O to be used when guesses are transmitted to the client:\n    def next_guess(self):\n        ...\n \n    async def send_number(self):                    # Changed\n        guess = self.next_guess()\n        self.guesses.append(guess)\n        await self.send(format(guess))              # Changed\nNo changes are required for processing the third command:\n    def receive_report(self, parts):\n        ...\nSimilarly, the client class needs to be reimplemented to inherit from \nAsyncConnectionBase:\nclass AsyncClient(AsyncConnectionBase):             # Changed\n    def __init__(self, *args):\n        ...\n \n    def _clear_state(self):\n        ...\nThe first command method for the client requires a few async and await \nkeywords to be added. It also needs to use the asynccontextmanager \nhelper function from the contextlib built-in module:\n    @contextlib.asynccontextmanager                 # Changed\n    async def session(self, lower, upper, secret):  # Changed\n        print(f'Guess a number between {lower} and {upper}!'\n              f' Shhhhh, it\\'s {secret}.')\n",
      "content_length": 1707,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "        self.secret = secret\n        await self.send(f'PARAMS {lower} {upper}')  # Changed\n        try:\n            yield\n        finally:\n            self._clear_state()\n            await self.send('PARAMS 0 -1')          # Changed\nThe second command again only requires the addition of async and \nawait anywhere coroutine behavior is required:\n    async def request_numbers(self, count):         # Changed\n        for _ in range(count):\n            await self.send('NUMBER')               # Changed\n            data = await self.receive()             # Changed\n            yield int(data)\n            if self.last_distance == 0:\n                return\nThe third command only requires adding one async and one await \nkeyword:\n    async def report_outcome(self, number):         # Changed\n        ...\n        await self.send(f'REPORT {decision}')       # Changed\n        ...\nThe code that runs the server needs to be completely reimplemented \nto use the asyncio built-in module and its start_server function:\nimport asyncio\n \nasync def handle_async_connection(reader, writer):\n    session = AsyncSession(reader, writer)\n    try:\n        await session.loop()\n    except EOFError:\n        pass\n \nasync def run_async_server(address):\n    server = await asyncio.start_server(\n        handle_async_connection, *address)\n    async with server:\n        await server.serve_forever()\nThe run_client function that initiates the game requires changes on \nnearly every line. Any code that previously interacted with the block-\ning socket instances has to be replaced with asyncio versions of \n \nItem 61: Know How to Port Threaded I/O to asyncio \n279\n",
      "content_length": 1638,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "280 \nChapter 7 Concurrency and Parallelism\nsimilar functionality (which are marked with # New below). All other \nlines in the function that require interaction with coroutines need to \nuse async and await keywords as appropriate. If you forget to add one \nof these keywords in a necessary place, an exception will be raised at \nruntime.\nasync def run_async_client(address):\n    streams = await asyncio.open_connection(*address)   # New\n    client = AsyncClient(*streams)                      # New\n \n    async with client.session(1, 5, 3):\n        results = [(x, await client.report_outcome(x))\n                   async for x in client.request_numbers(5)]\n \n    async with client.session(10, 15, 12):\n        async for number in client.request_numbers(5):\n            outcome = await client.report_outcome(number)\n            results.append((number, outcome))\n \n    _, writer = streams                                 # New\n    writer.close()                                      # New\n    await writer.wait_closed()                          # New\n \n    return results\nWhat’s most interesting about run_async_client is that I didn’t have \nto restructure any of the substantive parts of interacting with the \nAsyncClient in order to port this function over to use coroutines. Each \nof the language features that I needed has a corresponding asynchro-\nnous version, which made the migration easy to do.\nThis won’t always be the case, though. There are currently no asyn-\nchronous versions of the next and iter built-in functions (see Item \n31: “Be Defensive When Iterating Over Arguments” for background); \nyou have to await on the __anext__ and __aiter__ methods directly. \nThere’s also no asynchronous version of yield from (see Item 33: \n“Compose Multiple Generators with yield from”), which makes it \nnoisier to compose generators. But given the rapid pace at which \nasync functionality is being added to Python, it’s only a matter of time \nbefore these features become available.\nFinally, the glue needs to be updated to run this new asynchro-\nnous example end-to-end. I use the asyncio.create_task function to \nenqueue the server for execution on the event loop so that it runs in \nparallel with the client when the await expression is reached. This is \n",
      "content_length": 2258,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "another approach to causing fan-out with different behavior than the \nasyncio.gather function:\nasync def main_async():\n    address = ('127.0.0.1', 4321)\n \n    server = run_async_server(address)\n    asyncio.create_task(server)\n \n    results = await run_async_client(address)\n    for number, outcome in results:\n        print(f'Client: {number} is {outcome}')\n \nasyncio.run(main_async())\n>>>\nGuess a number between 1 and 5! Shhhhh, it's 3.\nServer: 5 is Unsure\nServer: 4 is Warmer\nServer: 2 is Unsure\nServer: 1 is Colder\nServer: 3 is Correct\nGuess a number between 10 and 15! Shhhhh, it's 12.\nServer: 14 is Unsure\nServer: 10 is Unsure\nServer: 15 is Colder\nServer: 12 is Correct\nClient: 5 is Unsure\nClient: 4 is Warmer\nClient: 2 is Unsure\nClient: 1 is Colder\nClient: 3 is Correct\nClient: 14 is Unsure\nClient: 10 is Unsure\nClient: 15 is Colder\nClient: 12 is Correct\nThis works as expected. The coroutine version is easier to follow \nbecause all of the interactions with threads have been removed. The \nasyncio built-in module also provides many helper functions and \nshortens the amount of socket boilerplate required to write a server \nlike this.\nYour use case may be more complex and harder to port for a variety \nof reasons. The asyncio module has a vast number of I/O, synchro-\nnization, and task management features that could make adopting \n \nItem 61: Know How to Port Threaded I/O to asyncio \n281\n",
      "content_length": 1399,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "282 \nChapter 7 Concurrency and Parallelism\ncoroutines easier for you (see Item 62: “Mix Threads and Coroutines \nto Ease the Transition to asyncio” and Item 63: “Avoid Blocking the \nasyncio Event Loop to Maximize Responsiveness”). Be sure to check \nout the online documentation for the library (https://docs.python.\norg/3/library/asyncio.html) to understand its full potential.\nThings to Remember\n✦ Python provides asynchronous versions of for loops, with state-\nments, generators, comprehensions, and library helper functions \nthat can be used as drop-in replacements in coroutines.\n✦ The asyncio built-in module makes it straightforward to port exist-\ning code that uses threads and blocking I/O over to coroutines and \nasynchronous I/O.\nItem 62:  Mix Threads and Coroutines to Ease the \nTransition to asyncio\nIn the previous item (see Item 61: “Know How to Port Threaded I/O to \nasyncio”), I ported a TCP server that does blocking I/O with threads \nover to use asyncio with coroutines. The transition was big-bang: \nI moved all of the code to the new style in one go. But it’s rarely  feasible \nto port a large program this way. Instead, you usually need to incre-\nmentally migrate your codebase while also updating your tests as \nneeded and verifying that everything works at each step along the way.\nIn order to do that, your codebase needs to be able to use threads \nfor blocking I/O (see Item 53: “Use Threads for Blocking I/O, Avoid \nfor Parallelism”) and coroutines for asynchronous I/O (see Item 60: \n“Achieve Highly Concurrent I/O with Coroutines”) at the same time \nin a way that’s mutually compatible. Practically, this means that you \nneed threads to be able to run coroutines, and you need coroutines to \nbe able to start and wait on threads. Luckily, asyncio includes built-in \nfacilities for making this type of interoperability straightforward.\nFor example, say that I’m writing a program that merges log files into \none output stream to aid with debugging. Given a file handle for an \ninput log, I need a way to detect whether new data is available and \nreturn the next line of input. I can do this using the tell method of \nthe file handle to check whether the current read position matches the \nlength of the file. When no new data is present, an exception should \nbe raised (see Item 20: “Prefer Raising Exceptions to Returning None” \nfor background):\nclass NoNewData(Exception):\n    pass\n \n",
      "content_length": 2412,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 306,
      "content": " Item 62: Mix Threads and Coroutines to Ease the Transition to asyncio \n283\ndef readline(handle):\n    offset = handle.tell()\n    handle.seek(0, 2)\n    length = handle.tell()\n \n    if length == offset:\n        raise NoNewData\n \n    handle.seek(offset, 0)\n    return handle.readline()\nBy wrapping this function in a while loop, I can turn it into a worker \nthread. When a new line is available, I call a given callback function \nto write it to the output log (see Item 38: “Accept Functions Instead of \nClasses for Simple Interfaces” for why to use a function interface for \nthis instead of a class). When no data is available, the thread sleeps \nto reduce the amount of busy waiting caused by polling for new data. \nWhen the input file handle is closed, the worker thread exits:\nimport time\n \ndef tail_file(handle, interval, write_func):\n    while not handle.closed:\n        try:\n            line = readline(handle)\n        except NoNewData:\n            time.sleep(interval)\n        else:\n            write_func(line)\nNow, I can start one worker thread per input file and unify their out-\nput into a single output file. The write helper function below needs to \nuse a Lock instance (see Item 54: “Use Lock to Prevent Data Races in \nThreads”) in order to serialize writes to the output stream and make \nsure that there are no intra-line conflicts:\nfrom threading import Lock, Thread\n \ndef run_threads(handles, interval, output_path):\n    with open(output_path, 'wb') as output:\n        lock = Lock()\n        def write(data):\n            with lock:\n                output.write(data)\n \n",
      "content_length": 1583,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 307,
      "content": "284 \nChapter 7 Concurrency and Parallelism\n        threads = []\n        for handle in handles:\n            args = (handle, interval, write)\n            thread = Thread(target=tail_file, args=args)\n            thread.start()\n            threads.append(thread)\n \n        for thread in threads:\n            thread.join()\nAs long as an input file handle is still alive, its corresponding worker \nthread will also stay alive. That means it’s sufficient to wait for the \njoin method from each thread to complete in order to know that the \nwhole process is done.\nGiven a set of input paths and an output path, I can call run_threads \nand confirm that it works as expected. How the input file handles are \ncreated or separately closed isn’t important in order to demonstrate \nthis code’s behavior, nor is the output verification function—defined \nin confirm_merge that follows—which is why I’ve left them out here:\ndef confirm_merge(input_paths, output_path):\n    ...\n \ninput_paths = ...\nhandles = ...\noutput_path = ...\nrun_threads(handles, 0.1, output_path)\n \nconfirm_merge(input_paths, output_path)\nWith this threaded implementation as the starting point, how can \nI incrementally convert this code to use asyncio and coroutines \ninstead? There are two approaches: top-down and bottom-up.\nTop-down means starting at the highest parts of a codebase, like in \nthe main entry points, and working down to the individual functions \nand classes that are the leaves of the call hierarchy. This approach \ncan be useful when you maintain a lot of common modules that you \nuse across many different programs. By porting the entry points first, \nyou can wait to port the common modules until you’re already using \ncoroutines everywhere else.\nThe concrete steps are:\n 1. Change a top function to use async def instead of def.\n 2. Wrap all of its calls that do I/O—potentially blocking the event \nloop—to use asyncio.run_in_executor instead.\n",
      "content_length": 1923,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 308,
      "content": " 3. Ensure that the resources or callbacks used by run_in_executor \ninvocations are properly synchronized (i.e., using Lock or the \nasyncio.run_coroutine_threadsafe function).\n 4. Try to eliminate get_event_loop and run_in_executor calls by \nmoving downward through the call hierarchy and converting \nintermediate functions and methods to coroutines (following the \nfirst three steps).\nHere, I apply steps 1–3 to the run_threads function:\nimport asyncio\n \nasync def run_tasks_mixed(handles, interval, output_path):\n    loop = asyncio.get_event_loop()\n \n    with open(output_path, 'wb') as output:\n        async def write_async(data):\n            output.write(data)\n \n        def write(data):\n            coro = write_async(data)\n            future = asyncio.run_coroutine_threadsafe(\n                coro, loop)\n            future.result()\n \n        tasks = []\n        for handle in handles:\n            task = loop.run_in_executor(\n                None, tail_file, handle, interval, write)\n            tasks.append(task)\n \n        await asyncio.gather(*tasks)\nThe run_in_executor method instructs the event loop to run a given \nfunction—tail_file in this case—using a specific ThreadPoolExecutor \n(see Item 59: “Consider ThreadPoolExecutor When Threads Are Neces-\nsary for Concurrency”) or a default executor instance when the first \nparameter is None. By making multiple calls to run_in_executor with-\nout corresponding await expressions, the run_tasks_mixed coroutine \nfans out to have one concurrent line of execution for each input file. \nThen, the asyncio.gather function along with an await expression \nfans in the tail_file threads until they all complete (see Item 56: \n“Know How to Recognize When Concurrency Is Necessary” for more \nabout fan-out and fan-in).\n Item 62: Mix Threads and Coroutines to Ease the Transition to asyncio \n285\n",
      "content_length": 1846,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 309,
      "content": "286 \nChapter 7 Concurrency and Parallelism\nThis code eliminates the need for the Lock instance in the write helper \nby using asyncio.run_coroutine_threadsafe. This function allows \nplain old worker threads to call a coroutine—write_async in this \ncase—and have it execute in the event loop from the main thread (or \nfrom any other thread, if necessary). This effectively synchronizes the \nthreads together and ensures that all writes to the output file are only \ndone by the event loop in the main thread. Once the asyncio.gather \nawaitable is resolved, I can assume that all writes to the output file \nhave also completed, and thus I can close the output file handle in \nthe with statement without having to worry about race conditions.\nI can verify that this code works as expected. I use the asyncio.run \nfunction to start the coroutine and run the main event loop:\ninput_paths = ...\nhandles = ...\noutput_path = ...\nasyncio.run(run_tasks_mixed(handles, 0.1, output_path))\n \nconfirm_merge(input_paths, output_path)\nNow, I can apply step 4 to the run_tasks_mixed function by moving \ndown the call stack. I can redefine the tail_file dependent function \nto be an asynchronous coroutine instead of doing blocking I/O by fol-\nlowing steps 1–3:\nasync def tail_async(handle, interval, write_func):\n    loop = asyncio.get_event_loop()\n \n    while not handle.closed:\n        try:\n            line = await loop.run_in_executor(\n                None, readline, handle)\n        except NoNewData:\n            await asyncio.sleep(interval)\n        else:\n            await write_func(line)\nThis new implementation of tail_async allows me to push calls to \nget_event_loop and run_in_executor down the stack and out of the \nrun_tasks_mixed function entirely. What’s left is clean and much eas-\nier to follow:\nasync def run_tasks(handles, interval, output_path):\n    with open(output_path, 'wb') as output:\n        async def write_async(data):\n            output.write(data)\n \n",
      "content_length": 1962,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 310,
      "content": "        tasks = []\n        for handle in handles:\n            coro = tail_async(handle, interval, write_async)\n            task = asyncio.create_task(coro)\n            tasks.append(task)\n \n        await asyncio.gather(*tasks)\nI can verify that run_tasks works as expected, too:\ninput_paths = ...\nhandles = ...\noutput_path = ...\nasyncio.run(run_tasks(handles, 0.1, output_path))\n \nconfirm_merge(input_paths, output_path)\nIt’s possible to continue this iterative refactoring pattern and convert \nreadline into an asynchronous coroutine as well. However, that func-\ntion requires so many blocking file I/O operations that it doesn’t seem \nworth porting, given how much that would reduce the clarity of the \ncode and hurt performance. In some situations, it makes sense to \nmove everything to asyncio, and in others it doesn’t.\nThe bottom-up approach to adopting coroutines has four steps that \nare similar to the steps of the top-down style, but the process tra-\nverses the call hierarchy in the opposite direction: from leaves to \nentry points.\nThe concrete steps are:\n 1. Create a new asynchronous coroutine version of each leaf func-\ntion that you’re trying to port.\n 2. Change the existing synchronous functions so they call the \ncoroutine versions and run the event loop instead of implement-\ning any real behavior.\n 3. Move up a level of the call hierarchy, make another layer of corou-\ntines, and replace existing calls to synchronous functions with \ncalls to the coroutines defined in step 1.\n 4. Delete synchronous wrappers around coroutines created in step 2 \nas you stop requiring them to glue the pieces together.\nFor the example above, I would start with the tail_file function since \nI decided that the readline function should keep using blocking I/O. \nI can rewrite tail_file so it merely wraps the tail_async coroutine \nthat I defined above. To run that coroutine until it finishes, I need to \n Item 62: Mix Threads and Coroutines to Ease the Transition to asyncio \n287\n",
      "content_length": 1984,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 311,
      "content": "288 \nChapter 7 Concurrency and Parallelism\ncreate an event loop for each tail_file worker thread and then call \nits run_until_complete method. This method will block the current \nthread and drive the event loop until the tail_async coroutine exits, \nachieving the same behavior as the threaded, blocking I/O version of \ntail_file:\ndef tail_file(handle, interval, write_func):\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n \n    async def write_async(data):\n        write_func(data)\n \n    coro = tail_async(handle, interval, write_async)\n    loop.run_until_complete(coro)\nThis new tail_file function is a drop-in replacement for the old one. \nI can verify that everything works as expected by calling run_threads \nagain:\ninput_paths = ...\nhandles = ...\noutput_path = ...\nrun_threads(handles, 0.1, output_path)\n \nconfirm_merge(input_paths, output_path)\nAfter wrapping tail_async with tail_file, the next step is to convert \nthe run_threads function to a coroutine. This ends up being the same \nwork as step 4 of the top-down approach above, so at this point, the \nstyles converge.\nThis is a great start for adopting asyncio, but there’s even more \nthat you could do to increase the responsiveness of your program \n(see Item 63: “Avoid Blocking the asyncio Event Loop to Maximize \nResponsiveness”).\nThings to Remember\n✦ The awaitable run_in_executor method of the asyncio event \nloop enables coroutines to run synchronous functions in \nThreadPoolExecutor pools. This facilitates top-down migrations to \nasyncio.\n✦ The run_until_complete method of the asyncio event loop enables \nsynchronous code to run a coroutine until it finishes. The \nasyncio.run_coroutine_threadsafe function provides the same \nfunctionality across thread boundaries. Together these help with \nbottom-up migrations to asyncio.\n",
      "content_length": 1819,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 312,
      "content": " \nItem 63: Avoid Blocking the asyncio Event Loop \n289\nItem 63:  Avoid Blocking the asyncio Event Loop to \nMaximize Responsiveness\nIn the previous item I showed how to migrate to asyncio incrementally \n(see Item 62: “Mix Threads and Coroutines to Ease the Transition to \nasyncio” for background and the implementation of various functions \nbelow). The resulting coroutine properly tails input files and merges \nthem into a single output:\nimport asyncio\n \nasync def run_tasks(handles, interval, output_path):\n    with open(output_path, 'wb') as output:\n        async def write_async(data):\n            output.write(data)\n \n        tasks = []\n        for handle in handles:\n            coro = tail_async(handle, interval, write_async)\n            task = asyncio.create_task(coro)\n            tasks.append(task)\n \n        await asyncio.gather(*tasks)\nHowever, it still has one big problem: The open, close, and write calls \nfor the output file handle happen in the main event loop. These opera-\ntions all require making system calls to the program’s host operating \nsystem, which may block the event loop for significant amounts of \ntime and prevent other coroutines from making progress. This could \nhurt overall responsiveness and increase latency, especially for pro-\ngrams such as highly concurrent servers.\nI can detect when this problem happens by passing the debug=True \nparameter to the asyncio.run function. Here, I show how the file and \nline of a bad coroutine, presumably blocked on a slow system call, \ncan be identified:\nimport time\n \nasync def slow_coroutine():\n    time.sleep(0.5)  # Simulating slow I/O\n \nasyncio.run(slow_coroutine(), debug=True)\n>>>\nExecuting <Task finished name='Task-1' coro=<slow_coroutine() \n¯done, defined at example.py:29> result=None created \n¯at .../asyncio/base_events.py:487> took 0.503 seconds\n...\n",
      "content_length": 1840,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 313,
      "content": "290 \nChapter 7 Concurrency and Parallelism\nIf I want the most responsive program possible, I need to minimize \nthe potential system calls that are made from within the event loop. \nIn this case, I can create a new Thread subclass (see Item 53: “Use \nThreads for Blocking I/O, Avoid for Parallelism”) that encapsulates \neverything required to write to the output file using its own event \nloop:\nfrom threading import Thread\n \nclass WriteThread(Thread):\n    def __init__(self, output_path):\n        super().__init__()\n        self.output_path = output_path\n        self.output = None\n        self.loop = asyncio.new_event_loop()\n \n    def run(self):\n        asyncio.set_event_loop(self.loop)\n        with open(self.output_path, 'wb') as self.output:\n            self.loop.run_forever()\n \n        # Run one final round of callbacks so the await on\n        # stop() in another event loop will be resolved.\n        self.loop.run_until_complete(asyncio.sleep(0))\nCoroutines in other threads can directly call and await on the write \nmethod of this class, since it’s merely a thread-safe wrapper around \nthe real_write method that actually does the I/O. This eliminates \nthe need for a Lock (see Item 54: “Use Lock to Prevent Data Races in \nThreads”):\n    async def real_write(self, data):\n        self.output.write(data)\n \n    async def write(self, data):\n        coro = self.real_write(data)\n        future = asyncio.run_coroutine_threadsafe(\n            coro, self.loop)\n        await asyncio.wrap_future(future)\nOther coroutines can tell the worker thread when to stop in a thread-\nsafe manner, using similar boilerplate:\n    async def real_stop(self):\n        self.loop.stop()\n \n",
      "content_length": 1677,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 314,
      "content": "    async def stop(self):\n        coro = self.real_stop()\n        future = asyncio.run_coroutine_threadsafe(\n            coro, self.loop)\n        await asyncio.wrap_future(future)\nI can also define the __aenter__ and __aexit__ methods to allow this \nclass to be used in with statements (see Item 66: “Consider contextlib \nand with Statements for Reusable try/finally Behavior”). This \nensures that the worker thread starts and stops at the right times \nwithout slowing down the main event loop thread:\n    async def __aenter__(self):\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, self.start)\n        return self\n \n    async def __aexit__(self, *_):\n        await self.stop()\nWith this new WriteThread class, I can refactor run_tasks into a fully \nasynchronous version that’s easy to read and completely avoids run-\nning slow system calls in the main event loop thread:\ndef readline(handle):\n    ...\n \nasync def tail_async(handle, interval, write_func):\n    ...\n \nasync def run_fully_async(handles, interval, output_path):\n    async with WriteThread(output_path) as output:\n        tasks = []\n        for handle in handles:\n            coro = tail_async(handle, interval, output.write)\n            task = asyncio.create_task(coro)\n            tasks.append(task)\n \n        await asyncio.gather(*tasks)\nI can verify that this works as expected, given a set of input handles \nand an output file path:\ndef confirm_merge(input_paths, output_path):\n    ...\n \n \nItem 63: Avoid Blocking the asyncio Event Loop \n291\n",
      "content_length": 1541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 315,
      "content": "292 \nChapter 7 Concurrency and Parallelism\ninput_paths = ...\nhandles = ...\noutput_path = ...\nasyncio.run(run_fully_async(handles, 0.1, output_path))\n \nconfirm_merge(input_paths, output_path)\nThings to Remember\n✦ Making system calls in coroutines—including blocking I/O and \nstarting threads—can reduce program responsiveness and increase \nthe perception of latency.\n✦ Pass the debug=True parameter to asyncio.run in order to detect \nwhen certain coroutines are preventing the event loop from reacting \nquickly.\nItem 64:  Consider concurrent.futures for True \nParallelism\nAt some point in writing Python programs, you may hit the perfor-\nmance wall. Even after optimizing your code (see Item 70: “Profile \nBefore Optimizing”), your program’s execution may still be too slow \nfor your needs. On modern computers that have an increasing num-\nber of CPU cores, it’s reasonable to assume that one solution would \nbe parallelism. What if you could split your code’s computation into \nindependent pieces of work that run simultaneously across multiple \nCPU cores?\nUnfortunately, Python’s global interpreter lock (GIL) prevents true \nparallelism in threads (see Item 53: “Use Threads for Blocking I/O, \nAvoid for Parallelism”), so that option is out. Another common sugges-\ntion is to rewrite your most performance-critical code as an extension \nmodule, using the C language. C gets you closer to the bare metal \nand can run faster than Python, eliminating the need for parallelism \nin some cases. C extensions can also start native threads indepen-\ndent of the Python interpreter that run in parallel and utilize multiple \nCPU cores with no concern for the GIL. Python’s API for C exten-\nsions is well documented and a good choice for an escape hatch. It’s \nalso worth checking out tools like SWIG (https://github.com/swig/\nswig) and CLIF (https://github.com/google/clif) to aid in extension \ndevelopment.\nBut rewriting your code in C has a high cost. Code that is short and \nunderstandable in Python can become verbose and complicated in C. \nSuch a port requires extensive testing to ensure that the functionality \n",
      "content_length": 2109,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 316,
      "content": " \nItem 64: Consider concurrent.futures for True Parallelism \n293\nis equivalent to the original Python code and that no bugs have been \nintroduced. Sometimes it’s worth it, which explains the large ecosys-\ntem of C-extension modules in the Python community that speed up \nthings like text parsing, image compositing, and matrix math. There \nare even open source tools such as Cython (https://cython.org) and \nNumba (https://numba.pydata.org) that can ease the transition to C.\nThe problem is that moving one piece of your program to C isn’t suffi-\ncient most of the time. Optimized Python programs usually don’t have \none major source of slowness; rather, there are often many signifi-\ncant contributors. To get the benefits of C’s bare metal and threads, \nyou’d need to port large parts of your program, drastically increasing \ntesting needs and risk. There must be a better way to preserve your \ninvestment in Python to solve difficult computational problems.\nThe multiprocessing built-in module, which is easily accessed via the \nconcurrent.futures built-in module, may be exactly what you need \n(see Item 59: “Consider ThreadPoolExecutor When Threads Are Neces-\nsary for Concurrency” for a related example). It enables Python to uti-\nlize multiple CPU cores in parallel by running additional interpreters \nas child processes. These child processes are separate from the main \ninterpreter, so their global interpreter locks are also separate. Each \nchild can fully utilize one CPU core. Each child has a link to the main \nprocess where it receives instructions to do computation and returns \nresults.\nFor example, say that I want to do something computationally inten-\nsive with Python and utilize multiple CPU cores. I’ll use an implemen-\ntation of finding the greatest common divisor of two numbers as a \nproxy for a more computationally intense algorithm (like simulating \nfluid dynamics with the Navier–Stokes equation):\n# my_module.py\ndef gcd(pair):\n    a, b = pair\n    low = min(a, b)\n    for i in range(low, 0, -1):\n        if a % i == 0 and b % i == 0:\n            return i\n    assert False, 'Not reachable'\nRunning this function in serial takes a linearly increasing amount of \ntime because there is no parallelism:\n# run_serial.py\nimport my_module\nimport time\n \n",
      "content_length": 2274,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 317,
      "content": "294 \nChapter 7 Concurrency and Parallelism\nNUMBERS = [\n    (1963309, 2265973), (2030677, 3814172),\n    (1551645, 2229620), (2039045, 2020802),\n    (1823712, 1924928), (2293129, 1020491),\n    (1281238, 2273782), (3823812, 4237281),\n    (3812741, 4729139), (1292391, 2123811),\n]\n \ndef main():\n    start = time.time()\n    results = list(map(my_module.gcd, NUMBERS))\n    end = time.time()\n    delta = end - start\n    print(f'Took {delta:.3f} seconds')\n \nif __name__ == '__main__':\n    main()\n>>>\nTook 1.173 seconds\nRunning this code on multiple Python threads will yield no speed \nimprovement because the GIL prevents Python from using multiple \nCPU cores in parallel. Here, I do the same computation as above but \nusing the concurrent.futures module with its ThreadPoolExecutor \nclass and two worker threads (to match the number of CPU cores on \nmy computer):\n# run_threads.py\nimport my_module\nfrom concurrent.futures import ThreadPoolExecutor\nimport time\n \nNUMBERS = [\n    ...\n]\n \ndef main():\n    start = time.time()\n    pool = ThreadPoolExecutor(max_workers=2)\n    results = list(pool.map(my_module.gcd, NUMBERS))\n    end = time.time()\n    delta = end - start\n    print(f'Took {delta:.3f} seconds')\n \n",
      "content_length": 1200,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 318,
      "content": "if __name__ == '__main__':\n    main()\n>>>\nTook 1.436 seconds\nIt’s even slower this time because of the overhead of starting and com-\nmunicating with the pool of threads.\nNow for the surprising part: Changing a single line of code causes \nsomething magical to happen. If I replace the ThreadPoolExecutor \nwith the ProcessPoolExecutor from the concurrent.futures module, \neverything speeds up:\n# run_parallel.py\nimport my_module\nfrom concurrent.futures import ProcessPoolExecutor\nimport time\n \nNUMBERS = [\n    ...\n]\n \ndef main():\n    start = time.time()\n    pool = ProcessPoolExecutor(max_workers=2)  # The one change\n    results = list(pool.map(my_module.gcd, NUMBERS))\n    end = time.time()\n    delta = end - start\n    print(f'Took {delta:.3f} seconds')\n \nif __name__ == '__main__':\n    main()\n>>>\nTook 0.683 seconds\nRunning on my dual-core machine, this is significantly faster! How is \nthis possible? Here’s what the ProcessPoolExecutor class actually does \n(via the low-level constructs provided by the multiprocessing module):\n 1. It takes each item from the numbers input data to map.\n 2. It serializes the item into binary data by using the pickle module \n(see Item 68: “Make pickle Reliable with copyreg”).\n 3. It copies the serialized data from the main interpreter process to \na child interpreter process over a local socket.\n \nItem 64: Consider concurrent.futures for True Parallelism \n295\n",
      "content_length": 1400,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 319,
      "content": "296 \nChapter 7 Concurrency and Parallelism\n 4. It deserializes the data back into Python objects, using pickle in \nthe child process.\n 5. It imports the Python module containing the gcd function.\n 6. It runs the function on the input data in parallel with other child \nprocesses.\n 7. It serializes the result back into binary data.\n 8. It copies that binary data back through the socket.\n 9. It deserializes the binary data back into Python objects in the \nparent process.\n 10. It merges the results from multiple children into a single list to \nreturn.\nAlthough it looks simple to the programmer, the multiprocessing mod-\nule and ProcessPoolExecutor class do a huge amount of work to make \nparallelism possible. In most other languages, the only touch point \nyou need to coordinate two threads is a single lock or atomic operation \n(see Item 54: “Use Lock to Prevent Data Races in Threads” for an exam-\nple). The overhead of using multiprocessing via ProcessPoolExecutor is \nhigh because of all of the serialization and deserialization that must \nhappen between the parent and child processes.\nThis scheme is well suited to certain types of isolated, high-leverage \ntasks. By isolated, I mean functions that don’t need to share state \nwith other parts of the program. By high-leverage tasks, I mean sit-\nuations in which only a small amount of data must be transferred \nbetween the parent and child processes to enable a large amount of \ncomputation. The greatest common divisor algorithm is one example \nof this, but many other mathematical algorithms work similarly.\nIf your computation doesn’t have these characteristics, then the over-\nhead of ProcessPoolExecutor may prevent it from speeding up your \nprogram through parallelization. When that happens, multiprocessing \nprovides more advanced facilities for shared memory, cross-process \nlocks, queues, and proxies. But all of these features are very com-\nplex. It’s hard enough to reason about such tools in the memory space \nof a single process shared between Python threads. Extending that \ncomplexity to other processes and involving sockets makes this much \nmore difficult to understand.\nI suggest that you initially avoid all parts of the multiprocessing \nbuilt-in module. You can start by using the ThreadPoolExecutor \nclass to run isolated, high-leverage functions in threads. Later you \ncan move to the ProcessPoolExecutor to get a speedup. Finally, when \n",
      "content_length": 2421,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 320,
      "content": "you’ve completely exhausted the other options, you can consider using \nthe multiprocessing module directly.\nThings to Remember\n✦ Moving CPU bottlenecks to C-extension modules can be an effective \nway to improve performance while maximizing your investment in \nPython code. However, doing so has a high cost and may introduce \nbugs.\n✦ The multiprocessing module provides powerful tools that can paral-\nlelize certain types of Python computation with minimal effort.\n✦ The power of multiprocessing is best accessed through the \nconcurrent.futures built-in module and its simple ProcessPoolExecutor \nclass.\n✦ Avoid the advanced (and complicated) parts of the multiprocessing \nmodule until you’ve exhausted all other options.\n \nItem 64: Consider concurrent.futures for True Parallelism \n297\n",
      "content_length": 787,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 321,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 322,
      "content": "8\nRobustness and \nPerformance\nOnce you’ve written a useful Python program, the next step is to \n productionize your code so it’s bulletproof. Making programs depend-\nable when they encounter unexpected circumstances is just as \nimportant as making programs with correct functionality. Python has \nbuilt-in features and modules that aid in hardening your  programs so \nthey are robust in a wide variety of situations.\nOne dimension of robustness is scalability and performance. When \nyou’re implementing Python programs that handle a non-trivial \namount of data, you’ll often see slowdowns caused by the algorith-\nmic complexity of your code or other types of computational overhead. \nLuckily, Python includes many of the algorithms and data structures \nyou need to achieve high performance with minimal effort.\nItem 65:  Take Advantage of Each Block in try/except\n/else/finally\nThere are four distinct times when you might want to take action \nduring exception handling in Python. These are captured in the func-\ntionality of try, except, else, and finally blocks. Each block serves a \nunique purpose in the compound statement, and their various com-\nbinations are useful (see Item 87: “Define a Root Exception to Insulate \nCallers from APIs” for another example).\nfinally Blocks\nUse try/finally when you want exceptions to propagate up but also \nwant to run cleanup code even when exceptions occur. One common \nusage of try/finally is for reliably closing file handles (see Item 66: \n“Consider contextlib and with Statements for Reusable try/finally \nBehavior” for another—likely better—approach):\ndef try_finally_example(filename):\n    print('* Opening file')\n",
      "content_length": 1662,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 323,
      "content": "300 \nChapter 8 Robustness and Performance\n    handle = open(filename, encoding='utf-8') # Maybe OSError\n    try:\n        print('* Reading data')\n        return handle.read()  # Maybe UnicodeDecodeError\n    finally:\n        print('* Calling close()')\n        handle.close()        # Always runs after try block\nAny exception raised by the read method will always propagate up to \nthe calling code, but the close method of handle in the finally block \nwill run first:\nfilename = 'random_data.txt'\n \nwith open(filename, 'wb') as f:\n    f.write(b'\\xf1\\xf2\\xf3\\xf4\\xf5')  # Invalid utf-8\n \ndata = try_finally_example(filename)\n>>>\n* Opening file\n* Reading data\n* Calling close()\nTraceback ...\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in \n¯position 0: invalid continuation byte\nYou must call open before the try block because exceptions that occur \nwhen opening the file (like OSError if the file does not exist) should \nskip the finally block entirely:\ntry_finally_example('does_not_exist.txt')\n>>>\n* Opening file\nTraceback ...\nFileNotFoundError: [Errno 2] No such file or directory: \n¯'does_not_exist.txt'\nelse Blocks\nUse try/except/else to make it clear which exceptions will be han-\ndled by your code and which exceptions will propagate up. When \nthe try block doesn’t raise an exception, the else block runs. The \nelse block helps you minimize the amount of code in the try block, \nwhich is good for isolating potential exception causes and improves \n",
      "content_length": 1466,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 324,
      "content": " \nItem 65: Take Advantage of Each Block in try/except/else/finally \n301\nreadability. For example, say that I want to load JSON dictionary data \nfrom a string and return the value of a key it contains:\nimport json\n \ndef load_json_key(data, key):\n    try:\n        print('* Loading JSON data')\n        result_dict = json.loads(data)  # May raise ValueError\n    except ValueError as e:\n        print('* Handling ValueError')\n        raise KeyError(key) from e\n    else:\n        print('* Looking up key')\n        return result_dict[key]         # May raise KeyError\nIn the successful case, the JSON data is decoded in the try block, \nand then the key lookup occurs in the else block:\nassert load_json_key('{\"foo\": \"bar\"}', 'foo') == 'bar'\n>>>\n* Loading JSON data\n* Looking up key\nIf the input data isn’t valid JSON, then decoding with json.loads \nraises a ValueError. The exception is caught by the except block and \nhandled:\nload_json_key('{\"foo\": bad payload', 'foo')\n>>>\n* Loading JSON data\n* Handling ValueError\nTraceback ...\nJSONDecodeError: Expecting value: line 1 column 9 (char 8)\n \nThe above exception was the direct cause of the following \n¯exception:\n \nTraceback ...\nKeyError: 'foo'\nIf the key lookup raises any exceptions, they propagate up to the \ncaller because they are outside the try block. The else clause ensures \nthat what follows the try/except is visually distinguished from the \nexcept block. This makes the exception propagation behavior clear:\nload_json_key('{\"foo\": \"bar\"}', 'does not exist')\n",
      "content_length": 1514,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 325,
      "content": "302 \nChapter 8 Robustness and Performance\n>>>\n* Loading JSON data\n* Looking up key\nTraceback ...\nKeyError: 'does not exist'\nEverything Together\nUse try/except/else/finally when you want to do it all in one com-\npound statement. For example, say that I want to read a descrip-\ntion of work to do from a file, process it, and then update the file \nin-place. Here, the try block is used to read the file and process it; the \nexcept block is used to handle exceptions from the try block that are \nexpected; the else block is used to update the file in place and allow \nrelated exceptions to propagate up; and the finally block cleans up \nthe file handle:\nUNDEFINED = object()\n \ndef divide_json(path):\n    print('* Opening file')\n    handle = open(path, 'r+')   # May raise OSError\n    try:\n        print('* Reading data')\n        data = handle.read()    # May raise UnicodeDecodeError\n        print('* Loading JSON data')\n        op = json.loads(data)   # May raise ValueError\n        print('* Performing calculation')\n        value = (\n            op['numerator'] /\n            op['denominator'])  # May raise ZeroDivisionError\n    except ZeroDivisionError as e:\n        print('* Handling ZeroDivisionError')\n        return UNDEFINED\n    else:\n        print('* Writing calculation')\n        op['result'] = value\n        result = json.dumps(op)\n        handle.seek(0)          # May raise OSError\n        handle.write(result)    # May raise OSError\n        return value\n    finally:\n        print('* Calling close()')\n        handle.close()          # Always runs\n",
      "content_length": 1560,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 326,
      "content": "In the successful case, the try, else, and finally blocks run:\ntemp_path = 'random_data.json'\n \nwith open(temp_path, 'w') as f:\n    f.write('{\"numerator\": 1, \"denominator\": 10}')\n \nassert divide_json(temp_path) == 0.1\n>>>\n* Opening file\n* Reading data\n* Loading JSON data\n* Performing calculation\n* Writing calculation\n* Calling close()\nIf the calculation is invalid, the try, except, and finally blocks run, \nbut the else block does not:\nwith open(temp_path, 'w') as f:\n    f.write('{\"numerator\": 1, \"denominator\": 0}')\n \nassert divide_json(temp_path) is UNDEFINED\n>>>\n* Opening file\n* Reading data\n* Loading JSON data\n* Performing calculation\n* Handling ZeroDivisionError\n* Calling close()\nIf the JSON data was invalid, the try block runs and raises an excep-\ntion, the finally block runs, and then the exception is propagated up \nto the caller. The except and else blocks do not run:\nwith open(temp_path, 'w') as f:\n    f.write('{\"numerator\": 1 bad data')\n \ndivide_json(temp_path)\n>>>\n* Opening file\n* Reading data\n* Loading JSON data\n* Calling close()\nTraceback ...\nJSONDecodeError: Expecting ',' delimiter: line 1 column 17 \n¯(char 16)\n \nItem 65: Take Advantage of Each Block in try/except/else/finally \n303\n",
      "content_length": 1213,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 327,
      "content": "304 \nChapter 8 Robustness and Performance\nThis layout is especially useful because all of the blocks work together \nin intuitive ways. For example, here I simulate this by running the \ndivide_json function at the same time that my hard drive runs out of \ndisk space:\nwith open(temp_path, 'w') as f:\n    f.write('{\"numerator\": 1, \"denominator\": 10}')\n \ndivide_json(temp_path)\n>>>\n* Opening file\n* Reading data\n* Loading JSON data\n* Performing calculation\n* Writing calculation\n* Calling close()\nTraceback ...\nOSError: [Errno 28] No space left on device\nWhen the exception was raised in the else block while rewriting the \nresult data, the finally block still ran and closed the file handle as \nexpected.\nThings to Remember\n✦ The try/finally compound statement lets you run cleanup code \nregardless of whether exceptions were raised in the try block.\n✦ The else block helps you minimize the amount of code in try blocks \nand visually distinguish the success case from the try/except \nblocks.\n✦ An else block can be used to perform additional actions after a suc-\ncessful try block but before common cleanup in a finally block.\nItem 66:  Consider contextlib and with Statements for \nReusable try/finally Behavior\nThe with statement in Python is used to indicate when code is run-\nning in a special context. For example, mutual-exclusion locks (see \nItem 54: “Use Lock to Prevent Data Races in Threads”) can be used \nin with statements to indicate that the indented code block runs only \nwhile the lock is held:\nfrom threading import Lock\n \nlock = Lock()\n",
      "content_length": 1551,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 328,
      "content": " \nItem 66: Consider contextlib and with Statements \n305\nwith lock:\n    # Do something while maintaining an invariant\n    ...\nThe example above is equivalent to this try/finally construction \nbecause the Lock class properly enables the with statement (see Item \n65: “Take Advantage of Each Block in try/except/else/finally” for \nmore about try/finally):\nlock.acquire()\ntry:\n    # Do something while maintaining an invariant\n    ...\nfinally:\n    lock.release()\nThe with statement version of this is better because it eliminates the \nneed to write the repetitive code of the try/finally construction, and \nit ensures that you don’t forget to have a corresponding release call \nfor every acquire call.\nIt’s easy to make your objects and functions work in with statements \nby using the contextlib built-in module. This module contains the \ncontextmanager decorator (see Item 26: “Define Function Decorators \nwith functools.wraps” for background), which lets a simple function be \nused in with statements. This is much easier than defining a new class \nwith the special methods __enter__ and __exit__ (the standard way).\nFor example, say that I want a region of code to have more debug \nlogging sometimes. Here, I define a function that does logging at two \nseverity levels:\nimport logging\n \ndef my_function():\n    logging.debug('Some debug data')\n    logging.error('Error log here')\n    logging.debug('More debug data')\nThe default log level for my program is WARNING, so only the error mes-\nsage will print to screen when I run the function:\nmy_function()\n>>>\nError log here\n",
      "content_length": 1571,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 329,
      "content": "306 \nChapter 8 Robustness and Performance\nI can elevate the log level of this function temporarily by defining a \ncontext manager. This helper function boosts the logging severity \nlevel before running the code in the with block and reduces the log-\nging severity level afterward:\nfrom contextlib import contextmanager\n \n@contextmanager\ndef debug_logging(level):\n    logger = logging.getLogger()\n    old_level = logger.getEffectiveLevel()\n    logger.setLevel(level)\n    try:\n        yield\n    finally:\n        logger.setLevel(old_level)\nThe yield expression is the point at which the with block’s contents \nwill execute (see Item 30: “Consider Generators Instead of Returning \nLists” for background). Any exceptions that happen in the with block \nwill be re-raised by the yield expression for you to catch in the helper \nfunction (see Item 35: “Avoid Causing State Transitions in Generators \nwith throw” for how that works).\nNow, I can call the same logging function again but in the \ndebug_logging context. This time, all of the debug messages are \nprinted to the screen during the with block. The same function run-\nning outside the with block won’t print debug messages:\nwith debug_logging(logging.DEBUG):\n    print('* Inside:')\n    my_function()\n \nprint('* After:')\nmy_function()\n>>>\n* Inside:\nSome debug data\nError log here\nMore debug data\n* After:\nError log here\nUsing with Targets\nThe context manager passed to a with statement may also return an \nobject. This object is assigned to a local variable in the as part of the \n",
      "content_length": 1530,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 330,
      "content": "compound statement. This gives the code running in the with block \nthe ability to directly interact with its context. \nFor example, say I want to write a file and ensure that it’s always \nclosed correctly. I can do this by passing open to the with statement. \nopen returns a file handle for the as target of with, and it closes the \nhandle when the with block exits:\nwith open('my_output.txt', 'w') as handle:\n    handle.write('This is some data!')\nThis approach is more Pythonic than manually opening and closing \nthe file handle every time. It gives you confidence that the file is even-\ntually closed when execution leaves the with statement. By highlight-\ning the critical section, it also encourages you to reduce the amount \nof code that executes while the file handle is open, which is good \npractice in general.\nTo enable your own functions to supply values for as targets, all you \nneed to do is yield a value from your context manager. For example, \nhere I define a context manager to fetch a Logger instance, set its \nlevel, and then yield it as the target:\n@contextmanager\ndef log_level(level, name):\n    logger = logging.getLogger(name)\n    old_level = logger.getEffectiveLevel()\n    logger.setLevel(level)\n    try:\n        yield logger\n    finally:\n        logger.setLevel(old_level)\nCalling logging methods like debug on the as target produces output \nbecause the logging severity level is set low enough in the with block \non that specific Logger instance. Using the logging module directly \nwon’t print anything because the default logging severity level for the \ndefault program logger is WARNING:\nwith log_level(logging.DEBUG, 'my-log') as logger:\n    logger.debug(f'This is a message for {logger.name}!')\n    logging.debug('This will not print')\n>>>\nThis is a message for my-log!\nAfter the with statement exits, calling debug logging methods on the \nLogger named 'my-log' will not print anything because the default \n \nItem 66: Consider contextlib and with Statements \n307\n",
      "content_length": 1993,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 331,
      "content": "308 \nChapter 8 Robustness and Performance\nlogging severity level has been restored. Error log messages will \nalways print:\nlogger = logging.getLogger('my-log')\nlogger.debug('Debug will not print')\nlogger.error('Error will print')\n>>>\nError will print\nLater, I can change the name of the logger I want to use by simply \nupdating the with statement. This will point the Logger that’s the as \ntarget in the with block to a different instance, but I won’t have to \nupdate any of my other code to match:\nwith log_level(logging.DEBUG, 'other-log') as logger:\n    logger.debug(f'This is a message for {logger.name}!')\n    logging.debug('This will not print')\n>>>\nThis is a message for other-log!\nThis isolation of state and decoupling between creating a context and \nacting within that context is another benefit of the with statement.\nThings to Remember\n✦ The with statement allows you to reuse logic from try/finally blocks \nand reduce visual noise.\n✦ The contextlib built-in module provides a contextmanager decorator \nthat makes it easy to use your own functions in with statements.\n✦ The value yielded by context managers is supplied to the as part \nof the with statement. It’s useful for letting your code directly access \nthe cause of a special context.\nItem 67: Use datetime Instead of time for Local Clocks\nCoordinated Universal Time (UTC) is the standard, time-zone- \nindependent representation of time. UTC works great for computers \nthat represent time as seconds since the UNIX epoch. But UTC isn’t \nideal for humans. Humans reference time relative to where they’re \ncurrently located. People say “noon” or “8 am” instead of “UTC 15:00 \nminus 7 hours.” If your program handles time, you’ll probably find \nyourself converting time between UTC and local clocks for the sake of \nhuman understanding.\n",
      "content_length": 1803,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 332,
      "content": " \nItem 67: Use datetime Instead of time for Local Clocks \n309\nPython provides two ways of accomplishing time zone conversions. \nThe old way, using the time built-in module, is terribly error prone. \nThe new way, using the datetime built-in module, works great with \nsome help from the community-built package named pytz.\nYou should be acquainted with both time and datetime to thoroughly \nunderstand why datetime is the best choice and time should be \navoided.\nThe time Module\nThe localtime function from the time built-in module lets you convert \na UNIX timestamp (seconds since the UNIX epoch in UTC) to a local \ntime that matches the host computer’s time zone (Pacific Daylight \nTime in my case). This local time can be printed in human-readable \nformat using the strftime function:\nimport time\n \nnow = 1552774475\nlocal_tuple = time.localtime(now)\ntime_format = '%Y-%m-%d %H:%M:%S'\ntime_str = time.strftime(time_format, local_tuple)\nprint(time_str)\n>>>\n2019-03-16 15:14:35\nYou’ll often need to go the other way as well, starting with user input \nin human-readable local time and converting it to UTC time. You can \ndo this by using the strptime function to parse the time string, and \nthen calling mktime to convert local time to a UNIX timestamp:\ntime_tuple = time.strptime(time_str, time_format)\nutc_now = time.mktime(time_tuple)\nprint(utc_now)\n>>>\n1552774475.0\nHow do you convert local time in one time zone to local time in \nanother time zone? For example, say that I’m taking a flight between \nSan Francisco and New York, and I want to know what time it will be \nin San Francisco when I’ve arrived in New York.\nI might initially assume that I can directly manipulate the return val-\nues from the time, localtime, and strptime functions to do time zone \nconversions. But this is a very bad idea. Time zones change all the time \ndue to local laws. It’s too complicated to manage yourself, especially if \nyou want to handle every global city for flight departures and arrivals.\n",
      "content_length": 1983,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 333,
      "content": "310 \nChapter 8 Robustness and Performance\nMany operating systems have configuration files that keep up with \nthe time zone changes automatically. Python lets you use these time \nzones through the time module if your platform supports it. On other \nplatforms, such as Windows, some time zone functionality isn’t avail-\nable from time at all. For example, here I parse a departure time from \nthe San Francisco time zone, Pacific Daylight Time (PDT):\nimport os\n \nif os.name == 'nt':\n    print(\"This example doesn't work on Windows\")\nelse:\n    parse_format = '%Y-%m-%d %H:%M:%S %Z'\n    depart_sfo = '2019-03-16 15:45:16 PDT'\n    time_tuple = time.strptime(depart_sfo, parse_format)\n    time_str = time.strftime(time_format, time_tuple)\n    print(time_str)\n>>>\n2019-03-16 15:45:16\nAfter seeing that 'PDT' works with the strptime function, I might also \nassume that other time zones known to my computer will work. Unfor-\ntunately, this isn’t the case. strptime raises an exception when it sees \nEastern Daylight Time (EDT), which is the time zone for New York:\narrival_nyc = '2019-03-16 23:33:24 EDT'\ntime_tuple = time.strptime(arrival_nyc, time_format)\n>>>\nTraceback ...\nValueError: unconverted data remains:  EDT\nThe problem here is the platform-dependent nature of the time mod-\nule. Its behavior is determined by how the underlying C functions \nwork with the host operating system. This makes the functionality of \nthe time module unreliable in Python. The time module fails to consis-\ntently work properly for multiple local times. Thus, you should avoid \nusing the time module for this purpose. If you must use time, use it \nonly to convert between UTC and the host computer’s local time. For \nall other types of conversions, use the datetime module.\nThe datetime Module\nThe second option for representing times in Python is the datetime \nclass from the datetime built-in module. Like the time module, \ndatetime can be used to convert from the current time in UTC to local \ntime.\n",
      "content_length": 1981,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 334,
      "content": "Here, I convert the present time in UTC to my computer’s local time, \nPDT:\nfrom datetime import datetime, timezone\n \nnow = datetime(2019, 3, 16, 22, 14, 35)\nnow_utc = now.replace(tzinfo=timezone.utc)\nnow_local = now_utc.astimezone()\nprint(now_local)\n>>>\n2019-03-16 15:14:35-07:00\nThe datetime module can also easily convert a local time back to a \nUNIX timestamp in UTC:\ntime_str = '2019-03-16 15:14:35'\nnow = datetime.strptime(time_str, time_format)\ntime_tuple = now.timetuple()\nutc_now = time.mktime(time_tuple)\nprint(utc_now)\n>>>\n1552774475.0\nUnlike the time module, the datetime module has facilities for reli-\nably converting from one local time to another local time. However, \ndatetime only provides the machinery for time zone operations with \nits tzinfo class and related methods. The Python default installation \nis missing time zone definitions besides UTC.\nLuckily, the Python community has addressed this gap with the pytz \nmodule that’s available for download from the Python Package Index \n(see Item 82: “Know Where to Find Community-Built Modules” for \nhow to install it). pytz contains a full database of every time zone \ndefinition you might need.\nTo use pytz effectively, you should always convert local times to UTC \nfirst. Perform any datetime operations you need on the UTC values \n(such as offsetting). Then, convert to local times as a final step.\nFor example, here I convert a New York City flight arrival time to a \nUTC datetime. Although some of these calls seem redundant, all of \nthem are necessary when using pytz:\nimport pytz\n \narrival_nyc = '2019-03-16 23:33:24'\nnyc_dt_naive = datetime.strptime(arrival_nyc, time_format)\n \nItem 67: Use datetime Instead of time for Local Clocks \n311\n",
      "content_length": 1716,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 335,
      "content": "312 \nChapter 8 Robustness and Performance\neastern = pytz.timezone('US/Eastern')\nnyc_dt = eastern.localize(nyc_dt_naive)\nutc_dt = pytz.utc.normalize(nyc_dt.astimezone(pytz.utc))\nprint(utc_dt)\n>>>\n2019-03-17 03:33:24+00:00\nOnce I have a UTC datetime, I can convert it to San Francisco local \ntime:\npacific = pytz.timezone('US/Pacific')\nsf_dt = pacific.normalize(utc_dt.astimezone(pacific))\nprint(sf_dt)\n>>>\n2019-03-16 20:33:24-07:00\nJust as easily, I can convert it to the local time in Nepal:\nnepal = pytz.timezone('Asia/Katmandu')\nnepal_dt = nepal.normalize(utc_dt.astimezone(nepal))\nprint(nepal_dt)\n>>>\n2019-03-17 09:18:24+05:45\nWith datetime and pytz, these conversions are consistent across all \nenvironments, regardless of what operating system the host computer \nis running.\nThings to Remember\n✦ Avoid using the time module for translating between different time \nzones.\n✦ Use the datetime built-in module along with the pytz community \nmodule to reliably convert between times in different time zones.\n✦ Always represent time in UTC and do conversions to local time as \nthe very final step before presentation.\nItem 68: Make pickle Reliable with copyreg\nThe pickle built-in module can serialize Python objects into a stream \nof bytes and deserialize bytes back into objects. Pickled byte streams \nshouldn’t be used to communicate between untrusted parties. The \npurpose of pickle is to let you pass Python objects between programs \nthat you control over binary channels.\n",
      "content_length": 1477,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 336,
      "content": " \nItem 68: Make pickle Reliable with copyreg \n313\nNote\nThe pickle module’s serialization format is unsafe by design. The serialized data \ncontains what is essentially a program that describes how to reconstruct the \noriginal Python object. This means a malicious pickle payload could be used to \ncompromise any part of a Python program that attempts to  deserialize it.\nIn contrast, the json module is safe by design. Serialized JSON data contains \na simple description of an object hierarchy. Deserializing JSON data does \nnot expose a Python program to additional risk. Formats like JSON should \nbe used for communication between programs or people who don’t trust \neach other.\nFor example, say that I want to use a Python object to represent the \nstate of a player’s progress in a game. The game state includes the \nlevel the player is on and the number of lives they have remaining:\nclass GameState:\n    def __init__(self):\n        self.level = 0\n        self.lives = 4\nThe program modifies this object as the game runs:\nstate = GameState()\nstate.level += 1  # Player beat a level\nstate.lives -= 1  # Player had to try again\n \nprint(state.__dict__)\n>>>\n{'level': 1, 'lives': 3}\nWhen the user quits playing, the program can save the state of the \ngame to a file so it can be resumed at a later time. The pickle mod-\nule makes it easy to do this. Here, I use the dump function to write \nthe GameState object to a file:\nimport pickle\n \nstate_path = 'game_state.bin'\nwith open(state_path, 'wb') as f:\n    pickle.dump(state, f)\nLater, I can call the load function with the file and get back the \nGameState object as if it had never been serialized:\nwith open(state_path, 'rb') as f:\n    state_after = pickle.load(f)\n \nprint(state_after.__dict__)\n",
      "content_length": 1745,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 337,
      "content": "314 \nChapter 8 Robustness and Performance\n>>>\n{'level': 1, 'lives': 3}\nThe problem with this approach is what happens as the game’s fea-\ntures expand over time. Imagine that I want the player to earn points \ntoward a high score. To track the player’s points, I’d add a new field to \nthe GameState class\nclass GameState:\n    def __init__(self):\n        self.level = 0\n        self.lives = 4\n        self.points = 0  # New field\nSerializing the new version of the GameState class using pickle will \nwork exactly as before. Here, I simulate the round-trip through a file \nby serializing to a string with dumps and back to an object with loads:\nstate = GameState()\nserialized = pickle.dumps(state)\nstate_after = pickle.loads(serialized)\nprint(state_after.__dict__)\n>>>\n{'level': 0, 'lives': 4, 'points': 0}\nBut what happens to older saved GameState objects that the user may \nwant to resume? Here, I unpickle an old game file by using a program \nwith the new definition of the GameState class:\nwith open(state_path, 'rb') as f:\n    state_after = pickle.load(f)\n \nprint(state_after.__dict__)\n>>>\n{'level': 1, 'lives': 3}\nThe points attribute is missing! This is especially confusing because \nthe returned object is an instance of the new GameState class:\nassert isinstance(state_after, GameState)\nThis behavior is a byproduct of the way the pickle module works. Its \nprimary use case is making object serialization easy. As soon as your \nuse of pickle moves beyond trivial usage, the module’s functionality \nstarts to break down in surprising ways.\nFixing these problems is straightforward using the copyreg built-in \nmodule. The copyreg module lets you register the functions responsible \n",
      "content_length": 1685,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 338,
      "content": "for serializing and deserializing Python objects, allowing you to con-\ntrol the behavior of pickle and make it more reliable.\nDefault Attribute Values\nIn the simplest case, you can use a constructor with default  arguments \n(see Item 23: “Provide Optional Behavior with Keyword Arguments” \nfor background) to ensure that GameState objects will always have all \nattributes after unpickling. Here, I redefine the constructor this way:\nclass GameState:\n    def __init__(self, level=0, lives=4, points=0):\n        self.level = level\n        self.lives = lives\n        self.points = points\nTo use this constructor for pickling, I define a helper function that \ntakes a GameState object and turns it into a tuple of parameters for \nthe copyreg module. The returned tuple contains the function to use \nfor unpickling and the parameters to pass to the unpickling function:\ndef pickle_game_state(game_state):\n    kwargs = game_state.__dict__\n    return unpickle_game_state, (kwargs,)\nNow, I need to define the unpickle_game_state helper. This func-\ntion takes serialized data and parameters from pickle_game_state \nand returns the corresponding GameState object. It’s a tiny wrapper \naround the constructor:\ndef unpickle_game_state(kwargs):\n    return GameState(**kwargs)\nNow, I register these functions with the copyreg built-in module:\nimport copyreg\n \ncopyreg.pickle(GameState, pickle_game_state)\nAfter registration, serializing and deserializing works as before:\nstate = GameState()\nstate.points += 1000\nserialized = pickle.dumps(state)\nstate_after = pickle.loads(serialized)\nprint(state_after.__dict__)\n>>>\n{'level': 0, 'lives': 4, 'points': 1000}\n \nItem 68: Make pickle Reliable with copyreg \n315\n",
      "content_length": 1694,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 339,
      "content": "316 \nChapter 8 Robustness and Performance\nWith this registration done, now I’ll change the definition of GameState \nagain to give the player a count of magic spells to use. This change is \nsimilar to when I added the points field to GameState:\nclass GameState:\n    def __init__(self, level=0, lives=4, points=0, magic=5):\n        self.level = level\n        self.lives = lives\n        self.points = points\n        self.magic = magic  # New field\nBut unlike before, deserializing an old GameState object will result in \nvalid game data instead of missing attributes. This works because \nunpickle_game_state calls the GameState constructor directly instead \nof using the pickle module’s default behavior of saving and restor-\ning only the attributes that belong to an object. The GameState con-\nstructor’s keyword arguments have default values that will be used \nfor any parameters that are missing. This causes old game state \nfiles to receive the default value for the new magic field when they are \ndeserialized:\nprint('Before:', state.__dict__)\nstate_after = pickle.loads(serialized)\nprint('After: ', state_after.__dict__)\n>>>\nBefore: {'level': 0, 'lives': 4, 'points': 1000}\nAfter:  {'level': 0, 'lives': 4, 'points': 1000, 'magic': 5}\nVersioning Classes\nSometimes you need to make backward-incompatible changes to your \nPython objects by removing fields. Doing so prevents the default argu-\nment approach above from working.\nFor example, say I realize that a limited number of lives is a bad idea, \nand I want to remove the concept of lives from the game. Here, I rede-\nfine the GameState class to no longer have a lives field:\nclass GameState:\n    def __init__(self, level=0, points=0, magic=5):\n        self.level = level\n        self.points = points\n        self.magic = magic\nThe problem is that this breaks deserialization of old game data. \nAll fields from the old data, even ones removed from the class, will \nbe passed to the GameState constructor by the unpickle_game_state \nfunction:\npickle.loads(serialized)\n",
      "content_length": 2022,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 340,
      "content": ">>>\nTraceback ...\nTypeError: __init__() got an unexpected keyword argument \n¯'lives'\nI can fix this by adding a version parameter to the functions supplied \nto copyreg. New serialized data will have a version of 2 specified when \npickling a new GameState object:\ndef pickle_game_state(game_state):\n    kwargs = game_state.__dict__\n    kwargs['version'] = 2\n    return unpickle_game_state, (kwargs,)\nOld versions of the data will not have a version argument present, \nwhich means I can manipulate the arguments passed to the GameState \nconstructor accordingly:\ndef unpickle_game_state(kwargs):\n    version = kwargs.pop('version', 1)\n    if version == 1:\n        del kwargs['lives']\n    return GameState(**kwargs)\nNow, deserializing an old object works properly:\ncopyreg.pickle(GameState, pickle_game_state)\nprint('Before:', state.__dict__)\nstate_after = pickle.loads(serialized)\nprint('After: ', state_after.__dict__)\n>>>\nBefore: {'level': 0, 'lives': 4, 'points': 1000}\nAfter:  {'level': 0, 'points': 1000, 'magic': 5}\nI can continue using this approach to handle changes between \nfuture versions of the same class. Any logic I need to adapt an \nold version of the class to a new version of the class can go in the \nunpickle_game_state function.\nStable Import Paths\nOne other issue you may encounter with pickle is breakage from \nrenaming a class. Often over the life cycle of a program, you’ll refac-\ntor your code by renaming classes and moving them to other mod-\nules. Unfortunately, doing so breaks the pickle module unless you’re \ncareful.\n \nItem 68: Make pickle Reliable with copyreg \n317\n",
      "content_length": 1595,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 341,
      "content": "318 \nChapter 8 Robustness and Performance\nHere, I rename the GameState class to BetterGameState and remove \nthe old class from the program entirely:\nclass BetterGameState:\n    def __init__(self, level=0, points=0, magic=5):\n        self.level = level\n        self.points = points\n        self.magic = magic\nAttempting to deserialize an old GameState object now fails because \nthe class can’t be found:\npickle.loads(serialized)\n>>>\nTraceback ...\nAttributeError: Can't get attribute 'GameState' on <module \n¯'__main__' from 'my_code.py'>\nThe cause of this exception is that the import path of the serialized \nobject’s class is encoded in the pickled data:\nprint(serialized)\n>>>\nb'\\x80\\x04\\x95A\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08__main__\n¯\\x94\\x8c\\tGameState\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x05level\n¯\\x94K\\x00\\x8c\\x06points\\x94K\\x00\\x8c\\x05magic\\x94K\\x05ub.'\nThe solution is to use copyreg again. I can specify a stable identifier \nfor the function to use for unpickling an object. This allows me to \ntransition pickled data to different classes with different names when \nit’s deserialized. It gives me a level of indirection:\ncopyreg.pickle(BetterGameState, pickle_game_state)\nAfter I use copyreg, you can see that the import path to \nunpickle_game_state is encoded in the serialized data instead of \nBetterGameState:\nstate = BetterGameState()\nserialized = pickle.dumps(state)\nprint(serialized)\n>>>\nb'\\x80\\x04\\x95W\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08__main__\n¯\\x94\\x8c\\x13unpickle_game_state\\x94\\x93\\x94}\\x94(\\x8c\n¯\\x05level\\x94K\\x00\\x8c\\x06points\\x94K\\x00\\x8c\\x05magic\\x94K\n¯\\x05\\x8c\\x07version\\x94K\\x02u\\x85\\x94R\\x94.'\n",
      "content_length": 1621,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 342,
      "content": " \nItem 69: Use decimal When Precision Is Paramount \n319\nThe only gotcha is that I can’t change the path of the module in \nwhich the unpickle_game_state function is present. Once I serialize \ndata with a function, it must remain available on that import path for \ndeserialization in the future.\nThings to Remember\n✦ The pickle built-in module is useful only for serializing and deseri-\nalizing objects between trusted programs.\n✦ Deserializing previously pickled objects may break if the classes \ninvolved have changed over time (e.g., attributes have been added \nor removed).\n✦ Use the copyreg built-in module with pickle to ensure backward \ncompatibility for serialized objects.\nItem 69: Use decimal When Precision Is Paramount\nPython is an excellent language for writing code that interacts with \nnumerical data. Python’s integer type can represent values of any \npractical size. Its double-precision floating point type complies with \nthe IEEE 754 standard. The language also provides a standard com-\nplex number type for imaginary values. However, these aren’t enough \nfor every situation.\nFor example, say that I want to compute the amount to charge a cus-\ntomer for an international phone call. I know the time in minutes \nand seconds that the customer was on the phone (say, 3 minutes \n42  seconds). I also have a set rate for the cost of calling Antarctica \nfrom the United States ($1.45/minute). What should the charge be?\nWith floating point math, the computed charge seems reasonable\nrate = 1.45\nseconds = 3*60 + 42\ncost = rate * seconds / 60\nprint(cost)\n>>>\n5.364999999999999\nThe result is 0.0001 short of the correct value (5.365) due to how IEEE \n754 floating point numbers are represented. I might want to round up \nthis value to 5.37 to properly cover all costs incurred by the customer. \nHowever, due to floating point error, rounding to the nearest whole \ncent actually reduces the final charge (from 5.364 to 5.36) instead of \nincreasing it (from 5.365 to 5.37):\nprint(round(cost, 2))\n",
      "content_length": 2004,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 343,
      "content": "320 \nChapter 8 Robustness and Performance\n>>>\n5.36\nThe solution is to use the Decimal class from the decimal built-in mod-\nule. The Decimal class provides fixed point math of 28 decimal places \nby default. It can go even higher, if required. This works around the \nprecision issues in IEEE 754 floating point numbers. The class also \ngives you more control over rounding behaviors.\nFor example, redoing the Antarctica calculation with Decimal results \nin the exact expected charge instead of an approximation:\nfrom decimal import Decimal\n \nrate = Decimal('1.45')\nseconds = Decimal(3*60 + 42)\ncost = rate * seconds / Decimal(60)\nprint(cost)\n>>>\n5.365\nDecimal instances can be given starting values in two different ways. \nThe first way is by passing a str containing the number to the Decimal \nconstructor. This ensures that there is no loss of precision due to the \ninherent nature of Python floating point numbers. The second way \nis by directly passing a float or an int instance to the constructor. \nHere, you can see that the two construction methods result in differ-\nent behavior.\nprint(Decimal('1.45'))\nprint(Decimal(1.45))\n>>>\n1.45\n1.4499999999999999555910790149937383830547332763671875\nThe same problem doesn’t happen if I supply integers to the Decimal \nconstructor:\nprint('456')\nprint(456)\n>>>\n456\n456\nIf you care about exact answers, err on the side of caution and use \nthe str constructor for the Decimal type.\n",
      "content_length": 1424,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 344,
      "content": "Getting back to the phone call example, say that I also want to sup-\nport very short phone calls between places that are much cheaper \nto connect (like Toledo and Detroit). Here, I compute the charge for a \nphone call that was 5 seconds long with a rate of $0.05/minute:\nrate = Decimal('0.05')\nseconds = Decimal('5')\nsmall_cost = rate * seconds / Decimal(60)\nprint(small_cost)\n>>>\n0.004166666666666666666666666667\nThe result is so low that it is decreased to zero when I try to round it \nto the nearest whole cent. This won’t do!\nprint(round(small_cost, 2))\n>>>\n0.00\nLuckily, the Decimal class has a built-in function for rounding to \nexactly the decimal place needed with the desired rounding behavior. \nThis works for the higher cost case from earlier:\nfrom decimal import ROUND_UP\n \nrounded = cost.quantize(Decimal('0.01'), rounding=ROUND_UP)\nprint(f'Rounded {cost} to {rounded}')\n>>>\nRounded 5.365 to 5.37\nUsing the quantize method this way also properly handles the small \nusage case for short, cheap phone calls:.\nrounded = small_cost.quantize(Decimal('0.01'),\n                              rounding=ROUND_UP)\nprint(f'Rounded {small_cost} to {rounded}')\n>>>\nRounded 0.004166666666666666666666666667 to 0.01\nWhile Decimal works great for fixed point numbers, it still has limita-\ntions in its precision (e.g., 1/3 will be an approximation). For repre-\nsenting rational numbers with no limit to precision, consider using \nthe Fraction class from the fractions built-in module.\n \nItem 69: Use decimal When Precision Is Paramount \n321\n",
      "content_length": 1537,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 345,
      "content": "322 \nChapter 8 Robustness and Performance\nThings to Remember\n✦ Python has built-in types and classes in modules that can repre-\nsent practically every type of numerical value.\n✦ The Decimal class is ideal for situations that require high precision \nand control over rounding behavior, such as computations of mon-\netary values.\n✦ Pass str instances to the Decimal constructor instead of float \ninstances if it’s important to compute exact answers and not float-\ning point approximations.\nItem 70: Profile Before Optimizing\nThe dynamic nature of Python causes surprising behaviors in its run-\ntime performance. Operations you might assume would be slow are \nactually very fast (e.g., string manipulation, generators). Language \nfeatures you might assume would be fast are actually very slow (e.g., \nattribute accesses, function calls). The true source of slowdowns in a \nPython program can be obscure.\nThe best approach is to ignore your intuition and directly measure \nthe performance of a program before you try to optimize it. Python \nprovides a built-in profiler for determining which parts of a program \nare responsible for its execution time. This means you can focus your \noptimization efforts on the biggest sources of trouble and ignore parts \nof the program that don’t impact speed (i.e., follow Amdahl’s law).\nFor example, say that I want to determine why an algorithm in a pro-\ngram is slow. Here, I define a function that sorts a list of data using \nan insertion sort:\ndef insertion_sort(data):\n    result = []\n    for value in data:\n        insert_value(result, value)\n    return result\nThe core mechanism of the insertion sort is the function that finds \nthe insertion point for each piece of data. Here, I define an extremely \ninefficient version of the insert_value function that does a linear scan \nover the input array:\ndef insert_value(array, value):\n    for i, existing in enumerate(array):\n        if existing > value:\n            array.insert(i, value)\n",
      "content_length": 1975,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 346,
      "content": " \nItem 70: Profile Before Optimizing \n323\n            return\n    array.append(value)\nTo profile insertion_sort and insert_value, I create a data set of ran-\ndom numbers and define a test function to pass to the profiler:\nfrom random import randint\n \nmax_size = 10**4\ndata = [randint(0, max_size) for _ in range(max_size)]\ntest = lambda: insertion_sort(data)\nPython provides two built-in profilers: one that is pure Python \n(profile) and another that is a C-extension module (cProfile). The \ncProfile built-in module is better because of its minimal impact on \nthe performance of your program while it’s being profiled. The pure- \nPython alternative imposes a high overhead that skews the results.\nNote\nWhen profiling a Python program, be sure that what you’re measuring is the \ncode itself and not external systems. Beware of functions that access the net-\nwork or resources on disk. These may appear to have a large impact on your \nprogram’s execution time because of the slowness of the underlying systems. \nIf your program uses a cache to mask the latency of slow resources like these, \nyou should ensure that it’s properly warmed up before you start profiling.\nHere, I instantiate a Profile object from the cProfile module and run \nthe test function through it using the runcall method:\nfrom cProfile import Profile\n \nprofiler = Profile()\nprofiler.runcall(test)\nWhen the test function has finished running, I can extract statistics \nabout its performance by using the pstats built-in module and its \nStats class. Various methods on a Stats object adjust how to select \nand sort the profiling information to show only the things I care \nabout:\nfrom pstats import Stats\n \nstats = Stats(profiler)\nstats.strip_dirs()\nstats.sort_stats('cumulative')\nstats.print_stats()\n",
      "content_length": 1768,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 347,
      "content": "324 \nChapter 8 Robustness and Performance\nThe output is a table of information organized by function. The data \nsample is taken only from the time the profiler was active, during the \nruncall method above:\n>>>\n         20003 function calls in 1.320 seconds\n \n   Ordered by: cumulative time\n \n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.000    0.000    1.320    1.320 main.py:35(<lambda>)\n        1    0.003    0.003    1.320    1.320 main.py:10(insertion_sort)\n    10000    1.306    0.000    1.317    0.000 main.py:20(insert_value)\n     9992    0.011    0.000    0.011    0.000 {method 'insert' of 'list' objects}\n        8    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\nHere’s a quick guide to what the profiler statistics columns mean:\n \n■ncalls: The number of calls to the function during the profiling \nperiod.\n \n■tottime: The number of seconds spent executing the function, \nexcluding time spent executing other functions it calls.\n \n■tottime percall: The average number of seconds spent in the \nfunction each time it is called, excluding time spent executing \nother functions it calls. This is tottime divided by ncalls.\n \n■cumtime: The cumulative number of seconds spent executing the \nfunction, including time spent in all other functions it calls.\n \n■cumtime percall: The average number of seconds spent in the \nfunction each time it is called, including time spent in all other \nfunctions it calls. This is cumtime divided by ncalls.\nLooking at the profiler statistics table above, I can see that the biggest \nuse of CPU in my test is the cumulative time spent in the insert_value \nfunction. Here, I redefine that function to use the bisect built-in mod-\nule (see Item 72: “Consider Searching Sorted Sequences with bisect”):\nfrom bisect import bisect_left\n \ndef insert_value(array, value):\n    i = bisect_left(array, value)\n    array.insert(i, value)\nI can run the profiler again and generate a new table of profiler sta-\ntistics. The new function is much faster, with a cumulative time spent \nthat is nearly 100 times smaller than with the previous insert_value \nfunction:\n",
      "content_length": 2152,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 348,
      "content": " \nItem 70: Profile Before Optimizing \n325\n>>>\n         30003 function calls in 0.017 seconds\n \n   Ordered by: cumulative time\n \n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.000    0.000    0.017    0.017 main.py:35(<lambda>)\n        1    0.002    0.002    0.017    0.017 main.py:10(insertion_sort)\n    10000    0.003    0.000    0.015    0.000 main.py:110(insert_value)\n    10000    0.008    0.000    0.008    0.000 {method 'insert' of 'list' objects}\n    10000    0.004    0.000    0.004    0.000 {built-in method _bisect.bisect_left}\nSometimes when you’re profiling an entire program, you might find \nthat a common utility function is responsible for the majority of exe-\ncution time. The default output from the profiler makes such a situ-\nation difficult to understand because it doesn’t show that the utility \nfunction is called by many different parts of your program.\nFor example, here the my_utility function is called repeatedly by two \ndifferent functions in the program:\ndef my_utility(a, b):\n    c = 1\n    for i in range(100):\n        c += a * b\n \ndef first_func():\n    for _ in range(1000):\n        my_utility(4, 5)\n \ndef second_func():\n    for _ in range(10):\n        my_utility(1, 3)\n \ndef my_program():\n    for _ in range(20):\n        first_func()\n        second_func()\nProfiling this code and using the default print_stats output gener-\nates statistics that are confusing:\n>>>\n         20242 function calls in 0.118 seconds\n \n   Ordered by: cumulative time\n \n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.000    0.000    0.118    0.118 main.py:176(my_program)\n       20    0.003    0.000    0.117    0.006 main.py:168(first_func)\n    20200    0.115    0.000    0.115    0.000 main.py:161(my_utility)\n       20    0.000    0.000    0.001    0.000 main.py:172(second_func)\n",
      "content_length": 1871,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 349,
      "content": "326 \nChapter 8 Robustness and Performance\nThe my_utility function is clearly the source of most execution time, \nbut it’s not immediately obvious why that function is called so much. \nIf you search through the program’s code, you’ll find multiple call \nsites for my_utility and still be confused.\nTo deal with this, the Python profiler provides the print_callers \nmethod to show which callers contributed to the profiling information \nof each function:\nstats.print_callers()\nThis profiler statistics table shows functions called on the left and \nwhich function was responsible for making the call on the right. Here, \nit’s clear that my_utility is most used by first_func:\n>>>\n   Ordered by: cumulative time\n \nFunction                                was called by...\n                                            ncalls  tottime  cumtime\nmain.py:176(my_program)                 <- \nmain.py:168(first_func)                 <-      20    0.003    0.117  main.py:176(my_program)\nmain.py:161(my_utility)                 <-   20000    0.114    0.114  main.py:168(first_func)\n                                               200    0.001    0.001  main.py:172(second_func)\nProfiling.md:172(second_func)           <-      20    0.000    0.001  main.py:176(my_program)\nThings to Remember\n✦ It’s important to profile Python programs before optimizing because \nthe sources of slowdowns are often obscure.\n✦ Use the cProfile module instead of the profile module because it \nprovides more accurate profiling information.\n✦ The Profile object’s runcall method provides everything you need \nto profile a tree of function calls in isolation.\n✦ The Stats object lets you select and print the subset of profil-\ning information you need to see to understand your program’s \nperformance.\nItem 71: Prefer deque for Producer–Consumer Queues\nA common need in writing programs is a first-in, first-out (FIFO) \nqueue, which is also known as a producer–consumer queue. A FIFO \nqueue is used when one function gathers values to process and \nanother function handles them in the order in which they were \nreceived. Often, programmers use Python’s built-in list type as a \nFIFO queue.\n",
      "content_length": 2153,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 350,
      "content": " \nItem 71: Prefer deque for Producer–Consumer Queues \n327\nFor example, say that I have a program that’s processing incoming \nemails for long-term archival, and it’s using a list for a producer–\nconsumer queue. Here, I define a class to represent the messages:\nclass Email:\n    def __init__(self, sender, receiver, message):\n        self.sender = sender\n        self.receiver = receiver\n        self.message = message\n    ...\nI also define a placeholder function for receiving a single email, pre-\nsumably from a socket, the file system, or some other type of I/O \nsystem. The implementation of this function doesn’t matter; what’s \nimportant is its interface: It will either return an Email instance or \nraise a NoEmailError exception:\nclass NoEmailError(Exception):\n    pass\n \ndef try_receive_email():\n    # Returns an Email instance or raises NoEmailError\n    ...\nThe producing function receives emails and enqueues them to be con-\nsumed at a later time. This function uses the append method on the \nlist to add new messages to the end of the queue so they are pro-\ncessed after all messages that were previously received:\ndef produce_emails(queue):\n    while True:\n        try:\n            email = try_receive_email()\n        except NoEmailError:\n            return\n        else:\n            queue.append(email)  # Producer\nThe consuming function does something useful with the emails. This \nfunction calls pop(0) on the queue, which removes the very first item \nfrom the list and returns it to the caller. By always processing items \nfrom the beginning of the queue, the consumer ensures that the items \nare processed in the order in which they were received:\ndef consume_one_email(queue):\n    if not queue:\n        return\n    email = queue.pop(0)  # Consumer\n",
      "content_length": 1764,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 351,
      "content": "328 \nChapter 8 Robustness and Performance\n    # Index the message for long-term archival\n    ...\nFinally, I need a looping function that connects the pieces together. \nThis function alternates between producing and consuming until the \nkeep_running function returns False (see Item 60: “Achieve Highly \nConcurrent I/O with Coroutines” on how to do this concurrently):\ndef loop(queue, keep_running):\n    while keep_running():\n        produce_emails(queue)\n        consume_one_email(queue)\n \ndef my_end_func():\n    ...\n \nloop([], my_end_func)\nWhy not process each Email message in produce_emails as it’s returned \nby try_receive_email? It comes down to the trade-off between latency \nand throughput. When using producer–consumer queues, you often \nwant to minimize the latency of accepting new items so they can be \ncollected as fast as possible. The consumer can then process through \nthe backlog of items at a consistent pace—one item per loop in this \ncase—which provides a stable performance profile and consistent \nthroughput at the cost of end-to-end latency (see Item 55: “Use Queue \nto Coordinate Work Between Threads” for related best practices).\nUsing a list for a producer–consumer queue like this works fine up \nto a point, but as the cardinality—the number of items in the list—\nincreases, the list type’s performance can degrade superlinearly. \nTo analyze the performance of using list as a FIFO queue, I can \nrun some micro-benchmarks using the timeit built-in module. Here, \nI define a benchmark for the performance of adding new items to the \nqueue using the append method of list (matching the producer func-\ntion’s usage):\nimport timeit\n \ndef print_results(count, tests):\n    avg_iteration = sum(tests) / len(tests)\n    print(f'Count {count:>5,} takes {avg_iteration:.6f}s')\n    return count, avg_iteration\n \ndef list_append_benchmark(count):\n    def run(queue):\n",
      "content_length": 1880,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 352,
      "content": "        for i in range(count):\n            queue.append(i)\n \n    tests = timeit.repeat(\n        setup='queue = []',\n        stmt='run(queue)',\n        globals=locals(),\n        repeat=1000,\n        number=1)\n \n    return print_results(count, tests)\nRunning this benchmark function with different levels of cardinality \nlets me compare its performance in relationship to data size:\ndef print_delta(before, after):\n    before_count, before_time = before\n    after_count, after_time = after\n    growth = 1 + (after_count - before_count) / before_count\n    slowdown = 1 + (after_time - before_time) / before_time\n    print(f'{growth:>4.1f}x data size, {slowdown:>4.1f}x time')\n \nbaseline = list_append_benchmark(500)\nfor count in (1_000, 2_000, 3_000, 4_000, 5_000):\n    comparison = list_append_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.000039s\n \nCount 1,000 takes 0.000073s\n 2.0x data size,  1.9x time\n \nCount 2,000 takes 0.000121s\n 4.0x data size,  3.1x time\n \nCount 3,000 takes 0.000172s\n 6.0x data size,  4.5x time\n \nCount 4,000 takes 0.000240s\n 8.0x data size,  6.2x time\n \nCount 5,000 takes 0.000304s\n10.0x data size,  7.9x time\n \nItem 71: Prefer deque for Producer–Consumer Queues \n329\n",
      "content_length": 1227,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 353,
      "content": "330 \nChapter 8 Robustness and Performance\nThis shows that the append method takes roughly constant time for \nthe list type, and the total time for enqueueing scales linearly as the \ndata size increases. There is overhead for the list type to increase its \ncapacity under the covers as new items are added, but it’s reasonably \nlow and is amortized across repeated calls to append.\nHere, I define a similar benchmark for the pop(0) call that removes \nitems from the beginning of the queue (matching the consumer func-\ntion’s usage):\ndef list_pop_benchmark(count):\n    def prepare():\n        return list(range(count))\n \n    def run(queue):\n        while queue:\n            queue.pop(0)\n \n    tests = timeit.repeat(\n        setup='queue = prepare()',\n        stmt='run(queue)',\n        globals=locals(),\n        repeat=1000,\n        number=1)\n \n    return print_results(count, tests)\nI can similarly run this benchmark for queues of different sizes to see \nhow performance is affected by cardinality:\nbaseline = list_pop_benchmark(500)\nfor count in (1_000, 2_000, 3_000, 4_000, 5_000):\n    comparison = list_pop_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.000050s\n \nCount 1,000 takes 0.000133s\n 2.0x data size,  2.7x time\n \nCount 2,000 takes 0.000347s\n 4.0x data size,  6.9x time\n \nCount 3,000 takes 0.000663s\n 6.0x data size, 13.2x time\n \n",
      "content_length": 1372,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 354,
      "content": "Count 4,000 takes 0.000943s\n 8.0x data size, 18.8x time\n \nCount 5,000 takes 0.001481s\n10.0x data size, 29.5x time\nSurprisingly, this shows that the total time for dequeuing items from \na list with pop(0) scales quadratically as the length of the queue \nincreases. The cause is that pop(0) needs to move every item in the \nlist back an index, effectively reassigning the entire list’s contents. \nI need to call pop(0) for every item in the list, and thus I end up \ndoing roughly len(queue) * len(queue) operations to consume the \nqueue. This doesn’t scale.\nPython provides the deque class from the collections built-in module \nto solve this problem. deque is a double-ended queue implementation. \nIt provides constant time operations for inserting or removing items \nfrom its beginning or end. This makes it ideal for FIFO queues.\nTo use the deque class, the call to append in produce_emails can \nstay the same as it was when using a list for the queue. The \nlist.pop method call in consume_one_email must change to call the \ndeque.popleft method with no arguments instead. And the loop \nmethod must be called with a deque instance instead of a list. Every-\nthing else stays the same. Here, I redefine the one function affected to \nuse the new method and run loop again:\nimport collections\n \ndef consume_one_email(queue):\n    if not queue:\n        return\n    email = queue.popleft()  # Consumer\n    # Process the email message\n    ...\n \ndef my_end_func():\n    ...\n \nloop(collections.deque(), my_end_func)\nI can run another version of the benchmark to verify that append \nperformance (matching the producer function’s usage) has stayed \nroughly the same (modulo a constant factor):\ndef deque_append_benchmark(count):\n    def prepare():\n        return collections.deque()\n \n \nItem 71: Prefer deque for Producer–Consumer Queues \n331\n",
      "content_length": 1829,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 355,
      "content": "332 \nChapter 8 Robustness and Performance\n    def run(queue):\n        for i in range(count):\n            queue.append(i)\n \n    tests = timeit.repeat(\n        setup='queue = prepare()',\n        stmt='run(queue)',\n        globals=locals(),\n        repeat=1000,\n        number=1)\n    return print_results(count, tests)\n \nbaseline = deque_append_benchmark(500)\nfor count in (1_000, 2_000, 3_000, 4_000, 5_000):\n    comparison = deque_append_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.000029s\n \nCount 1,000 takes 0.000059s\n 2.0x data size,  2.1x time\n \nCount 2,000 takes 0.000121s\n 4.0x data size,  4.2x time\n \nCount 3,000 takes 0.000171s\n 6.0x data size,  6.0x time\n \nCount 4,000 takes 0.000243s\n 8.0x data size,  8.5x time\n \nCount 5,000 takes 0.000295s\n10.0x data size, 10.3x time\nAnd I can benchmark the performance of calling popleft to mimic \nthe consumer function’s usage of deque:\ndef dequeue_popleft_benchmark(count):\n    def prepare():\n        return collections.deque(range(count))\n \n    def run(queue):\n        while queue:\n            queue.popleft()\n \n    tests = timeit.repeat(\n",
      "content_length": 1123,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 356,
      "content": "        setup='queue = prepare()',\n        stmt='run(queue)',\n        globals=locals(),\n        repeat=1000,\n        number=1)\n \n    return print_results(count, tests)\n \nbaseline = dequeue_popleft_benchmark(500)\nfor count in (1_000, 2_000, 3_000, 4_000, 5_000):\n    comparison = dequeue_popleft_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.000024s\n \nCount 1,000 takes 0.000050s\n 2.0x data size,  2.1x time\n \nCount 2,000 takes 0.000100s\n 4.0x data size,  4.2x time\n \nCount 3,000 takes 0.000152s\n 6.0x data size,  6.3x time\n \nCount 4,000 takes 0.000207s\n 8.0x data size,  8.6x time\n \nCount 5,000 takes 0.000265s\n10.0x data size, 11.0x time\nThe popleft usage scales linearly instead of displaying the super-\nlinear behavior of pop(0) that I measured before—hooray! If you \nknow that the performance of a program critically depends on the \nspeed of producer–consumer queues, then deque is a great choice. \nIf you’re not sure, then you should instrument your program to \nfind out (see Item 70: “Profile Before Optimizing”).\nThings to Remember\n✦ The list type can be used as a FIFO queue by having the producer \ncall append to add items and the consumer call pop(0) to receive \nitems. However, this may cause problems because the performance \nof pop(0) degrades superlinearly as the queue length increases.\n \nItem 71: Prefer deque for Producer–Consumer Queues \n333\n",
      "content_length": 1393,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 357,
      "content": "334 \nChapter 8 Robustness and Performance\n✦ The deque class from the collections built-in module takes constant \ntime—regardless of length—for append and popleft, making it ideal \nfor FIFO queues.\nItem 72:  Consider Searching Sorted Sequences \nwith bisect\nIt’s common to find yourself with a large amount of data in memory \nas a sorted list that you then want to search. For example, you may \nhave loaded an English language dictionary to use for spell check-\ning, or perhaps a list of dated financial transactions to audit for \ncorrectness.\nRegardless of the data your specific program needs to process, search-\ning for a specific value in a list takes linear time proportional to the \nlist’s length when you call the index method:\ndata = list(range(10**5))\nindex = data.index(91234)\nassert index == 91234\nIf you’re not sure whether the exact value you’re searching for is in the \nlist, then you may want to search for the closest index that is equal \nto or exceeds your goal value. The simplest way to do this is to lin-\nearly scan the list and compare each item to your goal value:\ndef find_closest(sequence, goal):\n    for index, value in enumerate(sequence):\n        if goal < value:\n            return index\n    raise ValueError(f'{goal} is out of bounds')\n \nindex = find_closest(data, 91234.56)\nassert index == 91235\nPython’s built-in bisect module provides better ways to accom-\nplish these types of searches through ordered lists. You can use the \nbisect_left function to do an efficient binary search through any \nsequence of sorted items. The index it returns will either be where the \nitem is already present in the list or where you’d want to insert the \nitem in the list to keep it in sorted order:\nfrom bisect import bisect_left\n \nindex = bisect_left(data, 91234)     # Exact match\nassert index == 91234\n \n",
      "content_length": 1821,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 358,
      "content": " \nItem 72: Consider Searching Sorted Sequences with bisect \n335\nindex = bisect_left(data, 91234.56)  # Closest match\nassert index == 91235\nThe complexity of the binary search algorithm used by the bisect \nmodule is logarithmic. This means searching in a list of length \n1 million takes roughly the same amount of time with bisect as \nlinearly searching a list of length 20 using the list.index method \n(math.log2(10**6) == 19.93...). It’s way faster!\nI can verify this speed improvement for the example from above by \nusing the timeit built-in module to run a micro-benchmark:\nimport random\nimport timeit\n \nsize = 10**5\niterations = 1000\n \ndata = list(range(size))\nto_lookup = [random.randint(0, size)\n             for _ in range(iterations)]\n \ndef run_linear(data, to_lookup):\n    for index in to_lookup:\n        data.index(index)\n \ndef run_bisect(data, to_lookup):\n    for index in to_lookup:\n        bisect_left(data, index)\n \nbaseline = timeit.timeit(\n    stmt='run_linear(data, to_lookup)',\n    globals=globals(),\n    number=10)\nprint(f'Linear search takes {baseline:.6f}s')\n \ncomparison = timeit.timeit(\n    stmt='run_bisect(data, to_lookup)',\n    globals=globals(),\n    number=10)\nprint(f'Bisect search takes {comparison:.6f}s')\n \nslowdown = 1 + ((baseline - comparison) / comparison)\nprint(f'{slowdown:.1f}x time')\n",
      "content_length": 1323,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 359,
      "content": "336 \nChapter 8 Robustness and Performance\n>>>\nLinear search takes 5.370117s\nBisect search takes 0.005220s\n1028.7x time\nThe best part about bisect is that it’s not limited to the list type; \nyou can use it with any Python object that acts like a sequence (see \nItem 43: “Inherit from collections.abc for Custom Container Types” \nfor how to do that). The module also provides additional features for \nmore advanced situations (see help(bisect)).\nThings to Remember\n✦ Searching sorted data contained in a list takes linear time using \nthe index method or a for loop with simple comparisons.\n✦ The bisect built-in module’s bisect_left function takes logarith-\nmic time to search for values in sorted lists, which can be orders of \nmagnitude faster than other approaches.\nItem 73: Know How to Use heapq for Priority Queues\nOne of the limitations of Python’s other queue implementations (see \nItem 71: “Prefer deque for Producer–Consumer Queues” and Item 55: \n“Use Queue to Coordinate Work Between Threads”) is that they are \nfirst-in, first-out (FIFO) queues: Their contents are sorted by the order \nin which they were received. Often, you need a program to process \nitems in order of relative importance instead. To accomplish this, a \npriority queue is the right tool for the job.\nFor example, say that I’m writing a program to manage books bor-\nrowed from a library. There are people constantly borrowing new \nbooks. There are people returning their borrowed books on time. And \nthere are people who need to be reminded to return their overdue \nbooks. Here, I define a class to represent a book that’s been borrowed:\nclass Book:\n    def __init__(self, title, due_date):\n        self.title = title\n        self.due_date = due_date\nI need a system that will send reminder messages when each book \npasses its due date. Unfortunately, I can’t use a FIFO queue for this \nbecause the amount of time each book is allowed to be borrowed var-\nies based on its recency, popularity, and other factors. For example, a \nbook that is borrowed today may be due back later than a book that’s \n",
      "content_length": 2075,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 360,
      "content": " \nItem 73: Know How to Use heapq for Priority Queues \n337\nborrowed tomorrow. Here, I achieve this behavior by using a standard \nlist and sorting it by due_date each time a new Book is added:\ndef add_book(queue, book):\n    queue.append(book)\n    queue.sort(key=lambda x: x.due_date, reverse=True)\n \nqueue = []\nadd_book(queue, Book('Don Quixote', '2019-06-07'))\nadd_book(queue, Book('Frankenstein', '2019-06-05'))\nadd_book(queue, Book('Les Misérables', '2019-06-08'))\nadd_book(queue, Book('War and Peace', '2019-06-03'))\nIf I can assume that the queue of borrowed books is always in sorted \norder, then all I need to do to check for overdue books is to inspect the \nfinal element in the list. Here, I define a function to return the next \noverdue book, if any, and remove it from the queue:\nclass NoOverdueBooks(Exception):\n    pass\n \ndef next_overdue_book(queue, now):\n    if queue:\n        book = queue[-1]\n        if book.due_date < now:\n            queue.pop()\n            return book\n \n    raise NoOverdueBooks\nI can call this function repeatedly to get overdue books to remind peo-\nple about in the order of most overdue to least overdue:\nnow = '2019-06-10'\n \nfound = next_overdue_book(queue, now)\nprint(found.title)\n \nfound = next_overdue_book(queue, now)\nprint(found.title)\n>>>\nWar and Peace\nFrankenstein\n",
      "content_length": 1311,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 361,
      "content": "338 \nChapter 8 Robustness and Performance\nIf a book is returned before the due date, I can remove the scheduled \nreminder message by removing the Book from the list:\ndef return_book(queue, book):\n    queue.remove(book)\n \nqueue = []\nbook = Book('Treasure Island', '2019-06-04')\n \nadd_book(queue, book)\nprint('Before return:', [x.title for x in queue])\n \nreturn_book(queue, book)\nprint('After return: ', [x.title for x in queue])\n>>>\nBefore return: ['Treasure Island']\nAfter return:  []\nAnd I can confirm that when all books are returned, the return_book \nfunction will raise the right exception (see Item 20: “Prefer Raising \nExceptions to Returning None”):\ntry:\n    next_overdue_book(queue, now)\nexcept NoOverdueBooks:\n    pass          # Expected\nelse:\n    assert False  # Doesn't happen\nHowever, the computational complexity of this solution isn’t \nideal. Although checking for and removing an overdue book has \na constant cost, every time I add a book, I pay the cost of sorting \nthe whole list again. If I have len(queue) books to add, and the \ncost of sorting them is roughly len(queue) * math.log(len(queue)), \nthe \ntime \nit \ntakes \nto \nadd \nbooks \nwill \ngrow \nsuperlinearly \n(len(queue) * len(queue) * math.log(len(queue))).\nHere, I define a micro-benchmark to measure this performance \nbehavior experimentally by using the timeit built-in module (see Item \n71: “Prefer deque for Producer–Consumer Queues” for the implemen-\ntation of print_results and print_delta):\nimport random\nimport timeit\n \ndef print_results(count, tests):\n    ...\n \n",
      "content_length": 1546,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 362,
      "content": "def print_delta(before, after):\n    ...\n \ndef list_overdue_benchmark(count):\n    def prepare():\n        to_add = list(range(count))\n        random.shuffle(to_add)\n        return [], to_add\n \n    def run(queue, to_add):\n        for i in to_add:\n            queue.append(i)\n            queue.sort(reverse=True)\n \n        while queue:\n            queue.pop()\n \n    tests = timeit.repeat(\n        setup='queue, to_add = prepare()',\n        stmt=f'run(queue, to_add)',\n        globals=locals(),\n        repeat=100,\n        number=1)\n \n    return print_results(count, tests)\nI can verify that the runtime of adding and removing books from the \nqueue scales superlinearly as the number of books being borrowed \nincreases:\nbaseline = list_overdue_benchmark(500)\nfor count in (1_000, 1_500, 2_000):\n    comparison = list_overdue_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.001138s\n \nCount 1,000 takes 0.003317s\n 2.0x data size,  2.9x time\n \nCount 1,500 takes 0.007744s\n 3.0x data size,  6.8x time\n \nCount 2,000 takes 0.014739s\n 4.0x data size, 13.0x time\n \nItem 73: Know How to Use heapq for Priority Queues \n339\n",
      "content_length": 1139,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 363,
      "content": "340 \nChapter 8 Robustness and Performance\nWhen a book is returned before the due date, I need to do a linear \nscan in order to find the book in the queue and remove it. Removing \na book causes all subsequent items in the list to be shifted back \nan index, which has a high cost that also scales superlinearly. Here, \nI define another micro-benchmark to test the performance of return-\ning a book using this function:\ndef list_return_benchmark(count):\n    def prepare():\n        queue = list(range(count))\n        random.shuffle(queue)\n \n        to_return = list(range(count))\n        random.shuffle(to_return)\n \n        return queue, to_return\n \n    def run(queue, to_return):\n        for i in to_return:\n            queue.remove(i)\n \n    tests = timeit.repeat(\n        setup='queue, to_return = prepare()',\n        stmt=f'run(queue, to_return)',\n        globals=locals(),\n        repeat=100,\n        number=1)\n \n    return print_results(count, tests)\nAnd again, I can verify that indeed the performance degrades super-\nlinearly as the number of books increases:\nbaseline = list_return_benchmark(500)\nfor count in (1_000, 1_500, 2_000):\n    comparison = list_return_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.000898s\n \nCount 1,000 takes 0.003331s\n 2.0x data size,  3.7x time\n \nCount 1,500 takes 0.007674s\n 3.0x data size,  8.5x time\n \n",
      "content_length": 1371,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 364,
      "content": "Count 2,000 takes 0.013721s\n 4.0x data size, 15.3x time\nUsing the methods of list may work for a tiny library, but it certainly \nwon’t scale to the size of the Great Library of Alexandria, as I want it to!\nFortunately, Python has the built-in heapq module that solves this \nproblem by implementing priority queues efficiently. A heap is a data \nstructure that allows for a list of items to be maintained where \nthe computational complexity of adding a new item or removing the \nsmallest item has logarithmic computational complexity (i.e., even \nbetter than linear scaling). In this library example, smallest means \nthe book with the earliest due date. The best part about this module \nis that you don’t have to understand how heaps are implemented in \norder to use its functions correctly.\nHere, I reimplement the add_book function using the heapq module. \nThe queue is still a plain list. The heappush function replaces the \nlist.append call from before. And I no longer have to call list.sort on \nthe queue:\nfrom heapq import heappush\n \ndef add_book(queue, book):\n    heappush(queue, book)\nIf I try to use this with the Book class as previously defined, I get this \nsomewhat cryptic error:\nqueue = []\nadd_book(queue, Book('Little Women', '2019-06-05'))\nadd_book(queue, Book('The Time Machine', '2019-05-30'))\n>>>\nTraceback ...\nTypeError: '<' not supported between instances of 'Book' and \n¯'Book'\nThe heapq module requires items in the priority queue to be compa-\nrable and have a natural sort order (see Item 14: “Sort by Complex \nCriteria Using the key Parameter” for details). You can quickly give \nthe Book class this behavior by using the total_ordering class dec-\norator from the functools built-in module (see Item 51: “Prefer Class \nDecorators Over Metaclasses for Composable Class Extensions” for \nbackground) and implementing the __lt__ special method (see Item \n43: “Inherit from collections.abc for Custom Container Types” for \n \nItem 73: Know How to Use heapq for Priority Queues \n341\n",
      "content_length": 2001,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 365,
      "content": "342 \nChapter 8 Robustness and Performance\nbackground). Here, I redefine the class with a less-than method that \nsimply compares the due_date fields between two Book instances:\nimport functools\n \n@functools.total_ordering\nclass Book:\n    def __init__(self, title, due_date):\n        self.title = title\n        self.due_date = due_date\n \n    def __lt__(self, other):\n        return self.due_date < other.due_date\nNow, I can add books to the priority queue by using the heapq.heappush \nfunction without issues:\nqueue = []\nadd_book(queue, Book('Pride and Prejudice', '2019-06-01'))\nadd_book(queue, Book('The Time Machine', '2019-05-30'))\nadd_book(queue, Book('Crime and Punishment', '2019-06-06'))\nadd_book(queue, Book('Wuthering Heights', '2019-06-12'))\nAlternatively, I can create a list with all of the books in any order and \nthen use the sort method of list to produce the heap:\nqueue = [\n    Book('Pride and Prejudice', '2019-06-01'),\n    Book('The Time Machine', '2019-05-30'),\n    Book('Crime and Punishment', '2019-06-06'),\n    Book('Wuthering Heights', '2019-06-12'),\n]\nqueue.sort()\nOr I can use the heapq.heapify function to create a heap in linear \ntime (as opposed to the sort method’s len(queue) * log(len(queue)) \ncomplexity):\nfrom heapq import heapify\n \nqueue = [\n    Book('Pride and Prejudice', '2019-06-01'),\n    Book('The Time Machine', '2019-05-30'),\n    Book('Crime and Punishment', '2019-06-06'),\n    Book('Wuthering Heights', '2019-06-12'),\n]\nheapify(queue)\n",
      "content_length": 1477,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 366,
      "content": "To check for overdue books, I inspect the first element in the list \ninstead of the last, and then I use the heapq.heappop function instead \nof the list.pop function:\nfrom heapq import heappop\n \ndef next_overdue_book(queue, now):\n    if queue:\n        book = queue[0]           # Most overdue first\n        if book.due_date < now:\n            heappop(queue)        # Remove the overdue book\n            return book\n \n    raise NoOverdueBooks\nNow, I can find and remove overdue books in order until there are \nnone left for the current time:\nnow = '2019-06-02'\n \nbook = next_overdue_book(queue, now)\nprint(book.title)\n \nbook = next_overdue_book(queue, now)\nprint(book.title)\n \ntry:\n    next_overdue_book(queue, now)\nexcept NoOverdueBooks:\n    pass          # Expected\nelse:\n    assert False  # Doesn't happen\n>>>\nThe Time Machine\nPride and Prejudice\nI can write another micro-benchmark to test the performance of this \nimplementation that uses the heapq module:\ndef heap_overdue_benchmark(count):\n    def prepare():\n        to_add = list(range(count))\n        random.shuffle(to_add)\n        return [], to_add\n \n    def run(queue, to_add):\n        for i in to_add:\n \nItem 73: Know How to Use heapq for Priority Queues \n343\n",
      "content_length": 1221,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 367,
      "content": "344 \nChapter 8 Robustness and Performance\n            heappush(queue, i)\n        while queue:\n            heappop(queue)\n \n    tests = timeit.repeat(\n        setup='queue, to_add = prepare()',\n        stmt=f'run(queue, to_add)',\n        globals=locals(),\n        repeat=100,\n        number=1)\n \n    return print_results(count, tests)\nThis benchmark experimentally verifies that the heap-based \npriority \nqueue \nimplementation \nscales \nmuch \nbetter \n(roughly \nlen(queue) * math.log(len(queue))), without superlinearly degrading \nperformance:\nbaseline = heap_overdue_benchmark(500)\nfor count in (1_000, 1_500, 2_000):\n    comparison = heap_overdue_benchmark(count)\n    print_delta(baseline, comparison)\n>>>\nCount   500 takes 0.000150s\n \nCount 1,000 takes 0.000325s\n 2.0x data size,  2.2x time\n \nCount 1,500 takes 0.000528s\n 3.0x data size,  3.5x time\n \nCount 2,000 takes 0.000658s\n 4.0x data size,  4.4x time\nWith the heapq implementation, one question remains: How should \nI handle returns that are on time? The solution is to never remove a \nbook from the priority queue until its due date. At that time, it will \nbe the first item in the list, and I can simply ignore the book if it’s \nalready been returned. Here, I implement this behavior by adding a \nnew field to track the book’s return status:\n@functools.total_ordering\nclass Book:\n    def __init__(self, title, due_date):\n        self.title = title\n        self.due_date = due_date\n",
      "content_length": 1439,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 368,
      "content": "        self.returned = False  # New field\n \n    ...\nThen, I change the next_overdue_book function to repeatedly ignore \nany book that’s already been returned:\ndef next_overdue_book(queue, now):\n    while queue:\n        book = queue[0]\n        if book.returned:\n            heappop(queue)\n            continue\n \n        if book.due_date < now:\n            heappop(queue)\n            return book\n \n        break\n \n    raise NoOverdueBooks\nThis approach makes the return_book function extremely fast \nbecause it makes no modifications to the priority queue:\ndef return_book(queue, book):\n    book.returned = True\nThe downside of this solution for returns is that the priority queue \nmay grow to the maximum size it would have needed if all books from \nthe library were checked out and went overdue. Although the queue \noperations will be fast thanks to heapq, this storage overhead may \ntake significant memory (see Item 81: “Use tracemalloc to Understand \nMemory Usage and Leaks” for how to debug such usage).\nThat said, if you’re trying to build a robust system, you need to plan \nfor the worst-case scenario; thus, you should expect that it’s possible \nfor every library book to go overdue for some reason (e.g., a natural \ndisaster closes the road to the library). This memory cost is a design \nconsideration that you should have already planned for and mitigated \nthrough additional constraints (e.g., imposing a maximum number of \nsimultaneously lent books).\nBeyond the priority queue primitives that I’ve used in this example, \nthe heapq module provides additional functionality for advanced use \ncases (see help(heapq)). The module is a great choice when its function-\nality matches the problem you’re facing (see the queue.PriorityQueue \nclass for another thread-safe option).\n \nItem 73: Know How to Use heapq for Priority Queues \n345\n",
      "content_length": 1842,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 369,
      "content": "346 \nChapter 8 Robustness and Performance\nThings to Remember\n✦ Priority queues allow you to process items in order of importance \ninstead of in first-in, first-out order.\n✦ If you try to use list operations to implement a priority queue, your \nprogram’s performance will degrade superlinearly as the queue \ngrows.\n✦ The heapq built-in module provides all of the functions you need to \nimplement a priority queue that scales efficiently.\n✦ To use heapq, the items being prioritized must have a natural sort \norder, which requires special methods like __lt__ to be defined for \nclasses.\nItem 74:  Consider memoryview and bytearray for \nZero-Copy Interactions with bytes\nAlthough Python isn’t able to parallelize CPU-bound computation \nwithout extra effort (see Item 64: “Consider concurrent.futures for \nTrue Parallelism”), it is able to support high-throughput, parallel I/O \nin a variety of ways (see Item 53: “Use Threads for Blocking I/O, Avoid \nfor Parallelism” and Item 60: “Achieve Highly Concurrent I/O with \nCoroutines”). That said, it’s surprisingly easy to use these I/O tools \nthe wrong way and reach the conclusion that the language is too slow \nfor even I/O-bound workloads.\nFor example, say that I’m building a media server to stream television \nor movies over a network to users so they can watch without having \nto download the video data in advance. One of the key features of \nsuch a system is the ability for users to move forward or backward \nin the video playback so they can skip or repeat parts. In the client \nprogram, I can implement this by requesting a chunk of data from the \nserver corresponding to the new time index selected by the user:\ndef timecode_to_index(video_id, timecode):\n    ...\n    # Returns the byte offset in the video data\n \ndef request_chunk(video_id, byte_offset, size):\n    ...\n    # Returns size bytes of video_id's data from the offset\n \nvideo_id = ...\ntimecode = '01:09:14:28'\nbyte_offset = timecode_to_index(video_id, timecode)\n",
      "content_length": 1979,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 370,
      "content": " \nItem 74: Consider memoryview for zero-copy interactions \n347\nsize = 20 * 1024 * 1024\nvideo_data = request_chunk(video_id, byte_offset, size)\nHow would you implement the server-side handler that receives the \nrequest_chunk request and returns the corresponding 20 MB chunk \nof video data? For the sake of this example, I assume that the com-\nmand and control parts of the server have already been hooked up \n(see Item 61: “Know How to Port Threaded I/O to asyncio” for what \nthat requires). I focus here on the last steps where the requested \nchunk is extracted from gigabytes of video data that’s cached in mem-\nory and is then sent over a socket back to the client. Here’s what the \nimplementation would look like:\nsocket = ...             # socket connection to client\nvideo_data = ...         # bytes containing data for video_id\nbyte_offset = ...        # Requested starting position\nsize = 20 * 1024 * 1024  # Requested chunk size\n \nchunk = video_data[byte_offset:byte_offset + size]\nsocket.send(chunk)\nThe latency and throughput of this code will come down to two fac-\ntors: how much time it takes to slice the 20 MB video chunk from \nvideo_data, and how much time the socket takes to transmit that \ndata to the client. If I assume that the socket is infinitely fast, I can \nrun a micro-benchmark by using the timeit built-in module to under-\nstand the performance characteristics of slicing bytes instances this \nway to create chunks (see Item 11: “Know How to Slice Sequences” for \nbackground):\nimport timeit\n \ndef run_test():\n    chunk = video_data[byte_offset:byte_offset + size]\n    # Call socket.send(chunk), but ignoring for benchmark\n \nresult = timeit.timeit(\n    stmt='run_test()',\n    globals=globals(),\n    number=100) / 100\n \nprint(f'{result:0.9f} seconds')\n>>>\n0.004925669 seconds\n",
      "content_length": 1802,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 371,
      "content": "348 \nChapter 8 Robustness and Performance\nIt took roughly 5 milliseconds to extract the 20 MB slice of data to \ntransmit to the client. That means the overall throughput of my \nserver is limited to a theoretical maximum of 20 MB / 5 milliseconds \n= 7.3 GB / second, since that’s the fastest I can extract the video \ndata from memory. My server will also be limited to 1 CPU-second / \n5  milliseconds = 200 clients requesting new chunks in parallel, which \nis tiny compared to the tens of thousands of simultaneous connec-\ntions that tools like the asyncio built-in module can support. The \nproblem is that slicing a bytes instance causes the underlying data to \nbe copied, which takes CPU time.\nA better way to write this code is by using Python’s built-in memoryview \ntype, which exposes CPython’s high-performance buffer protocol to \nprograms. The buffer protocol is a low-level C API that allows the \nPython runtime and C extensions to access the underlying data \nbuffers that are behind objects like bytes instances. The best part \nabout memoryview instances is that slicing them results in another \nmemoryview instance without copying the underlying data. Here, I cre-\nate a memoryview wrapping a bytes instance and inspect a slice of it:\ndata = b'shave and a haircut, two bits'\nview = memoryview(data)\nchunk = view[12:19]\nprint(chunk)\nprint('Size:           ', chunk.nbytes)\nprint('Data in view:   ', chunk.tobytes())\nprint('Underlying data:', chunk.obj)\n>>>\n<memory at 0x10951fb80>\nSize:            7\nData in view:    b'haircut'\nUnderlying data: b'shave and a haircut, two bits'\nBy enabling zero-copy operations, memoryview can provide enor-\nmous speedups for code that needs to quickly process large amounts \nof memory, such as numerical C extensions like NumPy and \nI/O-bound programs like this one. Here, I replace the simple bytes \nslicing from above with memoryview slicing instead and repeat the \nsame micro-benchmark:\nvideo_view = memoryview(video_data)\n \ndef run_test():\n    chunk = video_view[byte_offset:byte_offset + size]\n    # Call socket.send(chunk), but ignoring for benchmark\n \n",
      "content_length": 2101,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 372,
      "content": "result = timeit.timeit(\n    stmt='run_test()',\n    globals=globals(),\n    number=100) / 100\n \nprint(f'{result:0.9f} seconds')\n>>>\n0.000000250 seconds\nThe result is 250 nanoseconds. Now the theoretical maximum through-\nput of my server is 20 MB / 250 nanoseconds = 164 TB / second. \nFor parallel clients, I can theoretically support up to 1 CPU- second / \n250 nanoseconds = 4 million. That’s more like it! This means that \nnow my program is entirely bound by the underlying performance of \nthe socket connection to the client, not by CPU constraints.\nNow, imagine that the data must flow in the other direction, where \nsome clients are sending live video streams to the server in order to \nbroadcast them to other users. In order to do this, I need to store the \nlatest video data from the user in a cache that other clients can read \nfrom. Here’s what the implementation of reading 1 MB of new data \nfrom the incoming client would look like:\nsocket = ...        # socket connection to the client\nvideo_cache = ...   # Cache of incoming video stream\nbyte_offset = ...   # Incoming buffer position\nsize = 1024 * 1024  # Incoming chunk size\n \nchunk = socket.recv(size)\nvideo_view = memoryview(video_cache)\nbefore = video_view[:byte_offset]\nafter = video_view[byte_offset + size:]\nnew_cache = b''.join([before, chunk, after])\nThe socket.recv method returns a bytes instance. I can splice the \nnew data with the existing cache at the current byte_offset by using \nsimple slicing operations and the bytes.join method. To understand \nthe performance of this, I can run another micro-benchmark. I’m \nusing a dummy socket, so the performance test is only for the mem-\nory operations, not the I/O interaction:\ndef run_test():\n    chunk = socket.recv(size)\n    before = video_view[:byte_offset]\n    after = video_view[byte_offset + size:]\n    new_cache = b''.join([before, chunk, after])\n \n \nItem 74: Consider memoryview for zero-copy interactions \n349\n",
      "content_length": 1942,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 373,
      "content": "350 \nChapter 8 Robustness and Performance\nresult = timeit.timeit(\n    stmt='run_test()',\n    globals=globals(),\n    number=100) / 100\n \nprint(f'{result:0.9f} seconds')\n>>>\n0.033520550 seconds\nIt takes 33 milliseconds to receive 1 MB and update the video cache. \nThis means my maximum receive throughput is 1 MB / 33  milliseconds \n= 31 MB / second, and I’m limited to 31 MB / 1 MB = 31 simultaneous \nclients streaming in video data this way. This doesn’t scale.\nA better way to write this code is to use Python’s built-in bytearray \ntype in conjunction with memoryview. One limitation with bytes \ninstances is that they are read-only and don’t allow for individual \nindexes to be updated:\nmy_bytes = b'hello'\nmy_bytes[0] = b'\\x79'\n>>>\nTraceback ...\nTypeError: 'bytes' object does not support item assignment\nThe bytearray type is like a mutable version of bytes that allows for \narbitrary positions to be overwritten. bytearray uses integers for its \nvalues instead of bytes:\nmy_array = bytearray(b'hello')\nmy_array[0] = 0x79\nprint(my_array)\n>>>\nbytearray(b'yello')\nA memoryview can also be used to wrap a bytearray. When you slice \nsuch a memoryview, the resulting object can be used to assign data to a \nparticular portion of the underlying buffer. This eliminates the copy-\ning costs from above that were required to splice the bytes instances \nback together after data was received from the client:\nmy_array = bytearray(b'row, row, row your boat')\nmy_view = memoryview(my_array)\nwrite_view = my_view[3:13]\nwrite_view[:] = b'-10 bytes-'\nprint(my_array)\n",
      "content_length": 1556,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 374,
      "content": ">>>\nbytearray(b'row-10 bytes- your boat')\nMany library methods in Python, such as socket.recv_into and \nRawIOBase.readinto, use the buffer protocol to receive or read data \nquickly. The benefit of these methods is that they avoid allocating \nmemory and creating another copy of the data; what’s received goes \nstraight into an existing buffer. Here, I use socket.recv_into along \nwith a memoryview slice to receive data into an underlying bytearray \nwithout the need for splicing:\nvideo_array = bytearray(video_cache)\nwrite_view = memoryview(video_array)\nchunk = write_view[byte_offset:byte_offset + size]\nsocket.recv_into(chunk)\nI can run another micro-benchmark to compare the performance of \nthis approach to the earlier example that used socket.recv:\ndef run_test():\n    chunk = write_view[byte_offset:byte_offset + size]\n    socket.recv_into(chunk)\n \nresult = timeit.timeit(\n    stmt='run_test()',\n    globals=globals(),\n    number=100) / 100\n \nprint(f'{result:0.9f} seconds')\n>>>\n0.000033925 seconds\nIt took 33 microseconds to receive a 1 MB video transmission. This \nmeans my server can support 1 MB / 33 microseconds = 31 GB / \nsecond of max throughput, and 31 GB / 1 MB = 31,000 parallel \nstreaming clients. That’s the type of scalability that I’m looking for!\nThings to Remember\n✦ The memoryview built-in type provides a zero-copy interface for \nreading and writing slices of objects that support Python’s high- \nperformance buffer protocol.\n✦ The bytearray built-in type provides a mutable bytes-like type \nthat can be used for zero-copy data reads with functions like \nsocket.recv_from.\n✦ A memoryview can wrap a bytearray, allowing for received data to be \nspliced into an arbitrary buffer location without copying costs.\n \nItem 74: Consider memoryview for zero-copy interactions \n351\n",
      "content_length": 1798,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 375,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 376,
      "content": "9\nTesting and \nDebugging\nPython doesn’t have compile-time static type checking. There’s \n nothing in the interpreter that will ensure that your program will \nwork correctly when you run it. Python does support optional type \nannotations that can be used in static analysis to detect many kinds \nof bugs (see Item 90: “Consider Static Analysis via typing to Obviate \nBugs” for details). However, it’s still fundamentally a dynamic lan-\nguage, and anything is possible. With Python, you ultimately don’t \nknow if the functions your program calls will be defined at runtime, \neven when their existence is evident in the source code. This dynamic \nbehavior is both a blessing and a curse.\nThe large numbers of Python programmers out there say it’s worth \ngoing without compile-time static type checking because of the pro-\nductivity gained from the resulting brevity and simplicity. But most \npeople using Python have at least one horror story about a program \nencountering a boneheaded error at runtime. One of the worst exam-\nples I’ve heard of involved a SyntaxError being raised in production as \na side effect of a dynamic import (see Item 88: “Know How to Break \nCircular Dependencies”), resulting in a crashed server process. The \nprogrammer I know who was hit by this surprising occurrence has \nsince ruled out using Python ever again.\nBut I have to wonder, why wasn’t the code more well tested before \nthe program was deployed to production? Compile-time static type \nsafety isn’t everything. You should always test your code, regardless \nof what language it’s written in. However, I’ll admit that in Python it \nmay be more important to write tests to verify correctness than in \nother languages. Luckily, the same dynamic features that create risks \nalso make it extremely easy to write tests for your code and to debug \nmalfunctioning programs. You can use Python’s dynamic nature and \neasily overridable behaviors to implement tests and ensure that your \nprograms work as expected.\n",
      "content_length": 1990,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 377,
      "content": "354 \nChapter 9 Testing and Debugging\nYou should think of tests as an insurance policy on your code. Good \ntests give you confidence that your code is correct. If you refactor or \nexpand your code, tests that verify behavior—not implementation—\nmake it easy to identify what’s changed. It sounds counterintuitive, \nbut having good tests actually makes it easier to modify Python code, \nnot harder.\nItem 75: Use repr Strings for Debugging Output\nWhen debugging a Python program, the print function and format \nstrings (see Item 4: “Prefer Interpolated F-Strings Over C-style Format \nStrings and str.format”), or output via the logging built-in module, \nwill get you surprisingly far. Python internals are often easy to access \nvia plain attributes (see Item 42: “Prefer Public Attributes Over Pri-\nvate Ones”). All you need to do is call print to see how the state of \nyour program changes while it runs and understand where it goes \nwrong.\nThe print function outputs a human-readable string version of \n whatever you supply it. For example, printing a basic string prints the \ncontents of the string without the surrounding quote characters:\nprint('foo bar')\n>>>\nfoo bar\nThis is equivalent to all of these alternatives:\n \n■Calling the str function before passing the value to print\n \n■Using the '%s' format string with the % operator\n \n■Default formatting of the value with an f-string\n \n■Calling the format built-in function\n \n■Explicitly calling the __format__ special method\n \n■Explicitly calling the __str__ special method\nHere, I verify this behavior:\nmy_value = 'foo bar'\nprint(str(my_value))\nprint('%s' % my_value)\nprint(f'{my_value}')\nprint(format(my_value))\nprint(my_value.__format__('s'))\nprint(my_value.__str__())\n",
      "content_length": 1724,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 378,
      "content": " \nItem 75: Use repr Strings for Debugging Output \n355\n>>>\nfoo bar\nfoo bar\nfoo bar\nfoo bar\nfoo bar\nfoo bar\nThe problem is that the human-readable string for a value doesn’t \nmake it clear what the actual type and its specific composition are. \nFor example, notice how in the default output of print, you can’t dis-\ntinguish between the types of the number 5 and the string '5':\nprint(5)\nprint('5')\n \nint_value = 5\nstr_value = '5'\nprint(f'{int_value} == {str_value} ?')\n>>>\n5\n5\n5 == 5 ?\nIf you’re debugging a program with print, these type differences mat-\nter. What you almost always want while debugging is to see the repr \nversion of an object. The repr built-in function returns the printable \nrepresentation of an object, which should be its most clearly under-\nstandable string representation. For most built-in types, the string \nreturned by repr is a valid Python expression:\na = '\\x07'\nprint(repr(a))\n>>>\n'\\x07'\nPassing the value from repr to the eval built-in function should result \nin the same Python object that you started with (and, of course, in \npractice you should only use eval with extreme caution):\nb = eval(repr(a))\nassert a == b\nWhen you’re debugging with print, you should call repr on a value \nbefore printing to ensure that any difference in types is clear:\nprint(repr(5))\nprint(repr('5'))\n",
      "content_length": 1314,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 379,
      "content": "356 \nChapter 9 Testing and Debugging\n>>>\n5\n'5'\nThis is equivalent to using the '%r' format string with the % operator \nor an f-string with the !r type conversion:\nprint('%r' % 5)\nprint('%r' % '5')\n \nint_value = 5\nstr_value = '5'\nprint(f'{int_value!r} != {str_value!r}')\n>>>\n5\n'5'\n5 != '5'\nFor instances of Python classes, the default human-readable string \nvalue is the same as the repr value. This means that passing an \ninstance to print will do the right thing, and you don’t need to explic-\nitly call repr on it. Unfortunately, the default implementation of \nrepr for object subclasses isn’t especially helpful. For example, here \nI define a simple class and then print one of its instances:\nclass OpaqueClass:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n \nobj = OpaqueClass(1, 'foo')\nprint(obj)\n>>>\n<__main__.OpaqueClass object at 0x10963d6d0>\nThis output can’t be passed to the eval function, and it says nothing \nabout the instance fields of the object.\nThere are two solutions to this problem. If you have control of the \nclass, you can define your own __repr__ special method that returns \na string containing the Python expression that re-creates the object. \nHere, I define that function for the class above:\nclass BetterClass:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n \n",
      "content_length": 1333,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 380,
      "content": " \nItem 76: Verify Related Behaviors in TestCase Subclasses \n357\n    def __repr__(self):\n        return f'BetterClass({self.x!r}, {self.y!r})'\nNow the repr value is much more useful:\nobj = BetterClass(2, 'bar')\nprint(obj)\n>>>\nBetterClass(2, 'bar')\nWhen you don’t have control over the class definition, you can reach \ninto the object’s instance dictionary, which is stored in the __dict__ \nattribute. Here, I print out the contents of an OpaqueClass instance:\nobj = OpaqueClass(4, 'baz')\nprint(obj.__dict__)\n>>>\n{'x': 4, 'y': 'baz'}\nThings to Remember\n✦ Calling print on built-in Python types produces the human- \nreadable string version of a value, which hides type information.\n✦ Calling repr on built-in Python types produces the printable string \nversion of a value. These repr strings can often be passed to the \neval built-in function to get back the original value.\n✦ %s in format strings produces human-readable strings like str. %r \nproduces printable strings like repr. F-strings produce human- \nreadable strings for replacement text expressions unless you specify \nthe !r suffix.\n✦ You can define the __repr__ special method on a class to customize \nthe printable representation of instances and provide more detailed \ndebugging information.\nItem 76:  Verify Related Behaviors in TestCase \nSubclasses\nThe canonical way to write tests in Python is to use the unittest \nbuilt-in module. For example, say I have the following utility function \ndefined in utils.py that I would like to verify works correctly across a \nvariety of inputs:\n# utils.py\ndef to_str(data):\n",
      "content_length": 1573,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 381,
      "content": "358 \nChapter 9 Testing and Debugging\n    if isinstance(data, str):\n        return data\n    elif isinstance(data, bytes):\n        return data.decode('utf-8')\n    else:\n        raise TypeError('Must supply str or bytes, '\n                        'found: %r' % data)\nTo define tests, I create a second file named test_utils.py or \nutils_test.py—the naming scheme you prefer is a style choice—that \ncontains tests for each behavior that I expect:\n# utils_test.py\nfrom unittest import TestCase, main\nfrom utils import to_str\n \nclass UtilsTestCase(TestCase):\n    def test_to_str_bytes(self):\n        self.assertEqual('hello', to_str(b'hello'))\n \n    def test_to_str_str(self):\n        self.assertEqual('hello', to_str('hello'))\n \n    def test_failing(self):\n        self.assertEqual('incorrect', to_str('hello'))\n \nif __name__ == '__main__':\n    main()\nThen, I run the test file using the Python command line. In this case, \ntwo of the test methods pass and one fails, with a helpful error mes-\nsage about what went wrong:\n$ python3 utils_test.py\nF..\n===============================================================\nFAIL: test_failing (__main__.UtilsTestCase)\n---------------------------------------------------------------\nTraceback (most recent call last):\n  File \"utils_test.py\", line 15, in test_failing\n    self.assertEqual('incorrect', to_str('hello'))\nAssertionError: 'incorrect' != 'hello'\n- incorrect\n+ hello\n \n \n---------------------------------------------------------------\n",
      "content_length": 1479,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 382,
      "content": "Ran 3 tests in 0.002s\n \nFAILED (failures=1)\nTests are organized into TestCase subclasses. Each test case is a \nmethod beginning with the word test. If a test method runs without \nraising any kind of Exception (including AssertionError from assert \nstatements), the test is considered to have passed successfully. If one \ntest fails, the TestCase subclass continues running the other test \nmethods so you can get a full picture of how all your tests are doing \ninstead of stopping at the first sign of trouble.\nIf you want to iterate quickly to fix or improve a specific test, you can \nrun only that test method by specifying its path within the test mod-\nule on the command line:\n$ python3 utils_test.py UtilsTestCase.test_to_str_bytes\n.\n---------------------------------------------------------------\nRan 1 test in 0.000s\n \nOK\nYou can also invoke the debugger from directly within test methods \nat specific breakpoints in order to dig more deeply into the cause of \nfailures (see Item 80: “Consider Interactive Debugging with pdb” for \nhow to do that).\nThe TestCase class provides helper methods for making assertions in \nyour tests, such as assertEqual for verifying equality, assertTrue for \nverifying Boolean expressions, and many more (see help(TestCase) \nfor the full list). These are better than the built-in assert state-\nment because they print out all of the inputs and outputs to help \nyou understand the exact reason the test is failing. For example, here \nI have the same test case written with and without using a helper \nassertion method:\n# assert_test.py\nfrom unittest import TestCase, main\nfrom utils import to_str\n \nclass AssertTestCase(TestCase):\n    def test_assert_helper(self):\n        expected = 12\n        found = 2 * 5\n        self.assertEqual(expected, found)\n \n \nItem 76: Verify Related Behaviors in TestCase Subclasses \n359\n",
      "content_length": 1852,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 383,
      "content": "360 \nChapter 9 Testing and Debugging\n    def test_assert_statement(self):\n        expected = 12\n        found = 2 * 5\n        assert expected == found\n \nif __name__ == '__main__':\n    main()\nWhich of these failure messages seems more helpful to you?\n$ python3 assert_test.py\nFF\n===============================================================\nFAIL: test_assert_helper (__main__.AssertTestCase)\n---------------------------------------------------------------\nTraceback (most recent call last):\n  File \"assert_test.py\", line 16, in test_assert_helper\n    self.assertEqual(expected, found)\nAssertionError: 12 != 10\n \n===============================================================\nFAIL: test_assert_statement (__main__.AssertTestCase)\n---------------------------------------------------------------\nTraceback (most recent call last):\n  File \"assert_test.py\", line 11, in test_assert_statement\n    assert expected == found\nAssertionError\n \n---------------------------------------------------------------\nRan 2 tests in 0.001s\n \nFAILED (failures=2)\nThere’s also an assertRaises helper method for verifying excep-\ntions that can be used as a context manager in with statements (see \nItem 66: “Consider contextlib and with Statements for Reusable \ntry/finally Behavior” for how that works). This appears similar to a \ntry/except statement and makes it abundantly clear where the excep-\ntion is expected to be raised:\n# utils_error_test.py\nfrom unittest import TestCase, main\nfrom utils import to_str\n \nclass UtilsErrorTestCase(TestCase):\n",
      "content_length": 1530,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 384,
      "content": "    def test_to_str_bad(self):\n        with self.assertRaises(TypeError):\n            to_str(object())\n \n    def test_to_str_bad_encoding(self):\n        with self.assertRaises(UnicodeDecodeError):\n            to_str(b'\\xfa\\xfa')\n \nif __name__ == '__main__':\n    main()\nYou can define your own helper methods with complex logic in \nTestCase subclasses to make your tests more readable. Just ensure \nthat your method names don’t begin with the word test, or they’ll \nbe run as if they’re test cases. In addition to calling TestCase asser-\ntion methods, these custom test helpers often use the fail method to \nclarify which assumption or invariant wasn’t met. For example, here \nI define a custom test helper method for verifying the behavior of a \ngenerator:\n# helper_test.py\nfrom unittest import TestCase, main\n \ndef sum_squares(values):\n    cumulative = 0\n    for value in values:\n        cumulative += value ** 2\n        yield cumulative\n \nclass HelperTestCase(TestCase):\n    def verify_complex_case(self, values, expected):\n        expect_it = iter(expected)\n        found_it = iter(sum_squares(values))\n        test_it = zip(expect_it, found_it)\n \n        for i, (expect, found) in enumerate(test_it):\n            self.assertEqual(\n                expect,\n                found,\n                f'Index {i} is wrong')\n \n        # Verify both generators are exhausted\n        try:\n            next(expect_it)\n        except StopIteration:\n            pass\n \nItem 76: Verify Related Behaviors in TestCase Subclasses \n361\n",
      "content_length": 1522,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 385,
      "content": "362 \nChapter 9 Testing and Debugging\n        else:\n            self.fail('Expected longer than found')\n \n        try:\n            next(found_it)\n        except StopIteration:\n            pass\n        else:\n            self.fail('Found longer than expected')\n \n    def test_wrong_lengths(self):\n        values = [1.1, 2.2, 3.3]\n        expected = [\n            1.1**2,\n        ]\n        self.verify_complex_case(values, expected)\n \n    def test_wrong_results(self):\n        values = [1.1, 2.2, 3.3]\n        expected = [\n            1.1**2,\n            1.1**2 + 2.2**2,\n            1.1**2 + 2.2**2 + 3.3**2 + 4.4**2,\n        ]\n        self.verify_complex_case(values, expected)\n \nif __name__ == '__main__':\n    main()\nThe helper method makes the test cases short and readable, and the \noutputted error messages are easy to understand:\n$ python3 helper_test.py\nFF\n===============================================================\nFAIL: test_wrong_lengths (__main__.HelperTestCase)\n---------------------------------------------------------------\nTraceback (most recent call last):\n  File \"helper_test.py\", line 43, in test_wrong_lengths\n    self.verify_complex_case(values, expected)\n  File \"helper_test.py\", line 34, in verify_complex_case\n    self.fail('Found longer than expected')\nAssertionError: Found longer than expected\n \n",
      "content_length": 1324,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 386,
      "content": "===============================================================\nFAIL: test_wrong_results (__main__.HelperTestCase)\n---------------------------------------------------------------\nTraceback (most recent call last):\n  File \"helper_test.py\", line 52, in test_wrong_results\n    self.verify_complex_case(values, expected)\n  File \"helper_test.py\", line 24, in verify_complex_case\n    f'Index {i} is wrong')\nAssertionError: 36.3 != 16.939999999999998 : Index 2 is wrong\n \n---------------------------------------------------------------\nRan 2 tests in 0.002s\n \nFAILED (failures=2)\nI usually define one TestCase subclass for each set of related tests. \nSometimes, I have one TestCase subclass for each function that has \nmany edge cases. Other times, a TestCase subclass spans all func-\ntions in a single module. I often create one TestCase subclass for test-\ning each basic class and all of its methods.\nThe TestCase class also provides a subTest helper method that enables \nyou to avoid boilerplate by defining multiple tests within a single test \nmethod. This is especially helpful for writing data-driven tests, and it \nallows the test method to continue testing other cases even after one \nof them fails (similar to the behavior of TestCase with its contained \ntest methods). To show this, here I define an example data-driven test:\n# data_driven_test.py\nfrom unittest import TestCase, main\nfrom utils import to_str\n \nclass DataDrivenTestCase(TestCase):\n    def test_good(self):\n        good_cases = [\n            (b'my bytes', 'my bytes'),\n            ('no error', b'no error'),  # This one will fail\n            ('other str', 'other str'),\n            ...\n        ]\n        for value, expected in good_cases:\n            with self.subTest(value):\n                self.assertEqual(expected, to_str(value))\n \n \nItem 76: Verify Related Behaviors in TestCase Subclasses \n363\n",
      "content_length": 1869,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 387,
      "content": "364 \nChapter 9 Testing and Debugging\n    def test_bad(self):\n        bad_cases = [\n            (object(), TypeError),\n            (b'\\xfa\\xfa', UnicodeDecodeError),\n            ...\n        ]\n        for value, exception in bad_cases:\n            with self.subTest(value):\n                with self.assertRaises(exception):\n                    to_str(value)\n \nif __name__ == '__main__':\n    main()\nThe 'no error' test case fails, printing a helpful error message, but \nall of the other cases are still tested and confirmed to pass:\n$ python3 data_driven_test.py\n.\n===============================================================\nFAIL: test_good (__main__.DataDrivenTestCase) [no error]\n---------------------------------------------------------------\nTraceback (most recent call last):\n  File \"testing/data_driven_test.py\", line 18, in test_good\n    self.assertEqual(expected, to_str(value))\nAssertionError: b'no error' != 'no error'\n \n---------------------------------------------------------------\nRan 2 tests in 0.001s\n \nFAILED (failures=1)\nNote\nDepending on your project’s complexity and testing requirements, the pytest \n(https://pytest.org) open source package and its large number of community \nplug-ins can be especially useful.\nThings to Remember\n✦ You can create tests by subclassing the TestCase class from the \nunittest built-in module and defining one method per behavior \nyou’d like to test. Test methods on TestCase classes must start with \nthe word test.\n✦ Use the various helper methods defined by the TestCase class, such \nas assertEqual, to confirm expected behaviors in your tests instead \nof using the built-in assert statement.\n",
      "content_length": 1647,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 388,
      "content": " \nItem 77: Isolate Tests from Each Other \n365\n✦ Consider writing data-driven tests using the subTest helper method \nin order to reduce boilerplate.\nItem 77:  Isolate Tests from Each Other with setUp, \ntearDown, setUpModule, and tearDownModule\nTestCase classes (see Item 76: “Verify Related Behaviors in TestCase \nSubclasses”) often need to have the test environment set up before \ntest methods can be run; this is sometimes called the test harness. \nTo do this, you can override the setUp and tearDown methods of a \nTestCase subclass. These methods are called before and after each \ntest method, respectively, so you can ensure that each test runs in \nisolation, which is an important best practice of proper testing.\nFor example, here I define a TestCase that creates a temporary direc-\ntory before each test and deletes its contents after each test finishes:\n# environment_test.py\nfrom pathlib import Path\nfrom tempfile import TemporaryDirectory\nfrom unittest import TestCase, main\n \nclass EnvironmentTest(TestCase):\n    def setUp(self):\n        self.test_dir = TemporaryDirectory()\n        self.test_path = Path(self.test_dir.name)\n \n    def tearDown(self):\n        self.test_dir.cleanup()\n \n    def test_modify_file(self):\n        with open(self.test_path / 'data.bin', 'w') as f:\n            ...\n \nif __name__ == '__main__':\n    main()\nWhen programs get complicated, you’ll want additional tests to ver-\nify the end-to-end interactions between your modules instead of only \ntesting code in isolation (using tools like mocks; see Item 78: “Use \nMocks to Test Code with Complex Dependencies”). This is the differ-\nence between unit tests and integration tests. In Python, it’s important \nto write both types of tests for exactly the same reason: You have no \nguarantee that your modules will actually work together unless you \nprove it.\n",
      "content_length": 1840,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 389,
      "content": "366 \nChapter 9 Testing and Debugging\nOne common problem is that setting up your test environment for \nintegration tests can be computationally expensive and may require \na lot of wall-clock time. For example, you might need to start a data-\nbase process and wait for it to finish loading indexes before you can \nrun your integration tests. This type of latency makes it impracti-\ncal to do test preparation and cleanup for every test in the TestCase \nclass’s setUp and tearDown methods.\nTo handle this situation, the unittest module also supports \n module-level test harness initialization. You can configure expensive \nresources a single time, and then have all TestCase classes and their \ntest methods run without repeating that initialization. Later, when all \ntests in the module are finished, the test harness can be torn down \na single time. Here, I take advantage of this behavior by defining \nsetUpModule and tearDownModule functions within the module con-\ntaining the TestCase classes:\n# integration_test.py\nfrom unittest import TestCase, main\n \ndef setUpModule():\n    print('* Module setup')\n \ndef tearDownModule():\n    print('* Module clean-up')\n \nclass IntegrationTest(TestCase):\n    def setUp(self):\n        print('* Test setup')\n \n    def tearDown(self):\n        print('* Test clean-up')\n \n    def test_end_to_end1(self):\n        print('* Test 1')\n \n    def test_end_to_end2(self):\n        print('* Test 2')\n \nif __name__ == '__main__':\n    main()\n$ python3 integration_test.py \n* Module setup\n* Test setup\n* Test 1\n",
      "content_length": 1530,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 390,
      "content": " \nItem 78: Use Mocks to Test Code with Complex Dependencies \n367\n* Test clean-up\n.* Test setup\n* Test 2\n* Test clean-up\n.* Module clean-up\n \n---------------------------------------------------------------\nRan 2 tests in 0.000s\n \nOK\nI can clearly see that setUpModule is run by unittest only once, \nand it happens before any setUp methods are called. Similarly, \ntearDownModule happens after the tearDown method is called.\nThings to Remember\n✦ It’s important to write both unit tests (for isolated functionality) and \nintegration tests (for modules that interact with each other).\n✦ Use the setUp and tearDown methods to make sure your tests are \nisolated from each other and have a clean test environment.\n✦ For integration tests, use the setUpModule and tearDownModule \n module-level functions to manage any test harnesses you need for \nthe entire lifetime of a test module and all of the TestCase classes \nthat it contains.\nItem 78:  Use Mocks to Test Code with Complex \nDependencies\nAnother common need when writing tests (see Item 76: “Verify Related \nBehaviors in TestCase Subclasses”) is to use mocked functions and \nclasses to simulate behaviors when it’s too difficult or slow to use the \nreal thing. For example, say that I need a program to maintain the \nfeeding schedule for animals at the zoo. Here, I define a function to \nquery a database for all of the animals of a certain species and return \nwhen they most recently ate:\nclass DatabaseConnection:\n    ...\n \ndef get_animals(database, species):\n    # Query the database\n    ...\n    # Return a list of (name, last_mealtime) tuples\n",
      "content_length": 1595,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 391,
      "content": "368 \nChapter 9 Testing and Debugging\nHow do I get a DatabaseConnection instance to use for testing this \nfunction? Here, I try to create one and pass it into the function being \ntested:\ndatabase = DatabaseConnection('localhost', '4444')\n \nget_animals(database, 'Meerkat')\n>>>\nTraceback ...\nDatabaseConnectionError: Not connected\nThere’s no database running, so of course this fails. One solution is \nto actually stand up a database server and connect to it in the test. \nHowever, it’s a lot of work to fully automate starting up a database, \nconfiguring its schema, populating it with data, and so on in order to \njust run a simple unit test. Further, it will probably take a lot of wall-\nclock time to set up a database server, which would slow down these \nunit tests and make them harder to maintain.\nA better approach is to mock out the database. A mock lets you provide \nexpected responses for dependent functions, given a set of expected \ncalls. It’s important not to confuse mocks with fakes. A fake would \nprovide most of the behavior of the DatabaseConnection class but with \na simpler implementation, such as a basic in-memory, single-threaded \ndatabase with no persistence.\nPython has the unittest.mock built-in module for creating mocks and \nusing them in tests. Here, I define a Mock instance that simulates the \nget_animals function without actually connecting to the database:\nfrom datetime import datetime\nfrom unittest.mock import Mock\n \nmock = Mock(spec=get_animals)\nexpected = [\n    ('Spot', datetime(2019, 6, 5, 11, 15)),\n    ('Fluffy', datetime(2019, 6, 5, 12, 30)),\n    ('Jojo', datetime(2019, 6, 5, 12, 45)),\n]\nmock.return_value = expected\nThe Mock class creates a mock function. The return_value attribute \nof the mock is the value to return when it is called. The spec argu-\nment indicates that the mock should act like the given object, which \nis a function in this case, and error if it’s used in the wrong way. \n",
      "content_length": 1939,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 392,
      "content": " \nItem 78: Use Mocks to Test Code with Complex Dependencies \n369\nFor example, here I try to treat the mock function as if it were a mock \nobject with attributes:\nmock.does_not_exist\n>>>\nTraceback ...\nAttributeError: Mock object has no attribute 'does_not_exist'\nOnce it’s created, I can call the mock, get its return value, and ver-\nify that what it returns matches expectations. I use a unique object \nvalue as the database argument because it won’t actually be used by \nthe mock to do anything; all I care about is that the database param-\neter was correctly plumbed through to any dependent functions that \nneeded a DatabaseConnection instance in order to work (see Item 55: \n“Use Queue to Coordinate Work Between Threads” for another example \nof using sentinel object instances):\ndatabase = object()\nresult = mock(database, 'Meerkat')\nassert result == expected\nThis verifies that the mock responded correctly, but how do I know if \nthe code that called the mock provided the correct arguments? For \nthis, the Mock class provides the assert_called_once_with method, \nwhich verifies that a single call with exactly the given parameters was \nmade:\nmock.assert_called_once_with(database, 'Meerkat')\nIf I supply the wrong parameters, an exception is raised, and any \nTestCase that the assertion is used in fails:\nmock.assert_called_once_with(database, 'Giraffe')\n>>>\nTraceback ...\nAssertionError: expected call not found.\nExpected: mock(<object object at 0x109038790>, 'Giraffe')\nActual: mock(<object object at 0x109038790>, 'Meerkat')\nIf I actually don’t care about some of the individual parameters, such \nas exactly which database object was used, then I can indicate that \nany value is okay for an argument by using the unittest.mock.ANY \nconstant. I can also use the assert_called_with method of Mock to \nverify that the most recent call to the mock—and there may have \nbeen multiple calls in this case—matches my expectations:\nfrom unittest.mock import ANY\n \n",
      "content_length": 1964,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 393,
      "content": "370 \nChapter 9 Testing and Debugging\nmock = Mock(spec=get_animals)\nmock('database 1', 'Rabbit')\nmock('database 2', 'Bison')\nmock('database 3', 'Meerkat')\n \nmock.assert_called_with(ANY, 'Meerkat')\nANY is useful in tests when a parameter is not core to the behavior that’s \nbeing tested. It’s often worth erring on the side of under- specifying \ntests by using ANY more liberally instead of over-specifying tests and \nhaving to plumb through various test parameter expectations.\nThe Mock class also makes it easy to mock exceptions being raised:\nclass MyError(Exception):\n    pass\n \nmock = Mock(spec=get_animals)\nmock.side_effect = MyError('Whoops! Big problem')\nresult = mock(database, 'Meerkat')\n>>>\nTraceback ...\nMyError: Whoops! Big problem\nThere are many more features available, so be sure to see \nhelp(unittest.mock.Mock) for the full range of options.\nNow that I’ve shown the mechanics of how a Mock works, I can apply \nit to an actual testing situation to show how to use it effectively in \nwriting unit tests. Here, I define a function to do the rounds of feeding \nanimals at the zoo, given a set of database-interacting functions:\ndef get_food_period(database, species):\n    # Query the database\n    ...\n    # Return a time delta\n \ndef feed_animal(database, name, when):\n    # Write to the database\n    ...\n \ndef do_rounds(database, species):\n    now = datetime.datetime.utcnow()\n    feeding_timedelta = get_food_period(database, species)\n    animals = get_animals(database, species)\n    fed = 0\n \n",
      "content_length": 1507,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 394,
      "content": " \nItem 78: Use Mocks to Test Code with Complex Dependencies \n371\n    for name, last_mealtime in animals:\n        if (now - last_mealtime) > feeding_timedelta:\n            feed_animal(database, name, now)\n            fed += 1\n \n    return fed\nThe goal of my test is to verify that when do_rounds is run, the right \nanimals got fed, the latest feeding time was recorded to the data-\nbase, and the total number of animals fed returned by the function \nmatches the correct total. In order to do all this, I need to mock out \ndatetime.utcnow so my tests have a stable time that isn’t affected by \ndaylight saving time and other ephemeral changes. I need to mock \nout get_food_period and get_animals to return values that would \nhave come from the database. And I need to mock out feed_animal to \naccept data that would have been written back to the database.\nThe question is: Even if I know how to create these mock functions \nand set expectations, how do I get the do_rounds function that’s \nbeing tested to use the mock dependent functions instead of the \nreal versions? One approach is to inject everything as keyword-only \narguments (see Item 25: “Enforce Clarity with Keyword-Only and \nPositional-Only Arguments”):\ndef do_rounds(database, species, *,\n              now_func=datetime.utcnow,\n              food_func=get_food_period,\n              animals_func=get_animals,\n              feed_func=feed_animal):\n    now = now_func()\n    feeding_timedelta = food_func(database, species)\n    animals = animals_func(database, species)\n    fed = 0\n \n    for name, last_mealtime in animals:\n        if (now - last_mealtime) > feeding_timedelta:\n            feed_func(database, name, now)\n            fed += 1\n \n    return fed\nTo test this function, I need to create all of the Mock instances upfront \nand set their expectations:\nfrom datetime import timedelta\n \n",
      "content_length": 1855,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 395,
      "content": "372 \nChapter 9 Testing and Debugging\nnow_func = Mock(spec=datetime.utcnow)\nnow_func.return_value = datetime(2019, 6, 5, 15, 45)\n \nfood_func = Mock(spec=get_food_period)\nfood_func.return_value = timedelta(hours=3)\n \nanimals_func = Mock(spec=get_animals)\nanimals_func.return_value = [\n    ('Spot', datetime(2019, 6, 5, 11, 15)),\n    ('Fluffy', datetime(2019, 6, 5, 12, 30)),\n    ('Jojo', datetime(2019, 6, 5, 12, 45)),\n]\n \nfeed_func = Mock(spec=feed_animal)\nThen, I can run the test by passing the mocks into the do_rounds \nfunction to override the defaults:\nresult = do_rounds(\n    database,\n    'Meerkat',\n    now_func=now_func,\n    food_func=food_func,\n    animals_func=animals_func,\n    feed_func=feed_func)\n \nassert result == 2\nFinally, I can verify that all of the calls to dependent functions \nmatched my expectations:\nfrom unittest.mock import call\n \nfood_func.assert_called_once_with(database, 'Meerkat')\n \nanimals_func.assert_called_once_with(database, 'Meerkat')\n \nfeed_func.assert_has_calls(\n    [\n        call(database, 'Spot', now_func.return_value),\n        call(database, 'Fluffy', now_func.return_value),\n    ],\n    any_order=True)\nI don’t verify the parameters to the datetime.utcnow mock or how many \ntimes it was called because that’s indirectly verified by the return value \nof the function. For get_food_period and get_animals, I verify a single \ncall with the specified parameters by using assert_called_once_with. \n",
      "content_length": 1437,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 396,
      "content": " \nItem 78: Use Mocks to Test Code with Complex Dependencies \n373\nFor the feed_animal function, I verify that two calls were made—\nand their order didn’t matter—to write to the database using the \nunittest.mock.call helper and the assert_has_calls method.\nThis approach of using keyword-only arguments for injecting mocks \nworks, but it’s quite verbose and requires changing every function \nyou want to test. The unittest.mock.patch family of functions makes \ninjecting mocks easier. It temporarily reassigns an attribute of a mod-\nule or class, such as the database-accessing functions that I defined \nabove. For example, here I override get_animals to be a mock using \npatch:\nfrom unittest.mock import patch\n \nprint('Outside patch:', get_animals)\n \nwith patch('__main__.get_animals'):\n    print('Inside patch: ', get_animals)\n \nprint('Outside again:', get_animals)\n>>>\nOutside patch: <function get_animals at 0x109217040>\nInside patch:  <MagicMock name='get_animals' id='4454622832'>\nOutside again: <function get_animals at 0x109217040>\npatch works for many modules, classes, and attributes. It can be used \nin with statements (see Item 66: “Consider contextlib and with State-\nments for Reusable try/finally Behavior”), as a function decorator \n(see Item 26: “Define Function Decorators with functools.wraps”), or \nin the setUp and tearDown methods of TestCase classes (see Item 76: \n“Verify Related Behaviors in TestCase Subclasses”). For the full range \nof options, see help(unittest.mock.patch).\nHowever, patch doesn’t work in all cases. For example, to test do_rounds \nI need to mock out the current time returned by the datetime.utcnow \nclass method. Python won’t let me do that because the datetime class is \ndefined in a C-extension module, which can’t be modified in this way:\nfake_now = datetime(2019, 6, 5, 15, 45)\n \nwith patch('datetime.datetime.utcnow'):\n    datetime.utcnow.return_value = fake_now\n>>>\nTraceback ...\nTypeError: can't set attributes of built-in/extension type \n¯'datetime.datetime'\n",
      "content_length": 2012,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 397,
      "content": "374 \nChapter 9 Testing and Debugging\nTo work around this, I can create another helper function to fetch \ntime that can be patched:\ndef get_do_rounds_time():\n    return datetime.datetime.utcnow()\n \ndef do_rounds(database, species):\n    now = get_do_rounds_time()\n    ...\n \nwith patch('__main__.get_do_rounds_time'):\n    ...\nAlternatively, \nI \ncan \nuse \na \nkeyword-only \nargument \nfor \nthe \ndatetime.utcnow mock and use patch for all of the other mocks:\ndef do_rounds(database, species, *, utcnow=datetime.utcnow):\n    now = utcnow()\n    feeding_timedelta = get_food_period(database, species)\n    animals = get_animals(database, species)\n    fed = 0\n \n    for name, last_mealtime in animals:\n        if (now - last_mealtime) > feeding_timedelta:\n            feed_func(database, name, now)\n            fed += 1\n \n    return fed\nI’m going to go with the latter approach. Now, I can use the \npatch.multiple function to create many mocks and set their \nexpectations:\nfrom unittest.mock import DEFAULT\n \nwith patch.multiple('__main__',\n                    autospec=True,\n                    get_food_period=DEFAULT,\n                    get_animals=DEFAULT,\n                    feed_animal=DEFAULT):\n    now_func = Mock(spec=datetime.utcnow)\n    now_func.return_value = datetime(2019, 6, 5, 15, 45)\n    get_food_period.return_value = timedelta(hours=3)\n    get_animals.return_value = [\n        ('Spot', datetime(2019, 6, 5, 11, 15)),\n        ('Fluffy', datetime(2019, 6, 5, 12, 30)),\n        ('Jojo', datetime(2019, 6, 5, 12, 45))\n    ]\n",
      "content_length": 1529,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 398,
      "content": " Item 79: Encapsulate Dependencies to Facilitate Mocking and Testing \n375\nWith the setup ready, I can run the test and verify that the calls were \ncorrect inside the with statement that used patch.multiple:\n    result = do_rounds(database, 'Meerkat', utcnow=now_func)\n    assert result == 2\n \n    food_func.assert_called_once_with(database, 'Meerkat')\n    animals_func.assert_called_once_with(database, 'Meerkat')\n    feed_func.assert_has_calls(\n        [\n            call(database, 'Spot', now_func.return_value),\n            call(database, 'Fluffy', now_func.return_value),\n        ],\n        any_order=True)\nThe keyword arguments to patch.multiple correspond to names in the \n__main__ module that I want to override during the test. The DEFAULT \nvalue indicates that I want a standard Mock instance to be created for \neach name. All of the generated mocks will adhere to the specification \nof the objects they are meant to simulate, thanks to the autospec=True \nparameter.\nThese mocks work as expected, but it’s important to realize that it’s \npossible to further improve the readability of these tests and reduce \nboilerplate by refactoring your code to be more testable (see Item 79: \n“Encapsulate Dependencies to Facilitate Mocking and Testing”).\nThings to Remember\n✦ The unittest.mock module provides a way to simulate the behavior \nof interfaces using the Mock class. Mocks are useful in tests when \nit’s difficult to set up the dependencies that are required by the code \nthat’s being tested.\n✦ When using mocks, it’s important to verify both the behavior of the \ncode being tested and how dependent functions were called by that \ncode, using the Mock.assert_called_once_with family of methods.\n✦ Keyword-only arguments and the unittest.mock.patch family of \nfunctions can be used to inject mocks into the code being tested.\nItem 79:  Encapsulate Dependencies to Facilitate \nMocking and Testing\nIn the previous item (see Item 78: “Use Mocks to Test Code with \nComplex Dependencies”), I showed how to use the facilities of the \nunittest.mock built-in module—including the Mock class and patch \n",
      "content_length": 2102,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 399,
      "content": "376 \nChapter 9 Testing and Debugging\nfamily of functions—to write tests that have complex dependencies, \nsuch as a database. However, the resulting test code requires a lot of \nboilerplate, which could make it more difficult for new readers of the \ncode to understand what the tests are trying to verify.\nOne way to improve these tests is to use a wrapper object to encapsu-\nlate the database’s interface instead of passing a DatabaseConnection \nobject to functions as an argument. It’s often worth refactoring your \ncode (see Item 89: “Consider warnings to Refactor and Migrate Usage” \nfor one approach) to use better abstractions because it facilitates cre-\nating mocks and writing tests. Here, I redefine the various database \nhelper functions from the previous item as methods on a class instead \nof as independent functions:\nclass ZooDatabase:\n    ...\n \n    def get_animals(self, species):\n        ...\n \n    def get_food_period(self, species):\n        ...\n \n    def feed_animal(self, name, when):\n        ...\nNow, I can redefine the do_rounds function to call methods on a \nZooDatabase object:\nfrom datetime import datetime\n \ndef do_rounds(database, species, *, utcnow=datetime.utcnow):\n    now = utcnow()\n    feeding_timedelta = database.get_food_period(species)\n    animals = database.get_animals(species)\n    fed = 0\n \n    for name, last_mealtime in animals:\n        if (now - last_mealtime) >= feeding_timedelta:\n            database.feed_animal(name, now)\n            fed += 1\n \n    return fed\nWriting a test for do_rounds is now a lot easier because I no longer \nneed to use unittest.mock.patch to inject the mock into the code \nbeing tested. Instead, I can create a Mock instance to represent \n",
      "content_length": 1706,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 400,
      "content": " Item 79: Encapsulate Dependencies to Facilitate Mocking and Testing \n377\na ZooDatabase and pass that in as the database parameter. The Mock \nclass returns a mock object for any attribute name that is accessed. \nThose attributes can be called like methods, which I can then use to \nset expectations and verify calls. This makes it easy to mock out all of \nthe methods of a class:\nfrom unittest.mock import Mock\n \ndatabase = Mock(spec=ZooDatabase)\nprint(database.feed_animal)\ndatabase.feed_animal()\ndatabase.feed_animal.assert_any_call()\n>>>\n<Mock name='mock.feed_animal' id='4384773408'>\nI can rewrite the Mock setup code by using the ZooDatabase \nencapsulation:\nfrom datetime import timedelta\nfrom unittest.mock import call\n \nnow_func = Mock(spec=datetime.utcnow)\nnow_func.return_value = datetime(2019, 6, 5, 15, 45)\n \ndatabase = Mock(spec=ZooDatabase)\ndatabase.get_food_period.return_value = timedelta(hours=3)\ndatabase.get_animals.return_value = [\n    ('Spot', datetime(2019, 6, 5, 11, 15)),\n    ('Fluffy', datetime(2019, 6, 5, 12, 30)),\n    ('Jojo', datetime(2019, 6, 5, 12, 55))\n]\nThen I can run the function being tested and verify that all depen-\ndent methods were called as expected:\nresult = do_rounds(database, 'Meerkat', utcnow=now_func)\nassert result == 2\n \ndatabase.get_food_period.assert_called_once_with('Meerkat')\ndatabase.get_animals.assert_called_once_with('Meerkat')\ndatabase.feed_animal.assert_has_calls(\n    [\n        call('Spot', now_func.return_value),\n        call('Fluffy', now_func.return_value),\n    ],\n    any_order=True)\n",
      "content_length": 1550,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 401,
      "content": "378 \nChapter 9 Testing and Debugging\nUsing the spec parameter to Mock is especially useful when mocking \nclasses because it ensures that the code under test doesn’t call a mis-\nspelled method name by accident. This allows you to avoid a common \npitfall where the same bug is present in both the code and the unit \ntest, masking a real error that will later reveal itself in production:\ndatabase.bad_method_name()\n>>>\nTraceback ...\nAttributeError: Mock object has no attribute 'bad_method_name'\nIf I want to test this program end-to-end with a mid-level integration \ntest (see Item 77: “Isolate Tests from Each Other with setUp, tearDown, \nsetUpModule, and tearDownModule”), I still need a way to inject a mock \nZooDatabase into the program. I can do this by creating a helper func-\ntion that acts as a seam for dependency injection. Here, I define such \na helper function that caches a ZooDatabase in module scope (see Item \n86: “Consider Module-Scoped Code to Configure Deployment Envi-\nronments”) by using a global statement:\nDATABASE = None\n \ndef get_database():\n    global DATABASE\n    if DATABASE is None:\n        DATABASE = ZooDatabase()\n    return DATABASE\n \ndef main(argv):\n    database = get_database()\n    species = argv[1]\n    count = do_rounds(database, species)\n    print(f'Fed {count} {species}(s)')\n    return 0\nNow, I can inject the mock ZooDatabase using patch, run the test, and \nverify the program’s output. I’m not using a mock datetime.utcnow \nhere; instead, I’m relying on the database records returned by the \nmock to be relative to the current time in order to produce similar \nbehavior to the unit test. This approach is more flaky than mocking \neverything, but it also tests more surface area:\nimport contextlib\nimport io\nfrom unittest.mock import patch\n \n",
      "content_length": 1782,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 402,
      "content": " \nItem 80: Consider Interactive Debugging with pdb \n379\nwith patch('__main__.DATABASE', spec=ZooDatabase):\n    now = datetime.utcnow()\n \n    DATABASE.get_food_period.return_value = timedelta(hours=3)\n    DATABASE.get_animals.return_value = [\n        ('Spot', now - timedelta(minutes=4.5)),\n        ('Fluffy', now - timedelta(hours=3.25)),\n        ('Jojo', now - timedelta(hours=3)),\n    ]\n \n    fake_stdout = io.StringIO()\n    with contextlib.redirect_stdout(fake_stdout):\n        main(['program name', 'Meerkat'])\n \n    found = fake_stdout.getvalue()\n    expected = 'Fed 2 Meerkat(s)\\n'\n \n    assert found == expected\nThe results match my expectations. Creating this integration test was \nstraightforward because I designed the implementation to make it \neasier to test.\nThings to Remember\n✦ When unit tests require a lot of repeated boilerplate to set up mocks, \none solution may be to encapsulate the functionality of dependen-\ncies into classes that are more easily mocked.\n✦ The Mock class of the unittest.mock built-in module simulates \nclasses by returning a new mock, which can act as a mock method, \nfor each attribute that is accessed.\n✦ For end-to-end tests, it’s valuable to refactor your code to have more \nhelper functions that can act as explicit seams to use for injecting \nmock dependencies in tests.\nItem 80: Consider Interactive Debugging with pdb\nEveryone encounters bugs in code while developing programs. Using \nthe print function can help you track down the sources of many \nissues (see Item 75: “Use repr Strings for Debugging Output”). \n Writing tests for specific cases that cause trouble is another great way \nto isolate problems (see Item 76: “Verify Related Behaviors in TestCase \nSubclasses”).\n",
      "content_length": 1724,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 403,
      "content": "380 \nChapter 9 Testing and Debugging\nBut these tools aren’t enough to find every root cause. When you need \nsomething more powerful, it’s time to try Python’s built-in interactive \ndebugger. The debugger lets you inspect program state, print local \nvariables, and step through a Python program one statement at a time.\nIn most other programming languages, you use a debugger by spec-\nifying what line of a source file you’d like to stop on, and then exe-\ncute the program. In contrast, with Python, the easiest way to use \nthe debugger is by modifying your program to directly initiate the \ndebugger just before you think you’ll have an issue worth investigat-\ning. This means that there is no difference between starting a Python \nprogram in order to run the debugger and starting it normally.\nTo initiate the debugger, all you have to do is call the breakpoint \nbuilt-in function. This is equivalent to importing the pdb built-in mod-\nule and running its set_trace function:\n# always_breakpoint.py\nimport math\n \ndef compute_rmse(observed, ideal):\n    total_err_2 = 0\n    count = 0\n    for got, wanted in zip(observed, ideal):\n        err_2 = (got - wanted) ** 2\n        breakpoint()  # Start the debugger here\n        total_err_2 += err_2\n        count += 1\n \n    mean_err = total_err_2 / count\n    rmse = math.sqrt(mean_err)\n    return rmse\n \nresult = compute_rmse(\n    [1.8, 1.7, 3.2, 6],\n    [2, 1.5, 3, 5])\nprint(result)\nAs soon as the breakpoint function runs, the program pauses its exe-\ncution before the line of code immediately following the breakpoint \ncall. The terminal that started the program turns into an interactive \nPython shell:\n$ python3 always_breakpoint.py \n> always_breakpoint.py(12)compute_rmse()\n-> total_err_2 += err_2\n(Pdb)\n",
      "content_length": 1753,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 404,
      "content": "At the (Pdb) prompt, you can type in the names of local variables to \nsee their values printed out (or use p <name>). You can see a list of all \nlocal variables by calling the locals built-in function. You can import \nmodules, inspect global state, construct new objects, run the help \nbuilt-in function, and even modify parts of the running program—\nwhatever you need to do to aid in your debugging. \nIn addition, the debugger has a variety of special commands to con-\ntrol and understand program execution; type help to see the full list.\nThree very useful commands make inspecting the running program \neasier:\n \n■where: Print the current execution call stack. This lets you figure \nout where you are in your program and how you arrived at the \nbreakpoint trigger.\n \n■up: Move your scope up the execution call stack to the caller of the \ncurrent function. This allows you to inspect the local variables in \nhigher levels of the program that led to the breakpoint.\n \n■down: Move your scope back down the execution call stack one \nlevel.\nWhen you’re done inspecting the current state, you can use these five \ndebugger commands to control the program’s execution in different \nways:\n \n■step: Run the program until the next line of execution in the pro-\ngram, and then return control back to the debugger prompt. If the \nnext line of execution includes calling a function, the debugger \nstops within the function that was called.\n \n■next: Run the program until the next line of execution in the \ncurrent function, and then return control back to the debugger \nprompt. If the next line of execution includes calling a function, \nthe debugger will not stop until the called function has returned.\n \n■return: Run the program until the current function returns, and \nthen return control back to the debugger prompt.\n \n■continue: Continue running the program until the next break-\npoint is hit (either through the breakpoint call or one added by a \ndebugger command).\n \n■quit: Exit the debugger and end the program. Run this command \nif you’ve found the problem, gone too far, or need to make program \nmodifications and try again.\n \nItem 80: Consider Interactive Debugging with pdb \n381\n",
      "content_length": 2180,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 405,
      "content": "382 \nChapter 9 Testing and Debugging\nThe breakpoint function can be called anywhere in a program. If you \nknow that the problem you’re trying to debug happens only under \nspecial circumstances, then you can just write plain old Python \ncode to call breakpoint after a specific condition is met. For example, \nhere I start the debugger only if the squared error for a datapoint is \nmore than 1:\n# conditional_breakpoint.py\ndef compute_rmse(observed, ideal):\n    ...\n    for got, wanted in zip(observed, ideal):\n        err_2 = (got - wanted) ** 2\n        if err_2 >= 1:  # Start the debugger if True\n            breakpoint()\n        total_err_2 += err_2\n        count += 1\n    ...\nresult = compute_rmse(\n    [1.8, 1.7, 3.2, 7],\n    [2, 1.5, 3, 5])\nprint(result)\nWhen I run the program and it enters the debugger, I can confirm \nthat the condition was true by inspecting local variables:\n$ python3 conditional_breakpoint.py \n> conditional_breakpoint.py(14)compute_rmse()\n-> total_err_2 += err_2\n(Pdb) wanted\n5\n(Pdb) got\n7\n(Pdb) err_2\n4\nAnother useful way to reach the debugger prompt is by using post- \nmortem debugging. This enables you to debug a program after it’s \nalready raised an exception and crashed. This is especially helpful \nwhen you’re not quite sure where to put the breakpoint function call.\nHere, I have a script that will crash due to the 7j complex number \nbeing present in one of the function’s arguments:\n# postmortem_breakpoint.py\nimport math\n \ndef compute_rmse(observed, ideal):\n    ...\n \n",
      "content_length": 1510,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 406,
      "content": "result = compute_rmse(\n    [1.8, 1.7, 3.2, 7j],  # Bad input\n    [2, 1.5, 3, 5])\nprint(result)\nI use the command line python3 -m pdb -c continue <program path> \nto run the program under control of the pdb module. The continue \ncommand tells pdb to get the program started immediately. Once it’s \nrunning, the program hits a problem and automatically enters the \ninteractive debugger, at which point I can inspect the program state:\n$ python3 -m pdb -c continue postmortem_breakpoint.py \nTraceback (most recent call last):\n  File \".../pdb.py\", line 1697, in main\n    pdb._runscript(mainpyfile)\n  File \".../pdb.py\", line 1566, in _runscript\n    self.run(statement)\n  File \".../bdb.py\", line 585, in run\n    exec(cmd, globals, locals)\n  File \"<string>\", line 1, in <module>\n  File \"postmortem_breakpoint.py\", line 4, in <module>\n    import math\n  File \"postmortem_breakpoint.py\", line 16, in compute_rmse\n    rmse = math.sqrt(mean_err)\nTypeError: can't convert complex to float\nUncaught exception. Entering post mortem debugging\nRunning 'cont' or 'step' will restart the program\n> postmortem_breakpoint.py(16)compute_rmse()\n-> rmse = math.sqrt(mean_err)\n(Pdb) mean_err\n(-5.97-17.5j)\nYou can also use post-mortem debugging after hitting an uncaught \nexception in the interactive Python interpreter by calling the pm \nfunction of the pdb module (which is often done in a single line as \nimport pdb; pdb.pm()):\n$ python3\n>>> import my_module\n>>> my_module.compute_stddev([5])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"my_module.py\", line 17, in compute_stddev\n    variance = compute_variance(data)\n  File \"my_module.py\", line 13, in compute_variance\n    variance = err_2_sum / (len(data) - 1)\n \nItem 80: Consider Interactive Debugging with pdb \n383\n",
      "content_length": 1782,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 407,
      "content": "384 \nChapter 9 Testing and Debugging\nZeroDivisionError: float division by zero\n>>> import pdb; pdb.pm()\n> my_module.py(13)compute_variance()\n-> variance = err_2_sum / (len(data) - 1)\n(Pdb) err_2_sum\n0.0\n(Pdb) len(data)\n1\nThings to Remember\n✦ You can initiate the Python interactive debugger at a point of inter-\nest directly in your program by calling the breakpoint built-in \nfunction.\n✦ The Python debugger prompt is a full Python shell that lets you \ninspect and modify the state of a running program.\n✦ pdb shell commands let you precisely control program execution \nand allow you to alternate between inspecting program state and \nprogressing program execution.\n✦ The pdb module can be used for debug exceptions after they \nhappen in independent Python programs (using python -m pdb -c \ncontinue <program path>) or the interactive Python interpreter (using \nimport pdb; pdb.pm()).\nItem 81:  Use tracemalloc to Understand Memory \nUsage and Leaks\nMemory management in the default implementation of Python, \n CPython, uses reference counting. This ensures that as soon as all \nreferences to an object have expired, the referenced object is also \ncleared from memory, freeing up that space for other data. CPython \nalso has a built-in cycle detector to ensure that self-referencing objects \nare eventually garbage collected.\nIn theory, this means that most Python programmers don’t have to \nworry about allocating or deallocating memory in their programs. It’s \ntaken care of automatically by the language and the CPython run-\ntime. However, in practice, programs eventually do run out of mem-\nory due to no longer useful references still being held. Figuring out \nwhere a Python program is using or leaking memory proves to be a \nchallenge.\nThe first way to debug memory usage is to ask the gc built-in module \nto list every object currently known by the garbage collector. Although \n",
      "content_length": 1886,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 408,
      "content": " \nItem 81: Use tracemalloc to Understand Memory Usage and Leaks \n385\nit’s quite a blunt tool, this approach lets you quickly get a sense of \nwhere your program’s memory is being used.\nHere, I define a module that fills up memory by keeping references:\n# waste_memory.py\nimport os\n \nclass MyObject:\n    def __init__(self):\n        self.data = os.urandom(100)\n \ndef get_data():\n    values = []\n    for _ in range(100):\n        obj = MyObject()\n        values.append(obj)\n    return values\n \ndef run():\n    deep_values = []\n    for _ in range(100):\n        deep_values.append(get_data())\n    return deep_values\nThen, I run a program that uses the gc built-in module to print out \nhow many objects were created during execution, along with a small \nsample of allocated objects:\n# using_gc.py\nimport gc\n \nfound_objects = gc.get_objects()\nprint('Before:', len(found_objects))\n \nimport waste_memory\n \nhold_reference = waste_memory.run()\n \nfound_objects = gc.get_objects()\nprint('After: ', len(found_objects))\nfor obj in found_objects[:3]:\n    print(repr(obj)[:100])\n>>>\nBefore: 6207\nAfter:  16801\n",
      "content_length": 1090,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 409,
      "content": "386 \nChapter 9 Testing and Debugging\n<waste_memory.MyObject object at 0x10390aeb8>\n<waste_memory.MyObject object at 0x10390aef0>\n<waste_memory.MyObject object at 0x10390af28>\n...\nThe problem with gc.get_objects is that it doesn’t tell you anything \nabout how the objects were allocated. In complicated programs, \nobjects of a specific class could be allocated many different ways. \nKnowing the overall number of objects isn’t nearly as important as \nidentifying the code responsible for allocating the objects that are \nleaking memory.\nPython 3.4 introduced a new tracemalloc built-in module for solving \nthis problem. tracemalloc makes it possible to connect an object back \nto where it was allocated. You use it by taking before and after snap-\nshots of memory usage and comparing them to see what’s changed. \nHere, I use this approach to print out the top three memory usage \noffenders in a program:\n# top_n.py\nimport tracemalloc\n \ntracemalloc.start(10)                      # Set stack depth\ntime1 = tracemalloc.take_snapshot()        # Before snapshot\n \nimport waste_memory\n \nx = waste_memory.run()                     # Usage to debug\ntime2 = tracemalloc.take_snapshot()        # After snapshot\n \nstats = time2.compare_to(time1, 'lineno')  # Compare snapshots\nfor stat in stats[:3]:\n    print(stat)\n>>>\nwaste_memory.py:5: size=2392 KiB (+2392 KiB), count=29994 \n¯(+29994), average=82 B\nwaste_memory.py:10: size=547 KiB (+547 KiB), count=10001 \n¯(+10001), average=56 B\nwaste_memory.py:11: size=82.8 KiB (+82.8 KiB), count=100 \n¯(+100), average=848 B\nThe size and count labels in the output make it immediately clear \nwhich objects are dominating my program’s memory usage and where \nin the source code they were allocated.\n",
      "content_length": 1728,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 410,
      "content": "The tracemalloc module can also print out the full stack trace of each \nallocation (up to the number of frames passed to the tracemalloc.start \nfunction). Here, I print out the stack trace of the biggest source of \nmemory usage in the program:\n# with_trace.py\nimport tracemalloc\n \ntracemalloc.start(10)\ntime1 = tracemalloc.take_snapshot()\n \nimport waste_memory\n \nx = waste_memory.run()\ntime2 = tracemalloc.take_snapshot()\n \nstats = time2.compare_to(time1, 'traceback')\ntop = stats[0]\nprint('Biggest offender is:')\nprint('\\n'.join(top.traceback.format()))\n>>>\nBiggest offender is:\n  File \"with_trace.py\", line 9\n    x = waste_memory.run()\n  File \"waste_memory.py\", line 17\n    deep_values.append(get_data())\n  File \"waste_memory.py\", line 10\n    obj = MyObject()\n  File \"waste_memory.py\", line 5\n    self.data = os.urandom(100)\nA stack trace like this is most valuable for figuring out which partic-\nular usage of a common function or class is responsible for memory \nconsumption in a program.\nThings to Remember\n✦ It can be difficult to understand how Python programs use and leak \nmemory.\n✦ The gc module can help you understand which objects exist, but it \nhas no information about how they were allocated.\n✦ The tracemalloc built-in module provides powerful tools for under-\nstanding the sources of memory usage.\n \nItem 81: Use tracemalloc to Understand Memory Usage and Leaks \n387\n",
      "content_length": 1385,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 411,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 412,
      "content": "10\nCollaboration\nPython has language features that help you construct well-defined \nAPIs with clear interface boundaries. The Python community has \nestablished best practices to maximize the maintainability of code \nover time. In addition, some standard tools that ship with Python \nenable large teams to work together across disparate environments.\nCollaborating with others on Python programs requires being \n deliberate in how you write your code. Even if you’re working on your \nown, chances are you’ll be using code written by someone else via the \nstandard library or open source packages. It’s important to under-\nstand the mechanisms that make it easy to collaborate with other \nPython programmers.\nItem 82:  Know Where to Find Community-Built \nModules\nPython has a central repository of modules (https://pypi.org) that you \ncan install and use in your programs. These modules are built and \nmaintained by people like you: the Python community. When you find \nyourself facing an unfamiliar challenge, the Python Package Index \n(PyPI) is a great place to look for code that will get you closer to your \ngoal.\nTo use the Package Index, you need to use the command-line tool pip \n(a recursive acronym for “pip installs packages”). pip can be run with \npython3 -m pip to ensure that packages are installed for the correct \nversion of Python on your system (see Item 1: “Know Which Version of \nPython You’re Using”). Using pip to install a new module is simple. For \nexample, here I install the pytz module that I use elsewhere in this \nbook (see Item 67: “Use datetime Instead of time for Local Clocks”):\n$ python3 -m pip install pytz\nCollecting pytz\n",
      "content_length": 1655,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 413,
      "content": "390 \nChapter 10 Collaboration\n  Downloading ... \nInstalling collected packages: pytz\nSuccessfully installed pytz-2018.9\npip is best used together with the built-in module venv to consistently \ntrack sets of packages to install for your projects (see Item 83: “Use \nVirtual Environments for Isolated and Reproducible Dependencies”). \nYou can also create your own PyPI packages to share with the Python \ncommunity or host your own private package repositories for use \nwith pip.\nEach module in the PyPI has its own software license. Most of the \npackages, especially the popular ones, have free or open source \nlicenses (see https://opensource.org for details). In most cases, these \nlicenses allow you to include a copy of the module with your program; \nwhen in doubt, talk to a lawyer.\nThings to Remember\n✦ The Python Package Index (PyPI) contains a wealth of common \npackages that are built and maintained by the Python community.\n✦ pip is the command-line tool you can use to install packages \nfrom PyPI.\n✦ The majority of PyPI modules are free and open source software.\nItem 83:  Use Virtual Environments for Isolated and \nReproducible Dependencies\nBuilding larger and more complex programs often leads you to rely on \nvarious packages from the Python community (see Item 82: “Know \nWhere to Find Community-Built Modules”). You’ll find yourself run-\nning the python3 -m pip command-line tool to install packages like \npytz, numpy, and many others.\nThe problem is that, by default, pip installs new packages in a global \nlocation. That causes all Python programs on your system to be \naffected by these installed modules. In theory, this shouldn’t be an \nissue. If you install a package and never import it, how could it affect \nyour programs?\nThe trouble comes from transitive dependencies: the packages \nthat the packages you install depend on. For example, you can see \nwhat the Sphinx package depends on after installing it by asking pip:\n$ python3 -m pip show Sphinx\nName: Sphinx\n",
      "content_length": 1987,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 414,
      "content": " \nItem 83: Use Virtual Environments for Isolated Dependencies \n391\nVersion: 2.1.2\nSummary: Python documentation generator\nLocation: /usr/local/lib/python3.8/site-packages\nRequires: alabaster, imagesize, requests, \n¯sphinxcontrib-applehelp, sphinxcontrib-qthelp, \n¯Jinja2, setuptools, sphinxcontrib-jsmath, \n¯sphinxcontrib-serializinghtml, Pygments, snowballstemmer, \n¯packaging, sphinxcontrib-devhelp, sphinxcontrib-htmlhelp, \n¯babel, docutils\nRequired-by:\nIf you install another package like flask, you can see that it, too, \ndepends on the Jinja2 package:\n$ python3 -m pip show flask\nName: Flask\nVersion: 1.0.3\nSummary: A simple framework for building complex web applications.\nLocation: /usr/local/lib/python3.8/site-packages\nRequires: itsdangerous, click, Jinja2, Werkzeug\nRequired-by:\nA dependency conflict can arise as Sphinx and flask diverge over \ntime. Perhaps right now they both require the same version of \nJinja2, and everything is fine. But six months or a year from now, \nJinja2 may release a new version that makes breaking changes \nto users of the library. If you update your global version of Jinja2 \nwith python3 -m pip install --upgrade Jinja2, you may find that Sphinx \nbreaks, while flask keeps working.\nThe cause of such breakage is that Python can have only a single \nglobal version of a module installed at a time. If one of your installed \npackages must use the new version and another package must use \nthe old version, your system isn’t going to work properly; this situa-\ntion is often called dependency hell.\nSuch breakage can even happen when package maintainers try their \nbest to preserve API compatibility between releases (see Item 85: \n“Use Packages to Organize Modules and Provide Stable APIs”). New \n versions of a library can subtly change behaviors that API-consuming \ncode relies on. Users on a system may upgrade one package to a new \nversion but not others, which could break dependencies. If you’re not \ncareful there’s a constant risk of the ground moving beneath your feet.\nThese difficulties are magnified when you collaborate with other \ndevelopers who do their work on separate computers. It’s best to \nassume the worst: that the versions of Python and global packages \n",
      "content_length": 2219,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 415,
      "content": "392 \nChapter 10 Collaboration\nthat they have installed on their machines will be slightly different \nfrom yours. This can cause frustrating situations such as a codebase \nworking perfectly on one programmer’s machine and being completely \nbroken on another’s.\nThe solution to all of these problems is using a tool called venv, which \nprovides virtual environments. Since Python 3.4, pip and the venv \nmodule have been available by default along with the Python installa-\ntion (accessible with python -m venv).\nvenv allows you to create isolated versions of the Python environment. \nUsing venv, you can have many different versions of the same package \ninstalled on the same system at the same time without conflicts. This \nmeans you can work on many different projects and use many differ-\nent tools on the same computer. venv does this by installing explicit \nversions of packages and their dependencies into completely separate \ndirectory structures. This makes it possible to reproduce a Python \nenvironment that you know will work with your code. It’s a reliable \nway to avoid surprising breakages.\nUsing venv on the Command Line\nHere’s a quick tutorial on how to use venv effectively. Before using \nthe tool, it’s important to note the meaning of the python3 command \nline on your system. On my computer, python3 is located in the \n/usr/local/bin directory and evaluates to version 3.8.0 (see Item 1: \n“Know Which Version of Python You’re Using”):\n$ which python3\n/usr/local/bin/python3\n$ python3 --version\nPython 3.8.0\nTo demonstrate the setup of my environment, I can test that running \na command to import the pytz module doesn’t cause an error. This \nworks because I already have the pytz package installed as a global \nmodule:\n$ python3 -c 'import pytz'\n$\nNow, I use venv to create a new virtual environment called myproject. \nEach virtual environment must live in its own unique directory. The \nresult of the command is a tree of directories and files that are used \nto manage the virtual environment:\n$ python3 -m venv myproject\n$ cd myproject\n",
      "content_length": 2056,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 416,
      "content": "$ ls\nbin     include     lib     pyvenv.cfg\nTo start using the virtual environment, I use the source command \nfrom my shell on the bin/activate script. activate modifies all of \nmy environment variables to match the virtual environment. It also \nupdates my command-line prompt to include the virtual environment \nname (“myproject”) to make it extremely clear what I’m working on:\n$ source bin/activate\n(myproject)$\nOn Windows the same script is available as:\nC:\\> myproject\\Scripts\\activate.bat\n(myproject) C:>\nOr with PowerShell as:\nPS C:\\> myproject\\Scripts\\activate.ps1\n(myproject) PS C:>\nAfter activation, the path to the python3  command-line tool has moved \nto within the virtual environment directory:\n(myproject)$ which python3\n/tmp/myproject/bin/python3\n(myproject)$ ls -l /tmp/myproject/bin/python3\n... -> /usr/local/bin/python3.8\nThis ensures that changes to the outside system will not affect the \nvirtual environment. Even if the outer system upgrades its default \npython3 to version 3.9, my virtual environment will still explicitly \npoint to version 3.8.\nThe virtual environment I created with venv starts with no packages \ninstalled except for pip and setuptools. Trying to use the pytz pack-\nage that was installed as a global module in the outside system will \nfail because it’s unknown to the virtual environment:\n(myproject)$ python3 -c 'import pytz'\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nModuleNotFoundError: No module named 'pytz'\nI can use the pip command-line tool to install the pytz module into \nmy virtual environment:\n(myproject)$ python3 -m pip install pytz\nCollecting pytz\n \nItem 83: Use Virtual Environments for Isolated Dependencies \n393\n",
      "content_length": 1706,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 417,
      "content": "394 \nChapter 10 Collaboration\n  Downloading ... \nInstalling collected packages: pytz\nSuccessfully installed pytz-2019.1\nOnce it’s installed, I can verify that it’s working by using the same \ntest import command:\n(myproject)$ python3 -c 'import pytz'\n(myproject)$\nWhen I’m done with a virtual environment and want to go back to my \ndefault system, I use the deactivate command. This restores my envi-\nronment to the system defaults, including the location of the python3 \ncommand-line tool:\n(myproject)$ which python3\n/tmp/myproject/bin/python3\n(myproject)$ deactivate\n$ which python3\n/usr/local/bin/python3\nIf I ever want to work in the myproject environment again, I can just \nrun source bin/activate in the directory as before.\nReproducing Dependencies\nOnce you are in a virtual environment, you can continue installing \npackages in it with pip as you need them. Eventually, you might want \nto copy your environment somewhere else. For example, say that I \nwant to reproduce the development environment from my workstation \non a server in a datacenter. Or maybe I want to clone someone else’s \nenvironment on my own machine so I can help debug their code.\nvenv makes such tasks easy. I can use the python3 -m pip freeze \ncommand to save all of my explicit package dependencies into a file \n(which, by convention, is named requirements.txt):\n(myproject)$ python3 -m pip freeze > requirements.txt\n(myproject)$ cat requirements.txt\ncertifi==2019.3.9\nchardet==3.0.4\nidna==2.8\nnumpy==1.16.2\npytz==2018.9\nrequests==2.21.0\nurllib3==1.24.1\n",
      "content_length": 1534,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 418,
      "content": "Now, imagine that I’d like to have another virtual environment that \nmatches the myproject environment. I can create a new directory as \nbefore by using venv and activate it:\n$ python3 -m venv otherproject\n$ cd otherproject\n$ source bin/activate\n(otherproject)$\nThe new environment will have no extra packages installed:\n(otherproject)$ python3 -m pip list\nPackage    Version\n---------- -------\npip        10.0.1 \nsetuptools 39.0.1\nI can install all of the packages from the first environment by  running \npython3 -m pip install on the requirements.txt that I generated with \nthe python3 -m pip freeze command:\n(otherproject)$ python3 -m pip install -r /tmp/myproject/\n¯requirements.txt\nThis command cranks along for a little while as it retrieves and \ninstalls all of the packages required to reproduce the first environ-\nment. When it’s done, I can list the set of installed packages in the \nsecond virtual environment and should see the same list of depen-\ndencies found in the first virtual environment:\n(otherproject)$ python3 -m pip list\nPackage    Version \n---------- --------\ncertifi    2019.3.9\nchardet    3.0.4   \nidna       2.8     \nnumpy      1.16.2  \npip        10.0.1  \npytz       2018.9  \nrequests   2.21.0  \nsetuptools 39.0.1  \nurllib3    1.24.1\nUsing a requirements.txt file is ideal for collaborating with others \nthrough a revision control system. You can commit changes to your \ncode at the same time you update your list of package dependencies, \nensuring that they move in lockstep. However, it’s important to note \nthat the specific version of Python you’re using is not included in the \nrequirements.txt file, so that must be managed separately.\n \nItem 83: Use Virtual Environments for Isolated Dependencies \n395\n",
      "content_length": 1737,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 419,
      "content": "396 \nChapter 10 Collaboration\nThe gotcha with virtual environments is that moving them breaks \neverything because all of the paths, like the python3 command-line \ntool, are hard-coded to the environment’s install directory. But ulti-\nmately this limitation doesn’t matter. The whole purpose of virtual \nenvironments is to make it easy to reproduce a setup. Instead of mov-\ning a virtual environment directory, just use python3 -m pip freeze \non the old one, create a new virtual environment somewhere else, and \nreinstall everything from the requirements.txt file.\nThings to Remember\n✦ Virtual environments allow you to use pip to install many differ-\nent versions of the same package on the same machine without \nconflicts.\n✦ Virtual environments are created with python -m venv, enabled with \nsource bin/activate, and disabled with deactivate.\n✦ You can dump all of the requirements of an environment with \npython3 -m pip freeze. You can reproduce an environment by  running \npython3 -m pip install -r requirements.txt.\nItem 84:  Write Docstrings for Every Function, \nClass, and Module\nDocumentation in Python is extremely important because of the \ndynamic nature of the language. Python provides built-in support \nfor attaching documentation to blocks of code. Unlike with many \nother languages, the documentation from a program’s source code is \ndirectly accessible as the program runs.\nFor example, you can add documentation by providing a docstring \nimmediately after the def statement of a function:\ndef palindrome(word):\n    \"\"\"Return True if the given word is a palindrome.\"\"\"\n    return word == word[::-1]\n \nassert palindrome('tacocat')\nassert not palindrome('banana')\nYou can retrieve the docstring from within the Python program by \naccessing the function’s __doc__ special attribute:\nprint(repr(palindrome.__doc__))\n>>>\n'Return True if the given word is a palindrome.'\n",
      "content_length": 1882,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 420,
      "content": " \nItem 84: Write Docstrings for Every Function, Class, and Module \n397\nYou can also use the built-in pydoc module from the command line \nto run a local web server that hosts all of the Python documentation \nthat’s accessible to your interpreter, including modules that you’ve \nwritten:\n$ python3 -m pydoc -p 1234\nServer ready at http://localhost:1234/\nServer commands: [b]rowser, [q]uit\nserver> b\nDocstrings can be attached to functions, classes, and modules. This \nconnection is part of the process of compiling and running a Python \nprogram. Support for docstrings and the __doc__ attribute has three \nconsequences:\n \n■The accessibility of documentation makes interactive develop-\nment easier. You can inspect functions, classes, and modules to \nsee their documentation by using the help built-in function. This \nmakes the Python interactive interpreter (the Python “shell”) and \ntools like IPython Notebook (https://ipython.org) a joy to use \nwhile you’re developing algorithms, testing APIs, and writing code \nsnippets.\n \n■A standard way of defining documentation makes it easy to build \ntools that convert the text into more appealing formats (like \nHTML). This has led to excellent documentation-generation tools \nfor the Python community, such as Sphinx (https://www.sphinx-\ndoc.org). It has also enabled community-funded sites like Read \nthe Docs (https://readthedocs.org) that provide free hosting of \nbeautiful-looking documentation for open source Python projects.\n \n■Python’s first-class, accessible, and good-looking documentation \nencourages people to write more documentation. The members \nof the Python community have a strong belief in the importance \nof documentation. There’s an assumption that “good code” also \nmeans well-documented code. This means that you can expect \nmost open source Python libraries to have decent documentation.\nTo participate in this excellent culture of documentation, you need to \nfollow a few guidelines when you write docstrings. The full details are \ndiscussed online in PEP 257 (https://www.python.org/dev/peps/pep-\n0257/). There are a few best practices you should be sure to follow.\nDocumenting Modules\nEach module should have a top-level docstring—a string literal that is \nthe first statement in a source file. It should use three double quotes \n(\"\"\"). The goal of this docstring is to introduce the module and its \ncontents.\n",
      "content_length": 2381,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 421,
      "content": "398 \nChapter 10 Collaboration\nThe first line of the docstring should be a single sentence describing \nthe module’s purpose. The paragraphs that follow should contain the \ndetails that all users of the module should know about its operation. \nThe module docstring is also a jumping-off point where you can high-\nlight important classes and functions found in the module.\nHere’s an example of a module docstring:\n# words.py\n#!/usr/bin/env python3\n\"\"\"Library for finding linguistic patterns in words.\n \nTesting how words relate to each other can be tricky sometimes!\nThis module provides easy ways to determine when words you've\nfound have special properties.\n \nAvailable functions:\n- palindrome: Determine if a word is a palindrome.\n- check_anagram: Determine if two words are anagrams.\n...\n\"\"\"\n...\nIf the module is a command-line utility, the module docstring is also \na great place to put usage information for running the tool.\nDocumenting Classes\nEach class should have a class-level docstring. This largely follows \nthe same pattern as the module-level docstring. The first line is the \nsingle-sentence purpose of the class. Paragraphs that follow discuss \nimportant details of the class’s operation.\nImportant public attributes and methods of the class should be high-\nlighted in the class-level docstring. It should also provide guidance to \nsubclasses on how to properly interact with protected attributes (see \nItem 42: “Prefer Public Attributes Over Private Ones”) and the super-\nclass’s methods.\nHere’s an example of a class docstring:\nclass Player:\n    \"\"\"Represents a player of the game.\n \n    Subclasses may override the 'tick' method to provide\n    custom animations for the player's movement depending\n    on their power level, etc.\n \n",
      "content_length": 1749,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 422,
      "content": " \nItem 84: Write Docstrings for Every Function, Class, and Module \n399\n    Public attributes:\n    - power: Unused power-ups (float between 0 and 1).\n    - coins: Coins found during the level (integer).\n    \"\"\"\n    ...\nDocumenting Functions\nEach public function and method should have a docstring. This fol-\nlows the same pattern as the docstrings for modules and classes. The \nfirst line is a single-sentence description of what the function does. \nThe paragraphs that follow should describe any specific behaviors \nand the arguments for the function. Any return values should be \nmentioned. Any exceptions that callers must handle as part of the \nfunction’s interface should be explained (see Item 20: “Prefer Raising \nExceptions to Returning None” for how to document raised exceptions).\nHere’s an example of a function docstring:\ndef find_anagrams(word, dictionary):\n    \"\"\"Find all anagrams for a word.\n \n    This function only runs as fast as the test for\n    membership in the 'dictionary' container.\n \n    Args:\n        word: String of the target word.\n        dictionary: collections.abc.Container with all\n            strings that are known to be actual words.\n \n    Returns:\n        List of anagrams that were found. Empty if\n        none were found.\n    \"\"\"\n    ...\nThere are also some special cases in writing docstrings for functions \nthat are important to know:\n \n■If a function has no arguments and a simple return value, a \n single-sentence description is probably good enough.\n \n■If a function doesn’t return anything, it’s better to leave out any \nmention of the return value instead of saying “returns None.”\n \n■If a function’s interface includes raising exceptions (see Item 20: \n“Prefer Raising Exceptions to Returning None” for an example), its \ndocstring should describe each exception that’s raised and when \nit’s raised.\n",
      "content_length": 1846,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 423,
      "content": "400 \nChapter 10 Collaboration\n \n■If you don’t expect a function to raise an exception during normal \noperation, don’t mention that fact.\n \n■If a function accepts a variable number of arguments (see Item 22: \n“Reduce Visual Noise with Variable Positional Arguments”) or key-\nword arguments (see Item 23: “Provide Optional Behavior with \nKeyword Arguments”), use *args and **kwargs in the documented \nlist of arguments to describe their purpose.\n \n■If a function has arguments with default values, those defaults \nshould be mentioned (see Item 24: “Use None and Docstrings to \nSpecify Dynamic Default Arguments”).\n \n■If a function is a generator (see Item 30: “Consider Generators \nInstead of Returning Lists”), its docstring should describe what \nthe generator yields when it’s iterated.\n \n■If a function is an asynchronous coroutine (see Item 60: “Achieve \nHighly Concurrent I/O with Coroutines”), its docstring should \nexplain when it will stop execution.\nUsing Docstrings and Type Annotations\nPython now supports type annotations for a variety of purposes (see \nItem 90: “Consider Static Analysis via typing to Obviate Bugs” for how \nto use them). The information they contain may be redundant with \ntypical docstrings. For example, here is the function signature for \nfind_anagrams with type annotations applied:\nfrom typing import Container, List\n \ndef find_anagrams(word: str,\n                  dictionary: Container[str]) -> List[str]:\n    ...\nThere is no longer a need to specify in the docstring that the word \nargument is a string, since the type annotation has that infor-\nmation. The same goes for the dictionary argument being a \ncollections.abc.Container. There’s no reason to mention that the \nreturn type will be a list, since this fact is clearly annotated. And \nwhen no anagrams are found, the return value still must be a list, so \nit’s implied that it will be empty; that doesn’t need to be noted in the \ndocstring. Here, I write the same function signature from above along \nwith the docstring that has been shortened accordingly:\ndef find_anagrams(word: str,\n                  dictionary: Container[str]) -> List[str]:\n    \"\"\"Find all anagrams for a word.\n \n",
      "content_length": 2179,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 424,
      "content": " \nItem 85: Use Packages to Organize Modules and Provide Stable APIs \n401\n    This function only runs as fast as the test for\n    membership in the 'dictionary' container.\n \n    Args:\n        word: Target word.\n        dictionary: All known actual words.\n \n    Returns:\n        Anagrams that were found.\n    \"\"\"\n    ...\nThe redundancy between type annotations and docstrings should be \nsimilarly avoided for instance fields, class attributes, and methods. \nIt’s best to have type information in only one place so there’s less risk \nthat it will skew from the actual implementation.\nThings to Remember\n✦ Write documentation for every module, class, method, and function \nusing docstrings. Keep them up-to-date as your code changes.\n✦ For modules: Introduce the contents of a module and any important \nclasses or functions that all users should know about.\n✦ For classes: Document behavior, important attributes, and subclass \nbehavior in the docstring following the class statement.\n✦ For functions and methods: Document every argument, returned \nvalue, raised exception, and other behaviors in the docstring follow-\ning the def statement.\n✦ If you’re using type annotations, omit the information that’s already \npresent in type annotations from docstrings since it would be \nredundant to have it in both places.\nItem 85:  Use Packages to Organize Modules and \nProvide Stable APIs\nAs the size of a program’s codebase grows, it’s natural for you to reor-\nganize its structure. You’ll split larger functions into smaller func-\ntions. You’ll refactor data structures into helper classes (see Item 37: \n“Compose Classes Instead of Nesting Many Levels of Built-in Types” \nfor an example). You’ll separate functionality into various modules \nthat depend on each other.\nAt some point, you’ll find yourself with so many modules that you \nneed another layer in your program to make it understandable. For \n",
      "content_length": 1895,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 425,
      "content": "402 \nChapter 10 Collaboration\nthis purpose, Python provides packages. Packages are modules that \ncontain other modules.\nIn most cases, packages are defined by putting an empty file named \n__init__.py into a directory. Once __init__.py is present, any other \nPython files in that directory will be available for import, using a path \nrelative to the directory. For example, imagine that I have the follow-\ning directory structure in my program:\nmain.py\nmypackage/__init__.py\nmypackage/models.py\nmypackage/utils.py\nTo import the utils module, I use the absolute module name that \nincludes the package directory’s name:\n# main.py\nfrom mypackage import utils\nThis pattern continues when I have package directories present \nwithin other packages (like mypackage.foo.bar).\nThe functionality provided by packages has two primary purposes in \nPython programs.\nNamespaces\nThe first use of packages is to help divide your modules into separate \nnamespaces. They enable you to have many modules with the same \nfilename but different absolute paths that are unique. For example, \nhere’s a program that imports attributes from two modules with the \nsame filename, utils.py:\n# main.py\nfrom analysis.utils import log_base2_bucket\nfrom frontend.utils import stringify\n \nbucket = stringify(log_base2_bucket(33))\nThis approach breaks when the functions, classes, or submodules \ndefined in packages have the same names. For example, say that I \nwant to use the inspect function from both the analysis.utils and \nthe frontend.utils modules. Importing the attributes directly won’t \nwork because the second import statement will overwrite the value of \ninspect in the current scope:\n# main2.py\nfrom analysis.utils import inspect\nfrom frontend.utils import inspect  # Overwrites!\n",
      "content_length": 1758,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 426,
      "content": " \nItem 85: Use Packages to Organize Modules and Provide Stable APIs \n403\nThe solution is to use the as clause of the import statement to rename \nwhatever I’ve imported for the current scope:\n# main3.py\nfrom analysis.utils import inspect as analysis_inspect\nfrom frontend.utils import inspect as frontend_inspect\n \nvalue = 33\nif analysis_inspect(value) == frontend_inspect(value):\n    print('Inspection equal!')\nThe as clause can be used to rename anything retrieved with the \nimport statement, including entire modules. This facilitates accessing \nnamespaced code and makes its identity clear when you use it.\nAnother approach for avoiding imported name conflicts is to always \naccess names by their highest unique module name. For the exam-\nple above, this means I’d use basic import statements instead of \nimport from:\n# main4.py\nimport analysis.utils\nimport frontend.utils\n \nvalue = 33\nif (analysis.utils.inspect(value) ==\n    frontend.utils.inspect(value)):\n    print('Inspection equal!')\nThis approach allows you to avoid the as clause altogether. It also \nmakes it abundantly clear to new readers of the code where each of \nthe similarly named functions is defined.\nStable APIs\nThe second use of packages in Python is to provide strict, stable APIs \nfor external consumers.\nWhen you’re writing an API for wider consumption, such as an open \nsource package (see Item 82: “Know Where to Find Community-Built \nModules” for examples), you’ll want to provide stable functionality that \ndoesn’t change between releases. To ensure that happens, it’s import-\nant to hide your internal code organization from external users. This \nway, you can refactor and improve your package’s internal modules \nwithout breaking existing users.\nPython can limit the surface area exposed to API consumers by using \nthe __all__ special attribute of a module or package. The value of \n__all__ is a list of every name to export from the module as part of \nits public API. When consuming code executes from foo import *, \n",
      "content_length": 2000,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 427,
      "content": "404 \nChapter 10 Collaboration\nonly the attributes in foo.__all__ will be imported from foo. If __all__ \nisn’t present in foo, then only public attributes—those without a lead-\ning underscore—are imported (see Item 42: “Prefer Public Attributes \nOver Private Ones” for details about that convention).\nFor example, say that I want to provide a package for calculating col-\nlisions between moving projectiles. Here, I define the models module of \nmypackage to contain the representation of projectiles:\n# models.py\n__all__ = ['Projectile']\n \nclass Projectile:\n    def __init__(self, mass, velocity):\n        self.mass = mass\n        self.velocity = velocity\nI also define a utils module in mypackage to perform operations on the \nProjectile instances, such as simulating collisions between them:\n# utils.py\nfrom . models import Projectile\n \n__all__ = ['simulate_collision']\n \ndef _dot_product(a, b):\n    ...\n \ndef simulate_collision(a, b):\n    ...\nNow, I’d like to provide all of the public parts of this API as a set of \nattributes that are available on the mypackage module. This will allow \ndownstream consumers to always import directly from mypackage \ninstead of importing from mypackage.models or mypackage.utils. This \nensures that the API consumer’s code will continue to work even if the \ninternal organization of mypackage changes (e.g., models.py is deleted).\nTo do this with Python packages, you need to modify the __init__.py \nfile in the mypackage directory. This file is what actually becomes the \ncontents of the mypackage module when it’s imported. Thus, you can \nspecify an explicit API for mypackage by limiting what you import into \n__init__.py. Since all of my internal modules already specify __all__, \nI can expose the public interface of mypackage by simply import-\ning everything from the internal modules and updating __all__ \naccordingly:\n# __init__.py\n__all__ = []\nfrom . models import *\n",
      "content_length": 1913,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 428,
      "content": "__all__ += models.__all__\nfrom . utils import *\n__all__ += utils.__all__\nHere’s a consumer of the API that directly imports from mypackage \ninstead of accessing the inner modules:\n# api_consumer.py\nfrom mypackage import *\n \na = Projectile(1.5, 3)\nb = Projectile(4, 1.7)\nafter_a, after_b = simulate_collision(a, b)\nNotably, internal-only functions like mypackage.utils._dot_product \nwill not be available to the API consumer on mypackage because they \nweren’t present in __all__. Being omitted from __all__ also means \nthat they weren’t imported by the from mypackage import * state-\nment. The internal-only names are effectively hidden.\nThis whole approach works great when it’s important to provide an \nexplicit, stable API. However, if you’re building an API for use between \nyour own modules, the functionality of __all__ is probably unneces-\nsary and should be avoided. The namespacing provided by packages \nis usually enough for a team of programmers to collaborate on large \namounts of code they control while maintaining reasonable interface \nboundaries.\n \nItem 85: Use Packages to Organize Modules and Provide Stable APIs \n405\nBeware of import *\nImport statements like from x import y are clear because the \nsource of y is explicitly the x package or module. Wildcard imports \nlike from foo import * can also be useful, especially in interac-\ntive Python sessions. However, wildcards make code more diffi-\ncult to understand:\n \n■from foo import * hides the source of names from new read-\ners of the code. If a module has multiple import * statements, \nyou’ll need to check all of the referenced modules to figure out \nwhere a name was defined.\n \n■Names from import * statements will overwrite any conflicting \nnames within the containing module. This can lead to strange \nbugs caused by accidental interactions between your code and \noverlapping names from multiple import * statements.\nThe safest approach is to avoid import * in your code and explicitly \nimport names with the from x import y style.\n",
      "content_length": 2010,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 429,
      "content": "406 \nChapter 10 Collaboration\nThings to Remember\n✦ Packages in Python are modules that contain other modules. Pack-\nages allow you to organize your code into separate, non-conflicting \nnamespaces with unique absolute module names.\n✦ Simple packages are defined by adding an __init__.py file to a \ndirectory that contains other source files. These files become the \nchild modules of the directory’s package. Package directories may \nalso contain other packages.\n✦ You can provide an explicit API for a module by listing its publicly \nvisible names in its __all__ special attribute.\n✦ You can hide a package’s internal implementation by only import-\ning public names in the package’s __init__.py file or by naming \n internal-only members with a leading underscore.\n✦ When collaborating within a single team or on a single codebase, \nusing __all__ for explicit APIs is probably unnecessary.\nItem 86:  Consider Module-Scoped Code to Configure \nDeployment Environments\nA deployment environment is a configuration in which a program \nruns. Every program has at least one deployment environment: the \nproduction environment. The goal of writing a program in the first \nplace is to put it to work in the production environment and achieve \nsome kind of outcome.\nWriting or modifying a program requires being able to run it on the \ncomputer you use for developing. The configuration of your develop-\nment environment may be very different from that of your production \nenvironment. For example, you may be using a tiny single-board \ncomputer to develop a program that’s meant to run on enormous \nsupercomputers.\nTools like venv (see Item 83: “Use Virtual Environments for Isolated \nand Reproducible Dependencies”) make it easy to ensure that all envi-\nronments have the same Python packages installed. The trouble is \nthat production environments often require many external assump-\ntions that are hard to reproduce in development environments.\nFor example, say that I want to run a program in a web server con-\ntainer and give it access to a database. Every time I want to modify \nmy program’s code, I need to run a server container, the database \nschema must be set up properly, and my program needs the password \nfor access. This is a very high cost if all I’m trying to do is verify that \na one-line change to my program works correctly.\n",
      "content_length": 2333,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 430,
      "content": " \nItem 86: Consider Module-Scoped Code to Configure Enviornments \n407\nThe best way to work around such issues is to override parts of a pro-\ngram at startup time to provide different functionality depending on \nthe deployment environment. For example, I could have two different \n__main__ files—one for production and one for development:\n# dev_main.py\nTESTING = True\n \nimport db_connection\n \ndb = db_connection.Database()\n# prod_main.py\nTESTING = False\n \nimport db_connection\n \ndb = db_connection.Database()\nThe only difference between the two files is the value of the TESTING \nconstant. Other modules in my program can then import the __main__ \nmodule and use the value of TESTING to decide how they define their \nown attributes:\n# db_connection.py\nimport __main__\n \nclass TestingDatabase:\n    ...\n \nclass RealDatabase:\n    ...\n \nif __main__.TESTING:\n    Database = TestingDatabase\nelse:\n    Database = RealDatabase\nThe key behavior to notice here is that code running in module \nscope—not inside a function or method—is just normal Python code. \nYou can use an if statement at the module level to decide how the \nmodule will define names. This makes it easy to tailor modules to \nyour various deployment environments. You can avoid having to \nreproduce costly assumptions like database configurations when \nthey aren’t needed. You can inject local or fake implementations that \nease interactive development, or you can use mocks for writing tests \n(see Item 78: “Use Mocks to Test Code with Complex Dependencies”).\n",
      "content_length": 1519,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 431,
      "content": "408 \nChapter 10 Collaboration\nNote\nWhen your deployment environment configuration gets really complicated, \nyou should consider moving it out of Python constants (like TESTING) and into \ndedicated configuration files. Tools like the configparser built-in module let \nyou maintain production configurations separately from code, a distinction \nthat’s crucial for collaborating with an operations team.\nThis approach can be used for more than working around external \nassumptions. For example, if I know that my program must work dif-\nferently depending on its host platform, I can inspect the sys module \nbefore defining top-level constructs in a module:\n# db_connection.py\nimport sys\n \nclass Win32Database:\n    ...\n \nclass PosixDatabase:\n    ...\n \nif sys.platform.startswith('win32'):\n    Database = Win32Database\nelse:\n    Database = PosixDatabase\nSimilarly, I could use environment variables from os.environ to guide \nmy module definitions.\nThings to Remember\n✦ Programs often need to run in multiple deployment environments \nthat each have unique assumptions and configurations.\n✦ You can tailor a module’s contents to different deployment environ-\nments by using normal Python statements in module scope.\n✦ Module contents can be the product of any external condition, \nincluding host introspection through the sys and os modules.\nItem 87:  Define a Root Exception to Insulate Callers \nfrom APIs\nWhen you’re defining a module’s API, the exceptions you raise are \njust as much a part of your interface as the functions and classes you \ndefine (see Item 20: “Prefer Raising Exceptions to Returning None” for \nan example).\n",
      "content_length": 1624,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 432,
      "content": "Python has a built-in hierarchy of exceptions for the language and \nstandard library. There’s a draw to using the built-in exception types \nfor reporting errors instead of defining your own new types. For exam-\nple, I could raise a ValueError exception whenever an invalid parame-\nter is passed to a function in one of my modules:\n# my_module.py\ndef determine_weight(volume, density):\n    if density <= 0:\n        raise ValueError('Density must be positive')\n    ...\nIn some cases, using ValueError makes sense, but for APIs, it’s much \nmore powerful to define a new hierarchy of exceptions. I can do this \nby providing a root Exception in my module and having all other \nexceptions raised by that module inherit from the root exception:\n# my_module.py\nclass Error(Exception):\n    \"\"\"Base-class for all exceptions raised by this module.\"\"\"\n \nclass InvalidDensityError(Error):\n    \"\"\"There was a problem with a provided density value.\"\"\"\n \nclass InvalidVolumeError(Error):\n    \"\"\"There was a problem with the provided weight value.\"\"\"\n \ndef determine_weight(volume, density):\n    if density < 0:\n        raise InvalidDensityError('Density must be positive')\n    if volume < 0:\n        raise InvalidVolumeError('Volume must be positive')\n    if volume == 0:\n        density / volume\nHaving a root exception in a module makes it easy for consumers of \nan API to catch all of the exceptions that were raised deliberately. For \nexample, here a consumer of my API makes a function call with a \ntry/except statement that catches my root exception:\ntry:\n    weight = my_module.determine_weight(1, -1)\nexcept my_module.Error:\n    logging.exception('Unexpected error')\n>>>\nUnexpected error\n \nItem 87: Define a Root Exception to Insulate Callers from APIs \n409\n",
      "content_length": 1750,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 433,
      "content": "410 \nChapter 10 Collaboration\nTraceback (most recent call last):\n  File \".../example.py\", line 3, in <module>\n    weight = my_module.determine_weight(1, -1)\n  File \".../my_module.py\", line 10, in determine_weight\n    raise InvalidDensityError('Density must be positive')\nInvalidDensityError: Density must be positive\nHere, the logging.exception function prints the full stack trace of the \ncaught exception so it’s easier to debug in this situation. The try/\nexcept also prevents my API’s exceptions from propagating too far \nupward and breaking the calling program. It insulates the calling \ncode from my API. This insulation has three helpful effects.\nFirst, root exceptions let callers understand when there’s a problem \nwith their usage of an API. If callers are using my API properly, they \nshould catch the various exceptions that I deliberately raise. If they \ndon’t handle such an exception, it will propagate all the way up to \nthe insulating except block that catches my module’s root exception. \nThat block can bring the exception to the attention of the API con-\nsumer, providing an opportunity for them to add proper handling of \nthe missed exception type:\ntry:\n    weight = my_module.determine_weight(-1, 1)\nexcept my_module.InvalidDensityError:\n    weight = 0\nexcept my_module.Error:\n    logging.exception('Bug in the calling code')\n>>>\nBug in the calling code\nTraceback (most recent call last):\n  File \".../example.py\", line 3, in <module>\n    weight = my_module.determine_weight(-1, 1)\n  File \".../my_module.py\", line 12, in determine_weight\n    raise InvalidVolumeError('Volume must be positive')\nInvalidVolumeError: Volume must be positive\nThe second advantage of using root exceptions is that they can help \nfind bugs in an API module’s code. If my code only deliberately raises \nexceptions that I define within my module’s hierarchy, then all other \ntypes of exceptions raised by my module must be the ones that I didn’t \nintend to raise. These are bugs in my API’s code.\nUsing the try/except statement above will not insulate API consum-\ners from bugs in my API module’s code. To do that, the caller needs to \nadd another except block that catches Python’s base Exception class. \n",
      "content_length": 2202,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 434,
      "content": "This allows the API consumer to detect when there’s a bug in the API \nmodule’s implementation that needs to be fixed. The output for this \nexample includes both the logging.exception message and the default \ninterpreter output for the exception since it was re-raised:\ntry:\n    weight = my_module.determine_weight(0, 1)\nexcept my_module.InvalidDensityError:\n    weight = 0\nexcept my_module.Error:\n    logging.exception('Bug in the calling code')\nexcept Exception:\n    logging.exception('Bug in the API code!')\n    raise  # Re-raise exception to the caller\n>>>\nBug in the API code!\nTraceback (most recent call last):\n  File \".../example.py\", line 3, in <module>\n    weight = my_module.determine_weight(0, 1)\n  File \".../my_module.py\", line 14, in determine_weight\n    density / volume\nZeroDivisionError: division by zero\nTraceback ...\nZeroDivisionError: division by zero\nThe third impact of using root exceptions is future-proofing an API. \nOver time, I might want to expand my API to provide more spe-\ncific exceptions in certain situations. For example, I could add an \nException subclass that indicates the error condition of supplying \nnegative densities:\n# my_module.py\n...\n \nclass NegativeDensityError(InvalidDensityError):\n    \"\"\"A provided density value was negative.\"\"\"\n \n...\n \ndef determine_weight(volume, density):\n    if density < 0:\n        raise NegativeDensityError('Density must be positive')\n    ...\n \nItem 87: Define a Root Exception to Insulate Callers from APIs \n411\n",
      "content_length": 1486,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 435,
      "content": "412 \nChapter 10 Collaboration\nThe calling code will continue to work exactly as before because it \nalready catches InvalidDensityError exceptions (the parent class \nof NegativeDensityError). In the future, the caller could decide to \n special-case the new type of exception and change the handling \nbehavior accordingly:\ntry:\n    weight = my_module.determine_weight(1, -1)\nexcept my_module.NegativeDensityError:\n    raise ValueError('Must supply non-negative density')\nexcept my_module.InvalidDensityError:\n    weight = 0\nexcept my_module.Error:\n    logging.exception('Bug in the calling code')\nexcept Exception:\n    logging.exception('Bug in the API code!')\n    raise\n>>>\nTraceback ...\nNegativeDensityError: Density must be positive\n \nThe above exception was the direct cause of the following \n¯exception:\n \nTraceback ...\nValueError: Must supply non-negative density\nI can take API future-proofing further by providing a broader set of \nexceptions directly below the root exception. For example, imagine \nthat I have one set of errors related to calculating weights, another \nrelated to calculating volume, and a third related to calculating \ndensity:\n# my_module.py\nclass Error(Exception):\n    \"\"\"Base-class for all exceptions raised by this module.\"\"\"\n \nclass WeightError(Error):\n    \"\"\"Base-class for weight calculation errors.\"\"\"\n \nclass VolumeError(Error):\n    \"\"\"Base-class for volume calculation errors.\"\"\"\n \nclass DensityError(Error):\n    \"\"\"Base-class for density calculation errors.\"\"\"\n...\n",
      "content_length": 1501,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 436,
      "content": " \nItem 88: Know How to Break Circular Dependencies \n413\nSpecific exceptions would inherit from these general exceptions. Each \nintermediate exception acts as its own kind of root exception. This \nmakes it easier to insulate layers of calling code from API code based \non broad functionality. This is much better than having all callers \ncatch a long list of very specific Exception subclasses.\nThings to Remember\n✦ Defining root exceptions for modules allows API consumers to \n insulate themselves from an API.\n✦ Catching root exceptions can help you find bugs in code that \n consumes an API.\n✦ Catching the Python Exception base class can help you find bugs in \nAPI implementations.\n✦ Intermediate root exceptions let you add more specific types of \nexceptions in the future without breaking your API consumers.\nItem 88: Know How to Break Circular Dependencies\nInevitably, while you’re collaborating with others, you’ll find a mutual \ninterdependence between modules. It can even happen while you work \nby yourself on the various parts of a single program.\nFor example, say that I want my GUI application to show a dialog box \nfor choosing where to save a document. The data displayed by the dia-\nlog could be specified through arguments to my event handlers. But \nthe dialog also needs to read global state, such as user preferences, to \nknow how to render properly.\nHere, I define a dialog that retrieves the default document save loca-\ntion from global preferences:\n# dialog.py\nimport app\n \nclass Dialog:\n    def __init__(self, save_dir):\n        self.save_dir = save_dir\n    ...\n \nsave_dialog = Dialog(app.prefs.get('save_dir'))\n \ndef show():\n    ...\n",
      "content_length": 1656,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 437,
      "content": "414 \nChapter 10 Collaboration\nThe problem is that the app module that contains the prefs object \nalso imports the dialog class in order to show the same dialog on pro-\ngram start:\n# app.py\nimport dialog\n \nclass Prefs:\n    ...\n    def get(self, name):\n        ...\n \nprefs = Prefs()\ndialog.show()\nIt’s a circular dependency. If I try to import the app module from my \nmain program like this:\n# main.py\nimport app\nI get an exception:\n>>>\n$ python3 main.py \nTraceback (most recent call last):\n  File \".../main.py\", line 17, in <module>\n    import app\n  File \".../app.py\", line 17, in <module>\n    import dialog\n  File \".../dialog.py\", line 23, in <module>\n    save_dialog = Dialog(app.prefs.get('save_dir'))\nAttributeError: partially initialized module 'app' has no \n¯attribute 'prefs' (most likely due to a circular import)\nTo understand what’s happening here, you need to know how Python’s \nimport machinery works in general (see the importlib built-in package \nfor the full details). When a module is imported, here’s what Python \nactually does, in depth-first order:\n1. Searches for a module in locations from sys.path\n2. Loads the code from the module and ensures that it compiles\n3. Creates a corresponding empty module object\n4. Inserts the module into sys.modules\n5. Runs the code in the module object to define its contents\n",
      "content_length": 1329,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 438,
      "content": " \nItem 88: Know How to Break Circular Dependencies \n415\nThe problem with a circular dependency is that the attributes of a \nmodule aren’t defined until the code for those attributes has executed \n(after step 5). But the module can be loaded with the import state-\nment immediately after it’s inserted into sys.modules (after step 4).\nIn the example above, the app module imports dialog before defin-\ning anything. Then, the dialog module imports app. Since app still \nhasn’t finished running—it’s currently importing dialog—the app \nmodule is empty (from step 4). The AttributeError is raised (during \nstep 5 for dialog) because the code that defines prefs hasn’t run yet \n(step 5 for app isn’t complete).\nThe best solution to this problem is to refactor the code so that the \nprefs data structure is at the bottom of the dependency tree. Then, \nboth app and dialog can import the same utility module and avoid \nany circular dependencies. But such a clear division isn’t always pos-\nsible or could require too much refactoring to be worth the effort.\nThere are three other ways to break circular dependencies.\nReordering Imports\nThe first approach is to change the order of imports. For example, if I \nimport the dialog module toward the bottom of the app module, after \nthe app module’s other contents have run, the AttributeError goes \naway:\n# app.py\nclass Prefs:\n    ...\n \nprefs = Prefs()\n \nimport dialog  # Moved\ndialog.show()\nThis works because, when the dialog module is loaded late, its recur-\nsive import of app finds that app.prefs has already been defined (step \n5 is mostly done for app).\nAlthough this avoids the AttributeError, it goes against the PEP 8 \nstyle guide (see Item 2: “Follow the PEP 8 Style Guide”). The style \nguide suggests that you always put imports at the top of your Python \nfiles. This makes your module’s dependencies clear to new readers of \nthe code. It also ensures that any module you depend on is in scope \nand available to all the code in your module.\n",
      "content_length": 1992,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 439,
      "content": "416 \nChapter 10 Collaboration\nHaving imports later in a file can be brittle and can cause small \nchanges in the ordering of your code to break the module entirely. \nI suggest not using import reordering to solve your circular depen-\ndency issues.\nImport, Configure, Run\nA second solution to the circular imports problem is to have mod-\nules minimize side effects at import time. I can have my modules only \ndefine functions, classes, and constants. I avoid actually running \nany functions at import time. Then, I have each module provide a \nconfigure function that I call once all other modules have finished \nimporting. The purpose of configure is to prepare each module’s state \nby accessing the attributes of other modules. I run configure after \nall modules have been imported (step 5 is complete), so all attributes \nmust be defined.\nHere, I redefine the dialog module to only access the prefs object \nwhen configure is called:\n# dialog.py\nimport app\n \nclass Dialog:\n    ...\n \nsave_dialog = Dialog()\n \ndef show():\n    ...\n \ndef configure():\n    save_dialog.save_dir = app.prefs.get('save_dir')\nI also redefine the app module to not run activities on import:\n# app.py\nimport dialog\n \nclass Prefs:\n    ...\n \nprefs = Prefs()\n \ndef configure():\n    ...\n",
      "content_length": 1254,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 440,
      "content": "Finally, the main module has three distinct phases of execution—\nimport everything, configure everything, and run the first activity:\n# main.py\nimport app\nimport dialog\n \napp.configure()\ndialog.configure()\n \ndialog.show()\nThis works well in many situations and enables patterns like depen-\ndency injection. But sometimes it can be difficult to structure your \ncode so that an explicit configure step is possible. Having two dis-\ntinct phases within a module can also make your code harder to read \nbecause it separates the definition of objects from their configuration.\nDynamic Import\nThe third—and often simplest—solution to the circular imports prob-\nlem is to use an import statement within a function or method. This \nis called a dynamic import because the module import happens while \nthe program is running, not while the program is first starting up \nand initializing its modules.\nHere, I redefine the dialog module to use a dynamic import. The \ndialog.show function imports the app module at runtime instead of \nthe dialog module importing app at initialization time:\n# dialog.py\nclass Dialog:\n    ...\n \nsave_dialog = Dialog()\n \ndef show():\n    import app  # Dynamic import\n    save_dialog.save_dir = app.prefs.get('save_dir')\n    ...\nThe app module can now be the same as it was in the original exam-\nple. It imports dialog at the top and calls dialog.show at the bottom:\n# app.py\nimport dialog\n \n \nItem 88: Know How to Break Circular Dependencies \n417\n",
      "content_length": 1463,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 441,
      "content": "418 \nChapter 10 Collaboration\nclass Prefs:\n    ...\n \nprefs = Prefs()\ndialog.show()\nThis approach has a similar effect to the import, configure, and run \nsteps from before. The difference is that it requires no structural \nchanges to the way the modules are defined and imported. I’m simply \ndelaying the circular import until the moment I must access the other \nmodule. At that point, I can be pretty sure that all other modules \nhave already been initialized (step 5 is complete for everything).\nIn general, it’s good to avoid dynamic imports like this. The cost of the \nimport statement is not negligible and can be especially bad in tight \nloops. By delaying execution, dynamic imports also set you up for \nsurprising failures at runtime, such as SyntaxError exceptions long \nafter your program has started running (see Item 76: “Verify Related \nBehaviors in TestCase Subclasses” for how to avoid that). However, \nthese downsides are often better than the alternative of restructuring \nyour entire program.\nThings to Remember\n✦ Circular dependencies happen when two modules must call into \neach other at import time. They can cause your program to crash at \nstartup.\n✦ The best way to break a circular dependency is by refactoring \nmutual dependencies into a separate module at the bottom of the \ndependency tree.\n✦ Dynamic imports are the simplest solution for breaking a circular \ndependency between modules while minimizing refactoring and \ncomplexity.\nItem 89:  Consider warnings to Refactor and \nMigrate Usage\nIt’s natural for APIs to change in order to satisfy new requirements \nthat meet formerly unanticipated needs. When an API is small and has \nfew upstream or downstream dependencies, making such changes is \nstraightforward. One programmer can often update a small API and \nall of its callers in a single commit.\n",
      "content_length": 1828,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 442,
      "content": "However, as a codebase grows, the number of callers of an API can be \nso large or fragmented across source repositories that it’s infeasible \nor impractical to make API changes in lockstep with updating callers \nto match. Instead, you need a way to notify and encourage the people \nthat you collaborate with to refactor their code and migrate their API \nusage to the latest forms.\nFor example, say that I want to provide a module for calculating how \nfar a car will travel at a given average speed and duration. Here, \nI define such a function and assume that speed is in miles per hour \nand duration is in hours:\ndef print_distance(speed, duration):\n    distance = speed * duration\n    print(f'{distance} miles')\n \nprint_distance(5, 2.5)\n>>>\n12.5 miles\nImagine that this works so well that I quickly gather a large number \nof dependencies on this function. Other programmers that I collabo-\nrate with need to calculate and print distances like this all across our \nshared codebase.\nDespite its success, this implementation is error prone because the \nunits for the arguments are implicit. For example, if I wanted to see \nhow far a bullet travels in 3 seconds at 1000 meters per second, I \nwould get the wrong result:\nprint_distance(1000, 3)\n>>>\n3000 miles\nI can address this problem by expanding the API of print_distance to \ninclude optional keyword arguments (see Item 23: “Provide Optional \nBehavior with Keyword Arguments” and Item 25: “Enforce Clarity \nwith Keyword-Only and Positional-Only Arguments”) for the units of \nspeed, duration, and the computed distance to print out:\nCONVERSIONS = {\n    'mph': 1.60934 / 3600 * 1000,   # m/s\n    'hours': 3600,                  # seconds\n    'miles': 1.60934 * 1000,        # m\n    'meters': 1,                    # m\n    'm/s': 1,                       # m\n    'seconds': 1,                   # s\n}\n \n \nItem 89: Consider warnings to Refactor and Migrate Usage \n419\n",
      "content_length": 1917,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 443,
      "content": "420 \nChapter 10 Collaboration\ndef convert(value, units):\n    rate = CONVERSIONS[units]\n    return rate * value\n \ndef localize(value, units):\n    rate = CONVERSIONS[units]\n    return value / rate\n \ndef print_distance(speed, duration, *,\n                   speed_units='mph',\n                   time_units='hours',\n                   distance_units='miles'):\n    norm_speed = convert(speed, speed_units)\n    norm_duration = convert(duration, time_units)\n    norm_distance = norm_speed * norm_duration\n    distance = localize(norm_distance, distance_units)\n    print(f'{distance} {distance_units}')\nNow, I can modify the speeding bullet call to produce an accurate \nresult with a unit conversion to miles:\nprint_distance(1000, 3,\n               speed_units='meters',\n               time_units='seconds')\n>>>\n1.8641182099494205 miles\nIt seems like requiring units to be specified for this function is a much \nbetter way to go. Making them explicit reduces the likelihood of errors \nand is easier for new readers of the code to understand. But how can I \nmigrate all callers of the API over to always specifying units? How do \nI minimize breakage of any code that’s dependent on print_distance \nwhile also encouraging callers to adopt the new units arguments as \nsoon as possible?\nFor this purpose, Python provides the built-in warnings module. \nUsing warnings is a programmatic way to inform other programmers \nthat their code needs to be modified due to a change to an underly-\ning library that they depend on. While exceptions are primarily for \nautomated error handling by machines (see Item 87: “Define a Root \nException to Insulate Callers from APIs”), warnings are all about \ncommunication between humans about what to expect in their col-\nlaboration with each other.\n",
      "content_length": 1770,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 444,
      "content": "I can modify print_distance to issue warnings when the optional \nkeyword arguments for specifying units are not supplied. This way, \nthe arguments can continue being optional temporarily (see Item 24: \n“Use None and Docstrings to Specify Dynamic Default Arguments” \nfor background), while providing an explicit notice to people running \ndependent programs that they should expect breakage in the future if \nthey fail to take action:\nimport warnings\n \ndef print_distance(speed, duration, *,\n                   speed_units=None,\n                   time_units=None,\n                   distance_units=None):\n    if speed_units is None:\n        warnings.warn(\n            'speed_units required', DeprecationWarning)\n        speed_units = 'mph'\n \n    if time_units is None:\n        warnings.warn(\n            'time_units required', DeprecationWarning)\n        time_units = 'hours'\n \n    if distance_units is None:\n        warnings.warn(\n            'distance_units required', DeprecationWarning)\n        distance_units = 'miles'\n \n    norm_speed = convert(speed, speed_units)\n    norm_duration = convert(duration, time_units)\n    norm_distance = norm_speed * norm_duration\n    distance = localize(norm_distance, distance_units)\n    print(f'{distance} {distance_units}')\nI can verify that this code issues a warning by calling the function \nwith the same arguments as before and capturing the sys.stderr out-\nput from the warnings module:\nimport contextlib\nimport io\n \nfake_stderr = io.StringIO()\nwith contextlib.redirect_stderr(fake_stderr):\n \nItem 89: Consider warnings to Refactor and Migrate Usage \n421\n",
      "content_length": 1600,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 445,
      "content": "422 \nChapter 10 Collaboration\n    print_distance(1000, 3,\n                   speed_units='meters',\n                   time_units='seconds')\n \nprint(fake_stderr.getvalue())\n>>>\n1.8641182099494205 miles\n.../example.py:97: DeprecationWarning: distance_units required\n  warnings.warn(\nAdding warnings to this function required quite a lot of repetitive boil-\nerplate that’s hard to read and maintain. Also, the warning message \nindicates the line where warning.warn was called, but what I really \nwant to point out is where the call to print_distance was made with-\nout soon-to-be-required keyword arguments.\nLuckily, the warnings.warn function supports the stacklevel param-\neter, which makes it possible to report the correct place in the stack \nas the cause of the warning. stacklevel also makes it easy to write \nfunctions that can issue warnings on behalf of other code, reducing \nboilerplate. Here, I define a helper function that warns if an optional \nargument wasn’t supplied and then provides a default value for it:\ndef require(name, value, default):\n    if value is not None:\n        return value\n    warnings.warn(\n        f'{name} will be required soon, update your code',\n        DeprecationWarning,\n        stacklevel=3)\n    return default\n \ndef print_distance(speed, duration, *,\n                   speed_units=None,\n                   time_units=None,\n                   distance_units=None):\n    speed_units = require('speed_units', speed_units, 'mph')\n    time_units = require('time_units', time_units, 'hours')\n    distance_units = require(\n        'distance_units', distance_units, 'miles')\n \n    norm_speed = convert(speed, speed_units)\n    norm_duration = convert(duration, time_units)\n    norm_distance = norm_speed * norm_duration\n    distance = localize(norm_distance, distance_units)\n    print(f'{distance} {distance_units}')\n",
      "content_length": 1849,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 446,
      "content": "I can verify that this propagates the proper offending line by inspect-\ning the captured output:\nimport contextlib\nimport io\n \nfake_stderr = io.StringIO()\nwith contextlib.redirect_stderr(fake_stderr):\n    print_distance(1000, 3,\n                   speed_units='meters',\n                   time_units='seconds')\n \nprint(fake_stderr.getvalue())\n>>>\n1.8641182099494205 miles\n.../example.py:174: DeprecationWarning: distance_units will be \n¯required soon, update your code\n  print_distance(1000, 3,\nThe warnings module also lets me configure what should happen \nwhen a warning is encountered. One option is to make all warnings \nbecome errors, which raises the warning as an exception instead of \nprinting it out to sys.stderr:\nwarnings.simplefilter('error')\ntry:\n    warnings.warn('This usage is deprecated',\n                  DeprecationWarning)\nexcept DeprecationWarning:\n    pass  # Expected\nThis exception-raising behavior is especially useful for automated \ntests in order to detect changes in upstream dependencies and fail \ntests accordingly. Using such test failures is a great way to make it \nclear to the people you collaborate with that they will need to update \ntheir code. You can use the -W error command-line argument to the \nPython interpreter or the PYTHONWARNINGS environment variable to \napply this policy:\n$ python -W error example_test.py \nTraceback (most recent call last):\n  File \".../example_test.py\", line 6, in <module>\n    warnings.warn('This might raise an exception!')\nUserWarning: This might raise an exception!\n \nItem 89: Consider warnings to Refactor and Migrate Usage \n423\n",
      "content_length": 1603,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 447,
      "content": "424 \nChapter 10 Collaboration\nOnce the people responsible for code that depends on a deprecated \nAPI are aware that they’ll need to do a migration, they can tell the \nwarnings module to ignore the error by using the simplefilter and \nfilterwarnings functions (see https://docs.python.org/3/library/\nwarnings for all the details):\nwarnings.simplefilter('ignore')\nwarnings.warn('This will not be printed to stderr')\nAfter a program is deployed into production, it doesn’t make sense for \nwarnings to cause errors because they might crash the program at a \ncritical time. Instead, a better approach is to replicate warnings into \nthe logging built-in module. Here, I accomplish this by calling the \nlogging.captureWarnings function and configuring the corresponding \n'py.warnings' logger:\nimport logging\n \nfake_stderr = io.StringIO()\nhandler = logging.StreamHandler(fake_stderr)\nformatter = logging.Formatter(\n    '%(asctime)-15s WARNING] %(message)s')\nhandler.setFormatter(formatter)\n \nlogging.captureWarnings(True)\nlogger = logging.getLogger('py.warnings')\nlogger.addHandler(handler)\nlogger.setLevel(logging.DEBUG)\n \nwarnings.resetwarnings()\nwarnings.simplefilter('default')\nwarnings.warn('This will go to the logs output')\n \nprint(fake_stderr.getvalue())\n>>>\n2019-06-11 19:48:19,132 WARNING] .../example.py:227: \n¯UserWarning: This will go to the logs output\n  warnings.warn('This will go to the logs output')\nUsing logging to capture warnings ensures that any error reporting \nsystems that my program already has in place will also receive notice \nof important warnings in production. This can be especially useful if \nmy tests don’t cover every edge case that I might see when the pro-\ngram is undergoing real usage.\n",
      "content_length": 1719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 448,
      "content": "API library maintainers should also write unit tests to verify that \nwarnings are generated under the correct circumstances with clear \nand actionable messages (see Item 76: “Verify Related Behaviors in \nTestCase Subclasses”). Here, I use the warnings.catch_warnings func-\ntion as a context manager (see Item 66: “Consider contextlib and \nwith Statements for Reusable try/finally Behavior” for background) \nto wrap a call to the require function that I defined above:\nwith warnings.catch_warnings(record=True) as found_warnings:\n    found = require('my_arg', None, 'fake units')\n    expected = 'fake units'\n    assert found == expected\nOnce I’ve collected the warning messages, I can verify that their num-\nber, detail messages, and categories match my expectations:\nassert len(found_warnings) == 1\nsingle_warning = found_warnings[0]\nassert str(single_warning.message) == (\n    'my_arg will be required soon, update your code')\nassert single_warning.category == DeprecationWarning\nThings to Remember\n✦ The warnings module can be used to notify callers of your API about \ndeprecated usage. Warning messages encourage such callers to fix \ntheir code before later changes break their programs.\n✦ Raise warnings as errors by using the -W error command-line argu-\nment to the Python interpreter. This is especially useful in auto-\nmated tests to catch potential regressions of dependencies.\n✦ In production, you can replicate warnings into the logging module \nto ensure that your existing error reporting systems will capture \nwarnings at runtime.\n✦ It’s useful to write tests for the warnings that your code generates to \nmake sure that they’ll be triggered at the right time in any of your \ndownstream dependencies.\nItem 90:  Consider Static Analysis via typing to \nObviate Bugs\nProviding documentation is a great way to help users of an API under-\nstand how to use it properly (see Item 84: “Write Docstrings for Every \nFunction, Class, and Module”), but often it’s not enough, and incor-\nrect usage still causes bugs. Ideally, there would be a programmatic \n \nItem 90: Consider Static Analysis via typing to Obviate Bugs \n425\n",
      "content_length": 2125,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 449,
      "content": "426 \nChapter 10 Collaboration\nmechanism to verify that callers are using your APIs the right way, \nand that you are using your downstream dependencies correctly. \nMany programming languages address part of this need with com-\npile-time type checking, which can identify and eliminate some cate-\ngories of bugs.\nHistorically Python has focused on dynamic features and has not \nprovided compile-time type safety of any kind. However, more recently \nPython has introduced special syntax and the built-in typing mod-\nule, which allow you to annotate variables, class fields, functions, \nand methods with type information. These type hints allow for grad-\nual typing, where a codebase can be incrementally updated to specify \ntypes as desired.\nThe benefit of adding type information to a Python program is that \nyou can run static analysis tools to ingest a program’s source code \nand identify where bugs are most likely to occur. The typing built-in \nmodule doesn’t actually implement any of the type checking function-\nality itself. It merely provides a common library for defining types, \nincluding generics, that can be applied to Python code and consumed \nby separate tools.\nMuch as there are multiple distinct implementations of the Python \ninterpreter (e.g., CPython, PyPy), there are multiple implementa-\ntions of static analysis tools for Python that use typing. As of the time \nof this writing, the most popular tools are mypy (https://github.com/\npython/mypy), \npytype \n(https://github.com/google/pytype), \npyright \n(https://github.com/microsoft/pyright), and pyre (https://pyre-check.\norg). For the typing examples in this book, I’ve used mypy with the \n--strict flag, which enables all of the various warnings supported by the \ntool. Here’s an example of what running the command line looks like:\n$ python3 -m mypy --strict example.py\nThese tools can be used to detect a large number of common errors \nbefore a program is ever run, which can provide an added layer of \nsafety in addition to having good unit tests (see Item 76: “Verify \nRelated Behaviors in TestCase Subclasses”). For example, can you \nfind the bug in this simple function that causes it to compile fine but \nthrow an exception at runtime?\ndef subtract(a, b):\n    return a - b\n \nsubtract(10, '5')\n>>>\nTraceback ...\nTypeError: unsupported operand type(s) for -: 'int' and 'str'\n",
      "content_length": 2352,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 450,
      "content": "Parameter and variable type annotations are delineated with a colon \n(such as name: type). Return value types are specified with -> type \nfollowing the argument list. Using such type annotations and mypy, \nI can easily spot the bug:\ndef subtract(a: int, b: int) -> int:  # Function annotation\n    return a - b\n \nsubtract(10, '5')  # Oops: passed string value\n$ python3 -m mypy --strict example.py\n.../example.py:4: error: Argument 2 to \"subtract\" has \nincompatible type \"str\"; expected \"int\"\nAnother common mistake, especially for programmers who have \nrecently moved from Python 2 to Python 3, is mixing bytes and str \ninstances together (see Item 3: “Know the Differences Between bytes \nand str”). Do you see the problem in this example that causes a run-\ntime error?\ndef concat(a, b):\n    return a + b\n \nconcat('first', b'second')\n>>>\nTraceback ...\nTypeError: can only concatenate str (not \"bytes\") to str\nUsing type hints and mypy, this issue can be detected statically before \nthe program runs:\ndef concat(a: str, b: str) -> str:\n    return a + b\n \nconcat('first', b'second')  # Oops: passed bytes value\n$ python3 -m mypy --strict example.py\n.../example.py:4: error: Argument 2 to \"concat\" has \n¯incompatible type \"bytes\"; expected \"str\"\nType annotations can also be applied to classes. For example, this \nclass has two bugs in it that will raise exceptions when the program \nis run:\nclass Counter:\n    def __init__(self):\n        self.value = 0\n \n \nItem 90: Consider Static Analysis via typing to Obviate Bugs \n427\n",
      "content_length": 1521,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 451,
      "content": "428 \nChapter 10 Collaboration\n    def add(self, offset):\n        value += offset\n \n    def get(self) -> int:\n        self.value\nThe first one happens when I call the add method:\ncounter = Counter()\ncounter.add(5)\n>>>\nTraceback ...\nUnboundLocalError: local variable 'value' referenced before \n¯assignment\nThe second bug happens when I call get:\ncounter = Counter()\nfound = counter.get()\nassert found == 0, found\n>>>\nTraceback ...\nAssertionError: None\nBoth of these problems are easily found by mypy:\nclass Counter:\n    def __init__(self) -> None:\n        self.value: int = 0  # Field / variable annotation\n \n    def add(self, offset: int) -> None:\n        value += offset      # Oops: forgot \"self.\"\n \n    def get(self) -> int:\n        self.value           # Oops: forgot \"return\"\n \ncounter = Counter()\ncounter.add(5)\ncounter.add(3)\nassert counter.get() == 8\n$ python3 -m mypy --strict example.py\n.../example.py:6: error: Name 'value' is not defined\n.../example.py:8: error: Missing return statement\n",
      "content_length": 999,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 452,
      "content": "One of the strengths of Python’s dynamism is the ability to write \ngeneric functionality that operates on duck types (see Item 15: “Be \nCautious When Relying on dict Insertion Ordering” and Item 43: \n“Inherit from collections.abc for Custom Container Types”). This \nallows one implementation to accept a wide range of types, saving a \nlot of duplicative effort and simplifying testing. Here, I’ve defined such \na generic function for combining values from a list. Do you under-\nstand why the last assertion fails?\ndef combine(func, values):\n    assert len(values) > 0\n \n    result = values[0]\n    for next_value in values[1:]:\n        result = func(result, next_value)\n \n    return result\n \ndef add(x, y):\n    return x + y\n \ninputs = [1, 2, 3, 4j]\nresult = combine(add, inputs)\nassert result == 10, result  # Fails\n>>>\nTraceback ...\nAssertionError: (6+4j)\nI can use the typing module’s support for generics to annotate this \nfunction and detect the problem statically:\nfrom typing import Callable, List, TypeVar\n \nValue = TypeVar('Value')\nFunc = Callable[[Value, Value], Value]\n \ndef combine(func: Func[Value], values: List[Value]) -> Value:\n    assert len(values) > 0\n \n    result = values[0]\n    for next_value in values[1:]:\n        result = func(result, next_value)\n \n    return result\n \n \nItem 90: Consider Static Analysis via typing to Obviate Bugs \n429\n",
      "content_length": 1360,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 453,
      "content": "430 \nChapter 10 Collaboration\nReal = TypeVar('Real', int, float)\n \ndef add(x: Real, y: Real) -> Real:\n    return x + y\n \ninputs = [1, 2, 3, 4j]  # Oops: included a complex number\nresult = combine(add, inputs)\nassert result == 10\n$ python3 -m mypy --strict example.py\n.../example.py:21: error: Argument 1 to \"combine\" has \n¯incompatible type \"Callable[[Real, Real], Real]\"; expected \n¯\"Callable[[complex, complex], complex]\"\nAnother extremely common error is to encounter a None value when \nyou thought you’d have a valid object (see Item 20: “Prefer Raising \nExceptions to Returning None”). This problem can affect seemingly \nsimple code. Do you see the issue here?\ndef get_or_default(value, default):\n    if value is not None:\n        return value\n    return value\n \nfound = get_or_default(3, 5)\nassert found == 3\n \nfound = get_or_default(None, 5)\nassert found == 5, found  # Fails\n>>>\nTraceback ...\nAssertionError: None\nThe typing module supports option types, which ensure that pro-\ngrams only interact with values after proper null checks have been \nperformed. This allows mypy to infer that there’s a bug in this code: \nThe type used in the return statement must be None, and that doesn’t \nmatch the int type required by the function signature:\nfrom typing import Optional\n \ndef get_or_default(value: Optional[int],\n                   default: int) -> int:\n    if value is not None:\n        return value\n    return value  # Oops: should have returned \"default\"\n",
      "content_length": 1466,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 454,
      "content": "$ python3 -m mypy --strict example.py\n.../example.py:7: error: Incompatible return value type (got \n¯\"None\", expected \"int\")\nA wide variety of other options are available in the typing module. \nSee https://docs.python.org/3.8/library/typing for all of the details. \nNotably, exceptions are not included. Unlike Java, which has checked \nexceptions that are enforced at the API boundary of every method, \nPython’s type annotations are more similar to C#’s: Exceptions are \nnot considered part of an interface’s definition. Thus, if you want to \nverify that you’re raising and catching exceptions properly, you need \nto write tests.\nOne common gotcha in using the typing module occurs when you \nneed to deal with forward references (see Item 88: “Know How to \nBreak Circular Dependencies” for a similar problem). For example, \nimagine that I have two classes and one holds a reference to the other:\nclass FirstClass:\n    def __init__(self, value):\n        self.value = value\n \nclass SecondClass:\n    def __init__(self, value):\n        self.value = value\n \nsecond = SecondClass(5)\nfirst = FirstClass(second)\nIf I apply type hints to this program and run mypy it will say that \nthere are no issues:\nclass FirstClass:\n    def __init__(self, value: SecondClass) -> None:\n        self.value = value\n \nclass SecondClass:\n    def __init__(self, value: int) -> None:\n        self.value = value\n \nsecond = SecondClass(5)\nfirst = FirstClass(second)\n$ python3 -m mypy --strict example.py\n \nItem 90: Consider Static Analysis via typing to Obviate Bugs \n431\n",
      "content_length": 1542,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 455,
      "content": "432 \nChapter 10 Collaboration\nHowever, if you actually try to run this code, it will fail because \nSecondClass \nis \nreferenced \nby \nthe \ntype \nannotation \nin \nthe \nFirstClass.__init__ method’s parameters before it’s actually defined:\nclass FirstClass:\n    def __init__(self, value: SecondClass) -> None:  # Breaks\n        self.value = value\n \nclass SecondClass:\n    def __init__(self, value: int) -> None:\n        self.value = value\n \nsecond = SecondClass(5)\nfirst = FirstClass(second)\n>>>\nTraceback ...\nNameError: name 'SecondClass' is not defined\nOne workaround supported by these static analysis tools is to use \na string as the type annotation that contains the forward reference. \nThe string value is later parsed and evaluated to extract the type \ninformation to check:\nclass FirstClass:\n    def __init__(self, value: 'SecondClass') -> None:  # OK\n        self.value = value\n \nclass SecondClass:\n    def __init__(self, value: int) -> None:\n        self.value = value\n \nsecond = SecondClass(5)\nfirst = FirstClass(second)\nA better approach is to use from __future__ import annotations, \nwhich is available in Python 3.7 and will become the default in \nPython 4. This instructs the Python interpreter to completely ignore \nthe values supplied in type annotations when the program is being \nrun. This resolves the forward reference problem and provides a per-\nformance improvement at program start time:\nfrom __future__ import annotations\n \nclass FirstClass:\n    def __init__(self, value: SecondClass) -> None:  # OK\n        self.value = value\n \n",
      "content_length": 1548,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 456,
      "content": " \nItem 90: Consider Static Analysis via typing to Obviate Bugs \n433\nclass SecondClass:\n    def __init__(self, value: int) -> None:\n        self.value = value\n \nsecond = SecondClass(5)\nfirst = FirstClass(second)\nNow that you’ve seen how to use type hints and their potential bene-\nfits, it’s important to be thoughtful about when to use them. Here are \nsome of the best practices to keep in mind:\n \n■It’s going to slow you down if you try to use type annotations from \nthe start when writing a new piece of code. A general strategy is \nto write a first version without annotations, then write tests, and \nthen add type information where it’s most valuable.\n \n■Type hints are most important at the boundaries of a codebase, \nsuch as an API you provide that many callers (and thus other \npeople) depend on. Type hints complement integration tests (see \nItem 77: “Isolate Tests from Each Other with setUp, tearDown, \nsetUpModule, and tearDownModule”) and warnings (see Item 89: \n“Consider warnings to Refactor and Migrate Usage”) to ensure that \nyour API callers aren’t surprised or broken by your changes.\n \n■It can be useful to apply type hints to the most complex and error-\nprone parts of your codebase that aren’t part of an API. However, \nit may not be worth striving for 100% coverage in your type anno-\ntations because you’ll quickly encounter diminishing returns.\n \n■If possible, you should include static analysis as part of your \nautomated build and test system to ensure that every commit to \nyour codebase is vetted for errors. In addition, the configuration \nused for type checking should be maintained in the repository to \nensure that all of the people you collaborate with are using the \nsame rules.\n \n■As you add type information to your code, it’s important to run \nthe type checker as you go. Otherwise, you may nearly finish \nsprinkling type hints everywhere and then be hit by a huge wall \nof errors from the type checking tool, which can be disheartening \nand make you want to abandon type hints altogether.\nFinally, it’s important to acknowledge that in many situations, you \nmay not need or want to use type annotations at all. For small pro-\ngrams, ad-hoc code, legacy codebases, and prototypes, type hints \nmay require far more effort than they’re worth.\n",
      "content_length": 2278,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 457,
      "content": "434 \nChapter 10 Collaboration\nThings to Remember\n✦ Python has special syntax and the typing built-in module for \nannotating variables, fields, functions, and methods with type \ninformation.\n✦ Static type checkers can leverage type information to help you avoid \nmany common bugs that would otherwise happen at runtime.\n✦ There are a variety of best practices for adopting types in your pro-\ngrams, using them in APIs, and making sure they don’t get in the \nway of your productivity.\n",
      "content_length": 483,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 458,
      "content": "Index\nSymbols\n* (asterisk) operator\nkeyword-only arguments, 98\nvariable positional arguments, \n87–88\n@ (at) symbol, decorators, 101\n** (double asterisk) operator, \nkeyword arguments, 90–91\n/ (forward slash) operator, \npositional-only arguments, 99\n% (percent) operator\nbytes versus str instances, 8–9\nformatting strings, 11\n+ (plus) operator, bytes versus str \ninstances, 7\n_ (underscore) variable name, 149\n:= (walrus) operator\nassignment expression, 35–41\nin comprehensions, 112–114\n__call__ method, 154–155\n@classmethod, 155–160\n__format__method, 16\n__getattr__ method, 195–201\n__getattribute__ method, 195–201\n__init__ method, 160–164\n__init_subclass__ method\nregistering classes, 208–213\nvalidating subclasses, 201–208\n__iter__ method, 119, 244–245\n__missing__ method (dictionary \nsubclasses), 73–75\n@property decorator\ndescriptors versus, 190–195\nrefactoring attributes with, \n186–189\nsetter attribute, 182–185\n__set_name__ method, annotating \nattributes, 214–218\n__setattr__ method, 195–201\nCPython, 230\nA\nAPIs\nmigrating usage, 418–425\nroot exceptions for, 408–413\nstability, 403–405\narguments\ndynamic default values, 93–96\niterating over, 116–121\nkeyword, 89–92\nkeyword-only, 96–101\npositional-only, 96–101\nvariable positional, 86–89\nassertions in TestCase subclasses, \n359\nassignment expressions\nin comprehensions, 110–114\nscope and, 85\nwalrus (:=) operator, 35–41\nassociative arrays, 43\nasterisk (*) operator\nkeyword-only arguments, 98\nvariable positional arguments, \n87–88\nasyncio built-in module\navoiding blocking, 289–292\ncombining threads and \ncoroutines, 282–288\nporting threaded I/O to, 271–282\nat (@) symbol, decorators, 101\n",
      "content_length": 1642,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 459,
      "content": "436 \nIndex\nattributes\nannotating, 214–218\ndynamic, 181\ngetter and setter methods versus, \n181–185\nlazy, 195–201\npublic versus private, 169–174\nrefactoring, 186–189\nB\nbinary data, converting to Unicode, \n6–7\nbinary operators, bytes versus str \ninstances, 8\nbisect built-in module, 334–336\nblocking asyncio event loop, \navoiding, 289–292\nblocking I/O (input/output) with \nthreads, 230–235\nbreaking circular dependencies, \n413–418\nbreakpoint built-in function, \n379–384\nbuffer protocol, 348\nbuilt-in types, classes versus, \n145–148\nbytearray built-in type, 346–351\nbytecode, 230\nbytes instances, str instances \nversus, 5–10\nC\nC extensions, 292–293\nC3 linearization, 162\ncallables, 154\ncatch-all unpacking, slicing versus, \n48–52\ncharacter data, bytes versus str \ninstances, 5–10\nchecked exceptions, 82\nchild processes, managing, \n226–230\ncircular dependencies, breaking, \n413–418\nclasses, 145\nattributes. See attributes\nbuilt-in types versus, 145–148\ndecorators, 218–224\ndocumentation, 398–399\nfunction interfaces versus, \n151–155\ninitializing parent classes, \n160–164\nmetaclasses. See metaclasses\nmix-in classes, 164–169\npolymorphism, 155–160\npublic versus private attributes, \n169–174\nrefactoring to, 148–151\nregistering, 208–213\nserializing, 168–169\nvalidating subclasses, 201–208\nversioning, 316–317\nclosures, variable scope and, 83–86\ncollaboration\nbreaking circular dependencies, \n413–418\ndynamic import, 417–418\nimport/configure/run, \n415–416\nreordering imports, 415–416\ncommunity-built modules, \n389–390\ndocumentation, 396–401\nmigrating API usage, 418–425\norganizing modules into \npackages, 401–406\nroot exceptions for APIs, 408–413\nstatic analysis, 425–434\nvirtual environments, 390–396\ncollections.abc module, inheritance \nfrom, 174–178\ncombining iterator items, 139–142\ncommands for interactive debugger, \n381\ncommunity-built modules, 389–390\ncompile-time static type checking, \n353\ncomplex sort criteria with key \nparameter, 52–58\ncomprehensions, 107\nassignment expressions in, \n110–114\ngenerator expressions for, \n121–122\nmap and filter functions versus, \n107–109\n",
      "content_length": 2074,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 460,
      "content": " \nIndex \n437\nmultiple subexpressions in, \n109–110\nconcurrency, 225\navoiding threads for fan-out, \n252–256\nfan-in, 252\nfan-out, 252\nhighly concurrent I/O (input/\noutput), 266–271\nparallelism versus, 225\nwith pipelines, 238–247\npreventing data races, 235–238\nusing Queue class for, 257–263\nusing ThreadPoolExecutor for, \n264–266\nwith threads, 230–235\nwhen to use, 248–252\nconcurrent.futures built-in module, \n292–297\nconfiguring deployment \nenvironments, 406–408\nconflicts with dependencies, \n390–396\ncontainers\ninheritance from collections.abc \nmodule, 174–178\niterator protocol, 119–121\ncontextlib built-in module, 304–308\nCoordinated Universal Time (UTC), \n308\ncopyreg built-in module, 312–319\ncoroutines, 266–271\ncombining with threads, \n282–288\nC-style strings, f-strings versus, \n11–21\ncustom container types, inheritance \nfrom collections.abc module, \n174–178\nD\ndata races, preventing, 235–238\ndatetime built-in module, 308–312\ndebugging\nwith interactive debugger, \n379–384\nmemory usage, 384–387\nwith repr strings, 354–357\nwith static analysis, 425–434\nDecimal class, rounding numbers, \n319–322\ndecorators\nclass decorators, 218–224\nfunction decorators, 101–104\ndefault arguments\ndynamic, 93–96\nwith pickle built-in module, \n315–316\ndefault values in dictionaries\n__missing__ method, 73–75\ndefaultdict versus setdefault \nmethods, 70–72\nget method versus in \nexpressions, 65–70\ndefaultdict class, setdefault method \nversus, 70–72\ndependencies\nbreaking circular, 413–418\nconflicts, 390–396\nencapsulating, 375–379\ninjecting, 378–379\nreproducing, 394–396\ntesting with mocks, 367–375\ndependency hell, 391\ndeployment environments, \nconfiguring, 406–408\ndeque class, 326–334\ndescriptor protocol, 191\ndescriptors versus @property \ndecorator, 190–195\ndeserializing with pickle built-in \nmodule, 312–319\ndevelopment environment, 406–407\ndiamond inheritance, 161–162, \n207–208\ndictionaries, 43\ninsertion ordering, 58–65\nmissing keys\n__missing__ method, 73–75\ndefaultdict versus setdefault \nmethods, 70–72\nget method versus in \nexpressions, 65–70\nnesting, 145–148\ntuples versus in format strings, \n13–15\n",
      "content_length": 2096,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 461,
      "content": "438 \nIndex\ndictionary comprehensions, \n108–109\ndocstrings\nfor dynamic default arguments, \n93–96\nwriting, 396–401\nfor classes, 398–399\nfor functions, 399–400\nfor modules, 397–398\ntype annotations and, \n400–401\ndocumentation. See docstrings\ndouble asterisk (**) operator, \nkeyword arguments, 90–91\ndouble-ended queues, 331\nduck typing, 61, 429\ndynamic attributes, 181\ndynamic default arguments, 93–96\ndynamic import, 417–418\nE\nelse blocks\nfor statements, 32–35\nexception handling, 299–304\nencapsulating dependencies, \n375–379\nenumerate built-in function, range \nbuilt-in function versus, 28–30\nexcept blocks, exception handling, \n299–304\nexception handling with try/except/\nelse/finally blocks, 299–304\nexceptions\nraising, None return value \nversus, 80–82\nroot exceptions for APIs, 408–413\nexpressions\nhelper functions versus, 21–24\nPEP 8 style guide, 4\nF\nfakes, mocks versus, 368\nfan-in, 252\nwith Queue class, 257–263\nwith ThreadPoolExecutor class, \n264–265\nfan-out, 252\navoiding threads for, 252–256\nwith Queue class, 257–263\nwith ThreadPoolExecutor class, \n264–265\nFIFO (first-in, first-out) queues, \n326–334\nfile operations, bytes versus str \ninstances, 9–10\nfilter built-in function, \ncomprehensions versus, \n107–109\nfinally blocks\nexception handling, 299–304\nwith statements versus, 304–308\nfirst-class functions, 152\nfirst-in, first-out (FIFO) queues, \n326–334\nfor loops, avoiding else blocks, \n32–35\nformat built-in function, 15–19\nformat strings\nbytes versus str instances, 8–9\nC-style strings versus f-strings, \n11–21\nformat built-in function, \n15–19\nf-strings explained, 19–21\ninterpolated format strings, \n19–21\nproblems with C-style strings, \n11–15\nstr.format method, 15–19\nforward slash (/) operator, \npositional-only arguments, 99\nf-strings\nC-style strings versus, 11–21\nstr.format method versus, 15–19\nexplained, 19–21\nfunctions, 77. See also generators\nclosures, variable scope and, \n83–86\ndecorators, 101–104\ndocumentation, 399–400\ndynamic default arguments, \n93–96\nas hooks, 151–155\nkeyword arguments, 89–92\nkeyword-only arguments, 96–101\nNone return value, raising \nexceptions versus, 80–82\n",
      "content_length": 2109,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 462,
      "content": " \nIndex \n439\nin pipelines, 238–247\npositional-only arguments, \n96–101\nmultiple return values, 77–80\nvariable positional arguments, \n86–89\nfunctools.wraps method, 101–104\nG\ngc built-in module, 384–386\ngenerator expressions, 121–122\ngenerators, 107\nyield from for composing, \n123–126\ninjecting data into, 126–131\nitertools module with, 136–142\nreturning lists versus, 114–116\nsend method, 126–131\nthrow method, 132–136\ngeneric object construction, \n155–160\nget method for missing dictionary \nkeys, 65–70\ngetter methods, attributes versus, \n181–185\nGIL (global interpreter lock), 230–\n235, 292\ngradual typing, 426\nH\nhasattr built-in function, 198–199\nhash tables, 43\nheapq built-in module, 336–346\nheaps, 341\nhelper functions, expressions \nversus, 21–24\nhighly concurrent I/O, 266–271\nhooks, functions as, 151–155\nI\nif/else conditional expressions, 23\nimport paths, stabilizing, 317–319\nimporting modules, 5, 414–415\nin expressions for missing \ndictionary keys, 65–70\nindexing\nslicing and, 44\nunpacking versus, 24–28\ninheritance\nfrom collections.abc module, \n174–178\ndiamond inheritance, 161–162, \n207–208\ninitializing parent classes, 160–164\ninjecting\ndata into generators, 126–131\ndependencies, 378–379\nmocks, 371–375\ninput/output (I/O). See I/O (input/\noutput)\ninsertion ordering, dictionaries, \n58–65\ninstalling modules, 389–390\nintegration tests, unit tests versus, \n365\ninteractive debugging, 379–384\ninterfaces, 145\nsimple functions for, 151–155\ninterpolated format strings. See \nf-strings\nI/O (input/output)\navoiding blocking asyncio event \nloop, 289–292\nusing threads for, 230–235\nhighly concurrent, 266–271\nporting threaded I/O to asyncio \nbuilt-in module, 271–282\nzero-copy interactions, 346–351\nisolating tests, 365–367\niterator protocol, 119–121\niterators. See also loops\ncombining items, 139–142\nfiltering items, 138–139\ngenerator expressions and, \n121–122\ngenerator functions and, 115–116\nlinking, 136–138\nas function arguments, 116–121\nStopIteration exception, 117\nitertools module, 136–142\nitertools.accumulate method, \n139–140\nitertools.chain method, 136\nitertools.combinations method, 141\nitertools.combinations_with_\nreplacement method, 141–142\nitertools.cycle method, 137\n",
      "content_length": 2190,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 463,
      "content": "440 \nIndex\nitertools.dropwhile method, 139\nitertools.filterfalse method, 139\nitertools.islice method, 138\nitertools.permutations method, \n140–141\nitertools.product method, 140\nitertools.repeat method, 136\nitertools.takewhile method, 138\nitertools.tee method, 137\nitertools.zip_longest method, 31–32, \n137–138\nJ\njson built-in module, 313\nK\nkey parameter, sorting lists, 52–58\nKeyError exceptions for missing \ndictionary keys, 65–70\nkeys\nhandling in dictionaries\n__missing__ method, 73–75\ndefaultdict versus setdefault \nmethods, 70–72\nget method versus in \nexpressions, 65–70\nkeyword arguments, 89–92\nkeyword-only arguments, 96–101\nL\nlazy attributes, 195–201\nleaks (memory), debugging, \n384–387\nlinking iterators, 136–138\nlist comprehensions, 107–108\ngenerator expressions versus, \n121–122\nlists, 43. See also comprehensions\nas FIFO queues, 326–331\nas return values, generators \nversus, 114–116\nslicing, 43–46\ncatch-all unpacking versus, \n48–52\nstriding with, 46–48\nsorting\nwith key parameter, 52–58\nsearching sorted lists, \n334–336\nlocal time, 308–312\nLock class, preventing data races, \n235–238\nloops. See also comprehensions\nelse blocks, avoiding, 32–35\nrange versus enumerate built-in \nfunctions, 28–30\nzip built-in function, 30–32\nM\nmap built-in function, \ncomprehensions versus, \n107–109\nmemory usage, debugging, 384–387\nmemoryview built-in type, 346–351\nmetaclasses, 181\nannotating attributes, 214–218\nclass decorators versus, 218–224\nregistering classes, 208–213\nvalidating subclasses, 201–208\nmigrating API usage, 418–425\nmissing dictionary keys\n__missing__ method, 73–75\ndefaultdict versus setdefault \nmethods, 70–72\nget method versus in \nexpressions, 65–70\nmix-in classes, 164–169\nmocks\nencapsulating dependencies for, \n375–379\ntesting with, 367–375\nmodules\ndocumentation, 397–398\nimporting, 5, 414–415\ndynamic import, 417–418\nimport/configure/run, \n415–416\nreordering imports, 415–416\ninstalling, 389–390\norganizing into packages, \n401–406\nmodule-scoped code, 406–408\nmultiple assignment. See tuples\nmultiple return values, unpacking, \n77–80\nmultiple generators, composing with \nyield from expression, 123–126\n",
      "content_length": 2120,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 464,
      "content": " \nIndex \n441\nmultiprocessing built-in module, \n292–297\nmulti-threaded program, converting \nfrom single-threaded to, \n248–252\nmutexes (mutual-exclusion locks), \npreventing data races, \n235–238\nN\nnamedtuple type, 149–150\nnamespaces, 402–403\nnaming conventions, 3–4\nnegative numbers for slicing, 44\nnested built-in types, classes \nversus, 145–148\nNone\nfor dynamic default arguments, \n93–96\nraising exceptions versus \nreturning, 80–82\nnonlocal statement, 85–86\nO\nobjects, generic construction, \n155–160\noptimizing, profiling before, \n322–326\noption types, 430\noptional arguments, extending \nfunctions with, 92\nOrderedDict class, 61\norganizing modules into packages, \n401–406\nP\npackages\ninstalling, 389–390\norganizing modules into, \n401–406\nparallel iteration, zip built-in \nfunction, 30–32\nparallelism, 225\navoiding threads, 230–235\nconcurrency versus, 225\nwith concurrent.futures built-in \nmodule, 292–297\nmanaging child processes, \n226–230\nparent classes, initializing, 160–164\npdb built-in module, 379–384\nPEP 8 style guide, 2–5\npercent (%) operator\nbytes versus str instances, 8–9\ndictionaries versus tuples with, \n13–15\nformatting strings, 11\nperformance, 299\nfirst-in, first-out (FIFO) queues, \n326–334\npriority queues, 336–346\nprofiling before optimizing, \n322–326\nsearching sorted lists, 334–336\nzero-copy interactions, 346–351\npickle built-in module, 312–319\npip command-line tool, 389–390\npipelines\ncoordinating threads with, \n238–247\nparallel processes, chains of, \n228–229\nrefactoring to use Queue for, \n257–263\nplus (+) operator, bytes versus str \ninstances, 7\npolymorphism, 155–160\nporting threaded I/O to asyncio \nbuilt-in module, 271–282\npositional arguments, variable, \n86–89\npositional-only arguments, 96–101\npost-mortem debugging, 382–384\nprint function, debugging with, \n354–357\npriority queues, 336–346\nprivate attributes, public attributes \nversus, 169–174\nprocesses, managing child \nprocesses, 226–230\nProcessPoolExecutor class, 295–297\nproducer-consumer queues, \n326–334\nproduction environment, 406\nprofiling before optimizing, 322–326\npublic attributes, private attributes \nversus, 169–174\nPylint, 5\n",
      "content_length": 2121,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 465,
      "content": "442 \nIndex\nPyPI (Python Package Index), \n389–390\nPython\ndetermining version used, 1–2\nstyle guide. See PEP 8 style guide\nPython 2, 1–2\nPython 3, 1–2\nPython Enhancement Proposal #8. \nSee PEP 8 style guide\nPython Package Index (PyPI), \n389–390\nPythonic style, 1\npytz module, 311–312\nQ\nQueue class\ncoordinating threads with, \n238–247\nrefactoring to use for \nconcurrency, 257–263\nR\nraising exceptions, None return \nvalue versus, 80–82\nrange built-in function, enumerate \nbuilt-in function versus,\n28–30\nrefactoring\nattributes, 186–189\nto break circular dependencies, \n415\nto classes, 148–151\nto use Queue class for \nconcurrency, 257–263\nregistering classes, 208–213\nreordering imports, 415–416\nrepetitive code, avoiding, 35–41\nrepr strings, debugging with, \n354–357\nreproducing dependencies, 394–396\nreturn values\ngenerators versus lists as, \n114–116\nNone return value, raising \nexceptions versus, 80–82\nunpacking multiple, 77–80\nreusable @property methods, \n190–195\nreusable try/finally blocks, \n304–308\nrobustness, 299\nexception handling with try/\nexcept/else/finally blocks, \n299–304\nreusable try/finally blocks, \n304–308\nrounding numbers, 319–322\nserialization/deserialization with \npickle, 312–319\ntime zone conversion, 308–312\nroot exceptions for APIs, 408–413\nrounding numbers with Decimal \nclass, 319–322\nrule of least surprise, 181\nS\nscope, closures and, 83–86\nscoping bug, 85\nsearching sorted lists, 334–336\nsend method in generators, 126–131\nsequences\nsearching sorted, 334–336\nslicing, 43–46\ncatch-all unpacking versus, \n48–52\nstriding, 46–48\nserializing\nclasses, 168–169\nwith pickle built-in module, \n312–319\nset comprehensions, 108–109\nsetdefault method (dictionaries), \n68–70\ndefaultdict method versus, 70–72\nsetter methods, attributes versus, \n181–185\nsetUp method (TestCase class), \n365–367\nsetUpModule function, 365–367\nsingle-threaded program, \nconverting to multi-threaded, \n248–252\nslicing\nmemoryview instances, 348\nsequences, 43–46\n",
      "content_length": 1950,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 466,
      "content": " \nIndex \n443\ncatch-all unpacking versus, \n48–52\nstriding, 46–48\nsoftware licensing, 390\nsorting\ndictionaries, insertion ordering, \n58–65\nlists\nwith key parameter, 52–58\nsearching sorted lists, \n334–336\nspeedup, 225\nstabilizing import paths, 317–319\nstable APIs, 403–405\nstable sorting, 56–57\nstar args, 86–89\nstarred expressions, 49–52\nstatements, PEP 8 style guide, 4\nstatic analysis, 425–434\nStopIteration exception, 117\nstr instances, bytes instances \nversus, 5–10\nstr.format method, 15–19\nstriding, 46–48\nstrings, C-style versus f-strings, \n11–21\nformat built-in function, 15–19\ninterpolated format strings, \n19–21\nproblems with C-style strings, \n11–15\nstr.format method, 15–19\nsubclasses, validating, 201–208\nsubexpressions in comprehensions, \n109–110\nsubprocess built-in module, \n226–230\nsuper built-in function, 160–164\nT\ntearDown method (TestCase class), \n365–367\ntearDownModule function, 365–367\nternary expressions, 23\ntest harness, 365\nTestCase subclasses\nisolating tests, 365–367\nverifying related behaviors, \n357–365\ntesting\nencapsulating dependencies for, \n375–379\nimportance of, 353–354\nisolating tests, 365–367\nwith mocks, 367–375\nwith TestCase subclasses, \n357–365\nunit versus integration tests, 365\nwith unittest built-in module, \n357\nThreadPoolExecutor class, 264–266\nthreads\navoiding for fan-out, 252–256\ncombining with coroutines, \n282–288\nconverting from single- to multi-\nthreaded program, 248–252\ncoordinating between, 238–247\nporting threaded I/O to asyncio \nbuilt-in module, 271–282\npreventing data races, 235–238\nrefactoring to use Queue class for \nconcurrency, 257–263\nThreadPoolExecutor class, \n264–266\nwhen to use, 230–235\nthrow method in generators, \n132–136\ntime built-in module, 308–312\ntime zone conversion, 308–312\ntimeout parameter for \nsubprocesses, 229–230\ntracemalloc built-in module, \n384–387\ntry blocks\nexception handling, 299–304\nversus with statements, 304–308\ntuples\ndictionaries versus with format \nstrings, 13–15\nindexing versus unpacking, \n24–28\nnamedtuple type, 149–150\nsorting with multiple criteria, \n55–56\n",
      "content_length": 2057,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 467,
      "content": "444 \nIndex\nunderscore (_) variable name in, \n149\ntype annotations, 82\ndocstrings and, 400–401\nwith static analysis, 425–434\ntype hints, 426\ntyping built-in module, 425–434\nU\nunderscore (_) variable name, 149\nUnicode data, converting to binary, \n6–7\nunit tests, integration tests versus, \n365\nunittest built-in module, 357\nunpacking\nindexing versus, 24–28\nmultiple return values, 77–80\nslicing versus, 48–52\nUTC (Coordinated Universal Time), \n308\nV\nvalidating subclasses, 201–208\nvariable positional arguments \n(varargs), 86–89\nvariable scope, closures and, 83–86\nvenv built-in module, 392–394\nversioning classes, 316–317\nversions of Python, determining \nversion used, 1–2\nvirtual environments, 390–396\nW\nwalrus (:=) operator\nassignment expression, 35–41\nin comprehensions, 112–114\nwarnings built-in module, 418–425\nweakref built-in module, 194\nwhile loops, avoiding else blocks, \n32–35\nwhitespace, 3\nwith statements for reusable try/\nfinally blocks, 304–308\nwith as targets, 306–308\nwriting docstrings, 396–401\nfor classes, 398–399\nfor functions, 399–400\nfor modules, 397–398\ntype annotations and, 400–401\nY\nyield from expressions, composing \nmultiple generators, 123–126\nZ\nzero-copy interactions, 346–351\nzip built-in function, 30–32\n",
      "content_length": 1235,
      "extraction_method": "PyMuPDF_fallback"
    }
  ],
  "enrichment": {
    "version": "1.0.0",
    "generated_by": "generate_chapter_metadata.py",
    "contains": [
      "keywords",
      "concepts",
      "summary"
    ]
  }
}