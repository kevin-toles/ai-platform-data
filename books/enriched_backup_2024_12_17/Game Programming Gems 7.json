{
  "metadata": {
    "title": "Game Programming Gems 7",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 632,
    "conversion_date": "2025-11-23T11:13:15.642092",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Game Programming Gems 7.pdf"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": "GAME\n\nPROGRAMMING\n\nEdited by Scott Jacobs\n\nGame\nProgramming\nGems 7\nEdited by \nScott Jacobs\nAustralia • Brazil • Japan • Korea • Mexico • Singapore • Spain • United Kingdom • United States\nCharles River Media\nA part of Course Technology, Cengage Learning\n\n\n© 2008 Course Technology, a part of Cengage Learning. \nALL RIGHTS RESERVED. No part of this work covered by the copyright\nherein may be reproduced, transmitted, stored, or used in any form or by\nany means graphic, electronic, or mechanical, including but not limited to\nphotocopying, recording, scanning, digitizing, taping, Web distribution,\ninformation networks, or information storage and retrieval systems, except\nas permitted under Section 107 or 108 of the 1976 United States Copyright\nAct, without the prior written permission of the publisher.\nPublisher and General Manager, \nCourse Technology PTR: Stacy L. Hiquet\nAssociate Director of Marketing:\nSarah Panella\nManager of Editorial Services: Heather\nTalbot\nMarketing Manager: Jordan Casey\nSenior Acquisitions Editor: Emi Smith\nProject/Copy Editor: Kezia Endsley\nCRM Editorial Services Coordinator:\nJen Blaney\nInterior Layout Tech: Judith Littlefield\nCover Designer: Tyler Creative Services\nCD-ROM Producer: Brandon Penticuff\nIndexer: Valerie Haynes Perry\nProofreader: Sue Boshers\nPrinted in the United States of America\n1 2 3 4 5 6 7 11 10 09 08\nFor product information and technology assistance, contact us at\nCengage Learning Customer & Sales Support, 1-800-354-9706\nFor permission to use material from this text or product,\nsubmit all requests online at cengage.com/permissions\nFurther permissions questions can be emailed to\npermissionrequest@cengage.com\nLibrary of Congress Control Number: 2007939358\nISBN-13: 978-1-58450-527-3\nISBN-10: 1-58450-527-3\nCourse Technology\n25 Thomson Place\nBoston, MA  02210\nUSA\nCengage Learning is a leading provider of customized learning solutions\nwith office locations around the globe, including Singapore, the United\nKingdom, Australia, Mexico, Brazil, and Japan. Locate your local office at:\ninternational.cengage.com/region\nCengage Learning products are represented in Canada by \nNelson Education, Ltd.\nFor your lifelong learning solutions, visit courseptr.com\nVisit our corporate website at cengage.com\neISBN-10: 1-30527-676-0\n\n\niii\nContents\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\nAbout the Cover Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\nAcknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv\nContributor Bios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii\nSECTION 1 GENERAL PROGRAMMING . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nAdam Lake, Graphics Software Architect\n1.1\nEfficient Cache Replacement Using the Age and Cost Metrics. . . . . . . 5\nColt “MainRoach” McAnlis, Microsoft Ensemble Studios\n1.2\nHigh Performance Heap Allocator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nDimitar Lazarov, Luxoflux\n1.3\nOptical Flow for Video Games Played with Webcams . . . . . . . . . . . . . 25\nArnau Ramisa, Institut d’Investigació, en Intelligència Artificial\nEnric Vergara, GeoVirtual\nEnric Martí, Universitat Autónoma de Barcelona\n1.4\nDesign and Implementation of a Multi-Platform Threading Engine. . . 35\nMichael Ramsey\n1.5\nFor Bees and Gamers: How to Handle Hexagonal Tiles. . . . . . . . . . . . 47\nThomas Jahn, King Art\nJörn Loviscach, Hochschule Bremen\n1.6\nA Sketch-Based Interface to Real-Time Strategy Games Based on a\nCellular Automaton . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\nCarlos A. Dietrich, Luciana P. Nedel, João L. D. Comba\n1.7\nFoot Navigation Technique for First-Person Shooting Games. . . . . . . 69\nMarcus Aurelius C. Farias, Daniela G. Trevisan, Luciana P. Nedel\n1.8\nDeferred Function Call Invocation System . . . . . . . . . . . . . . . . . . . . . 81\nMark Jawad, Nintendo of America Inc.\n1.9\nMultithread Job and Dependency System . . . . . . . . . . . . . . . . . . . . . . 87\nJulien Hamaide\n\n\n1.10\nAdvanced Debugging Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\nMartin Fleisz\nSECTION 2 MATH AND PHYSICS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\nGraham Rhodes, Applied Research Associates, Inc.\n2.1\nRandom Number Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\nChris Lomont\n2.2\nFast Generic Ray Queries for Games. . . . . . . . . . . . . . . . . . . . . . . . . 127\nJacco Bikker, IGAD/NHTV University of Applied Sciences—Breda, The Netherlands\n2.3\nFast Rigid-Body Collision Detection Using Farthest Feature Maps . . 143\nRahul Sathe, Advanced Visual Computing, SSG, Intel Corp.\nDillon Sharlet, University of Colorado at Boulder\n2.4\nUsing Projective Space to Improve Precision of Geometric\nComputations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\nKrzysztof Kluczek, Gda´nsk University of Technology\n2.5\nXenoCollide: Complex Collision Made Simple . . . . . . . . . . . . . . . . . . 165\nGary Snethen, Crystal Dynamics\n2.6\nEfficient Collision Detection Using Transformation Semantics. . . . . 179\nJosé Gilvan Rodrigues Maia, UFC\nCreto Augusto Vidal, UFC\nJoaquim Bento Cavalcante-Neto, UFC\n2.7\nTrigonometric Splines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\nTony Barrera, Barrera Kristiansen AB\nAnders Hast, Creative Media Lab, University of Gävle\nEwert Bengtsson, Centre For Image Analysis, Uppsala University\n2.8\nUsing Gaussian Randomness to Realistically Vary Projectile Paths . 199\nSteve Rabin, Nintendo of America Inc.\nSECTION 3 AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\nBrian Schwab\n3.1\nCreating Interesting Agents with Behavior Cloning. . . . . . . . . . . . . . 209\nJohn Harger\nNathan Fabian\niv\nContents\n\n\n3.2\nDesigning a Realistic and Unified Agent-Sensing Model. . . . . . . . . . 217\nSteve Rabin, Nintendo of America Inc.\nMichael Delp, WXP Inc.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming \nApproach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\nIskander Umarov\nAnatoli Beliaev\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, \nand NPC Personalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\nMichael F. Lynch, Ph.D., Rensselaer Polytechnic Institute, Troy, NY\n3.5\nUnderstanding Intelligence in Games Using Player Traces and\nInteractive Player Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265\nG. Michael Youngblood, UNC Charlotte\nPriyesh N. Dixit, UNC Charlotte\n3.6\nGoal-Oriented Plan Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\nMichael Dawe\n3.7\nBeyond A*: IDA* and Fringe Search . . . . . . . . . . . . . . . . . . . . . . . . . . 289\nRobert Kirk DeLisle\nSECTION 4 AUDIO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297\nAlexander Brandon\n4.1\nAudio Signal Processing Using Programmable Graphics Hardware . 299\nMark France\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine. . . . . . . . 305\nJason Page, Sony Computer Entertainment, Europe\n4.3\nListen Carefully, You Probably Won’t Hear This Again . . . . . . . . . . . 321\nStephan Schütze\n4.4\nReal-Time Audio Effects Applied . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\nKen Noland\n4.5\nContext-Driven, Layered Mixing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\nRobert Sparks\nContents\nv\n\n\nSECTION 5 GRAPHICS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\nTimothy E. Roden, Angelo State University\n5.1\nAdvanced Particle Deposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\nJeremy Hayes, Intel Corporation\n5.2\nReducing Cumulative Errors in Skeletal Animations . . . . . . . . . . . . . 365\nBill Budge, Sony Entertainment of America\n5.3\nAn Alternative Model for Shading of Diffuse Light for \nRough Materials. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\nTony Barrera, Barrera Kristiansen AB\nAnders Hast, Creative Media Lab, University of Gävle\nEwert Bengtsson, Centre For Image Analysis, Uppsala University\n5.4\nHigh-Performance Subdivision Surfaces . . . . . . . . . . . . . . . . . . . . . . 381\nChris Lomont\n5.5\nAnimating Relief Impostors Using Radial Basis Functions Textures . 401\nVitor Fernando Pamplona, Instituto de Informática: UFRGS\nManuel M. Oliveira, Instituto de Informática: UFRGS\nLuciana Porcher Nedel, Instituto de Informática: UFRGS\n5.6\nClipmapping on SM1.1 and Higher . . . . . . . . . . . . . . . . . . . . . . . . . . 413\nBen Garney\n5.7\nAn Advanced Decal System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423\nJoris Mans\nDmitry Andreev\n5.8\nMapping Large Textures for Outdoor Terrain Rendering. . . . . . . . . . 435\nAntonio Seoane, Javier Taibo, Luis Hernández, and \nAlberto Jaspe, VideaLAB, University of La Coruña\n5.9\nArt-Based Rendering with Graftal Imposters. . . . . . . . . . . . . . . . . . . 447\nJoshua A. Doss, Advanced Visual Computing, Intel Corporation\n5.10\nCheap Talk: Dynamic Real-Time Lipsync. . . . . . . . . . . . . . . . . . . . . . 455\nTimothy E. Roden, Angelo State University\nSECTION 6 NETWORKING AND MULTIPLAYER . . . . . . . . . . . . . . . . . . . 463\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465\nDiana Stelmack\nvi\nContents\n\n\n6.1\nHigh-Level Abstraction of Game World Synchronization . . . . . . . . . . 467\nHyun-jik Baeb\n6.2\nAuthentication for Online Games. . . . . . . . . . . . . . . . . . . . . . . . . . . . 481\nJon Watte\n6.3\nGame Network Debugging with Smart Packet Sniffers . . . . . . . . . . . 491\nDavid L. Koenig, The Whole Experience, Inc.\nSECTION 7 SCRIPTING AND DATA-DRIVEN SYSTEMS . . . . . . . . . . . . . 499\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501\nScott Jacobs\n7.1\nAutomatic Lua Binding System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503\nJulien Hamaide\n7.2\nSerializing C++ Objects Into a Database Using Introspection . . . . . . 517\nJoris Mans\n7.3\nDataports. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535\nMartin Linklater\n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine. . . . . . . . 541\nCurtiss Murphy, Alion Science and Technology\n7.5\nDance with Python’s AST. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555\nZou Guangxian\nAbout the CD-ROM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563\nContents\nvii\n",
      "page_number": 1,
      "chapter_number": 1,
      "summary": "This chapter covers segment 1 (pages 1-8). Key topics include game, introduction, and section.",
      "keywords": [
        "Scott Jacobs",
        "Cengage Learning",
        "United States",
        "University",
        "Angelo State University",
        "Technology",
        "Section",
        "Learning",
        "United States Copyright",
        "State University",
        "United",
        "Introduction",
        "Nintendo of America",
        "Edited by Scott",
        "Jacobs"
      ],
      "concepts": [
        "game",
        "introduction",
        "section",
        "contents",
        "michael",
        "university",
        "advanced",
        "graphic",
        "audio",
        "technology"
      ],
      "similar_chapters": [
        {
          "book": "Game_Engine_Architecture",
          "chapter": 1,
          "title": "Segment 1 (pages 1-18)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 11,
          "title": "Segment 11 (pages 111-118)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 53,
          "title": "Segment 53 (pages 511-519)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 3,
          "title": "Segment 3 (pages 42-63)",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank \n\n\nix\nPreface\nS\nix volumes of Game Programming Gems have preceded the edition now in your\nhands. Useful, practical ideas and techniques have spilled out of each and every\none of them. Referred to in online discussions, contemplated by inquisitive readers,\nand consulted by both amateur and professional game developers, I believe the previ-\nous editions have contributed toward making the games we all play more innovative,\nentertaining, and satisfying. Significant efforts have been made so that this 7th volume\ncontinues this tradition.\nPassion for Game Development\nGame development is a fantastic endeavor of which to be a part. It can be a true mer-\nitocracy allowing passion and talent to shine. Degrees and experience can help get you\nin the door, but it often comes down to results. Is your code maintainable? Does its\nperformance meet or exceed targets? Are the visuals and audio compelling? Is the\ngameplay fun? The challenge of excelling in these areas certainly contributes to the\nexcitement of game development and I imagine is one of the motivations that inspired\nthe authors of the following gems to share their ideas and experiences. I hope the same\ndesires to excel have brought you to this book, as the intention of this volume and\nindeed the entire series is to provide tools and inspiration to do so.\nThere are not many industries where passion for the work runs so high that work-\ning professionals gather together with interested amateurs for weekend “jams” to do\nalmost exactly what they just spent the previous five days doing. Maybe some of the\nlumberjack competitions I’ve seen on TV come close. But how many lumberjacks\nhave a logging project going on at home just to try out some new ideas for fun and\nexperience?\nThe necessity of domain expertise requirements means that often game develop-\ners become relegated to a particular role: graphics programmer, AI programmer, and\nso on. The sections of this book certainly reflect some of the common dividing lines\nbetween disciplines, although I must respect those who wish to quarrel with a few of\nthe classifications within those categories as they sometimes don’t always easily fall\ninto just a single area. While I hope those with a more narrow focus find gems to suit\ntheir interests, I’m very excited about the diverse range and ability to appeal to those\nwith a passion for all areas of game development. I want graphics programmers to\nread audio gems and vice versa!\n\n\nWanting to Make Games\nEnthusiasm for game development from industry insiders may help explain why so\nmany seem so eager to join up as game developers. Although self-taught independent\nrenegades can still get their foot in the door (sometimes even making their own fan-\ntastic doors!), it is becoming increasingly easy to find quality educational help for\nthose trying to enter game development as a first career choice. Besides the traditional\nmath and computer science educational routes and a wealth of quality introductory to\nadvanced publications, specialized game development degrees and courses are avail-\nable at secondary schools and universities around the world, sometimes working in\nclose collaboration with professional development studios. A wide variety of game\ngenres are represented by published titles able to be modded, offering unprecedented\naccess to cutting-edge multi-million-dollar game engines and a great way to enhance\nyour experience or demo portfolio. Additionally, for most genres of games you can\neasily locate quality Open Source titles or engines available for inspection, experimen-\ntation, and contribution.\nThe opportunity to contribute to gaming also looks good for those passionate\namateurs with significant non-game-related software development experience. We \ncan use them. As game designs, target hardware, and development teams themselves\nbecome increasingly large and complex, the industry finds itself continuing its vora-\ncious appetite for good ideas from the rest of the software development industry. Does\nyour development team include a DBA (pipe down, MMO developers!)? Inside you’ll\nfind a gem that suggests ways to integrate your object system with a relational data-\nbase. We have a networking gem that applies tools to multiplayer development that\nare common to many network administrators, but may not yet have widespread use in\nour industry. Recognizing trends and successes in the wider software development\ncommunity, development teams are increasingly adopting formalized project manage-\nment and production methodologies like Agile and Scrum, where we can benefit\nfrom the general experience of our colleagues outside of game development. Making\ngames isn’t like making word processors, but good solutions for managing ever\nincreasing team sizes, facilitating efficient intra-team communication, and managing\ncustomer (publisher!) relationships can’t help but be similar to good solutions to the\nsame problems experienced by those outside our industry. The shift to multi-core\nmachines, whether on a PC or current-day consoles, has developers looking beyond\nthe traditional C/C++ programming languages to solve problems of concurrency and\nsynchronization and we are actively seeking out the experiences of those versed in lan-\nguages like Haskell and Erlang to see of what we may make use.\nPassion for Fun\nGames are appealing because of their ability to challenge, amuse, and entertain. Many\nof our gems deal with the messy behind-the-curtain bits that don’t directly contribute\nto making a game fun. A genre re-definer played over and over again and a clunker\nx\nPreface\n\n\nabandoned prior to the first boss fight can be using the same collision detection sys-\ntem or C++ to scripting language interface. It is the experience created by playing the\ngame that produces the fun. So, in addition to gems addressing core bits, there are\ngems that contribute directly to a player’s experience of the game, including audio\nproduction gems and human-game interactions. People are hungry for and eager to\ntry new ways to interact with their games. The recent successes of Rock Band, the Gui-\ntar Hero franchise, Dance Dance Revolution, and of course, Nintendo’s Wii, have\ndemonstrated this without a doubt. New interfaces have given long-time gamers new\nexperiences as well as tempted those not normally enticed by electronic games to give\nthem a try, often opening a whole new avenue of fun for them, and new markets for\nus. I’m proud that this volume introduces three gems related to under-explored ideas\nin human-game interaction and greatly look forward to what will come in the future\nas these ideas and others are tried and refined.\nInto this world of passionate developers, eager newcomers, voracious production\nrequirements, and demands for innovating and entertaining gameplay and design\ncomes this volume. Asking one book to meet the needs of all these interests is a tall\norder, but I feel confident that what follows will deliver, and I hope you agree. Let me\nknow when your game is released. I want to check it out!\nPreface\nxi\n\n\nThis page intentionally left blank \n\n\nxiii\nAbout the Cover Image\nC\nhristopher Scot Roby created the Game Programming Gems 7 cover. The cover\nrepresents a few of the early steps in producing content for a game. Starting with\nthe very left edge of the image, there are remnants of the original sketch, which was\nlater painted on in Photoshop, resulting in the concept art seen on the left. The right\nportion reflects one of the very latest procedures for turning concepts into playable\nassets. Google Sketchup’s Photo Match feature is used to block in geometry conform-\ning to the concept image. Depending upon the pipeline, this geometry can then be\ndirectly exported into a form usable by a game, greatly speeding up the process of\ngoing from concept to something playable!\n\n\nThis page intentionally left blank \n\n\nxv\nAcknowledgments\nI\nmust start by thanking Jenifer Niles and the Game Programming Gems 6 editor\nMichael Dickheiser for giving me the opportunity and enjoyable burden of editing\nthis volume. I also need to thank my section editors, both returning veterans and\neager newcomers. Without these people and the years of experience they bring, I\nwould have been overwhelmed by the fantastic articles supplied by our gems authors,\nmany well outside my areas of expertise. I cannot overstate how much of their hard\nwork is directly reflected in these pages.\nAdditionally, I would like to thank for their patience and efforts working with me\nto bring this book and accompanying CD-ROM together Emi Smith from Cengage,\nKezia Endsley, Brandon Penticuff, and the unmet people undoubtedly supporting\nthem.\nMy wife Veronica Noechel needs to be acknowledged for her understanding each\ntime another night went by with cage cleanings postponed, TVs asked to be turned\ndown, and cooking duties unshared. My parents should be thanked for so much as\nwell, but I’ll specially call out all the times my father brought home for my use his\nwork PC. Honestly, they really used to be quite expensive, heavy, and require multiple\ntrips across the parking lot to the car!\n",
      "page_number": 9,
      "chapter_number": 2,
      "summary": "This chapter covers segment 2 (pages 9-16). Key topics include game, gaming, and developers.",
      "keywords": [
        "Game Development",
        "Game",
        "Game Programming Gems",
        "Development",
        "Gems",
        "Game Programming",
        "Programming Gems",
        "experience",
        "game development degrees",
        "software development",
        "specialized game development",
        "enter game development",
        "fun",
        "ideas",
        "development teams"
      ],
      "concepts": [
        "game",
        "gaming",
        "developers",
        "gems",
        "gem",
        "volumes",
        "teams",
        "work",
        "useful",
        "preface"
      ],
      "similar_chapters": [
        {
          "book": "Game_Engine_Architecture",
          "chapter": 2,
          "title": "Segment 2 (pages 19-41)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 1,
          "title": "Segment 1 (pages 1-18)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 42,
          "title": "Segment 42 (pages 849-853)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 47,
          "title": "Segment 47 (pages 448-460)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 36,
          "title": "Segment 36 (pages 719-740)",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 17-25)",
      "start_page": 17,
      "end_page": 25,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank \n\n\nxvii\nContributor Bios\nContains bios for those contributors who submitted one.\nDmitry Andreev\nDmitry is a software engineer specializing in 3D graphics and tools at 10Tacle Studios,\nBelgium. Previously, he was a lead programmer at Burut CT/World Forge, the\nfounder of the X-Tend gaming technology used in numerous releases, including the\nrecent UberSoldier and Sparta: Ancient Wars. Dmitry started programming on ZX-\nSpectrum about 17 years ago. He is a well known demomaker and a two-time winner\nin 64K intro competition at the Assembly demo party. He has a B.S. in Applied\nMathematics and Mechanics.\nHyun-jik Bae\nAfter Hyun-jik Bae developed the Speed Game (see his Bio in Game Programming\nGems 5), he developed Boom Boom Car (similar to Rally-X) by using Turbo Pascal\nwhen he was 11. Boom Boom Car was also mentioned by the hero actor in the movie\nWho Are You?” (from a love story of a gamer and a game developer). He is now a direc-\ntor with Dyson Interactive Inc. and developing another online game title. His major\ninterests include designing and implementing high-performance game servers, scal-\nable database applications, realistic renderers, and physics simulators, as well as piano,\ngolf, and touring with his wife and son.\nTony Barrera\nTony Barrera is an autodidact mathematician and computer graphics researcher. He\nspecializes in algorithms for performing efficient mathematical calculations, especially\nin connection with computer graphics. He published his first paper “An Integer Based\nSquare-Root Algorithm” in BIT 1993 and has since published more than 20 papers.\nHe has worked as a consultant for several companies in computer graphics and related\nfields. Currently, he is developing computationally efficient basic graphics algorithms\ntogether with Ewert Bengtsson and Anders Hast.\nAnatoli Beliaev\nAnatoli Beliaev (beliaev@trusoft.com) is a software engineer with more than 15 years\nof diverse development experience. Since 2001, he has been working for TruSoft as\nLead Engineer responsible for the architecture of behavior-capture AI technologies.\nHe is especially focused on adaptive and generic programming approaches, and their\n\n\napplication to constructing highly efficient and flexible software in performance-\ndemanding areas. Mr. Beliaev graduated from Bauman Moscow State Technical\nUniversity with an M.S. in Computer Science.\nEwert Bengtsson \nEwert Bengtsson has been professor of Computerized Image Analysis at Uppsala Uni-\nversity since 1988 and is currently head of the Centre for Image Analysis in Uppsala.\nHis main research interests are to develop methods and tools for biomedical applica-\ntions of image analysis and computer assisted visualization of 3D biomedical images.\nHe is also interested it computationally efficient algorithms in graphics and visualiza-\ntion. He has published about 130 international research papers and supervised about\n30 Ph.D. students. He is a senior member of IEEE and member of the Royal Swedish\nAcademy of Engineering Sciences. \nJacco Bikker\nBikker is a lecturer for the International Architecture and Design course of the Univer-\nsity of Applied Sciences, Breda, the Netherlands. Before that, he worked in the Dutch\ngame industry for 10 years, for companies such as Lost Boys Interactive, Davilex,\nOverloaded PocketMedia, and W!Games. Besides his job, he has written articles on\ntopics such as ray tracing, rasterization, visibility determination, artificial intelligence,\nand game development for developer Websites such as Flipcode.com and Gamasutra.\nBill Budge\nEver since he got his first set of blocks at age 2, Bill Budge has loved building things.\nAt age 15, he discovered computer programming, the greatest set of blocks ever\ninvented. Since then, his life’s work has been to use these “blocks” to build even bet-\nter sets of blocks. Among these have been Bill Budge’s 3D Game Toolkit and Pinball\nConstruction Set. He is currently building game editors in the Tools and Technology\nGroup at Sony Computer Entertainment, America.\nJoaquim Bento Cavalcante-Neto\nJoaquim Bento Cavalcante-Neto is a professor of Computer Graphics in the Depart-\nment of Computing at the Federal University of Ceará (UFC) in Brazil. He received\nhis Ph.D. in Civil Engineering from the Pontifical Catholic University of Rio de\nJaneiro (PUC-Rio), Brazil, in 1998. While pursuing his Ph.D., he spent a year work-\ning with Computer Graphics/Civil Engineering at Cornell University. He was also a\npost-doctoral research associate at Cornell University from 2002 to 2003. During\nboth his Ph.D. and post-doc, he worked with applied computer graphics. His current\nresearch interests are computer graphics, virtual reality, and computer animation. He\nxviii\nContributor Bios\n\n\nalso works with numerical methods, computational geometry, and computational\nmechanics. He has been the coordinator of several government-sponsored projects\nand the coordinator of the graduate program (master’s and Ph.D. programs) in com-\nputer science at the Federal University of Ceará (UFC).\nMichael Dawe\nMichael’s route to joining the games industry after college included three years in the\nconsulting industry, two cross-country moves, and one summer devoted entirely to \nfinishing his thesis. After earning a B.S. in Computer Science and a B.S. in Philosophy\nfrom Rensselaer Polytechnic Institute, Michael went on to earn an M.S. in Computer\nScience from DigiPen Institute of Technology while cutting his teeth in the industry at\nAmaze Entertainment. Michael is currently employed as an artificial intelligence and\ngameplay programmer at Big Huge Games.\nRobert (Kirk) DeLisle\nRobert (Kirk) DeLisle started programming in the early 1980s and has always had an\ninterest in artificial intelligence, numerical analysis, and algorithms. During graduate\nschool, he developed applications for his laboratory that were used for analysis of\nmolecular biological data and were distributed internationally. Currently, he works as\na Computational Chemist developing and applying artificial intelligence methods to\ncomputer-aided drug design and cheminformatics. He is author of a number publica-\ntions and is named as co-inventor of various patents in the fields of computational\nchemistry and drug development.\nMichael Delp\nMichael is the Lead Artificial Intelligence Engineer at WXP Inc. in Seattle where he\nbuilt an FPS AI from scratch in four months, which won critical acclaim. He’s been\nan AI, physics, and gameplay software engineer throughout his career, including work\non FPS, sports, and vehicle AI at small companies, like his current one, as well as large\nones like EA and Sega. He has also lectured at the Game Developers Conference and\ntaught an AI course for the University of Washington Extension. He earned his Com-\nputer Science degree at UC Berkeley.\nCarlos Dietrich\nCarlos Augusto Dietrich received a B.S. in Computer Science from the Federal Univer-\nsity of Santa Maria, Brazil, and an M.S. in Computer Science from the Federal Univer-\nsity of Rio Grande do Sul, Brazil. His research interests include graphics, visualization,\nand the use of GPUs as general purpose processors. He is currently a third-year Ph.D.\nstudent working in the Computer Graphics Group at the Federal University of Rio\nGrande do Sul, Brazil.\nContributor Bios\nxix\n\n\nJoão Dihl\nJoão Luiz Dihl Comba received a B.S. in Computer Science from the Federal Univer-\nsity of Rio Grande do Sul, Brazil, and an M.S. in Computer Science from the Federal\nUniversity of Rio de Janeiro, Brazil. After that, he received a Ph.D. in Computer Sci-\nence from Stanford University. He is an associate professor of computer science at the\nFederal University of Rio Grande do Sul, Brazil. His main research interests are in\ngraphics, visualization, spatial data structures, and applied computational geometry.\nHis current projects include the development of algorithms for large-scale scientific\nvisualization, data structures for point-based modeling and rendering, and general-\npurpose computing using graphics hardware. He is a member of the ACM\nSIGGRAPH. \nPriyesh N. Dixit\nPriyesh N. Dixit is a games researcher in the Department of Computer Science at The\nUniversity of North Carolina at Charlotte and part of the Game Intelligence Group\n(playground.uncc.edu). His primary focus these days is in contributing to the devel-\nopment of the Common Games Understanding and Learning (CGUL) toolkit. He is\nthe designer and developer of the CGUL PlayerViz and HIIVVE tools. He received\nhis Bachelor of Science in Computer Science from UNC Charlotte and will complete\nhis master’s degree in Spring 2008 with the Game Design and Development certifi-\ncate. His areas of interest are in learning and understanding from playtesting, building\ntools to support interactive artificial intelligence, and working on all types of com-\nputer games.\nJoshua A. Doss\nJoshua Doss started his career at 3Dlabs in the Developer Relations division creating\nseveral tools and samples for the professional graphics developer community. His soft-\nware contributions include ShaderGen, an Open Source application that dynamically\ncreates programmable shaders to emulate most fixed-function OpenGL as well as\nseveral of the first high-level GLSL shaders. Joshua is currently at Intel Corporation\nworking with the Advanced Visual Computing team to create world-class graphics\ntools and samples for game developers.\nNathan Fabian\nNathan is a veteran hobbyist game developer, with 12 years’ experience working\nsatellite projects for Sandia National Labs. He’s often thought about joining the game\nindustry proper, but never taken the plunge. He has tinkered with various game\ntechnologies for over 20 years and cannot decide whether he prefers graphics special\neffects, physics simulation, or artificial intelligence. In the end, he’d really like to make\na game involving 3D sound localization as a key element. Until then, he’s finishing his\nmaster’s degree in Computer Science at the University of New Mexico.\nxx\nContributor Bios\n\n\nMarcus Aurelius Cordenunsi Farias\nMarcus Aurelius Cordenunsi Farias graduated with a degree in Computer Science from\nUniversidade Federal de Santa Maria (UFSM), Brazil in 2004. He obtained his mas-\nter’s degree in Computer Science (Computer Graphics) from Universidade Federal do\nRio Grande do Sul, Brazil (UFRGS), in 2006. Last year, he worked in developing new\ninteraction techniques for casual games for Zupple Games, a startup enterprise in\nBrazil. He has experience in the development of new interaction modalities for games,\nincluding computer vision and noise-detection techniques. Currently, he is working at\nCWI Software company in Porto Alegre, Rio Grande do Sul, Brazil.\nMark France\nMark recently graduated with a B.S. in Computer Games Technology. He is also a co-\nfounder of the independent game development studio “Raccoon Games.”\nBen Garney\nBen Garney has been working at GarageGames since it was eight guys in a tiny two-\nroom office. He sat in the hallway and documented the Torque Game Engine. Since\nthen, he’s done a lot with the Torque family of engines, working on graphics, net-\nworking, scripting, and physics. He’s also helped out on nearly every game from\nGG—most notably Marble Blast Ultra and Zap. More recently, he’s re-learning Flash\nand PHP for an avatar-creation Website. In his spare time, Ben plays piano, climbs\nbuttes, and finds ways to survive living with his fuzzy kitty, Tiffany.\nJulien Hamaide\nJulien started programming a text game on his Commodore 64 at the age of eight.\nHis first assembly programs followed soon after. He has always been self-taught, read-\ning all of the books his parents were able to buy. He graduated four years ago at the\nage of 21 as a Multimedia Electrical Engineer at the Faculté Polytechnique de Mons\nin Belgium. After two years working on speech and image processing at TCTS/Multi-\ntel, he is now working as lead programmer on next-generation consoles at 10Tacle\nStudios Belgium/Elsewhere Entertainment. Julien has contributed several articles to\nthe Game Programming Gems and AI Game Programming Wisdom series.\nAnders Hast\nAnders Hast has been a lecturer in computer science at the University of Gävle since\n1996. In 2004, he received his Ph.D. from Uppsala University based on a thesis about\ncomputationally efficient fundamental algorithms for computer graphics. He has\npublished more than 20 papers in that field. He is currently working part time as\nVisualization Expert at Uppsala Multidisciplinary Center for Advanced Computa-\ntional Science.\nContributor Bios\nxxi\n\n\nJeremy Hayes\nJeremy Hayes is a software engineer in the Advanced Visual Computing group at\nIntel. Before he joined Intel, Jeremy was part of the Developer Relations group at\n3Dlabs. His research interests include procedural content generation (especially ter-\nrain), independent game design, and Dodge Vipers.\nScott Jacobs\nScott Jacobs has been working in the games industry since 1995. Currently, he is a\nSenior Software Engineer at Destineer. Prior to this he worked as a software engineer at\nthe serious games company Virtual Heroes, two Ubisoft studios including Redstorm\nEntertainment, and began in the game development industry at Interactive Magic. He\nalso served as the Network & Multiplayer section editor for Game Programming Gems\n6. He lives in North Carolina with his wife and a house full of creatures. \nThomas Jahn\nThomas Jahn has been studying multimedia engineering at the Hochschule Bremen\nsince 2002. In addition to his studies, he began working for KING Art Entertainment\nin 2005, where he contributed to the development of a turn-based strategy game.\nIntrigued by the tiling characteristics of hexagons, he dedicated his final thesis to\nhexagonal grids. After receiving his degree in Computer Science in the summer of\n2007, Thomas was offered a full-time position at KING Art, Bremen, where he is cur-\nrently working on the adventure game Black Mirror 2.\nAlberto Jaspe\nAlberto Jaspe was born in 1981 in La Coruña, Spain. His interest in computer graph-\nics started with the demoscene movement when he was 15. During his studies of\ncomputer science at the University of La Coruña, he was developing 3D medical\nimage visualization systems at the RNASA-Lab. Since 2003, he has been working as\nsoftware engineer and researcher at the computer graphics group VideaLAB, taking\npart in different projects and publications, from terrain visualization to virtual reality\nand rendering.\nMark Jawad\nMark began programming when he was in the second grade, and never bothered to\nstop. He began his career in the videogame industry when he was 21, and soon spe-\ncialized in Nintendo’s hardware and software platforms. He spent eight years in Los\nAngeles writing many games, tools, and engines for systems like the N64, Game Boy\nAdvance, GAMECUBE, and Nintendo DS. Mark moved to Redmond in 2005 to\njoin Nintendo of America’s developer support group, where he currently serves as the\nteam’s Technical Leader. In his spare time, he enjoys studying compilers and runtime\nsystems, and spending time with his family.\nxxii\nContributor Bios\n\n\nKrzysztof Kluczek\nKrzysztof was interested in game programming since he was 10. As 3D technology in\ngames began evolving, he became more and more interested in the graphical aspect of\n3D games. He received his master’s degree in Computer Science at Gdansk University\nof Technology. Currently, he is pursuing his Ph.D. degree there, enjoying learning\nnew technologies, making games, and being a part of the gamedev.pl community in\nhis free time.\nDavid L. Koenig\nDavid L. Koenig is a Senior Software Engineer with The Whole Experience, Inc. in\nSeattle. His main focus is on network and resource loading code. He serves as an\ninstructor for the networking and multiplayer programming course at the University\nof Washington. David is a returning author to the Game Programming Gems series.\nHis work has contributed to many titles, including SceneIt? Lights, Camera, Action\n(Xbox 360), Greg Hastings’ Tournament Paintball Max’d (PS2), Tron 2.0 (PC), Chicago\nEnforcer (Xbox), and many more titles. He holds a bachelor’s degree in Computer\nEngineering Technology from the University of Houston. His personal Website is\nhttp://www.rancidmeat.com.\nAdam Lake\nAdam Lake is a Graphics Software Architect in the Software Solutions Group leading\ndevelopment of a Graphics SDK at Intel. Adam has held a number of positions dur-\ning his nine years at Intel, including research in non-photorealistic rendering and\ndelivering the shockwave3D engine. He has designed a stream-programming archi-\ntecture that included the design and implementation of simulators, assemblers, com-\npilers, and programming models. Previous to working at Intel, he obtained an M.S. in\ncomputer graphics at UNC-Chapel Hill and worked in the Computational Science\nmethods group at Los Alamos National Laboratory. More information is available at\nwww.cs.unc.edu/~lake/vitae.html. He has several publications in computer graphics,\nand has reviewed papers for SIGGRAPH, IEEE, and several book chapters on com-\nputer graphics, and has over 35 patents or pending patents in computer graphics and\ncomputer architecture. In his spare time he is a mountain biker, road cyclist, hiker,\ncamper, avid reader, snowboarder, and Sunday driver.\nDimitar Lazarov\nDimitar Lazarov is a senior software engineer at Luxoflux (an Activision owned stu-\ndio). He has more than 10 years of experience in the game industry and has worked\non a variety of games, ranging from children-oriented titles such as Tyco RC, Casper,\nand Kung Fu Panda, to more mature titles such as Medal of Honor and True Crime. He\nconsiders himself a generalist programmer with passion for graphics, special effects,\nContributor Bios\nxxiii\n\n\nanimations, system libraries, low-level programming, and optimizations. He also has\na recurring dream of writing his own programming language one day. In his spare\ntime, Dimitar likes to read books, travel, play sports, and relax on the beach with his\nwife.\nMartin Linklater\nMartin Linklater is currently a lead programmer at SCEE Liverpool (UK). He has 14\nyears of experience in the games industry and has worked on many titles, including\nWipeout HD, Wipeout Pure, and Quantum Redshift.\nChris Lomont\nChris Lomont, http://www.lomont.org, is a research scientist working on government\nsponsored projects at Cybernet Systems Corporation in Ann Arbor. He is currently\nresearching image processing for NASA and designing hardware/software to prevent\nmalware from infecting PCs for Homeland Security. At Cybernet, he has worked in\nquantum computing, has taught advanced C++ programming, and has unleashed\nchaos. Chris specializes in algorithms and mathematics, rendering, computer security,\nand high-performance scientific computing. After obtaining a triple B.S. in math,\nphysics, and computer science, he spent several years programming in Chicago. He\neventually left the video game company in Chicago for graduate school at Purdue,\nWest Lafayette, resulting in a Ph.D. in mathematics, with additional graduate course-\nwork in computer science. His hobbies include building gadgets (http://www.hyp-\nnocube.com), playing sports, hiking, biking, kayaking, reading, learning math and\nphysics, traveling, watching movies with his wife Melissa, and dropping superballs on\nunsuspecting coworkers. He has two previous gems.\nJörn Loviscach\nJörn Loviscach has been a professor of computer graphics, animation, and simulation\nat Hochschule Bremen (University of Applied Sciences) since 2000. Before that, he\nwas deputy editor-in-chief at c’t computer magazine in Hanover, Germany. Jörn also\npossesses a Ph.D. in Physics. Since his return to academia he has contributed to GPU\nGems, Shader X3, Shader X5, and Game Programming Gems 6. Additionally, he has\nauthored and co-authored a number of works on computer graphics and interactive\ntechniques presented at conferences such as Eurographics and SIGGRAPH. \nMichael F. Lynch, Ph.D.\nDr. Lynch’s background is in Electrical Engineering where he worked in various tech\njobs before returning to grad school at an advanced age, where he crossed the great\nchasm over to the social sciences. Today he is a member of the faculty for the new\nGames and Simulations Arts and Sciences (GSAS) major at Rensselaer Polytechnic\nxxiv\nContributor Bios\n",
      "page_number": 17,
      "chapter_number": 3,
      "summary": "This chapter covers segment 3 (pages 17-25). Key topics include game, developed, and programs. Previously, he was a lead programmer at Burut CT/World Forge, the\nfounder of the X-Tend gaming technology used in numerous releases, including the\nrecent UberSoldier and Sparta: Ancient Wars.",
      "keywords": [
        "Computer Science",
        "computer graphics",
        "computer",
        "Game",
        "Science",
        "Game Programming Gems",
        "graphics",
        "Game Programming",
        "Computer Graphics Group",
        "University",
        "Computer Games Technology",
        "Federal University",
        "software engineer",
        "programming",
        "applied computer graphics"
      ],
      "concepts": [
        "game",
        "developed",
        "programs",
        "including",
        "include",
        "graphical",
        "computationally",
        "computing",
        "worked"
      ],
      "similar_chapters": [
        {
          "book": "Game_Engine_Architecture",
          "chapter": 42,
          "title": "Segment 42 (pages 849-853)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "makinggames",
          "chapter": 38,
          "title": "Segment 38 (pages 332-339)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 9,
          "title": "Segment 9 (pages 160-181)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 13,
          "title": "Segment 13 (pages 115-123)",
          "relevance_score": 0.42,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 2,
          "title": "Segment 2 (pages 19-41)",
          "relevance_score": 0.42,
          "method": "api"
        }
      ]
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 26-33)",
      "start_page": 26,
      "end_page": 33,
      "detection_method": "topic_boundary",
      "content": "Institute in Troy, New York, where he teaches “A History and Culture of Games” and\na forthcoming course in AI for games. In his spare time, he does a little gourmet cook-\ning, plays strange electronic music, and occasionally samples the Interactive Story-\ntelling kool-aid (and plays games, too).\nJosé Gilvan Rodrigues Maia\nJosé Gilvan Rodrigues Maia is a doctoral student in the Department of Computing at\nthe Federal University of Ceará (UFC) in Brazil. He received a B.S. and an M.S. in\nComputer Science from the Department of Computing at the Federal University of\nCeará (UFC) in Brazil. During his M.S., he spent two years working with computer\ngame technologies, specially rendering and collision detection. His current research\ninterests are computer graphics, computer games, and computer vision. He has been\nworking on research projects at the Federal University of Ceará (UFC).\nJoris Mans\nJoris Mans has a master’s in Computer Science from the Free University, Brussels.\nCombining the knowledge gathered during his education and the hands-on experience\nas a developer in the demo-scene allowed him to start working in the games industry\ndirectly after graduating. He currently works as a lead programmer at 10Tacle Studios\nBelgium, poking around with stuff ranging from console data cache optimization to\ndatabase backends.\nEnric Martí\nEnric Martí joined the Computer Science department at the Universitat Autònoma\nde Barcelona (UAB) in 1986. In 1991, he received a Ph.D. at the UAB with a thesis\nabout the analysis of handmade line drawings as 3D objects. That same year he also\nbecame an associate professor. Currently, he is a researcher at the Computer Vision\nCenter (CVC) and is interested in document analysis, and more specifically, Web\ndocument analysis, graphics recognition, computer graphics, mixed reality and\nhuman computer interaction. He is working on text segmentation in low-resolution\nWeb images (for example, CIF and JPEG) using wavelets to extract text information\nfrom Web pages. Additionally, he is working on the development of a mixed-reality\nenvironment with 3D interfaces (data glove and optical lenses). He is also the\nreviewer of the Computer & Graphics and Electronic Letters on Computer Vision and\nImage Analysis (ELCVIA) journals.\nColt McAnlis\nColt “MainRoach” McAnlis is a graphics programmer at Microsoft Ensemble Studios\nspecializing in rendering techniques and systems programming. He is also an adjunct\nprofessor at SMU’s Guildhall school of game development where he teaches advanced\nContributor Bios\nxxv\n\n\nrendering and mathematics courses. After receiving an advanced degree from the\nAdvanced Technologies Academy in Las Vegas Nevada, Colt earned his B.S. in Com-\nputer Science from Texas Christian University.\nCurtiss Murphy\nCurtiss Murphy has been developing and managing software projects for 15 years. As a\nproject engineer, he leads the design and development of several Serious Game efforts\nfor a variety of Marine, Navy, and Joint government and military organizations. Cur-\ntiss routinely speaks at conferences with the intent to help the government leverage\nlow-cost, game-based technologies to enhance training. Recent game efforts include a\ndozen projects based on the Open Source gaming engine, Delta3D (www.delta3d.org)\nand a public affairs game for the Office of Naval Research based on America’s Army.\nCurtiss holds a B.S. in Computer Science from Virginia Polytechnic University and\ncurrently works for Alion Science and Technology in Norfolk, Virginia.\nLuciana Nedel\nLuciana Porcher Nedel received a Ph.D. in Computer Science from the Swiss Federal\nInstitute of Technology in Lausanne, Switzerland, under the supervision of Prof.\nDaniel Thalmann in 1998. She received an M.S. in Computer Science from the Fed-\neral University of Rio Grande do Sul, Brazil, and a B.S. in Computer Science from the\nPontifical Catholic University, Brazil. During her sabbatical in 2005, she spent two\nmonths at the Université Paul Sabatier in Toulouse, France, and two months at the\nUniversité Catholique de Louvain in Louvain-la-Neuve, Belgium, doing research on\ninteraction. She is an assistant professor at the Federal University of Rio Grande do\nSul, Brazil. Since 1991 she has been involved in computer animation research and\nsince 1996 she has been doing research in virtual reality. Her current projects include\ndeformation methods, virtual humans simulation, interactive animation, and 3D\ninteraction using virtual reality devices. \nKen Noland\nKen Noland is a programmer at Whatif Productions and has worked on several inter-\nnal projects that extend the technology and the infrastructure of the proprietary game\nengine. His focus tends to be on audio, networking, and gameplay, and he also works\non the code behind the elusive content-driven technology that drives the Whatif\nengine, also known as the WorLd Processor. He’s been involved in the games industry\nin one form or another for over six years. When not actively working, you can gener-\nally find him walking around town in a daze wondering what to do with this thing\ncalled “free time.”\nxxvi\nContributor Bios\n\n\nJason Page\nJason Page has worked in the computer games industry since 1988 and has held job\npositions of games programmer, audio programmer, “musician” (audio engineer/\ncontent creator), and currently is the Audio Manager at Sony Computer Entertain-\nment Europe’s R&D division. This role includes managing, designing, and program-\nming various parts of the PlayStation 3 audio SDK libraries (the PS3 audio library\n“MultiStream” is used by many developers worldwide), as well as supporting develop-\ners on all PlayStation platforms with any audio issues. Of course, none of this would\nbe possible without the hard work of his staff—so thanks also to them. Jason’s past\nwork as a audio content creator includes music and sound effects for titles such as\nRainbow Islands, The Chaos Engine, Sensible World of Soccer, Cool Boarders 2, and Gran\nTurismo. Although he no longer creates music for games, he still writes various pieces\nfor events such as the SCEE DevStation conference. Finally, he would like to say\nthank you to his wife, Emma, for putting up with his constant “I’ve just got to answer\na developer’s email” when at home. Jason personally still blames the invention of\nlaptops and WiFi.\nVitor Fernando Pamplona\nVitor Fernando Pamplona received his B.S. in Computer Science from Fundação\nUniversidade Regional de Blumenau. Since 2006 he has been a Ph.D. student at Uni-\nversidade Federal do Rio Grande do Sul, Brazil. His research interests are in computer\ngraphics, agile development, and free software. He also manages the java virtual com-\nmunity called JavaFree.org and leads seven free software projects.\nSteve Rabin\nSteve is a Principal Software Engineer at Nintendo of America, where he researches\nnew techniques for Nintendo’s next generation systems, develops tools, and supports\nNintendo developers. Before Nintendo, Steve worked primarily as an AI engineer at\nseveral Seattle start-ups including Gas Powered Games, WizBang Software Produc-\ntions, and Surreal Software. He managed and edited the AI Game Programming Wis-\ndom series of books, the book Introduction to Game Development, and has over a dozen\narticles published in the Game Programming Gems series. He’s spoken at the Game\nDevelopers Conference and currently moderates the AI roundtables. Steve is an\ninstructor for the Game Development Certificate Program through the University of\nWashington Extension and is also an instructor at the DigiPen Institute of Technol-\nogy. Steve earned a B.S. in Computer Engineering and an M.S. in Computer Science,\nboth from the University of Washington.\nContributor Bios\nxxvii\n\n\nArnau Ramisa\nArnau Ramisa graduated with a degree in Computer Science from the Universitat\nAutònoma de Barcelona and is now working on a Ph.D. in Computer Vision and\nArtificial Intelligence at the Institut d’Investigació in Intelligència Artificial. He is\ninterested in computer vision, augmented reality, human-machine interfaces and, of\ncourse, video games.\nMike Ramsey\nMike Ramsey is the principle programmer and scientist on the GLR-Cognition\nEngine. After earning a B.S. in Computer Science from MSCD, Mike has gone on to\ndevelop core technologies for the Xbox 360 and PC. He has shipped a variety of\ngames, including Men of Valor (Xbox and PC), Master of the Empire, and several Zoo\nTycoon 2 products, among others. He has also contributed multiple gems to both the\nGame Programming Gems and AI Wisdom series. Mike’s publications can be found at\nhttp://www.masterempire.com. He also has an upcoming book entitled, A Practical\nCognitive Engine for AI. In his spare time, Mike enjoys strawberry and blueberry pick-\ning and playing badminton with his awesome daughter Gwynn!\nGraham Rhodes\nGraham Rhodes is a principal scientist at the Southeast Division of Applied Research\nAssociates, Inc., in Raleigh, North Carolina. Graham was the lead software developer\nfor a variety of Serious Games projects, including a series of sponsored educational\nmini-games for the World Book Multimedia Encyclopedia; and more recently,\nfirst/third-person action/role-playing games for industrial safety and humanitarian\ndemining training. He is currently involved in developing software that provides pro-\ncedural modeling and physics-based solutions for simulation and training. Graham\ncontributed articles to several books in the Game Programming Gems series, and\nauthored the section on real-time physics for Introduction to Game Development. He \nis the moderator and frequent contributor to the math and physics section at\ngamedev.net, has presented at the annual Game Developer’s Conference (GDC) and\nother industry events, and regularly attends GDC and the annual ACM/SIGGRAPH\nconference. He is a member of ACM/SIGGRAPH, the International Game Devel-\noper’s Association (IGDA), and the North Carolina Advanced Learning Technologies\nAssociation (NC ALTA).\nTimothy E. Roden\nTimothy Roden is an associate professor and head of the Department of Computer\nScience at Angelo State University in San Angelo, Texas. He teaches courses in game\ndevelopment, computer graphics, and programming. His research interests include\nxxviii\nContributor Bios\n\n\nentertainment computing with a focus on procedural content creation. His published\npapers in entertainment computing appear in ACM Computers in Entertainment,\nInternational Conference on Advances in Entertainment Technology, the International\nConference on Entertainment Computing and Microsoft Academic Days on Game\nDevelopment Conference. He also contributed to Game Programming Gems 5. Before\njoining academia, Roden spent 10 years as a graphics software developer in the simu-\nlation industry.\nRahul Sathe\nRahul P. Sathe is working as a software engineer in the Advanced Visual Computing\nGroup at Intel Corp where he is developing an SDK for next generation high-end\ngraphics hardware. He’s currently involved in designing advanced graphics algorithms\nfor game developers. Earlier in his career, he worked on various aspects of CPU archi-\ntecture and design. He holds a bachelor’s degree from Mumbai University (1997) in\nElectronics Engineering and his master’s in Computer Engineering from Clemson\nUniversity (1999). He has prior publications in the computer architecture area. His\ncurrent interests include graphics, mathematics, and computer architecture.\nStephan Schütze\nStephan Schütze currently resides in Tokyo where he is working in both the anime\nand game industries while trying to learn both the Japanese language and its culture.\nSee Stephan@stephanschutze.com and www.stephanschutze.com.\nAntonio Seoane\nAntonio Seoane is a researcher who works in “VideaLAB” (http://videalab.udc.es), the\nVisualization For Engineering, Architecture, and Urban Design Group of the Univer-\nsity of A Coruña, in Spain. He has been working for 10 years in the research and devel-\nopment of real-time graphics applications focused on topics like simulation, civil\nengineering, urban design, cultural heritage, terrain visualization, and virtual reality.\nMain research interests and experiences are terrain visualization (http://videalab.udc.\nes/santi/), and in recent years working on GPGPU and Artificial Intelligence.\nDillon Sharlet\nDillon Sharlet is an undergraduate student in Mathematics and Electrical Engineer-\ning at the University of Colorado at Boulder. He has been interested in computer\ngraphics for years. He likes exploring new algorithms in graphics. In his free time, he\nlikes to go climbing and skiing.\nContributor Bios\nxxix\n\n\nGary Snethen\nGary’s passion for computing began before he had access to a computer. He taught\nhimself to program from examples in computer magazines, and wrote and “ran” his\nfirst programs using pencil and paper. When his parents balked at buying a computer,\nGary set out to create his own. When his parents discovered hand-drawn schematics\nfor functional digital adders and multipliers, they decided it was more than a passing\ninterest and purchased his first computer. Gary’s early interests were focused heavily\non games and 3D graphics. Gary wrote his first wireframe renderer at age 12, and\nspent many late nights throughout his teens writing one-on-one modem games to\nshare with his friends.\nGary is currently a principal programmer at Crystal Dynamics, where he shipped\nLegacy of Kain: Defiance and Tomb Raider Legend. Gary’s current professional interests\ninclude constrained dynamics, advanced collision detection, physics-based anima-\ntion, and techniques for improving character fidelity in games.\nRobert Sparks\nThis is Robert’s seventh year programming audio in the games industry. Most recently\nhe was a technical lead on Scarface: The World Is Yours (Radical Entertainment, Van-\ncouver, Canada). His past game credits include The Simpsons: Hit & Run, The Simp-\nsons: Road Rage, The Hulk, Tetris Worlds, Dark Angel, and Monsters, Inc.\nDiana Stelmack\nDiana Stelmack works at Red Storm Entertainment, where she has worked on the\nGhost Recon series since Spring 2001. Since the PC version of this title, Xbox Live\nOnline services and multiplayer game system support have become her development\nfocus. Prior to games, her network background included telecommunications, IP\nSecurity, and DoD Network communications. Just remember “If you stumble, make\nit look like part of the dance”!\nJavier Taibo\nJavier Taibo graduated in computer science at the University of Coruña in 1998. He\nhas worked on computer graphics R&D projects in the “Visualization for Engineer-\ning, Architecture and Urban Design Group” (a.k.a. VideaLAB). The fields he has\nbeen working on include real-time 3D terrain rendering and GIS visualization, virtual\nreality, and panoramic image and video. At present he is also teaching computer ani-\nmation and 3D interaction at the University of A Coruña.\nxxx\nContributor Bios\n\n\nDaniela Gorski Trevisan\nDaniela Gorski Trevisan graduated with a degree in Informatics from Universidade\nFederal de Santa Maria (UFSM), Brazil, in 1997. She obtained her master’s degree in\nComputer Science (Computer Graphics) from Universidade Federal do Rio Grande do\nSul (UFRGS), Porto Alegre, Brazil, in 2000. She earned her Ph.D. degree in applied\nsciences from Université catholique de Louvain (UcL), Belgium, in 2006. Nowadays\nshe is a CNPq researcher and member of the Computer Graphics group of Informatics\nInstitute at UFRGS. Her research interest topics are focused on the development and\nevaluation of alternative interaction systems, including multimodal, augmented, and\nmixed reality technologies.\nIskander Umarov\nIskander Umarov (umarov@trusoft.com) is the technical director of TruSoft (www.\ntrusoft.com). TruSoft develops behavior-capture AI technologies and provides AI\nmiddleware and consulting services to other companies to implement innovative AI\nsolutions for computer and video games and simulation applications on PC, PlaySta-\ntion 2, PlayStation 3, and Xbox 360 platforms. Mr. Umarov authored the original\nideas behind TruSoft’s Artificial Contender AI technology. Artificial Contender pro-\nvides behavior-capture AI agents for games and simulation applications. These AI\nagents capture human behavior, learn, and adapt. Mr. Umarov is currently responsible\nfor managing development of TruSoft’s AI solutions and leads TruSoft’s R&D efforts,\nincluding joint projects with Sony, Electronic Arts, and Lockheed Martin. Mr. Umarov\nholds a B.S. in Applied Mathematics and an M.S. in Computer Science, both from\nMoscow Technological University, where he specialized in instance-based learning\nmethods and graph theory.\nEnric Vergara\nEnric Vergara is a computer engineer at the Universitat Autònoma de Barcelona and\nrecently received a master’s degree in Video Games Creation at Pompeu Fabra Univer-\nsity. He now works as a C++ programmer for GeoVirtual.\nCreto Augusto Vidal\nCreto Augusto Vidal is an associate professor of computer graphics in the Department\nof Computing at the Federal University of Ceará in Brazil. He received his Ph.D. in\ncivil engineering from the University of Illinois at Urbana-Champaign in 1992 and\nwas a post-doctoral research associate at the Department of Mechanical and Industrial\nEngineering at the University of Illinois at Urbana-Champaign from 1992 to 1994.\nHe is currently a visiting researcher at VRLab at EPFL (École Polytechnique Fédérale\nContributor Bios\nxxxi\n\n\nde Lausanne) in Switzerland. His current research interests are computer graphics, vir-\ntual reality applications, and computer animation. He has been the coordinator of\nseveral government-sponsored projects investigating the use of networked virtual\nenvironments for training and education.\nJon Watte\nIn addition to his job as CTO of serious virtual world platform company Forterra\nSystems, Jon is a regular contributor to the independent game development commu-\nnity. As a Microsoft DirectX/XNA MVP and moderator of the Multiplayer and Net-\nworking forum on the independent games site GameDev.Net, he enjoys sharing\nexperience from his time in the business, with his earliest shipping title a cassette-\nbased game for the Commodore VIC 20. Prior to heading up Forterra Systems, Jon\nworked on products such as There.com, BeOS, and Metrowerks CodeWarrior.\nG. Michael Youngblood\nG. Michael Youngblood, Ph.D., is an Assistant Professor in the Department of Com-\nputer Science at The University of North Carolina at Charlotte, co-director of the Games\n+ Learning Lab, and head of the Games Intelligence Group (playground.uncc.edu). His\nwork studies how artificial agents and real people interact in virtual environments,\nincluding computer games and high-fidelity simulations in order to understand the ele-\nments and patterns of learning for the development of better artificial agents. He has\nworked with real-time computer games, intelligent environments, and robotics since\n1997. His research interests are in interactive artificial intelligence, entertainment com-\nputing, and intelligent systems.\nxxxii\nContributor Bios\n",
      "page_number": 26,
      "chapter_number": 4,
      "summary": "This chapter covers segment 4 (pages 26-33). Key topics include games, gaming, and include. He currently works as a lead programmer at 10Tacle Studios\nBelgium, poking around with stuff ranging from console data cache optimization to\ndatabase backends.",
      "keywords": [
        "Computer Science",
        "Computer",
        "computer graphics",
        "computer games",
        "Games",
        "Game Programming Gems",
        "University",
        "Federal University",
        "Computer Science department",
        "Science",
        "game development",
        "Gilvan Rodrigues Maia",
        "computer vision",
        "Game Programming",
        "computer game technologies"
      ],
      "concepts": [
        "games",
        "gaming",
        "include",
        "including",
        "graphics",
        "engineer",
        "engineering",
        "developer",
        "university",
        "working"
      ],
      "similar_chapters": [
        {
          "book": "Game_Engine_Architecture",
          "chapter": 42,
          "title": "Segment 42 (pages 849-853)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 2,
          "title": "Segment 2 (pages 19-41)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 1,
          "title": "Segment 1 (pages 1-13)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 25,
          "title": "Segment 25 (pages 240-248)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 34-47)",
      "start_page": 34,
      "end_page": 47,
      "detection_method": "topic_boundary",
      "content": "1\nS E C T I O N\n1\nGENERAL\nPROGRAMMING\n\n\nThis page intentionally left blank \n\n\n3\nIntroduction\nAdam Lake, Graphics Software Architect,\nAdvanced Visual Computing Group (AVC), Intel\nadam.t.lake@intel.com\nG\name development continues to undergo significant changes in every aspect. Extract-\ning performance in previous generations of hardware meant a focus on scheduling\nassembly instructions, exploiting small vector instructions (SSE, for example), and\nensuring data remains in registers or cache while processing. Although these issues are\nstill relevant, they are now secondary relative to exploiting the multithreaded hardware\nin current generation consoles and PCs. To this end, we have included articles on tools\nand techniques for game programmers to take advantage of this new hardware: Design\nand Implementation of a Multi-Platform Threading Engine by Michael Ramsey, an imple-\nmentation of a Multithread Job and Dependency System by Julien Hamaide, and a\nDeferred Function Call Invocation System by Mark Jawad. These issues have become\nmore prevalent in the time since the last Game Programming Gems publication.\nIn the category of systems we have three articles. Two systems software articles\ninclude High Performance Heap Allocator by Dimitar Lazarov and Efficient Cache\nReplacement Using the Age and Cost Metrics by Colt McAnlis. Martin Fleisz also brings\nus an article on Advanced Debugging Techniques. Martin covers issues related to excep-\ntion handling, stack overflows, and memory leaks—common issues we all encounter\nduring application development. \nWe also have an exciting set of articles in this edition related to user interfaces for\ngames. Carlos Dietrich et al. have an article on sketch-based interfaces for real-time\nstrategy games. Arnau Ramisa et al. have written an article on optical flow for video\ngames played with a Webcam, and Marcus C. Farias et al, have described a new first-\nperson shooter interface in Foot Navigation Technique for First-Person Shooting Games.\nFinally, a wonderful paper on using hexagonal tiling instead of a traditional square grid\nis presented by Thomas Jahn and Jörn Loviscach entitled For Bees and Gamers: How to\nHandle Hexagonal Tiles.\nEach of these authors brings to the table his or her own unique perspective, per-\nsonality, and vast technical experience. My hope is that you benefit from these articles\nand that these authors inspire you to give back when you too have a gem to share.\nEnjoy.\n\n\nThis page intentionally left blank \n\n\n5\n1.1\nEfficient Cache Replacement\nUsing the Age and Cost\nMetrics\nColt “MainRoach” McAnlis\nMicrosoft Ensemble Studios\ncmcanlis@ensemblestudios.com\nI\nn memory-constrained game environments, custom media caches are used to amplify\nthe amount of data in a scene, while leaving a smaller memory footprint than contain-\ning the entire media in memory at once. The most difficult aspect of using a cache sys-\ntem is identifying the proper victim page to vacate when the cache fills to its upper\nbounds. As cache misses occur, the choice of page-replacement algorithm is essential—\nthis choice is directly linked to the performance and efficiency of hardware memory\nusage for your game. A bad algorithm will often destroy the performance of your title,\nwhereas a well implemented algorithm will enhance the quality of your game by a sig-\nnificant factor, without affecting performance. Popular cache-replacement algorithms,\nsuch as LRU, work well for their intended environment, but often struggle in situations\nthat require more data to make accurate victim page identifications. This gem presents\nthe Age and Cost metrics to be used as values in constructing the cache-replacement\nalgorithm that best fits your game’s needs.\nOverview\nWhen data is requested from main memory, operating systems will pull the data into\na temporary area of memory (called a cache), which can be accessed at a faster speed\nthan main memory. The cache itself is a predefined size, segmented into smaller sets\nof memory called pages. As memory accesses occur, the cache itself can get filled, at\nwhich time the operating system must choose a page from the cache in which to\nreplace with the incoming page data. This occurrence is called a cache miss. When the\namount of needed pages exceeds the size of the cache by a significant amount (usually\n2x or more) a thrash occurs, that is, the entire cache will be dumped in order to make\nroom for the entirety of the incoming data set. Thrashing is considered the worst-case\nscenario for any caching algorithm, and is the focal point of any performance testing\nof cache replacements.\n\n\n6\nSection 1\nGeneral Programming \nEach time the memory handler has to fetch information from main memory,\nthere is an associated cost involved with it. Reading from the cached memory has a\nsmaller cost, usually due to introduction of an extra memory chip closer to the proces-\nsor itself. So, in determining which page to dump from memory when a cache miss\noccurs, one of the key goals is to pick a page that does not need an active spot in the\ncache. For instance, if you randomly chose a page to evict during the fault, and that\npage happens to be needed immediately after its eviction, you would incur another\nperformance overhead to re-fetch that data from the main memory. The goal is to\ncreate an algorithm that best describes which page is ideal to remove from the cache\nto reduce the amount of cache misses and performance burden. \nThese types of algorithms, called victim page determination or page-replacement\nalgorithms were a hot topic in the 1960s and 1970s, and reached a plateau with the\nintroduction of the Least Recently Used (LRU) algorithm (as well as other working-\nset systems). Since that time, derivations of these algorithms have been generated \nin order to address some of the issues that LRU presents when working in specific\nsubject areas. For example, [O’Neil93] described an offshoot of LRU, called LRU-K,\nwhich was described to work more efficiently in software database systems. Adaptive\nReplacement Cache (ARC) is an algorithm developed by IBM used both in hardware\ncontrollers as well as other popular database systems [Megiddo03]. (See the “Refer-\nences” section at the end of this gem for more information.)\nIn game development, programmers have to deal with caches both on the hard-\nware and on the software level, especially in the game console arena where program-\nmers constantly struggle to increase the amount of content in the game while still\nfitting within memory constraints. Like many other replacement algorithms tailor-\nmade to solve a specific problem, there are common game and graphics systems that\nrequire a replacement system that better resembles the memory patterns and usage\nmodels. This gem describes two cache page metrics that can be interweaved together\nin a way that better fits into the video game development environment.\nCache-Replacement Algorithms\nSince their creation, cache-replacement systems have been an active area of research,\nresulting in a large number of various algorithms custom tuned to solve various\ninstances of the problem space. In order to have a frame of reference, I’ll cover a few\nof the most common algorithms here.\nBelady’s Min (OPT)\nThe most efficient replacement algorithm would always replace the page that would\nnot be needed for the longest period of time since its eviction from the cache. Imple-\nmenting this type of algorithm in a working system would require the foreknowledge\nof system usage, which would be impossible to define. The results of implementing\nOPT in test situations where the inputs over time are known can be used as a bench-\nmark to test other algorithms.\n\n\nIn a multithreaded environment, where there is a producer-consumer architec-\nture between threads, it is possible to get close to OPT using information from the\nproducer thread if the producer thread is multiple frames ahead of the consumer.\nLeast Recently Used (LRU)\nThe LRU algorithm replaces the page that hasn’t been used for the longest amount of\ntime. When a new page is loaded into the cache, data is kept per page that represents\nhow long since the given page has been used. Upon a cache miss, the victim page is\nthen the one that hasn’t been used in the longest time span. LRU does some cool\nthings, but is prone to excessive thrashing. That is, you’ll always have an oldest page in\nthe cache, which means that unless you’re careful, you can actually clear the entire\ncache when workloads are large.\nMost Recently Used (MRU)\nMRU replaces the page that was just replaced. That is, the youngest page in the cache.\nMRU will not replace the entire cache; rather, during heavy thrashing it will always\nchoose to replace the same page. Although not as popular and robust as LRU, MRU\nhas its uses. \nIn [Carmack00], John Carmack lists a nice hybrid system between LRU and\nMRU for texture cache page replacement. In short, it works in the form that you use\nLRU most of the time, until you reach the point that you need to evict an entry every\nframe, at which time you switch to an MRU replacement policy. As mentioned previ-\nously, LRU has the problem that it will potentially thrash your entire cache if not\nenough space is available. Swapping to MRU at the point you would begin to thrash\nthe cache creates a scratch pad in memory for one page, leaving most of the cache\nunharmed. This can be useful if the amount of textures being used between frames is\nrelatively low. When you’re dealing with a situation in which the extra pages needed\nare double the available cache size this method degenerates; this might stall the ren-\ndering process.\nNot Frequently Used (NFU)\nThe not frequently used (NFU) page-replacement algorithm changes the access heuristic\nto keep a running counter of accesses for every page in the cache. Starting at zero, any\npage accessed during the current time interval will increase their counter by one. The\nresult is a numeric qualifier referencing how often a page has been used. To replace a page,\nyou then must look for the page with the lowest counter during the current interval.\nThe most serious problem with this system is that the counter metric does not\nkeep track of access patterns. That is, a page that was used heavily upon load and hasn’t\nbeen used since then can have the same count as a page used every other frame for the\nsame time interval. The information needed to differentiate these two usage patterns\nis not available from a single variable. The Age metric, presented in the next section,\n1.1\nEfficient Cache Replacement Using the Age and Cost Metrics\n7\n\n\nis a modification of NFU that changes how the counter is represented, so that you can\nderive extra information from it during thrashing. \nFor a more expansive list of algorithms, hit up your favorite search engine with\n“Page Replacement Algorithms,” or dust off your operating systems textbook from\ncollege.\nAge and Cost Metrics\nFor the purpose of illustration, the rest of the gem references a problem facing most\ngames in the current generation of hardware: the design of a custom texture cache. To\ndo this, you will reserve a static one-dimensional array of cache pages into which data\ncan be loaded and unloaded. Assume that you are working with a multi-dimensional\ntexture cache; that is, the data that you’re placing in the cache is of texture origin. The\ncache is multi-dimensional in the sense that multiple textures of various sizes could fit\ninto a single cache page. For example, if the cache page size fits a 256 \u0002 256 texture,\nyou can also support four 64 \u0002 64 textures, 16 32 \u0002 32 textures, and so on, includ-\ning multiples of each size existing in the same page in harmony. \nEven in this simple example, you have already laid the groundwork for standard\nreplacement functions to under-perform. Consider the case of the multi-dimensional\ncache where you need to insert a new 256 \u0002 256 page into the cache when it is\nentirely filled with only 32 \u0002 32 textures. Simple LRU/MRU schemes do not have\nthe required data available to properly calculate which full cache page is the optimal\none to replace and which group of 32 \u0002 32 textures needs to be dumped as the access\npatterns depend greatly on more than the time at which the page was last replaced. To\nthis purpose, a new set of replacement metrics are presented in order to better analyze\nthe best pages to replace when in such a situation. \nThe Age Algorithm\nThe OPT algorithm knows the amount of usage for a page in the cache and replaces\nthe one that will be used the farthest away in time. Most replacement algorithms\nattempt their best to emulate this algorithm with various data access patterns. The Age\nalgorithm emulates this process by keeping a concept of usage over the previous frames\nin order to best predict future usage; that is, you need to keep track of how many times\na page has been accessed in a window of time. To accomplish this task, every page in\nthe cache keeps a 32-bit integer variable that is initialized to 1 (0x00000001) when the\npage first enters the cache.\nEvery frame, all active pages in the cache are bit-shifted left by one bit, signifying\ntheir deprecation over time. If an active page is used in this frame, the least significant\nbit of the Age variable is set to one (1); otherwise, it is set to zero (0). This shifting and\nsetting pattern allows you to keep a usage evaluation for the past 32 frames.\nFor example, a page that is accessed every other frame would have an Age variable\n0xAAAAAAAA (010101010….01), whereas a page that was accessed heavily when it\n8\nSection 1\nGeneral Programming \n\n\nwas first loaded, and has not been used since, would have an Age variable\n0xFFFF0000 (1111…1100000…00).\nTo show how the Age variable evolves over time, consider a page that is used in the\nfirst, third, fourth, and eighth frames over an eight-frame window. The Age variable\nwould change like such:\nframe1 - 00000001 (used)\nframe2 - 00000010 (not used)\nframe3 - 00000101 (used)\nframe4 - 00001011 (used)\nframe5 - 00010110 (not used)\nframe6 - 00101100 (not used)\nframe7 - 01011000 (not used)\nframe8 - 10110001 (used) \nWith this information layout, you can calculate the Age Percentage Cost (APC)\nof the given window. You average the number of frames a page has been used versus\nthe number of pages not used by dividing the number of frames used (ones) by the\ntotal number of frames in the Age variable. This data can be extracted using assembly\nand processor heuristics rather than higher-level code. Although you can represent\nthis data your own way, the APC presented exists as a normalized single value between\n[0, 1], and as described in the “Age and Cost” section, can be used as a scalar against\nother metrics.\nWhen using age to determine the target victim page, you seek to choose the pages\nthat have not been used in a certain time, as well as pages that are not frequently used\nin general. For example, a page that has been accessed every frame in the window will\nhave a 100% APC and will be almost impossible to replace, whereas a page with an\nAPC of 25% will have a higher chance of being replaced. \nWhat I like about Age is that it gives a number of implicit benefits: \n• It biases old textures, forcing textures to prove that they are needed for the scene.\nOnce they prove this fact, they are kept around. Once an APC gets above 50%, it\ngets difficult to release it from the cache. \n• It creates a scratch pad. New textures that haven’t proved they are valuable yet are\nturned into scratch pads. This is a good thing, as new textures can often be tem-\nporary and have a high probability to vanish the next frame \n• It is a modified NRU (not recently used) scheme. Textures that might have been\nvisible a high percentage for a short time could easily shoot up to 50% APC, but\nthen drop out, and slowly work their APC back down over a number of frames.\nAge offers a modified representation of the access variable, and allows extra analy-\nsis, so if the APC > 60%, but the texture hasn’t been used in the most recent X\nframes, you can check for this and eliminate it early. \nThe APC variable as presented so far is powerful, but not without fault—multiple\npages in the cache can have radically different access patterns but have the same APC\nvalue. That is, 0xAAAAAAAA and 0xFFFF0000 have the same usage percentage but\n1.1\nEfficient Cache Replacement Using the Age and Cost Metrics\n9\n\n\nit’s easy to see that the usage patterns for these two Age variables are dramatically differ-\nent. Subsequent analysis patterns on the binary data in the basic Age variable could\nhelp separate those pages with similar APC values (such as analyzing sub-windows for\nsecondary APC values) but fall into similar problems.\nExpanded Age Algorithm\nThe previously presented Age algorithm makes the assumption that you’d like to keep\nthe memory required to deduce page information fairly low; hence storing a used/\nunused flag per frame over a window of 32 frames. It should be noted, however, that\nin situations where the required page amount is larger by a significant factor, the Age\nalgorithm degenerates just as any others would. If, for example, you had 50 textures\nthat were used every frame, and only 12 cache pages in which to put them, there\nwould not be enough space in the cache for your entire memory footprint at once.\nEvery frame would thrash the entire cache, replacing each page.\nIn this situation, the constant loading/reloading of pages and textures would cause\nevery page to have an Age counter set to 1, and thus would lack any additional infor-\nmation that would be helpful in the identification of specific victim pages. To help\nsolve this problem, the Age variable can store more information per frame than just a\nused/unused bit, and, in fact, store the usage count of a texture instead. So rather than\nstoring a 0/1 in a single 32-bit integer variable, you could store a list of numbers, each\nstoring how often that page was used in the frame. This would resemble a list of [1, 18,\n25, 6, 0, 0,...1] rather than 01001010011..1. This extra information is particularly\nhelpful in the degenerate case, as you now have additional data to assist in the identifi-\ncation of a victim page.\nFor example, consider two pages (TextureA and TextureB) loaded into the cache \nat the same time with TextureA being used on 50% of the objects in the scene and\nTextureB being used on 10%. At this point, both pages would have the same APC\nvalue, although clearly, you can identify that these two textures have dramatically dif-\nferent usage amounts. When a victim page must be found, you must take into account\nthat TextureA, being used a larger amount in the current frame, increases the probabil-\nity that it will be used in the subsequent frame as well. Thus a lower usage texture,\nTextureB, should be replaced instead.\nBy storing this extra data per frame, you make available other statistical analysis\noperations to help identify the best page to evict from the cache:\n• The APC variable can still be derived from the expanded Age algorithm by divid-\ning the number of non-zero frames in the window by the total frames.\n• Finding the page with the MIN usages in a given frame window will identify the\nleast used page in general, which is helpful to identify victims. \n• Using the MAX analysis, you could identify the pages with the most accesses in\nyour window, in order to help avoid dumping them from the cache.\n• Finding the AVG usage in the window is just as easy and derives a second simplis-\ntic variable similar to APC.\n10\nSection 1\nGeneral Programming \n\n\nDepending on your implementation needs and data formats, either the simple\nAge algorithm or the Expanded Age algorithm are viable. The best idea is to sit down\nand analyze your data to decide which is the most efficient and useful algorithm for\nyour title.\nThe Cost of Doing Replacements\nMost victim page identification algorithms use only a single heuristic. That is, their\nalgorithms are tailor-made to specialize to the access patterns that result in the least\namount of cache page misses. For example, LRU only keeps a pointer to the oldest\npage. However, for a custom software cache there often is a second heuristic that is\ninvolved with a cache miss—the cost of filling the page of the cache with the new data.\nFor most hardware caches this is a constant cost associated with the time it takes the\nmemory handler to access main memory and retrieve the desired data.\nFor your software needs, however, this cost can often fluctuate between pages\nthemselves. Therefore, it would make sense that the victim page determination takes\ninto account the amount of performance hit involved with actually filling a page with\na given chunk of memory. This performance cost (or just cost) can come from a num-\nber of sources. It can be hand-defined by an external data set (for example, an XML\nfile that defines which textures are really used) or it can be defined by the actual cost\nof filling the page.\nConsider that in the previous example, an incoming texture page is generated by\nstreaming it off an optical drive. So, larger textures have a larger performance time\ninvolved with getting them into the cache, because they have more information to be\nstreamed from media, whereas smaller or simpler textures have fractions of that cost.\nIn this situation, it would make a great deal of sense to consider the cost involved with\npotentially replacing a page in memory during victim page determination. If you vic-\ntimize a page that has a high cost associated with it, you can incur unneeded overhead\nin the next few frames if that page is required. If, instead, you eliminate a page that\nhas a lower cost, the performance hit from incorrectly removing it from the cache is\nmuch lower. In a nutshell, factoring in cost as a page-replacement variable allows you\nto answer the question “Is it cheaper to dump five smaller textures to make space for\n1 larger texture?”\nTo review, cost allows you to be concerned with how much a given page will hurt\nperformance to reload it into the cache. If required, an expansion of this system allows\na victim page identification function to be more concerned about performance cost of\na miss, as opposed to coherence between frames. \nBy itself, cost contains the same problems that other cache replacement algorithms\nhave. When a thrash occurs, you find the cheapest texture in the cache and get rid of \nit. Because there’s always a cheaper page available, the entire cache could potentially\nthrash if the load is big enough. This algorithm also has the problem that it can leave\nhighly expensive pages in the cache indefinitely. If something like a skybox texture is\nloaded into the cache, this is a good trait as the skybox will be active every frame and\n1.1\nEfficient Cache Replacement Using the Age and Cost Metrics\n11\n\n\nmost likely not want to be removed from the cache due to its large size. Most of the\ntime, this is a bad trait and one that needs constant attention.\nCost is a powerful ally when combined with other heuristics. By biasing the\nreplacement identification of access pattern algorithms with replacement metrics, you\nallow your cache to find a nice medium between page-replacement requirements and\nthrashing. Additionally, the access pattern identification helps remove the problems\ninvolved with pure cost metrics, allowing high-cost items to eventually be freed from\nthe cache when they reach the state that they are no longer needed. \nAge and Cost (A&C) \nThe previous example assumed that each page in the texture cache had an associative\nAPC as well as a relative cost (RC), and was updated every frame. This example\nassumes that the RC is a more important metric and allows that value to be an integer\nvariable that has no upper limit. For example, if you are moving textures into your\ncache by streaming them from disk, the RC may be a result of the texture size divided\nby the time it takes to stream a fraction of that texture from the media. Consider the\nAPC a normalized floating-point variable between (0,1). \nIn a simple implementation, you can combine these two values into a single result\nwhere the APC acts as a scalar of the RC, thus giving ThrashCost = RC*APC. In general,\nthis turns out to be a very nice heuristic for identifying the proper victim page. To prove\nthis, I’ve provided a few examples of APC/RC ratio, and a description of the replacement\npattern. For the following data, assume that the highest RC value can be 10. \nAPC \nRC \nTC \nPattern\n1.0 \n10 \n10 \nThis page is highly expensive to replace, thus will be a hard candidate\nto move. With the APC of 1.0, this identifies that the page has been\nused every frame over the Age window. At this point, the only way this\npage can be replaced is if it’s forced by a full cache dump, which might\nnot be allowed depending on your implementation.\n0.2 \n8 \n4 \nThis page has a relatively high RC, but its APC shows that it’s rarely\nused, and thus could be considered as a valid replacement. However,\nbecause the RC is so high, it’s worth doing a second APC test on sub-\nwindows of the data in order to determine if this texture should really\nbe replaced. \n1.0 \n5 \n5 \nThis page is at the halfway point. That is, it’s relatively cheap to\nreplace, however its APC says that it will be needed next frame.\nReplacing this page would be no problem, but due to the high APC, \nit might be worth the extra search to find another page with a higher\nRC but lower APC. \n0.01 \n10 \n0.1 \nThis page has an extremely low APC, which points to the fact that it\nhas either just been introduced to the cache, or it is infrequently used.\nIn either case, the TC is so low that other pages with lower RCs could\nbump it if their APC is higher. Because the RC is so high, however,\nit’s worth doing a second APC test on sub-windows of the data in\norder to determine if this texture should really be replaced. \n12\nSection 1\nGeneral Programming \n\n\nAs seen in this table, using the simplistic RC*APC value can result in pages with\ndiffering APC/RC values having the same ThrashCost. This will mean that texture A\nwith an APC/RC relation of 0.5/100 can have the same cost as texture B with an\nAPC/RC of 1.0/50. The question here is how do you determine which page to\nreplace? In theory, either page is a valid target, both containing the same numerical\nweight for potential replacement. Texture A has a higher cost, and thus would be\nmore expensive to replace if it were needed in the next frame. Texture B has a lower\ncost, but has an APC value of 100%, so there’s a high probability that this page will be\nneeded immediately. \nIn practice, I’ve found that biasing this decision to replace a lower APC when\nmultiple pages return the same value works much better. In fact, that’s why the Age\nmetric is used. By analyzing the usage pattern, along with cost, you can see that\nalthough a page is more expensive, it is used less often, and thus has a lower probabil-\nity of being incorrectly thrashed. In these situations, it’s a good idea to re-scan the\ncache to find a page with a higher ThrashCost, but with a lower APC value. If one\nisn’t found, it’s safe to assume that this page may need to be replaced for the sake of\nthe cache. However, depending on your system, the better page to replace may vary. \nThis table mentions one other instance that needs discussing. As mentioned, the\nA&C system has the ability to introduce a page that can become static. This can be a\ngood thing if your cache includes textures that are visible from most every camera angle,\nsuch as a skybox texture, or an avatar skin. This can also be bad, however, if this is not\ndesired. If too many of these pages are introduced into the cache, the available working\nsize shrinks considerably, causing more cache misses and overall less cache efficiency. If\nthis is the case, it might be wise to generate a separate cache for these static textures.\nConclusion\nCustom media cache systems are critical for any high-performance environment. As\nthe usage of out-of-core media increases, the need for an accurate control model also\nincreases for game environments. Because older replacement algorithms were designed\nwith operating system memory management and hardware memory access patterns in\nmind, they lack some key properties that allow them to evaluate the more complex\nreplacement situations that can exist. Using a combination of the Age and Cost metrics\nintroduces a great deal of additional information, for a very low overhead, that fits and\nworks well in gaming environments. The Cost metric introduces a concept of overall\nperformance in page loading, which for an on-demand out-of-core system can become\na major bottleneck. The Age metric allows a more per-frame based view of usage pat-\nterns that ties easier into the concept of the game simulation than traditional metrics,\nand also contains enough usable information to create valid replacement cases in any\nenvironment-specific edge case. Because cache replacement needs change over the\ncourse of simulation, this allows a great deal of customization and second order analy-\nsis to evaluate the best victim page for best results. \n1.1\nEfficient Cache Replacement Using the Age and Cost Metrics\n13\n\n\nThe advantage of using this powerful duo of metrics is an overall increase in\ncache-page replacement performance, resulting in a lower overhead of thrashes and\ngeneral cost required to fill the cache. At the end of the day, that’s really what you’re\nlooking for. \nAcknowledgments\nBig thanks to John Brooks at Blue Shift for the metric in the “Expanded Age Algo-\nrithm” section, as well as help getting all this off the ground!\nReferences\n[Carmack00] Carmack, J. “Virtualized Video Card Local Memory Is the Right\nThing,” Carmack .Plan file, March 07, 2000.\n[Megiddo03] Megiddo, Nimrod and Modha, Dharmendra S. “ARC: A Self-Tuning,\nLow Overhead Replacement Cache,” USENIX File and Storage Technologies\n(FAST), March 31, 2003, San Francisco, CA.\n[O’Neil93] O’Neil, Elizabeth J. and others. “The LRU-K Page-Replacement Algo-\nrithm for Database Disk Buffering,” ACM SIGMOD Conf., pp. 297–306, 1993. \n14\nSection 1\nGeneral Programming \n",
      "page_number": 34,
      "chapter_number": 5,
      "summary": "Martin covers issues related to excep-\ntion handling, stack overflows, and memory leaks—common issues we all encounter\nduring application development Key topics include pages, cache, and caching.",
      "keywords": [
        "cache",
        "Age",
        "APC",
        "cache page",
        "Age Algorithm",
        "Cost",
        "Cache Replacement",
        "Efficient Cache Replacement",
        "victim page",
        "pages",
        "cache page replacement",
        "Cost Metrics",
        "Age variable",
        "Expanded Age Algorithm",
        "cache page metrics"
      ],
      "concepts": [
        "pages",
        "cache",
        "caching",
        "texture",
        "replacement",
        "replace",
        "replacements",
        "cost",
        "algorithm",
        "memory"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 29,
          "title": "Segment 29 (pages 285-293)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 10,
          "title": "Segment 10 (pages 75-82)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 27,
          "title": "Segment 27 (pages 261-275)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 8,
          "title": "Segment 8 (pages 61-75)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.51,
          "method": "api"
        }
      ]
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 48-56)",
      "start_page": 48,
      "end_page": 56,
      "detection_method": "topic_boundary",
      "content": "15\n1.2\nHigh Performance Heap\nAllocator\nDimitar Lazarov, Luxoflux\ndimitar.lazarov@usa.net\nT\nhis gem shows you a novel technique for building a high performance heap allo-\ncator, with specific focus on portability, full alignment support, low fragmenta-\ntion, and low bookkeeping overhead. Additionally, it shows you how to extend the\nallocator with debugging features and additional interface functions to achieve better\nperformance and usability.\nIntroduction\nThere is a common perception that heap allocation is slow and inefficient, causing all\nkinds of problems from fragmentation to unpredictable calls to the OS and other unde-\nsirable effects for embedded systems such as game consoles. To a large extent this has\nbeen true, mostly because historically console manufacturers didn’t spend a lot of time\nor effort implementing high-performance standard C libraries, which include all the\nheap allocation support. As a result many game developers recommend not using heap\nallocation, or go as far as outright banning its use in runtime components of the game\nengine. In its place, many teams use hand-tuned memory pools, which unfortunately is\na continuous and laborious process that is debatably inflexible and error-prone. All this\ngoes against a lot of the modern C++ usage patterns, and more specifically the use of\nSTL containers, strings, smart pointers, and so on. Arguably, all these provide a lot of\npowerful features that could dramatically improve code development, so it’s not so diffi-\ncult to see why you would want to have the best of both worlds. With this in mind, we\nset out to create a heap allocator that can give the programmer the performance charac-\nteristics of a memory pool but without the need to manually tinker with thousands of\nlines of allocation-related code.\nRelated Works\nA popular open source allocator by [Lea87], regarded as the benchmark in heap alloca-\ntion, uses a hybrid approach, where allocations are handled by two separate methods,\nbased on the requested allocation size. Small allocations are handled by bins of linked\nlists of similarly sized chunks, whereas large allocations are handled by bins of “trie”\n\n\nbinary tree structures. In both cases, a “header” structure is allocated together with\nevery requested block. This structure is crucial in determining the size of the block\nduring a free operation, and also for coalescing with other neighboring blocks. Alloca-\ntions with non-default alignment are handled by over-allocating and shifting the start\naddress to the correct alignment. \nBoth of these factors, header structure and over-allocating, contribute unfavorably\nto alignment heavy usage patterns as is common in game programming. A slightly diff-\nferent approach by [Alexandrescu01] suggests using a per-size template pool-style allo-\ncator. A pool-style allocator uses a large chunk of memory, divided into smaller chunks\nof equally sized blocks that are managed by a single linked list of free blocks, com-\nmonly known as the free-list. This has the nice property of not needing a header struc-\nture and hence has zero memory overhead and naturally gets perfect alignment for its\nelements. Unfortunately, during deallocation, you have to either perform a search to\ndetermine where the block came from or the original size of the block needs to be sup-\nplied as an additional parameter. Combining ideas from the previously described work\nand adding our little gem, we arrive at our solution.\nOur Solution\nOur solution uses a hybrid approach, whereby we split our allocator in two parts—\none handling small allocations and the other handling the rest.\nSmall Allocator\nThe minimum and maximum small allocations are configurable and are set by default\nto 8 and 256 bytes, respectively. All sizes in this range are then rounded up to the\nnearest multiple of the smallest allocation, which by default would create 32 bins that\nhandle sizes 8, 16, 24, 32, and so on, all the way to 256, as shown in Figure 1.2.1.\n16\nSection 1\nGeneral Programming \nFIGURE 1.2.1\nSelection of the appropriate bin based upon allocation size.\n\n\nNotice that since the small allocations are arranged into bins of specific sizes, you\ncan keep any size-related information just once for the whole bin, instead of with\nevery allocation. Although this saves memory overhead, you might rightly wonder\nhow you find this information when, during a free operation, you are supplied only\nwith the address of the payload block.\nTo explain this, we need to establish how the small allocator would deal with\nmanaging memory. You might want to allocate large blocks of memory from the OS,\nas is traditionally done with pool-style allocators, but let’s do it on demand so as to\nminimize fragmentation and wasted memory. Additionally, you want to return those\nsame large blocks (let’s call them pages) back to the OS once you know when they are\nnot used anymore and you need the memory somewhere else. \nWhat is important to note here, for the correctness of this method, is that it asks\nfor naturally aligned pages from the OS. In other words, if the selected page size is\n64KB, this method expects it to be 64KB aligned.\nOnce a naturally aligned page is acquired, the method places the bookkeeping\ninformation at the back of the page. There, it stores a free-list that manages all\nelements inside the page. A use count determines when the page is completely empty,\nand a bin index determines which bin this page belongs to. Most importantly, all\npages that belong to the same bin are linked in a doubly-linked list, so that you can\neasily add, remove, or arrange pages. \nThe last piece of the puzzle is almost straightforward—during a free operation,\nthe provided payload address is aligned with the page boundary. Then you can access\nthe free list and the rest of the bookkeeping information, as shown in Figure 1.2.2.\n1.2\nHigh Performance Heap Allocator\n17\nFIGURE 1.2.2\nThe layout of a single page in a bin.\n\n\nThis method places the bookkeeping information at the back of the page, as\nopposed to the front, because there is often a small piece of remaining memory at the\nback, due to the page size not being exactly divisible by the element size. Therefore,\nyou get the bookkeeping cost for free in those situations.\nThis method also ensures that whenever a page becomes completely full it is\nplaced at the back of the list of pages for the corresponding bin. This way, you just\nneed to check the very first page’s free-list to see whether you have a free element avail-\nable. If no elements are available, the method requests an additional page from the\nOS, initializes its free-list and other bookkeeping information, and inserts it at the\nfront of the bin’s page list.\nWith this setup, small allocations and deallocations become trivially simple. For\nexample, when the allocation size is known in advance, as is the case with “new”-ing\nobjects, the compiler can completely inline and remove any bin index calculations\nand the final code becomes very comparable in speed and size to an object specific\npool-style allocator.\nLarge Allocator\nThe small allocator is simple and fast; however, as allocation sizes grow, the benefits of\nbinning and pool allocations quickly disappear. To combat this, this solution switches\nto a different allocator that uses a header structure and an embedded red-black tree to\nmanage the free nodes. A red-black tree has several nice properties that are helpful in\nthis scenario. First, it self-balances and thus provides a guaranteed O(log(N)) search,\nwhere N is the number of free nodes. Second, it also provides a sorted traversal which\nis very important when dealing with alignment constraints. Finally, it is very handy to\nhave an embedded red-black tree implementation around.\n18\nSection 1\nGeneral Programming \nFIGURE 1.2.3\nLayout of memory use in the large allocator.\nAs shown in Figure 1.2.3, the header structures are organized in a linked list of\nmemory blocks arranged in order of their respective addresses. There is no explicit\n“next” pointer, because the location of the next header structure can be computed\nimplicitly from the current header structure plus the size of the payload memory\nblock. In addition to this, you need to store information about whether a block is\ncurrently free. This information can be stored in the least significant bit of the “size”\nfield, because the requested sizes for large allocations are rounded up to the size of the\n\n\nheader structure (8 bytes). This rounding is necessary to allow header structures to\nnaturally align between payload blocks.\nWhen a block is freed, you use the empty space to store the previously mentioned\nred-black tree node. The red-black tree is a straightforward implementation, as in\n[Cormen90], with a few notable modifications.\n1.2\nHigh Performance Heap Allocator\n19\nFIGURE 1.2.4\nThe layout of the red-black tree nodes.\nAs shown in Figure 1.2.4, you have the classic left, right and parent pointers.\nThere are also two additional pointers, previous and next, that form a linked list of\nnodes that have the same key value as the current node. This helps tremendously with\nperformance, because it’s quite common to have lots of free blocks with identical\nsizes. In contrast, a traditional red-black tree would store same key value nodes as\neither left or right children, depending on convention. This would predictably reduce\nperformance, because when searching through these nodes, the search space is not\nhalved as usual to achieve O(log(N)) speed, but is merely walked in a linear fashion of\nO(N).\nBoth left/right and previous/next pointers are organized as arrays of two entries.\nThis is done mostly to simplify operations such as “rotate left” and “predecessor,” which\nnormally have mirrored counterparts such as “rotate right” and “successor.” Using an\nindex to signify left or right, you can then have a generic version that can become either.\nFurthermore, in each node, you keep information on which side of its parent that\nnode is attached—left or right—as well as its “color”—red or black. This allocator uses\na similar packing trick as with the header structure, and places the parent side index\n\n\nand the node color in the two least significant bits of the parent pointer. The parent\nside index is quite important for performance, especially when combined with a red-\nblack tree that uses the so-called “nil” node, because the essential “rotate” operation can\nthen become completely branch-free.\n20\nSection 1\nGeneral Programming \nFIGURE 1.2.5\nThe nil node’s place in the tree.\nAs shown in Figure 1.2.5, the “nil” node is a special node that all terminal nodes\npoint to, and is also the node to which the root of the tree is attached. The fact that\nthe root is to the left side of the “nil” node might appear random, but in fact this is\nvery important for traversal operations. It is easy to notice that running a predecessor\noperation on the “nil” node would give the maximum element in the tree, which is\nexactly what you want to happen when you iterate the tree backward starting from the\n“nil” node—in STL terminology that is the “end” of the container. This way, the pre-\ndecessor operation doesn’t need to handle any special cases.\nWith all this setup done, during an allocation you search the red-black tree for the\nappropriate size. If the acquired block is too big, it is chopped up and the remainder is\nreturned to the tree. If there are no available blocks, more are requested from the OS,\nusually in multiples of large pages.\nDuring a free operation, you look up the header structure and determine whether\nany of the neighbors are free so they can be coalesced. Because the coalescing is done\nfor every freed block, there could be no more than one adjacent free block on each side,\nand thus this operation needs to check only two neighbors. The resulting free block is\nthen added to the red-black tree.\n\n\nThe more interesting use of the red-black tree happens when you need to allocate\nwith non-default alignment. This allocator uses the fact that you can iterate a binary\ntree in sorted order, and notice that you need to check nodes with sizes equal or larger\nthan the requested size, but smaller than the requested size plus the requested align-\nment. You then can use the binary tree operations “lower bound” with the requested\nsize and “upper bound” with the requested size plus alignment. You can then iterate\nthrough this range until you find a block that satisfies the alignment constraint. Iter-\nating through a red-black tree is an O(log(N)) operation, so obviously larger align-\nments would take longer to find. The important thing to notice is that this will\nguarantee the smallest-fit block criteria, which is considered to be one of the major\nfactors in reducing fragmentation, something that the traditional approach of over-\nallocating and shifting doesn’t satisfy very well.\nCombining the Allocators\nNow that you have two allocators, there is another problem. The small allocator relies\non the page alignment to find its bookkeeping information. With two allocators\nthough, you need a way to distinguish which allocator a given address comes from.\nThere are several solutions, none of them is perfect, but one is the simplest. You go\nahead and access the page info structure, and try to recognize it. You need to make\nsure the large allocator always uses pages that are at least as large as the small allocator’s\npages, which makes accessing the page info safe. You place a per bin marker that gets\nhashed with the page info address and store it inside the page info. Then, during a free\noperation, you access the page info and compute the hash again. If it matches, you\naccept the address as originating from the small allocator; otherwise, you forward it to\nthe large allocator. This solution is very simple and fast, but it has the nagging prob-\nlem that it might, with a very small probability, match the incorrect page. There is a\nway to detect this mistake and in the reference implementation this verification is per-\nformed for debug builds. If a misdetection ever happens, one can increase the security\nby using bigger hashes or extra checks. \nThere are at least a few other ways to solve this problem. One is to use a bit array, \none bit for every page in the address space. On 32-bit machines with 64KB pages, this \nbit array is merely 8KB, but unfortunately this doesn’t scale very well to 64-bit machines. \nA second solution is to keep an array of pointers in every bin, each entry pointing\nback at their respective page, while the page itself has a bin and array offset indices.\nThis guarantees that the page truly belongs to that bin, but unfortunately makes the\nmemory management of that array quite complicated.\nA third solution is to use a reserved virtual address range for all small allocations,\nand with a simple check you can immediately determine whether a pointer belongs to\nit or not. Of course, this requires the use of virtual memory, which is either not present\nor severely limited on many game consoles, and most importantly requires specifying\nsome upper bound on small allocations that might not be possible in certain situations.\n1.2\nHigh Performance Heap Allocator\n21\n\n\nMultithreading\nIt is quite important these days to have robust multithreading, and the allocator is\noften the very first thing that needs to be properly implemented. Our solution is not\nground-breaking but provides good efficiency in mild levels of contention. Notice that\nthe small allocator can have a mutex per bin because once the bin is selected there is no\nsharing of data between bins. This means that allocations that go to different bins can\nproceed in full parallel. On the other hand, the large allocator is more complicated and\nyou need to lock it as soon as an operation needs access to the red-black tree.\nThe only faster alternative to having a mutex per bin is to use thread local storage.\nThat might be feasible especially on systems that can afford some additional memory\nslack per thread. Unfortunately, thread local store is still non-portable and has all\nkinds of quirks. \nDebugging Features\nSo far, we have a high-performance allocator that can easily be used as a malloc/free\nreplacement. Now, you can easily add features that can help you with debugging or\nextracting additional performance or functionality.\nThere are many ways to add debugging support and the reference implementation\ntakes a classic approach of keeping debug records for every allocation made. These\nrecords are kept in separately managed memory so that they interfere with the payload\nas little as possible. We reuse several of the methods developed for the main allocator,\nspecifically we use an embedded red-black tree to quickly search debug records and also\nwe use a novel container that we call a book. The idea is that we have a hybrid data\nstructure similar to a list which we manage in “pages” and we can only add elements to\nthe end, as one would write words in a book, hence the name. The need for such a\nspecific structure arises from the fact that you need to use large memory chunks\ndirectly from the OS and you want the data structure to take care of that memory,\nsimilar to how dynamic arrays over-allocate and then fill in elements one at a time.\nWhen you combine the embedded red-black tree with the “book” container, you\ncan manage the debug records quite efficiently. Besides the size and the address of the\npayload block, the debug record stores a partial copy of the call stack at the time of the\nallocation request, which can be quite useful for tracking memory leaks. Additionally,\nyou can store the so-called “debug guard,” which is a series of bytes of memory that is\nover-allocated with the payload and is filled with a sequence of numbers.\nThen, during a free operation, the “debug guard” is examined to verify the\nintegrity of the block. If the expected sequence of numbers is not found, it’s highly\nlikely there is some memory stomping going on. It is a bit more difficult to determine\nwho exactly did the stomping, but often it is code related to whoever allocated that\nblock, so that’s a good starting point. If the stomp is reproducible, a well placed hard-\nware breakpoint can quickly reveal the offender.\n22\nSection 1\nGeneral Programming \n\n\nExtensions\nSo far, this allocator follows quite closely the malloc/free/realloc interface. We have\nfound that exposing a bit more functionality can be very beneficial for performance.\nThe first addition is what we call “resize”—the ability to change the size of an\nalready allocated block, but without changing the address of that block. Obviously, this\nmeans the block can only grow from the back, and even though that seems limited, it\nturns out it can be very useful for implementing dynamic arrays or other structures\nthat need to periodically expand.\nAnother extension is the provision of free operations that take as additional param-\neters the original requested size and alignment. This is beneficial to this allocator\nbecause it can use that size to determine whether the small or the large allocator\nshould free that block. These additional functions cannot be used on blocks that have\nbeen reallocated or resized, and the only significant use for these additional functions\nwould be to implement high-performance “new” and “delete” replacements. \nOn the CD\nThere is a reference implementation of the allocator on the included CD. It has been\nused in practice in a next generation game engine together with a custom STL imple-\nmentation that takes advantage of our allocator’s features and extensions. We also\nprovide a simple synthetic benchmark to verify that the allocator has any performance\nadvantages. \nFurthermore, we show several ways to integrate this allocator with existing or\nfuture C++ code. The easiest way, of course, is to just override the global new and\ndelete operators. This has several disadvantages, but most notably it is not an appro-\npriate solution for a middleware library. Per class new and delete operators, as tempt-\ning as they sound, in practice prove to be quite difficult to work with and tend to have\nfrustrating limitations. We show one possible way to have correct custom per object\nnew and delete functionality with template functions. \nReferences\n[Alexandrescu01] Alexandrescu, Andrei. Modern C++ Design, Addison-Wesley\nLongman, 2001.\n[Berger01] Berger, Emery D. “Composing High-Performance Memory Allocators,”\navailable online at ftp://ftp.cs.utexas.edu/pub/emery/papers/pldi2001.pdf.\n[Cormen90] Cormen, Thomas. Introduction to Algorithms, The MIT Press, 1990.\n[Hoard00] Berger, Emery. “The Hoard Memory Allocator,” available online at\nhttp://www.hoard.org/.\n[Lea87] Lea, Doug. “A Memory Allocator,” available online at http://g.oswego.edu/\ndl/html/malloc.html.\n1.2\nHigh Performance Heap Allocator\n23\n",
      "page_number": 48,
      "chapter_number": 6,
      "summary": "This chapter covers segment 6 (pages 48-56). Key topics include allocator, allocation, and allocations. Additionally, it shows you how to extend the\nallocator with debugging features and additional interface functions to achieve better\nperformance and usability.",
      "keywords": [
        "Performance Heap Allocator",
        "Allocator",
        "heap allocator",
        "Small Allocator",
        "red-black tree",
        "memory",
        "High Performance Heap",
        "size",
        "Large Allocator",
        "block",
        "tree",
        "Performance Heap",
        "free",
        "Small allocations",
        "bin"
      ],
      "concepts": [
        "allocator",
        "allocation",
        "allocations",
        "allocated",
        "allocate",
        "memory",
        "block",
        "size",
        "sized",
        "pages"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 21,
          "title": "Segment 21 (pages 193-203)",
          "relevance_score": 0.78,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 22,
          "title": "Segment 22 (pages 204-211)",
          "relevance_score": 0.77,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 17,
          "title": "Segment 17 (pages 154-161)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 38,
          "title": "Segment 38 (pages 351-358)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "Segment 43 (pages 858-879)",
          "relevance_score": 0.71,
          "method": "api"
        }
      ]
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 57-65)",
      "start_page": 57,
      "end_page": 65,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank \n\n\n25\n1.3\nOptical Flow for Video Games\nPlayed with Webcams\nArnau Ramisa, Institut d’Investigació \nen Intelligència Artificial\naramisa@iiia.csic.es\nEnric Vergara, GeoVirtual\nevergara@geovirtual.com\nEnric Martí, Universitat Autónoma \nde Barcelona\nEnric.Marti@uab.cat\nO\nne of the most widely used techniques in computer vision commercial games is\nthe concept of optical flow. Optical flow is the movement between the pixels of\nsuccessive frames of a video stream provided, for example, by a modern Webcam.\nMultiple techniques, with different properties, exist in the computer vision liter-\nature to determine the optical flow field between a pair of images [Beauchemin95].\nThis gem explains three of these techniques—the direct subtraction of successive\nframes, motion history, and a more advanced technique called the Lucas and Kanade\nalgorithm. These three techniques have a different degree of robustness and also a dif-\nferent computational cost, therefore the choice depends on the requirements of each\napplication.\nIntroduction\nThe proposed algorithms are prepared and ready to use, with many other computer\nvision techniques, in the OpenCV library. This library is an open source project that\nimplements in C the most important algorithms and data structures used in computer\nvision applications. In 2006, the first stable version of OpenCV (1.0) was released after\n\n\nfive years of development. Game Programming Gems 6 contains an article where this\nlibrary is used to detect in which position the face of the player is located [Ramisa06].\nThis information was used to map the action of leaning around a corner in a third- or\nfirst-person shooter.\nThe optical flow is an array of vectors that describes the movement that has\noccurred by pixels between successive frames of a video sequence. Usually it is taken as\nan approximation to the real movement of the objects in the world. The optical flow\ninformation is used not only by many computer vision applications, but also by bio-\nlogical systems such as flying insects or even humans. An example of this application\nis the MPEG4 video compression standard, which uses the optical flow of blocks of\npixels to eliminate redundant information in video files.\nIn computer vision games, the optical flow is used to determine if a particular\npixel of the screen is “activated” or not. If enough pixels of a certain area are activated,\none can consider that a “button” has been pressed or, in general, that an action has\nbeen performed.\nFor this, it is usually not necessary to estimate the full optical flow field and com-\nputationally cheaper methods that compute only if a given pixel has changed with\nrespect to the previous frame can be used.\nOpenCV Code\nThis section explains the most important functions of the webcamInput class. The usage\nof the class is structured around a main loop where new frames are acquired from the\nWebcam.\n#define ESC_KEY 27\nwebcamInput webcam(true,1);\n// method = (1: Lucas-Kanade,\n2: Differences, 3: Motion History)\nwhile(1)\n{\nwebcam.queryFlow();\nif(cvWaitKey(10)==ESC_KEY)\nbreak;\n}\nIn this code, a new webcamInput object called webcam is created. This object en-\ncapsulated all the logic to acquire images through the camera and process them. The\nconstructor requires two parameters: a Boolean that indicates if the camera connection\nshould be initialized during the object construction or not, and an integer from 1 to 3\nthat indicates which method of the optical flow will be used. Later, inside the loop, \nthe queryFlow() function is used to acquire a new frame and process it. Finally, cvWait-\nKey() is used to pause for 10ms waiting for the Escape key to possibly stop the process.\nDuring initialization all required variables, depending on the method, are allocated\nin memory:\n26\nSection 1\nGeneral Programming \n\n\nvoid webcamInput::queryFlow()\n{\ngetImage();\nswitch(method)\n{\ncase 1:\nlucaskanade();\nbreak;\ncase 2:\ndifferences();\nbreak;\ncase 3:\nflowHistory();\nbreak;\n}\nflowRegions();\ncvCopy( image, imageOld );\n//to save the previous image to do optical flow\n}\nThe function getImage() acquires a new image from the Webcam and converts it\nto gray-level, then, depending on the method chosen, the appropriate function is\ncalled. Finally, flowRegions() is used to count the activated pixels in the defined\nregions of interest, and the current image is copied to imageOld for using it in the next\noptical flow computation.\nFirst Method: Image Differences\nIn this section, the three methods used are explained. The first method is the simplest\none: image differences. This method is really simple, and consists of subtracting the\ncurrent camera image from the previous one:\nvoid webcamInput::differences()\n{\ncvSmooth( image, image, CV_GAUSSIAN, 5, 5);\ncvAbsDiff( imageOld, image, mask );\ncvThreshold( mask, mask, 5, 255, CV_THRESH_BINARY ); // and\nthreshold it\n/* OPTIONAL */ cvMorphologyEx(mask, mask, NULL,kernel,\nCV_MOP_CLOSE ,1);\ncvShowImage(\"differences\", mask);\n}\nThe cvAbsDiff function subtracts image from imageOld, which are the current\nand previous images of the Webcam. The result of the operation is stored in mask, and\nin the next line its content is binarized: all pixels with a value lower than 5 (which\nindicates similar pixels in image and imageOld) are discarded, whereas the ones with a\nhigher value are marked as active. This threshold depends on the sensitivity of the\n1.3\nOptical Flow for Video Games Played with Webcams\n27\n\n\nused Webcam. If a lower value is used, more valid active pixels would be correctly\ndetected, but at the same time more false active pixels would appear, generated by\nnoise in the images. To reduce the effect of noise in the estimation, the first step con-\nsists in smoothing the image using a Gaussian kernel of 5 \u0002 5 pixels. This is done\nusing the function cvSmooth().\nIndividual or small groups of activated pixels are not likely to correspond to real\nmoving objects, therefore the function cvMorphologyEx() applies a closure to eliminate\nthese spurious activated pixels while not changing the correct ones [Morphology]. The\nsize and shape of the morphologic operator is defined in the structure kernel. Finally,\nthe last parameter specifies the number of times in a row that the erosion and dilation\nare performed. The closure is a powerful yet computationally expensive operation, and\ncan be avoided if the noise level of the images is low, or by raising the value of the\nthreshold.\nThis method is faster than the Lucas and Kanade method, but it is not as robust\nat handling bad camera quality.\nSecond Method: Motion History\nThe second method is called motion history. It uses the same principle of the image\ndifferences but uses more than just the previous image, and “remembers” recently\nactive pixels.\nvoid webcamInput::flowHistory()\n{\n//FLOW HISTORY\ncvSmooth( image, image, CV_GAUSSIAN, 5, 5);\ncvCopy( image, buf[last]); \nIplImage* silh;\nint idx2;\nint idx1=last;\nidx2 = (last + 1) % N; // index of (last - (N-1))th frame\nlast=idx2;\nint diff_threshold=30;\nsilh = buf[idx2];\ndouble timestamp = (double)clock()/CLOCKS_PER_SEC;\ncvAbsDiff( buf[idx1], buf[idx2], silh );\ncvThreshold( silh, silh, diff_threshold, 1, CV_THRESH_BINARY );\ncvUpdateMotionHistory( silh, mhi, timestamp, MHI_DURATION );\ncvCvtScale( mhi, mask, 255./MHI_DURATION,\n(MHI_DURATION - timestamp)*255./MHI_DURATION );\ncvShowImage(\"M_history\",mask);\n}\n28\nSection 1\nGeneral Programming \n\n\nIn this function, buff is a cycling buffer where the last N images from the Web-\ncam are stored. When a new image enters the buffer, the oldest and the new image are\nsubtracted, and the result is thresholded to find pixels where reliable change has\noccurred. Then the motion history image mhi is updated with this new information\nand the timestamp for the latest frame. Finally, the new motion history image is\nscaled to an 8 bits per pixel mask image. Moreover, this motion history image can be\nused to determine the motion gradient and to segment different moving objects. A\ncomplete example of this method can be found in the motempl.c file of the OpenCV\nsamples.\nThird Method: Lucas and Kanade Algorithm\nThe first two methods don’t give as output the real optical flow, they just detect pixels\nwhere motion has occurred. However, in this case, you can trade the destination posi-\ntion of the moved pixels for less computational load. The last method presented is the\noptical flow estimation method by Lucas and Kanade [Lucas81]. This method esti-\nmates the position where a given pixel has moved using a procedure similar to how\nthe Newton-Raphson method finds the zeroes of a function.\nvoid webcamInput::lucaskanade()\n{\ncvCalcOpticalFlowLK(imageOld,image,cvSize\n(SIZE_OF,SIZE_OF), flowX, flowY);\ncvPow( flowX, flowXX, 2 );\ncvPow( flowY, flowYY, 2 );\ncvAdd( flowXX, flowYY, flowMOD );\ncvPow( flowMOD, flowMOD, 0.5 );\ncvThreshold( flowMOD, flowAUX, 10, 255, CV_THRESH_BINARY );\n/* OPTIONAL */ cvMorphologyEx\n(flowAUX, flowAUX, NULL,kernel, CV_MOP_CLOSE ,1);\ncvShowImage(\"LUKAS_KANADE\",flowAUX);\n}\nThe function cvCalcOpticalFlowLK() computes the optical flow between the\ngray-level images imageOld and image using the Lucas and Kanade method. The\nremaining parameters of the function are cvSize(SIZE_OF,SIZE_OF), which specifies\nthe size of the window that will be used to locate the corresponding pixel in the other\nimage, and flowX and flowY, where the components of the optical flow vectors will be\nstored. You are interested in the module of the vectors, which indicates the strength of\nthe movement. Once you have the module, you threshold it to eliminate all active\npixels caused by noise. If the camera is really noisy, it is also possible to use a morpho-\nlogical closure to remove isolated pixels.\n1.3\nOptical Flow for Video Games Played with Webcams\n29\n\n\nThe time values shown in Table 1.3.1 were obtained on a Pentium IV 3GHz\ncomputer running the openSUSE 10.2 operating system where the execution time\nwas measured over 100 iterations. The image size was 640 \u0002 480.\nTable 1.3.1\nTime Required for Each Iteration of the Motion Detection Algorithms\nMean\nMedian\nStd\nImage differences\n0.020867s\n0.025000s\n0.0063869s\nMotion history\n0.027794s\n0.025000s\n0.0056899s\nLucas and Kanade\n0.21114s\n0.19500s\n0.043091s\nOptical Flow Game\nIn order to see the results of the optical flow in a real application, we developed a simple\ngame resembling Eye Toy: Play (a game developed by SCEE London, using a digital cam-\nera similar to a Web camera, for PlayStation 2). The idea behind the game is very simple:\nclean the stains that appear on the screen through bodily movements (see Figure 1.3.1).\nBecause players could move their arms about frantically, thereby cleaning all stains\nas they appear without any difficulty, we added some toxic stains that players have to\navoid in order to keep on playing. This forces players to be careful with their move-\nments, introducing an element of skill which makes the game more fun.\n30\nSection 1\nGeneral Programming \nFIGURE 1.3.1\nA screenshot of the game.\n\n\nThe point is to use the optical flow in the game, so we have encapsulated the\nfunctionality described previously in a C++ class called webcamInput, which makes it\neasier to apply to the game. The public interface of this class is as follows:\nclass webcamInput\n{\npublic:\nwebcamInput( int method=1 );\n// method = (1: Lukas-Kanade,\n2: Differences, 3: M History)\n~webcamInput( );\nvoid     GetSizeImage        ( int &w, int &h );\nuchar*   GetImageForRender   ( void );\nvoid     QueryFlow           ( void );\nvoid     AddRegion           ( int x, int y, int w, int h );\nstd::vector<float> FlowRegions ( void );\nprivate:\n...\n}\nIn the class’s constructor, you can decide which method, from the three introduced\nin the previous section, you want to use in order to calculate the optical flow. As the\ndefault, we use the Lucas and Kanade method, which seems to yield the best results. \nIn order to show on the screen the color image coming from the Webcam (the player’s\nbody), this class provides two important functions. The first one is GetSizeImage(),\nwhich informs you of the size of the image coming from the Webcam. The second func-\ntion is GetImageForRender(), which returns an array of unsigned char values that repre-\nsent the RGB (red, green, blue) components of the pixels, ordered in rows. By means of\nthe QueryFlow() function, you can calculate the optical flow of the current frame.\nYet, what you need for the game is to be able to query the amount of movement\nthat has taken place in a particular area of the image (the one occupied by a stain to be\ncleaned). To achieve this, at the beginning of the game you can define as many regions\nas you want by making use of the function AddRegion(int x, int y, int w, int h).\nThis function creates a region with its origin at the coordinates (x, y) of the image\ngenerated by the Webcam and a width and height given by (w, h).\nOnce the regions have been defined, every time QueryFlow() is called, you can get\nthe extent of the movement in each region by using the function FlowRegions(). This\nfunction returns a list of floating-point values (as a std::vector<float>) comprising\nvalues within the [0,1] range that represent the percentage of pixels where movement\nhas been detected.\nIn the game, you have to instantiate an object (mWebCam) of this class, subsequently\ncalling its functions from several parts of the code, as described here:\n1.3\nOptical Flow for Video Games Played with Webcams\n31\n\n\n1. Upon game initialization, as we have already pointed out, you need to partition\nthe query for optical flow in several distinct regions. In particular you have \nto divide the initial image into 16 regions as a 4 \u0002 4 grid (see Figure 1.3.2). \nIn order to achieve this, you need to use the function mWebCam.AddRegion() as\nfollows:\nint width, height;\nmWebCam.GetSizeImage( width, height );\nfor(int col = 0; col < 4; col++)\n{\nfor(int row = 0; row < 4; row++)\n{\nmWebCam.AddRegion(col* (width/4), row*(height/4),\nwidth/4, height/4);\n}\n}\n2. During rendering, you can call the function mWebCam.GetImageForRender()\nin order to paint the colour image coming from the Webcam. On top of the\nimage, you paint the stains with a certain alpha component, depending on\nhow clean they are. The image of the stains will occupy all the space deter-\nmined by every one of the 16 regions.\n3. During the game updates, the function mWebCam.QueryFlow() will be called\nfirst, followed by a call to mWebCam.FlowRegions(). This way, you know the\nextent of the movement that has taken place in each one of the 16 regions,\nmaking more and more transparent (based on the movement detected) the\nimages of the stains that are active at that particular moment. Stains vanish\nas they become completely transparent.\n32\nSection 1\nGeneral Programming \nFIGURE 1.3.2\nScreenshot of the game, partitioning the screen in several regions.\n",
      "page_number": 57,
      "chapter_number": 7,
      "summary": "This gem explains three of these techniques—the direct subtraction of successive\nframes, motion history, and a more advanced technique called the Lucas and Kanade\nalgorithm Key topics include images, methods, and functions. Covers method.",
      "keywords": [
        "Optical Flow",
        "image",
        "Flow",
        "Optical",
        "motion history image",
        "method",
        "pixels",
        "Video Games Played",
        "Optical Flow Game",
        "Lucas and Kanade",
        "Game",
        "motion history",
        "Webcam",
        "function",
        "Image Differences"
      ],
      "concepts": [
        "images",
        "methods",
        "functions",
        "functionality",
        "games",
        "pixels",
        "different",
        "differences",
        "value",
        "flow"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 38,
          "title": "Segment 38 (pages 363-370)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 53,
          "title": "Segment 53 (pages 511-519)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 4,
          "title": "Segment 4 (pages 64-82)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 41,
          "title": "Segment 41 (pages 405-415)",
          "relevance_score": 0.45,
          "method": "api"
        }
      ]
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 66-75)",
      "start_page": 66,
      "end_page": 75,
      "detection_method": "topic_boundary",
      "content": "In order to keep an acceptable frame rate, you need to consider two points. The\nfirst one is that the Update() function of the game should not call QueryFlow() every\ntime, as the latter method is computationally very costly. To circumvent this problem,\nyou can restrict the call to QueryFlow() to a certain frequency, x times per second,\nlower than that of the update calls. This will make the game run smoothly on most\nmodern computers without the game’s responsiveness being affected.\nA further consideration that needs to be taken into account affects the resolution\nof the image coming from the Webcam. That is, the higher the resolution, the more\ncalculations the CPU will have to carry out before obtaining the optical flow. This\nmeans that you have to set that parameter to, for example, 320 \u0002 240 pixels. \nAs a final remark, the CD that accompanies this book contains not only an exe-\ncutable version of the project for Microsoft Windows, but also the source code for it,\nas well as a .pdf file with the UML class diagram. The game has been programmed in\nC++, with Microsoft Visual Studio 2005, and using the DirectX and OpenCV\nlibraries.\nReferences\n[Beauchemin95] Beauchemin, S.S and Barron, J.L. “The Computation of Optical\nFlow,” ACM Computing Surveys (CSUR), Vol. 27, No. 3, pp. 433–466, ACM\nPress New York, NY, USA, 1995.\n[Lucas81] Lucas, B.D. and Kanade, T. “An Iterative Image Registration Technique\nwith an Application to Stereo Vision.” \n[Morphology] \n“Morphological \nImage \nProcessing,” \navailable \nonline \nat \nhttp://en.wikipedia.org/wiki/Morphological_image_processing.\n[Ramisa06] Ramisa, A., Vergara, E., and Marti, E. “Computer Vision in Games\nUsing the OpenCV Library,” Game Programming Gems 6, edited by M. Dick-\nheiser, Charles River Media, 2006, pp. 25–37.\n1.3\nOptical Flow for Video Games Played with Webcams\n33\n\n\nThis page intentionally left blank \n\n\n35\n1.4\nDesign and Implementation\nof a Multi-Platform\nThreading Engine\nMichael Ramsey\nmiker@masterempire.com\nE\nngine development is changing. As the paint of an ancient masterpiece fades over\ntime, so are some of the tried and true techniques of the past era of single-core\nengine development. Developers must now embrace architectures that execute their\ngames on multi-core systems. In some cases, performance differs on a per-core basis!\nThe development of an architecture in a multi-core environment must be acknowl-\nedged, designed, planned, and finally implemented. The implementation aspect is where\nthis gem assists, by providing a theoretical foundation and a practical framework for a\nmulti-platform threading system.\nA fundamental precept for a game engine architecture is the requirement that it \nbe able to exploit a multi-core environment. The exploitation of the environment is a\nsystem that by definition allows for multiple tasks to be executed in parallel. Addition-\nally, you want the performance of the system in question to be real-time; this real-time\nperformance requirement demands that the threading system also be lightweight. Data\nstructures, object construction, cache concerns, and data access patterns are just a few\nof the issues that you must be continuously aware of during development of objects\nthat will use the threading system.\nThis gem focuses on the development of a threading engine that has been engi-\nneered for both an Xbox 360 and a standard multi-core PC game engine. With that\nbeing said, I’ve provided details on the core systems that are applicable to all architec-\ntures, not just a multi-core desktop (Windows/Linux) or Xbox 360. So while you will\nread about cache lines, they will focus on the principles that make them important\nacross multi-platforms and operating systems.\n\n\nSystem Design for a Practical Threading Architecture\nOne of the most important aspects of designing a multithreaded program is spending\nthe time upfront to design and plan your game architecture. Some of the high-level\nissues that need to be addressed include the following:\n• Task dependencies\n• Data sharing\n• Data synchronization\n• Acknowledgment and flow of data access patterns\n• Decoupling of communication points to allow for reading, but not necessarily\nwriting, data\n• Minimizing event synchronization\nOne of the most basic, yet the most efficient, principles of threading a game system\nis to identify large systems that have relatively few dependencies (or even focused points\nof intersystem communication) and thread them. If the points of communication\nbetween the systems are focused enough, a few simple synchronization primitives (such\nas a spinlock or mutex) are usually sufficient. If a stall is ever detected, it is straight-\nforward to identify and reduce the granularity of that particular event through routing\nto a different interobject manager. It is important when designing a multithreaded game\nengine to not only be stable but to also strive for extensibility.\nA Pragmatic Threading Architecture\nOn the book’s CD, you’ll find the source to a complete multi-platform threading sys-\ntem. The GLRThreading library has been engineered in a platform-agnostic manner\nwith a provided implementation for the Windows operating system. The interfaces\nare conducive to expansion onto the Xbox 360. The GLRThreading library supports\nall general threading features from the Win32 API. Some functionality has been\nencapsulated for ease of use (for example, GLRThreadExecutionProperties). The stan-\ndard Win32 model of threading is preemptive in nature. What preemptive means in\nthis context is that any thread can be suspended by the operating system to allow for\nanother thread to execute. This allows the OS to simulate multiple processes while\nonly having a single processor. Preempting can be directly influenced by an attributed\nGLRThreadTask property, but generally you should be aware that once a task has been\nexecuted or resumed inside the GLRThreading library (that is, made available to the\nWindows OS), it could and most likely will be preempted or have its execution time\nreduced/increased by the operating system.\nBasic Components of the GLRThreading Library\nAs you learn about the structure of the GLRThreading library, use Figure 1.4.1 to better\nunderstand the system’s components and dependencies. The foundational interface to\nthe GLRThreading system is aptly named GLRThreadFoundation. GLRThreadFoundation\n36\nSection 1\nGeneral Programming \n\n\nis a singleton that should be available to all game systems that need access to thread-\ning capabilities. Usually, GLRThreadFoundation is placed inside a precompiled header,\nwhich is then subsequently included in all files inside an engine. Through GLRThread-\nFoundation you control the submission of tasks. But before you can look at that, you\nhave to determine and define some basic properties for the execution environment;\nthis is where the system descriptions come in.\n1.4\nDesign and Implementation of a Multi-Platform Threading Engine\n37\nFIGURE 1.4.1\nThe GLR thread library.\nIn order to execute properly, the threading system needs the ability to query infor-\nmation about its environment. This includes determining the number of processors,\nthe memory load, and whether or not hyper-threading is supported. To accommodate\nthis in a platform-agnostic manner, there is the GLRISystemDescription. The platform-\nspecific implementations are derived from this basic interface. The system description\nfor MS Windows is the GLRWindowSysDesc, and the Xbox 360 is implemented by\nGLRXBox360SysDesc.\n\n\nGLRThreadFoundation Usage\nThe GLRThreadFoundation is the focal point for all threading interactions. The types of\nthreading interactions that you can execute include the ability to execute tasks from a\ngame’s objects, as well as accessing threads from the pool. Inside the codebase there will\nbe a single instance of the thread foundation. For example:\nGLRThreadFoundation glrThreadingSystem;\nTo access functionality inside the threading system, you use the following method\nsyntax:\nglrThreadingSystem.FunctionName();\nwhere FunctionName is any of the platform-agnostic functions that can be executed by\nthe game level components.\nThreads\nThreads are segments of code that can be scheduled for execution by the operating\nsystem or by an internal scheduling system. See Figure 1.4.2 for a comparison between\na typical single-threaded environment and its multithreaded counterpart. These \ncode snippets can be single functions, objects, or they can be entire systems. The\nGLRThreading libraries interface is designed to be platform-agnostic, so all manipu-\nlation is done on the GLRThread level.\n38\nSection 1\nGeneral Programming \nFIGURE 1.4.2\nComparison between a single threaded and multithreaded programming\nmodel.\n\n\nGLRThread is the platform-agnostic implementation for a thread within the\nGLRThreading library. The GLRThread interface allows the following operations on a\nplatform-implemented thread:\n• Creating a thread\n• Executing a thread\n• Altering a thread’s properties\n• Resuming a thread\n• Terminating a thread\n• Temporarily suspending a thread\n• Querying the status of a thread \nThere are several variations of property management and thread execution. Also\nassociated with a GLRThread are the following control mechanisms: GLRThread\nProperties and GLRThreadTask. These mechanisms control (among other aspects)\nwhere and generally how a thread will execute a task.\nPreemptible and Simultaneously Executed Threads\nIt is vital that every engine developer be aware of the performance characteristics of the\ncores that their engine not only targets but also is developed on. This is because a sig-\nnificant amount of development is initially implemented on a desktop PC, which usu-\nally has very different execution characteristics from a typical multi-core console. One\nof the most important aspects of engineering a threading system is the consideration of\nthreads that can be preempted and those that are agnostically called non-preemptible.\nOne of the standard paradigms that a thread follows is that an OS can suspend execu-\ntion of a thread to allow another thread to execute. When the OS decides to suspend\nexecution of a current thread, it will save the context of the currently executing thread\nand restore the context state of the next thread. Context switching is not free; there is\nsome overhead but generally the cost of idling a thread to not incur the overhead of \na context switch is not worth the added code complexity. The switching of threads\ncreates the illusion of a multitasking system.\nThere is also support on consoles for the ability to create threads that are essentially\nnon-preemptible. A non-preemptible thread is a thread that cannot be interrupted by\nthe OS. This power is not the pinnacle of blissfulness. The independent execution of\nthreads (on the same core) usually share the same L1 cache, which generally means you\nstill want like tasks to execute on the same core in order to utilize any cache coherency\ninherent in the data structures. This is to minimize the cache thrashing that could\noccur when two disparate systems execute tasks on the same core.\nThread Properties\nGLRThreadProperties is the general mechanism that stores a particular thread handle\nand its associated ID. There is also the ability to alter the thread’s default stack size.\nThe thread’s stack is the location for its variables as well as its call stack. The OS can\n1.4\nDesign and Implementation of a Multi-Platform Threading Engine\n39\n\n\nautomatically grow the stack for you, but this is a performance hit on consoles. To\navoid changing thread stacks on the fly, you should anticipate and set the stack size\nbeforehand. \nThread context switching is in general pretty fast—but as with anything there are\nassociated costs. One of these costs is memory associated with the thread to store its\ncontext information. The size of the default GLRThread context is 64KB. This 64KB\ncan and should be manually adjusted depending on the platform that you are target-\ning. If you need to increase the size of thread stacks on a Windows-based OS (such as\na PC or Xbox 360), there is a property that can be set from the Visual Studio develop-\nment environment.\nOne of the gotchas to be on the lookout for is when your stack needs to be 128KB\nor 256KB. This type of situation usually requires a scaling down of either the size of\nthe task that is being executed (that is, reducing the granularity) or identifying objects\nthat can be further decomposed into smaller implementations, as in [Ramsey05].\nThread Execution Properties\nA thread execution property is a platform-agnostic interface that allows for more granu-\nlar control over a task’s execution. GLRThreadExecutionProps provides for a number of\nproperties such as defining a task’s preferred processing element, a task’s priority, and a\ntask’s affinity mask. A thread’s execution property also has the ability to define its ideal\nprocessor element. This allows for a management system to group like tasks on a partic-\nular processor for execution. This is defined inside GLRThreadExecutionProps.h.\nOn a single physical processor with hyper-thread capabilities, the GetProcess\nAffinityMask() will return bits 1 and 2 with bit 3 set as well, to indicate that you have\nat least one physical CPU with two logical CPUs. On a dual CPU (physical) machine\nwith hyper-threading capabilities, GetProcessAffinityMask() would show 1 + 2 + 4 + 8\n= 15. This indicates two physical CPUs with two logical CPUs apiece. CPUs begin their\nidentification at 1. It should be noted that the implementation of GLRThreadExecution\nProps is inside the GLRThreadExecutionProps.h header.\nProcessor Affinity\nAffinity requests threads to execute on a specific processor. This allows the system\ndeveloper to target specific processors for repeated operations. By way of repeated\noperations, you can also group associated operations together, to further increase the\ncache coherency of similar objects. Some systems may regard thread affinity as a hint\nand not a requirement, so check the documentation before assuming any problems in\nyour threading libraries. \nSpecifying a task’s affinity is done through a simple label of either PA_HARD or\nPA_SOFT. These flags tell the OS to either use a particular processor or just the \nspecified processing element as a hint, respectively. This is defined in GLRThread\nExecutionProps.h.\n40\nSection 1\nGeneral Programming \n\n\nTask Priority\nChanging the priority on a thread is suggested only for certain systems. The\nGLRThreading library allows you to designate a thread’s priority according to the fol-\nlowing scale:\n• TP_Low\n• TP_Normal\n• TP_High\n• TP_Critical\n• TP_TimeCritical\nThe default setting for newly created thread execution properties priority is nor-\nmal. This can be changed depending upon a particular tasks requirements. This is\ndefined inside GLRThreadExecutionProps.h. \nThread Allocation Strategies\nThere are numerous ways to build a threading system, from the naive allocation of\nthreads on the fly to the more sophisticated implementation of preallocated work sys-\ntems. One of the underlying paradigms for the GLRThreading system is that you\nwant to do as much allocation up front as possible. This is important in developing a\nconsole title, because you not only want to be aware of the memory consumption at\nstart-up, but also memory usage of system-level components at runtime.\nNaive Allocation\nThe simplest and most straightforward way to create a threading system is to have a\nthread manager object, implemented as a singleton, that processes requests in a create-\nfor-use paradigm. Whereas this is probably the easiest way to get started, it is not an\nadvantageous decision when factoring in the complete length of a product cycle as\nwell as the runtime performance of constantly allocating and deallocating a thread.\nThread Pools\nWith the underlying principle of front-loading system level allocations in mind, the\ninnards of the GLRThreadingPool rely upon as much preallocation as possible. The\nthread pool is a system that front-loads the creation of threads. This obviates the need\nfor runtime creation of resources that can be dealt with once and for all upon start-up.\nWhereas the creation of a single thread is not that expensive, the constant allocation\nand deallocation during runtime is an unnecessary burden. For instance, the system\nexperiences memory fragmentation if threads are constantly allocated and deallo-\ncated. The actual number of threads created for the pool is dependent on the system\nand can be modified based upon the game’s needs and performance criteria.\n1.4\nDesign and Implementation of a Multi-Platform Threading Engine\n41\n\n\nThe thread pool is a subsystem that works off of the time-tested paradigm of task\nsubmission. At regular intervals, worker threads look for a task to execute. If there is\nwork available, the execution attributes are set up and the thread is resumed. Once the\ntask has been executed, the thread is suspended and made available for subsequent\ntasks.\nThread Pool Properties\nThe thread pool has properties that allow for the defining of several characteristics\nthat will prove useful when the system is used in various games. The thread pool needs\nthe ability to change the number of created threads, the number of tasks that the\nthreads can work on, and the ability to lock the task pool at any particular time.\nMultiple Pools\nSo you might be asking yourself if one pool is good, then possibly creating multiple\nthread pools for different subsystems in a game engine might be an even better idea.\nThe issues with introducing multiple pools are manifold—the primary issue is that if\nyou have multiple pools, with differing performance characteristics (through the use\nof thread properties, task scheduling, and so on), you have to introduce another layer\nof complexity into the system—the need for interpool communication. This com-\nplexity is simply not wanted in such a performance-critical system; the more straight-\nforward the underlying thread system is, the more likely it is you’ll avoid difficulties\nintroduced through the complexity of the system.\nObject Threading\nThe GLRThreading library provides its threading capabilities through a process of\ncreating an object and then submitting that newly created object to the threading\nlibrary. To submit an object to the GLRThreading system, you just need to make the\ncall:\nGLRThreadFoundation.submitTask(&newGameSystemFunction);\nTo actually have the worker threads grab tasks from the task list inside the thread-\ning pool, you need to make a call to distribute() using this call:\nGLRThreadFoundation.distribute();\nYour object is now off and running on the same processor as the invoking process.\nTo make the execution of the objects easier, the process of allowing threads to execute\nshould be inserted in your main game update. In this manner, game systems can just\ncreate tasks, submit them, and let the threading system deal with allocation, execution,\nand all the details. \n42\nSection 1\nGeneral Programming \n",
      "page_number": 66,
      "chapter_number": 8,
      "summary": "This chapter covers segment 8 (pages 66-75). Key topics include threading, task, and execute. The\nfirst one is that the Update() function of the game should not call QueryFlow() every\ntime, as the latter method is computationally very costly.",
      "keywords": [
        "thread",
        "threading system",
        "system",
        "thread pool",
        "game",
        "Threading Engine",
        "Task",
        "thread execution",
        "Thread Execution Properties",
        "Multi-Platform Threading Engine",
        "execution",
        "Engine",
        "game engine",
        "GLRThreading library",
        "multi-platform threading"
      ],
      "concepts": [
        "threading",
        "task",
        "execute",
        "executed",
        "execution",
        "game",
        "implementation",
        "implementations",
        "core",
        "performance"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "Segment 33 (pages 330-337)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 52,
          "title": "Segment 52 (pages 1041-1063)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 39,
          "title": "Segment 39 (pages 388-398)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 76-90)",
      "start_page": 76,
      "end_page": 90,
      "detection_method": "topic_boundary",
      "content": "Thread Safety, Reentrancy, Object Synchronicity, \nand Data Access\nDealing with issues of designing game systems that are thread-safe and reentrant is a\nthorny business and is beyond the scope of this gem. A lot of the issues are highly\ndependent on the architecture and data-access patterns of the game. There are a few\nprinciples and practices to keep in mind when creating threadable objects and systems.\nA rule of thumb about reentrancy is that you should make a system reentrant\nonly if it needs to be. It costs a lot of time and involves a lot of effort to make your\nunderlying game libraries 100% reentrant and the truth of the matter is that the\nmajority of them don’t have to be reentrant. Sure, libraries such as your memory man-\nager and task submission system need to be reentrant and thread-safe, but a lot of the\nmanagerial systems can serve as a gatekeeper through the use of a cheap synchroniza-\ntion construct. By using something as simple as a hardware-based spinlock, you can\npush the burden of thread safety up to the system managers. This makes even more\nsense, because they should control their own data flow. So, once you’ve identified the\ngeneral data flow inside your engine, the process of determining what actually has to\nbe reentrant is usually clear.\nDancing the Line (or Cache Coherency)\nObject alignment along cache boundaries is important for a standard single-core\nengine but it becomes paramount when you develop for a multi-core environment.\nNormally, a cache is broken into cache lines of 32 or 64 bytes. When main memory is\ndirect-mapped to the cache, the general strategy is to not be concerned with the amount\nof memory being mapped, but with the number of cache lines that are being accessed.\nThere are three basic types of cache misses:\n• Compulsory miss. This occurs the first time a block of memory is read into the cache. \n• Capacity miss. This occurs when a memory block is too large for the cache to hold.\n• Conflict miss. This occurs when you have memory blocks that map to the same\ncache line. In a multi-core environment, conflict misses should be attacked with\na vengeance. Conflict misses are usually systemic of an engine’s architecture that\ncontains poorly designed data structures. It’s the coupling of these data structures\nwith the general non-deterministic pattern of the threads’ executions that causes\nconflict misses to negatively affect performance.\nThe GLRThreading library includes a basic utility that will aid you in creating\ncache-aligned data structures. The principle utility is the GLRCachePad macro.\n#define GLRCachePad(Name,BytesSoFar) \\\nGLRByte Name[CACHE_ALIGNMENT – (BytesSoFar) %\nCACHE_ALIGNMENT)]\n1.4\nDesign and Implementation of a Multi-Platform Threading Engine\n43\n\n\nThe GLRCachePad macro groups the data together in cache line(size) chunks and\non cache line boundaries. You want the access pattern of different CPUs to be sepa-\nrated by at least one cache line boundary. The cache alignment value is different from\nplatform to platform, so you might need to implement a different cache-padding\nscheme depending on your target system. The final caveat is that you want the GLR-\nCachePad call to occur at the end of a data structure; this will force the following data\nstructure to a new cache line [Culler99].\nHow to Use the GLRThreading Library\nThis section illustrates a simple example of how to use the threading system. A typical\ngame object will be defined and is called TestSystem (see Listing 1.4.1). \nListing 1.4.1\nThe Test Game Object\nclass TestSystem\n{\npublic:\nTestSystem();\n~TestSystem();\nvoid theIncredibleGameObject( void );\nvoid objectThreadPump( void );\nprivate:\nGLRThreadedTask<TestSystem> mThreadedTask;\nGLRThreadExecutionProps    *mThreadedProps;\n};\nThe TestSystem game object has two private data structures: a GLRThreadedTask\nand a GLRThreadExecutionProps. The GLRThreadedTask member is used to reference\nthis particular object and a function within the object that will be executed by the\nthreading system. Listing 1.4.2 contains an example of how to register an instantiated\nobject, as well as the function that will be distributed for execution.\nListing 1.4.2\nThe Implementation of the Game Object’s Threadable Function\nvoid TestSystem::objectThreadPump( void )\n{\nmThreadedTask.createThreadedTask(this,\n&TestSystem::theIncredibleGameObject,mThreadedProps );\nglrThreadingSystem.submitTask( &mThreadedTask );\n}\nThe sample code in Listing 1.4.3 shows how you use the TestSystem object\nfrom Listing 1.4.1. Listing 1.4.3 shows how to instantiate the two threadable \nobjects, myTestObject and myTestObject2. As noted previously, when you call\nobjectThreadPump, you create a task, which in turn obtains a thread from the thread\n44\nSection 1\nGeneral Programming \n\n\npool and then submits a new task (myTestObject and myTestObject2) for execution to\nthe GLRThreadingSystem. The tasks are not instantly executed; they’ve only been\nadded to the task queue for execution. This allows a scheduler to rearrange submitted\ntasks based upon the game’s load and the tasks’ similarities. An eventual call to \ndistribute() is required to start the execution of these objects.\nListing 1.4.3\nCode That Creates and Executes the Test Objects\n//Create a couple test objects to thread\nTestSystem myTestObject;\nmyTestObject.objectThreadPump();\nTestSystem myTestObject2;\nmyTestObject2.objectThreadPump();\n//This call should be placed in your main game loop.\nglrThreadingSystem.distribute();\nOn the CD enclosed with this book, you will find a solution that allows you to\ncompile and execute this example code.\nConclusion\nThis gem has covered a lot of ground, including the architecture of a practical thread-\ning engine that is functional and efficient on multiple platforms. You’ve also looked at\na couple different methods to allocate threads, read about task execution, and finally\nlooked briefly at a method for more efficient data structure usage in a multi-core envi-\nronment. So as you begin developing or retrofitting your engine for the multi-core\nmarket, keep in mind some of the paint strokes that this article has covered and use\nthem to start painting your own masterpiece.\nReferences\n[Bevridge96] Bevridge, Jim. Multithreading Applications in Win32: The Complete\nGuide to Threads, Addison-Wesley, 1996.\n[Culler99] Culler, David E. Parallel Computer Architecture, Morgan Kaufmann, 1999.\n[Geist94] Geist, A. PVM. MIT Press, 1996.\n[Gerber04] Gerber, Richard. Programming with Hyper-Threading Technology, Intel\nPress, 2004.\n[Hughes04] Hughes, Cameron. Parallel and Distributed Programming Using C++,\nAddison-Wesley, 2004.\n[Nichols96] Nichols, Bradford. Pthreads Programming: A POSIX Standard for Better\nMultiprocessing, O’Reilly, 1996.\n1.4\nDesign and Implementation of a Multi-Platform Threading Engine\n45\n\n\n[Ramsey05] Ramsey, Michael. “Parallel AI Development with PVM,” In Game Pro-\ngramming Gems 5, Charles River Media, 2005.\n[Ramsey08] Ramsey, Michael. A Practical Cognitive Engine for AI, To Be Published,\n2008.\n[Richter99] Richter, Jeffrey. Programming Applications for Microsoft Windows,\nMicrosoft Press, 1999.\n46\nSection 1\nGeneral Programming \n\n\n47\n1.5\nFor Bees and Gamers: How\nto Handle Hexagonal Tiles\nThomas Jahn, King Art\ntjahn@kingart.de\nJörn Loviscach, Hochschule Bremen\njlovisca@informatik.hs-bremen.de\nG\nrids are the one of the most prominent tools to simplify complex structures and\nrelationships in order to simulate or visualize them. Their use in games ranges\nfrom the graphical tiles of 8 \u0002 8 pixels used in handheld games to the space represen-\ntation for AI agents. The typical choice, a square grid, is biased by the square’s simple\ncomputational rules; they do not show a surpassing behavior in simulation. Hexago-\nnal tiles, in contrast, offer highly attractive features in both logic and look. However,\nhexagonal grids are awkward in software development. This gem introduces concepts\nand techniques to deal with this issue.\nIntroduction\nA tiling is created when a shape or a fixed set of shapes is repeated endlessly to cover\nthe infinite plane without any gaps or overlaps. Tilings come in a two variations—\nwhereas non-periodic tilings are used to create textures, most applications rely on\nperiodic tilings, which are much easier to handle computationally. To increase sym-\nmetry, regular tilings are used, where the tiling is formed from a regular polygon such\nas a square, an equilateral hexagon, or an equilateral triangle.\nFor their space-filling efficiency, biology prefers hexagonal grids to square grids:\nThey appear in the layout of honeycombs and the placement of the light receptors in\nthe retina. Even though hexagonal grids have a number of other benefits, they require\ncomplex and thus error-prone code. Object-oriented abstraction comes to the rescue.\nThis gem describes a software design to hide the intricacies in a framework. \n\n\nThe Pros and Cons of Hexagonal Tilings\nTo be able to judge whether a square or a hexagonal tiling fits the task best, you have\nto consider a number of aspects ranging from adjacency to the choice of a coordinate\nsystem.\nNeighborhoods and Stride Distances\nIn a square tiling, there are two common definitions of neighborhoods. Neighbors\neither have to share an edge (4-neighborhood) or it suffices that they share a vertex \n(8-neighborhood). This ambiguity has its consequences. Take a strategy game that is\nbased on square tiles as an example. Any movement is broken into a series of steps,\nwhere a step means the transition from one tile to one of its neighbor tiles. Now you\nas the developer have to make a choice. You can allow transitions in only four direc-\ntions, which means it will take 41% more steps to move the same distance along \na diagonal axis than along a vertical or horizontal axis, as shown in Figure 1.5.1.\nHowever, allowing transitions in eight directions isn’t much better—now the distance\ncovered by moving a number of steps along a diagonal axis will be larger by 41% than\nalong a vertical/horizontal axis. To avoid this distortion, diagonal transitions have to\nbe handled differently, adding to the complexity of both the code and the game’s\nrules.\n48\nSection 1\nGeneral Programming \nFIGURE 1.5.1\nMarching on a square grid\nshows fast and slow directions, no matter which\ndefinition of neighborhood is employed.\nHexagonal tilings offer an advantage here. Each tile has six equidistant neighbors,\neach of which connects to a different edge. No two tiles share only one vertex or more\nthan one edge. Thus, the notion of a neighbor isn’t ambiguous for hexagonal grids. In\naddition, the distance covered by moving a number of steps in an arbitrary direction\nwill vary by only 15%. See Figure 1.5.2.\n\n\nIsotropy and Packing Density\nOf all shapes that can tile the plane, regular hexagons have the smallest perimeter for\na given area, meaning there’s no “rounder” type of tile. Thus, whenever a grid has to\nrepresent a continuous structure with no inherently preferred directions, a hexagonal\ntiling will work best. This is a major reason to pursue image processing with hexago-\nnal pixels [Middleton05].\nThanks to their compact shape, regular hexagons form the tiling with the highest\npacking density. A circular disk inscribed in a square occupies 79% of its area, as\nopposed to 91% percent for a circular disk inscribed in a hexagon. Thus, with a\nhexagonal grid you can reach the accuracy of a square grid with about 10% fewer\ncells. This is a chance to reduce the memory consumption and improve the speed of\ngrid-based algorithms by roughly the same number.\nVisual Appearance\nIn games, grids are often used to represent playing fields. This has a major impact on\nvisual appearance. A square grid is suited well to create cities and indoor scenes. The\nedges of hexagons, however, connect more smoothly, forming angles of 120 degrees.\nAssemblies of hexagonal tiles possess slightly jagged-looking outlines because no par-\nallel line segments are connected directly. This and the absence of sharp edges make\nthis type of grid more suited for the representation of natural scenes, as you can see in\nFigure 1.5.3.\n1.5\nFor Bees and Gamers: How to Handle Hexagonal Tiles\n49\nFIGURE 1.5.2\nThe length of the shortest connection on a\nhexagonal grid is close to that of a straight line.\n\n\nAxes of Symmetry\nA square grid can be mapped easily to an ordinary Cartesian system of integer coordi-\nnates. These can also be employed as indices of a two-dimensional array. Apart from\nthe two directions of symmetry parallel to the square’s sides, there are two diagonal\ndirections of symmetry. Although rarely seen in practice, these could serve as coordi-\nnate axes, too.\nHexagonal grids, however, possess 12 directions of symmetry that one could use\nas coordinate axes. There are two basic layouts, as shown in Figure 1.5.4—a hexago-\nnal grid with horizontally aligned tiles, where every second row is indented by half the\nwidth of a tile, and a vertically aligned grid.\n50\nSection 1\nGeneral Programming \nFIGURE 1.5.3\nWhereas square tiles are ideal for cities (left), hexagonal tiles lend themselves\nto organic shapes.\nFIGURE 1.5.4\nDepending on which direction of symmetry is employed as the x axis, the\ntiles of a hexagonal appear horizontally or vertically aligned.\n\n\nAny pair of axes that you can choose suffers from one of two defects—if the axes\nare perpendicular to each other, they will not be geometrically equivalent. In particu-\nlar, one of the axes runs parallel to some edges of the lattice whereas the other does\nnot. On top of that, you have to deal with fractional coordinates; see Figure 1.5.5. If\nthe axes indeed are chosen to be geometrically equivalent, they will enclose an angle of\n60 degrees, meaning for instance that distances can’t be computed naively through the\nPythagorean Theorem. To better display the symmetry, you might even work with\nthree barycentric coordinates, one of which is redundant.\n1.5\nFor Bees and Gamers: How to Handle Hexagonal Tiles\n51\nFIGURE 1.5.5\nA hexagonal grid allows perpendicular coordinate axes with half-integer val-\nues or skewed coordinate axes.\nMastering the Hexagonal Grid\nThanks to object-oriented programming, the issues of a hexagonal grid can be hidden\nbehind an elegant façade. Actually, this gem proposes two software layers at an increas-\ning level of abstraction: addressing and accessing.\nAddress Layer\nEvery scheme to translate an address into a spatial location on a hexagonal grid and\nvice versa has its benefits and limitations. Thus, the first step is to hide the addressing\nbehind a layer of abstraction. Data container classes supporting random access and\nclasses representing addressing schemes that map grid addresses to container elements.\nThe first option for the container is an indexed random-access container such as\nthe vector of the C++ Standard Template Library (STL). Its index can be computed\nfrom the tile’s address given in perpendicular or skewed coordinates. Because the index\nrange of the container is limited, so has to be the range of addresses. If perpendicular\n\n\ncoordinate axes are used, a rectangular section of cells can be defined through an upper\nand lower boundary for each coefficient. In this case, the index can be computed like\nindex = y * width + x.\nIf the coordinate system has skewed axes, this approach would result in a trape-\nzoidal set of cells. This can be avoided by altering the computation of the index so\nthat the indices again point to a rectangular patch of cells. In this case, the index\ncalculation could look similar to this: index = Math.Floor(y * (width + 0.5) + x).\nSee Figure 1.5.6.\n52\nSection 1\nGeneral Programming \nFIGURE 1.5.6\nThrough a shift in the index\ncomputation, skewed axes, too, can be used\nto define a rectangular domain.\nThe second option for the container is a key-based random-access container such\nas C++ STL map. Whereas these containers access data slower than indexed containers,\nyou can use the cell addresses directly as keys. The biggest benefits are that there is no\nbuilt-in limit to the address range and that empty cells don’t consume memory. Thus,\na map is a good choice for sparse data points.\nTo support a highly specific setting, you can use a standard container wrapped by\na class that implements the addressing scheme of your choice. The next step toward\nflexibility would be to make this class generic through type parameters so that it isn’t\nlimited in which kind of data it can store.\nTo achieve ultimate flexibility, you can split the functionality into two classes, keep-\ning the actual storage and the addressing scheme apart. That way it’s possible to pick the\noptimal combination of an addressing scheme and a container for the task at hand.\nHere, the class that deals with the mapping between addresses and data is called\nAddressingScheme.\n\n\nAccess Layer\nThe second layer is built on top of the addressing scheme. This layer employs the iterator\ndesign pattern, which is also used in the STL. An iterator serves as a pointer to an element\nstored in a container. All of the STL containers provide the same basic interface regarding\niterators, hiding the details of the container’s implementation. Thus, code that is based\non iterators can be used with any container. In addition to being flexible, iterators are also\nsimple to use. You can ask any container for an iterator to its first element; you can just\ncall the iterator’s next() method until you reach the last element in the list.\nA similar strategy can be employed to avoid much hassle with the addressing\nschemes of hexagonal grids. We suggest two different approaches:\n• The first one can be called a Walker class. Its instance can be set to represent any cell\nin the grid and provides an interface to read and write the target cell’s data. After \nthe initialization, the referenced cell can be changed by calling a method similar \nto an iterator’s next(). Instead of iterating through the cells in a pre-determined\nsequence, the Walker class offers a move(dir) method taking an argument that spec-\nifies one of the six natural directions on the grid. Calling this method will cause the\nWalker object to point to the neighbor of the old target that is specified by the\npassed direction; see Figure 1.5.7. This class provides free movement on the grid,\nhence its name.\n• The second approach, the Enumerator class, works exactly like an iterator, but\nonly steps through a sequence of cells that represent a specific subset of the grid—\nfor example, only the next neighbors of a given cell. The framework provides \nEnumerator classes to iterate over different neighborhoods, even customized ones\nlike all cells within a certain radius of a given center cell or all cells currently visi-\nble on the screen. In a strategy game, an Enumerator could provide access to all\ncells within the attack or viewing range of a certain unit or all cells that are occu-\npied by enemies. Decoupling the logic of the grid from your actual game logic\nmakes the code a lot cleaner and better to maintain.\n1.5\nFor Bees and Gamers: How to Handle Hexagonal Tiles\n53\nFIGURE 1.5.7\nWhereas a Walker can access any neighbor of a tile, the\nEnumerator iterates over a predefined pattern of tiles in the vicinity.\n\n\nImplementation Tips\nMany algorithms never need to know actual addresses. They can use Walker objects to\nwrite and read the data. Thus, we suggest basing as much code as possible on an abstract\nWalker base class that defines a common interface but doesn’t rely on a specific address-\ning scheme. After you have decided on which kind of addressing you want and have\nimplemented a matching AddressingScheme class, you can write a compatible Walker\ninheriting the abstract base.\nAnother suggestion is to implement these classes as generics, such as C++’s class\ntemplates. This will allow specifying the data access independent from the data’s type.\nC# and Java (but not C++) provide specific interfaces to implement, resulting in\nneater code on the client side. For instance, by implementing C#’s IEnumeration inter-\nface, it’s possible to step through all the cells in the specific selection using a foreach\nloop with the same syntax as the regular for loop (unlike the for_each of the C++ STL).\nAssume that a cell in the grid is of type CellData and you have a generic Enumerator\ncalled Neighborhood and a Walker class CellPointer. The instance of Neighborhood is\ncreated and initialized by passing a Walker instance center defining which cells neigh-\nborhood you want neighbors to list. As Neighborhood implements the IEnumeration\ninterface, iterating through all the neighbors of the center cell becomes as easy as this:\nforeach(CellPointer<DataType> cell\nin new Neighborhood<DataType>(center))\n{\n// do something with cell\n}\nThe core functionality of the AddressingScheme is to provide data access on a grid\naddress layer. However, there is additional functionality this class could provide. Many\nuse cases require you to find a cell based on screen or world coordinates. This is trivial\nwith a rectangular grid, so we suggest the following approach. Partition a hexagonal\ngrid into rectangular sections as shown in Figure 1.5.8. To resolve a coordinate pair xy,\n54\nSection 1\nGeneral Programming \nFIGURE 1.5.8\nPartitioning the grid into rectangular\ncells allows simple hit-test computations.\n\n\nthe first step is to decide in which section the point lies. A section has one of two possi-\nble layouts; in both cases it is divided into three subsections, each associated with a dif-\nferent tile. Once you have resolved your coordinate to a location within a sector, there\nare only three choices left and it becomes trivial to compute the correct cell address.\nApplications\nTo show some practical benefits, consider three scenarios where hexagonal grids may\nbe used.\nSpatial Search\nA game world is populated by numerous entities with the ability to interact if they are\nwithin a certain range of each other. When a specific entity has to decide whether an\ninteraction is possible, it would be computationally expensive to consider all other\nactive entities. Instead, a grid can be used to preselect objects within a certain radius.\nThe grid divides the game world into tiles that behave as cells; based on its location,\neach entity is registered at one of these cells. To find all entities within a certain radius\nof a game object it suffices to consider objects registered with cells that contain points\nwithin the search radius.\nIf the region to be searched is circular, hexagonal grids would be the optimal\nchoice. Furthermore, with an adequate abstraction the code to perform the search can\nbe very simple.\nforeach(CellPointer<List<GameObject>> cell\nin new SearchZone<List<GameObject>>(center))\n{\nforeach(GameObject obj in cell.GetData())\n{\n// do something with obj\n}\n}\nEach cell consists of a list of instances of GameObject. A child SearchZone of\nthe Enumerator class is defined that allows iteration through all cells in the search\nzone. It yields a Walker object pointing to a specific cell. As the cell’s data is a list of\nGameObjects, another foreach loop can be used to iterate through the game objects\nassociated with this cell.\nPathfinding\nA number of games use tiles as building blocks for their game worlds. If agents are\nrequired to move within the world, path finding has to take place on the grid level. A\ntransition is possible only between adjacent tiles, where some neighbors may even be\nblocked, for instance, because they contain walls. This calls for a standard algorithm\nsuch as Dijkstra’s or A* [Mesdaghi04] to be implemented in the object-oriented\nframework.\n1.5\nFor Bees and Gamers: How to Handle Hexagonal Tiles\n55\n\n\nThe basic idea is to expand the start node until the goal is reached. On the address\nlevel, this is cumbersome. Six different offsets have to be applied to the current cells\naddress to access adjacent cells. For an orthogonal addressing scheme, these offsets will\nvary depending on whether the current cell is in an even or odd row (or column, if the\ngrid is aligned vertically).\nThe listing below performs a simple breadth-first search on a grid of cells to find\nthe shortest sequence of moveable cells connecting startCell with goalCell. The\nneighbors of a cell are accessed using a matching neighborhood enumerator. Thanks\nto object-oriented abstraction, the algorithm becomes independent of a specific grid\nlayout or addressing scheme.\nQueue<CellPointer<PathCell>> openCells\n= new Queue<CellPointer<PathCell>>();\nopenCells.Enqueue(startCell);\n// expand\nwhile(openCells.Count > 0)\n{\nCellPointer<PathCell> current\n= openCells.Dequeue();\nforeach(CellPointer<PathCell> cell\nin new Neighborhood(current))\n{\nif(cell.GetData().Moveable &&\ncell.GetData().ExpandedFrom == null)\n{\ncell.GetData().ExpandedFrom\n= current.GetData();\nopenCells.Enqueue(cell);\n}\n}\n}\n// resolve\nStack<PathCell> path= new Stack<PathCell>();\nPathCell pc = goalCell.GetData().ExpandedFrom;\nwhile(pc != null && pc != m_Start.GetData())\n{\npath.Push(pc);\npc = pc.ExpandedFrom;\n}\nCellular Automata\nCellular automata [Wolfram02] can be used to model and simulate complex dynamic\nsystems by letting a virtually infinite number of simple components interact locally.\nIn the two-dimensional setting, the interacting components are usually cells on a grid\nwhere the next state of a cell is computed based on the current state of itself and the\nadjacent cells.\n56\nSection 1\nGeneral Programming \n\n\nThe direction insensitivity of hexagonal grids makes them attractive for cellular\nautomata as well, for instance for the simulation of liquids. The obvious choice for \nthe set of cells that determine a cell’s next state is itself and its six direct neighbors (see\nFigure 1.5.9), even though sometimes other neighborhoods are used. A smaller set of\nonly three neighbors may be sufficient and allow for faster simulation. In other cases,\nsix neighbors might not provide enough data, so the selection is expanded to the\nclosest 12 or even 18 cells.\n1.5\nFor Bees and Gamers: How to Handle Hexagonal Tiles\n57\nFIGURE 1.5.9\nTo start with a simple set of rules, Conway’s Game of Life,\nthe classic 2D cellular automaton, can be carried over to hexagonal tiles.\nIf the simulation code is directly operating on cell addresses, it is hard to experiment\nwith different selections of influencing cells. If, however, an Enumerator provides the\ncollection of influencing cells, the choice of cells can be changed with ease, even at\nruntime.\nConclusion\nBy introducing another layer of abstraction on top of the address layer, it is possible to\nwrite code that’s highly decoupled from the addressing scheme and the storage of the\ncell data. This not only increases maintainability and flexibility, but also greatly sim-\nplifies working on a hexagonal grid. You can keep your game logic clean of all the\nnasty details that make hexagonal grids so cumbersome to work with.\n",
      "page_number": 76,
      "chapter_number": 9,
      "summary": "This chapter covers segment 9 (pages 76-90). Key topics include cells, tiles, and tiling. A lot of the issues are highly\ndependent on the architecture and data-access patterns of the game.",
      "keywords": [
        "Handle Hexagonal Tiles",
        "hexagonal grids",
        "Hexagonal Tiles",
        "Hexagonal",
        "grid",
        "cells",
        "General Programming FIGURE",
        "Tiles",
        "Handle Hexagonal",
        "game",
        "Data",
        "game object",
        "Cache",
        "Programming FIGURE",
        "Object Synchronicity"
      ],
      "concepts": [
        "cells",
        "tiles",
        "tiling",
        "tilings",
        "object",
        "games",
        "grid",
        "direct",
        "directions",
        "direction"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 36,
          "title": "Segment 36 (pages 360-367)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "makinggames",
          "chapter": 35,
          "title": "Segment 35 (pages 308-315)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 31,
          "title": "Segment 31 (pages 291-298)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 25,
          "title": "Segment 25 (pages 234-242)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 48,
          "title": "Segment 48 (pages 461-471)",
          "relevance_score": 0.58,
          "method": "api"
        }
      ]
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 91-99)",
      "start_page": 91,
      "end_page": 99,
      "detection_method": "topic_boundary",
      "content": "References\n[Mesdaghi04] Mesdaghi, Syrus. “Path Planning Tutorial,” AI Game Programming\nWisdom 2, CD-ROM, Charles River Media Inc., 2004.\n[Middleton05] Middleton, Lee, and Sivaswamy, Jayanthi. Hexagonal Image Process-\ning—A Practical Approach, Springer New York Inc., 2005.\n[Wolfram02] Wolfram, Stephen. A New Kind of Science, Wolfram Media Inc., 2002.\n58\nSection 1\nGeneral Programming \n\n\n59\n1.6\nA Sketch-Based Interface to\nReal-Time Strategy Games\nBased on a Cellular\nAutomaton\nCarlos A. Dietrich\nLuciana P. Nedel\nJoão L. D. Comba\nR\neal-time strategy games (RTS) are one of the most popular game genres in the\nworld. The combination of action and strategy is simply addictive, with lots of\ndevoted players spending days on game campaigns or instant battles over the Internet.\nWe have not been seeing, however, significant improvements in the RTS game-\nplay in recent years. By comparing a recent title to the very early ones, you can say\nthat now there are more units on the screen (maybe hundreds of them), new beautiful\ngraphical engines, and wider battlefields than ever before, but the essence of the\ngameplay is still the same—selecting units and defining their tasks by clicking with\nthe mouse. This process entails a very simple and efficient interface, on which players\nare usually well trained. But what should you do when the game demands more? How\ncan you control hundreds of units in a realistic and efficient way? And because the\ninterface is designed for hand-to-hand combat, what should you do when the army\nbecomes huge and there is no clear way to direct it? We are facing such situations in\ncurrent game titles, and, despite recent improvements, common unit-based interfaces\nsometimes leave players frustrated.\n\n\nThis gem describes an alternative that may improve gameplay in such situations.\nWe propose a one-click higher-level interface that controls the movement of entire\narmies or groups of soldiers. The idea behind this approach is very simple, and can be\nillustrated with any battlefield map from the old history books, such as the one shown\nin Figure 1.6.1.\n60\nSection 1\nGeneral Programming \nFIGURE 1.6.1\nMovements of soldier troops in Italy (left) and Sicily (right) invasions during\nWWII, 1943.\nIn Figure 1.6.1, the troop movements are illustrated by arrows that indicate the\ndirection of movement of some soldiers or entire battalions. Note that there is no spe-\ncific information on the individual tasks each soldier performs or which formation\nthey kept during the movement. And why should there be? In a battlefield, it is nearly\nimpossible to call soldiers by name or give them specific tasks, such as to attack this or\nthat enemy. The commander shouts an instruction to the battalion commanders,\nwhich propagates the instruction to company commanders, and so on along the line\nof command, until some soldiers hear and follow their instructions.\nWe designed a tool that tries to simulate this behavior by allowing the users to\nsketch a path or a target on the screen that units must follow or stay. The user inter-\nface is very simple: using the mouse, the user draws a sketch line (or point) on the\nscreen, which is then converted into a path (or target) on the battlefield (see Figure\n1.6.2). This sketch creates an “influence zone” in the battlefield, and every unit inside\nthis zone must follow the sketched path or target.\nSimilar attempts to create such a tool have surfaced in recent RTS titles [Bosch06],\nbut we believe there is still lots of room for improvements, mainly in the implementa-\ntion of the interface infrastructure. The goal with this work is to augment such\napproaches with a more dynamic control, designed to be used inside the battle, which\nallows improved gameplay and strategy planning while playing RTS games. The next\nsections summarize our approach and illustrate it with some examples.\n\n\nFocus-Context Control Level\nIn the hierarchical structure of an army, generals do not deal directly with soldiers, but\ninstead their orders follow the chain of command until reaching lower ranked units. In\nmodern RTS interfaces, however, the general (represented by the player) deals directly\nwith soldiers (the units). This interface keeps the player focus on the hand-to-hand\ncombat instead of the context (army placement). In such interfaces, no matter how\ngood the players are, the one who clicks faster wins [Philip07]. In some circumstances,\nhowever, the player must deal with soldiers. For instance, when the combat starts, a\ndetailed control to instruct the units on how to pursue desired targets is necessary. \nThe proposed interface presents a focus context combination of both approaches—\nthe sketch-based interface that allows macro-management of the context, and a unit-\nbased interface to control the micro-management of the unit’s movement (see Figure\n1.6.3).\n1.6\nA Sketch-Based Interface to Real-Time Strategy Games\n61\nFIGURE 1.6.2\nBy using the mouse, the user draws a sketch on the screen, which pushes the\nunits inside the sketch influence zone to the desired target.\nFIGURE 1.6.3\nFocus-context interface: combination of a sketch-based interface\nthat controls the context (army disposal, at left) and unit-based interface that\ncontrols the micro-management (hand-to-hand fight, at right).\n\n\nIt is easy to see a situation where you could arrange the army disposal with the help\nof sketches, from a far view, and turn to the unit-based interface when soldiers engage in\nhand-to-hand combat.\nImplementation Details\nThere are many ways to implement a sketch-based interface and the most important\naspect is to choose an efficient way to communicate user intentions to units on the\nbattlefield. We propose here an implementation based on a totalistic cellular automa-\nton [Weisstein07b], which is simple and fast enough to be added to any game engine.\nThe implementation has two major stages:\n• User input capture\n• Command propagation in the battlefield\nCapturing the user input is very simple, and is discussed in the “Patch Sketching”\nsection. Processing of user commands uses a totalistic cellular automaton, which is\nresponsible for iteratively spreading the command on the battlefield. This cellular\nautomaton is discussed in the section entitled “Moving the Soldiers.” Finally, in the sec-\ntion entitled “Putting It All Together,” we explain how this is used in a game interface.\nPath Sketching\nAs explained previously, we propose an interface where the user controls an army by\nsketching curves or points directly on the battlefield. This implementation is straight-\nforward, and must accomplish two tasks:\n• Capture of screen coordinates from user input\n• Projection of these coordinates onto the battlefield\nIn the first task, let’s assume that the player creates the sketch by simply clicking and\ndragging the mouse on the screen (creating a path), or simply clicking on the screen\n(creating a target). The outcome from this operation is an array of one or more points\ngiven in 2D screen coordinates (see Figure 1.6.4). These points are stored internally,\nwithout worrying if they form a continuous line, which will be taken into account in\nthe second stage. \nIn the second task, we unproject each 2D point onto a 3D position in the battle-\nfield. Using standard graphics API features such as the gluUnProject function from\nOpenGL, we map window coordinates to object coordinates using the transform and\nviewport matrices. Care must be taken with battlefield obstacles and screen positions,\nwhich have no correspondent positions on the battlefield. This can be avoided by ren-\ndering the battlefield at a coarser resolution first, and using the generated depth buffer\nas input to gluUnProject. This simplifies the test for invalid projections, and eliminates\nthe interference of battlefield obstacles. As a result we obtain an array of 3D points on\nthe battlefield, which will be the entry for the second stage of the implementation.\n62\nSection 1\nGeneral Programming \n\n\nMoving the Soldiers\nThe second stage of the implementation is responsible for handling the reaction of\nunits to the sketches. In this approach, each sketch is converted into forces that act\ndirectly over units by pushing them to desired positions on the battlefield. A grid\ndiscretization of the battlefield is used here, storing at each cell a vector representing \na force that indicates the direction in which you want to move the units. Forces are\nupdated throughout time and vary according with the user sketch. As Figure 1.6.5\nillustrates, forces are stronger in cells closer to the sketch, and are linearly attenuated\nas they move away from the sketch. This brings the notion of range of the sketch into\nplay, which resembles the behavior of a command in a real battlefield.\n1.6\nA Sketch-Based Interface to Real-Time Strategy Games\n63\nFIGURE 1.6.4\nIn the first stage of the implementation, 2D coordinates of the sketch (left)\nare captured and projected on the battlefield (right). The result is an array of 3D points that\nserves as input to the cellular automaton.\nFIGURE 1.6.5\nA sketch (left) and its underlying discretization as a grid of forces on the\nbattlefield (right). Forces affect the unit’s movement, pushing them to desired positions on\nthe battlefield.\n\n\nThe proposed representation and sketch update can be efficiently done with a cel-\nlular automaton, which is simply a grid of cells encoding information that evolves\nthrough time according to a set of rules [Weisstein07]. Each rule is locally evaluated\nfor each cell based on information stored in neighboring cells. \nFigure 1.6.5 shows a rectangular grid of square cells, which is the configuration of\nthe cellular automaton. The update of the grid information is made by a very simple\ntotalistic rule. As previously stated, each cell stores a force (a vector quantity), which\nis given by the sketch position and direction on the grid (see Figure 1.6.6). Forces are\nspread out on the battlefield, attenuated by the distance to the sketch. We implement\nthis with a rule that propagates the state of each cell to its neighbors iteratively until\nthe system reaches the equilibrium. The update of each cell corresponds to averaging\nthe quantities of the neighboring cells through time.\n64\nSection 1\nGeneral Programming \nFIGURE 1.6.6\nThe 3D coordinates of the sketch (see Figure 1.6.4) are converted to 2D\ncoordinates in the cellular automaton grid. Each point on the grid corresponding to a sketch\npoint is marked with a force vector, which is iteratively smoothed through time.\nAs previously mentioned, such an automaton is formally called a totalistic cellular\nautomaton. You have a continuous range of states (because forces can be given in any\nmagnitude at each cell), a simple neighborhood (you are only looking to adjacent cells’\nstates to update each cell), and the rule depends only on the average of the values of the\nneighboring cells. This last defines a totalistic cellular automaton [Weisstein07b], a\ncomplex name for a simple technique.\nPutting It All Together\nIn order to use the proposed approach in real RTS environments, you need a way to\ndefine commands by sketches and to integrate the sketch-based interface with the cur-\nrent unit-based interfaces. We propose some useful commands that can be translated to\nthe force grid approach, and also suggest a simple way to integrate sketch-based unit\nmanagement in an existing implementation.\n\n\nIn RTS gameplay, you frequently need to move troops through the battlefield,\nguiding them around and in between natural obstacles, until you find some interest-\ning target. These two commands (guide and point targets) have a natural translation\nwith this approach. Guiding units can be made by line sketches, which are directly\nconverted to force vectors on the grid (see Figure 1.6.7). It can be necessary, however,\nto ensure that the sketch is represented by a sufficient number of points on the grid.\nThis can be accomplished by rasterizing the lines defined between the sketch points\ndirectly over the automaton grid. \nPointing out a target on the battlefield can be made by means of a small circular\nsketch (or even a point), which is then converted to a set of vectors around the sketch,\npointing to the sketch center, as shown in Figure 1.6.7. The update of vectors around\nthe sketch creates a vector field pointing to the sketch center, which pushes units\ncloser to the desired target.\n1.6\nA Sketch-Based Interface to Real-Time Strategy Games\n65\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nFIGURE 1.6.7\nDifferent types of sketches (first row), their representation in the automaton\ngrid (middle row), and the resulting vector field (last row). The sketch footprint is stored in\nthe grid, which automatically updates the vector field through time. The “erase” command\n(illustrated in the third column) is a special case, where the sketch cancels the forces of the grid.\nIt is easy to see, however, that the insertion of some vectors in the automaton grid\nis not enough to create a stable vector field. The update mechanism smooths the cell\ncontents every timestep, and this information quickly fades away. In order to prevent\nthis from happening, we suggest the use of a command lifetime. The command life-\ntime is the number of iterations in which cells containing vectors originated by the\n\n\ncommand are not updated, thus allowing more time for the command to spread on\nthe grid. The lifetime can be adjusted for each command type, being naturally high \nin long sketches (giving more time to units traveling along the battlefield) and small\nin sketches indicating target or smaller movements.\nThe integration of a sketch-based interface with an existing unit-based interface is\nsimple because both implementations are independent (one does not affect the\nother). The sketch-based approach just adds a new item in the unit movement equa-\ntion, which is a vector quantity indicating a direction to follow. All vectors are stored\nin a grid, which has a homeomorphic (one-to-one) mapping to the battlefield, which\nmeans that any unit on the battlefield can query the grid for the direction in which it\nshould go. The grid update can be made in parallel, because it is independent of any\nother process. This suggests that you can encapsulate the sketch control in a black box\nthat receives arrays of 2D points from the application interface. The sketch control\ncan then be queried for forces in any battlefield position (see Figure 1.6.8). \n66\nSection 1\nGeneral Programming \nFIGURE 1.6.8\nA possible integration of sketch- and unit-based interfaces. The\nsketch control can be seen as a black box, which receives arrays of 2D points from\nthe application interface and can be queried for forces in any battlefield position.\nConclusion\nThis gem discussed a simple and efficient approach for implementing sketch-based\ninterfaces. The efficiency comes primarily from the simplicity of the algorithms involved\n(such as projection of points and grid update through a cellular automaton), thus allow-\ning an easy port for any RTS game engine. It is important to observe, however, that the\nchosen grid resolution plays a fundamental role on the performance and memory\nrequirements of the application. In our experiments, we observed that a good compro-\nmise between reasonable sketch drawings and performance (or memory consumption)\ncan be obtained with very small grids (30 \u0002 30 or 50 \u0002 50 cells). On the other hand,\nlarge grids are in general too expensive and tend to create smaller influence zones, which\nresults in lines of units crossing the battlefield.\n",
      "page_number": 91,
      "chapter_number": 10,
      "summary": "This chapter covers segment 10 (pages 91-99). Key topics include sketch, sketches, and units. “Path Planning Tutorial,” AI Game Programming\nWisdom 2, CD-ROM, Charles River Media Inc., 2004.",
      "keywords": [
        "sketch",
        "battlefield",
        "Interface",
        "Sketch-Based Interface",
        "grid",
        "General Programming FIGURE",
        "units",
        "Strategy Games",
        "Programming FIGURE",
        "Real-Time Strategy Games",
        "cellular automaton",
        "River Media Inc.",
        "Automaton",
        "Game",
        "Charles River Media"
      ],
      "concepts": [
        "sketch",
        "sketches",
        "units",
        "commander",
        "game",
        "grid",
        "interface",
        "cell",
        "users",
        "battlefields"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 30,
          "title": "Segment 30 (pages 279-290)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 2,
          "title": "Segment 2 (pages 19-41)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 21,
          "title": "Segment 21 (pages 198-207)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 25,
          "title": "Segment 25 (pages 240-248)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 100-107)",
      "start_page": 100,
      "end_page": 107,
      "detection_method": "topic_boundary",
      "content": "Our proposal can be easily extended to other types of grid patterns, which might\nbe necessary on other applications. For instance, you can use a hexagonal grid to avoid\nthe repetitive patterns of movement directions that we experience with rectangular\ngrids, or even more general irregular grids can be employed to represent battlefields\nwith many obstacles, such as mountains or rivers. The only modification to accom-\nmodate other grid types relies on smaller modifications of the automaton-updating\nrule, which can be easily adjusted to each grid type by accessing their neighborhood\ninformation.\nReferences\n[Bosch06] Bosch, Marc ten. “InkBattle,” available online at http://inkbattle.\nmarctenbosch.com, May 6, 2006.\n[Philip07] Philip G. “Too Many Clicks! Unit-Based Interfaces Considered Harmful,”\navailable online at http://gamasutra.com/features/20060823/goetz_01.shtml,\nJune 23, 2007.\n[Weisstein07] Weisstein, Eric W. “Cellular Automaton,” available online at\nhttp://mathworld.wolfram.com/CellularAutomaton.html, June 23, 2007.\n[Weisstein07b] Weisstein, Eric W. “Totalistic Cellular Automaton,” available online\nat http://mathworld.wolfram.com/TotalisticCellularAutomaton.html, June 23,\n2007.\n1.6\nA Sketch-Based Interface to Real-Time Strategy Games\n67\n\n\nThis page intentionally left blank \n\n\n69\n1.7\nFoot Navigation Technique for\nFirst-Person Shooting Games\nMarcus Aurelius C. Farias\nDaniela G. Trevisan\nLuciana P. Nedel\nI\nnteraction control in first-person shooting (FPS) games is a complex task that nor-\nmally involves the use of the mouse and the keyboard simultaneously, and the mem-\norization of many shortcuts. Because FPS games are based on the character movement\nin the virtual world, a combination of left and right hands (keyboard and mouse) is\nused to control navigation and action. This gem proposes a technique where the player\ncontrols navigation with the foot, keeping both hands free for other types of interac-\ntion, such as shooting, weapon selection, or object manipulation. \nThis foot-based navigation technique allows walking forward and backward,\nturning left and right, and controlling acceleration. The tracking of the foot can be\ndone by using any motion capture device with at least two degrees of freedom, one\ntranslation and one rotation, although one or two additional degrees of freedom can\nbe useful too. \nIntroduction\nWe implemented the navigation technique in two ways. In the first implementation, a\nvery precise magnetic tracker (Flock of Birds from Ascension Technology Corporation)\nwas used to capture foot translation and rotation (see Figure 1.7.1 for an example of\nthis setup). Despite the very good results produced, this device is too expensive for\ndomestic users.\nThen, we tested a low cost and wireless solution for the same problem. In the sec-\nond implementation, we used ARToolKit—an open source library—and a regular\nWebcam to capture and identify the translation and orientation of a printed marker\n\n\nattached to the player’s foot (see Figure 1.7.1 for an overview of the setup). Because\nthis second implementation also presented good results and can be easily reproduced\nby everyone with average programming skills, we present it in detail in this gem,\navoiding explanation of the first implementation. However, the code for the first\nimplementation is available in the CD.\nThe following sections present the fundamentals of the foot-based navigation\ntechnique, as well as how we implemented this using computer vision—in other words,\nby exploring ARToolKit features. This gem also provides a sample game developed\nwith the specific purpose of evaluating the technique’s usability and precision. User\ntests reveal that because most players are used to playing with keyboard and mouse,\nthey were not as fast and precise with the use of the foot as a video game controller as\nexpected. However, all of them completed the experience in reasonable time and were\nable to avoid all obstacles easily. These results encourage us to believe that with some\ntraining, users can rapidly increase their performance and become familiar with this\nnew interaction technique, in the same way they are becoming experts with the new\nNintendo Wii controller, for example.\n70\nSection 1\nGeneral Programming \nFIGURE 1.7.1\nEnvironment setup using Flock of Birds motion capture\ndevice (left), and a square marker pattern attached to the player’s foot and a\nWebcam (right).\nNavigating with the Foot\nThe navigation technique proposed allows the players to control their movement speed\nand direction in an FPS game by using only one of their feet. First, users can choose if\nthey want to sit down or stand up to play. Then, to start walking at a constant speed,\nthey must move their foot forward (see Figure 1.7.2(c)). The farther forward the users\nplace their feet, the faster they will move in the virtual environment. To stop, they\n\n\nsimply move back to the starting position (see Figure 1.7.2(b)). To walk backward, the\nplayers slightly move their foot a few centimeters back (see Figure 1.7.2(a)). If the\nplayers want to turn left or right, they just turn their foot left or right, as can be seen in\nFigure 1.7.2(d–f). \nThe following sections explain how to implement this navigation technique using\ncomputer vision.\n1.7\nFoot Navigation Technique for First-Person Shooting Games\n71\nFIGURE 1.7.2\nGame navigation control using the right foot: backward (a); rest position (b);\nforward (c); turn left (d); rest position (e); and turn right (f).\nRequirements for an Implementation Based on Computer Vision\nBecause the computer can interpret the users’ movements, gestures, and glances, com-\nputer vision is a potentially powerful tool to facilitate human-computer interaction.\nSome basic vision-based algorithms include tracking, shape recognition, and motion\nanalysis. In this gem, we propose the use of a marker-based approach provided by\nARToolKit (http://sourceforge.net/projects/artoolkit), an open source library for build-\ning augmented reality applications we used to capture the movement of the player’s foot.\nMore details about the ARToolKit marker-recognition principle can be found at\nhttp://www.hitl.washington.edu/artoolkit/documentation/index.html.\n\n\nWe used a 3GHz Pentium 4 CPU, with 1GB of RAM, and an NVIDIA GeForce\n5200 graphics card. Taking into account the scenario shown in Figure 1.7.1 and consid-\nering that the captured image has 320 \u0002 240 pixels, we achieve approximately 1 milli-\nsecond for the marker recognition process. Such processing does not introduce any kind\nof delay in the interactive game response. More details involving performance studies\nand the minimum CPU requirements can be found at the ARToolKit Website.\nARToolKit can track the position and orientation of special markers—black\nsquares with a pattern in the middle—that can be easily printed and used to provide\ninteractive response to a player’s hand or body positions. In this case, the marker\nshould be printed and attached on the player’s foot in such a way that it remains\nalways visible to the Webcam, as shown in Figure 1.7.1.\nThe technique detailed in the next section requires that you print out the fiducial\nmarker defined in the file hiroPatt.pdf, available on the CD-ROM. Best performance\nis achieved if this is glued to a piece of cardboard, keeping it flat.\nInteraction implementation is almost trivial once you know how to extract the\nright information from ARToolKit. The first step consists of checking the foot rota-\ntion and detecting whether it is rotated to the left, to the right, or if it is pointing for-\nward or backward. You might need to use a different multiplier for each direction,\nbecause most people will find it easier to turn to one direction than the other depend-\ning on whether the right or the left foot is used to control the program.\nFirst, define a minimum value that will make the character start moving. When\nyou detect that the player’s foot has moved farther than this threshold, the character\nwill start moving accordingly, forward or backward. The farther the players move\ntheir foot, the faster they will go. The “Implementation” section discusses more details\nabout this.\nImplementation\nThe initialization of ARToolKit requires a few steps, but it is not hard to follow. We\nset up the camera and load the file that describes the pattern to be detected. There is\nalso an XML file (not shown here) with a few configurations, such as pixel format. It\ncan also be set up to show the settings at start-up so the users will be able to select the\npreferred camera settings. The following code is based on sample programs that come\nwith ARToolKit. \n#include <AR/config.h>\n#include <AR/video.h>\n#include <AR/param.h>\n#include <AR/ar.h>\n#include <AR/gsub_lite.h>\nARGL_CONTEXT_SETTINGS_REF argl_settings = NULL;\nint patt_id;\n72\nSection 1\nGeneral Programming \n\n\nvoid setup()\n{\nconst char *cparam_name = \"Data/camera_para.dat\"; \nconst char *patt_name = \"Data/patt.hiro\";\nchar vconf[] = \"Data/WDM_camera.xml\";\nsetup_camera(cparam_name, vconf, &artcparam);\n// Set up argl library for current context.\n// Don't forget to catch these exceptions :-)\nif((argl_settings = arglSetupForCurrentContext()) == NULL){\nthrow runtime_error(\n\"Error in arglSetupForCurrentContext().\\n\");\n}\nRead the pattern definition with the default pattern file Data/patt.hiro:\nif((patt_id = arLoadPatt(patt_name)) < 0){\nthrow runtime_error(\"Pattern load error!!\");\n}\natexit(quit);\n}\npatt_id is a pattern identification previously identified.\nvoid setup_camera(\nconst char *cparam_name, char *vconf, ARParam *cparam)\n{\nARParam wparam;\nint xsize, ysize;\n// Open the video path\nif(arVideoOpen(vconf) < 0){\nthrow runtime_error(\"Unable to open connection to camera.\\n\");\n}\n// Find the size of the window\nif(arVideoInqSize(&xsize, &ysize) < 0)\nthrow runtime_error(\"Unable to set up AR camera.\");\nfprintf(\nstdout, \"Camera image size (x,y) = (%d,%d)\\n\", xsize, ysize);\n// Load the camera parameters, resize for the window and init\nif (arParamLoad(cparam_name, 1, &wparam) < 0) {\nthrow runtime_error((boost::format(\n\"Error loading parameter file %s for camera.\\n\") %\ncparam_name).str());\n}\nNext, parameters are transformed for the current image size, because camera\nparameters change depending on the image size, even if the same camera is used.\narParamChangeSize(&wparam, xsize, ysize, cparam);\n1.7\nFoot Navigation Technique for First-Person Shooting Games\n73\n\n\nThe camera parameters are set to those read in and printed on the screen:\nfprintf(stdout, \"*** Camera Parameter ***\\n\");\narParamDisp(cparam);\narInitCparam(cparam);\nif(arVideoCapStart() != 0){\nthrow runtime_error(\n\"Unable to begin camera data capture.\\n\");\n}\n}\nThe quit function, referenced by setup, releases the resources previously allocated\nby ARToolKit.\nvoid quit()\n{\narglCleanup(argl_settings);\narVideoCapStop();\narVideoClose();\n}\nLet’s now show some sample code illustrating how to get the position of the marker\nusing ARToolKit. You first detect the marker in the frame using arDetectMarker in this\nway:\nARMarkerInfo *marker_info;\nint marker_num; // Count the amount of markers detected\narDetectMarker(image, thresh, &marker_info, &marker_num);\nThe markers found in the image by the library are returned in an array. This is\nuseful if you need to detect several markers at the same time. You can tell which\nmarker was found using marker_info[i].id and comparing it with the identifier\nreturned by arLoadPatt. In the following code, you will see how to get the transfor-\nmation matrix for the marker found at marker_info[i].\ndouble patt_centre[2] = {0.0, 0.0};\ndouble patt_width = 80.0;\ndouble patt_trans[3][4];\ndouble m[16];\narGetTransMat(&marker_info[i], patt_centre, patt_width, patt_trans);\narglCameraViewRH(patt_trans, m, 1.0);\nAs you can see, you need to call two functions: arGetTransMat and arglCamera\nViewRH. The former retrieves the transformation matrix used by ARToolKit, whereas\nthe latter converts it to the same format used by OpenGL so that you can use it to\ntransform scene objects (not needed here) or simply to see the transformation in a for-\nmat with which you are more familiar. \n74\nSection 1\nGeneral Programming \n",
      "page_number": 100,
      "chapter_number": 11,
      "summary": "This chapter covers segment 11 (pages 100-107). Key topics include marker, foot, and games.",
      "keywords": [
        "Foot Navigation Technique",
        "Navigation Technique",
        "Foot",
        "marker",
        "Technique",
        "Navigation",
        "Foot Navigation",
        "camera",
        "patt",
        "First-Person Shooting Games",
        "ARToolKit",
        "Shooting Games",
        "implementation",
        "error",
        "cparam"
      ],
      "concepts": [
        "marker",
        "foot",
        "games",
        "sections",
        "section",
        "patterns",
        "based",
        "camera",
        "users",
        "easily"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 11,
          "title": "Segment 11 (pages 111-118)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "makinggames",
          "chapter": 30,
          "title": "Segment 30 (pages 266-273)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "makinggames",
          "chapter": 35,
          "title": "Segment 35 (pages 308-315)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 108-115)",
      "start_page": 108,
      "end_page": 115,
      "detection_method": "topic_boundary",
      "content": "You then extract the translations and the rotation around the y-axis from matrix m.\n// m[12], m[13], m[14] == x, y, z\nif(m[12] > start_position_x + mov_eps){\nwalk_fwd(m[12] * mov_mult);\n}else if(m[12] < start_position_x - mov_eps){\nwalk_bck(m[12] * mov_mult);\n}\ndouble angle_y = asin(mat[8]);\nif(angle_y > rot_eps){\nturn_left(angle_y);\n}else if(angle_y < -rot_eps){\nturn_right(angle_y);\n}\nAs explained, the previous code can be used when the camera is on the player’s\nright side, so that it can see the right leg of the user. You just need to reverse the signs\nif you prefer to put the camera on the left. \nYou will also need some multipliers and epsilon values to adjust control sensitivity.\nWe suggest 10 for move_eps and 0.3 for rot_eps as start values (you can tweak the\nvalues according to your needs). We used 0.0625 for mov_mult, but this value depends\non the scale used in your virtual world. The variable start_position_x must be ini-\ntialized with the position that you want to use as neutral; that is, a position that guar-\nantees the character will not move. The simplest implementation is to assign to the\nfirst m[12] captured by ARToolKit when the program starts.\nThere are other values that can be useful in matrix m, including m[13] and m[14],\nbecause they inform the translation in the other two axes. For example, m[13] can be\nused for jumping and m[14] for strafing (sidestepping). However, you’ll likely find\nthat it’s too hard to control rotation and walk/strafe at the same time, so choose the\ncontrols for your game wisely. The other rotation axes do not make much sense in this\ncontext, so we will not discuss them.\nA Sample Game\nIn order to evaluate usability and playability of an FPS using foot navigation, we\nimplemented a sample FPS game containing a simple map that the user can explore\nusing the navigation technique proposed here. \nIn the first contact with the game, the player can interact in the training zone, the\nfirst area of the map (see Figure 1.7.3), gaining confidence and permitting input cali-\nbration. The game starts only when the user passes over the cyan tile (see Figure\n1.7.4). In the remaining regions of the environment, there are a few obstacles and\nsome red checkpoints on the floor that become green when crossed by the user (see\nFigure 1.7.5). Collisions with obstacles, including walls, are detected and visual feed-\nback (screen changes color, as shown in Figure 1.7.6) is sent to the player each time it\n1.7\nFoot Navigation Technique for First-Person Shooting Games\n75\n\n\nhappens. The game is over when, after passing over all checkpoints, the player attains\nthe exit shown in Figure 1.7.3 as the “end point.” The player’s goal is to complete this\ntask in the shortest time with as few collisions with obstacles and walls as possible.\n76\nSection 1\nGeneral Programming \nFIGURE 1.7.3\nSketch of the game circuit.\nFIGURE 1.7.4\nGame start indicator, indicated by the lighter\ntile in the foreground.\n\n\nAll game events are logged in a text file, so you can detect when users have trouble\navoiding a wall or finding a checkpoint. \nTests with Real Users\nWe have tested the proposed navigation technique with 15 people who played the\nsample game so we could measure their performance and hear their suggestions. We\nasked how comfortable they felt playing the game, how easy it was to use and learn,\nand how efficient they think it is. Six people found the navigation with the foot com-\nfortable, whereas four others found it more or less comfortable. Only one person\nconsidered the technique hard to use. Three people found the technique hard to learn\n(as opposed to “hard to use”). Regarding the efficiency, three people thought the tech-\nnique is inefficient, seven rated it as more or less efficient, and five people found it is\nefficient.\n1.7\nFoot Navigation Technique for First-Person Shooting Games\n77\nFIGURE 1.7.5\nView of the scenario with a checkpoint to be reached (left) and the same\ncheckpoint reached (right).\nFIGURE 1.7.6\nTwo frames of a game—before (left) and after (right) a collision with an\nobstacle.\n\n\nThis data shows us that the interaction is intuitive and reasonably easy to use after\nsome practice. In the sample game, we measured the number of collisions and the time\nthe users took to cross all checkpoints and reach the end goal. Because most users had\nexperience in the use of a keyboard and mouse combination, it is expected that they\nwere not as fast when using their foot as a video game controller. However, everyone\ncompleted the task in a reasonable time and easily avoided all obstacles. We noticed\nthat it is especially easy to move fast and suddenly stop, because the acceleration control\nis intuitive (you just move your foot forward as far as you can, as long as the camera still\nsees it) and when you want to stop, you need only to go back to the rest position).\nConclusion\nThis gem presented a new technique to allow navigation for FPS games using one of\nthe player’s feet. It also described a low-cost solution to implement it using computer\nvision, more specifically ARToolKit open source library that easily tracks foot move-\nments and then transforms them into interaction controls for the game environment. \nThere are some limitations inherent in purely computer-vision-based systems.\nNaturally, the marker position is computed only when the tracking marks are in the\ncamera’s field of view. This may limit the movement of the interaction, also meaning\nthat if the users cover up part of the pattern with other objects, navigation is stopped.\nOther factors such as range issues, pattern complexity, marker orientation relative to the camera,\nand lighting conditions influence detection of the pattern.\nOn the other hand, there are basically three advantages in this technique against\ntraditional approaches. First, by using a tracker, users have more degrees of freedom to\nwork with. The users can move their foot in the 1D, 2D, or 3D space and rotate\naround one axis (left-right rotation). Moreover, the mouse and keyboard could be used\nin other ways, because navigation is no longer a concern. For example, they can be used\nfor aiming, shooting, selecting, and manipulating on-screen objects. Lastly, using the\nwhole body to interact with the game gives a deeper immersion for the player, as games\nfor the Nintendo Wii console have shown.\nFuture Work\nThere are many ways to explore the possibilities of this interaction technique. The\nfirst one is to add new simple commands, such as jump or sidestepping. Another\nalternative is to reuse the mouse and keyboard commands that are no longer needed\nand to assign more ergonomic commands to them.\nMultiplayer games are also an untapped possibility, because the marker-based\ninteraction technique, as well as the Flock of Birds, allow a multiplayer functionality.\nIt is possible to attach and track a different marker for each player as long as the Web-\ncam can capture them. If this is not the case, it is always possible to use two or more\ncameras concurrently.\n78\nSection 1\nGeneral Programming \n\n\nTo address portability, as well as to avoid the occlusion problem that can occur\nbetween the Webcam and the markers, some kind of interaction based on remote\ncontrolled devices could be implemented. The main idea remains the same, namely:\nthe controller should be attached to the user’s foot while movements should be per-\nformed in exactly the same way. Finally, we estimate a different game design, such as\na Super Monkey Ball style–game, could be more attractive to this type of control.\nAcknowledgments\nThe authors would like to thank Fábio Dapper and Alexandre Azevedo for their work\nin the conception and first implementation of this technique, as well as all people who\nplayed the game, giving us valuable feedback. Finally, we thank the Brazilian Council\nfor Research and Development (CNPq) for financial support.\n1.7\nFoot Navigation Technique for First-Person Shooting Games\n79\n\n\nThis page intentionally left blank \n\n\n81\n1.8\nDeferred Function Call\nInvocation System\nMark Jawad, Nintendo of America Inc.\nmark.jawad@gmail.com\nI\nt seems that lately it has become common for most computing systems to be designed\naround multiple processors; in fact that appears to be a core design axiom within the\nengineering community right now. Modern video game machines are no exception to\nthe trend, with multi-core CPU designs being prevalent in many of the game machines\non sale today. Additionally, most of them rely on auxiliary processing acceleration chips\nsuch as programmable IO controllers, DMA engines, math co-processors, and so on.\nThese run in parallel with the CPU(s), and are there as part of the system so that we as\nartists can push our craft ever farther. Each of these components may complete their\ntasks independently of the others, but all send along a notification to the game when the\ntask is done (usually in the form of an interrupt), so that the game can schedule other\nwork. \nThe way in which a game handles these notifications can either lead to a fairly\npainless and bug-free experience, or to a frustrating world of head-scratching, bug-\nchasing hurt. This article discusses the implications of asynchronous events and other\ntiming-related issues, and provides a system to handle them gracefully.\nA Matter of Time\nFrom the game’s point of view, these notifications may happen at any time, and thus\nshould be treated as true asynchronous events. These event notifications are great because\nthe game can run on the main processor at the pace it desires, while other processors do\nauxiliary work at whatever rate they can. However, there are some problems. If handled\nimproperly, these notifications can lead to timing-related bugs or system instability. \nTiming-related bugs are notoriously difficult to track down, and can happen if you try to\nuse memory before another system client is done with it, or if you change the game state\nwithout proper synchronization primitives. System instability bugs are even worse, and\ncan occur if a callback/interrupt handler runs for too long during an interrupt period,\nthus causing other interrupt signals (or subsequent interrupt signals of the same type) to\nbe missed. Such a situation can lead to all sorts of odd program behavior.\n\n\nDevelopers working on handheld gaming systems are faced not only with these\nissues but additional ones as well. A major focus that these systems have is the concept\nof a vertical blanking period, which is the point in time where the graphics engine(s)\ngo idle while the display device prepares for the next frame of output. It is during this\nsmall window of time that developers are allowed to access the memory and registers\nof the graphics system. During this time, you need to quickly determine what new\ndata is to be uploaded, as well as what settings need to change on the graphics chip. A\ndeveloper must make the changes and data uploads as quickly as possible. If they fail\nto do the work within the window, the graphics may show corruption or other notice-\nable artifacts.\nThese various issues all deal with time in some form or another—time is always\nan enemy of game developers and we never seem to get enough of it. Because we\nprobably can’t get more time in which to do work, we’ll have to settle for making\nsmarter use of the time that we’ve got. One approach is that instead of doing all the\nwork at once, you do some of it now and the rest of it later. In essence, you’re making\na decision now but deferring the actual work to some point in the future. Because\nmost work is handled by making function calls, this gem uses a system that queues up\nfunction calls and their parameters, and has the ability to invoke them sometime\nlater—hence, a deferred function call system.\nCase Studies\nLet’s examine the vertical blank period where you have very limited time. The ideal\nsituation is to not do any time-consuming logic at all during this window; rather you\nshould be purely focused on uploading lots of new data as quickly as possible. So why\nnot do all of the logic relating to what data to upload (and where to put it) earlier on\nin the frame when you are not under such extreme time pressure? You queue up\nsource and destination addresses, transfer size, and perhaps take note of the “type” of\ndata (textures, palettes, and so on). Then when the blanking window opens, you run\nthrough the queue and do all of the transfers.\nBut what happens if the type of data determines the choice of function used to\nupload the data? Well, then you have a switch statement, jump table, or cascading\nseries of if statements in order to determine which function to call. Such an approach\nisn’t worth a second thought on a PC or home console. On a portable machine with a\nlow clock speed it is definitely worth a second thought, especially given the small\nblanking window coupled with how precious each cycle is. Therefore, the ideal\napproach is to also pre-determine the function to call when you’re setting up the\naddresses and transfer sizes. Then, during the vertical blanking window, all you have\nto do is load the function’s address along with the necessary parameters and jump off\nto it. In essence, you’re setting up the function call earlier in the game loop, but defer-\nring the actual invocation until the vertical blank period arrives.\n82\nSection 1\nGeneral Programming \n",
      "page_number": 108,
      "chapter_number": 12,
      "summary": "The other rotation axes do not make much sense in this\ncontext, so we will not discuss them Key topics include game, gaming, and control. The variable start_position_x must be ini-\ntialized with the position that you want to use as neutral; that is, a position that guar-\nantees the character will not move.",
      "keywords": [
        "game",
        "Foot Navigation Technique",
        "navigation technique",
        "time",
        "First-Person Shooting Games",
        "technique",
        "Shooting Games",
        "Sample Game",
        "navigation",
        "foot navigation",
        "System",
        "foot",
        "work",
        "sample FPS game",
        "angle"
      ],
      "concepts": [
        "game",
        "gaming",
        "control",
        "controller",
        "time",
        "timing",
        "user",
        "foot",
        "interrupt",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "makinggames",
          "chapter": 19,
          "title": "Segment 19 (pages 161-168)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 10,
          "title": "Segment 10 (pages 182-202)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 39,
          "title": "Segment 39 (pages 371-380)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 22,
          "title": "Segment 22 (pages 207-219)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 40,
          "title": "Segment 40 (pages 383-392)",
          "relevance_score": 0.46,
          "method": "api"
        }
      ]
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 116-124)",
      "start_page": 116,
      "end_page": 124,
      "detection_method": "topic_boundary",
      "content": "The same tactic can be used on home consoles to deal with asynchronous notifi-\ncations. Maybe you get a callback that informs you that a certain file read has com-\npleted, or a memory card was inserted, or that the user has inserted or removed a\ncontroller peripheral. Some of these situations require more work than others, but\nmost of them are such that you don’t need to do the bulk of the work right there and\nthen. Yes, the game logic needs to know about the file read completion, or the new\ncontroller, but why not schedule the real processing of the event sometime later near\nthe end of the current processing frame?\nQueue up the incoming notification parameters (taking care to save any important\ntemporal data that might get lost) and deal with them later. This way you exit the call-\nback/interrupt as fast as possible, which is always a good thing. By hoisting the notifi-\ncation processing out of the interrupt handler and into a dedicated place in your game\nloop, you help keep the game behavior deterministic. This in turn nearly eliminates\ntiming-related bugs. An additional benefit of the approach is that you can guarantee\nthat no one on the team will mistakenly run a process that takes “too long” while inter-\nrupts are disabled, because the handling process is now at a known point in the simu-\nlation loop and not in an interrupt handler. So it can take all the time it needs.\nCategorizing a Function Call\nMost functions are set up to take their arguments directly in the function parameters,\nsuch as this one from the C standard library\nvoid *memccpy(void *dest, const void *src, int c, size_t count);\nwhich takes four parameters in its argument list. This is the most common function\ncall type in C-derived languages today, and is categorized as taking “direct” parame-\nters. Other functions, such as this one from the Windows SDK\nATOM RegisterClassEx(CONST WNDCLASSEX *lpwcx);\ntake a single argument, but really that argument just points to a control structure\nwhere the 12 “actual” parameters reside. This type of call can be categorized as a func-\ntion that takes an “indirect” parameter. Often, the parameter for the indirect argu-\nment sits on the stack of the callee (as opposed to being stored in the heap), and is\ntherefore lost once the callee exits. \nOn the surface, it would appear that the second type is just a subset of the first.\nHowever, there is an important distinction that you need to make note of. As men-\ntioned earlier, sometimes you need to store data temporally. In the case of a deferred\nfunction, where do you store indirect argument data blocks? The callee function that\nwould normally have created the argument structure will be long gone by the time the\ndeferred function is called (and with it, the data that was in its stack frame). The solu-\ntion is to hand out a pool of memory large enough to store the argument. This pool\n1.8\nDeferred Function Call Invocation System\n83\n\n\nwill come from the deferred system itself, and the argument will be constructed in-\nplace there instead of on the stack.\nA Look at the System\nThe header file for the system, deferred_proc.h, is quite small, and should be trivial to\nintegrate into your game. The header contains one function for initializing an instance\nof the system, a couple of functions and macros for adding deferred calls to lists of\ndirect-argument (DA) or indirect-argument (IA) functions, and one for executing the\ndeferred functions (and then resetting the lists when that is done).\nThe majority of the code is presented in C, although there is one function that\nmust be written in assembly code. That function is the deferred function caller. This\none function is platform-dependent, and must take into account the ABI (application\nbinary interface) of the platform that it’s being run on. So to port the system, you\nneed only to rewrite the deferred function caller. \nPlease note that the system presented herein is limited to making function calls\nthat only use arguments kept in general purpose registers (GPRs). Floating point val-\nues, native floating point vectors, or other data types that aren’t intended for a general\npurpose register are not allowed as parameters to the deferred functions. This is done\nto keep the system simple, although support for such things could be added if needed.\nAlso in keeping things simple the system allows a maximum of four parameters.\nI’ve found four parameters to be sufficient for most cases. One consideration was\nthat if more than four are used, you might have to transfer some of the parameters in\nregisters and others on the stack, depending on the platform being targeted, which\nagain could increase complexity—and complexity is something that this system\nactively tries to avoid.\nBecause this system is limited to four direct parameters, you must use the indirect\ncall for anything where five or more parameters are needed. (Don’t forget that the hid-\nden this parameter of C++ instance functions counts as one of the four parameters\nthat you can accept for direct-argument function calls!)\nThe deferred function caller, dfpProcessAndClear, may be a little difficult to\nunderstand because it’s in assembly, but the idea behind it is very simple. All it has to\ndo is loop through the contents of the list and dispatch (call) each entry. At each loop\niteration, it needs to load at minimum a control word that describes the following:\n• The function type (DA or IA)\n• The number of GPR parameters it should load\n• Any additional bytes of data that the call took from the list\nOf course, it will also need to load the target function’s address. Then it needs to\nload any parameters stored for use with the function. Once this information is loaded\nit can branch off to the target function. When that function returns, you go on to the\nnext iteration of the loop. Once done, you reset the list and exit. Please note that the\nsample code is not thread-safe. Please see the CD for the complete source code.\n84\nSection 1\nGeneral Programming \n\n\nConclusion\nDeferred functions are an extremely useful tool for game developers on all platforms,\nespecially those working on home consoles or handheld systems. Their existence can\nhelp make the most of time critical sections of code, such as the vertical blank period\nand interrupt processing sequences, and the system that tracks them can handle many\ndifferent types of function signatures. The idea is flexible, easily extendable, efficient,\nand very portable—traits that all of us can surely appreciate.\nReferences\n[Earnshaw07] Earnshaw, Richard. “Procedure Call Standard for the ARM Architec-\nture,” available online at http://www.arm.com/pdfs/aapcs.pdf, January 19, 2007.\n1.8\nDeferred Function Call Invocation System\n85\n\n\nThis page intentionally left blank \n\n\n87\n1.9\nMultithread Job and\nDependency System\nJulien Hamaide\nJulien.hamaide@gmail.com\nT\nhis gem puts next generation multi-core capabilities into the hands of all program-\nmers without the need to comprehend more complex multithreading concepts to\ncreate tasks. By providing a simple system that automatically manages dependencies\nbetween tasks, there is almost no need of other more specific synchronization mecha-\nnisms. The system is adapted for small- to medium-sized tasks such as animation\nblending or particle system updates.\nIntroduction\nWhen thinking about multithreading, synchronization problems come quickly to mind.\nDeadlock is a fear for every programmer. Even for small projects, a multithreading\nsolution is typically rewritten every time due to the interdependencies unique to the\nproject. Therefore, programmers require knowledge of synchronization primitives and\ntheir intricacies. For example, if the system is time-critical, a lock-free algorithm must be\nused. Additionally, the more complex the system, the higher the chance of creating bugs. \nThe primary goal of the system is to provide a cross-platform framework that hides\nthe complexity of multithreading issues. Any programmer, even those not familiar with\ncommon multithreading concepts, should be able to use the system. There is no con-\ncept of threads in the system; it is replaced by the concept of jobs, which are defined as\nunits of work.\nThe process of creating a job is shown in the following code. Cross-platform\ncompatibility is achieved by encapsulating primitives such as critical sections and\nthreads in classes. The framework code is then created with those classes. The demo\non the CD contains the job manager and the wrapper classes.\nvoid MultithreadEntryFunction( void * context )\n{\n//Do your stuff here\n}\nPARALLEL_JOB_HANDLE handle;\n\n\nhandle = PARALLEL_JOB_MANAGER_CreateJob( \n&MultithreadEntryFunction, context);\nPARALLEL_JOB_MANAGER_ScheduleJob( handle );\nThe performance of the system is also optimized by preventing thread creation\nand deletion for small-sized tasks. The framework creates a thread pool that executes\nthe tasks given to the system. The threads are created only once at the launch of the\napplication. The number of threads depends on the platform and is dynamically eval-\nuated if necessary. (On a PC, the number of cores on the target platform may not be\nknown at compile time.) This approach is similar to OpenMP’s approach to distribut-\ning work over multiple threads [OpenMP].\nThe system also supports prioritization of tasks. If some tasks take more time or\nhave many dependent tasks, their priority can be increased to ensure early execution.\nSupport for idle tasks is reviewed in the future work section, but has not yet been\nimplemented.\nThe synchronization of jobs is handled by a dependency system. It allows the\ncreation of synchronization points and job dependencies. The dependency system is\ncovered in detail in following sections.\nThe Job System\nThe job system is mainly composed of four types of objects:\n• The job, which is unit of work delimited in a callback function. It must be\ndesigned to be thread-safe.\n• The manager, which maintains the job list by priority and type.\n• The scheduler, which chooses the task that suits best depending on priority and\ntype.\n• The workers, which execute the jobs assigned to them.\nJob\nJobs are represented by the class shown in the following code. It simply encapsulates a\nclosure (a function with a predefined argument). The structure also contains the pri-\nority and the handle of the job. PARALLEL_JOB_PRIORITY is an enum containing classic\npriority values (that is, High, Low, and so on). The handle identifies the job in the\nsystem. It is the object that is used outside the system to reference a job. The Type field\nis explained later.\nclass PARALLEL_JOB\n{\npublic :\nvoid Execute()\n{\nFunction( Context );\n}\n88\nSection 1\nGeneral Programming \n\n\nprivate :\nPARALLEL_JOB_HANDLE\nJobHandle;\nPARALLEL_JOB_PRIORITY\nPriority;\nPARALLEL_JOB_TYPE\nType;\nPARALLEL_JOB_FUNCTION\nFunction;\nvoid\n* Context;\n};\nManager\nThe manager class is a singleton providing the public interface of the system. As the\nsystem’s central point, it maintains the list of jobs in a multithread-safe manner. The\nmanager is responsible for worker and scheduler thread creation and initialization.\nThe interface of the manager is shown in the following code. The creation and sched-\nuling of jobs is separated to allow the user to add dependencies to and on the created\njob. In the demo code, every call is protected with critical sections [MSDN1]. Lock-\nfree algorithms are better choices but increase the complexity of the system. To ease\nthe understanding of concepts, only the critical section version is given. Implementa-\ntion of lock-free algorithms is left as an exercise for the developers.\ntypedef void (*PARALLEL_JOB_FUNCTION )( void* );\nPARALLEL_JOB_HANDLE CreateJob(\nPARALLEL_JOB_FUNCTION function,\nvoid * context,\nPARALLEL_JOB_PRIORITY priority = PARALLEL_JOB_PRIORITY_Default\n);\nPARALLEL_JOB_HANDLE CreateAndScheduleJob(\nPARALLEL_JOB_FUNCTION function,\nvoid * context,\nPARALLEL_JOB_PRIORITY priority = PARALLEL_JOB_PRIORITY_Default\n);\nvoid ScheduleJob(\nPARALLEL_JOB_HANDLE job_handle\n);\nThe PARALLEL_JOB_HANDLE is a structure that contains a unique identifier and a\ndependency index. The dependency index is covered in the following sections.\nScheduler\nThe scheduler is a thread object that waits for the worker threads to finish. As soon as\none is free, it selects the next job, assigns it to the worker thread, and puts itself back\nto sleep. The following code shows the main loop of the scheduler in pseudocode. \n1.9\nMultithread Job and Dependency System\n89\n\n\nPARALLEL_JOB_MANAGER_GetNextJob returns the best job to be executed next. The\ndecision rules are presented later. The WaitForJobEvent allows the scheduler to sleep\nif there is no new job available and some worker threads are free. When new jobs are\navailable, the WaitForJobEvent is signaled; otherwise, it is reset [MSDN2].\nwhile ( !ThreadMustStop )\n{\nthread_index\n= PARALLEL_WaitMultipleObjects( worker_thread_table );\nif( PARALLEL_JOB_MANAGER_GetNextJob( next_job, thread_index ) )\n{\nworker_thread_table[ thread_index ]->SetAssignedJob(next_job);\nworker_thread_table[ thread_index ]->WakeUp();\n}\nelse\n{\nWaitForJobEvent.Wait();\n}\n}\nWorker Threads\nThe worker does the dirty work. When the operating system permits, it is assigned to\na processor. The pseudocode is shown in the following code. The worker waits for\nDataIsReadyEvent to be signaled, meaning it has been assigned to a new job. It then\nexecutes it. When finished, it informs the dependency manager that its job is finished.\nFinally, it signals the scheduler that it is waiting for a new job. \nIn the implementation, the number of worker threads is set to the number of\navailable processors. If the main thread is always busy, it can be useful to set the num-\nber of worker threads to the number of available processors, minus one. \nwhile ( !ThreadMustStop )\n{\nPARALLEL_WaitObject( DataIsReadyEvent, INFINITE );\nAssignedJob.Execute();\nPARALLEL_DEPENDENCY_MANAGER_SetJobIsFinished( AssignedJob );\nWaitingForDataEvent.Signal();\n}\nCache Coherency\nTo ensure code and data cache coherency, both jobs and worker threads have been\nassigned types. The type of the job can be anything you want, depending on your sys-\ntem. In this implementation, we choose types such as particle systems, animation,\npathfinding, and so on. The type will be used in job selection, so choose them care-\nfully. The cache coherency is really specific to each platform. Some tuning can be nec-\nessary to achieve the maximum speed.\n90\nSection 1\nGeneral Programming \n\n\nJob Selection \nWhen the scheduler asks for the next job to execute, the manager uses simple rules to\nchoose it. The pseudocode is shown here after. The selection tries to balance between\nthread type changes and high priority tasks. This algorithm is not adaptive, but the\nhighest priority thread can be boosted every time a lower priority thread is chosen due\nto a type difference. In this way, you can allow scheduling of only two or three lower\npriority threads before the highest priority thread is scheduled.\ncurrent_type = type of worker thread\nif( job of type current_type exists )\n{\nnew_job = job of type current_type with highest priority\nif( priority of new_job – highest priority available > 2 )\n{\nnew_job = job with highest priority\nworker thread type = new job type\n}\n}\nelse\n{\nnew_job = job with highest priority\nworker thread type = new job type\n}\nThe Dependency Manager\nThe dependency manager is an object-based system that ensures synchronization of\ntasks. The current implementation has two types of entry: jobs and groups. The system\nconstructs a graph of dependencies between entries. Groups allow the users to create\nsynchronization points such as the start of rendering. Figure 1.9.1 shows a typical\ndependency graph created by the system.\n1.9\nMultithread Job and Dependency System\n91\nFIGURE 1.9.1\nExample dependency graph.\n",
      "page_number": 116,
      "chapter_number": 13,
      "summary": "This chapter covers segment 13 (pages 116-124). Key topics include function, functions. Queue up the incoming notification parameters (taking care to save any important\ntemporal data that might get lost) and deal with them later.",
      "keywords": [
        "Job",
        "System",
        "PARALLEL",
        "Function",
        "priority",
        "deferred function",
        "type",
        "thread",
        "Job System",
        "jobs",
        "Dependency System",
        "Function Call",
        "Deferred Function Call",
        "Call Invocation System",
        "manager"
      ],
      "concepts": [
        "function",
        "functions",
        "job",
        "jobs",
        "void",
        "type",
        "thread",
        "depending",
        "dependency",
        "dependent"
      ],
      "similar_chapters": [
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 12,
          "title": "Segment 12 (pages 103-116)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 52,
          "title": "Segment 52 (pages 1041-1063)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 12,
          "title": "Segment 12 (pages 104-111)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 32,
          "title": "Segment 32 (pages 322-329)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 15,
          "title": "Segment 15 (pages 132-140)",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 125-136)",
      "start_page": 125,
      "end_page": 136,
      "detection_method": "topic_boundary",
      "content": "The Dependency Graph\nThe dependency graph is stored inside dependency entries. Each stores a list of depen-\ndent objects. A dependency can be in two states: met or blocked. If a dependency entry\nis met, it means that every other entry that depends on it can be executed. If only one\nof its dependencies is blocked, an entry is also blocked.\nTo prevent polling of dependencies, an entry contains a dependency count. For\neach blocked dependency an entry depends upon, the counter is increased. When the\ncounter is zero, the dependency is met; otherwise, it is blocked. When a dependency\nenters the met state, it iterates over all its dependent objects to decrease their counts.\nSimilarly, when a dependency becomes blocking, it iterates all its dependent objects\nto increase their counts. The met and blocked information is propagated along the\ngraph. Figures 1.9.2 and 1.9.3 show a step of propagation. When job 1 becomes met,\nthe Animation group also becomes met. The dependency count of job 3 decreases by\n1, whereas the dependency count of PreRender group decreases by 2.\n92\nSection 1\nGeneral Programming \nFIGURE 1.9.2\nInitial graph with dependency count.\nFIGURE 1.9.3\nGraph after propagation.\n\n\nDependency Storage\nThe dependency table is a sparse vector of pointers to entries. An index dispenser is\nused to allocate a free slot to the current dependencies. The table grows if necessary,\nbut never shrinks. When an entry is created, an index is requested from the index dis-\npenser. On deletion its index is recycled and will be used by the next entry created.\nThis recycling means that the dependency cannot be identified by its index alone.\nThe PARALLEL_DEPENDENCY_IDENTIFIER has been introduced to solve this problem.\nThis structure contains an index to the table of dependency entries and a unique\nindex. This identifier serves as a handle for the users. \nThe following code shows the creation of a dependency group and links. In this\nexample, the PreRender group will wait until the Animation group is met. This can be\nused to ensure that all animations are computed before the rendering occurs. All ani-\nmation jobs will set up a dependency over the Animation group. Figure 1.9.4 shows\nthe graph created by such a construction. If a dependency no longer exists (that is, the\njob is finished), it is considered as met. \nPARALLEL_DEPENDENCY_IDENTIFIER\nanimation_identifier, prerender_identifier;\nprerender_identifier\n= PARALLEL_DEPENDENCY_MANAGER_CreateEntry( \"PreRender\nSynchronization\");\nanimation_identifier\n= PARALLEL_DEPENDENCY_MANAGER_CreateEntry( \"Animation \nSynchronization\");\nPARALLEL_DEPENDENCY_MANAGER_AddDependency(\nanimation_identifier, prerender_identifier );\n// during the game\nblend_job_handle\n= PARALLEL_JOB_MANAGER_CreateJob( &MultithreadBlendAnim, context);\nPARALLEL_DEPENDENCY_MANAGER_AddDependency(\nblend_job_handle, animation_identifier );\nPARALLEL_JOB_MANAGER_ScheduleJob( handle );\n1.9\nMultithread Job and Dependency System\n93\nFIGURE 1.9.4\nDependency graph created by previous example.\n\n\nThis example raises a problem—the job is a volatile object and when finished, it\nis destructed. Groups are stable entries. To address these needs, two types of link have\nbeen created: \n• Dynamic link: As soon as the dependency is met, the link is removed.\n• Static link: The link is established at launch and always remains valid.\nThe entry contains two tables, one for static links and one for dynamic links.\nWhen the entry is met, the dynamic link table is emptied. In the previous example,\nthe link between the Animation Synchronization entry and the PreRender Synchro-\nnization entry is transformed into a static link. This example is used in the demo.\nGroup Entry\nA group entry creates a synchronization point. The manager provides a way to wait until\nthe dependency is met. A call to PARALLEL_DEPENDENCY_MANAGER_WaitForDependency\nwill stall until the dependency is met. It is useful, for example, to wait for all computa-\ntions to finish before starting the rendering. This behavior is implemented with events.\nAn event is created for each instance of a group. This means that an event is created \nfor each group, even if you don’t want to be able to wait for it in your code. If you want\nto avoid this waste of events, you can create two types of groups—waitable and non-\nwaitable groups. \nJob Entry\nA job entry is used for two purposes: \n• To synchronize other entries with the job\n• To synchronize the launch of the job\nThe entry is created with a dependency count set to one. This represents the\ndependency that the execution of the job sets on the entry. To prevent the need for\ntwo entries, one for the launch and one for the end of the job, the entry count is\npolled by the job manager. If the entry’s dependency count is one, there is no depen-\ndency left other than the job execution itself, so the job can be executed. As shown in\nthe worker thread pseudocode, it reports that the job is finished to the dependency\nmanager. The dependency count is then set to zero, and the entry becomes met and\npropagates the information to all entries that depend on it.\nFuture Work\nThe following sections discuss the future work—areas of this system that might be\ngood candidates for enhancements and extensions.\n94\nSection 1\nGeneral Programming \n\n\nIdle Tasks and Preemption\nIn the presented system, every started task occupies its worker thread until it is fin-\nished. This constraint prevents users from scheduling low priority tasks that can be\ncomputed over several frames. Three solutions are possible to fix this problem: \n• Preemption: Low-priority tasks could be preempted if a new job is pushed into the\nsystem. Not all operating systems allow the users to code their own version of\nthread context switching. This solution is also platform-specific.\n• Cooperative multithreading: Low-priority tasks can be implemented using coopera-\ntive multithreading, but each task should use a special function to allow the system\nthe opportunity to execute higher-priority jobs. Such a system can be implemented\nusing, for example, Windows fibers [MSDN3].\n• Low-priority thread: The system can create new lower-priority threads, letting the\noperating system scheduler preempt them when other worker threads are work-\ning. This solution is cross-platform and easy to implement as long as the OS\nscheduler is good enough.\nThe first solution is the best from a control point of view. No special work is needed\nin the job code, you control exactly when the job is preempted but it is platform-specific\nand can be tricky to implement. The second solution is easier to implement, but it is still\nplatform-specific. Another drawback is that the job code must be adapted to allow pre-\nemption. The third solution is by far the easiest to implement, but you don’t have much\ncontrol about when and how your thread will be preempted. Ideally, the third solution\nshould be implemented first, with a view to switching to other solutions if necessary.\nIntegration of Synchronization Primitives \ninto the Dependency System\nThe dependency system supports only two types of entry: job and group. It means that\nyou can’t interface the system with an already existing synchronization mechanism. For\nexample, a loading thread can use semaphores to communicate with other threads. If you\nwant your job to wait for some resource to be loaded, the system does not allow it. A solu-\ntion is to extend the entry to other types: semaphores, events, and so on. The dependency\nsystem has been designed to be extensible. The PARALLEL_DEPENDENCY_ENTRY can be\nderived to support primitives easily. Virtual functions that inform the dependency state\nof the new synchronization mechanism must be written. The job is complete when the\nimplementation of those virtual functions has been written. The new system is then\nready to interact with the dependency system.\nConclusion\nThe presented system provides the power of multithreading to all programmers. It\nabstracts the complexity and the danger of using synchronization primitives. The critical\ncode is consolidated in a single place and synchronization issues are solved only once. By\n1.9\nMultithread Job and Dependency System\n95\n\n\nproviding a dependency graph system, the jobs can be chained without any other action\nthan creating a dependency link between them. Already rather complete, the system has\nbeen designed to be extensible. The implementation is cross-platform; the only things\nyou need to port are the wrapper classes provided in the demo package (mainly thread,\ncritical section, and event). Performance is also targeted by limiting the overhead of\nthread creation. The system can also interface with existing synchronization techniques\nwith the help of the PARALLEL_DEPENDENCY_ENTRY interface. All your programmers should\nnow be able to create and schedule a job enjoying considerably less threading-related\ncomplexity than without this gem. Enjoy!\nReferences\n[MSDN1] “Critical Section Objects (Windows),” available online at http://msdn2.\nmicrosoft.com/en-us/library/ms682530.aspx, June 1, 2007.\n[MSDN2] “Event Objects (Windows),” available online at http://msdn2.microsoft.\ncom/en-us/library/ms682655.aspx, June 1, 2007.\n[MSDN3] “Fibers (Windows),” available online at http://msdn2.microsoft.com/en-\nus/library/ms682661.aspx, June 1, 2007.\n[OpenMP] OpenMP Architecture Review Board. “OpenMP: Simple, Portable, Scal-\nable SMP Programming,” available online at http://www.openmp.org.\n96\nSection 1\nGeneral Programming \n\n\n97\n1.10\nAdvanced Debugging\nTechniques\nMartin Fleisz\nMartin.fleisz@kabsi.at\nD\nue to the high expectations that players have in games nowadays, development\ngets more and more complex. Adding new features or supporting new technolo-\ngies often requires more code to be written, which automatically leads to more bugs.\nGame projects are usually tightly scheduled and hard-to-find bugs are often the cause\nfor delays. Although you cannot avoid that erroneous code is written, you can try to\nimprove the process of finding and fixing these errors. If an application crashes or\ndoes something wrong, information about the crash becomes the most important\ndata in order to find the cause of a malfunction. Therefore, this gem presents a small\nframework that detects and reports a vast number of programming errors and can be\neasily integrated into any existing project.\nApplication Crashes\nThe reasons that an application crashes are manifold. Stack overflows, dividing by\nzero, or accessing an invalid memory address are just a few of them. All of these errors\nare signaled to the application through exceptions, which, if you fail to handle them\nproperly, will terminate the game. You can distinguish between two different types of\nexceptions. Asynchronous exceptions are unexpected and often caused by the hardware\n(for example, when your application is accessing an invalid memory address). \nSynchronous exceptions are expected and usually handled in an organized manner.\nThese exceptions are explicitly thrown by the application—that is, using the throw\nkeyword in C++. \nException Handling\nThe C++ language offers built-in support for handling synchronous exceptions\n(through the try/throw/catch keywords). However, this is not the case for handling\nasynchronous exceptions. Their handling depends on how the underlying platform\nimplements them.\n\n\nMicrosoft unified the handling of both exception types on their Windows plat-\nforms and refers to it under the term Structured Exception Handling (SEH). If you\nwant to know how SEH and the new Vectored Exception Handling technologies\nwork in detail, refer to [Pietrek97] and [Pietrek01].\nWhat you have to know is what happens if the operating system does not find a\nhandler for an exception. In this case, the UnhandledExceptionFilter function is called\nby the kernel. [Pietrek97] shows a pseudocode sample of what this function does \nin detail. The most interesting part is that the function will execute a user-defined\ncallback that you can register using the SetUnhandledExceptionFilter API. Using this\ncallback, you will be notified when your application crashes so that you can do the\nerror reporting.\nUNIX-based operating systems use a different approach. In the case of an asyn-\nchronous exception, the system sends a signal to the application, at which time its\nnormal execution flow is stopped and the application’s signal handler is called. Using\nthe sigaction function, you can install custom signal handlers where you are going to\ndo your error reporting later on.\nBesides doing some error reporting, you might want to consider saving the cur-\nrent game in your exception handler. Nothing bothers a gamer more than a crashing\ngame after having played for hours and not having saved. Of course such tasks should\nall be done after you have finished your reporting.\nReporting Unhandled Exceptions\nIf an unhandled exception occurs and the process is being debugged, the debugger\nwill break at the code location that caused the error. Of course it would be nice to\nhave similar information about crashes even if there is no debugger attached to the\napplication. For this purpose, you can utilize crash dump files that contain a snapshot\nof the process at the time of the crash.\nOn Windows platforms, you can use the Microsoft Debugging Tools API that is\nprovided by the dbghelp.dll library. With help of the MiniDumpWriteDump function,\nyou can create a mini-dump file of the running process. Unlike a usual crash dump,\nlike the ones the old Dr. Watson utility created, a mini-dump does not have to con-\ntain the whole process space. Some sections are simply not required for most debug-\nging, like the ones that contain loaded modules. You just need to know the version\ninformation of these files so that you can provide them to the debugger later on. This\nmeans that the dump files created with this API are usually much smaller in size than\na full dump.\nThe information stored in a dump file can be controlled with the DumpType para-\nmeter. Whereas MiniDumpNormal includes only basic information like stack traces and\nthe thread listing, MiniDumpWithFullMemory writes all accessible process memory to\nthe file. To find out what dump type is the right one for you, I recommend reading\n[Starodumov05]. For further information about the MiniDumpWriteDump API and its\nparameters, refer to [MSDNDump07]. You should also consider distributing the \n98\nSection 1\nGeneral Programming \n\n\nlatest version of dbghelp.dll when releasing your game because older versions of this\nlibrary (like the one that comes with Windows 2000) do not have the right exports.\nAlso keep in mind that the API exposed by dbghelp.dll is not thread safe! This means\nit is the caller’s responsibility to synchronize calls to any of these functions.\nIn order to explicitly create a core dump on UNIX platforms, you have to use a\nthird-party tool. The Google coredumper project [Google05] is an open source\nlibrary that provides a simple API for creating core dumps of a running process. The\nWriteCoreDump function writes a dump of the current process into a specified file for\nyou. Alternatively, you can also use WriteCompressedCoreDump which writes the dump\nin either bzip2 or gzip compressed format. Finally, there is GetCoreDump, which creates\na copy-on-write snapshot of the process and returns a file handle from which the\ndump can be read. Currently the library supports x86, x64, and ARM systems and is\npretty easy to include in a project. \nHandling Stack Overflows\nSo far, this implementation can handle almost all cases that can cause an application\nto crash. The only condition where the error handling fails is in case of a stack over-\nflow. Before being able to handle this special error situation, you need to know the\nstatus of the thread after a stack overflow. When the application starts, the stack is ini-\ntially set to a small size (see Figure 1.10.1a). After the last page of the stack, a so-called\nguard page is placed. If the game consumes all of the reserved stack memory, the\nfollowing things happen (see Figure 1.10.1b):\n1. When accessing the stack while the stack pointer is pointing to the guard\npage, a STATUS_GUARD_PAGE_VIOLATION exception is raised.\n2. The guard protection is removed from the page, which now becomes part\nof the stack.\n3. A new guard page is allocated, one page below the last one.\n4. Execution is continued from the instruction that caused the exception.\nIf the stack reaches its maximum size, the allocation of a new guard page in step 3\nfails. In this case, a stack overflow exception is raised, which can be handled by the\nthread’s exception block. As you can see in Figure 1.10.1c, the stack now has just one free\npage left to work with. If you take up all of this space in the exception handler, you will\ncause an access violation and the application will be terminated by the operating system.\nThis means you do not have a large scope left to work with. For instance, if \nyou are calling MiniDumpWriteDump or even MessageBox, the game will cause an access\nviolation because the functions have too large stack overhead. However, you can over-\ncome this problem with a rather simple trick. Because you have enough space left to\ncall the CreateThread function, you can move all the exception handling into a new\nthread. Then you can work with a clean stack and do whatever kind of reporting you\nwant. In the exception handler, you wait until the worker thread has finished its work\nand returns.\n1.10\nAdvanced Debugging Techniques\n99\n\n\nUnfortunately, this approach does not work on UNIX platforms. When the sig-\nnal handler is called, the stack is so exhausted that you can’t properly report the error.\nThe only way to overcome this problem is by using the sigaltstack function to\ninstall an alternative stack for the signal handler. All signal handlers that are installed\nwith the SA_ONSTACK flag will be delivered on that stack, whereas all other handlers\nwill still be executed on the current stack.\nIn order to successfully create a core dump, you have to allocate at least 40KB of\nmemory for the alternative stack. In case you want to do even more reporting in the\nhandler, the size should be adjusted appropriately. When using sigaltstack, you\nshould check its documentation for your target platform because some implementa-\ntions can cause problems when used in multithreaded applications.\nMemory Leaks\nIf you are not using a global memory manager to satisfy the memory needs of your\ngame, you should certainly think about using a memory leak detector. There are a vast\namount of tools available that can help you find memory leaks. However, using them\nis not always straightforward and can often be a real pain. One of the techniques used\nmost often is to overload the new/delete operators. However, this approach has quite\na few flaws:\n• System headers must be included before the operator overloading while project\nheaders must be included afterward. Violating this rule can result in erroneous\nleak reports or even application crashes. \n• Leaks caused by allocations using malloc won’t be detected.\n• Conflicts with other libraries that overload these operators (MFC, for instance)\nare possible.\nOf course there are also various external leak detection tools available; however,\nthey are usually quite expensive. The leak detector presented in this article uses \nso-called allocation hooks for tracking the application’s memory requests. Allocation\n100\nSection 1\nGeneral Programming \nFIGURE 1.10.1\nInitial stack, stack growth, and stack overflow.\n\n\nhooks are provided by the C runtime library and allow you to override memory man-\nagement functions like malloc or free. The advantage of using hooks is that no mat-\nter where or how memory is allocated or freed, you will be notified by the runtime\nlibrary through the hook functions. The only thing you have to do is install the hooks\nbefore doing any allocations. \nInstalling Allocation Hooks\nThe Microsoft CRT allows you to install an allocation hook through its _CrtSetAlloc\nHook function. When or where do you install an allocation hook? The answer is as\nsoon as possible so that you are not missing any allocation requests. However, this is\nnot as easy as it sounds. Inside the main method can be too late if there are global\nobjects that dynamically allocate memory on construction. Even if you define the leak\ndetector as a global instance, there is no guarantee it will be initialized before any\nother global object. One way to overcome this problem is to use the Microsoft specific\n#pragma init_seg(lib) directive. By using this preprocessor command, you can tell\nthe compiler to initialize the objects defined in the current source file before any other\nobjects (this does not include CRT library initializations).\nThe GNU C library even allows you to completely replace the memory functions\nusing global hook variables defined in malloc.h. By assigning your own handlers to\n__malloc_hook, __realloc_hook, and __free_hook, you gain full control over all\nmemory-management requests. Initialization of the hooks can be easily achieved using\nthe __malloc_initialize_hook handler. This is a simple function without parameters\nor return value that is called after the library finishes installing its default allocation\nhooks. It is important to back up the default hooks for later use in your own hook\nfunctions. Otherwise, you would have to provide a complete implementation for the\noverloaded memory functions. \nImplementing Allocation Hooks\nThe leak detector manages an allocation registry that contains information about the\nallocated memory blocks. Each of these blocks contains a unique identifier, the\nrequested block size, and the call stack during the allocation request. The call stack\ndata will be used during the reporting for symbol resolving to get meaningful infor-\nmation. After a memory block is freed its block information is removed from the reg-\nistry. When the application ends, you have to enumerate the entries left in the registry\nand report them as memory leaks. The task for the allocation hooks is to update that\nregistry on each memory request with the required information.\nThe hook function passed to _CrtSetAllocHook must have the following signature:\nint AllocHook(int allocType, void* data, size_t size, int blockType,\nlong request, const unsigned char* filename, int line);\nThe most interesting parameters are allocType, blockType, and request. The\nallocType parameter specifies what operation was triggered (_HOOK_ALLOC, _HOOK_\n1.10\nAdvanced Debugging Techniques\n101\n\n\nREALLOC, or _HOOK_FREE). blockType specifies the memory block type that can be used to\nfilter memory allocations (in this implementation, I exclude all CRT allocations from\nleak detection). Finally, request specifies the order of the allocation, which you can use\nas a unique ID for bookkeeping. When the function has finished its work, it returns an\ninteger that specifies whether the allocation succeeds (returns TRUE) or fails (returns\nFALSE). For more information on the allocation hook function, refer to [MSDNHook07].\nThe signature of the GNU C library hooking functions is similar to the runtime\nfunctions they overload. Each function receives one additional parameter that con-\ntains the return address found on the stack when malloc, realloc, or free was called.\nThe hook functions all work in the following way:\n1. The original hook functions are restored.\n2. Call malloc, realloc or free.\n3. Update information for the memory leak detection.\n4. Back up the current hook functions.\n5. Install the custom hook functions.\nThis is the recommended workflow for allocation hooks, as described in the GNU\nC library documentation [GNUC06]. Instead of the request ID, use the address\nreturned by malloc or realloc as the unique memory block ID. \nWindows Error Reporting (WER)\nWith Windows Vista Microsoft introduced the Windows Feedback Platform that\nallows vendors and developers to access crash dumps sent through WER to Microsoft.\nThis is done through the Windows Quality Online Services (WinQual), an online\nportal offered by Microsoft. The service is free but requires a VeriSign Code Signing\nID to verify the identity of a company that is submitting software or accessing the\nWER database. WinQual organizes crash dumps into so-called buckets where each\nbucket contains crash reports caused by the same bug. The following parameters are\nused for bucket organization:\n• Application name—For example, game.exe\n• Application version—For example, 1.0.1234.0\n• Module name—For example, input.dll\n• Module version—For example, 1.0.123.1\n• Offset into module—For example, 00003cbb\nBy default, WER calls dwwin.exe, which collects the bucket data and creates a\nmini-dump of the crashed process. WinQual additionally offers the possibility to\nspecify feedback or request further information on a bucket. If users experience an\nalready known bug, they will be notified by WER. The users may be pointed to the\nvendor’s support site to download a hotfix, or can be informed of the current state of\nany bug fixes. Vendors can also request further information by executing WMI\nqueries, listing registry keys, or asking users to fill out a questionnaire.\n102\nSection 1\nGeneral Programming \n\n\nThe Client API\nOn Windows XP, the WER implementation is rather simple. Just two APIs (ReportFault\nand AddERExcludedApplication) are available to the developer. The ReportFault API\ninvokes WER and must be called inside the application’s exception handler. It will exe-\ncute the operating system utility dwwin.exe, which creates the crash report and prompts\nthe users to send it to Microsoft. AddERExcludedApplication can be used to exclude an\napplication from error reporting (this function requires write access to HKEY_LOCAL_\nMACHINE in the Windows Registry).\nWindows Vista offers a completely new API to work with WER. The library\nincludes several functions to create and submit error reports. Another difference from\nthe old WER API is that reports can be generated at any time during execution. The\nfollowing table gives a short overview of the most important functions offered by\nWER on Windows Vista. For a complete listing of available WER functions, refer to\n[MSDNWER07].\nFunction\nDescription\nWerAddExcludedApplication\nExcludes the specified application from error reporting.\nWerRegisterFile\nRegisters a file to be added to reports generated for the\ncurrent process.\nWerRemoveExcludedApplication\nReverts a previous call to WerAddExcludedApplication.\nWerReportAddDump\nAdds a dump to the report.\nWerReportAddFile\nAdds a file to the report.\nWerReportCloseHandle\nCloses the report.\nWerReportCreate\nCreates a new report.\nWerReportSetUIOption\nSets the user interface options.\nWerReportSubmit\nSubmits the report.\nWerUnregisterFile\nRemoves a file from the reports generated for the current\nprocess.\nAnother feature introduced with Windows Vista is the new Application Recovery\nand Restart API. It enables an application to register itself to get restarted or recovered\nafter a crash. Especially interesting is the RegisterApplicationRecoveryCallback\nfunction, which enables you to register a recovery callback. This function will be\ncalled by WER when an application becomes unresponsive or encounters an unhan-\ndled exception. This would be the ideal place to try saving game and player data,\nenabling the player to continue the game later. For detailed information about this\nnew Vista API, refer to [MSDNARR07].\nThe Framework\nThis section provides a short overview of the debugging framework provided on the\nCD. The CDebugHelp class contains helper functions for creating dumps and call\nstacks. Inside of DebugHelp.cpp, you will also find CdbgHelpDll, which is a wrapper\n1.10\nAdvanced Debugging Techniques\n103\n",
      "page_number": 125,
      "chapter_number": 14,
      "summary": "This chapter covers segment 14 (pages 125-136). Key topics include function, functions, and stack. Each stores a list of depen-\ndent objects.",
      "keywords": [
        "Dependency",
        "Dependency System",
        "job",
        "dependency count",
        "System",
        "Stack",
        "entry",
        "application",
        "hook functions",
        "hook",
        "Windows",
        "memory",
        "function",
        "functions",
        "Dependency Graph"
      ],
      "concepts": [
        "function",
        "functions",
        "stack",
        "memory",
        "reports",
        "dependency",
        "depends",
        "dependencies",
        "dependent",
        "exceptions"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 38,
          "title": "Segment 38 (pages 345-352)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 9,
          "title": "Segment 9 (pages 68-76)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 7,
          "title": "Segment 7 (pages 53-62)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 4,
          "title": "Segment 4 (pages 25-32)",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 137-144)",
      "start_page": 137,
      "end_page": 144,
      "detection_method": "topic_boundary",
      "content": "class for Microsoft’s Debugging Tools API. It will automatically try to load the newest\ndbghelp.dll instead of taking the default one in the system directory. Maybe you also\nnoticed the MiniDumpCallback function? MiniDumpWriteDump allows you to specify a\ncallback to control what information is added to the dump. In my implementation, I\nexclude the exception handler thread that you start when handling a stack overflow\nexception. The UNIX implementation of these functions is simple and doesn’t require\nany further explanation (documentation for backtrace and backtrace_symbols can\nbe found at [GNUC06]).\nException Handling\nThe exception handlers are part of the CDebugFx class declared in DebugFx.h. On\nWindows, you install an UnhandledExceptionFilter that creates a mini-dump and\ncalls a user-defined callback to do any more work when an exception occurs. In the\nUNIX implementation, the framework registers signal handlers for SIGSEGV and\nSIGABRT. I included the source of the Google coredumper into the library to elimi-\nnate the dependency on an external library. \nMemory Leak Detector\nThe memory leak detector is globally instanced and automatically active as soon as\nyou link the debugging framework to your application. Reporting is done via a\nreporting API so that the users can implement their own reporting. The default\nreporter will write the leak information into the debug output window when run on\na Windows platform and to the error output on UNIX. Another feature of the default\nreporter is the filtering of useless information from the call stack. Listing function\ncalls for operator new or the CRT allocation functions only bloats the output and\ndoesn’t help in finding bugs.\nSometimes there are problems with the symbol resolving, in particular when a\nmodule that is referenced in the call stack was already unloaded. In this case, the sym-\nbol resolving APIs cannot resolve the address to a symbolic name. On Windows, you\ncan reload the symbol table of a module using the SymLoadModule64 function. To use\nthis function, you just have to store the module base address (which is equal to the\nmodule handle) and the module name, along with the allocation information. Unfor-\ntunately, the GNU C library provides only a single function (backtrace_symbols) for\nsymbol resolving. The only solution on this platform is to store the resolved symbols\ninstead of the call stack for all allocations. Because this method would result in a huge\nmemory overhead and decreased performance, the leak detector currently doesn’t\nsupport this feature on UNIX platforms.\nConclusion\nWith the proper handling of application crashes, you can gain invaluable information\nthat can help you during debugging. Depending on the platform, you have different\n104\nSection 1\nGeneral Programming \n\n\nmethods to handle unexpected exceptions, like Windows Unhandled Exception Filters\nor UNIX signal handlers. Crash dumps are a great tool for post-mortem debugging of\nyour application because they provide a snapshot of the process when it crashed. By\nusing a threaded exception handler on Windows or an alternative signal handler stack\non UNIX, you can even handle stack overflows. You can also get rid of nasty memory\nleaks with a memory leak detector. Using allocation hooks, provided by the CRTs, you\ncan report all leaks with a complete stack trace when your game exits. Finally, you get a\ncomplete debugging framework providing unhandled exception handling with crash\ndump creation using the CDebugFx class. Memory leak detection is implemented in the\nCMemLeakDetector class and is automatically enabled when you link the library to your\ngame. With this set of tools, bugs won’t have any chance in your future projects.\nReferences\n[GNUC06] GNU C library documentation. “The GNU C library,” available online\nat http://www.gnu.org/software/libc/manual/, December 6, 2006.\n[Google05] Google coredumper library available online at http://code.google.com/\np/google-coredumper/, 2005.\n[Pietrek97] Pietrek, Matt. “A Crash Course on the Depths of Win32 Structured\nException Handling,” available online at http://www.microsoft.com/msj/0197/\nexception/exception.aspx, Microsoft Systems Journal, January, 1997.\n[Pietrek01] Pietrek, Matt. “New Vectored Exception Handling in Windows XP,”\navailable online at http://msdn.microsoft.com/msdnmag/issues/01/09/hood/\ndefault.aspx, MSDN Magazine, September, 2001.\n[MSDNDump07] MSDN Library. “MiniDumpWriteDump,” available online at\nhttp://msdn2.microsoft.com/en-us/library/ms680360.aspx, June 1, 2007.\n[MSDNHook07] \nMSDN \nLibrary. \n“Run-Time \nLibrary \nReference—\n_CrtSetAllocHook,” available online at http://msdn2.microsoft.com/en-us/\nlibrary/820k4tb8(VS.80).aspx, June 1, 2007.\n[MSDNWER07] MSDN Library. “WER Functions,” available online at\nhttp://msdn2.microsoft.com/en-us/library/bb513635.aspx, June 1, 2007.\n[MSDNARR07] MSDN Library. “Application Recover and Restart Reference,” avail-\nable online at http://msdn2.microsoft.com/en-us/library/aa373342.aspx, June 1,\n2007.\n[Starodumov05] Starodumov O. “Effective Mini-Dumps,” available online at\nhttp://www.debuginfo.com/articles/effminidumps.html, July 2, 2005.\n[Wikipedia] Wikipedia. “Signal (Computing),” available online at http://\nen.wikipedia.org/wiki/Signal_(computing)#List_of_signals.\n1.10\nAdvanced Debugging Techniques\n105\n\n\nThis page intentionally left blank \n\n\n107\nS E C T I O N\n2\nMATH AND PHYSICS\n\n\nThis page intentionally left blank \n\n\n109\nIntroduction\nGraham Rhodes, Applied Research\nAssociates, Inc.\ngrhodes@nc.rr.com\nM\nathematics makes the world go around! At least within the realm of electronic\ngame development, there is real truth in that statement. We use math, in one\nway or another, for just about everything. The heaviest uses of math by game develop-\ners these days lie in the rendering of game worlds, both two- and three-dimensional,\nartificial intelligence algorithms of all sorts, and physics-based simulation, which is\nevolving at a frantic pace. I’m awed by the current level of sophistication being\napplied in all of these areas! Model surfaces rendered in real-time 3D no longer resem-\nble shiny plastic, thanks to the advent of programmable graphics hardware and the\nability to apply advanced physically-based lighting and material models on a per-pixel\nbasis.\nCharacter AI sits on the cusp of Mori’s uncanny valley, and we are starting to see\nflash glimpses that one day it might be crossed. Crossing that valley visually, within\nthe realm of real-time game simulation, will rely not only on AI, but also on physically-\nbased animation techniques that enable game characters to interact with a dynamic\ngame world where collectible items are not necessarily located at scripted locations,\nand where the game world itself is subject to play-driven geometric change. Some of\nthese physically-based character animation technologies are already appearing in\nmiddleware products that are emerging into the industry. The dynamically-changing \ngame worlds themselves, with which sophisticated characters must interact, are also\nbecoming more physically based each year. Several current games, released or immi-\nnent, feature fully dynamic environments that exploit the many robust physics\nengines that are available today. Mankind’s and nature’s mathematical models enable\nall of these great entertainment technologies, of course.\nI would like to make a few brief comments about one important emerging area of\ngame development that is heavily math-driven. And that is the area of procedural\nmodeling, otherwise known as procedural generation, generative modeling, and prob-\nably a dozen other terms. One fact of commercial game development has been that\nthe cost of content development using traditional digital content-creation tools has\nincreased significantly as computer hardware and game engines have become able to\nsupport all of the features mentioned previously. The tools also have evolved, and\nartists can work more efficiently than ever before. But it is often the sheer volume of\ncontent that becomes a problem.\n\n\nSo large studios are now beginning to pay homage, in a way, to the game develop-\nment ways of old, and are mimicking developers from the demo scene who have been\ninventing content-generating algorithms for years. Procedural modeling, in all its\nmany forms, can greatly reduce the cost of content development, by effectively\nremoving the need for an art team to model and place every tree/bush/clump of grass\nthat is to appear in a scene. Procedural modeling of flora for game levels is currently\nquite heavily used for commercial game development. Tools exist, but are not yet\nubiquitous, for the procedural modeling of other game level elements, such as build-\nings and structures with interiors and exteriors modeled at multiple levels of detail,\nauto-populated with furniture and clutter, with everything looking good from a first-\nperson camera. There is even a highly anticipated game in development that claims to\napply procedural modeling to create cellular organisms, planets, stars, nebulae, and\nmany things in between.\nIt is clear to me that there is going to be a strong future for procedural modeling\nin games, although artists should not worry that they may be out of a job as a result!\n(That won’t be the case.) For this to work, of course, tools and game developers must\napply the proper mathematical or physically-based techniques.\nIn this section, you will find a variety of useful gems that provide insight into\nclassical techniques, as well as new techniques that you can apply to core problems. A\nduo of gems provides you with a deeper understanding of random number genera-\ntion, for application toward artificial intelligence, physics techniques, and procedural\ngeneration. Chris Lomont provides a comprehensive overview of random number\ngeneration techniques, their strengths, weakness, and occasionally dramatic failures,\nwhereas Steve Rabin’s gem on Gaussian randomness provides a highly efficient\nimplementation.\nTony Barrera, Anders Hast, and Ewert Bengtsson summarize an efficient tech-\nnique for evaluating trigonometric splines, which can be used to generate curves made\nfrom straight line segments and perfect elliptical arcs. Combining several of these\ntrigonometric splines can generate curves that are visually more elegant and contain\nfewer curvature artifacts than other piecewise spline techniques such as traditional\ncubic polynomial splines—just perfect for digital content-creation tools as well as in-\nengine procedural generation of model geometry. Krzysztof Kluczek continues the\npresentation of techniques that are quite useful for procedural model generation in his\nchapter, describing the use of a projective space to enable highly robust geometric\noperations while reducing storage requirements and computational expense com-\npared with other techniques for achieving similar results.\nThe last four gems focus on a variety of techniques for collision detection and\nother geometric queries, which continue to be areas of active research within the indus-\ntry and academia alike. Jacco Bikker provides a fantastic summary of the kD-tree\nspatial partitioning technique, with a strong practical focus on minimizing storage\nrequirements and on building trees that are optimized according to query type. He also\n110\nSection 2\nMath and Physics\n\n\ndescribes approaches for dealing with dynamic scenes. José Gilvan Rodrigues Maia,\nCreto Augusto Vidal, and Joaquim Bento Cavalcante-Neto describe transformation\nsemantics, giving a sort of geometric intuition to the interpretation of transformation\nmatrices. Their discussion shows how this intuition can be applied in practice to the\nvarious phases that are common to modern collision detection algorithms.\nRahul Sathe and Dillon Sharlet describe a new technique for collision detection\nthat can be applied from broad phase through narrow phase. Finally, Gary Snethen\ndescribes a collision detection technique inspired by the GJK algorithm that is elegant\nin its simplicity and intuitiveness, while being also quite flexible. Take these ideas,\nyoung men and women and go forth and develop!\nIntroduction\n111\n",
      "page_number": 137,
      "chapter_number": 15,
      "summary": "Unfor-\ntunately, the GNU C library provides only a single function (backtrace_symbols) for\nsymbol resolving Key topics include library, game, and exception. Covers exception.",
      "keywords": [
        "game",
        "library",
        "MSDN Library",
        "exception",
        "Exception Handling",
        "Techniques",
        "online",
        "Memory Leak Detector",
        "procedural modeling",
        "procedural",
        "game development",
        "UNIX",
        "Leak Detector",
        "Memory Leak",
        "Windows"
      ],
      "concepts": [
        "library",
        "game",
        "exception",
        "exceptions",
        "available",
        "function",
        "functions",
        "techniques",
        "model",
        "provides"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 53,
          "title": "Segment 53 (pages 1064-1083)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 8,
          "title": "Segment 8 (pages 142-159)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 9,
          "title": "Segment 9 (pages 76-97)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 10,
          "title": "Segment 10 (pages 79-89)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 37,
          "title": "Segment 37 (pages 741-758)",
          "relevance_score": 0.52,
          "method": "api"
        }
      ]
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 145-153)",
      "start_page": 145,
      "end_page": 153,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank \n\n\n113\n2.1\nRandom Number Generation\nChris Lomont\nwww.lomont.org\nT\nhis article is an introduction to random number generators (RNGs). The main\ngoal is to present a starting point for programmers needing to make decisions\nabout RNG choice and implementation. A second goal is to present better alterna-\ntives for the ubiquitous Mersenne Twister (MT). A final goal is to cover various classes\nof random number generators, providing strengths and weaknesses of each. \nBackground: Random Number Generation\nRandom number generators (RNGs) are essential to many computing applications.\nFor some problems, algorithms employing random choices perform better than any\nknown algorithm not using random choices. It is often easier to find an algorithm to\nsolve a given problem if randomness is allowed. (The class of problems efficiently\nsolvable on a [Turing] machine equipped with a random number generator is BPP,\nand it is an open problem if BPP=P, P being the class of problems efficiently solvable\non a computer without random choice.)\nMost random numbers used in computing are not considered truly random, but\nare created using pseudo-random number generators (PRNGs). PRNGs are deter-\nministic algorithms, and are the only type of random number that can be algorithmi-\ncally generated without an external source of entropy, such as thermal noise or user\nmovements. \nDesigning good RNGs is hard and best left to professionals. (Robert R. Coveyou\nof Oak Ridge National Laboratory humorously once titled an article, “The Genera-\ntion of Random Numbers Is Too Important to Be Left to Chance.” Like cryptogra-\nphy, the history of RNGs is littered with bad algorithms and the consequences of\nusing them. A few historical mistakes are covered near the end of this article.\nUses\nRandom numbers are used in many applications, including the following:\n• AI algorithms, such as genetic algorithms and automated opponents.\n• Random game content and level generation. \n• Simulation of complex phenomena such as weather and fire.\n\n\n114\nSection 2\nMath and Physics\n• Numerical methods such as Monte-Carlo integration.\n• Until recently, primality proving used randomized algorithms. \n• Cryptography algorithms such as RSA use random numbers for key generation. \n• Weather simulation and other statistical physics testing.\n• Optimization algorithms use random numbers significantly—simulated anneal-\ning, large space searching, and combinatorial searching.\nHardware RNGs\nBecause an algorithm cannot create “true” random numbers, many hardware-based\nRNGs have been devised. Quantum mechanical events cannot be predicted, and are\nconsidered a very good source of randomness. Such quantum phenomena include:\n• Nuclear decay detection, similar to a smoke detector.\n• Quantum mechanical noise source in electronic circuits called “shot noise.”\n• Photon streams through a partially silvered mirror.\n• Particle spins created from high energy x-rays.\nOther sources of physical randomness are as follows:\n• Atmospheric noise (see www.freewebs.com/pmutaf/iwrandom.html for a way to\nget random numbers from WiFi noise).\n• Thermal noise in electronics.\nOther physical phenomena are often used on computers, like clock drift, mouse\nand keyboard input, network traffic, add-on hardware devices, or images gathered\nfrom moving scenery. Each source must be analyzed to determine how much entropy\nthe source has, and then how many high-quality random bits can be extracted.\nHere are a few Websites offering random bits of noise and the method used to\nobtain them:\n• http://random.org/—Atmospheric noise.\n• http://www.fourmilab.ch/hotbits/—Radioactive decay of Cesium-137.\n• http://www.lavarnd.org/—Noise in CCD images.\nPseudo-Random Number Generators (PRNGs)\nPRNGs generate a sequence of “random” numbers using an algorithm, operating on\nan internal state. The initial state is called the seed, and selecting a good seed for a given\nalgorithm is often difficult. Often the internal state is also the returned value. Due to\nthe state being finite, the PRNG will repeat at some point, and the period of an RNG\nis how many numbers it can return before repeating. A PRNG using n bits for its state\nhas a period of at most 2n. Starting a PRNG with the same seed allows repeatable ran-\ndom sequences, which is very useful for debugging among other things. When a\nPRNG needs a “random” seed, often sources of entropy from the system or external\nhardware are used to seed the PRNG.\n\n\nDue to computational needs, memory requirements, security needs, and desired\nrandom number “quality,” there are many different RNG algorithms. No one algo-\nrithm is suitable for all cases, in the same way that no sorting algorithm is best in all\nsituations. Many people default to C/C++ rand() or the Mersenne Twister, both of\nwhich have their uses. Both are covered in this gem.\nCommon Distributions\nMost RNGs return an integer selected uniformly from the range [0,m] for some max-\nimum value m. C/C++ implementations provide the rand() function, with m being\n#defined as RAND_MAX, quite often the 15-bit value, 32767. srand(seed) sets the initial\nseed, often using the current time as an entropy source like srand(time(NULL)). Most\nC/C++ rand() functions are Linear Congruential Generators, which are poor choices\nfor cryptography. Most C/C++ implementations (as well as other languages) generate\npoor quality random numbers that exhibit various kinds of bias.\nThe most common distribution used in games is a uniform distribution, where\nequally likely random integers are needed in a range [a,b]. A common mistake is to use\nC code like (rand()%(b-a+1)) + a. The mistake is that not all values are equally likely\nto occur due to modulus wrapping around. This only works if b – a + 1 divides\nRAND_MAX+1. For example, if RAND_MAX is 32767, then trying to generate numbers in the\nrange [0,32766] using this method causes 0 to be twice as likely as any other single\nvalue. A valid (although slower) solution is to scale the rand output to [0,1] and back\nto [a,b], using:\ndouble v = (static_cast<double>( rand()) ) / RAND_MAX;\nreturn static_cast<long>(v*(b-a+1)+a);\nThe second most commonly used distribution is a Gaussian Distribution, which\ncan be generated from a uniform distribution. Let randf() return uniformly distrib-\nuted real numbers in [0,1]. Then the polar form of the Box-Muller transformation\ngives two Gaussian values, y1 and y2, per call.\nfloat x1, x2, w, y1, y2;\ndo {\nx1 = 2.0 * randf() - 1.0;\nx2 = 2.0 * randf() - 1.0;\nw  = x1 * x1 + x2 * x2;\n} while ( w >= 1.0 );\nw = sqrt( (-2.0 * log( w ) ) / w );\ny1 = x1 * w;\ny2 = x2 * w;\nBoost [Boost07] documents techniques for generating other distributions start-\ning with a uniform distribution.\n2.1\nRandom Number Generation\n115\n\n\nRandomness Testing\nTo test if a sequence is “random,” a definition of “random” is needed. However “random-\nness” is very difficult to make precise. In practice (because many PRNGs are useful) tests\nhave been designed to test the quality of RNGs by detecting sequence behavior that does\nnot behave like a random sequence should.\nThe most famous randomness-testing suite is DIEHARD [Marsaglia95], made of\n12 tests (for more information, see http://en.wikipedia.org/wiki/Diehard_tests).\nDIEHARD has been expanded into the open source (GPL) set of tests DieHarder\n[Brown06], which includes the DIEHARD tests as well as many new ones. Also\nincluded are many RNGs and a harness to add new ones easily. A third testing frame-\nwork is TestU01 [L’Ecuyer06]. Each framework provides some assurance a tested\nRNG is not clearly bad.\nSoftware Whitening\nMany sources of random bits have some bias or bit correlation, and methods to\nremove the bias and correlation are known as whitening algorithms. Some choices:\n• John von Neumann. Take bits two at a time, discard 00 and 11 cases, and output\n1 for 01 and 0 for 10, removing uniform bias, at the cost of needing more bits.\n• Flip every other bit, removing uniform bias.\n• XOR with another known good source of bits, as in Blum Blum Shub.\n• Apply cryptographic hashes like Whirlpool or RIPEMD-160. Note MD5 is no\nlonger considered secure.\nThese whitened streams should still not be considered a secure source of random\nbits without further processing.\nNon-Cryptographic RNG Methods\nNon-cryptographically secure methods are usually faster than cryptographic methods,\nbut should not be used when security is needed, hence the classification. Each of the\nfollowing methods is a PRNG with output sequence Xn. Some have a hidden internal\nstate Sn from which Xn is derived. Either X0 or S0 is the seed, as appropriate.\nMiddle Square Method\nThis was suggested by John von Neumann in 1946—take a 10-digit number as a\nseed, square it, and return the middle 10 digits as the next number and seed. It was\nused in ENIAC, is a poor method with statistical weaknesses, and is no longer used.\nLinear Congruential Generator (LCG)\nThese are the most common methods in widespread use, but are slowly being replaced\nby newer methods. They are computed with Xn+1 = (aXn + b)modm, for constants a\n116\nSection 2\nMath and Physics\n\n\nand b. The modulus m is often chosen as a power of 2, making it efficiently imple-\nmented as a bitmask. Careful choice of a and b is required to guarantee maximal period\nand avoid other problem cases. LCGs have various pathologies, one of which is that\nchoosing points in three-tuples and plotting them in space shows the points fall onto\nplanes, as exhibited later in the section on RANDU, and is a result of linear relations\nbetween successive points. LCGs with power-of-two modulus m = 2e are known to be\nbadly behaved, especially in their least significant bits [L’Ecuyer90]. For example\nNumerical Recipes in C [Press06] recommends a = 1664525, b = 1013904223, m =\n2^32, and the lowest order bit then merely alternates.\nLCGs’ strengths are they are relatively fast and use a small state, making them\nuseful in many places including embedded applications. If the modulus is not a power\nof two then the modulus operation is often expensive.\nRepresenting an LCG as LCG(m, a, b), Table 2.1.1 shows some LCGs in use.\nTable 2.1.1\nSome LCGs in Use\nLCG\nUse\nLCG(231, 65539, 0)\nThe infamous RANDU covered later in this gem.\nLCG(224, 16598013, 12820163)\nMicrosoft VisualBasic 6.0.\nLCG(248, 25214903917, 11)\ndrand48 from the UNIX standard library; was used in\njava.util.Random.\nLCG(1012 \u0003 11, 427419669081, 0) \nUsed in Maple 9.5 and in MuPAD 3. Replaced by MT19937\n(below) in Maple 10.\nTruncated Linear Congruential Generator (TLCG)\nThese store an internal state Si updated using an LCG, which in turn is used to generate\nthe output Xi. Symbolically, Sn+1 = (aSn + b)modm,\n. This allows\nusing the fast m as a power of two but avoids the poor low order bits in the LCGs. If\nK is a power of 2, the division is also fast. This algorithm is used extensively through-\nout Microsoft products (likely as a result of being compiled with VC++), including\nVC++ rand(), with the implementation\n/* MS algorithm for rand() */\nstatic unsigned long seed;\nseed = 214013L * seed + 2531011L;\nreturn (seed>>16)&0x7FFF; // return bits 16-30\nThis is not secure. In fact, for a cryptographic analysis project, this author has\ndetermined only three successive outputs from this algorithm are enough to deter-\nmine the internal state (up to an unneeded most significant bit), and thereby know all\nfuture output. A simple way to compute the state is to notice the top bit of the state\nX\nFloor\nS\nK\nn\nn\n+\n+\n=\n⎡\n⎣\n⎢\n⎢\n⎤\n⎦\n⎥⎥⎥⎥⎥⎥⎥⎥\n1\n1⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥\n2.1\nRandom Number Generation\n117\n\n\nhas no bearing on future output; so only 31 bits are unknown. The first output gives\n15 bits of the state, leaving 17 bits unknown. Now, given two more outputs, take the\nfirst known 15 bits and test each of the possible 217 unknown bit states to see which\ngives the other two known outputs. This probably determines the internal state. Two\noutputs are not enough because they do not uniquely determine the state.\nBorland C++ and TurboC also used TLCGs with a = 22695477 and b = 1.\nAlthough the C specification does not force a rand implementation, the example one\nin the C Programming Language [Kernighan91] is a TLCG with a = 113515245 and\nb = 12345, with a RAND_MAX of the minimum allowable 32767.\nLinear Feedback Shift Register (LFSR)\nA Linear Feedback Shift Register (LFSR, see Figure 2.1.1) generates bits from an\ninternal state by shifting them out, one at a time. New bits are shifted into the state,\nand are a linear function of bits already in the state. LFSRs are popular because they\nare fast, easy to do in hardware, and can generate a wide range of sequences. Tap\nsequences can be chosen to make an n bit LFSR have period 2n – 1. Given 2n bits of\noutput the structure and feedback connections can be deduced, so they are definitely\nnot secure.\n118\nSection 2\nMath and Physics\nFIGURE 2.1.1\nLinear Feedback Shift Register (LFSR).\nInversive Congruential Generator\nThese are similar to LCGs but are nonlinear, using Xn+1 = (aXn\n–1 + b)modm, where\nXn\n–1 is the multiplicative inverse modm, that is, XnXn\n–1 ≡1modm. These are expensive\nto compute due to the inverse operation, and are not often used.\nLagged Fibonacci Generator (LFG)\nUse k words of state Xn = (Xn–j\nXn–k)modm, O < j < k where \nis some binary oper-\nation (plus, times, xor, others). These are very hard to get to work well and hard to ini-\ntialize. The period depends on a starting seed and the space of reached values breaks\ninto hard to predict cycles. They are now disfavored due to the Mersenne Twister and\nlater generators. Boost [Boost07] includes variants of LFGs.\n⊗\n⊗\n\n\nCellular Automata\nMathematica prior to Version 6.0 uses the cellular automata Wolfram rule 30 to gener-\nate large integers (see http://mathworld.wolfram.com/Rule30.html). Version 6.0 uses a\nvariety of methods.\nLinear Recurrence Generators\nThese are a generalization of the LFSRs, and most fast modern PRNGs are derived\nfrom these over binary finite fields. Note that none of these pass linear recurrence test-\ning due to being linear functions. The next few are special examples of this type of\nPRNG, and are considered the best general-purpose RNGs.\nMersenne Twister\nIn 1997, Makoto Matsumoto and Takuji Nishimura published the Mersenne Twister\nalgorithm [Matsumoto98], which avoided many of the problems with earlier genera-\ntors. They presented two versions, MT11213 and MT19937, with periods of 211213-1\nand 219937-1 (approximately 106001), which represents far more computation than is\nlikely possible in the lifetime of the entire universe. MT19937 uses an internal state of\n624 longs, or 19968 bits, which is about expected for the huge period. It is (perhaps\nsurprisingly) faster than the LCGs, is equidistributed in up to 623 dimensions, and\nhas become the main RNG used in statistical simulations.\nThe speed comes from only updating a small part of the state for each random\nnumber generated, and moving through the state over multiple calls. Mersenne\nTwister is a Twisted Generalized Feedback Shift register (TGFSR). It is not crypto-\ngraphically secure: observing 624 sequential outputs allows you to determine the\ninternal state, and then predict the remaining sequence. Mersenne Twister has some\nflaws, covered in the “WELL Algorithm” section that follows.\nLFSR113, LFSR258\n[L’Ecuyer99] introduces combined LFSR Tausworthe generators LFSR113 and\nLFSR258 designed specially for 32-bit and 64-bit computers, respectively, with peri-\nods of approximately 2113 and 2258, respectively. They are fast, simple, and have a\nsmall memory footprint. For example, here is C/C++ code for LFSR113 that returns\na 32-bit value:\nunsigned long z1, z2, z3, z4; /* the state  */\n/* NOTE: the seed MUST satisfy \nz1 > 1, z2 > 7, z3 > 15, and z4 > 127 */\nunsigned long lfsr113(void) \n{ /* Generates random 32 bit numbers.    */ \nunsigned long b; \nb  = (((z1 << 6) ^ z1)   >> 13); \nz1 = (((z1 & 4294967294) << 18) ^ b); \nb  = (((z2 << 2) ^ z2)   >> 27); \nz2 = (((z2 & 4294967288) <<  2) ^ b); \nb  = (((z3 << 13) ^ z3)  >> 21); \nz3 = (((z3 & 4294967280) <<  7) ^ b); \n2.1\nRandom Number Generation\n119\n\n\nb  = (((z4 << 3) ^ z4)   >> 12); \nz4 = (((z4 & 4294967168) << 13) ^ b); \nreturn (z1 ^ z2 ^ z3 ^ z4); \n}\nBecause 2113 is approximately 1034, this already represents a huge number of val-\nues, and has a much smaller footprint than MT19937. The LFSR generators also are\nwell equidistributed, and avoid LCGs problems.\nWELL Algorithm\nMatsumoto (co-creator of the Mersenne Twister), L’Ecuyer (a major RNG researcher),\nand Panneton introduced another class of TGFSR PRNGs in 2006 [Panneton06].\nThese algorithms produce numbers with better equidistribution than MT19937 and\nimprove upon “bit-mixing” properties. WELL stands for Well Equidistributed Long-\nPeriod Linear, and they seem to be better choices for anywhere MT19937 is currently\nused. They are fast, come in many sizes, and most importantly produce higher quality\nrandom numbers.\nWELL period sizes are presented for period 2n for n = 512, 521, 607, 800, 1024,\n19937, 21701, 23209, and 44497, with corresponding state sizes. This allows users to\ntrade period length for state size. All run at similar speed. 2512 is about 10154, and it is\nunlikely any video game will ever need that many random numbers, because it is far\nlarger than the number of particles in the universe. The larger period ones aren’t really\nneeded except for computations like weather modeling or earth simulations. A stan-\ndard PC needs over a googol of years to count to 2512. (A googol is 10100. Google it.)\nA significant place the WELL PRNGs perform better than MT19937 is in escap-\ning states with a large number of zeros. If MT19937 is seeded with many zeros, or\nsomehow falls into such a state, the generated numbers have heavy bias toward zeros\nfor many iterations. The WELL algorithms behave much better, escaping zero bias\nstates quickly.\nThe only downside is that they are slightly slower than MT19937, but not much.\nThe upside is the numbers are considered to be higher quality, and the code is signif-\nicantly simpler. Here is WELL512 C/C++ code written by the author and placed in\nthe public domain (if you use it, I’d appreciate a reference or at least an email with\nthanks). It is about 40% faster than the code presented on L’Ecuyer’s site, and is about\n40% faster than MT19937 presented on Matsumoto’s site. \n/* initialize state to random bits  */\nstatic unsigned long state[16];\n/* init should also reset this to 0 */\nstatic unsigned int index = 0;\n/* return 32 bit random number      */\nunsigned long WELLRNG512(void)\n{\nunsigned long a, b, c, d;\na  = state[index];\nc  = state[(index+13)&15];\nb  = a^c^(a<<16)^(c<<15);\n120\nSection 2\nMath and Physics\n",
      "page_number": 145,
      "chapter_number": 16,
      "summary": "This chapter covers segment 16 (pages 145-153). Key topics include random, randomized, and generation. The main\ngoal is to present a starting point for programmers needing to make decisions\nabout RNG choice and implementation.",
      "keywords": [
        "Random Number Generation",
        "Random Number",
        "random number generators",
        "Random",
        "Number",
        "state",
        "Number Generation",
        "number generators",
        "random bits",
        "bits",
        "bit random number",
        "quality random numbers",
        "Mersenne Twister",
        "internal state",
        "rand"
      ],
      "concepts": [
        "random",
        "randomized",
        "generation",
        "generators",
        "generated",
        "generate",
        "algorithms",
        "state",
        "bits",
        "bit"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 11,
          "title": "Segment 11 (pages 99-106)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 15,
          "title": "Segment 15 (pages 132-139)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 14,
          "title": "Segment 14 (pages 124-131)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 37,
          "title": "Segment 37 (pages 343-350)",
          "relevance_score": 0.51,
          "method": "api"
        }
      ]
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 154-161)",
      "start_page": 154,
      "end_page": 161,
      "detection_method": "topic_boundary",
      "content": "c  = state[(index+9)&15];\nc ^= (c>>11);\na  = state[index] = b^c; \nd  = a^((a<<5)&0xDA442D20UL);\nindex = (index + 15)&15;\na  = state[index];\nstate[index] = a^b^d^(a<<2)^(b<<18)^(c<<28);\nreturn state[index];\n}\nCryptographic RNG Methods\nCryptographically Secure PRNGs (CSPRNGs) make it hard for an attacker to deduce\nthe internal state of the generator or to predict future output given large amounts of\noutput. Several CSPRNGs have been standardized and can be found online (check\nout http://en.wikipedia.org/wiki/CSPRNG). Two RFCs dealing with randomness\nrequirements for security are RFC1750 and RFC4086 (see www.ietf.org/rfc). Any\nimplementation of these methods has to be done very carefully to avoid many pitfalls.\nWhenever possible, use an implementation from a trusted and competent source. \nBlum Blum Shub\nPublished in 1986 by Lenore Blum, Manuel Blum, and Michael Shub, Blum Blum\nShub [Blum86] is considered a secure PRNG. It is computed via Sn+1 = (Sn\n2)modm\nwhere m = pq for two properly chosen large primes p,q. Then the output Xn+1 is some\nfunction on Sn+1, which often is taken as bit parity or some particular bits of Sn+1. Its\nstrength relies on the hardness of integer factoring, which is the same problem RSA\npublic key encryption relies on for security. Blum Blum Shub is only useful for cryp-\ntography, because it is much slower than the non-cryptographic PRNGs. (Note Shor’s\nquantum factoring algorithm factors integers efficiently, so once quantum computers\nare in use Blum Blum Shub will become insecure.)\nISAAC, ISAAC+\n[Jenkins96] introduced ISAAC, a CSPRNG based on a variant of the RC4 cipher. It\nis relatively fast for a CSPRNG, requiring an amortized 18.75 instructions to produce\na 32-bit value. There are no cycles in ISAAC shorter than 240 values, and the expected\ncycle length is 28295 values. ISAAC-64, a version for 64-bit machines, requires 19\ninstructions to produce a 64-bit result.\n/dev/random\nAlthough not a specific algorithm, Linux and many UNIX flavors implement a source\nof randomness in /dev/random, which returns random numbers based on system\nentropy, so it is considered a true random number generator. /dev/random blocks,\nthat is, does not return until enough entropy has been gathered to satisfy the request.\n2.1\nRandom Number Generation\n121\n\n\nAs a result, many programs use the non-blocking /dev/urandom. However, these\nnumbers are not as secure, and use of /dev/urandom depletes system entropy, allow-\ning some attacks on bad implementations. The underlying algorithm is not specified;\nsome systems use Yarrow as mentioned in a following section.\n[Gutterman06] revealed exploitable weaknesses in the Linux implementation at\nthe time, which should have been fixed by now. Overall /dev/random is the preferred\nplace on Linux to get CSPRNGs.\nMicrosoft’s CryptGenRandom \nMicrosoft’s CryptoAPI function CryptGenRandom function fills a buffer with cryp-\ntographically secure random bytes. Like /dev/random, it is considered a true random\nnumber generator. Although closed source, it is FIPS validated, and is considered\nsecure. This author is unaware of any weaknesses with recent implementations. On\nWindows, it is the preferred source of CSPRNGs.\nYarrow\n[Kelsey99] introduces Yarrow, which uses system entropy to generate random numbers.\nIt is explicitly unpatented and royalty-free, and no license is required to use it. Yarrow is\nused in Mac OS X and FreeBSD to implement /dev/random. Yarrow is no longer sup-\nported by the designers, who have released an improved design titled Fortuna.\nFortuna\nFortuna is another CSPRNG from the book Practical Cryptography [Ferguson03].\nThe generator is based on any good block cipher, and encrypts in counter mode,\nencrypting successive values of a counter. The key is changed periodically to prevent\nsome statistical weaknesses. It uses entropy pools that gather information from ran-\ndom sources available to the system, and is considered a true RNG because it uses\nexternal entropy.\nCommon Mistakes in Creating Random Number Generators\nCreation of good RNGs is not trivial and the history of RNGs is scattered with exam-\nples of bad design. You can always learn something from the failures of others, so let’s\ntake a look at some common mistakes in this area.\nKnuth Example\nEven algorithm master Donald Knuth tells a story in [Knuth98] about trying his\nhand at making a random number generator by creating a “super-random” generator.\nHis first run settled onto a 10-digit number that then repeated forever. His second\nrun began to repeat itself after 7401 values with a cycle of 3178.\n122\nSection 2\nMath and Physics\n\n\nHere are a few more examples that hopefully will prevent people from using\nhomemade RNGs in critical applications. \nRANDU\nRANDU is an infamous LCG used since the 1960s; it is LGC(231,65539,0), and requires\nan odd initial seed. The constants were chosen for easy and fast implementation. As all\nLCGs, it suffers from linear relations between successive numbers. Figure 2.1.2 shows\nthe output of 10,000 triplets (x,y,z) plotted in 3D, which happen to fall into planes.\n2.1\nRandom Number Generation\n123\nFIGURE 2.1.2\nLCG bias.\nNetscape\nAn early version of Netscape needed a CSPRNG, but seeded it with three values that\nweren’t very well spread out (time of day, process ID, and parent process ID) and used\nthe result for cryptography. [Goldberg96] published a successful attack on Netscape’s\nSSL protocol, with the exploitable flaw being a poor choice of seed. \nFolklore Algorithms\nThe author encountered a folklore algorithm from a game programmer around 1992,\nwho explained that he had a fast and simple PRNG for his NES code. The basic idea\nwas to shift bits out of a seed, and whenever the seed had 1 bit about to shift off,\n\n\nexclusive-or in a constant. This was fast and looked nice in assembly, but being a skep-\ntic this author thought about the claim of randomness, and said this should produce\nnumbers that tend to decrease, and every so often jump back up, making saw tooth\noutputs. This led the original programmer to discover a bug in his AI that was using\nthe PRNG which he hadn’t suspected (he had used the PRNG for years). Although\nanecdotal, it is wise to test a new RNG and see that it behaves as expected before com-\nmitting it to your toolbox.\nOne last particularly funny example is the xkcd Webcomic version of a random\nnumber generator at http://xkcd.com/c221.html, reproduced for your viewing pleasure:\nint getRandomNumber()\n{\nreturn 4; // chosen by fair dice roll.\n// guaranteed to be random.\n}\nCode\nThere are many online places to obtain source code for the algorithms covered in this\narticle. Boost [Boost07] contains high-quality implementations for many of them,\nand Wikipedia contains more information and links to most of the presented topics.\nL’Ecuyer’s Web page (www.iro.umontreal.ca/~lecuyer/papers.html) is a good source of\npapers and many implementations. In addition, Technical Review 1 (TR1) for the\nC++ language includes many distributions and generators (including MT19337), so\nit is likely C++ will someday have some of these features built-in.\nConclusion\nThis gem has provided basics of RNGs, including many common algorithms. LFSR113,\nLFSR258, and the WELL generators offer better choices than the Mersenne Twister for\nmany applications, and this presentation brings knowledge of them to a wider audience.\nStrengths and weaknesses were presented for algorithms where possible. Knowledge\nabout RNG types and when to apply them should be in the toolkit of any serious devel-\noper, just as any serious developer should know multiple sorting algorithms, or numerous\ntree structures. Hopefully, this gem provides you with a base and reference for such\nknowledge.\nReferences\n[Blum86] Blum, Lenore, Blum, Manuel, and Shub, Michael. “A Simple Unpre-\ndictable Pseudo-Random Number Generator,” SIAM Journal on Computing, Vol.\n15, pp. 364–383, May 1986.\n[Boost07] “The Boost C++ Library,” 2007, www.boost.org.\n124\nSection 2\nMath and Physics\n\n\n[Brown06] Brown, Robert G., and Eddelbuettel, Dirk. “DieHarder: A Random\nNumber Test Suite Version 2.24.4,” available online at www.phy.duke.edu/~rgb/\nGeneral/rand_rate.php.\n[Ferguson03] Ferguson, Niels, and Schneier, Bruce. Practical Cryptography, Wiley,\n2003. ISBN 0-471-22357-3.\n[Goldberg96] Goldberg, Ian, and Wagner, David. “Randomness and the Netscape\nBrowser,” Dr. Dobb’s Journal, January 1996, pp. 66–70. Available online at\nwww.cs.berkeley.edu/~daw/papers/ddj-netscape.html.\n[Gutterman06] Gutterman, Pinkas, and Reinman. “Open to Attack: Vulnerabilities\nof the Linux Random Number Generator,” March 2006, Black Hat 2006,\nwww.pinkas.net/PAPERS/gpr06.pdf. \n[Jenkins96] Jenkins, Bob. “ISAAC and RC4,” www.burtleburtle.net/bob/rand/\nisaac.html as of 2007. \n[Kelsey99] Kelsey, J., Schneier, B., and Ferguson, N. “Yarrow-160: Notes on the\nDesign and Analysis of the Yarrow Cryptographic Pseudorandom Number Gen-\nerator,” Sixth Annual Workshop on Selected Areas in Cryptography, Springer\nVerlag, August 1999, www.schneier.com/paper-yarrow.html.\n[Kernighan91] Kernighan, B., and Ritchie, Dennis. The C Programming Language,\nSecond Edition, Prentice-Hall, 1991.\n[Knuth98] Knuth, Donald. The Art of Computer Programming, Volume 2: Seminumer-\nical Algorithms, Third Edition, Addison-Wesley, 1998.\n[L’Ecuyer90] L’Ecuyer, P. “Random Numbers for Simulation,” Communications of\nthe ACM, 33 (1990), pp. 85–98, www.iro.umontreal.ca/~lecuyer/papers.html.\n[L’Ecuyer99] L’Ecuyer, P. “Tables of Maximally-Equidistributed Combined LFSR\nGenerators,” Mathematics of Computation, 68, 225 (1999), pp. 261–269,\nwww.iro.umontreal.ca/~lecuyer/papers.html.\n[L’Ecuyer06] L’Ecuyer, P. and Simard, R. “TestU01: A C Library for Empirical Testing\nof Random Number Generators,” May 2006, Revised November 2006, ACM\nTransactions on Mathematical Software, 33, 4, Article 1, December 2007, to\nappear. www.iro.umontreal.ca/~lecuyer/papers.html.\n[Marsaglia95] Marsaglia, George. “DIEHARD,” http://www.csis.hku.hk/~diehard/.\n[Matsumoto98] Matsumoto, M., and Nishimura, T. “Mersenne Twister: A 623-\nDimensionally Equidistributed Uniform Pseudorandom Number Generator,”\nACM Trans. Model. Comput. Simul. 8, 3 (1998), www.math.sci.hiroshima-\nu.ac.jp/~m-mat/MT/emt.html.\n[Panneton06] Panneton, F., L’Ecuyer, P., and Matsumoto, M. “Improved Long-\nPeriod Generators Based on Linear Recurrences Modulo 2,” ACM Transactions on\nMathematical Software, 32, 1 (2006), pp. 1–16, www.iro.umontreal.ca/~lecuyer/\npapers.html.\n[Press06] Press, William H. (Editor), Teukolsky, Saul A., Vetterling, William T., and\nFlannery, Brian P. Numerical Recipes in C++: The Art of Scientific Computing,\nCambridge University Press, 2nd edition, 2002.\n2.1\nRandom Number Generation\n125\n\n\nThis page intentionally left blank \n\n\n127\n2.2\nFast Generic Ray Queries \nfor Games\nJacco Bikker, IGAD/NHTV University of\nApplied Sciences—Breda, The Netherlands\nbikker.j@nhtv.nl\nR\necently, real-time ray tracing on consumer PCs has become possible. A real-time\nray tracer traces millions of rays per second, per core, using instruction-level par-\nallelism (in particular, SIMD code [Wald04]). Thread-level parallelism allows you to\nscale this number almost linearly with the number of cores.\nRay tracing is useful for more than just rendering. This gem describes implemen-\ntation details for a generic ray tracer that can be used for various parts of a game, such\nas line-of-sight queries, physics, and sound propagation. The focus is on efficient tra-\nversal of individual rays.\nThe kD-tree, built using the surface area heuristic (SAH) [McDonald90], is an\nefficient spatial subdivision for ray queries in a static scene. This gem describes some\napproaches to extend this algorithm to support dynamic scenery.\nIntroduction to Ray Tracing\nRay tracing is a simple algorithm. When applied to rendering, the core algorithm\nlooks like this:\nfor each screen pixel\nconstruct a ray from the camera to the pixel\nintersect the ray with each primitive in the scene\nshade the pixel based on the intersection results\nHere, a ray is an infinite line with an origin, O, and a direction, D, as shown in\nFigure 2.2.1.\nTracing a single ray is referred to as ray casting. The basic algorithm was invented\nby Appel in 1963 [Appel63]. In 1979, Whitted extended this process by adding\nrecursion [Whitted79]. At the intersection point, a new ray to each light source is cre-\nated, to see whether the light affects the intersection point. If the material at the inter-\nsection point is reflective or refractive, this will also recursively spawn new rays.\n\n\nIf you look at the process of tracing a single ray alone, you see a visibility query.\nThe camera needs to know which primitive is visible through a screen pixel; the inter-\nsection point wants to know which lights are visible from that point. Queries like this\nare very useful in a game:\n• An enemy AI wants to know if the player is visible or audible.\n• The user highlights an object in the scene to select it.\n• A sniper fires a bullet at the player. The bullet ricochets around the scenery if it\nmisses the player, or if it cannot hit the player directly. Or perhaps it simply con-\ntinues its path after it pierced through the player.\n• A hovering vehicle tries to avoid obstacles and probes the surroundings to do so.\n• A destroyed opponent emits a collectable item that should drop until it hits the\nfloor.\nSome of these obviously require a ray query (player visibility, bullets, and ray\npicking), and some are often left to the physics engine’s internal collision detection\nsystem (when dropping items), even though they could be easily implemented using\none or more rays. Most 3D engines support ray queries in one form or another, but\noften for performance reasons game developers choose not to use them; they are\nassumed to be too slow. If rays are traced using a naive search of the scene geometry,\nthis is a good assumption. However, we can do (much) better. Using an optimized\nspatial subdivision, a ray query can be reduced to a few tree traversal steps and a lim-\nited number of ray/triangle intersections.\nThe remainder of this gem describes the kD-tree, considered to be the most effi-\ncient acceleration structure for ray tracing [Havran01], and approaches to a highly\nefficient implementation. Because this structure takes some preprocessing time, it is\nbest used for static scenery. Because of the importance in games and other real-time\ninteractive applications, this gem presents alternatives for dynamic scenery as well.\nYou can combine these for mixed environments. This is demonstrated in the accom-\npanying demo application on the CD-ROM.\n128\nSection 2\nMath and Physics\nFIGURE 2.2.1\nA ray consists of an origin and a direction.\n",
      "page_number": 154,
      "chapter_number": 17,
      "summary": "This chapter covers segment 17 (pages 154-161). Key topics include algorithm, rays. Any\nimplementation of these methods has to be done very carefully to avoid many pitfalls.",
      "keywords": [
        "random number generator",
        "random number",
        "index",
        "number generator",
        "random",
        "number",
        "Ray",
        "Blum",
        "Blum Shub",
        "Linux Random Number",
        "true random number",
        "random numbers based",
        "generator",
        "Number Generation",
        "ray tracing"
      ],
      "concepts": [
        "algorithm",
        "ray",
        "rays",
        "generator",
        "generation",
        "generate",
        "number",
        "source",
        "implementation",
        "implement"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 15,
          "title": "Segment 15 (pages 132-139)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 14,
          "title": "Segment 14 (pages 124-131)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 16,
          "title": "Segment 16 (pages 156-165)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 15,
          "title": "Segment 15 (pages 129-137)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 24,
          "title": "Segment 24 (pages 231-239)",
          "relevance_score": 0.47,
          "method": "api"
        }
      ]
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 162-170)",
      "start_page": 162,
      "end_page": 170,
      "detection_method": "topic_boundary",
      "content": "kD-Tree Concepts and Storage Considerations\nThe kD-tree (k-dimensional tree) is a structure that recursively splits space in two\nhalves. In this sense, it is a BSP tree. There is, however, a restriction—the splitting planes\nare axis-aligned, instead of the arbitrary planes that a generic BSP tree uses. An example\nis shown in Figure 2.2.2.\n2.2\nFast Generic Ray Queries for Games\n129\nFIGURE 2.2.2\nThree iterations of the kD-tree\nsplitting process.\n\n\nNote that in the third iteration (c), the bottom-left cell is not split, because it does\nnot contain any geometry. Also note how geometry is quickly isolated from empty\nspace, because of the arbitrary split plane position.\nMaking the split planes axis-aligned may seem limiting. In practice, however, it\nallows you to traverse a ray more efficiently. Finding the intersection of a ray with an\naxis-aligned plane is computationally very efficient, as shown in the following equation:\nt = (splitpos[axis] – ray.origin[axis]) / ray.direction[axis]   (1)\nHere, t is the distance along the ray, starting at the origin. By pre-calculating the\nreciprocal of the ray direction, this is reduced to a subtraction and a multiplication.\nA kD-tree node thus contains a split plane position and an orientation. Because\nthis can be the x, y, or z axis, two bits suffice to store the orientation. Besides the split\nplane, a node stores a list of primitives or as a pointer to a left and right child node.\nFinally, a flag is stored to identify a node as either an internal node (which has no\ngeometry but is split into two child nodes) or a leaf node (which has geometry but no\nchild nodes). \nstruct KdTreeNode\n{\nfloat splitpos;\nint axis;\nKdTreeNode* left, *right;\nbool leaf;\nPrimitive** primitive;\nint primcount;\n};\nUsing a careful layout of the data, this can be stored in just eight bytes:\n• By allocating child nodes at each split in pairs, the KdTreeNode that the right\npointer points to is simply left + 1. This way, only a single child node pointer\nneeds to be stored.\n• By storing pointers to primitives in a separate array, you can index this array from\nthe kD-tree node using an index and a count (see Figure 2.2.3). In order to be\nable to store both the index and the count in a single unsigned integer, you must\nuse five bits for the count and the remaining 27 bits to index the array.\n• If a node is a leaf, it doesn’t require the child node pointer. If it is an internal\nnode, it doesn’t reference primitives. These can thus safely occupy the same vari-\nable in the KdTreeNode data structure.\n• An optimized KdTreeNode requires eight bytes of storage. By aligning KdTreeNodes\nto 8-byte boundaries, the address of a KdTreeNode (and thus the value of the left\npointer) is guaranteed to be a multiple of 8. The lowest bits are thus zero; these\ncan be used for the leaf flag (1 bit) and the split plane axis (2 bits). Because this\ndata is shared by the object list index and count, this data must also be shifted by\nthree bits.\n130\nSection 2\nMath and Physics\n\n\nThe KdTreeNode structure now becomes:\nstruct KdTreeNode\n{\n// member data access methods\nvoid SetAxis( int a_Axis ) { m_Data = (m_Data & -4) + a_Axis; }\nint GetAxis(){ return m_Data & 3; }\nvoid SetLeft( KdTreeNode* a_Left )\n{ m_Data = (unsigned long)a_Left + (m_Data & 7); }\nKdTreeNode* GetLeft(){ return (KdTreeNode*)(m_Data & -8); }\nKdTreeNode* GetRight(){ return GetLeft() + 1; }\nint IsLeaf() { return (m_Data & 4); }\nvoid SetLeaf( bool a_Leaf )\n{m_Data = (a_Leaf)?(m_Data|4):(m_Data & -5);}\nint GetObjOffset() { return (m_Data >> 8); }\nint GetObjCount() { return (m_Data & 248) >> 3; }\n// member data\nfloat m_Split;\nunsigned long m_Data;\n};\nIn this structure, the m_Data member contains the leaf bit, the split plane axis,\nand either a pointer to the left child node or a pointer to an entry in the primitive\n2.2\nFast Generic Ray Queries for Games\n131\nFIGURE 2.2.3\nA kD-tree showing the use of an intermediate data structure to reduce the\nsize of the nodes.\n\n\npointer array, plus a primitive count. The various Get and Set methods filter out the\nrelevant bits for the requested information.\nkD-Tree Construction\nThe previous section described the basic concept of a kD-tree, along with its efficient\nstorage. The biggest unanswered question is how to determine the split plane position.\nExactly what makes a good kD-tree depends on what you want to use it for. If the\ngoal is to walk a list of primitives front-to-back in as few steps as possible, you will\nprobably want a balanced tree with few primitives spanning split planes. For ray trac-\ning, you have a different objective—reduce the number of triangles that you need to\nintersect. And that means that you would rather traverse empty space. Consider a\nscene with a single tiny triangle in the top-left corner, as shown in Figure 2.2.4.\n132\nSection 2\nMath and Physics\nFIGURE 2.2.4\nFinding the optimal split plane position.\nWhat is the optimal tree for this situation? There are a few options:\n• No split at all.\n• A single vertical split through the right vertex of the triangle.\n• A single horizontal split through the bottom vertex of the triangle.\n• Multiple splits.\nNote that splits will always occur at bounding box extrema; all other positions will\neither split the triangle or will add empty space to a node. No split at all means that\nevery ray will be tested against the triangle. A single vertical split will prevent this for\nsome percentage of the rays, and so does a horizontal split. Several splits will reduce this\npercentage even further. It turns out that, for the illustrated case, the vertical split is the\n\n\nbest option. In 3D, the chance that you hit any node with the triangle is proportional to\nthe area of the bounding surface of the node. The vertical split produces a non-empty\nleaf node with a smaller area than the horizontal split, so it reduces the chance that a ray\nneeds to be intersected with the triangle more than the horizontal split.\nWhether or not a split is needed at all depends on the cost of intersecting a triangle,\nversus the cost of traversing one level of the kD-tree. Usually, intersecting a triangle is\nmore expensive and so it pays to do a split, and perhaps more than a single split. This\nis the idea of the surface area heuristic (SAH)—determining the best split plane posi-\ntion, calculating an expected cost for each candidate position, and choosing the split\nwith the lowest expected cost. You should perform a split only if that cost is lower than\nthe cost of not splitting at all.\nThe cost of a given split can be computed using the following equation:\ncost_nosplit = primitive_count * total_area * intersection_cost;  (2)\nThe cost of a split can be expressed as the following:\nleft_cost = left_count * intersection_cost;\nright_cost = right_count * intersection_cost;  (3)\nsplit_cost = traversal_cost + left_area * left_cost\n+ right_area * right_cost;\nYou now have a heuristic to find the best plane, and a termination criterion: If you\ncannot find a cost lower than the cost of not splitting at all, you do not split. Addition-\nally, it is a good idea to stop splitting at a certain depth, to limit memory usage and\nconstruction time.\nEven though the SAH is elegant and produces high-quality trees, it needs some\nintervention to produce trees that are more suitable for fast ray tracing. By itself, the\nSAH will not try to isolate empty space. To encourage empty space cutoff, scores for\nsplits that produce empty leafs are lowered.\nFast kD-Tree Construction\nConstructing the kD-tree using the surface area heuristic is a potentially time-\nconsuming process. Because you need the number of triangles to the left and right of\neach possible split plane, you must walk the list of triangles 2N times per split, per\naxis, which amounts to 6N2 iterations of the inner loop. Because you are building a\ntree, the expected runtime of the build process is thus O(N2logN). For any realistic\nscene, this will take too long. This can be improved however, as shown by Wald and\nHavran [Wald06a]. As pointed out, the main bottleneck in the SAH algorithm is the\ntriangle counting. Luckily, you can get rid of this expensive process altogether.\nIn Figure 2.2.5, a simple scene of two triangles is shown. There are four possible\nsplit plane positions along the horizontal axis. At the first position, the number of\ntriangles to the left is zero, and the number of triangles to the right is two. Now,\nwhenever a new triangle begins (left side of bounding box), the left count increases.\n2.2\nFast Generic Ray Queries for Games\n133\n\n\nWhenever a triangle ends, the right count decreases. So, if you create a sorted list of\nthese events (start events and end events), the left and right triangle counts can be\nupdated incrementally, while walking the list of events from left to right. \nAlthough this already improves performance considerably, there are some remain-\ning problems. First of all, the events need to be sorted per axis. Secondly, at each level\nof the tree, the events need to be resorted, because many triangles will no longer be in\nthe list, whereas others were clipped, introducing new events.\n134\nSection 2\nMath and Physics\nFIGURE 2.2.5\nBounding box start and end events.\nIt turns out that it is possible to do the sorting just once, at the top level of the\ntree. For this, you use a rather complex data structure. The EventBox (see the code\nthat follows), is a linked list with no less than six next pointers, one for each side of a\nprimitive bounding box, for each axis in 3D space. \nFor the scene shown in Figure 2.2.5, two EventBoxes are needed, storing four\nevents per axis. Consider the EventBox for the left triangle: It has two next pointers\nper axis—one of them points to the start event of the second EventBox, the other one\npoints to the end event of the second EventBox.\nstruct EventBox \n{\nEventBoxSide side[2];\nPrimitive* prim; \n};\n\n\nThe first EventBoxSide contains the start events for the x, y, and z axes. The sec-\nond element contains the end events. Using two instances of the EventBoxSide object\nallows you to point from one side to another. Each EventBoxSide instance stores a\nposition along each axis, three next pointers, and the side. The pointers and the side\nvalue are stored in a single 32-bit value for efficiency.\nstruct EventBoxSide\n{\nEventBoxSide* next( int axis ) { return (EventBoxSide*)\n(n[axis] & -3); }\nvoid next( int axis, EventBoxSide* p ) \n{\nn[axis] = (n[axis] & 3) + (unsigned long)p; \n}\nint side( int axis ) { return n[axis] & 3; }\nvoid side( int axis, int side ) { n[axis] = (n[axis] & -3) \n+ side; }\nunsigned long n[3];\nfloat pos[3];\n};\nOnce the list is constructed, it can be updated on-the-fly. This way, sorting is lim-\nited to a single sort at the top level of the tree; tree construction time is now reduced\nto O(NlogN).\nkD-Tree Traversal\nOrdered traversal (front-to-back, for ray tracing) of a kD-tree is identical to BSP tree\ntraversal. Traversal starts by finding the leaf node that contains the ray origin. For each\nnode, the side of the split plane that the origin is on is determined, and the branch for\nthat side is followed, while the other side is pushed on a stack. Once a leaf is found, a\nnode is popped from the stack, and the process is continued until the stack is empty.\nFor ray queries, there is an extra termination criterion—once an intersection is found,\nthere is no need to look beyond the current node.\nThis process visits nodes in the correct order. However, for ray queries it needs\nsome adjustments. Specifically, you need the exact entry and exit points for the ray in\nthe current node, tmin and tmax. These values are needed because some intersection\npoints might be outside the current node, illustrated in Figure 2.2.6.\nThe large triangle resides partially in the node that contains the ray origin. If \nyou allow intersections outside the current node, intersecting the ray with this triangle\nwill result in a hit, and so traversal is terminated. The smaller triangle is thus never\nconsidered.\nYou must therefore keep track of tmin and tmax. To do this, you first calculate tmin\nand tmax by clipping the ray against the scene bounding box. After that, tmin and tmax\nare incrementally updated.\n2.2\nFast Generic Ray Queries for Games\n135\n\n\nThe three situations that can occur during traversal are shown in Figure 2.2.7:\n• (Figure 2.2.7a): The intersection of the line with the split plane (tsplit) lies between\ntmin and tmax. The left node is traversed first, with tmax = tsplit. The right node is\npushed on the stack, with tmin = tsplit.\n• (Figure 2.2.7b): tsplit lies beyond tmax. You need only to traverse the left node, and\nno changes to the interval are needed.\n• (Figure 2.2.7c): tsplit lies before tmin. You need only to traverse the right node, and\nno changes to the interval are needed.\nRays that hit the split plane from the right side are handled in the same manner;\nthe only difference is that the left and right child nodes are now swapped.\nThis translates to the following code:\n// precomputed data\nint raydir[8][3][2];\nfor ( int i = 0; i < 8; i++ )\n{\nint rdx = i & 1;\nint rdy = (i >> 1) & 1;\nint rdz = (i >> 2) & 1;\nraydir[i][0][0] = rdx, raydir[i][0][1] = rdx ^ 1;\nraydir[i][1][0] = rdy, raydir[i][1][1] = rdy ^ 1;\nraydir[i][2][0] = rdz, raydir[i][2][1] = rdz ^ 1;\n}\n136\nSection 2\nMath and Physics\nFIGURE 2.2.6\nFalse hit for the larger triangle in the first node.\n\n\n// prepare data for traversal\nKdTreeNode* node = Scene::GetKdTree()->GetRoot();\nvector3 O = ray.origin;\nvector3 D = ray.direction;\nvector3 R( 1 / ray.direction.x, \n1 / ray.direction.y, \n1 / ray.direction.z );\nint oct = ((D.x < 0)?1:0) + ((D.y < 0)?2:0) + ((D.z < 0)?4:0);\nint* rdir = &raydir[oct][0][0];\nint stackptr = 0;\n// actual traversal\nwhile (1)\n{\nwhile (!node->IsLeaf())\n{\nint axis = node->GetAxis();\nKdTreeNode* front = node->GetLeft() + rdir[axis * 2];\nKdTreeNode* back  = node->GetLeft() + rdir[axis * 2 + 1];\nfloat tsplit = (node->m_Split - O.cell[axis]) * R.cell[axis];\nnode = back;\nif (tsplit < tnear) continue;\nnode = front;\nif (tsplit > tfar) continue;\nstack[stackptr].tfar = tfar;\nstack[stackptr++].node = back;\ntfar = MIN( tfar, tsplit );\n}\n// leaf node found, process triangles\nint start = node->GetObjOffset();\nint count = node->GetObjCount();\nfor (int i = 0; i < count; i++ ) // intersect triangles\n// terminate, or pop node from stack\nif ((dist < tfar) || (!stackptr)) break;\nnode = stack[--stackptr].node;\ntnear = tfar;\ntfar = stack[stackptr].tfar;\n}\nThe array raydir is used to swap the left and right child nodes efficiently. The\nlayout of this array is [octant][axis][child]. Each entry in the array contains the\noffset of the near and far nodes (with respect to the ray direction) for an axis, for an\noctant.\n2.2\nFast Generic Ray Queries for Games\n137\n",
      "page_number": 162,
      "chapter_number": 18,
      "summary": "This chapter covers segment 18 (pages 162-170). Key topics include node, axis, and splits.",
      "keywords": [
        "split",
        "node",
        "split plane",
        "data",
        "axis",
        "Generic Ray Queries",
        "Ray",
        "left",
        "int",
        "Ray Queries",
        "cost",
        "triangle",
        "split plane axis",
        "split plane position",
        "Fast Generic Ray"
      ],
      "concepts": [
        "node",
        "axis",
        "splits",
        "triangles",
        "tree",
        "ray",
        "rays",
        "left",
        "bits",
        "bit"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 36,
          "title": "Segment 36 (pages 347-355)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 34,
          "title": "Segment 34 (pages 317-324)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 10,
          "title": "Segment 10 (pages 80-87)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 45,
          "title": "Segment 45 (pages 431-441)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 25,
          "title": "Segment 25 (pages 231-249)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 171-180)",
      "start_page": 171,
      "end_page": 180,
      "detection_method": "topic_boundary",
      "content": "138\nSection 2\nMath and Physics\nFIGURE 2.2.7\nkD-tree traversal cases.\n\n\nDynamic Objects\nThe method for performing ray queries described so far relies on the availability of a\nhigh-quality kD-tree, a structure that generally requires offline precomputations. The\nscenery therefore must be static: In a game, this is generally not the case. There is no\neasy solution to this problem. Ray tracing dynamic scenes is an area of active research.\nDepending on your specific needs, there are several possible solutions.\nThe main reason for not updating the kD-tree every frame is the expensive SAH\nheuristic. You can however use two trees. The first tree contains the static geometry,\nwhereas the second one contains only the dynamic objects. Tracing rays in a mixed\nenvironment (dynamic and static triangles) is then implemented using a two-stage\ntraversal process. First, the ray traverses the tree for the static scenery; then, the ray\ntraverses the tree that contains the dynamic triangles. This may seem inefficient at\nfirst, but in practice, most rays will miss the dynamic geometry, because this geometry\ntypically covers only a small area of the screen. Most of these rays will therefore travel\na small number of empty nodes, without intersecting any triangles.\nIn the proposed scheme, the tree containing the dynamic triangles is rebuilt each\nframe. Using the SAH, determining the split plane position is by far the most expen-\nsive part. By fixing plane positions, a tree for the dynamic triangles can be built in\nvery little time. One way to do this is to choose the spatial median for alternating\naxes. This essentially creates an octree. Using a separate tree for dynamic triangles\nassumes the scenery contains more static geometry than dynamic geometry, which is\ngenerally the case.\nAlternatively, the kD-tree can be abandoned altogether. Different acceleration\nstructures yield reasonable results, but can be built in less time than a kD-tree. Exam-\nples are bounding volume hierarchies (BVHs) [Wald07], the bounding interval hier-\narchy [Wächter06], and nested grids [Wald06b].\nDemo Application\nThe demo application on the CD-ROM demonstrates the described concepts. It\nreads a scene file (in OBJ format) and displays a wireframe representation of the\nmesh. A small dynamic object is also loaded. Per frame, the dynamic object is rotated,\nand a small kD-tree is built. The beam consists of 5,000 rays, of which every 64th ray\nis drawn. These rays are traced through the static kD-tree and the dynamic kD-tree to\ndetermine visibility from the center of the scene. Figure 2.2.8 is a screenshot from the\ndemo. Note that even though the visualization is 2D, the actual data is 3D, and so are\nthe ray queries.\n2.2\nFast Generic Ray Queries for Games\n139\n\n\nConclusion\nGeneric ray queries in a polygonal environment offer a powerful tool for many oper-\nations in a game. This gem described how these queries are efficiently implemented\nusing a high-quality kD-tree based on the surface area heuristic. Dynamic geometry is\nstored in a less efficient kD-tree, which can however be built much quicker.\nImplemented well, the presented approach lets you trace 1 million rays per sec-\nond easily. Should you need more, it is worthwhile to explore SIMD-enhanced packet\ntraversal (traversing multiples of four rays simultaneously).\nYou might even want to explore the fascinating world of real-time ray traced\ngraphics, one of those rare algorithms that you can never throw enough processing\npower at. You will love the intuitive nature of ray tracing—whether you use it for\ngraphics or other purposes.\nReferences\n[Appel63] Appel, A. “Some Techniques for Shading Machine Renderings of Solids,”\nProceedings of the Spring Joint Computer Conference 32 (1968), pp. 37–45.\n[Havran01] Havran, V. “Heuristic Ray Shooting Algorithms,” PhD thesis, Czech\nTechnical University, Praha, Czech Republic, 2001.\n[McDonald90] MacDonald, J., and Booth, K. “Heuristics for Ray Tracing using Space\nSubdivision,” The Visual Computer, Vol. 6, No. 3 (June 1990), pp. 153–166.\n140\nSection 2\nMath and Physics\nFIGURE 2.2.8\nThe demo application displays a wireframe\nrepresentation of the mesh.\n\n\n[Wächter06] Wächter, C., and Keller, A. “Instant Ray Tracing: The Bounding Inter-\nval Hierarchy,” In Rendering Techniques 2006, Proceedings of the 17th Eurographics\nSymposium on Rendering (2006), pp. 139–149.\n[Wald04] Wald, I. “Realtime Ray Tracing and Interactive Global Illumination,” PhD\nthesis, Saarland University, 2004.\n[Wald06a] Wald, I., and Havran, V. “On Building Fast kD-trees for Ray Tracing, and\nOn Doing That in O(NlogN),” Proceedings of the 2006 IEEE Symposium on\nInteractive Ray Tracing (2006), pp. 61–69.\n[Wald06b] I. Wald, I., Ize, T., Kensler, A., Knoll, A., and Parker, S. “Ray Tracing Ani-\nmated Scenes Using Coherent Grid Traversal,” ACM Transactions on Graphics,\nProceedings of ACM SIGGRAPH 2006, pp. 485–493.\n[Wald07] Wald, I., Boulos, S., and Shirley, P. “Ray Tracing Deformable Scenes Using\nDynamic Bounding Volume Hierarchies,” ACM Transactions on Graphics (2007),\nVol. 26, No. 1.\n[Whitted79] Whitted, T. “An Improved Illumination Model for Shaded Display,”\nCommunications of the ACM (August 1979), Vol. 23, No. 6, pp. 343–349.\n2.2\nFast Generic Ray Queries for Games\n141\n\n\nThis page intentionally left blank \n\n\n143\n2.3\nFast Rigid-Body Collision\nDetection Using Farthest\nFeature Maps\nRahul Sathe, Advanced Visual Computing,\nSSG, Intel Corp.\nrahul.p.sathe@intel.com\nDillon Sharlet, University of Colorado \nat Boulder\ndillon.sharlet@colorado.edu\nW\nith the rapidly increasing horsepower of processors and graphics cards, physics\nhas become an important aspect of real-time interactive graphics and games.\nWithout physics, although every frame might look quite realistic, the overall realism\nwill be missing during the gameplay. Modeling of real-world physical phenomena can\nbe quite mathematical in nature and computationally intensive. Game developers\nusually try to simplify the models without compromising visual fidelity. At the same\ntime, they try to improve the computational efficiency of the models. One of the\nmost computationally intensive tasks is collision detection of objects in gameplay.\nThis involves determining whether two objects are colliding (or have collided in the\nlast timestep). If they are colliding, the physical simulation requires some more quan-\ntities like penetration depth and a separating axis.\nThis gem tries to solve some of the complexities involved in the process using\nsome acceleration data structures. It introduces a new data structure called the farthest\nfeature map that is used to accelerate the discovery of a potentially colliding set (PCS) of\ntriangles at runtime. This algorithm, like our previous algorithm based on distance\ncube-maps, works only with convex rigid bodies and has a preprocessing step and a\nruntime step. \nThe intuition behind this new proposed approach is as follows—convex rigid\nbodies when far from each other behave roughly like point masses. However, when\n\n\nthey approach each other, this point mass approximation is not good enough. At that\npoint, you must perform more detailed analysis using the local properties of the bod-\nies near the contact manifold. \nBackground\nCollision detection can be modeled in two ways—discrete collision detection or con-\ntinuous collision detection. The former updates the positions of the game objects at\ndiscrete timesteps and tries to find out if two objects are intersecting at a moment in\ntime. One technical approach to discrete collision detection tries to find intersecting\ntriangles by using a recursively subdivided hierarchy of bounding boxes that are either\naxes aligned (AABB) [Bergen97] or oriented (OBB) [Gottschalk96]. Continuous\ncollision detection systems use a variable timestep and modify that timestep for each\npotentially colliding pair such that two objects never intersect each other. Continuous\ncollision detection is often modeled by algorithms like the one by Gilbert Johnson\nand Keerthy [GJK88]. Refer to these and other references for further details on the\nfundamentals of each approach.\nOur recent work tries to move a lot of work to the preprocessing step for rigid\nbodies [Sathe07]. At runtime, all you do is access the distance cube-maps to find the\ncollision. Distance cube-maps access can be hardware accelerated but there is some\nroom for improvement and optimization, which the new farthest feature map concept\ncan provide.\nFollowing are some terms that are used throughout this gem:\n• Principle curvatures: In differential geometry, principle curvatures at any point on a\nsurface are the curvatures corresponding to the curves with maximum and mini-\nmum curvature at that point. These curves are always at right angles to each other.\n• Mean curvature: The average of principle curvatures is called the mean curvature.\n• Best-fitting sphere: For the typical game mesh, represented by planar triangles with\ncreases at neighboring edges rather than continuous geometry with smooth deriv-\natives at every seam, the curvature definitions cannot be applied in a meaningful\nway to many algorithms that require geometric interrogation. So you have to come\nup with some approximation of the curvatures. The ring-1 neighborhood of a vertex\nis the triangle fan around that vertex. We consider the ring-1 neighborhood\naround a given vertex and try to find the sphere that has center along the normal\naxis passing through that vertex. This circle is a crude approximation of the sphere\nwith the radius equal to that corresponding to mean curvature for a triangle mesh. \nPreprocessing\nThis approach uses a data structure that is similar to a cube-map; that is, it is a directional\nlookup, although the stored data exceeds the capacity of cube-map texture formats for\ncurrent generation graphics hardware. Place the object’s centroid at the origin. Then\nimagine an axis-aligned cube centered at the origin and then shoot rays from the origin\n144\nSection 2\nMath and Physics\n\n\nthrough all the pixel centers on the faces of the cube-map. For each of these rays, you pro-\nject the line segment from the centroid to the vertices on this ray. You select the maxima\nof these projections and store the corresponding vertex (vertices) for that sampled direc-\ntion. We call this data structure a farthest feature in that sampled direction.\nAn intuitive way to visualize a farthest feature map is to think of a perpendicular\nplane for a given sampled direction. Then, think of moving that plane outward from\nthe centroid keeping it perpendicular to that sampled direction. This plane will inter-\nsect with the convex object to yield some polygon in that plane. If you keep on mov-\ning this plane outward, eventually the plane will intersect with one or more of the\nvertices of the convex model. As you continue moving the plane farther away from\nthe centroid, it will eventually move far enough to not intersect with the convex\ngeometry. At this point, the distance to the perpendicular plane is the farthest dis-\ntance in that direction and any vertices from geometry that lay on the plane just prior\nto the farthest distance are the farthest features.\nFigures 2.3.1 through 2.3.3 show this in the 2D case. Here, the thick solid line is\nthe convex geometry of the game object being processed. The geometry is defined\nhere by vertices V0 through V4. The plane is shown in thick dotted lines. Arrows rep-\nresent the directions that they are being moved in order to find the farthest features\nfor the two sampled directions dir1 and dir2.\n2.3\nFast Rigid-Body Collision Detection Using Farthest Feature Maps\n145\nFIGURE 2.3.1\nPlanes perpendicular to the sampled directions\ndir1 and dir2 are shown in dotted lines. They are moving away\nfrom the centroid.\n\n\n146\nSection 2\nMath and Physics\nFIGURE 2.3.2\nPlanes moving farther away from the\ncentroid.\nFIGURE 2.3.3\nPlanes perpendicular to the sampled\ndirections dir1 and dir2 in their farthest positions. V0\nand V1 form the farthest features in dir2 direction and\nV2 is the farthest feature in dir1 direction.\n\n\nFigures 2.3.4 and 2.3.5 illustrate the construction of the farthest feature map in\n3D, showing the farthest features in two different sampled directions.\n2.3\nFast Rigid-Body Collision Detection Using Farthest Feature Maps\n147\nFIGURE 2.3.4\nThe plane that is perpendicular to dir1 moving\naway from the centroid C is in its final position. CQ1 is the\nprojection of CP1 on dir1. P1 and Q1 lie on the plane.\ni\nFIGURE 2.3.5\nThe plane that is perpendicular to dir2 moving away\nfrom the centroid C is in its final position. CQ2 is the projection of\nCP2 on dir2. P2 and Q2 lie on the plane.\n",
      "page_number": 171,
      "chapter_number": 19,
      "summary": "This may seem inefficient at\nfirst, but in practice, most rays will miss the dynamic geometry, because this geometry\ntypically covers only a small area of the screen Key topics include rays, dynamic.",
      "keywords": [
        "Ray tracing",
        "ray",
        "farthest feature map",
        "farthest feature",
        "Generic Ray Queries",
        "Ray tracing dynamic",
        "ray queries",
        "collision detection",
        "Farthest",
        "Dynamic",
        "rays",
        "plane",
        "tracing",
        "Fast Generic Ray",
        "Generic Ray"
      ],
      "concepts": [
        "ray",
        "rays",
        "dynamic",
        "traverses",
        "traversing",
        "planes",
        "fast",
        "maps",
        "map",
        "triangles"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 44,
          "title": "Segment 44 (pages 422-430)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 43,
          "title": "Segment 43 (pages 413-421)",
          "relevance_score": 0.43,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 32,
          "title": "Segment 32 (pages 306-318)",
          "relevance_score": 0.42,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 48,
          "title": "Segment 48 (pages 456-465)",
          "relevance_score": 0.42,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 32,
          "title": "Segment 32 (pages 296-313)",
          "relevance_score": 0.39,
          "method": "api"
        }
      ]
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 181-193)",
      "start_page": 181,
      "end_page": 193,
      "detection_method": "topic_boundary",
      "content": "The data that you store in a given sampled direction (equivalent to a single pixel\ncenter in the cube-map analogy) is represented by class Node. Here, you store the\nindex of the vertex representing the farthest feature along the direction. If there is\nmore than one vertex, you store the index of all of them for this direction. At runtime,\nyou can start off with any one of these features.\nThe detailed farthest feature information for the vertices is stored separately. \nFor every vertex, you store its neighborhood information as shown in the class \nVertexNeighborhood. You store the following data per vertex—the center of the best\nfitting sphere, as well as equations of planes, face IDs, and edges. You also store the\nnormal and center for the best-fitting sphere for each of the vertices that form the\nfarthest feature for a given direction. Edges are used to find edge-edge intersections.\nThe reason for splitting this into per vertex and per direction (farthest feature\nmap) is storage optimization. For low poly objects, if you oversample the farthest fea-\nture map, you want to store as little information as possible per sample; for example,\nthe indices into the VertexNeighborhood buffer.\nClass Node \n{\nint *Index;\n}\nClass VertexNeighborhood\n{\nVector < Plane > Faces;\nVector < DWORD > Triangles;\nVector < Pair < DWORD, DWORD > > Edges;\nVector3 CenterOfBestFittingSphere;\n}\nThe best fitting sphere at the farthest feature is the sphere that best fits the trian-\ngle fan around the farthest feature. In the 2D case, this will correspond to a circle that\nbest fits the ring-1 neighborhood of the vertex, which for a 2D piecewise linear curve\nis always two vertices. It’s always possible to find the exact circle that passes through\nthree points (unless they are collinear). This is shown in Figure 2.3.6. In the 3D case,\nit usually is not possible to find a sphere that exactly passes through all of the ring-1\nvertices, because ring-1 will usually contain more vertices than needed to minimally\ndefine a sphere, and the surface will rarely happen to be exactly spherical at any given\npoint.\nRuntime Queries\nAt runtime, you start off with the assumption that two convex meshes under consid-\neration behave like a point mass. This assumption is true when they are far away from\neach other. The question that you have to answer then is how far is far enough? It\nturns out that you can approximate these objects with point masses so long as their\nbounding volumes do not intersect each other. The bounding volumes that are used\n148\nSection 2\nMath and Physics\n\n\nto approximate the object determine how far the objects can be. Bounding spheres,\naxis-aligned bounding boxes, and oriented bounded boxes are popular bounding\nvolume approximations.\nAt runtime, you proceed as follows. First, connect the two centroids. Along this\ndirection connecting two centroids, find the farthest features of the two objects using\nfarthest feature-maps that were created during preprocessing. In some sense, you have\nused the farthest feature map to quickly generate the bounding volumes. However,\nthey differ from conventional bounding volumes because they can be different along\nany direction. In this case, you are looking at the bounds along the line joining two\ncentroids.\n2.3\nFast Rigid-Body Collision Detection Using Farthest Feature Maps\n149\nFIGURE 2.3.6\nBest fitting circle in 2D.\nIf the sum of the distances to the farthest feature maps from the respective cen-\ntroids along the line joining centroids is more than the distance between the centroids,\nyou know that the bounding volume test along this axis has failed, and the objects\nbelong to the potentially colliding set. At this point, you lose the liberty to treat convex\nobjects as point masses located at their centroids and you have to perform a more\ndetailed analysis.\nIn this case, you query the farthest feature map to find the center of the best\nfitting sphere at the farthest features for both the objects. Then, connect these centers\nof best fitting spheres at farthest features of the two objects. This step in some sense\napproximates the two interacting objects locally with spheres of radii corresponding\nto the best fitting spheres. The important thing to note here is that the approximation\n\n\nis only a “local” approximation. Find the farthest features along the line joining these\ncenters and do the distance test that you did earlier when you joined the centroids. If\nthe distance test fails, continue finding the best fitting spheres at the farthest features\nuntil, at two successive steps, you find the same farthest feature. At this point, you\nconclude the algorithm.\nThe pseudocode for the algorithm follows:\nlastC1 = lastC2 = null;\nConnect the two centroids C1C2\nFind the distance C1C2 between two centroids\nFind the distance d1 and d2 to farthest features along C1C2\nwhile (C1C2 < d1+d2 || lastC1 != C1 || lastC2 != C2)\n{\nlastC1 = C1\nlastC2 = C2\nC1 = center of best fitting sphere at farthest feature \nOf object 1\nC2 = center of best fitting sphere at farthest feature \nOf object 2\nd1 = Distance to farthest feature along C1C2 from C1\nd2 = Distance to farthest feature along C2C1 from C2\n}\nWhen you conclude the algorithm, you are left with two farthest features from two\nobjects and their ring-1 neighborhoods. For a typical game mesh, these two triangle fans\nwill have around six triangles each. At this point, you have to determine if a triangle from\nthe triangle-fan for one object intersects with any triangle from the triangle-fan cor-\nresponding to the other object. You thus are left with having to do triangle-triangle\nintersection tests for typically 36 (triangle-triangle) pairs. All these triangle-triangle\nintersection tests can be performed in parallel, using the appropriate processor technol-\nogy and programming language.\nPerformance Analysis and Concluding Remarks\nIn most cases, the algorithm converges in O(1) time. This will be the case when the\ndistribution of the vertex density of the two convex meshes doesn’t vary too much\nwith the directions. If the contact point is in the region where concentration of\nvertices is higher, the algorithm takes longer to converge.\nIt will be interesting to see how this algorithm can be extended to work with con-\ncave objects. With the ever-increasing general programmability of the graphics hard-\nware, we want to look at how one can exploit a flexible cube-map that can store more\nthan just a fixed format texture data. Adaptively sampled cube-maps will save a lot of\nstorage but will increase the lookup costs. Some clever encoding scheme that solves\nthis problem will be nice. \n150\nSection 2\nMath and Physics\n\n\nAcknowledgments\nWe would like to thank the management group in Advanced Visual Computing\nGroup, SSG at Intel for allowing us to pursue this work. We would also like to thank\nour co-workers Adam Lake and Oliver Heim for their help at various stages of this\nwork. \nReferences\n[Bergen97] Van den Bergen, G. “Efficient Collision Detection of Complex\nDeformable Models Using AABB Trees,” Journal of Graphics Tools, Vol. 2, No. 4,\npp. 1–14, 1997.\n[GJK88] Gilbert, E.G., Johnson, D.W., and Keerthi, S.S. “A Fast Procedure for Com-\nputing the Distance Between Objects in 3 Dimensional Space,” IEEE Journal of\nRobotics and Automation, Vol. RA-4, pp. 193–203, 1988.\n[Gottschalk96] Gottschalk, S., Lin, M.C., and Manocha, D. “OBBTree: A Hierarchi-\ncal Structure for Rapid Interference Detection,” Proc. SIGGRAPH 1996, pp.\n171–180.\n[Sathe07] Sathe, Rahul. “Collision Detection Shader Using Cube-Maps,” ShaderX5\nAdvanced Rendering Techniques, Charles River Media, 2007.\n2.3\nFast Rigid-Body Collision Detection Using Farthest Feature Maps\n151\n\n\nThis page intentionally left blank \n\n\n153\n2.4\nUsing Projective Space \nto Improve Precision of\nGeometric Computations\nKrzysztof Kluczek, Gda´nsk University \nof Technology\nkrzych82@poczta.onet.pl\nM\nany geometric algorithms used in modeling, rendering, physics, and AI modules\ndepend on point-line and, in case of three-dimensional space, point-plane tests.\nThe finite precision of computing introduces problems when collinear or coplanar cases\nare to be detected. A common solution to this problem is detecting such cases based on\na set minimum distance (epsilon), below which elements being processed are assumed\nto be collinear or coplanar. This set minimum distance solves the problem only partially\nbecause checking results against such a distance is prone to signaling false-positives,\nwhich reduces the overall robustness of any algorithm using such a solution.\nIn order to create robust geometric algorithms that will operate correctly in every\ncase, you can’t use any operations that will truncate intermediate results (for example,\noperations on floating-point numbers). Truncation leads to loss of information, which\nunder certain circumstances can be needed to obtain results. This leaves math based on\nintegers the only way to ensure that truncation doesn’t happen. The straightforward\nrepresentation of points using integer Cartesian coordinates isn’t the solution because\nthe intersection of a pair of lines with endpoints with integer coordinates doesn’t\nnecessarily occur at integer coordinates. Using rational numbers can solve the problem\n(see [Young02]), but this requires storing six integer values per vector (numerator and\ndenominator for each component) and implementations of efficient operations on\nsuch vectors aren’t straightforward in three-dimensional space. Using projective space\ncan reduce this number to four integers per vector by using vectors with one extra\ndimension and storing only a single integer per vector component, which is demon-\nstrated in this chapter.\n\n\nProjective Space\nThe concept of projective space is fundamental to understanding algorithms presented\nhere. RP2 projective space can be described as a space of all lines in R3 space passing\nthrough point [0,0,0], as shown on Figure 2.4.1. Each line can be uniquely identified\nby a point in R3/[0,0,0], which lies on the line, and so you can use an [x,y,z] coordi-\nnate vector to identify each element in this space, where [x,y,z] \u0004 [0,0,0]. Still, \nyou have to keep in mind that any pair of vectors P and Q identifies the same line if\nP = Qc for certain scalars c \u0004 0. Representation of the elements of RP2 as [x,y,z]\nvectors is known as homogeneous coordinate representation. Keeping in mind that\nelements of RP2 can be understood as lines crossing the origin of R3, let’s define a \nz = 1 plane in this R3 space. Every line crosses the z = 1 plane exactly once, unless it’s\nparallel to this plane. The intersection point can be calculated from the line equation\nand in the case of lines crossing the origin it is located at [x/z,y/z,1] where [x,y,z] is any\npoint on the line other than [0,0,0]. Therefore, you can easily relate points in R2 and\nelements of RP2. Any point [s,t] in R2 can be represented by element [s,t,1] in RP2 or\nin general, any [s,z,tz,z] where z \u0004 0. Therefore, any vector [x,y,z] with z \u0004 0 repre-\nsenting an element of RP2 space will represent point [x/z,y/z] in R2 space. This repre-\nsentation can be easily extended to higher-dimensional spaces, where [p1,...,pn,w] in\nRPn is related to [p1/w,...,pn/w] in Rn.\n154\nSection 2\nMath and Physics\nFIGURE 2.4.1\nRP2 projective space as a space of lines\nin R3 crossing at the origin. Each line can be uniquely\nidentified by a point in R3/[0,0,0] and each line crosses\nthe Z = 1 plane exactly once unless it’s parallel to it.\nThis gem focuses on the representation of R2 space by RP2 projective space and\nrepresentation of R3 space by RP3 space. The printed portion of this gem focuses on\noperations in R2 space using RP2 projective space. Because a point in RP2 space can\nbe represented by a three-component vector, computations on such data are far easier\nto imagine and understand than computations on four-component vectors used to\nrepresent elements in RP3 space. The CD-ROM contains a separate document that\nexpands the concepts described for RP2 space for application to RP3 space to allow\ncomputations on three-dimensional data.\n\n\nBasic Objects in R2\nIn two-dimensional space R2, you will focus on two types of objects, points and\ndirected lines, which let you define more complex structures. You define a directed\nline in R2 space as a line with specified running direction, which allows us to define\nleft and right sides of the line with respect to its running direction. This directional\nfeature of the line definition is essential for efficient polygon representation, where we\ncan assume that the interior of the polygon lies on a certain side of the line. In the case\nof convex polygons, which can be represented by an ordered list of directed lines\ndefining its boundary, the above assumption can lead to very efficient algorithms for\noperating on such polygons. Just like convex polygons, many other objects in R2 such\nas segments and rays (half-lines) can be represented using points and directed lines, so\nit’s safe to focus only on these two types of objects.\nPoints and Directed Lines in RP2\nAs it was defined at the beginning of this chapter, a point [s,t] in R2 can be represented\nby element [s,t,1] in RP2 space, so conversion from R2 to RP2 space is straightforward.\nBecause scaling vectors in RP2 space doesn’t affect their meaning, point [s,t] can be rep-\nresented by any vector [sz,tz,z] where z \u0004 0. For simplicity of future computations, let’s\nassume z \u0005 0. If this is not the case, the entire vector can be scaled by –1 to fix this.\nRecall that vectors with z = 0 don’t represent valid points in R2. To obtain the formula\nfor conversion of vectors from RP2 projective space to points in R2 space, you can use\nthe same rule. Every vector [x,y,z] in RP2 space represents the point [x/z,y/z] in R2\nspace. This is similar to perspective projection onto z = 1 plane with the center of pro-\njection placed at the origin. Therefore, you can think of RP2 space almost like R3 space\nkeeping in mind that it is not the vector [x,y,z] that is important, but its perspective\nprojection onto the z = 1 plane, which results in the vector [x/z,y/z,1] representing\npoint [x/z,y/z] in R2 space. As you can see, this is very similar to the representation of\npoints in RP2 using rational coordinates with a common denominator (in this case z),\nbut defining it as a vector in RP2 space will give you efficient tools for performing com-\nputations on such points. Functions used to perform transformations of points\nbetween R2 and RP2 spaces are shown in Equations 2.4.1 and 2.4.2. Note that these\nfunctions operate on elements of R3 space as they take an argument or result in a vec-\ntor in R3 referring to one of the elements (lines passing through the origin) of RP2 pro-\njective space and not the element of RP2 projective space itself.\n(2.4.1)\n(2.4.2)\nThe other object in R2 space of particular interest is a directed line. Just as points\nare perspective projections of vectors onto the z = 1 plane, lines in RP2 are perspective\ng\nx y z\nx z y z\n, ,\n/ , /\n⎡⎣\n⎤⎦\n(\n) =⎡⎣\n⎤⎦\ng\nz\n: R\nR\n≠→\n0\n3\n2\nf\ns t\ns t\n,\n, ,\n⎡⎣\n⎤⎦\n(\n) =⎡⎣\n⎤⎦\n1\nf :\n,\nR\nR\n2\n3\n→\n2.4\nUsing Projective Space to Improve Precision of Geometric Computations\n155\n\n\nprojections of planes onto the z = 1 plane. The only case when perspective projection\nof a plane results in a line is when the plane passes through the center of the projec-\ntion, so every plane you use to define a line has to pass through point [0,0,0]. In fact,\naccording to the definition of RP2 space you use, you can’t define a plane in this space\nthat does not pass through the origin, because RP2 space elements are lines passing\nthrough the origin and every plane you define in RP2 must contain entire lines. The \nz = 1 plane is the only exception to this rule because it is used only for projection of\nRP2 space to R2 space. Considering that every plane is passing through the origin, the\ndefinition of a line as projection of a plane in RP2 onto z = 1 plane can be simplified.\nThe defined line is simply the intersection of the given plane and the z = 1 plane.\nAgain, because all such planes pass through the origin, it’s sufficient to store only their\nnormal vectors. You don’t require a normal to be normalized and in general it won’t be\nnormalized, because you will be using only integer coordinates for the normal vectors.\nThe previous definition of a line can be extended to a definition of a directed line\nby simply taking into account the direction of the normal vector of the plane used for\nthis representation. You define the positive side of a plane as a half-space containing all\nvectors for which the dot product with the normal of this plane results in a positive\nvalue. To put it more straightforward, the positive side of the plane is the half-space the\nnormal vector is pointing at. Similarly, you can define the negative side of this plane.\nBecause a line represented with a plane is an intersection of this plane with the z = 1\nplane, the plane divides the z = 1 plane into two half-planes, one lying in the positive\nhalf-space and the other in the negative one. You can call the half-plane on the positive\nside the right side of the directed line and the other half-plane the left side. From this,\nyou can derive the direction of the directed line, which makes complete definition of\nrepresentation of a directed line with a plane in RP2 space. Figure 2.4.2a contains an\nexample representation of a directed line with a plane in RP3 projective space.\n156\nSection 2\nMath and Physics\nFIGURE 2.4.2\nPoints and lines in RP2 projective space: (a) a line passing through a\npair of points; vectors u and v represent points u’ and v’ and the line is represented\nby a plane with normal N, (b) intersection of a pair of lines; N and M are normals\nof planes representing the lines and their cross product results in a vector represent-\ning the intersection point.\n\n\nBasic Operations in RP2\nThere are three basic operations on points and directed lines. Given an ordered pair of\npoints, you can find a directed line passing through them in the given order. Given a\npoint and a directed line, you can determine which side of the line the point lies on or\nif it lies on the line. Finally, given a pair of directed lines you can find their intersec-\ntion point. All these operations are fairly easy and straightforward in RP2 space.\nGiven a pair of vectors representing two points in RP2 space, you can look for a\ndirected line passing through the points. Because of the nature of RP2 space, both\nvectors representing the points have to lie on the plane representing the line you are\nlooking for; otherwise, the points resulting from their perspective projection wouldn’t\nlay on this plane as every vector lying on the plane is parallel to the plane normal.\nGiven a pair of such vectors, you can find the normal you are looking for using the\ncross product of these vectors. An example of a line led through a pair of points is\nshown in Figure 2.4.2a. The normal computed this way will correctly define the plane\nin RP2 space representing the directed line you are looking for. The direction of the\nline depends on the ordering of points used to compute the line, because swapping\ncomponents of cross product inverts the result. It’s desired that the line computed in\nthis operation will be directed in such a way that its running direction will be the\ndirection from first point to the second one, so you have to make sure that computed\nnormal correctly defines the right side of the directed line by correctly defining the\npositive side of the plane in RP2 space. Because this depends on the handedness of \nthe coordinate system being used, be sure to check this during the implementation of\nthis operation and reverse the order of the vectors in the cross product if needed.\nAnother operation of particular interest is a point-line test, where you can find\nwhere a given point lies with respect to a given directed line. As stated previously,\nwhen a point lies on the line, the vector in RP2 space representing the point will lie on\nthe plane representing this line and therefore it will be perpendicular to the plane’s\nnormal. This makes the dot product the perfect tool for this check. If a result of a dot\nproduct of vector representing the point and the normal of the plane is zero we can be\nsure that the point lies on the line. Because the normal of the plane defines its positive\nside and the right side of the directed line, if the point doesn’t lie on the line, you can\nuse the sign of the dot product to determine the side of the line the point lies on.\nThanks to the assumption of z \u0005 0 for coordinates of the vector representing the\npoint, the result of the dot product will always be positive when the point lies on the\nright side of the directed line and it will always be negative when it lies on the left side\nof this line.\nThe last important basic operation on points and directed lines is finding the\nintersection point of a pair of lines. Again, the vector representing the point you are\nlooking for has to be perpendicular to the normals of both planes representing the\ngiven directed lines, as shown on Figure 2.4.2b. By performing the cross product on\nthese normals, you can find the vector in RP2 space representing the point you are\nlooking for. To make sure that the z \u0005 0 condition is met, even when z \u0006 0 in the\n2.4\nUsing Projective Space to Improve Precision of Geometric Computations\n157\n\n\nresulting vector, the entire vector has to be scaled by –1 to obtain the proper vector rep-\nresenting the point. If the given directed lines are parallel, the resulting vector will have\nthe coordinate z = 0, which indicates that intersection point of these lines doesn’t exist.\nPrecise Geometrical Computations in RP2 Using\nInteger Coordinates\nAll of the operations described previously are based mainly on basic comparisons and\ndot and cross products of vectors. Both dot and cross products require addition, sub-\ntraction, and multiplication to implement. Note that if only vectors with integer\ncoordinates are used, none of these operations will result in a fractional number, or a\nvector with fractional components. Therefore, by using only points with integer coor-\ndinates, these operations are guaranteed to return exact results, free of numerical\nerrors that are inherent to floating point computations. Unfortunately, to make such\na system usable, you have to make sure it works correctly with floating point input\ndata as most existing systems, like modeling packages, can provide only floating point\ndata. In most cases, you can choose a certain finite precision (for example, 10–3), scale\nthe data up, round it to nearest integers and, after performing all necessary opera-\ntions, scale the result back. Note that rounding used here only affects positions of\ninput points, but doesn’t affect further operations, especially tests where it should be\ndetermined whether or not a point lies on a line.\nNumber Range Limits in Geometrical Computations in RP2\nBecause integer multiplication is a frequent operation in computations described in\nthis chapter, you have to be aware of range limits of an integer representation. Con-\nsecutive multiplications quickly make the values you operate on large. To estimate\nranges used in each stage of computations, you can use symmetrical range estimates\n[–a,a] that define the minimum and maximum values that can be achieved at a given\nstage in a worst-case scenario. Knowing the range of input values, you can easily esti-\nmate the range of results of operations such as addition, subtraction, and multiplica-\ntion and using this, the range of results of more complex operations like dot and cross\nproducts can be estimated. Having two values within ranges [–a,a] and [–b,b], you\ncan be sure that their sum will be within range [–a–b,a+b]. Because the estimated\nranges are symmetrical, the difference of a pair of given values results in values within\nexactly the same range [–a–b,a+b]. Similarly, the result of multiplication of two values\nwithin ranges [–a,a] and [–b,b] can be proven to lie within the range [–ab,ab]. Know-\ning this, you can analyze the numerical ranges required for performing computations\nat every step.\nBecause consecutive operations on points result in consecutive multiplications\nintroducing large numbers, you can limit operations to three classes of objects that are\nenough for most geometric computations. The first class is a point being a part of\ninput data. Such a point has its coordinates in R2 space given explicitly (for example,\n158\nSection 2\nMath and Physics\n\n\nimported from a digital content creation program) and therefore you can easily pre-\ndict the range of its coordinates based on game level extents and the required preci-\nsion of its representation. The extents and precision are used for scaling model\ncoordinates during the import of the data. Although it is desired to increase allowed\nrange of coordinates of input points because it allows for larger game levels and/or\nmore precision, this range is the main factor influencing all numerical ranges used in\ncomputations. The numerical range analysis aims to find a compromise between algo-\nrithm efficiency (depending on ranges of intermediate values) and range and preci-\nsion of input data, because large integers require both more storage space as well as\nmore processing power.\nThe second class of objects is directed lines computed as lines passing through a\npair of points from input data. Input point coordinates are given explicitly, so the nor-\nmal vectors of planes representing directed lines of this class are obtained using a sin-\ngle cross product. You can easily estimate the ranges required to represent components\nof normal vectors of planes representing such lines.\nThe last class of objects being used is a class of points obtained as a result of inter-\nsection of a pair of lines of previous class. Vectors representing such points in RP2\nspace are the results of cross products on normal vectors of planes representing the\nlines, which make these vectors the results of two consecutive cross products. In\neffect, the range of their coordinates is larger than the range of coordinates of vectors\nrepresenting other classes of objects.\nBecause of a growing numerical range of coordinates used with the various classes\nof objects, you should aim to use only these three classes of objects (points from input\ndata, lines led through such points, and intersection points of these lines). Operations\nresulting in objects outside these three classes, like defining a directed line passing\nthrough an intersection point, should be avoided because they can make resulting\nnumbers grow without a control. Fortunately, many geometrical algorithms, includ-\ning constructive solid geometry (CSG) algorithms, can be implemented in a way that\ndoesn’t require operations on objects outside the three classes specified. If for some\nreason an algorithm cannot be implemented with just the three listed classes, you can\nintroduce new classes of objects, such as lines passing through a pair of intersection\npoints. But be aware that the numerical range required for operations on these new\nclasses can grow exponentially with the number of classes of objects and this range\nshould be estimated for each introduced class.\nThe analysis of numerical range being used in described operations is shown in\nTable 2.4.1. It can be seen that subsequent operations make numerical range of result-\ning values grow quickly. Table 2.4.2 shows the maximum ranges of input point coor-\ndinates and ranges of further computation results depending on number of bits used\nto perform the computations. Even with 64-bit integers, the allowed input point\ncoordinate range is only [–20936,20936]. In some applications, the [–20936,20936]\nrange can be sufficient and in this case algorithms described can be implemented\n2.4\nUsing Projective Space to Improve Precision of Geometric Computations\n159\n\n\nwithout much effort, allowing geometric computations even on devices lacking float-\ning point support (for example, cell phones). However, in many applications this\nrange won’t allow for required precision and game level extents. For these applica-\ntions, long integer math can provide a solution.\nTable 2.4.1\nNumerical Ranges of Results in Used Projective Space Operations for\nTwo-Dimensional Geometry\nObject\nEquation\nCoordinates\nRange\nInput point (P)\nP = [x,y,1]\nx,y\n[–n,n]\nz\n1\nNormal of a plane (N)\nN = P1 × P2\nx,y\n[–2n,2n]\nz\n[–2n2,2n2]\nIntersection point (Q)\nQ = N1 × N2\nx,y\n[–8n3,8n3]\nz\n[–8n2,8n2]\nInput point check versus line\nN \u0007 P\n[–6n2,6n2]\nIntersection point check versus line\nN \u0007 Q\n[–48n4,48n4]\nTable 2.4.2\nMaximum Values Allowed at Every Step with Respect to Length of Used\nInteger Representation\nRange\n16 Bits\n32 Bits\n64 Bits\n[–n,n]\n5\n81\n20 936\n[–2n,2n]\n10\n162\n41 872\n[–2n2,2n2]\n50\n13 122\n876 632 192\n[–8n2,8n2]\n200\n52 488\n3 506 528 768\n[–8n3,8n3]\n1 000\n4 251 528\n73 412 686 286 848\n[–48n4,48n4]\n30 000\n2 066 242 608\n9 221 808 000 608 698 368\nMaximum value\n32 767\n2 147 483 647\n9 223 372 036 854 775 807\nTo find out the length of the integer representation required for a given applica-\ntion, you have to decide how large the game level extents are and how much precision\nyou need. With that information, you can derive the required range of integer coordi-\nnates being used. For example, if desired workspace size is [–1000,1000] range of\npoint coordinates and required precision is 0.01, after scaling the point data during\nthe import, the required range of input coordinates of points used in further opera-\ntions is equal to [–100000,100000]. Then you can find out the ranges required to\ncarry out operations being used without risking overflows. This in turn gives you the\nnumber of bits required for geometry representation with integers (be sure to include\nthe sign bit in this number of bits).\n160\nSection 2\nMath and Physics\n",
      "page_number": 181,
      "chapter_number": 20,
      "summary": "This chapter covers segment 20 (pages 181-193). Key topics include points, vector, and line. For every vertex, you store its neighborhood information as shown in the class \nVertexNeighborhood.",
      "keywords": [
        "Space",
        "line",
        "point",
        "directed line",
        "Projective Space",
        "Plane",
        "farthest feature",
        "Vector",
        "Range",
        "objects",
        "farthest",
        "directed",
        "operations",
        "intersection point",
        "point lies"
      ],
      "concepts": [
        "points",
        "vector",
        "line",
        "space",
        "operate",
        "operations",
        "operating",
        "operation",
        "computing",
        "computations"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 34,
          "title": "Segment 34 (pages 326-336)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 30,
          "title": "Segment 30 (pages 279-290)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 9,
          "title": "Segment 9 (pages 160-181)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 20,
          "title": "Segment 20 (pages 182-197)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 32,
          "title": "Segment 32 (pages 306-318)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 194-209)",
      "start_page": 194,
      "end_page": 209,
      "detection_method": "topic_boundary",
      "content": "Although the maximum range used can require a large number of bits for integer\nrepresentation (often over 64 bits), this number is required only for the most complex\ncases; most basic operations can be performed using much shorter integers. To find\nthe number of bits required, in every case an analysis similar to the one given previ-\nously should be done. The range estimates given in Table 2.4.1 can be useful during\nimplementation of used long integer math, because these estimates can be used to\nfind required range of integers according to input data point coordinate range.\nExample Application of Operations in RP2\nTo prove the usefulness of geometric operations in RP2 space, a simple algorithm per-\nforming Boolean operations on polygons is presented. For simplicity, polygons are\nassumed to be convex, as every set of input polygons can be partitioned into a set of\nconvex polygons. A convex polygon can be described with a loop of edges. Each edge\nis defined by its points and a directed line running along the edge in such a direction\nthat the interior of the polygon lies on the right side of this line. The example assumes\nthat in a polygon no three vertices are collinear.\nThe basic operation useful during Boolean operations on convex polygons is\ncutting a polygon with a directed line. This operation can be accomplished using the\nfollowing algorithm, which is illustrated in Figure 2.4.3:\n1. For each polygon vertex, determine which side of the cutting line the vertex\nlies on or whether it lies on the line.\n2. If there are no vertices on the right side of the cutting line or there are no\nvertices on its left side, stop. The line doesn’t intersect the interior of the\npolygon.\n3. Split each edge for which starting and ending points lie on opposite sides of\nthe cutting line. Two edges are created in place of the edge being split. The\nmiddle point of the split is the intersection of the line running along the\nedge and the cutting line.\n4. Create the first of resulting polygons from edges lying on the right side of\nthe cutting line. Close this polygon by adding an edge running along the\ncutting line between vertices lying on this line (if the initial polygon was\ncorrect, there are exactly two such vertices).\n5. Similarly, create the second resulting polygon from edges lying on the left\nside of the cutting line. Close this polygon by adding an edge running along\nthe cutting line, but in the opposite direction (reverse the normal of the\nplane defining the cutting line in RP2 space).\nIt’s worthwhile to note that this algorithm uses only the three classes of objects in\nRP2 space listed earlier. In step 3, new intersection points are introduced to the poly-\ngon and in steps 3, 4, and 5, existing directed lines are used to define polygon edges.\nThe algorithm doesn’t create new lines using existing points, so it isn’t important whether\nthe vertices in the initial polygon are points from input data or intersection points.\n2.4\nUsing Projective Space to Improve Precision of Geometric Computations\n161\n\n\nThe algorithm will work flawlessly on both types of vertices. It’s also worthwhile to\nnote that in step 3, the splitting point of the edge has to be computed as an intersec-\ntion of a pair of lines. In the case of floating point–based computations this point\ncould have been computed using linear interpolation of edge endpoints based on dis-\ntances of these points to the cutting line. In the case of integer-based operations using\nRP2 space, you don’t have an operation computing point-line distance defined. How-\never, as this algorithm shows, this operation isn’t required in this case.\nAlthough cutting polygons with lines can be a useful operation by itself, you can\nuse this operation to implement Boolean set operations on polygons. The following\nalgorithm describes the initial steps required to perform such operations, which is\nillustrated in Figure 2.4.4.\n162\nSection 2\nMath and Physics\nFIGURE 2.4.3\nCutting a polygon with a line: (a) determining which side of the line each\nvertex lies on (steps 1 and 2), (b) finding intersection vertices and splitting edges (step 3), \n(c) separating edges and closing resulting polygons (steps 4 and 5).\nFIGURE 2.4.4\nFinding the intersection of a pair of polygons. Steps (a) to (c) show resulting\npolygons after each cut done during step 3 of the algorithm. Initial polygon B is filled gray,\ncurrent polygon C is outlined with thick line, and polygons in set outA have normal outline.\n\n\n1. Let A and B be a pair of given polygons.\n2. Let C be a copy of A that will be used for finding the intersection.\n3. For each edge of polygon B, cut polygon C using directed line associated\nwith this edge. Add the part of former C lying on the left side of the cutting\nline to set outA and replace polygon C with the part of this polygon lying\non the right side of the cutting line. If no part of C lies on the right side of\nthe line, stop, as polygons A and B don’t intersect.\n4. Repeat step 3 for each edge of B until all edges have been considered (unless\nthe algorithm was stopped).\nWhen this algorithm finishes, providing that initial polygons A and B intersected\n(indicating that the algorithm didn’t stop early), the polygon C after all operations\nwill be the intersection of initial polygons A and B and set outA will contain parts of\ninitial polygon A lying outside polygon B. Basic Boolean operations on initial A and\nB can be expressed as follows, which can be seen in Figure 2.4.5.\nA ∪B = outA ∪B\nA ∩B = C\nA\\B = outA\nIn these formulas, ∪is the union of a pair polygons, ∩is their intersection, and \\\nis their difference. When operating on sets of polygons, ∪is a union of a pair of sets.\nWhen polygons in set outA and polygon B don’t overlap, the union outA ∪B can be\ncomputed by simply adding polygon B to set outA.\n2.4\nUsing Projective Space to Improve Precision of Geometric Computations\n163\nFIGURE 2.4.5\nBoolean operations on polygons: (a) sum of polygons A and B as a sum of\noutA and B, (b) intersection of polygons A and B as resulting polygon C, (c) difference of\npolygons A and B as resulting set outA.\nBecause this algorithm is based entirely on integer operations in RP2 space, it can\nbe proven to work with all possible sets of valid input data, which is rarely the case\nwhen it comes to algorithms performing Boolean operations. However, this algorithm\nmight not be useable in all applications because it can generate T-intersections in a\nresulting polygon mesh. To address this problem, the algorithm could be extended to\nwork on mesh data with connectivity information—for example, using a half-edge\n\n\ndata structure. But this extension is outside of the scope of this chapter and is not pre-\nsented here—this algorithm is demonstrated only as a proof-of-concept for integer\noperations using RP2 space.\nExtension into the Third Dimension\nThe previous discussion introduced the use of projective space for efficient and error-\nfree computational geometry operations in two-dimensional space. The extension to\nthe third dimension, critical for most of today’s games, is straightforward, but it\nrequires using four-dimensional vectors and operations on them. The CD-ROM con-\ntains a document providing a discussion similar to the one you’ve read here, for three-\ndimensional space and RP3 projective space.\nConclusion\nMany geometric algorithms suffer from rounding and loss of precision due to used\nnumerical representation, which may lead to wrong behavior of such algorithms in\nthe case of nearly collinear or coplanar points in input data. This is especially true for\nconstructive solid geometry (CSG) algorithms, because collinear and coplanar cases\npresent a serious problem in many implementations. The presented operations in\nprojective space allow implementation of most of these algorithms in such a way that\nthey can be proven to operate correctly for all cases of valid input data. This is done at\nthe cost of additional computational power required for operations on large integers,\nbut resulting robustness of the algorithms based on operations in projective space may\nbe worth its cost. On the other hand, as 64-bit processors and their new instruction\nsets extensions will become more and more common, the extra computational cost of\nsuch algorithms doesn’t have to be very large if long integer math is implemented in\nan efficient way.\nRobust CSG algorithms used in a digital content creation tool or in-engine level\neditor will give artists and “mod” developers more freedom and will save development\ntime when they would otherwise have to find workarounds where other CSG algo-\nrithms failed. Also, the robustness of a CSG algorithm can be critical when such an\nalgorithm is used in the game itself, allowing the player to interact with the environment\nin every imaginable way without concern that a CSG failure will result in a catastrophic\ngameplay bug.\nReferences\n[Hollash91] Hollasch, Steven R. “2.1 Vector Operations and Points in Four-\nDimensional Space,” in “Four-Space Visualization of 4D Objects,” Chapter 2,\navailable online at http://steve.hollasch.net/thesis/chapter2.html.\n[Young02] Young, Thomas. “Using Vector Fractions for Exact Geometry,” Game Pro-\ngramming Gems 3, pp. 160–169.\n164\nSection 2\nMath and Physics\n\n\n165\n2.5\nXenoCollide: Complex\nCollision Made Simple\nGary Snethen, Crystal Dynamics\ngary@snethen.com\nT\nhis gem introduces a new collision algorithm that works on a limitless variety of\nconvex shapes, including boxes, spheres, ellipsoids, capsules, cylinders, cones,\npyramids, frustums, prisms, pie slices, convex polytopes, and many other common\nshapes. The algorithm detects overlap and can also provide contact normals, points of\ncontact, and penetration depths for rigid body dynamics. A working implementation,\nalong with a simple rigid body simulator, is provided on the CD-ROM for immedi-\nate use and experimentation.\nThe algorithm is simple and geometric in nature, so it’s easy to visualize and\ndebug. The algorithm reduces to a series of point-plane clipping checks, so the math\ncan be understood by anyone familiar with dot products and cross products. \nIntroduction\nCreating a robust collision system can require a great deal of time and effort. The most\ncommon approach is to choose a handful of basic primitives and create O(N 2) separate\ncollision routines, one for each possible pair of primitives. Using this approach, the\namount of coding, testing, and debugging can quickly balloon out of control. Even a\nsmall set of four simple primitives, such as spheres, boxes, capsules, and triangles will\nrequire 10 separate collision routines. Each of these routines will have special cases and\nfailure modes that need to be tested and debugged. Each time an additional collision\nprimitive is added, multiple new collision routines need to be created, one for each pre-\nexisting primitive plus an additional routine to collide the new primitive with itself.\nThis gem introduces an efficient and compact collision algorithm that works on\nevery convex shape commonly found in real-time collision systems. New shapes can\nbe quickly and easily introduced without changing the algorithm’s implementation.\nAll that’s required to add a new shape is a simple mathematical description of the\nshape. If desired, collision shapes can be inexpensively modified in real-time for use\non animated objects.\n\n\nThe algorithm is named XenoCollide. It is an example of a broader class of algo-\nrithms based on a technique called Minkowski Portal Refinement (MPR).\nXenoCollide and the MPR technique presented in this gem are novel, but they\nshare important similarities to the GJK collision detection algorithm introduced by\n[GJK88]. The differences are outlined in the section entitled “Comparison of MPR\nand GJK.”\nThis gem proceeds by introducing support mappings and Minkowski differences.\nIf you are already comfortable with these concepts, you can skip ahead to the section\nentitled “Detecting Collision Using Minkowski Portal Refinement.”\nRepresenting Shapes with Support Mappings\nAlgorithms that work on a large number of shapes need a uniform way to represent\nthose shapes. XenoCollide relies on support mappings to fill this role. Support map-\npings provide a simple and elegant way to represent a limitless variety of convex shapes.\nA support mapping is a mathematical function (often a very simple one) that takes a\ndirection vector as input and returns the point on a convex shape that lies farthest in\nthat direction. (Support mappings are frequently defined as returning the point farthest\nin the direction opposite the normal. I’ve chosen the opposite convention to avoid an\nexcessive number of confusing negations in the equations.) If multiple points satisfy the\nrequirement, any one of the points can be chosen, so long as the same point is always\nreturned for any given input.\nSupport mappings are intuitive and easy to visualize. Imagine that you are given\nthe normal of a plane. If you slide this plane toward a convex shape along the plane’s\nnormal, the plane and the shape will eventually touch, as illustrated in Figure 2.5.1.\n166\nSection 2\nMath and Physics\nFIGURE 2.5.1\nVisualization of a support mapping as a moving plane.\n\n\nAt the moment they touch, the shape is being supported by the plane and the\npoint on the shape that is touching the plane is the support point for that plane.\nIf an entire edge or face touches the plane, any one of the points on the edge or\nface can be chosen as the support point for that normal, as shown in Figure 2.5.2.\n2.5\nXenoCollide: Complex Collision Made Simple\n167\nFIGURE 2.5.2\nChoosing a support point\nwhen an entire edge supports the plane.\nBasic Shapes\nMany common shapes have simple support mappings. Consider a sphere of radius r,\ncentered at the origin. If you slide a plane from any direction, n, the first point you\nwill encounter on the sphere will be in the direction n from the origin at a distance r,\nas illustrated in Figure 2.5.3. \nFIGURE 2.5.3\nSupport mapping of a sphere.\nWritten as a function, the support mapping for the sphere is as follows:\nSsphere(n) = rn\n(2.5.1)\n\n\nTable 2.5.1 lists the support mappings for several other common shapes.\nTable 2.5.1\nSupport Mappings for Simple Basic Shapes\nShape\nDescription\nSupport Mapping\nPoint\np\nSegment\nRectangle\nBox\nDisc\nSphere\nEllipse\nEllipsoid\nTranslating and Rotating Support Mappings\nThe support mappings in Table 2.5.1 represent shapes that are axis-aligned and cen-\ntered at the origin. To support shapes in world space, you need to rotate and translate\nsupport mappings.\nThe support mapping for a rotated and translated object can be found by first\ntransforming n into the object’s local space, and then finding the support point in local\nspace, and finally transforming the resulting support point back into world space:\n(2.5.2)\nFor the remainder of this gem, all support mappings are in world coordinates and\naccount for the rotation and translation of their respective shapes.\nS\nn\nRS\nR n\nT\nworld\nlocal\n( )\n(\n)\n=\n+\n−1\nr n\nr n\nr n\nr n\nr n\nr n\nx\nx\ny\ny\nz\nz\nx\nx\ny\ny\nz\nz\n2\n2\n2\n⎡⎣\n⎤⎦\n⎡⎣\n⎤⎦\nr n\nr n\nr n\nr n\nx\nx\ny\ny\nx\nx\ny\ny\n2\n2\n0\n0\n⎡⎣\n⎤⎦\n⎡⎣\n⎤⎦\nr n\nr\nn\nn\nn\nn\nx\ny\nx\ny\n0\n0\n⎡⎣\n⎤⎦\n⎡⎣\n⎤⎦\nr\nn\nr\nn\nr\nn\nx\nx\ny\ny\nz\nz\nsgn\nsgn\nsgn\n⎡⎣\n⎤⎦\nr\nn\nx\nx\nsgn\n0\n0\n⎡⎣\n⎤⎦\nr\nn\nx\nx\nsgn\n0\n0\n⎡⎣\n⎤⎦\n168\nSection 2\nMath and Physics\n\n\nCompound Support Mappings\nSupport mappings provide an efficient and compact way to represent basic shapes.\nThey can also be used to represent more complex shapes. This is done by mathemati-\ncally combining the support mappings of two or more simple primitives.\nYou can “shrink-wrap” a set of shapes by finding the support points for each\nshape and returning the point that is farthest along the direction vector. For example,\na disc centered at the origin can be shrink-wrapped with a translated point to create a\ncone, as given by Equation 2.5.3.\n(2.5.3)\nTo simplify the appearance of compound support mappings, drop the (n) from\nthe function names. Every support mapping requires a normal, so you can assume it’s\nalways present:\n(2.5.4)\nA second way to combine support mappings is to add them together. This results\nin one shape being “inflated by” or “swept about” the other. For example, if you add\nthe support mapping of a small sphere to the support mapping of a large box, you’ll\nget a larger box with rounded corners, as given by Equation 2.5.5:\n(2.5.5)\nSome useful combinations of support mappings are listed in Table 2.5.2.\nTable 2.5.2\nSupport Mappings for Compound Shapes\nShape\nDescription\nSupport Mapping\nCapsule\nmaxsupport(Ssphere,Ssphere + [length 0  0])\nor Sedge + Ssphere\nLozenge\nSrectangle + Ssphere\nRounded box\nSbox + Ssphere\nCylinder\nmaxsupport(Sdisc,Sdisc + [0  0 height])\nor Sedge + Sdisc\n→\nS\nS\nS\nsmoothbox =\n+\nbox\nsphere\nS\nmaxsupport S\nS\ncone =\n(\n)\npoint\ndisc\n,\nS\nn\nmaxsupport S\nn S\nn\ncone( )\n( ),\n( )\n=\n(\n)\npoint\ndisc\n2.5\nXenoCollide: Complex Collision Made Simple\n169\n\n\nTable 2.5.2\n(Continued)\nShape\nDescription\nSupport Mapping\nCone\nmaxsupport(Sdisc,Spoint + [0  0 height])\nWheel\nSdisc + Ssphere\nFrustum\nmaxsupport(Srectangle1,Srectangle2 + [0  0 height])\nPolygon\nPolyhedron\nmaxsupport(Svert1,Svert2,...)\nExtremely complex shapes can be easily created by algebraically combining many\nbasic and compound shapes. These algebraic operations can be turned into intuitive\ncontrols for content creators as well. Artists and designers can use the basic shapes to\nsketch out the major external features of an object and then use interactive shrink-\nwrapping and sweeping/extruding to handle the rest.\nSimplifying Collision Detection Using Minkowski Differences\nEvery convex shape can be treated as a convex set of points in world space. Something\ninteresting happens if you subtract every point in one solid shape from every point in\na second solid shape. If the two shapes overlap, there will be at least one point within\nshape A that shares the same position in world space as a point within shape B. \nWhen one of these points is subtracted from the other, the result will be the zero\nvector (that is, the origin). Similarly, if the two shapes do not overlap, no point from\nthe first shape will be equal to any point in the second shape and the new shape will\nnot contain the origin.\nTherefore, if you can detect whether the origin is in the new shape, you have\ndetected whether or not the two original shapes are colliding.\nThe shape that’s formed by the subtraction of one convex shape from another is\ncalled the Minkowski difference of the two shapes. The Minkowski difference, B–A, is\nalso a convex shape. If B–A contains the origin, A and B must overlap. If B–A does\nnot contain the origin, A and B are disjoint.\nIt would be prohibitively expensive to actually subtract every point in one shape\nfrom every point in the other. However, you can easily determine the support map-\nping of the Minkowski difference B–A if you’re given the support mappings of A and\nB. Using Equation 2.5.6, you can reduce the problem of detecting collision between\n170\nSection 2\nMath and Physics\n\n\ntwo convex shapes, A and B to that of determining whether the origin lies within a\nsingle convex shape, B–A.\n(2.5.6)\nDetecting Collision Using Minkowski Portal Refinement\nUp to this point, everything described applies equally well to GJK and XenoCollide. For\nthe remainder of this gem, the discussion will focus on XenoCollide and the MPR tech-\nnique. If you’re interested in reading more about GJK, [van den Bergen03], [GJK88],\nand [Cameron97] are excellent references. If you want additional background and\ndetails on XenoCollide, please see [Snethen07].\nHere’s the pseudocode for XenoCollide:\n// Phase 1: Portal Discovery\nfind_origin_ray();\nfind_candidate_portal();\nwhile ( origin ray does not intersect candidate )\n{\nchoose_new_candidate();\n}\n// Phase 2: Portal Refinement\nwhile (true)\n{\nif (origin inside portal) return hit;\nfind_support_in_direction_of_portal();\nif (origin outside support plane) return miss;\nif (support plane close to portal) return miss;\nchoose_new_portal();\n}\nEach step is described in detail next.\nThe find_origin_ray(); Step\nStart by finding a point known to be in the interior of the Minkowski difference B–A.\nSuch a point can be easily obtained by taking a point known to be inside B and sub-\ntracting a point known to be inside A. The geometric center (or center of mass) is a\nconvenient point to use. However, any deep interior point will work. The point that\nresults from the subtraction is the interior point of B–A. The interior point is labeled\nV0 in Figure 2.5.4.\nThe interior point is important because it lies on the inside of B–A. If the ray\ndrawn from the interior point through the origin, called the origin ray, passes through\nthe surface of B–A before it encounters the origin, the origin lies outside of B–A.\nConversely, if the ray passes through the origin before the surface, the origin is inside\nof B–A.\nS\nn\nS\nn\nS\nn\nB-A ( ) =\n( )−\n−(\n)\nB\nA\n2.5\nXenoCollide: Complex Collision Made Simple\n171\n\n\nThe find_candidate_portal(); Step\nThis step of the algorithm uses the support mapping of B–A to find three non-collinear\npoints on the surface of B–A that form a triangular portal through which the origin ray\nmay (or may not) pass. See Figure 2.5.5.\nThere are many ways to obtain three non-collinear surface points. XenoCollide\nuses the following support points:\n// Find support in the direction of the origin ray\nV1 = S( normalize(-V0) );\n// Find support perpendicular to plane containing\n// origin, interior point, and first support\nV2 = S( normalize(V1 x V0) );\n// Find support perpendicular to plane containing\n// interior point and first two supports\nV3 = S( normalize((V2-V0) x (V1-V0)) );\nThe while ( origin ray does not intersect candidate ) Step\nYou now test the candidate triangle to determine whether the origin ray intersects it.\nYou do this by testing whether the origin lies on the inside of each of the three planes\nformed by the triangle edges and the interior point—(v0,v1,v2); (v0, v2, v3); and\n(v0,v3,v1). If the origin lies within all three planes, you’ve found a valid portal and\ncan move on to the next step. If not, you need to choose a new portal candidate \nand try again.\n172\nSection 2\nMath and Physics\nFIGURE 2.5.4\nStep 1 involves finding the origin ray.\n\n\nThe choose_new_candidate(); Step\nIf the origin lies outside one of the planes, use that plane’s outer-facing normal to find\na new support point, as illustrated in Figure 2.5.6.\n2.5\nXenoCollide: Complex Collision Made Simple\n173\nFIGURE 2.5.5\nStep 2 involves finding a candidate portal.\nFIGURE 2.5.6\nStep 3 involves finding a new candidate portal.\n\n\nThis new support point is used to replace the triangle vertex that lies on the inside\nof the plane. The resulting support points provide you with a new portal candidate\n(see Figure 2.5.7); repeat the loop until you obtain a hit. \n174\nSection 2\nMath and Physics\nFIGURE 2.5.7\nStep 4 involves finding a valid portal.\nThe if (origin inside portal) return hit; Step\nThe points V0, V1, V2, and V3 form a tetrahedron. Due to the convexity of B–A,\nthis entire tetrahedron lies inside B–A. If the origin lies within the tetrahedron, it\nmust also lie within B–A. You know that the origin lies within three of these faces,\nbecause the origin ray starts at V0 and passes through the portal, which forms the\nopposite side of the tetrahedron. If the origin lies within the portal, it lies within the\ntetrahedron and therefore lies within B–A. In this case, you return with a hit.\nThe find_support_in_direction_of_portal(); Step\nIf you make it here, the origin lies on the far side of the portal. However, you don’t\nknow whether it lies within B–A nearby on the outside of the portal or whether it lies\ncompletely outside of B–A. You need more information about what lies on the far side\nof the portal, so you should use the exterior facing normal of the portal to obtain a\nnew support point that lies outside of the portal’s plane. See Figure 2.5.8.\nThe if (origin outside support plane) return miss; Step\nIf the origin lies outside of the new support plane formed by the support normal and\nthe new support point, the origin lies outside B–A and the algorithm reports a miss.\n\n\nThe choose_new_portal(); Step\nThe origin lies between the portal and the support plane, so you need to refine your\nsearch by finding a new portal that is closer to the surface of B–A. Consider the tetra-\nhedron formed by the support point and the portal.\nThe origin passes into the tetrahedron through the portal and is therefore guaran-\nteed to exit the tetrahedron through one of the three outer faces formed by the sup-\nport point and the three edges of the portal. This step determines which of the three\nouter faces the ray passes through and replaces the old portal with this new portal. To\ndetermine which of the three outer faces the origin ray passes through, you test the\norigin against the three planes—(V4, V0, V1); (V4, V0, V2); and (V4, V0, V3).\nThe origin will lie on the same side of two of these planes. The face that borders\nthese two planes becomes the new portal, as illustrated in Figure 2.5.9.\nThe if (support plane close to portal) return miss; Step\nAs the algorithm iterates, the refined portals will rapidly approach the surface of B–A;\nhowever, if B–A has a curved surface, the origin may lie infinitesimally close to this\ncurved surface. In this case, the refined portals may require an arbitrary number of\niterations to pass the origin. To terminate under these conditions, you have to rely on\na tolerance. When the portal gets sufficiently close to the surface (as measured by the\ndistance between the portal and its parallel support plane), you terminate the algo-\nrithm. You can terminate with a hit or a miss, depending on whether you prefer to err\non the side of imprecise positive or negative results.\n2.5\nXenoCollide: Complex Collision Made Simple\n175\nFIGURE 2.5.8\nStep 5 involves finding a new support point\nin direction of portal.\n\n\nFor physical simulation, it’s generally better to err on the side of a false negative\n(that is, returning a miss even though the point may be slightly below the surface).\nVery slight penetration won’t be noticed, but any visible gap at the contact point\nwould appear unnatural.\nTermination\nThe algorithm will continue running until one of the following conditions is met:\n• The origin lies on the inside of a portal (hit)\n• The origin lies on the outside of a support plane (miss)\n• The distance between the portal and its parallel support plane drops below a small\ntolerance (close call—can be treated as a hit or miss depending on the application)\nA formal proof of termination is beyond the scope of this gem. However, an infor-\nmal proof can be found in [Snethen07].\nUsing MPR for Contact Information\nIf you need only to detect overlap, MPR can be terminated as soon as the portal passes\nthe origin. However, if your application requires contact information, MPR can con-\ntinue executing to discover a contact point, contact normal, and penetration depth.\nMPR offers several possible ways of acquiring contact information. The tech-\nnique employed by XenoCollide is to simply continue projecting the origin ray out to\nthe surface of the Minkowski difference. This represents pushing the objects away\nfrom each other along the line connecting their interior points until they are just\ntouching and then using the normal of this first contact as the collision normal. This\nis efficient and simple, and it results in stable contact information.\n176\nSection 2\nMath and Physics\nFIGURE 2.5.9\nStep 6 involves establishing the new portal.\n",
      "page_number": 194,
      "chapter_number": 21,
      "summary": "This chapter covers segment 21 (pages 194-209). Key topics include point, supported, and polygons. The range estimates given in Table 2.4.1 can be useful during\nimplementation of used long integer math, because these estimates can be used to\nfind required range of integers according to input data point coordinate range.",
      "keywords": [
        "support mappings",
        "support",
        "point",
        "support point",
        "origin",
        "Portal",
        "origin lies",
        "Collision Made Simple",
        "polygon",
        "shapes",
        "support plane",
        "algorithm",
        "Complex Collision Made",
        "step",
        "lies"
      ],
      "concepts": [
        "point",
        "supported",
        "polygons",
        "algorithms",
        "shapes",
        "collision",
        "step",
        "operations",
        "operation",
        "operating"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 40,
          "title": "Segment 40 (pages 381-388)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 19,
          "title": "Segment 19 (pages 173-181)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 17,
          "title": "Segment 17 (pages 166-173)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 41,
          "title": "Segment 41 (pages 389-403)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 32,
          "title": "Segment 32 (pages 306-318)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 210-218)",
      "start_page": 210,
      "end_page": 218,
      "detection_method": "topic_boundary",
      "content": "A second choice is to find the relative velocity of a pair of overlapping points, one\nfrom each object, and then project a new ray from the origin to the surface of B–A\nalong the negative direction of the velocity vector. This results in a calculation of pen-\netration along the direction of motion, which may result in more realistic dynamic\ncollisions.\nAdditional Optimizations\nXenoCollide exhibits good performance. However, highly optimized routines that\ntarget specific pairs of shapes will inevitably be faster than any general-purpose rou-\ntine. Readers interested in pursuing ideal performance can begin with XenoCollide as\na foundation to handle all shapes and then add special-case routines for handling the\npairs of shapes that will benefit the most from optimization later in development.\nOne obvious candidate for optimization is a sphere-sphere check, which has minimal\ncost when implemented as a special case.\nAnother important optimization is caching the results of each collision test to\nbootstrap the same check in the next timestep. If a separating support plane is found\nthat proves that two objects do not collide, this same separating plane can be used as\nan early-out separation test in subsequent frames. Likewise, if a portal is found that\nlies outside the origin, this same portal can often be used to quickly verify that the\nobjects are still in contact.\nThe support mapping functions are called multiple times during each collision\ncheck. To maximize performance, transform, normalization, and function call over-\nhead should be kept to a minimum. In some cases, it may be better to perform the\nsupport mapping evaluation in world space. For example, it’s generally less expensive\nto check a segment using dot(n, d) in world space than it is to transform n to local\nspace, test only the x component, and then transform the result back again.\nComparison of MPR and GJK\nGJK was one of the inspirations for MPR. GJK also supports a limitless variety of\nconvex shapes, but it suffers from several limitations that MPR attempts to correct:\n• The simplex refinement algorithm in GJK is based on an algebraic formulation\nthat isn’t intuitive to most game programmers. This formulation relies on deter-\nminants and cofactors, which are notoriously sensitive to floating point precision\nproblems. The combination of precision issues and hard-to-visualize mathematics\nmakes it difficult for many game developers to implement GJK robustly. \n• As a result of the previous issue, many variations on GJK have been created that\nattempt to frame GJK as a geometric problem rather than an algebraic one. How-\never, every approach to GJK requires considering the Voronoi regions of 8 to 15\nfeatures (points, edges, faces, and interior) of a tetrahedron, to see which is clos-\nest to the origin. This can be a complex and potentially expensive task due to the\nmany different conditions and branching operations.\n2.5\nXenoCollide: Complex Collision Made Simple\n177\n\n\n• In the general case, GJK doesn’t provide an accurate contact normal, penetration\ndepth, or point of surface contact. A separate algorithm, such as EPA [van den\nBergen03], is required to obtain this information.\nMPR addresses each of these issues:\n• MPR is simple and geometric. Each step of the technique can be easily visualized\nand verified on-screen, which makes it easier to understand, test, and debug.\n• MPR requires fewer branching tests. Instead of choosing among 8 to 15 separate\nfeatures, only 2 or 3 need to be evaluated.\n• MPR can be used both to detect collision and to identify collision details that are\nwell-behaved and consistent from frame to frame.\nConclusion\nThis gem introduced a simple algorithm for detecting collision among shapes chosen\nfrom a limitless pool of useful convex designs. Introducing new shapes is extremely\neasy and can be wrapped in a graphical user interface for artists and designers. The\nalgorithm provides a robust foundation for a general purpose collision system for\ngameplay and rigid body dynamics. The algorithm is efficient; however, if additional\nperformance is ever required, optimized pair-specific routines can be layered on the\ngeneric framework as needed.\nAcknowledgements\nThe author wishes to thank the managers, programmers, and developers of Crystal\nDynamics for their support and review of this gem. The author would also like to\nthank Erin Catto, for his friendship and encouragement to share this work.\nReferences\n[Cameron97] Cameron, S. “Enhancing GJK: Computing Minimum and Penetration\nDistances Between Convex Polyhedra,” Proceedings of IEEE International Confer-\nence on Robotics and Automation (1997), pp. 3112–3117.\n[GJK88] Gilbert, E.G., Johnson, D.W., and Keerthi, S.S. “A Fast Procedure for Com-\nputing the Distance Between Complex Objects in Three-Dimensional Space,”\nIEEE Journal of Robotics and Automation, Vol. 4, No. 2 (1988), pp. 193–203.\n[Snethen07] Snethen, Gary. “XenoCollide Website,” available online at http://\nwww.xenocollide.com, September, 2007.\n[van den Bergen03] van den Bergen, Gino. Collision Detection in Interactive 3D Envi-\nronments, Morgan Kauffman, 2003.\n178\nSection 2\nMath and Physics\n\n\n179\n2.6\nEfficient Collision Detection\nUsing Transformation\nSemantics\nJosé Gilvan Rodrigues Maia, UFC\ngilvanmaia@gmail.com\nCreto Augusto Vidal, UFC\ncvidal@lia.ufc.br\nJoaquim Bento Cavalcante-Neto, UFC\njoaquimb@lia.ufc.br\nH\now does a first-person shooter game determine whether a shot hit the head of an\nenemy? How can you avoid game objects passing through walls or falling to infin-\nity? How do you check whether the player touched an item or reached a checkpoint?\nHow does an engine determine intersections for a dynamics simulation? Although\nthere are many possible ways for solving these problems, collision detection (CD) is a\nvery important tool for dealing with such situations. CD allows for checking whether\na given set of geometric entities are overlapping. Because of this, it plays an essential\nrole in almost any computer game.\nCurrent rendering technology provides support for placing geometry in a scene\nby means of transformation matrices. Therefore, it is important for a game program-\nmer to know how to construct and manipulate these matrices. Moreover, collision\ndetection methods must consider not only the shape of objects but also their corre-\nsponding matrices in order to carry out intersection tests.\nThis gem shows you a method for inverting transformation matrices used for\nplacing models in games and for extracting useful semantic information from them. It\nalso explains how this information can be used for implementing efficient collision\ndetection tasks.\n\n\nAffine Transforms and Games\nThis section briefly discusses a few concepts of linear algebra. Affine transforms are\nused to map between two vector spaces by means of a linear transformation (that is, a\nmatrix multiplication) followed by an origin shift vector that is added to the result. An\naffine transform (also called affine mapping) is defined as follows:\n(2.6.1)\nAffine mappings are used in many computer graphics applications. Vertex trans-\nformation from world space to camera space in a transformation pipeline, for exam-\nple, is implemented using a change of basis—a particular type of affine mapping.\nBecause computer graphics systems implement linear transformations using 4 \u0002 4\nmatrices, we define the mapping in Equation 2.6.1 using block matrix form:\n(2.6.2)\nObserve that in Equation 2.6.2 the affine mapping, A, is a 3 \u0002 3 matrix and the\norigin shift, t, is a 3 \u0002 1 (column) matrix. Except for the last item, all components in \nthe fourth row are defined as zero. For computer games, it can be assumed that input\nvertices to be transformed have their homogeneous w component set to 1 before the\nmatrix multiplication is carried out. Three-dimensional geometry is specified using ordi-\nnary Cartesian coordinates, and vertices are then processed in homogeneous coordinates. \nFrom the game programming perspective, the problem is to detect collisions\nbetween the models in a scene. Recall that each model has a corresponding matrix\nthat places it in world coordinates; hence, this matrix must be considered for collision\ndetection against a given model.\nObserve that typical matrices placing models in world space are affine mappings\nmatching Equation 2.6.2, because these matrices usually arise from a combination of\nbasic transformations, each in itself an affine mapping—scaling, rotation, and trans-\nlation. These basic transformations can be used to “explain” matrices and answer some\ncommon questions about the placement of a model. What is its size? Which direction\nis the model facing? Where is the model’s origin? Because of this, scaling, rotation,\nand translation are called transformation semantics.\nSemantics can be used to simplify and speed up computations involved in the\ncollision detection process. The next section presents an efficient decomposition\nmethod for both inverting and obtaining semantics from a transformation matrix.\nThis answers the following questions—given a 4 \u0002 4 array representing an affine\nmapping, what is its inverse, and what are its corresponding scaling, rotation, and\ntranslation parts?\nM\nA\nt\n0\n=\n⎡\n⎣⎢\n⎤\n⎦⎥\n1\nq\nAp\nt\n=\n+\n180\nSection 2\nMath and Physics\n\n\nExtracting Semantics from Matrices\nSome semantics information can be obtained in the general case, as shown briefly\nnext. However, in game development, you are not usually interested in a method that\ndecomposes an arbitrary affine mapping. Instead, most of this gem focuses on matri-\nces such as those described previously, used to place models in a typical game. This\nenables you to apply a more efficient method.\nThe General Affine Mapping Case\nCan you extract some semantics from any affine mapping? Yes, you can. As shown in\nEquation 2.6.2, t represents an origin shift. Hence, the translation part is available for\nfree—you just get it from the last column of the given matrix. Some orientation infor-\nmation can also be extracted easily. Take a closer look at matrix multiplication over a\ngiven vertex:\n(2.6.3)\nGrouping Equation 2.6.3 by each axis-aligned component from p, you obtain:\n(2.6.4)\nUsing Equation 2.6.4, it is clear that the first, second, and third columns from A\nrepresent the new x, y, and z axes, respectively, after a point gets transformed. This\nmeans you can use columns from your affine transform for solving problems consider-\ning orientation and scale—a convenient example is detailed later. Although Equation\n2.6.4 provides a clear view about how the orientation of vertices is modified by matri-\nces, only translation and orientation semantics are obtained through this analysis. \nThe inverse of an affine matrix is well-known, and can be written as:\n(2.6.5)\nReaders interested in inverting a general affine mapping Equation 2.6.2 are\ninvited to take a look at Kevin Wu’s article about this matter [Wu91].\nM\nA\nt\n0\nA\nA t\n0\n−\n−\n−\n−\n=\n⎡\n⎣⎢\n⎤\n⎦⎥\n=\n−\n⎡\n⎣\n⎢\n⎤\n⎦\n⎥\n1\n1\n1\n1\n1\n1\nAp =\n⎡\n⎣\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n+\n⎡\n⎣\n⎢\n⎢\n⎢\n⎤\n⎦\np\nA\nA\nA\np\nA\nA\nA\nx\ny\n11\n21\n31\n12\n22\n32\n⎥\n⎥\n⎥\n+\n⎡\n⎣\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\np\nA\nA\nA\nz\n13\n23\n33\nAp =\n⎡\n⎣\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\nA\nA\nA\nA\nA\nA\nA\nA\nA\np\np\nx\n11\n12\n13\n21\n22\n23\n31\n32\n33\ny\nz\nx\ny\nz\nx\ny\np\np A\np A\np A\np A\np A\n⎡\n⎣\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n=\n+\n+\n+\n11\n12\n13\n21\n22 +\n+\n+\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\np A\np A\np A\np A\nz\nx\ny\nz\n23\n31\n32\n33\n2.6\nEfficient Collision Detection Using Transformation Semantics\n181\n\n\nOur Specific Case: Model Matrices\nConsider a transformation matrix M that transforms a model from its local space into\nworld coordinates. You can assume M results from a composition of a non-uniform\nscaling matrix, followed by as many rotations and translations as needed, in this order.\nThis assumption is reasonable because such form is compatible with most scene rep-\nresentations, such as scene graphs. For convenience, M can be written as the product\nof just three matrices—S (scaling), R (rotation), and T (translation):\n(2.6.6)\nHere, each Mi represents rotations or translations.\nKevin Wu [Wu94] used an equivalent matrix form in his gem, and described a\nsimple and efficient method for inverting either matrices that preserve angles between\nvectors or matrices that preserve vector lengths. However, his method only considers\nuniform scaling. Our matrix representation is more general (considers non-uniform\nscales) and can be rewritten using a block matrix format:\n(2.6.7)\nSome details about this representation must be observed: t represents the transla-\ntion part; i, j, and k are column matrices that form an orthonormal basis; and sx, sy,\nand sz are non-zero (otherwise M is singular) scaling factors. As M results from a prod-\nuct of basic transforms, its inverse is known and can be written as:\n(2.6.8)\nNow, let’s see how a matrix in this form can be inverted without computing any\nsquare roots. Because i is a unit vector, the squared scale factor in the x axis can be\ncomputed by taking the dot product of the first column with itself:\n(2.6.9)\ns\ns\ns\ns\ns\nx\nx\nx\nx\nx\ni\ni\ni\ni\n(\n)⋅(\n) =\n⋅\n(\n) =\n( ) =\n2\n2\n2\n1\nM\nTRS\nS R T\ni\ni t\nj\nj t\nk\nT\nT\n−\n−\n−\n−\n−\n=(\n)\n=\n=\n−⋅\n−⋅\n1\n1\n1\n1\n1\ns\ns\ns\ns\nx\nx\ny\ny\nT\nk t\n0\n1\ns\ns\nz\nz\n−\n⋅\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\n⎥\nM\nTRS\nI\nt\n0\ni\nj\nk\n0\n=\n=\n⎡\n⎣⎢\n⎤\n⎦⎥\n⎡\n⎣⎢\n⎤\n⎦⎥\n1 0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\ns\ns\ns\nx\ny\nz\nx\ny\nz\ns\ns\ns\n0\n0\n0\n0\n1\n0\n0\n0\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\n⎥\n=\n⎡\n⎣\n⎢\n⎢\n⎤\n⎦\n⎥\ni\nj\nk\nt\n1⎥\nM\nM\nM S\nTRS\n=\n=\n1K\nk\n182\nSection 2\nMath and Physics\n\n\nThis also holds for the second and third columns\n(2.6.10)\n(2.6.11)\nThis property becomes quite useful, because the squared scale factor can be used\nto compute the first row of the 3 \u0002 3 part of M-1 (Equation 2.6.8) by simple division:\n(2.6.12)\nKnowing that, you can compute the inverse as follows: compute the squared scale\nfactors using Equations 2.6.9, 2.6.10, and 2.6.11; transpose the 3 \u0002 3 submatrix A of\nEquation 2.6.2; and divide its resulting rows by the corresponding squared scale factors.\nObserve that, at this point, you have computed A-1. Finally, the product –A-1t corre-\nsponding to the last column is easily computed.\nThis inversion method offers the flexibility of supporting non-uniform scale, and\nit is quite efficient. Only 27 multiplications, three divisions, and 12 additions are\nrequired. Table 2.6.1 provides a comparison of this method with the brute-force\nmethods (Cofactors and Gaussian Elimination with pivoting), with Wu’s method for\naffine mappings [Wu91], and with Wu’s method for angle-preserving matrices\n[Wu94], which only supports uniform scaling. In Table 2.6.1, only a “raw” C imple-\nmentation is considered. Readers are encouraged to implement these methods using\nSIMD/MIMD instructions—SSE, SSE2, and so on.\nTable 2.6.1\nComparison of Matrix Inversion Methods in Terms of Required Operations\nInversion Method\nDivisions\nMultiplications\nAdditions\n[Wu94] (angle-preserving)\n1\n21\n8\nOur method\n3\n27\n12\n[Wu91] (general case)\n1\n48\n22\nCofactors\n1\n280\n101\nGaussian elimination with partial pivoting\n10\n51\n47\nTransformation semantics extraction requires a slight modification of the method\npresented here. As shown in Equation 2.6.2, the translation part comes for free.\nSquare root computation over Equations 2.6.9, 2.6.10, and 2.6.11 gives you the scale\nfactors. The rotation part can be obtained dividing the first three columns by the cor-\nresponding scale factors. Therefore, semantics extraction demands three square roots,\nthree divisions, 18 multiplications, and six additions.\ns\ns\ns\nx\nx\nT\nT\nx\ni\ni\n2\n⎛\n⎝\n⎜⎜\n⎞\n⎠\n⎟⎟=\ns\ns\ns\ns\ns\nz\nz\nz\nz\nz\nk\nk\nk\nk\n(\n)⋅(\n) =\n⋅\n(\n) =\n( ) =\n2\n2\n2\n1\ns\ns\ns\ns\ns\ny\ny\ny\ny\ny\nj\nj\nj\nj\n(\n)⋅(\n) =\n⋅\n(\n) =\n( ) =\n2\n2\n2\n1\n2.6\nEfficient Collision Detection Using Transformation Semantics\n183\n\n\nWhen only uniform scaling is considered, it is evident that the inversion method\nis reduced to Wu’s method for angle-preserving matrices [Wu94]. Moreover, seman-\ntics extraction in this case demands one square root, one division, 12 multiplications,\nand two additions.\nNow you can invert and extract semantics efficiently from a typical matrix that\nplaces models in a scene. The next section explains how semantics information can be\nused when performing collision detection tasks.\nUsing Semantics for Collision Detection Tasks\nCollision detection methods for real-world applications must consider a given set of\nmodels, and this work is usually split into two successive tasks. The first task, known as\nbroad phase or collision culling, is responsible for finding all object pairs that are in prox-\nimity—the potentially colliding set. More importantly, this phase aims at discarding\ndistant pairs that cannot collide, thus avoiding expensive intersection tests. The next\ntask, known as narrow phase or pair processing, consists of effective intersection tests\nover object pairs collected during the broad phase. Transformation semantics can be\nused in these collision detection tasks. \nSpeeding Up Broad Phase\nGiven N objects, potentially O(N2) pairs have to be checked for collision. Neverthe-\nless, it happens that objects in most pairs are not even close to each other, so, many\npairs can be discarded quickly—this explains why this phase is also known as collision\nculling. Spatial hashing and sweep-and-prune methods, among others, were specially\ndesigned for this purpose. Common implementations of these techniques require\nknowledge of the axis-aligned bounding box (AABB) tightly fitting each model in\nworld coordinates.\nScene representations usually maintain a local AABB per geometric model; for\nexample, an AABB in the model’s own local space. This box also gets transformed by\nthe model’s matrix, so that it becomes an oriented bounding box (OBB) in world\nspace. The problem at this point is the following: how can you efficiently compute the\nmodel’s global AABB from its local AABB and a transformation matrix M? A brute-\nforce approach transforms all eight corner vertices from the AABB first and then com-\nputes the AABB of the transformed vertices—this requires 21 branches. Avoiding not\nonly computations but also branches is very important for improved performance.\nAlthough modern CPUs can predict the behavior of code before it is executed,\nbranchless code may still run a bit faster.\nCharles Bloom proposed an elegant, efficient solution for this problem in his\ngame engine [Bloom06]. His approach is purely geometric and is based on the orien-\ntation semantics shown in Equation 2.6.4. Consider a min-max representation for\nAABBs. First, the minimum extreme vertex is transformed using the model’s matrix,\nas usual, obtaining a point p. Orientation semantics are used in order to obtain the\n184\nSection 2\nMath and Physics\n\n\nnew axes in world coordinates after the model gets transformed by M. These axes are\nthen multiplied by the respective box dimensions, obtaining the transformed box’s\nedge vectors. Each sign of each component of these edges is then checked.\nObserve that negative components “move” p towards the global AABB’s mini-\nmum extreme vertex, so this extreme point is obtained by adding all negative edge\ncomponents to p. Conversely, all positive edge components are added to p in order to\nobtain the maximum extreme vertex—this requires only nine branches. See Figure\n2.6.1.\n2.6\nEfficient Collision Detection Using Transformation Semantics\n185\nFIGURE 2.6.1\nA local AABB fitting a given model (a) is transformed into world coordinates\nthrough a matrix (b). Observe that p lies on the boundary of the global AABB that tightly\nfits the transformed AABB. The global box is obtained by adding the components of the\nedge vectors (c) to the minima point, moving it toward the global extreme vertices.\nIn a gem from the previous edition, Chris Lomont explained how to perform many\ntricks efficiently, using floating points [Lomont07]. His approaches can be used for sign\nbit extraction in order to come up with a branchless implementation of this method,\nwhich is provided on the CD-ROM. \nNarrow Phase\nThe final step for collision detection is to process all pairs reported from broad phase.\nThis task must consider two models A and B, as well as their respective transforma-\ntion matrices MA and MB. The following question must be answered: do the models\nintersect after they are transformed? A list of intersecting primitive pairs (that is, trian-\ngles) describing the contact surface is reported in case the models intersect.\nBounding volume hierarchies are well-suited for this problem because they pro-\nvide a multi-resolution representation of the models that is useful for discarding non-\nintersecting parts. AABB-trees are particularly useful for dealing with deformable\nmodels (characters, for example) because they are much cheaper to refit as geometry\ngets deformed [Bergen97].\n",
      "page_number": 210,
      "chapter_number": 22,
      "summary": "This chapter covers segment 22 (pages 210-218). Key topics include transform, transformations, and collisions. If a separating support plane is found\nthat proves that two objects do not collide, this same separating plane can be used as\nan early-out separation test in subsequent frames.",
      "keywords": [
        "Collision Detection",
        "Efficient Collision Detection",
        "collision",
        "collision detection tasks",
        "collision detection methods",
        "matrix",
        "Semantics",
        "transformation semantics",
        "affine mapping",
        "Equation",
        "Detection",
        "Affine",
        "model",
        "method",
        "Transformation"
      ],
      "concepts": [
        "transform",
        "transformations",
        "collisions",
        "matrix",
        "matrices",
        "efficient",
        "points",
        "game",
        "methods",
        "semantics"
      ],
      "similar_chapters": [
        {
          "book": "Game_Engine_Architecture",
          "chapter": 32,
          "title": "Segment 32 (pages 632-651)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 19,
          "title": "Segment 19 (pages 173-181)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 33,
          "title": "Segment 33 (pages 314-325)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 24,
          "title": "Segment 24 (pages 222-231)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 40,
          "title": "Segment 40 (pages 381-388)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 219-228)",
      "start_page": 219,
      "end_page": 228,
      "detection_method": "topic_boundary",
      "content": "Let’s define two useful matrices:\n(2.6.13)\n(2.6.14)\nMAB maps geometry from A’s local space into B’s local space. Conversely, MBA\nmaps geometry from B’s local space into A’s local space. These matrices can be obtained\nusing the presented inversion method followed by a matrix multiplication. Triangles\ncan be mapped from the local space of one object into that of another object, provid-\ning support for direct intersection tests.\nActually, transformation semantics provides flexibility for processing geometry\ncoming from a given space (A, B, or world spaces) in a common space where compu-\ntations can be carried out more comfortably. Using this approach, you can choose from\nseven coordinate systems for performing computations, as illustrated in Figure 2.6.2.\nM\nM M\nBA\nA\nB\n=\n−1\nM\nM M\nAB\nB\nA\n=\n−1\n186\nSection 2\nMath and Physics\nFIGURE 2.6.2\nSchematic view illustrating how transformation semantics can be used for\nprocessing geometry coming from different local spaces. \nIn Figure 2.6.2, arrows show how geometry coming from a given space (A, B, or\nworld) is transformed into a common space, which is more adequate for processing.\nFollowing this scheme, it can be seen that a mapping from B to A’ space corresponds\nto RA\n-1TA\n-1TBRBSB (transformations are ordered from right to left).\nThe box-box overlap test is also necessary in order to perform collision detection\nbetween a pair of AABB-trees. It is important to notice that occurrence of a non-\nintersecting box pair avoids many triangle-triangle tests because entire subtrees cannot\ncollide given that their bounding boxes are not overlapping. Moreover, the box-box\n\n\noverlap test also allows for computing collision detection between a model and an ori-\nented box placed in world coordinates. In a game, this can be used to determine whether\nthe player has touched an important item in order to trigger some AI, for example.\nThe box-box test is usually implemented using the Separating Axis Theorem\n[Gottschalk96], also known as SAT. This method allows for very early exits based on\niterative search for a separating axis, so that box projection intervals over that axis are\nnot overlapping. The given boxes are intersecting only when there is no separation\nalong any of the 15 potentially separating axes. \nBergen pointed out that about 60% of the separating axes found in SAT corre-\nspond to the normal of a box’s face [Bergen97]. Based on this, he adopted an approxi-\nmate intersection test between boxes (SAT-lite) that checks only this kind of separating\naxes. Although all separation cases cannot be handled, this test provides faster collision\nculling because only 6 of 15 axes are tested.\nUsing transformation semantics, the box-box test using SAT can be performed as\nfollows. First, scale each box using the scale semantics extracted from its respective\nmodel’s transformation matrix; after this, the intersection method can be carried out\nnormally considering only the rotation and translation semantics during the search\nfor a separating axis using either SAT or SAT-lite. Observe that this scale adjust causes\nthe test to be performed as boxes are coming from A’ and B’ spaces (see Figure 2.6.2).\nOf course, any box-box intersection method can benefit from this slight modification\nin order to provide support for scaling models. Moreover, only 12 additional multipli-\ncations are necessary, which is not excessive given that scaling is supported efficiently.\nIn this example, only AABB-trees are considered in collision tests. Actually, other\nuseful intersection tests can be performed against a model using volumes, rays, or\nlines placed in world space. Examples of volumes are sphere, cone, AABB, OBB, and\ncapsule (also called a line-swept sphere). \nBasically, support for testing a given primitive against an AABB-tree requires two\nintersection methods. The first method checks whether the primitive intersects a box\ncoming from the transformed AABB-tree, providing means for fast collision culling of\nsubtrees. Finally, the second method checks overlap between the primitive and a trian-\ngle from the model. This second method can be implemented by transforming trian-\ngles using the model’s matrix before carrying out the intersection test.\nAs shown in Figure 2.6.2, transformation semantics extraction allows for choosing\none of seven coordinate systems in order to carry out tests. This choice affects both the\ncomplexity and efficiency of intersection tests. For example, choosing to transform a\nsphere from world coordinates into the local space of a model (A, for example) may\ncause the deformation of a sphere into an ellipsoid due to non-uniform scaling, giving\nrise to a more complicated intersection tests. On the other hand, the sphere can be\ntransformed into the A’ space, allowing for simpler intersection tests: just scale triangles\nand boxes before carrying out the computations.\n2.6\nEfficient Collision Detection Using Transformation Semantics\n187\n\n\nYou might deduce from this that avoiding the local model space during intersection\ntests provides simpler and faster intersection tests. Actually, this does not hold when\nconsidering a model against a ray. Transforming a ray into the model’s local space can\ndistort its direction and length. As result, unit vectors representing the ray direction in\nworld space may not be unit after they are transformed into the model’s local space. \nHowever, observe that this process is based on a linear transformation. Because of\nthis, any parametric point reported in this local space is still mapped at the same posi-\ntion in world space by using the same parameter. Hence, a faster computation is\nobtained by using an intersection test that does not rely on unit direction vectors and\npre-computing the ray in A space. Our tests on a Pentium D processor using different\nmodels in a ray-casting algorithm pointed out that performing intersection computa-\ntions in the model’s local space (A) is about 57% faster when compared to an imple-\nmentation based on the A’ space.\nConclusion\nThis gem covered how to use semantics information in order to speed up and simplify\ncomputations arising from collision detection tasks. You have seen how to efficiently\ninvert and extract semantics information from matrices placing models in a typical\ngame. Moreover, now you can efficiently perform collision detection tasks using trans-\nformation semantics. Although only the 3D case was shown, the concepts presented in\nthis gem are straightforward and could be used in other dimensions and in other sorts\nof problems.\nIdeas presented here were used to modify OPCODE [Terdiman03], an existing\noptimized collision detection library written by Pierre Terdiman, in order to add sup-\nport for scaling models. The modified version also provides support for non-indexed\ngeometry, as well for triangle fans, strips, and point grids representing terrain. This\nversion is being used in a number of open source projects, and can be obtained at the\nfollowing link: http://www.vdl.ufc.br/gilvan/coll/opcode/.\nReferences\n[Bergen97] Bergen, G. Van Den. “Efficient Collision Detection of Complex\nDeformable Models Using AABB Trees,” Journal of Graphics Tools, Vol. 2, No. 4,\npp. 1–13, 1997.\n[Bloom06] Bloom, Charles. “Galaxy3,” available online at http://www.cbloom.com/\n3d/galaxy3/index.html, January 22, 2006.\n[Cohen95] Cohen, J.D., Lin, M.C., Manocha, D., and Ponamgi, M.K. “I-COL-\nLIDE: An Interactive and Exact Collision Detection System for Large-Scale\nEnvironments,” Symposium on Interactive 3D Graphics, pp. 189–196, 1995.\n188\nSection 2\nMath and Physics\n\n\n[Gottschalk96] Gottschalk, S. “Separating Axis Theorem,” Technical Report\nTR96–024, Dept. of Computer Science, UNC Chapel Hill, 1996.\n[Lomont07] Lomont, Chris. “Floating-Point Tricks,” Game Programming Gems 6,\nCharles River Media, 2007.\n[Terdiman03] Terdiman, Pierre. “OPCODE,” available online at http://www.coder-\ncorner.com/Opcode.htm, December 13, 2003.\n[Wu91] Wu, Kevin. “Fast Matrix Inversion,” Graphics Gems II, Academic Press, Inc.,\n1991.\n[Wu94] Wu, Kevin. “Fast Inversion of Length- and Angle-Preserving Matrices,”\nGraphics Gems IV, Academic Press, Inc., 1994.\n2.6\nEfficient Collision Detection Using Transformation Semantics\n189\n\n\nThis page intentionally left blank \n\n\n191\n2.7\nTrigonometric Splines\nTony Barrera, Barrera Kristiansen AB\ntony.barrera@spray.se\nAnders Hast, Creative Media Lab, \nUniversity of Gävle\naht@hig.se\nEwert Bengtsson, Centre For Image Analysis,\nUppsala University\newert@cb.uu.se\nT\nhe user interfaces, level, and actor designs for modern games, both 2D and 3D,\noften contain geometry that an artist intended to be a perfect circle or elliptical\narc. The mathematics of gameplay in some cases requires that a game’s runtime engine\nbe capable of generating the same types of shapes as efficiently as possible and with\nminimal error. There are numerous mathematical techniques that can be used to gen-\nerate arcs, and each has its benefits and weaknesses. The robustness of digital content-\ncreation tools, level editors, and game runtime code can be maximized if developers\nuse a unified approach for generating geometric shapes, rather than having special\ncase math for different basic types of shapes.\nThis gem shows you how splines can be constructed that can create both straight\nlines and perfect circle arcs. The latter is not possible with ordinary cubic splines and\nthe trigonometric spline will make it possible to create new forms for 3D models.\nFurthermore, this gem will show you that the trigonometric functions involved can\nbe computed without the use of the sine and cosine functions in the inner loop,\nwhich enables higher performance.\n\n\nBackground\nCubic splines cannot be used to create perfect circle arcs but trigonometric splines can\nbe used for this purpose. Trigonometric splines were introduced by Schoenberg\n[Schoenberg64] and are sometimes called trigonometric polynomials. They have been\ninvestigated extensively in the literature of math and computer-aided geometry and\nsome examples are found in [Lyche79] and [Han03]. However, they have not gained\nmuch interest in the computer graphics community, perhaps because they involve the\ncomputation of trigonometric functions which are relatively computationally expensive.\nAs hardware becomes faster they may gain more interest in the field of computer\ngraphics as a modelling tool, because it is possible to construct everything from straight\nlines to perfect circle arcs. A later section shows how you can evaluate a trigonometric\npolynomial without using sine and cosine in the loop and this enables fast evaluation,\neven if no specialized graphics hardware is available.\nTrigonometric Splines\nA trigonometric spline [Alba04] can be constructed from a truncated Fourier series\n[Schoenberg64]. The Hermite spline is defined by two points and the tangents at\nthese points, which are depicted in Figure 2.7.1.\n(2.7.1)\nTherefore, you need four terms in the Fourier series. The trigonometric curve is\ndefined over the interval [0,\b/2] as:\n(2.7.2)\nThe coefficients for the curve can be found by using the constraints in Equation\n2.7.1, producing the following system which must be solved:\n(2.7.3)\n1\n1\n0\n1\n1\n0\n1\n1\n0\n0\n1\n0\n0\n1 0\n0\n−\n−\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\na\nb\nc\nd\n⎥\n⎥\n⎥\n=\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\np\np\nT\nT\n0\n1\n0\n1\nP\na\nb\nc\nd\nθ\nθ\nθ\nθ\n( ) =\n+\n+\n+\ncos\nsin\ncos2\nP\nP\nP\nP\nP\nT\nP\nT\n0\n1\n0\n0\n1\n0\n1\n1\n( ) =\n( ) =\n′( ) =\n′( ) =\n192\nSection 2\nMath and Physics\n\n\nThe solution for this equation is:\n(2.7.4)\nIt should also be noted that the trigonometric Hermite spline can also be written\nin the following form: \n(2.7.5)\nHere, the coefficients A, B, C, and D are different from a, b, c, and d. You can prove\nthis by starting with Equation 2.7.2 and first expanding cos2θ to obtain the following:\n(2.7.6)\nThen you put Equation 2.7.4 into Equation 2.7.6 to obtain:\n(2.7.7)\nP\n1\n2\nP\nP\nT\nT\n1\n2\nP\nP\nT\nT\nT\n0\n1\n0\n1\n0\n1\n0\n1\n1\nθ\nθ\n( ) =\n+\n−\n+\n(\n)−\n−\n+\n+\n(\n)\n−\ncos +\n+\n−\n+\n+\n(\n)\nT\n1\n2\nP\nP\nT\nT\n0\n0\n1\n0\n1\nsin\ncos\nθ\nθ\n2\n2\nP\na\nb\nc\nd\na d\nb\nc\nθ\nθ\nθ\nθ\nθ\n( ) =\n+\n+\n+\n−=\n−\n+\n+\ncos\nsin\n( cos\n)\ncos\ns\n2\n1\n2\nin\ncos\nθ\nθ\n+ 2\n2\nd\nP\nA\nB\nC\nD\nθ\nθ\nθ\nθ\nθ\n( ) =\n+\n+\n+\ncos\ncos\nsin\nsin\n2\n2\na\n1\n2\nP\nP\nT\nT\nb\nT\nc\nT\nd\n1\n2\nP\nP\nT\nT\n0\n1\n0\n1\n1\n0\n0\n1\n0\n1\n=\n+\n−\n+\n(\n)\n= −\n=\n=\n−\n+\n+\n(\n)\n2.7\nTrigonometric Splines\n193\nFIGURE 2.7.1\nA trigonometric curve and its constraints.\n\n\nSimplify and you get:\n(2.7.8)\nYou can split the last term in two parts and rewrite one of them using the trigono-\nmetric identity:\n(2.7.9)\nFinally, simplifying you get:\n(2.7.10)\nThis shows that you can rewrite the curve in different forms. You can use Equa-\ntion 2.7.2 or Equation 2.7.10. But you can also use Equation 2.7.8. This flexibility\nwill be useful when you evaluate the function, as shown in the next section.\nFast Evaluation of the Trigonometric Functions\n[Barrera04] illustrated a technique for efficiently interpolating between vectors or\nquaternions. The main idea is to use spherical linear interpolation (SLERP), which was\nintroduced to the computer graphics society by Shoemake [Shoemake85]. SLERP is\ndifferent from linear interpolation in the way that the angle between each vector or\nquaternion will be constant; that is, the movement will have a constant speed. SLERP\ncan be set up between two orthogonal unit vectors A and B as:\n(2.7.11)\nIn this case, both the cosine and sine functions need to be evaluated per step in\nthe interpolation. In [Barrera04] it is shown how this can be done for k steps using\nC++ code in the following way:\n#include <math.h>\n#include <stdio.h>\n#define M_PI       3.14159265358979323846\nvoid main() {\nint k=10; // nr of steps\ndouble A[2]={1,0}; \ndouble B[2]={0,1};\ndouble tm1[2];\ndouble t0[2];\ndouble tp1[2];\ndouble t=M_PI/2.0;\ndouble kt=t/k; // step angle\nP\nA\nB\nθ\nθ\nθ\n( ) =\n+\ncos\nsin\nP\nT\nT\nP\nT\nP\nT\n1\n0\n0\n1\n1\n0\nθ\nθ\nθ\nθ\n( ) = −\n+\n+\n+\n+\n−\n(\n)\ncos\nsin\n(\n)cos\nsin\n2\n2θ\nP\nP\nT\nT\nT\nP\nT\nT\nP\n1\n0\n1\n0\n0\n1\n0\n1\nθ\nθ\nθ\nθ\n( ) =\n−\n−\n+\n+\n+\n+\n−\ncos\nsin\n(\n)cos2\n(\n)\n−\n(\n)\n1\n2\nsin θ\nP\nP\nT\nT\nT\nP\nP\nT\nT\n1\n0\n1\n0\n0\n1\n0\n1\nθ\nθ\nθ\nθ\n( ) =\n−\n−\n+\n+\n−\n+\n+\n(\n)\ncos\nsin\ncos2\n194\nSection 2\nMath and Physics\n\n\ntm1[0]=A[0]*cos(kt)-B[0]*sin(kt);\ntm1[1]=A[1]*cos(kt)-B[1]*sin(kt);\nt0[0]=A[0];\nt0[1]=A[1];\ndouble u=2*cos(kt);\ntp1[0]=t0[0];\ntp1[1]=t0[1];\nprintf(\"%f %f\\n\", tp1[0], tp1[1]);\nfor(int n=2; n<=k+1; n++) {\ntp1[0]=u*t0[0]-tm1[0];\ntp1[1]=u*t0[1]-tm1[1];\nprintf(\"%f %f\\n\",tp1[0], tp1[1]);\n// switch\ntm1[0]=t0[0];\ntm1[1]=t0[1];\nt0[0]=tp1[0];\nt0[1]=tp1[1];\n}\n}\nThis code prints the cosine and sine between A and B and the result is in the vari-\nable tp1. If you want to compute cosine and sine between two arbitrary vectors, you\ncan compute an orthogonal vector using the Gram Schmidt orthogonalization algo-\nrithm, as shown in the referenced paper.\nDiscussion\nBy forcing d to be equal to zero in Equation 2.7.2, you get\n(2.7.12)\nThis is obviously the equation for an ellipse and this proves that it is possible to\nconstruct a perfect circle/elliptical arc with the trigonometric splines. Moreover, because\nthe curve is parametric, it is possible to construct straight lines using the trigonometric\nsplines. The coefficients are vectors and the function produces a point in space. Each\ncoordinate has its own expression and the only thing that differs is the coefficients.\nTherefore, it is no problem to construct a straight line even though trigonometric func-\ntions are involved. Figure 2.7.2. shows a perfect arc drawn using the trigonometric spline.\nNote also that when you set d = 0, T0 + T1 = P1 – P0 from Equation 2.7.4.\nP\na\nb\nc\nθ\nθ\nθ\n( ) =\n+\n+\ncos\nsin\n2.7\nTrigonometric Splines\n195\n",
      "page_number": 219,
      "chapter_number": 23,
      "summary": "Actually, transformation semantics provides flexibility for processing geometry\ncoming from a given space (A, B, or world spaces) in a common space where compu-\ntations can be carried out more comfortably Key topics include intersection, intersecting, and space.",
      "keywords": [
        "local space",
        "space",
        "collision detection",
        "Trigonometric Splines",
        "Trigonometric",
        "intersection tests",
        "cos",
        "cos sin",
        "collision",
        "transformation semantics",
        "intersection",
        "Splines",
        "model",
        "Efficient Collision Detection",
        "sin"
      ],
      "concepts": [
        "intersection",
        "intersecting",
        "space",
        "trigonometric",
        "box",
        "boxes",
        "computations",
        "computing",
        "compute",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "Game_Engine_Architecture",
          "chapter": 32,
          "title": "Segment 32 (pages 632-651)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 24,
          "title": "Segment 24 (pages 222-231)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 40,
          "title": "Segment 40 (pages 381-388)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 33,
          "title": "Segment 33 (pages 314-325)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 41,
          "title": "Segment 41 (pages 389-403)",
          "relevance_score": 0.48,
          "method": "api"
        }
      ]
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 229-237)",
      "start_page": 229,
      "end_page": 237,
      "detection_method": "topic_boundary",
      "content": "Conclusion\nThe trigonometric spline is interesting because it is possible to construct perfect circle\narcs. The trigonometric nature of the spline makes it possible to construct splines and\nsurfaces that are not really possible with ordinary cubic splines, unless the surface is\napproximated with several cubic splines, in which case ripples in curvature can create\ngeometric artifacts that are visually obvious and undesirable.\nAlthough this discussion has been purely theoretical, we believe trigonometric\nsplines have great potential for solving certain problems that occur in modern game\ndevelopment. \nOne natural place for the application of trigonometric splines is within digital \ncontent-creation (DCC) tools. These splines are a perfect, simple solution to the problem\nof modelling game geometry that needs to be a pristine conic section.\nBeyond DCC, these splines can make an impact within a game’s runtime envi-\nronment as well. We envision a technique, for example, using trigonometric splines to\nimplement specialized game physics solutions, such as simulating various “machin-\nery” and Rube Goldberg machines with perfect arcs that osculate at perfect tangents,\nin a beautiful way that is visually free of aliasing that piecewise polylines or traditional\ncubic splines might introduce.\nWe also believe that there is great potential to apply this technique to various pro-\ncedural geometry, procedural texturing, and procedural animation techniques. The\ntechnique is fairly efficient, and may be suitable for implementation in the geometry\nshader stage introduced with the most recent graphics hardware. Imagine the possibil-\nities that might include a whole new generation of awesome roller coaster games!\n196\nSection 2\nMath and Physics\nFIGURE 2.7.2\nA circle arc is drawn using the trigonometric curve.\n\n\nReferences\n[Alba04] Alba-Fernandez, V., Ibanez-Perez, M.J., and Jimenez-Gamero, M. D. “A\nBootstrap Algorithm for the Two-Sample Problem Using Trigonometric Hermite\nSpline Interpolation,” Communications in Nonlinear Science and Numerical Simu-\nlation (April 2004), Vol. 9, No. 2, pp. 275–286.\n[Barrera04] Barrera, T., Hast, A., and Bengtsson, E. “Incremental Spherical Linear\nInterpolation,” Proceedings of SIGRAD (2004), Vol. 13, pp. 7–10.\n[Han03] Han, X. “Piecewise Quadratic Trigonometric Polynomial Curves,” Mathe-\nmatics of Computation (July 2003), Vol. 72, No. 243, pp. 1369–1377.\n[Lyche79] Lyche, T. “A Newton Form for Trigonometric Hermite Interpolation,”\nBIT Numerical Mathematics (June 1979), Vol. 19, No. 2, pp. 229–235.\n[Schoenberg64] Schoenberg, I. J. “On Trigonometric Spline Interpolation,” Journal\nof Mathematics and Mechanics (1964), Vol. 13, No. 5, pp. 795–825.\n[Shoemake85] Shoemake, K. “Animating Rotation with Quaternion Curves,”\nProceedings of the 12th Annual Conference on Computer Graphics and Interactive\nTechniques, ACM SIGGRAPH (1985), Vol. 19, No. 3, pp. 245–254.\n2.7\nTrigonometric Splines\n197\n\n\nThis page intentionally left blank \n\n\n199\n2.8\nUsing Gaussian Randomness\nto Realistically Vary\nProjectile Paths\nSteve Rabin, Nintendo of America Inc.\nsteve.rabin@gmail.com\nW\nhether shooting a gun or firing off some arrows, we all have an intuitive sense\nof what the shot distribution on a bull’s eye target should look like. Shots will\ngenerally be peppered near the center with a few straying shots. This isn’t the kind of\ndistribution generated from rand(), but rather a special kind of randomness com-\nmonly represented by a bell curve. Fortunately, there are random number generators\nthat are capable of generating this type of Gaussian randomness. This gem discusses a\nvery efficient Gaussian random number generator, detailing how it should be applied\nto simulate natural variations in the paths of projectiles.\nGaussian Distribution\nPseudo-random number generators (PRNGs) like rand() produce uniform distribu-\ntions, in which there is an equal (or uniform) chance of any given number being\nselected from a given range. For example, in the range [0, 1], the odds of selecting 0.3\nor 0.5 are the same. A Gaussian distribution, which is sometimes known as a normal\ndistribution, favors positive and negative numbers centered near zero. When the stan-\ndard deviation of this distribution is 1.0, it is called a standard normal distribution, as\nshown in Figure 2.8.1. This distribution is often referred to as a bell curve because of\nits shape.\nTo interpret Figure 2.8.1, consider the 68–95–99.7 rule. According to this rule, \n68% of the values lie within one standard deviation of the mean [–1, 1], 95% of the\nvalues lie within two standard deviations [–2, 2], and 99.7 % of the values lie within\nthree standard deviations [–3, 3]. The remaining 0.3% of the values lie beyond \nthis range, with the chance of seeing numbers beyond [±]5.0 being less than one in a\nmillion.\nNow that you have a better feel for what a Gaussian distribution looks like, let’s\nlook at a few random number generators that can create such distributions.\n\n\nGenerating Gaussian Randomness\nGaussian random number generators (GRNGs) are useful for statistical analysis and\nhave been studied in-depth for both speed and quality [Thomas07]. Speed is a concern\nbecause large simulations require billions of normally distributed random numbers.\nThese simulations, which might perform communications or financial modeling, are\nconcerned with the quality and accuracy of the distribution in the far tails (beyond six\nstandard deviations). The reason is that extremely rare events can affect the outcome of\nimportant features that these simulations wish to explore. \nFortunately, video games don’t require this level of rigor. In fact, games have the\nopposite problem in that a GRNG shouldn’t generate extreme, but rare, numbers\nbecause they would appear as an error to the player. For example, if tree heights were\ndetermined with a GRNG, it would be odd to see most trees between 10 and 15 meters\nwith just one really tall 30 meter tree. Consequently, for most purposes, any GRNG you\nuse should reject (but not clamp) values beyond three standard deviations.\nGaussian Random Number Generators\nOne popular high-quality GRNG is polar-rejection [Knop69] (also known as the polar\nform of the Box-Mueller transform). This algorithm was made popular by its inclusion\nin the book Numerical Recipes in C [Press97] and is notable because its most expensive\noperations consist of only one logarithm and one square root (avoiding sine and\ncosine, which are required in the original Box-Mueller transform [Box58]). Although\nthis is a reasonable GRNG, there are faster algorithms.\n200\nSection 2\nMath and Physics\nFIGURE 2.8.1\nGaussian distribution (normal distribution) with a mean of zero and a \nstandard deviation of 1.0. The horizontal axis represents the value of the random numbers\ngenerated and the vertical axis is the likelihood of seeing any particular value. The tails of \nthis distribution are the seldom-seen values beyond three standard deviations (less than –3.0\nand more than 3.0).\n\n\nThe ziggurat method [Marsaglia00] is a second popular GRNG and is often cited\nas the best algorithm given speed versus quality tradeoffs [Thomas07]. It’s faster than\npolar-rejection, due to 1KB of lookup tables and very rare calls to transcendental func-\ntions. However, for game development, accessing these lookup tables will result in data\ncache pollution, causing the game to run slower than with polar-rejection, due to the\nhigh cost of memory access relative to the CPU speed on most platforms. For example,\nwhile the ziggurat method is actually very fast, the lookups will evict other parts of the\ngame program from the cache, negatively affecting the overall game speed more than\npolar-rejection.\nGiven that the two previous algorithms are overkill for game applications, the best\nmethod is a simple and efficient technique called the central limit theorem, sometimes\nreferred to as the sum-of-uniforms [Thomas07]. This algorithm takes several uniform\nrandom numbers, such as those generated by a PRNG like rand(), and adds them.\nAccording to the central limit theorem, the sum of these uniform random numbers will\nresult in a single Gaussian distributed random number. This algorithm performs poorly\nin the tails, but this is acceptable because you generally aren’t interested in values beyond\nthree standard deviations.\nMore precisely, the central limit theorem states that the sum of K uniform ran-\ndom numbers in the range [–1, 1] will approach a Gaussian distribution with mean\nzero and standard deviation \n. For example, if you add three uniform random\nnumbers, they will have a mean of zero and a standard deviation of \n(which is very convenient because the mean and standard deviation are identical to a\nstandard normal distribution). The following code generates a Gaussian distribution\nby adding three 32-bit signed uniform random numbers (generated by a very fast xor-\nshift PRNG [Marsaglia03]).\ndouble gaussrand(void)\n{\nstatic unsigned long seed = 61829450;\ndouble sum = 0;\nfor(int i=0; i<3; i++)\n{\nunsigned long hold = seed;\nseed^=seed<<13; seed^=seed>>17; seed^=seed<<5;\nlong r = hold+seed;\nsum += (double)r * (1.0/0x7FFFFFFF);\n}\nreturn sum; //Returns [-3.0,3.0]\n}\nThe function gaussrand() returns a double in the range [–3.0, 3.0]. If you want\na number in the [–1.0, 1.0] range, simply divide the result by 3.0 (which will conse-\nquently shrink the standard deviation to 0.33). The distribution roughly follows the\n68–95–99.7 rule, but because the tails are missing, the distribution for this particular\nalgorithm (with this seed) is 66.7–95.8–100.\n3 3\n1 0\n= .\nK 3\n2.8\nUsing Gaussian Randomness to Realistically Vary Projectile Paths\n201\n\n\nThe central limit theorem method can be made more accurate, especially in the\ntails beyond three standard deviations, by summing more numbers (increased K ).\nHowever, this makes the algorithm slower for not much benefit, because you gener-\nally don’t care about the tails.\nVarying Projectile Paths\nAn ideal application for a GRNG in games is adding random variation to projectile\npaths. As discussed earlier, projectiles, like bullets and arrows, are expected to have\nsome variation that follows a Gaussian distribution (probably due to many random\nvariables like wind, hand shakiness, and projectile irregularities that additively con-\ntribute to the final path). However, this Gaussian distribution needs to be expanded\ninto 2D, as shown in Figure 2.8.2.\n202\nSection 2\nMath and Physics\nFIGURE 2.8.2\nThe left target shows a Gaussian probability distribution in 2D. This can best\nbe visualized as the middle figure, which is a Gaussian distribution revolved around the center.\nThe right target is an example of 30 bullets perturbed by the revolved Gaussian distribution.\nBecause the rings are placed at one and two standard deviations, roughly 68 % of the bullets\nstrike within the smallest ring and 95% of the bullets strike within the two smallest rings.\nThe distribution in Figure 2.8.2 was created with the help of polar coordinates.\nThis requires two random numbers for each 2D point: an angle and a distance. The\nbullets in Figure 2.8.2 were computed by generating a uniform random angle in the\nrange [0, 2\b], along with the absolute value of a Gaussian random distance in the range\n[–1, 1]. By using a uniform random number for the angle, you guarantee that the bul-\nlets are evenly distributed at all angles around the center. By using a Gaussian random\nnumber for the distance, you guarantee that the bullets are concentrated near the center,\nfollowing a normal distribution and the 68–95–99.7 rule.\nThe 2D distribution in Figure 2.8.2 is not technically a 2D Gaussian distribution\n(also known as a multivariate normal distribution). A true 2D Gaussian distribution\nis constructed with two Gaussian random numbers plotted against each other in\nCartesian coordinates (not polar coordinates). This distribution is useful in statistics,\nbut is not desirable for what you’re trying to model.\n\n\nThe flaw in this kind of a distribution, for these purposes, can be seen in the fol-\nlowing example. If x is a Gaussian random number and y is a Gaussian random num-\nber, a coordinate of (1.41, 1.41) is statistically less likely than a coordinate of (2.0,\n0.0), even though these coordinates are equidistant from the origin. Therefore, a true\n2D Gaussian distribution will favor the coordinate axes over the diagonals, which is\nundesirable for a 2D projectile distribution.\nAdditional Applications\nGaussian randomness is useful for many game applications other than projectiles. For\nexample, if there are multiple characters or vehicles that move together, there is a ten-\ndency to see lockstep movement. This can be avoided by perturbing each agent’s\nacceleration, top velocity, or animation speed by a GRNG. This will cause small vari-\nations around an average that will break up any synchronized movement or anima-\ntions. The result is subtle variations with a few outliers.\nAnother application is to use a GRNG to perturb the heights of characters, trees,\nor buildings. If you have algorithmic control over the geometry of objects in your\ngame, realistic variability can be created with a GRNG. This helps when the number\nof visible objects at any one time is large and you need natural variation. In general,\nmany physical characteristics or attributes that should be randomized around an aver-\nage will likely benefit from a Gaussian distribution. \nGaussian Distributions in Nature\nWhy do many distributions in nature follow a Gaussian distribution (or bell curve),\nsuch as human intelligence or the heights of trees? The central limit theorem alludes to\nthe answer. When there are many uniform (or even non-uniform) random variations\nthat contribute to a given property, the distribution of that property becomes more\nnormal (rather than remaining uniform). Although this is a gross simplification of\nmost systems in nature, it does shed light on why so many properties and systems\nroughly display a Gaussian distribution.\nFor example, if scores on an IQ test are influenced by genetics, diet, schooling,\nlife experiences, and environment, each of these variables combine into the single IQ\nscore. If all of these variables were uniform and weighted equally, the central limit the-\norem says that the result would be a normal distribution. Of course, each of these\nvariables is not likely to be uniform, but rather the sum of other random variables,\nwhich are in turn affected by even more random variables. Therefore, many of these\nrandom variations like diet or schooling that influence IQ probably already follow a\nnormal distribution. Ultimately, you can approximate many properties in nature by\nassuming that many small, independent effects are additively contributing to a given\nproperty.\n2.8\nUsing Gaussian Randomness to Realistically Vary Projectile Paths\n203\n\n\nConclusion\nGenerating Gaussian randomness for games is embarrassingly simple and efficient\nusing the central limit theorem. However, many game programmers aren’t even aware\nof this type of random number generator. Therefore, the biggest challenge is simply\ngetting the word out and letting developers know that this extra tool exists. \nMany physical systems and characteristics tend to have a normal distribution that\ncan be modeled using Gaussian randomness. By combining uniform randomness\nwith Gaussian randomness in polar coordinates, applications like adding realistic vari-\nation to projectiles can easily be accomplished.\nReferences\n[Box58] Box, G.E.P. and Muller, Mervin E. “A Note on the Generation of Random\nNormal Deviates,” The Annals of Mathematical Statistics (1958), Vol. 29, No. 2,\npp. 610–611.\n[Knop69] Knop, R. “Remark on Algorithm 334 [g5]: Normal Random Deviates,”\nCommun. ACM, 12(5), 1969.\n[Marsaglia00] Marsaglia, George, and Tsang, Wai Wan. “The Ziggurat Method for\nGenerating Random Variables,” Journal of Statistical Software, Vol. 5, 2000, paper\nand code available online at http://www.jstatsoft.org/index.php?vol=5.\n[Marsaglia03] Marsaglia, George. “Xorshift RNGs,” Journal of Statistical Software,\nVol. 8, 2003, available online at http://www.jstatsoft.org/v08/i14/xorshift.pdf.\n[Press97] Press, W.H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. Numer-\nical Recipes in C, 2nd edition, Cambridge University Press, 1997.\n[Thomas07] Thomas, David B., Leong, Philip G.W., Luk, Wayne, and Villasenor,\nJohn D. “Gaussian Random Number Generators,” ACM Computing Surveys 39,\n2007.\n204\nSection 2\nMath and Physics\n",
      "page_number": 229,
      "chapter_number": 24,
      "summary": "This chapter covers segment 24 (pages 229-237). Key topics include game, distribution, and distributions.",
      "keywords": [
        "Gaussian random number",
        "Gaussian Distribution",
        "Gaussian random",
        "Gaussian",
        "random number",
        "distribution",
        "random number generators",
        "random",
        "uniform random numbers",
        "normal distribution",
        "trigonometric splines",
        "number",
        "uniform random",
        "standard deviations",
        "Generating Gaussian Randomness"
      ],
      "concepts": [
        "game",
        "distribution",
        "distributions",
        "likely",
        "algorithm",
        "algorithmic",
        "randomness",
        "randomized",
        "generation",
        "generated"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 28,
          "title": "Segment 28 (pages 261-272)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 14,
          "title": "Segment 14 (pages 124-131)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 49,
          "title": "Segment 49 (pages 475-482)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 15,
          "title": "Segment 15 (pages 132-139)",
          "relevance_score": 0.5,
          "method": "api"
        }
      ]
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 238-245)",
      "start_page": 238,
      "end_page": 245,
      "detection_method": "topic_boundary",
      "content": "205\nS E C T I O N\n3\nAI\n\n\nThis page intentionally left blank \n\n\n207\nIntroduction\nBrian Schwab\nE\nvery year, new games come out. Game programmers rush to see what they’ll do to\nfurther our ideas of what a game can accomplish. Will they be photo-realistic?\nWill all physical interactions be modeled with true-to-life physics? Will the audio\nmake you feel like you’re six feet from the action, and your heart pumping hard with\nthe music? Finally, and most importantly to this section of Game Programming Gems\n7, will the in-game characters have half a damn brain?\nAI is one of the hardest categories to get right. Sure, art and music are pretty sub-\njective, but what seems intelligent to people is an individual distinction on a whole\nother level. Take for instance a gut level reaction. You’re walking down the street, and\na full grown, adult male lion steps out from behind the next tree in front of you.\nWhat’s the intelligent thing to do? The vast majority of people would say one of the\nfollowing things:\n• Run.\n• Wait and see what’s going to happen.\n• Walk backward slowly and don’t make any sudden moves.\nHowever, any kind of question like this would also guarantee you plenty of not-\nso-common responses:\n• Look for something to defend yourself with.\n• Flag down a cab and get out of there.\n• Throw your huge leather purse at the lion, hoping he’d stop to eat it.\n• Scream at the top of your lungs, scaring him off.\n• Pepper spray him.\nWhich one is correct? The answer might easily be all of them. In real life, roughly\n70% of all people in this situation would freeze exactly like a deer in the headlights.\nWe’re hardwired to sit perfectly still, mostly because during evolution, we learned that\nmost of our predators had motion-based eyesight (meaning, they see motion much\nbetter than other types of visual stimulus, like color or shape). Had lions and hyenas\non the plains of Africa during our evolution instead been equipped with color-based\neyesight, humans might today be able to change color like the chameleon, which\nwould be a tragedy if you’re a tattoo artist.\nObviously, we can’t model that kind of standstill in games. If you came around\nthe corner in a shooter and pretty much everybody froze, players would likely think\nthe game was broken. However, if everybody ran, the game would get monotonous\n\n\nquick (although this still might work for some games). If everybody readied a weapon\nof some sort, you’ve got a pretty overwhelming scene. But a combination of these \ncan work. You can even tune what mix of them they use in order to get the degree of\nchallenge you want the player to overcome.\nThe point is, you can’t always base your game’s AI behaviors on realism. You also\ncan’t base your AI on what any one of us might think is the correct behavior, because\npeople’s notion of what constitutes an intelligent response can vary so greatly. Only \nby continuing the search for new techniques can you ever hope to convey a little\nperceived “intelligence” from your creations.\nThe gems in this section show just how far AI is coming. No longer is the devel-\nopment community as concerned about the trivial matters of AI implementation.\nNow we’re delving into issues like more realistic perception models, using new pro-\ngramming techniques to simplify the creation of our AI systems, giving our AI char-\nacters true personality traits, and analysis of our AI at a statistical trend level.\nJohn Harger and Nathan Fabian have written a gem in which you’ll learn how to\nuse a supervised learning technique called behavior cloning, which can be used to\ncapture human performances. Steve Rabin and Michael Delp detail a unified sensing\nmodel, showing that you can model large portions of reality quite effectively with a\nnice, orderly, systemic approach. Iskander Umarov and Anatoli Beliaev explain how to\nuse generic programming to create and manage hugely complex AI systems using\ncode that is small, fast, and robust. Michael F. Lynch brings you a detailed gem\nconcerning modeling attitudes within your AI agents. G. Michael Youngblood and\nPriyesh N. Dixit describe advanced player logging analysis, which can uncover useful\npatterns of gameplay and player interaction that can be difficult to see with just cur-\nsory observation. Michael Dawe shows you how to improve your planning systems \nby using plan merging to increase the reactivity of your systems without incurring full\nre-planning costs. Finally, because you might never tire of hearing about ways to\nimprove usage and understanding of path-finding algorithms, Robert Kirk DeLisle\nprovides insight into an A* technique called fringe search.\n208\nSection 3\nAI \n\n\n209\n3.1\nCreating Interesting Agents\nwith Behavior Cloning\nJohn Harger\nNathan Fabian\nH\numan opponents are interesting. The popularity of gaming online may have\nsomething to do with the kinds of opponents you find there (or maybe mostly\nthe idea that you can “pwn some noobs”) but regardless of why it’s popular, it wouldn’t\nhurt to breathe some life into the offline, single player opponents (or even allies). This\ngem explains how to use a machine-learning technique called behavior cloning \n[Sammut92] to borrow from styles and strategies of humans playing the game and\nplace them into game agents.\nBehavior cloning is essentially a version of supervised learning. In supervised learn-\ning, the idea is that the human trainer provides a set of labels for particular objects and\nthe algorithm learns to recognize, or predict, labels based on the attributes of those\nobjects. When it sees similar characteristics in a new object, it should correctly label it.\nIn behavior cloning, the trainer acts in response to a stimulus. The response becomes\nthe label associated with the stimulus, which is the object. The algorithm then learns to\nrepeat the same kinds of actions in the presence of the same kinds of stimuli.\nThis extends very nicely into games where it’s easy to find the response a person\nwill make to a stimulus, that is, the game state. (In the game nearly all interaction takes\nplace within the game context and each game session is fairly similar to the last. It is\nimportant to note, however, that in some cases the game can include out-of-band infor-\nmation like clan membership and rivalries that the agent wouldn’t be able to simulate.)\nBecause there is so much similarity between supervised learning and behavior\ncloning, this technique can be done with any off-the-shelf supervised learning algo-\nrithm. This gem uses a decision tree because the output is easier to edit than, say, the\nweights on a neural net.\nIn other words, it is not necessary to become a machine learning expert to exploit\nadvances using this technique. Even better, the game playing trainers don’t even need\nto know that the learning is going on in order for it to be effective. This gem shows\nyou how well the tried-and-true decision tree learning algorithm works when borrow-\ning human characteristics to create interesting, playable game agents.\n\n\n210\nSection 3\nAI \nExample: The Demo Game\nThroughout this gem, the examples refer to a game design that resembles the classic\ncomputer game Space War. That is, two ships face off on an infinite 2D playing field;\nthe goal of the game is to destroy the other ship. This simple design provides an easy-\nto-understand game environment in which you can train an agent.\nHow to Set Up the Feature Space Output\nThe feature space is the most important thing in instance-based machine learning,\nwhich is what you use to do behavior cloning. It is a set of attributes or “features” used\nto record instances of the game state for learning (see Figure 3.1.1). You must carefully\nconsider the design of the features to be able to train interesting agents, and perhaps to\nbe able to train an agent at all.\nFigure 3.1.1\nExample of a feature space.\nThe features must provide enough information to make decisions. Remember, the\ngoal is to train an agent to behave like a particular human. Try thinking of how you\nwould approach the situation; you might consider the distance to your opponent’s\nship, the direction of his or her ship from your current heading and even the direction\nhe or she is facing from you. Don’t be afraid to keep adding features; some players\nmight act differently if their opponent is “off radar,” and that distinction is going to\nprovide more information than distance alone. It might take the learning algorithm 30\nminutes to train an agent with detailed features, but it will likely produce a richer one.\n\n\nHowever, you should be careful not to use absolute values for information such as\nposition and orientation. If the agent is trained to accelerate based on an absolute\ndirection, the resulting actions might be the opposite of what was expected! For exam-\nple, if your ship is at point (2,1) and the enemy is at (0,0), you may have turned right.\nIf this is what the agent learns, it will always turn right when its opponent is located\nat (0,0), even if the trainer would have turned left. If the feature was relative, such as\n15° to the left (\t2.5°), the agent will turn right when the opponent is just to the left,\nregardless of its absolute position. Think of it this way—would you consider your\nabsolute heading, latitude and longitude when entering a brawl? Neither should the\nagent—at least if you want it to act realistic.\nIn addition to erratic behavior, absolute values can give a search space that might\nbe much too large to process. If the range of location is infinite, the learning algo-\nrithm might never be able to classify behavior correctly. Instead of a tight, well struc-\ntured tree, you end up with a noisy mess.\nTable 3.1.1\nEquations for Calculating the Features Shown in Figure 3.1.1\nName \nEquation \nDescription \nDistance \nDistance between the two ships\nDirectionTo\nAngle to Ship2 from Ship1’s facing direction\n(–180° to 180°) \nDirectionFrom\nAngle from Ship2’s facing direction to Ship1 \n(–180° to 180°)\nVNorm\nChange in distance between ships\nDDirectionTo \nChange in DirectionTo \nDDirectionFrom \nChange in DirectionFrom \nTraining an Agent\nNow comes the fun part: training the agent. This can be done with nearly any machine-\nlearning technique available. When developing your own game, feel free to experiment\nwith existing implementations or write your own. If you are interested in machine learn-\ning, take a look at [Witten99].\nWe chose to create our own simple decision tree implementation. It is certainly\nnot the best—because it performs no linear regression, it is limited to working with\ndiscreet values. Because predefined linear ranges must be mapped to integers, the trees\nproduced are probably not going to fit the data as tightly as they could otherwise. As\nθ\nθ\nτ\nτ\n2\n2\n1\n−\n−\n(\n)\nθ\nθ\nτ\nτ\n1\n1\n1\n−\n−\n(\n)\nr\nd\nd\nτ\nτ\n−\n−1\nθ 2\n1\n2\n2\n=\n⋅\n⋅\n−\n⊥\ntan\nr\nr\nr\nr\nd\nv\nd\nv\nθ1\n1\n1\n1\n=\n⋅\n⋅\n−\n⊥\ntan\nr\nr\nr\nr\nd\nv\nd\nv\nr\nd\n3.1\nCreating Interesting Agents with Behavior Cloning\n211\n\n\na result, if distance 0.5 through 1.0 is a predefined interval, and one trainer tended to\nreact at 0.54 and the other at 0.98, they both would branch at distance 1.0, giving\nboth agents a similar feel.\nWhen you run our demo AIShooter, found on the CD-ROM, the game records\nthe state 100 times a second. These states fill in the feature space defined earlier: the\ndistance between the two ships, their relative directions to each other, and so on. In\naddition, the states of the player’s controls are also recorded, such as turning left or\nright and accelerating forward or backward, and you offset these controls by 150ms to\naccount for human reaction time. This is the stimulus/response information you need\nto build the decision tree.\nBuilding the Trees from the Samples\nOnce you have a game session recorded, shown in Table 3.1.2, you can use that infor-\nmation to build the decision trees. Different control groups are split between different\ntrees. Forward, Reverse, and None are the possible decisions for one tree. Left, Right,\nand None are possible decisions for another tree. The nodes of the tree test for all the\nvalues that one feature takes on. There is one child and one path down the tree for\neach value the feature can take on. Each child is passed from its parent all the data that\ncorresponds to the value of that feature for that path. Recursively then, the child node\ndetermines whether the data is pure (meaning all the same).\nTable 3.1.2\nSample Recorded Game State Data\nDistance\nDirectionFrom \nHitpoints \nTurning\n“2 to ∞”\n1\n100\nRIGHT\n“0 to 1”\n1\n100\nLEFT\n“0 to 1”\n1\n100\nLEFT\n“1 to 2”\n1\n100\nNONE\n“0 to 1”\n3\n100\nNONE\n“2 to ∞”\n2\n100\nLEFT\n“1 to 2”\n2\n100\nNONE\n“2 to ∞”\n3\n100\nRIGHT\n“0 to 1”\n3\n100\nRIGHT\n“2 to ∞”\n2\n100\nLEFT\n“0 to 1”\n3\n100 \nRIGHT\nThere is enough data left to make the determination and undergo searches for a\nfeature, if so. If not, the node is a leaf and determines which label (that is, Forward) is\nthe majority in its data and sets that as the return value. Listing 3.1.1 shows the recur-\nsive algorithm for Node learning.\n212\nSection 3\nAI \n",
      "page_number": 238,
      "chapter_number": 25,
      "summary": "This chapter covers segment 25 (pages 238-245). Key topics include game, gaming, and learned. Game programmers rush to see what they’ll do to\nfurther our ideas of what a game can accomplish.",
      "keywords": [
        "Game",
        "behavior cloning",
        "agent",
        "learning",
        "behavior",
        "left",
        "game agents",
        "Feature",
        "tree",
        "Feature Space",
        "game state",
        "distance",
        "Interesting Agents",
        "cloning",
        "decision tree"
      ],
      "concepts": [
        "game",
        "gaming",
        "learned",
        "likely",
        "tree",
        "humans",
        "michael",
        "behaviors",
        "players",
        "agents"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 25,
          "title": "Segment 25 (pages 240-248)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 28,
          "title": "Segment 28 (pages 261-272)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 2,
          "title": "Segment 2 (pages 19-41)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 22,
          "title": "Segment 22 (pages 205-213)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 246-254)",
      "start_page": 246,
      "end_page": 254,
      "detection_method": "topic_boundary",
      "content": "Listing 3.1.1\nRecursive Node Learning Algorithm\nTreeNode* DataSet::learnNode (const string& targetName,\nconst string& columnName, const col_t& column,\nconst col_set_t& workingSet, unsigned int threshold)\n{\ncol_t newTarget = getColumn (targetName, workingSet);\nint majority = for_each (newTarget.begin (), newTarget.end (),\nMajority ());\nfloat purity = (float) count (column.begin (), column.end (),\nmajority) / (float) column.size ();\nif (column.size () <= threshold || purity >= 0.99f) {\nreturn new TreeNode (majority);\n}\nint max = *max_element (column.begin (), column.end ());\nTreeNode *node = new TreeNode (columnName, 0, max);\nfor (int i = 0; i <= max; i ++) {\ncol_set_t newWorkingSet = getAllWhere (columnName, i,\nworkingSet);\nnewWorkingSet.erase (columnName);\nif (newWorkingSet.empty ()) {\ncontinue;\n}\npair<string, col_t> best = getBest (targetName, \nnewWorkingSet);\nnode->addChild (i, learnNode (targetName, best.first,\nbest.second, newWorkingSet, threshold));\n}\nnode->fillIn();\nreturn node;\n}\nTo determine which feature to use for the node, you must find the feature that\nleads to the most uniform data. Because the tree is trying to find the one label that\nmakes sense for a given state, you want to find the set of states that describe that one\nlabel. This is the crux of what the decision tree does by encoding the features of those\nstates that correspond to the label. Ideally, when the tree is at a leaf node, as described\npreviously, there is one label in every state.\nFor example, assume you are training a tree from the data in Table 3.1.2 for\ndeciding whether to turn, and your feature space defines distance on three intervals: 0\nto 1, 1 to 2, and 2 to infinity. Let’s take a look at the distance feature in terms of infor-\nmation gain. First, look at Equation 3.1.1 for computing the information in a set of\ndata.\n(3.1.1)\nInfo V\nv\nv\nn\nn\nn\nn\nv\nv V\nv V\n( )\nlog\nlog\n,\n=\n−\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟+\n=\n∈\n∈\n∑\n∑\n2\n2\n3.1\nCreating Interesting Agents with Behavior Cloning\n213\n\n\nWithout going into detail on the meaning of information theory, the purpose here\nis to track how different the data is. The less information the better because it means\nthe labels are more uniform. In the example, there are three None, four Left, and \nfour Right labels, giving an information of Info([3,4,4]) = (–3 log2 3 – 4 log2 4 – 4 log2\n4 + 11 log2 11)/11 = 1.5726.\nIf you look at only those labels where Distance is “1 to 2,” you have only two\ninstances and both are None, giving you Info([2]) = (–2 log2 2 + 2 log2 2)/2 = 0. In\nother words, there is no information in that set because they are all the same.\nThe information in “0 to 1” is Info([1,2,2]) = 1.5219, and “2 to ∞” is Info([2,2]) =\n1. Notice these are both close to the original information because the distributions of val-\nues are very similar.\nNow you find the gain as 1.5726 – 3/11 * 0 – 4/11 * 1.5219 – 4/11 * 1 = 0.6555.\nIt is very easy to see that the gain on the hitpoints would be 0, as there is no change.\nThe gain using direction is 0.4816. Getting a uniform set in one of those examples\nreally helps the gain when you use distance.\n(3.1.2)\nIn each of the three new nodes, you pass the subset of data that corresponds with\nthat value of distance and recurse using only that. There is no reason to use distance\nagain in the second pass (it would no longer improve the information), so direction\nbecomes an obvious choice as hitpoints still provides no information about the labels.\nThe tree is shown in Figure 3.1.2. Notice again for distance “1 to 2” that since the set\nis already pure you just return that label; there is no reason to continue the recursion.\nGain O F\nInfo O\nInfo O F\n( ; )\n( )\n( | )\n=\n−\n214\nSection 3\nAI \nFigure 3.1.2\nExample decision tree.\nYou pick the label, or control, by finding the majority value of that set. It is impor-\ntant to note that for a majority to make sense in statistics, you want to make sure there\nare enough samples to make the data meaningful. In Listing 3.1.1 for learnNode, you’ll\n\n\nnotice a check against the column to ensure that there are at least a certain number of\nrows. This is the “no free lunch” rule of machine learning. There is always one free\nparameter and it often depends on the data. We use 250 as we guess that seeing about\n2.5 seconds worth of a game is enough to counteract any accidental key presses, which\nare considered noise. Additionally, we offset the controls in the game state by 150ms to\nsimulate human reaction time.\nBuilding the AI Script from the Trees\nOnce your trees are finished, you need to convert them into a form that’s usable by\nyour game. Although you could go with some tree interpreter, which is usually fast\nand efficient, this example translates them into script form, so they can be read and\nedited by hand. All you really have to do is convert your tree into the scripting lan-\nguage of your choice using conditional statements. For example, the turning tree in\nFigure 3.1.2 written as a Lua script would look like the code in Listing 3.1.2. (Please\nnote that we used strings in some places for demonstration purposes, whereas a real\nscript would use numbers.)\nListing 3.1.2\nTurning Tree as a Lua Script\nfunction turn\n--Root node\nif GameState.Distance == \"0 to 1\" then --Left branch\nif GameState.DirectionFrom == 1 then \nShip.turn (\"LEFT\")\nelseif GameState.DirectionFrom == 2 then\nShip.turn (\"NONE\")\nelseif GameState.DirectionFrom == 3 then\nShip.turn (\"RIGHT\")\nelse\nShip.turn (\"NONE\")\nend\nelseif GameState.Distance == \"1 to 2\" then --Center branch\nShip.turn (\"NONE\")\nelseif GameState.Distance == \"2 to inf\" then --Right branch\nif GameState.DirectionFrom == 1 then\nShip.turn (\"RIGHT\")\nelseif GameState.DirectionFrom == 2 then\nShip.turn (\"LEFT\")\nelseif GameState.DirectionFrom == 3 then\nShip.turn (\"RIGHT\")\nelse\nShip.turn (\"NONE\")\nend\nelse\nShip.turn (\"NONE\")\nend\nend\n3.1\nCreating Interesting Agents with Behavior Cloning\n215\n\n\nThat’s all there is to it. When the agent is running, you execute this script func-\ntion after setting the current game state, and the decision tree does its work. The agent\nwill respond to the state in a way that approximates the human who trained it.\nConclusion\nThis gem illustrates a very concise and convenient way to make agents that learn\nbehaviors from humans in a simple game. One of the best places to use this technique\nis for creating those supporting-role characters, like guards, that normally have a very\nlimited behavior, but could benefit from the introduction of some variety, especially\nin how they respond to the players.\nSome of the details we didn’t get into here involve expanding the feature space to\naccount for more opponents or obstacles. As you can imagine, adding the distance\nand direction for each one of those can start to really grow. It gets even worse if you\nconsider needing to add the distance and direction from an ally to an opponent. It is\na fully connected graph. The trick in this case is to group the objects and treat them as\na large mass with a single distance and direction.\nThese agents have limited understanding of the passage of time as well. Instead of\nknowing time directly, they know damage to the ship, which lowers as time goes on.\nHowever, if the ship was repaired back up to a certain level they would have no mem-\nory of having been damaged in the first place. You could consider adding time as a fea-\nture itself, but adding the absolute time poses the same problems as adding absolute\nposition or orientation. However, as you’ll see if you play around with the demo on\nthe CD-ROM, it isn’t necessary to have that memory to get pretty good behaviors.\nAs with anything, “there ain’t such a thing as a free lunch.” You cannot make the\nend game super villain with this algorithm, but you can add some variety of behaviors\nto his (or her) minions. We have set up a forum at http://www.tosos.com to talk\nabout some of the issues and solutions to creating more complex behaviors and deal-\ning with more complex games. There are links there that go into more depth as to\nwhy this works and the theoretical background. We would love to have you join us\nand share your experiences!\nReferences\n[Sammut92] Sammut, C., Hurst, S., Kedizer, D., and Michie, D. “Learning to Fly,”\nProceedings of the Ninth International Conference on Machine Learning\n(ICML–1992), Aberdeen: Morgan Kaufmann, pp. 385–393.\n[Witten99] Witten, Ian H., and Frank, Eibe. Data Mining: Practical Machine Learn-\ning Tools and Techniques with Java Implementations, Morgan Kaufmann, 1999.\n216\nSection 3\nAI \n\n\n217\n3.2\nDesigning a Realistic and\nUnified Agent-Sensing Model\nSteve Rabin, Nintendo of America Inc.\nsteve.rabin@gmail.com\nMichael Delp, WXP Inc.\nmichaeljdelp@gmail.com\nW\nith increased visual realism, players expect agents to sense the game world with\ngreater fidelity and subtlety. However, agent vision models in games have tradi-\ntionally been very simplistic, using a combination of view distance, view cone, and\nline-of-sight checks [Rabin05]. Hearing models, when implemented, have also been\nfairly simple, usually testing against some cutoff distance to verify whether a sound is\nheard [Tozour02]. Although these basic agent-sensing models are efficient and simple\nto program, they are transparent and appear shallow to game players. For example,\nwhen agents use a discrete distance check for vision, it results in an absolute blind\nzone beyond a certain distance. Players intimately know this and routinely use this\nknowledge to manipulate the enemy AI. This is commonly seen when players lure\nindividual enemies away from enemy groups by repeatedly inching toward them and\nrunning away.\nOnce the developer realizes that current agent-sensing models are rather primi-\ntive, dozens of clever ways to enhance these basic models begin to appear. This gem\ncovers many such additions, eventually combining them into a unified sensing model,\nbecause all senses should collaboratively inform an agent’s awareness of the world.\nThe final model may then be used in any game genre as a core part of the AI.\nThe Basic Vision Model\nBefore the gem begins looking at specific enhancements to vision, this section recaps\nthe core vision model used in the majority of games today. The three core vision cal-\nculations are view distance, view cone, and line-of-sight. Note that these three checks\nare usually computed in this order for efficiency reasons, because a radius test is very\ncheap and a line-of-sight test is very expensive. Figure 3.2.1 illustrates all three tests.\n\n\nThe computation for the view distance check is a simple distance test. However,\nit is more efficient to test against the distance squared instead of the actual distance,\nbecause it avoids taking a square root. For example, if the agent can see up to 10\nmeters away, is at coordinate (0,0,0), and the player is at coordinate (5,8,0), you can\ncompare the dot product of the vector between the two entities against the square of\nthe view distance. The dot product of the vector between them is 52 + 82 + 02 = 89.\nCompare this against the view distance squared (102 = 100) and you find that the\nagent can see the player, because 89 is less than 100. The distance squared optimiza-\ntion can be used because you’re only interested in the relative distance, not the actual\ndistance.\nThe second common step is to do a view cone check. This is done by taking the\ndot product of the agent’s normalized forward vector with the normalized vector that\npoints from the agent to the player (refer to the two vectors in Figure 3.2.1). If the\nresult is greater than zero, the player is within the agent’s 180° view cone. If the result\nis greater than 0.5, the player is within the agent’s 120° view cone (cos 60° = 0.5). As\nan optimization, if only the 180° view cone test is required, there is no need to nor-\nmalize the vectors (which potentially eliminates two square root calculations).\nThe final check, the line-of-sight test, is the most costly to perform. This test\nshoots a ray from the agent’s eye level to the location of the player. If it intersects any\ngeometry before it hits the player, the agent can’t see the player. This test can be opti-\n218\nSection 3\nAI \nFigure 3.2.1\nExample of view distance check, view cone check, and line-of-sight\ncheck. In this case, the enemy sees the player because the player passes all three tests. \n\n\nmized by testing against bounding boxes that surround the level geometry, instead of\ntesting against individual polygons. For testing against objects in the world, the\nobject’s lowest LOD can be useful, as well as bounding boxes.\nThese three tests lay the groundwork for a vision model, but as you’ll see later,\nthere are many improvements that can be made.\nThe Basic Hearing Model\nGames that simulate agent hearing, such as games that emphasize stealth, typically\nimplement this feature by having objects emit single-shot sound events that travel a\nparticular distance. For example, each footstep of a player might send out a sound\nevent that gets delivered only to agents within a particular distance from the player.\nThe distance the sound event travels depends on the loudness of each footstep, which\nin this case usually corresponds to the speed of the player. A tiptoeing player spawns\nvery weak sound events that travel only a meter or so, whereas a running player gen-\nerates sound events that travel a great distance.\nFor many games, this is a sufficient hearing model, but it suffers from a similar\nproblem as the vision model, namely that there is an absolute and arbitrary distance\ncutoff. It seems odd and unnatural that a distance of one centimeter might make the\ndifference between completely hearing and recognizing a sound and not hearing it at\nall. As you might suspect, there are many improvements that can be made to this basic\nsensing model.\nAugmenting the Vision Model Toolbox with Ellipses\nThe simple vision tests discussed previously don’t model human vision well. In partic-\nular, view cones have several drawbacks.\n• The agent potentially won’t be able to see entities right next to itself.\n• Visual acuity is highest in the center of vision and degrades with distance. View\ncones overestimate the area of vision far away and underestimate the area of vision\nclose by.\n• To avoid overly large fields of vision far away, designers tend to make view dis-\ntances unrealistically short.\nOne way that designers have dealt with these issues is by testing against multiple\ncones to model human vision [Leonard03]. However, multiple cones can leave holes\nin the agent’s vision. The left portion of Figure 3.2.2 shows a vision model that uses\ntwo cones and a circle to model vision. The narrow cone models the center of focus,\nwhich extends to far distances. The wider cone provides a broader field of view at\nshort distances. The circle catches any entities adjacent to the agent or even behind\nhim (as humans tend to have a sense when someone is right behind them). Note the\nlarge gaps in the vision model outside the intersection of the two cones.\n3.2\nDesigning a Realistic and Unified Agent-Sensing Model\n219\n\n\nA simple solution that solves all of these problems is to use an ellipse for the field\nof view. As you see in the right side of Figure 3.2.3, an ellipse gracefully deals with the\ndegradation of visual acuity with distance without leaving holes in the vision. The\nellipse “starts” a few feet behind the agent to model the sixth sense humans have about\npeople right behind them and encompasses entities adjacent to the agent.\n220\nSection 3\nAI \nFigure 3.2.2\nThe left figure illustrates a vision model using two view angles\nand a circle. Note the holes in the vision system. The right figure illustrates\nan ellipse overlaying the old model. The ellipse gracefully encompasses the\nvarious view angles to give a more accurate model of vision.\nFigure 3.2.3\nThe left figure shows the important components of an ellipse. The\nmiddle figure shows how an example point on the ellipse is calculated. The right\nfigure shows an example view from an agent where [\n] is half the view angle and a is\nhalf the maximum view distance.\n\n\nEllipse Implementation\nIn order to model vision with an ellipse, it is important to understand its components.\nTake a look at the left side of Figure 3.2.3. The length of the major axis is 2a, and the\nlength of the minor axis is 2b. The two focal points (f1 and f2) are at \t c from the\ncenter of the ellipse, where c2 = a2 – b2. The middle figure illustrates an important fact\nabout ellipses: the distance from the two foci to any point on the outside of the ellipse\nequals 2a. To determine whether something is within the ellipse, you must find the\npositions of the focal points.\nTo model human vision, you place one end of the ellipse at the agent’s eyes. The\ndesigner can then specify a view angle much as he or she would with view cones. The\nview angle will make a triangle with the agent’s eyes and the endpoints of the center\naxis of the ellipse as in the right side of Figure 3.2.3. The designer can also specify a\nmaximum viewing distance; half of which will be the distance from the agent to the\ncenter of the ellipse. So given that [\n] is half the view angle, and a is half the view dis-\ntance, you must first find the equation for c given \n and a:\n(3.2.1)\n(3.2.2)\n(3.2.3)\nSubstitute Equation 3.2.2 into Equation 3.2.3 to get the following:\n(3.2.4)\n(3.2.5)\n(3.2.6)\nEquation 3.2.6 can be precalculated at initialization. Now to find the equations\nfor the focal points, you can use the following equations given the agents’ eyes are sit-\nuated at vPos, and they are looking in the direction vDir.\n(3.2.7)\nYou want the ellipse to start fBehindDist behind the character (so the character\ncan sense characters right next to him). The final equations are as follows.\n(3.2.8)\nOf course fBehindDist could be subtracted from a at initialization in order to\nsave the extra subtractions. \nF\nF\nvPos\nvDir a\nfBehindDist\nc\n1\n2\n,\n=\n+\n−\n±\n(\n)\nF\nF\nvPos\nvDir a\nc\n1\n2\n,\n=\n+\n±\n(\n)\nc\na\n=\n−\n1\n2\ntan θ\nc\na\n2\n2\n2\n1\n=\n−\n(\n)\ntan θ\nc\na\na\n2\n2\n2\n=\n−(\n)\ntanθ\nc\na\nb\n2\n2\n2\n=\n−\na\nb\ntanθ =\ntanθ = b\na\n3.2\nDesigning a Realistic and Unified Agent-Sensing Model\n221\n",
      "page_number": 246,
      "chapter_number": 26,
      "summary": "There is no reason to use distance\nagain in the second pass (it would no longer improve the information), so direction\nbecomes an obvious choice as hitpoints still provides no information about the labels Key topics include distance, model, and agents.",
      "keywords": [
        "distance",
        "Vision Model",
        "Model",
        "view",
        "view distance",
        "vision",
        "agent",
        "model human vision",
        "view cone",
        "agent vision models",
        "view distance check",
        "ellipse",
        "player",
        "tree",
        "view cone check"
      ],
      "concepts": [
        "distance",
        "model",
        "agents",
        "players",
        "cone",
        "game",
        "time",
        "view",
        "node",
        "script"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 31,
          "title": "Segment 31 (pages 291-298)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 30,
          "title": "Segment 30 (pages 279-290)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 32,
          "title": "Segment 32 (pages 306-318)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 38,
          "title": "Segment 38 (pages 363-370)",
          "relevance_score": 0.41,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 31,
          "title": "Segment 31 (pages 295-305)",
          "relevance_score": 0.41,
          "method": "api"
        }
      ]
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 255-262)",
      "start_page": 255,
      "end_page": 262,
      "detection_method": "topic_boundary",
      "content": "To determine whether an entity is within the agent’s field of view, you simply take\nthe entity’s distance from each of the focal points, add the distances, and check that\nthey are less than the maximum view distance (2a). So two distance checks for each\nentity is all that is needed per agent. Note that you cannot use squared distances in\nthis equation because you have to add them together. These equations work for 3D or\n2D ellipses. Use 3D if height is important to your world.\nUsing an ellipse to model vision is easy to calculate and is not much more expen-\nsive than a cone solution. It is the first building block in providing a more accurate\nhuman-sensing model. \nModeling Human Vision with Certainty\nAs observed previously, the fact that objects are either seen completely or not seen at all\nis an unfortunate side effect of discrete vision tests. The flaw of the discrete vision test is\nmost obvious when you consider the subtlety of real human vision, such as peripheral\nvision in which objects are sometimes only partially recognized. In order to understand\nthis more precisely, let’s quickly review the mechanics of real human vision.\nHuman vision has been studied in-depth, from the retina to the neurons in the\nbrain, but for the purposes here, let’s extract useful measurements and properties that\ncan be modeled in this artificial vision system. Humans have two eyes, of course, and\ntogether they can see a collective range of 200° with 120° of overlap (binocular vision)\n[Wandell95]. The eye focuses light onto the back of the eye, which uses rod and cone\ncells to detect light and color. Visual acuity is greatest at the center of fixation and\ndecreases rapidly with distance from the center. Cones detect color and are densely\npacked toward the center of the retina, whereas rods are 100 times more sensitive to\nlight and are primarily responsible for night vision and peripheral vision. As a result,\nperipheral vision has very little color response but is extremely sensitive to movement. \nWith a little more science behind this model now, you can start to make several\nobservations. The first is that visual acuity and color detection is highest in the center\nof vision and falls off rapidly in the periphery. The second is that, while peripheral\nvision is poor, it is adept at detecting movement.\nUsing the discrete tests in the toolbox, a vision model can be created that scores\nobjects by which area they occupy in the range of vision. In Figure 3.2.4, the percent-\nages represent the certainty that a particular object is identified. Objects in the center\nof vision are fully identified, whereas objects in the near-peripheral, mid-peripheral,\nfar-peripheral, and the behind-the-head (sixth sense) areas have lower certainties. \nAn important feature of this model is that moving objects get a score increase of\n50%, which accounts for the special perception of movement. Depending on the game,\nwalking and running might trigger the 50% increase, whereas sneaking or crawling \ndoes not.\nIf camouflage or hiding (possibly in shadows) plays a significant role, identification\ncan be decreased depending on a combination of contrast and size of exposed profile.\nFor example, when players are crouched in a dark corner, their profiles are smaller and\n222\nSection 3\nAI \n\n\nthey are difficult to see, which should consequently discount their identification by as\nmuch as 100%. Even if the agent is looking directly at the player, the agent might stare\nfor a few seconds and move on, because it can’t identify the object 100%. If this level of\nsubtlety is employed, it might be advisable to add a smaller ellipse in which identifica-\ntion is unconditionally 100%, regardless of profile or contrast.\n3.2\nDesigning a Realistic and Unified Agent-Sensing Model\n223\nFigure 3.2.4\nCertainty in vision as a collection of dis-\ncrete tests. In this model, objects are fully identified at\n100%, highly suspect at or above 80%, and slightly suspect\nat or above 50%. Moving objects get an extra 50% score\nincrease in order to model peripheral sensitivity to move-\nment. Camouflaged, hiding, or crouching (reduced pro-\nfile) objects decrease certainty by as much as 50%.\nThe percentages of certainty can be interpreted in whatever way makes sense within\nthe game design. One approach would be to put thresholds at which the agent would\nperform particular actions. For example, at 100% certainty the object in question is\nfully identified and the agent might shoot at the object. At 80% or higher, the agent\nmight turn their head and start approaching the object in question. At 50% or higher,\nthe agent might only turn their head. Anything below 50% might not be enough stim-\nulus to take any action.\nOne downside of the model in Figure 3.2.4 is that it still contains arbitrary dis-\ncrete zones. The model can be further refined to support gradual falloff with angle\nand distance. Figure 3.2.5 shows a vision model in which the inner circle falls off with\nthe angle and the outer circle combines a distance falloff with the angle falloff. The\nellipse remains a discrete test with 100% certainty in the forward direction and 80%\nbehind.\n\n\nThe arbitrary models depicted in Figures 3.2.4 and 3.2.5 are only examples and\nshould be modified as needed for the game design. They are very coarse approximations\nof real human vision based on the particular features identified here. Clearly, these mod-\nels take great liberties and approximate the science. For example, these models favor\n180° vision over 200° for simplicity reasons. However, these models are a big improve-\nment in terms of subtlety and sensitivity compared with typical game vision models.\nAnother important feature of this vision model is to take into account the mental\nalertness of the agent. When the agent is highly alert, the percentages should be\nincreased and the zones enlarged. If the agent is distracted or sleepy, the percentages\nshould be decreased and the zones reduced.\nModeling Human Hearing with Certainty\nAs demonstrated with the vision model, calculating sensory identification as a per-\ncentage can be an effective way to introduce subtlety into a sensing model. Similarly\nit’s worth constructing a hearing model that produces percentages of certainty. How-\never, before you dive in and create a hearing model, let’s look at some issues related to\nsound and hearing.\nThe most important property of sound is that intensity falls off exponentially\nwith distance. Although sound propagates easily through air, only lower tones travel\nwell through walls. This makes conversations and many high pitched tones hard to\nhear from adjacent rooms. Lastly, sound reflects off walls and reverberates within\nrooms, making sounds capable of getting around most obstacles.\n224\nSection 3\nAI \nFigure 3.2.5\nVision model with gradient zones of certainty. The inner\ncircle falls off with angle, whereas the outer zone falls off with angle and\ndistance. The ellipse test remains discrete. Note that black equals 100%\ncertainty and white equals 0% certainty.\n\n\nWhen constructing a hearing model, a simple radius check for whether an agent\nhears a particular sound could be augmented in several ways. First, the volume of the\nsound should affect how far it travels, with clear recognition falling off to uncertain\nrecognition. Second, walls might cause some degree of uncertain recognition depending\non thickness and such. Third, because sound bounces off surfaces, if the line-of-sight\nbetween the source and listener is blocked, a path could be computed to see whether a\nclear route can be found. If a path is found, the distance of the path can be used to deter-\nmine the falloff. Alternatively, as a less processor intensive solution, zones could be used\nin which all sounds made within a particular zone or an adjacent zone can be heard\nregardless of walls between the source and listener. In some cases, the coarse search space\nused for hierarchical pathfinding can also be used for determining sound zones.\nFigure 3.2.6 demonstrates sound falloff coupled with the zone approach. Based on\nthe sound intensity, the sound will have a radius at which it is recognized at 100%.\nBeyond that radius the certainty drops off to zero after some distance. However, the\nsound is heard only if the listener is in the same or adjacent zone from the sound source.\n3.2\nDesigning a Realistic and Unified Agent-Sensing Model\n225\nFigure 3.2.6\nHearing model demonstrating sound inten-\nsity falloff coupled with zones. An agent can hear a sound\nonly if the sound was made in the same or an adjacent zone.\nIn this example, the sound does not propagate to Zone C\neven though the radius check would allow it.\nIf a zone approach requires too much preprocessing of the game world or isn’t\nsuitable for randomized maps, the pathfinding engine can be exploited to determine\nwhether a sound can propagate from the source to a listener in the case that the line-\nof-sight is blocked. Although this is more processor intensive, it can accurately tell if\n\n\nthe sound can travel unimpeded to the listener and an approximate distance that the\nsound traveled.\nIn the way that walls can block vision, other sounds can drown out a particular\nsound and make it hard to hear. In order to model this effect, you need to consider all\nsounds, including ambient sounds, and determine whether a louder sound might be\noverpowering and masking all other sounds. For example, if a train is rushing by\nwhen the player is running, their footsteps might not be heard. However, if the player\nshoots their gun, the gunshot sound might be reduced by the noise level of the train,\nmaking it much more difficult to hear. This kind of modeling opens up new gameplay\nopportunities because players are then encouraged to time noisy actions with other\nnoisy events. \nSimilar to how a sixth sense was added to the vision model, the hearing model\ncan also include other senses such as smell. For example, if a rotting corpse creates a\nsmell, that smell travels some distance and then falls off, just like sound. Additionally,\nstronger smells might overpower weaker smells, so consider modeling this feature as\nwell. Smell might not be interesting in every game, but it can be extra information\nthat can add to the identification of an object when vision isn’t sufficient, thus open-\ning up more gameplay opportunities.\nUnified Sensing Model\nHaving created sensing models for vision, hearing, smell, and a sixth sense, the final\ntask is to combine them into a single unified sensing model. The motivation is that all\nsenses should together inform the agents of their surroundings, combining their clues\ninto a complete picture of the current situation as best the agents can sense.\nBecause this example has been working with percentages representing certainty,\nthe natural extension is to combine them in some way. There are three options. The\nfirst is to take the maximum certainty between the vision, hearing, and smell senses,\nas shown in the left diagram in Figure 3.2.7. For example, if a vision zone has 30%\ncertainty and hearing is 50% certainty, that zone would have max(30%, 50%) = 50%\ncertainty. The second option is to add the certainty of all senses, as shown in the mid-\ndle diagram of Figure 3.2.7. For example, if a vision zone has 30% certainty and hear-\ning is 50%, that zone would have 30% + 50% = 80% certainty. The third option is to\ntake the vision model certainties and add half of the remaining headroom for hear-\ning/smell, as shown in the right diagram of Figure 3.2.7. For example, if a particular\nvision zone had 30% certainty, hearing a sound in that zone would add (100 – 30) /\n2 = 35% resulting in a total 75% certainty. This last option avoids having any zone\nwith greater than 100% certainty.\nTo understand the repercussions of this unified sensing model, consider the white\ncircle in Figure 3.2.7 to be the player. If the player was both quiet and motionless, he\nwould go completely undetected by the agent with a certainty of only 30%. If the\nplayer makes a loud noise, he is identified at 50%, 80%, or 75%, respectively, using\n226\nSection 3\nAI \n\n\neach method. This might result in the agents turning their heads in response. If the\nobject was running and fired a loud shot, it would be identified at 80%, 100%, and\n90%, respectively, using each method. In this case, the agents might turn their heads\nand bodies quickly and shoot once they are facing the player.\nAdding Memory to the Unified Sensing Model\nTo achieve a greater degree of realism, agents must have short term memory to augment\ntheir sensing model. This is necessary in order for agents to not forget about objects that\nthey have recently identified. For example, if the player moves quickly through the mid-\nperipheral vision of the agent, the player will be identified at 100%. If the player out-\nruns the agent, moving into the far-peripheral vision area and stops, the memory of the\nplayer at 100% identification needs to be retained for some period of time (even though\nthe player should now technically be identified at only 30%). This makes sense, because\nthe agent identified the player and still has visual contact, making it reasonable to\nassume that it is the same object that is still fully identified.\nIn order to implement this type of memory, each object that enters the sensing\nmodel needs to be tracked. The object should have some unique identification num-\nber that can be associated with varying levels of identification. A timestamp and the\nlocation of the object’s last known position should also be recorded. This information\nwill be stored in the agent. The general rule is to allow only the certainty level to\nincrease, as clues only add to the knowledge of the object. Once the object is not\nsensed for several seconds, the structure can be purged from memory.\n3.2\nDesigning a Realistic and Unified Agent-Sensing Model\n227\nFigure 3.2.7\nThree examples of the unified sensing model combining vision with\nhearing/smell. Hearing has a max certainty of 50% with no falloff shown. The left diagram\ntakes the max sense (vision, hearing) from each zone. The middle diagram adds vision with\nhearing. The right diagram takes vision and adds half of the remaining overhead due to\nhearing (to avoid certainties above 100%). In this example, the white circle is the player\nmaking a loud sound, which results in 50%, 80%, and 75% certainty respectively in each\n\n\nConclusion\nThe unified sensing model brings together vision, hearing, smell, and even a sixth\nsense to give game agents a coherent and detailed view of the game world. Many very\ncompelling features have been folded into the model, such as movement detection,\nhiding, and alertness, which allow for very rich and interesting gameplay to emerge.\nAs players better understand the underlying sensing model, they can devise innovative\nways to manipulate and deceive the agents, which adds greatly to the quality of the\nexperience.\nAs presented, the unified sensing model is intended to bring subtlety and added\nrealism to game agents. However, it is a very flexible model. The zones and percent-\nages given are simply suggestions and they will inevitably need to be tweaked for any\nparticular game. Consider each of the tools at your disposal and create your own sens-\ning model that matches and enhances your particular game design.\nReferences\n[Leonard03] Leonard, Tom. “GDC 2003: Building an AI Sensory System: Examining\nthe Design of Thief: The Dark Project,” available online at http://www.gamasutra.\ncom//gdc2003/features/20030307/leonard_01.htm#, 2003.\n[Orkin05] Orkin, Jeff. “Agent Architecture Consideration for Real-Time Planning in\nGames,” AIIDE Proceedings, Artificial Intelligence for Interactive Digital Enter-\ntainment Conference, 2005.\n[Rabin05] Rabin, Steve. Introduction to Game Development, Charles River Media,\n2005.\n[Tozour02] Tozour, Paul. “First-Person Shooter AI Architecture,” AI Game Program-\nming Wisdom, edited by Steve Rabin, Charles River Media, 2002, pp. 387–396.\n[Wandell95] Wandell, Brian. Foundations of Vision, Sinauer Associates, 1995.\n228\nSection 3\nAI \n\n\n229\n3.3\nManaging AI Algorithmic\nComplexity: Generic\nProgramming Approach\nIskander Umarov\nAnatoli Beliaev\nD\nuring the past seven years, TruSoft International Inc. has been focusing on the\nresearch and development of behavior-capture AI technologies. These technolo-\ngies allow a new type of AI game agent to be created that can learn and adapt in the\nway real humans do. They do this by learning the playing styles of human players and\nadapting these strategies to achieve set goals.\nThe system allows game designers to train behavior-capture AI agents directly, by\nsitting down with a console or a PC and playing the role of the agent to be trained.\nAgents can then learn tactics and strategies straight from the human controller, with-\nout the need for coding. End users can also train game characters using the same sys-\ntem, bringing traditional bot development to a whole new level. By simply playing\nthe game, a behavior-capture enabled system allows the users to create AI-controlled\nagents that play with very distinct styles.\nIntroduction\nDuring the work on our behavior-capture AI technology—Artificial Contender—we\nencountered an interesting challenge common to many AI systems. Sometimes the\ncomplexity of AI decision-making related algorithms grows out of control. They start\nas a simple piece of code and end up as chaos in the form of handcrafted loops and\nbranches. We needed a method of managing this complexity without introducing sig-\nnificant abstraction penalties.\nThis Artificial Contender technology is an instance-based learning system. It col-\nlects instances of learned behaviors and utilizes them during the decision-making\nprocess. The data collected while learning may not be applicable directly to the cur-\nrent situation. The learned instances are reevaluated. Possible actions can be filtered\nout or modified, generalized or specialized, and the priorities can be adjusted. An\naction should be chosen from a group of actions, and, if the action is not good\n",
      "page_number": 255,
      "chapter_number": 27,
      "summary": "This chapter covers segment 27 (pages 255-262). Key topics include sound, modeling, and game. Modeling Human Vision with Certainty\nAs observed previously, the fact that objects are either seen completely or not seen at all\nis an unfortunate side effect of discrete vision tests.",
      "keywords": [
        "vision",
        "Unified Sensing Model",
        "model",
        "vision model",
        "sensing model",
        "Certainty",
        "sound",
        "agent",
        "hearing model",
        "Human Vision",
        "game vision models",
        "zone",
        "game",
        "Hearing",
        "object"
      ],
      "concepts": [
        "sound",
        "modeling",
        "game",
        "certainty",
        "certainties",
        "agent",
        "zones",
        "make",
        "sense",
        "sensing"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 51,
          "title": "Segment 51 (pages 433-440)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 53,
          "title": "Segment 53 (pages 511-519)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 57,
          "title": "Segment 57 (pages 483-490)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 22,
          "title": "Segment 22 (pages 205-213)",
          "relevance_score": 0.46,
          "method": "api"
        }
      ]
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 263-270)",
      "start_page": 263,
      "end_page": 270,
      "detection_method": "topic_boundary",
      "content": "enough, another group of actions should be analyzed. You might need to apply differ-\nent algorithms for the next group, or different filtering criteria. However, if the next\ngroup does not contain better actions, you might need to reconsider actions from the\nprevious group, compare the consistency of data from each group, and so on. Algo-\nrithms of this level of complexity are quite typical.\nHow might you go about solving this problem? Let’s consider the most straightfor-\nward implementation first. No over-design, no premature optimization. When you have\nto deal with a group of actions, you just create a container of objects describing actions.\nWhen you have to iterate through actions, you implement a loop. When you have to\niterate through a subset of actions, you implement a nested loop. When you have to fil-\nter actions, you check the necessary conditions inside the loop. Trying to implement it\nthis way, we ran into problems:\n• Each part of the algorithm increases the complexity of the implementation. The\ncode becomes difficult to follow. The approach that appeared simplest and the\nmost straightforward leads to very complex code.\n• Maintaining the code in this form becomes very expensive. It is difficult to\nunderstand and change. Debugging it is very challenging.\n• It also becomes increasingly difficult to optimize the performance of this algo-\nrithm. It is difficult to find performance bottlenecks. It is difficult to change the\ncode and make sure it is still correct. It is difficult to create customized versions of\nthe code, optimized for different environments and conditions.\n• The pieces of code are tightly coupled, which makes it impossible to reuse them.\n• The risk of introducing bugs while making changes is high and unit testing does\nnot eliminate the risk, because often it becomes difficult to determine the\nexpected results for the entire algorithm.\nHow do you manage this complexity? The most obvious answer is to use decompo-\nsition. There are different ways to decompose. We wanted to make the implementation\neasy to understand, modify, and reuse. We wanted to be able to build these algorithms\nquickly, and make fast and safe changes. On the other hand, we could not sacrifice the\ntechnology’s performance characteristics. When you decompose a system into compo-\nnents, you have to deal with inter-component communication issues—sending data\nback and forward, converting data, and so on. Artificial Contender processes a lot of\ndata, and introducing even a little overhead to processing every single data item can lead\nto unacceptable processing time and resource consumption increases.\nAction Choosing Workflow\n“Pipes and Filters” Design Pattern\nWe have found that workflow is the most appropriate metaphor for representing this\nclass of algorithms. The idea is based on the well-known “Pipes and Filters” design\n230\nSection 3\nAI \n\n\npattern [Buschmann96]. Consider the reasons that make this design pattern a good\nfit, as follows. (Note that we use the term “block” instead of the term “filter” from the\noriginal “Pipes and Filters” pattern, in order to avoid ambiguity—filter in our work-\nflow metaphor is a block of a special type.)\n• We want to construct workflows out of separate blocks.\n• Each block should be responsible for exactly one aspect of the algorithm.\n• Each block should have well-defined inputs and outputs.\n• The structure of the workflow should be homogeneous, so that we can connect\nblocks in different ways, unless the nature of the implemented aspects does not\nallow the blocks to be connected.\n• We want to be able to create non-linear workflow configurations, containing\nbranches, merges, and loops.\n• We want to be able to change workflow configurations with minimum effort.\nThe advantages of the “Pipes and Filters” pattern are well known [Buschmann96]:\nflexibility of the processing line configuration, reusability of the components, potential\nparallel processing, and so on. Let’s see how this pattern can help in this case, setting\naside the implementation issues for now, but remembering the priorities here—manage\ncomplexity and do not sacrifice performance.\nThe following benefits help to manage complexity:\n• Separation of concerns. The processing algorithm is broken into a sequence of indi-\nvidual transformations. Every transformation is a distinct and independent task.\n• Modularity. Every processing task is encapsulated by a separate block, which can\nbe coded independently.\n• Reduced coupling. Blocks communicate only through well-defined channels. Nor-\nmally, blocks do not share state and are unaware of surrounding blocks’ implemen-\ntations, as long as the surrounding blocks adhere to the common requirements.\n• Testability. Every block can be tested independently. It is much easier to specify the\nrequired results for a separate simple task than for the entire algorithm. If it is easy\nto change inputs and check outputs, each block can be treated as a black box. It is\nalso possible to test the result of cooperation of any combination of the blocks.\n• Configuration flexibility. It is possible to build different configurations out of the\nsame set of blocks. It is also possible to compose simpler blocks into aggregates\nthat can be, in turn, treated as more complex blocks.\n• Specialization flexibility. Blocks can have alternative replaceable implementations,\nspecialized for different environments.\n• Reusability. Low coupling allows you to treat blocks as standalone modules that\ncan be easily reused. Avoiding extra dependencies makes blocks more adaptable.\nThe following benefits help to improve performance:\n• Parallelism. Blocks performing incremental processing do not have to wait until\nthe surrounding blocks complete their calculations; they can continue working\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n231\n\n\nconcurrently. This makes the workflow significantly faster, because it takes advan-\ntage of multiprocessor architectures.\n• Specialization flexibility. You can create alternative implementations of blocks\nspecifically for the purposes of performance improvement.\n• Performance profiling. Breaking the processing down into separate components\nmakes it easier to profile the code and find performance bottlenecks.\n• Partial results. This aspect is very important for the Artificial Contender decision-\nmaking algorithms, and is discussed it in more detail in later sections.\nPartial Results\nArtificial Contender decision-making algorithms operate on sequences of data describing\npotential actions. The lowest blocks (sources) generate action data sequences, usually\nextracting data stored in the knowledge database, or based on statistics, or suggested \nby heuristic rules. Querying some sources is expensive in terms of processing time. If\nthe current game situation is well known, only the data from the “cheapest” sources is\nnecessary. Only if there is no exact match for the current game situation, is it necessary to\nquery more sophisticated and expensive sources.\nIn most cases the complete action data sequences are not really needed. Partially\ncalculated sequences may be enough to make the final decision, and expensive calcu-\nlation can be avoided. If an action is good enough, it can be accepted and calculations\ncan be stopped. If you calculate everything in advance, it is very probable that you’ll\nhave to throw away most of the calculated results anyway, and you cannot afford that.\nIt is possible to implement the workflow in a way that the higher blocks control the\nexecution flow. It is up to higher blocks to decide how, when, and for how long they\nwant to continue getting the results from lower blocks. They can interrupt querying\nlower blocks at any moment, or even not start querying some of the lower blocks. If\npossible, the lower blocks should not pre-calculate the results until they are asked.\nThere is one more consideration. Working in real-time environments, sometimes\nit is better to make a decision that is not perfect, but acceptable, than to spend more\ntime calculating. It is possible to arrange the blocks of the workflow in such a way that\nthey calculate and defer the “acceptable but not perfect” actions first, and then con-\ntinue calculations while it is not too late to act, and to accept one of the deferred\nactions if nothing better has been found.\n“Pipes and Filters” Liabilities\nHowever, the “Pipes and Filters” pattern is not free from liabilities. Let’s take a look at\nthem and what can be done to minimize them.\n• Sharing state information. If the blocks need to share state information, it can be\ninefficient or inflexible. It does not seem to be a serious issue for this application,\nbecause most of the blocks do not need to share any state information directly. In\nsome exceptional cases, you can relax the “Pipes and Filters” restriction and allow\n232\nSection 3\nAI \n\n\nblocks to communicate in special efficient ways, bypassing the “official” block\ninputs and outputs. Those exceptions should be very rare, though.\n• Communication and data transformation overhead. Blocks have to exchange data,\nand this can incur some overhead. Data manipulations that do not contribute\ndirectly to the implemented algorithm may be required. If the blocks are inter-\nchangeable, they have to agree upon a common communication protocol, some-\ntimes as low as a character stream. Serializing and parsing can be expensive\nenough to make the pattern not applicable. We are going to address this issue and\nminimize or completely eliminate this overhead.\n• Parallel processing disappointments. For different reasons, the performance of par-\nallel processing can be disappointing [Buschmann96]:\n• It can happen because of the communication overhead, but we have already\npromised to minimize it.\n• It can also happen because of architecture-dependent reasons, such as context-\nswitching and synchronizing overhead, and we are not going to consider these\nissues for now, because they seem to be common for all parallel-processing\nsolutions.\n• It can happen because of the nature of the processing, or because of bad cod-\ning, like when a block consumes all input data before emitting any output data.\nThis block can become a performance bottleneck of the entire workflow. In\norder to avoid this problem, you should make processing incremental when-\never possible.\n• Complex flow control logic. The workflow processing nature is mostly sequential,\nso it becomes difficult to implement branches, loops, and other complex con-\nstructs. However, in this application, we were almost always able to rethink and\nredefine algorithms in terms of sequential processing. These new definitions are\nvery beneficial themselves. When a complex task that seems to require complex\ncontrol of the execution flow is transformed to a sequence of simpler steps, it def-\ninitely improves the internal quality of the implementation. In rare cases when\nyou are unable to do it, special constructs outside of the traditional “Pipes and\nFilters” patterns can be used.\n• Error handling. In general, error handling can be quite complicated in the “Pipes\nand Filters” pattern. However, the nature of Artificial Contender decision-making\nalgorithms does not involve any recovery after errors. So, the whole error-handling\nissue is not important for this application.\n• Complexity, increased maintainability efforts. The pattern introduces its own complex-\nity and maintainability efforts, because decomposing a monolithic implementation\ninto multiple components increases the number of components and dependencies.\nThis problem is not specific to this pattern, it is rather a consequence of any decom-\nposition. Our solution is as simple and lightweight as possible.\nLet’s take a closer look at the Artificial Contender decision-making workflows\nand their specific requirements.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n233\n\n\nWorkflow Diagram\nAny workflow can have a visual representation, for instance a block diagram. We devel-\noped a special graphical language that allows you to express different workflow configu-\nrations in a very compact and vivid form. Using different shapes and connecting lines,\nyou can illustrate sequences of processing steps, data flows, and interdependencies.\nFigure 3.3.1 shows an example of a workflow of average complexity. The descrip-\ntion of this workflow in English would be cumbersome and perplexing. However, for\ndevelopers familiar with these diagrams, it is quite easy to understand what is going on.\n234\nSection 3\nAI \nFigure 3.3.1\nArtificial Contender decision-making workflow example.\n\n\nThese diagrams are compact and readable. They also make it very easy to modify\nworkflows. You can swap the blocks around, rearrange and reconnect, add and remove.\nFor example, you may want to modify actions before or after filtering. Take a look at\nthe diagram, and you will know what the current workflow does. If you want to change\nthe order, just reconnect the blocks. Compare this to the first straightforward imple-\nmentation of the algorithm. How long would it take to change the order of action\nfiltering and modification? How can you make sure that the change is correct? The\nworkflow diagrams make the answers obvious.\nExecution Flow\nA workflow diagram shows just a static picture of the workflow. It represents the work-\nflow configuration in a declarative manner, but it does not illustrate the actual execution\nsequence. It is enough when the reader of the diagram knows the basic rules. Omitting\nthe details of data and execution flows is what makes the diagrams compact and expres-\nsive. But what is going on here?\nBlocks work with sequences of separate objects representing some knowledge\nabout a single action or a set of actions. These objects are called ActionInfo. Blocks\nconsume, process, and emit sequences of ActionInfo objects. ActionInfo objects travel\nfrom lower blocks to higher blocks. On their way, they can be transformed to other\nActionInfo objects, they can be filtered out, they can be split into sets of separate\nActionInfo objects, they can be merged with other objects, and so on.\nHow and in what order do blocks process ActionInfo objects? To answer this\nquestion, consider a very simple workflow shown in Figure 3.3.2.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n235\nFigure 3.3.2\nVery simple workflow.\nThis is how this workflow is supposed to work:\n• Source generates ActionInfo objects (based on, for example, the Artificial Con-\ntender knowledge base).\n• Modifier changes ActionInfo objects, adjusting them to the current game situation.\n\n\n• Filter checks whether ActionInfo objects are good enough and lets them through\nor filters them out.\n• Acceptor accepts the first ActionInfo that is emitted by Filter.\nYou could make it work in exactly that sequence, implementing the “push”\nmodel—start processing from Source, pass the data generated by Source to Modifier,\nand so on. This is how many implementations of the “Pipes and Filters” pattern work.\nHowever, this is not what you need for the Artificial Contender system. Querying\nsome of the knowledge sources can be expensive in terms of performance.\nFirst of all, it might not be necessary to query all of them. Second, you might not\nneed the whole sequence of ActionInfo from each of them. The workflow in this\nexample may accept the first ActionInfo generated by Source if it makes it through\nFilter; in that case there is no need to generate more than one ActionInfo. But Source\nis not supposed to be aware of that fact, so you cannot let Source decide when to\ngenerate the whole ActionInfo sequence. Higher blocks must be able to decide which\nlower blocks to query and when to stop. Although it is theoretically possible to imple-\nment this using the “push” model, the “pull” model looks more natural:\n• Acceptor asks for one ActionInfo from Filter.\n• Filter asks for ActionInfo from Modifier and checks them one by one, looking for\nActionInfo that should be returned to Acceptor.\n• Modifier queries Source retrieving ActionInfo one by one, modifies them, and\nreturns to Filter in the same manner: one by one.\n• Source answers Modifier’s requests, emitting ActionInfo objects one by one.\n• As soon as the top block (Acceptor) stops asking for more ActionInfo, the work-\nflow stops.\nFigure 3.3.3 shows the sequence diagram of a “pull” workflow.\nTypical Blocks\nThere are a few categories of blocks that you usually need for AC decision-making\nalgorithms:\n• Sources generate ActionInfo objects based on data external relative to the decision-\nmaking workflow: from AC knowledge database, from heuristic algorithms, from\nstatistics tables, and so on.\n• Filters determine whether consumed ActionInfo satisfy specific conditions, and\noutput or ignore these ActionInfo. Usually filters make sure that the potential\nactions are applicable to the current game situation.\n• Modifiers change consumed ActionInfo objects and output the changed objects.\nFor example, they can adjust the actions retrieved from the knowledge according\nto the current game situation, or they can adjust priorities of the actions.\n236\nSection 3\nAI \n\n\n• Sorters consume sequences of ActionInfo objects and output the same objects in a\nnew order. For example, they can sort actions by priorities, by estimated effect, by\ncategories, and so on. Sorting criteria can be very flexible and do not have to spec-\nify a deterministic order. They can perform weighted random reordering, thus\nintroducing more variety into AC agent’s behavior.\n• Splitters divide the incoming flow of ActionInfo objects into multiple flows and\nredirect these flows to multiple outputs. Splitters are used to separate some\nActionInfo objects for special processing.\n• Mergers have multiple inputs and redirect the flows of ActionInfo objects from all\ninputs to a single output. They are often used to query a set of ActionInfo sources\nsequentially and join the results into one set.\n• Selectors have multiple inputs and redirect the flow of ActionInfo objects from\none of the inputs to a single output. Usually, selectors have a special control chan-\nnel that makes it possible to switch the active input. They are also often used to\nquery a set of ActionInfo sources sequentially. But, as distinct from mergers, they\nallow treating the ActionInfo objects from each source as a separate group.\n• Repeaters consume and output all available ActionInfo objects, and then perform\na specified action, and then consume and output all available ActionInfo objects\nagain, and so on. Cooperating with selectors, repeaters make it possible to imple-\nment loops that are querying and processing actions from different sources.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n237\nFigure 3.3.3\nPull workflow.\n",
      "page_number": 263,
      "chapter_number": 28,
      "summary": "This chapter covers segment 28 (pages 263-270). Key topics include block, processes, and processing. However, if the next\ngroup does not contain better actions, you might need to reconsider actions from the\nprevious group, compare the consistency of data from each group, and so on.",
      "keywords": [
        "blocks",
        "ActionInfo objects",
        "ActionInfo",
        "Workflow",
        "objects",
        "Pipes and Filters",
        "Filters",
        "Artificial Contender",
        "actions",
        "data",
        "processing",
        "Pipes",
        "make",
        "Generic Programming Approach",
        "lower blocks"
      ],
      "concepts": [
        "block",
        "processes",
        "processing",
        "process",
        "workflow",
        "actions",
        "data",
        "sources",
        "complexity",
        "filters"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "Segment 2 (pages 9-17)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 583-590)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "Segment 62 (pages 607-613)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "AntiPatterns",
          "chapter": 14,
          "title": "Segment 14 (pages 114-126)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 60,
          "title": "Segment 60 (pages 591-598)",
          "relevance_score": 0.48,
          "method": "api"
        }
      ]
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 271-279)",
      "start_page": 271,
      "end_page": 279,
      "detection_method": "topic_boundary",
      "content": "Generic implementations of the frequently used blocks are included in the Artifi-\ncial Contender SDK. However, this is not an exhaustive list of block types. It is possi-\nble to develop more customized and specialized blocks. Combining these blocks into\ndifferent configurations, you can build very versatile and flexible workflows.\nConstraints\nBlocks can be connected in many different ways. However, system freedom is not\nunlimited. Some combinations do not make sense, because the nature of the blocks\ncan be very different. For example, if you want a block to perform a weighted random\nchoice of an action, you have to make sure that the inputted ActionInfo objects have\nweights associated with them. You want to be able to express these constraints and\nassociate them with blocks. Then you can visualize the restrictions and check them\nautomatically, ensuring the correctness of the workflow.\nImplementation\nGeneric Programming and C++\nWe chose C++ as the main implementation language for Artificial Contender. This\nlanguage provides tools to deal with abstractions, and still allows control over low-\nlevel implementation details in order to achieve high performance. Using C++, we\ntake advantage of the “Pipes and Filters” pattern benefits, and overcome the potential\nperformance hits at the same time.\nGeneric programming helps us achieve both goals simultaneously. Implementing\ncode in a very general and abstract form without sacrificing efficiency is one of the key\nideas of generic programming [JLMS98].\nIn order to make the workflow flexible enough, we apply generic programming\nprinciples while designing workflow blocks. Block implementation should make min-\nimal assumptions about the surrounding environment. The less it relies on implemen-\ntation details, such as concrete data types, the more adaptable the implementation is.\nBlocks that have well-defined orthogonal responsibilities should be aware only of the\ndetails directly related to those responsibilities, and in the most generic way. The main\nrule while designing a block is no over-specification. If the essential functionality of the\nblock does not depend on a particular data type, do not even mention this data type\nin the implementation. If it depends on a data type, but still is able work with differ-\nent types, make this data type a parameter. This lack of concrete details can make the\nimplementation look a little bit vague, but in fact it is exactly the opposite—it\nbecomes succinct and precise.\nPolymorphic Workflow Blocks\nThe most common requirements to workflow blocks is that they should process and\noutput data. If necessary, blocks can also consume data generated by other blocks. At\nthe same time, it must be possible to connect blocks in different ways.\n238\nSection 3\nAI \n\n\nThe Dependency Inversion Principle [Martin02] states that blocks should not\ndepend directly on each other. Instead, they should depend on abstract requirements\nto input and output. In this case, changing one block does not require changing other\nblocks, as long as all the blocks satisfy the requirements. How abstract are the require-\nments? More abstraction gives more flexibility, but makes it more difficult to ensure\nthat the developers of the blocks have enough information to really satisfy the require-\nments in the concrete implementation. You have to balance these issues, considering\nthe requirements and the tools’ limitations.\nAlthough the nature of blocks can be absolutely different, they still have a common\nproperty: the ability to input and output data items. If you implement this property\nsimilarly in all blocks, it will give you an opportunity to build different configurations\nfrom the same blocks. You can then connect and reconnect blocks without taking care\nof different input/output interfaces.\nBecause we’re implementing the “pull” model, blocks should only provide a com-\nmon way of getting data. If the blocks know how other blocks output data, they auto-\nmatically have a way to input data. We’ll use some form of polymorphism in order to\nmake this process look unified.\nHow do you make the blocks polymorphic? Developers with object-oriented\nbackground might already have an answer—unified interfaces based on virtual func-\ntions or another form of dynamic binding.\nA block that requires input can hold a reference to an object implementing the\nBase interface. The block does not need to know the concrete type of other blocks; it\ncan just rely on the interface. See the following code:\nclass BaseBlock {\npublic:\nvirtual OutputData getData() = 0;\n};\nclass Modifier : public BaseBlock {\npublic:\nModifier(BaseBlock& input) : input_(input) { }\nvirtual OutputData getData() { return modify(input_.getData()); }\nprivate:\nBaseBlock& input_;\n};\nIt looks flexible enough, right? But it will not do for this implementation. Why\nnot? The performance of this system is not high enough.\nPolymorphism based on virtual functions can introduce significant performance\nhits. If you follow the Single Responsibility Principle [Martin02] and make the blocks\nfine-grained, you will end up having a lot of functions with simple or even trivial\nimplementation.\nSometimes you can ignore the overhead of indirect function calls, but you defi-\nnitely cannot ignore the fact that these indirect calls create optimization bottlenecks\nfor compilers. Usually, compilers can optimize a sequence of static function calls. If\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n239\n\n\nthe function definition is visible, it is possible to inline it, eliminate the overhead of\npassing parameters and returning results, and generate very compact and efficient\nexecutable code. However, if the compiler does not know which function implemen-\ntation is going to be called, inlining is not an option anymore, and all this overhead is\nnecessary. \nIn the previous code example, how would you define the OutputData type? You\nhave a similar challenge here. The output data objects should be either fixed or poly-\nmorphic, because the blocks should process data objects that are acquired from other\nblocks. However, the problem looks even more severe in this case. Every block can\nexpect different properties from input data objects. There are almost no common\nrequirements. Most of the requirements are block specific, and do not make any sense\nin the context of other blocks. If you fix the data type, the type will have to imple-\nment all imaginable function and data member placeholders. Only some of them will\nbe really used, and (even more scary) some of them must not be used until initialized\nproperly. Instead, you could try to extract the interface, covering everything possible,\nand then build the inheritance hierarchy and override virtual functions, providing\nstub implementations of methods that are not “legal” for particular subtypes. How-\never, it provides perfect opportunities to violate the Liskov Substitution Principle and\nsuffer from the Refused Bequest code smell. And, even if you manage to implement it\nthis way, you’ll run into the performance issues described previously.\nWhy not use other forms of polymorphism? You could use the unbounded\ndynamic polymorphism, similar to the one available in dynamically typed languages,\nsuch as Smalltalk and Ruby. These approaches require additional work for C++ devel-\nopers, but are definitely possible. You could even introduce reflection and runtime\nmeta-programming capabilities, analyze block requirements dynamically, and build\nappropriate objects in runtime. \nThe main obstacle is the same: performance. Although they are incredibly flexible,\nall kinds of dynamic polymorphism, both bounded and unbounded [Czarnecki00], do\nnot seem to be applicable to this problem, mostly because of performance overhead.\nThe more fine-grained the blocks are, the more visible the performance hit becomes. \nAlso, it would make the workflows too flexible. You would not be able to rely on\nstatic type checking anymore, and you would have to execute the workflow just to\ndetect obvious constraint violations. This would make designing workflows over-\ncomplicated, which defeats the purpose of the solution. Reflection and runtime meta-\nprogramming would make the performance problems even worse.\nAll types of dynamic polymorphism provide you with the ability to substitute\nobjects of different types at runtime, but they make you pay for this ability with per-\nformance. You do not want to pay for flexibility that you are not going to use and nor-\nmally you do not need to change the configuration of the workflow at runtime. You\nneed as much flexibility as possible while the workflow is designed, but you do not\nneed to change it after the code is compiled.\n240\nSection 3\nAI \n\n\nStatic polymorphism can help you move as much work as possible from runtime to\ncompile-time. This is why we choose the generic programming approach based on\nC++ templates. In this scenario, you still can construct workflows out of fine-grained\nblocks, but you do not have to pay for that with processing time or memory. Also,\ncompile-time type safety is intact. Let’s use this idea to implement the Modifier block\nagain, as follows:\ntemplate <typename Input>\nclass Modifier {\npublic:\nModifier(Input& input) : input_(input) { }\nOutputData getData() { return modify(input_.getData()); }\nprivate:\nInput& input_;\n};\nNo inheritance, no virtual functions, but still polymorphic. The “implement\nBaseBlock interface” requirement is replaced with a fuzzy, but much more flexible\none: “implement getData function returning an object that behaves like OutputData.”\nImplementing static polymorphism, you are not losing the opportunity to return\nto dynamic polymorphism when it is more appropriate (for example, if it is necessary\nto change the workflow configuration at runtime). You do not have to change the\nblock implementation, you just need a simple adapter that converts the statically\npolymorphic interface to a similar dynamically polymorphic interface, based on vir-\ntual functions or message dispatching. See the next example:\ntemplate <typename AdaptedBlock>\nclass Adapter : public BaseBlock {\npublic:\nAdapter(AdaptedBlock& adapted) : adapted_(adapted) { }\nvirtual OutputData getData() { return adapted_.getData(); }\nprivate:\nAdaptedBlock& adapted_;\n};\nModifier modifier;\nAdapter<Modifier> adaptedModifier(modifier);\nSimilar adapters can be implemented for unbound dynamic polymorphism, too.\nObviously, the performance issues come back when these adapters are used. But now\nyou have a choice; you can use it only when necessary.\nActionInfo Flow\nWhat does the output data look like? You need some degree of polymorphic behavior\nfrom ActionInfo objects. However, the performance considerations should steer you\ntoward avoiding the involved overhead. The ActionInfo implementation details are\ndiscussed later. For now, assume that you have already defined the ActionInfo type\nthat is generic enough to satisfy the requirements of every block in the workflow.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n241\n\n\nMost of the time you’ll work with sequences of ActionInfo objects. How should\nyou store the sequences? How do you pass them between blocks? Should you pre-\nallocate memory before querying a block? Should the called block allocate memory\nitself? Acquiring the whole sequence might be expensive; should you do that if you\nmight end up choosing the first action anyway?\nFortunately, most of the time you don’t really need the whole sequence. At least,\nnot at the same time. In most cases, you can process ActionInfo objects one by one,\nand make appropriate decisions regarding these objects separately. Instead of deciding\nhow to store and pass the data that you might not even need, you can pass functions\ninstead of requesting data. We applied the “tell, don’t ask” approach. Instead of asking\nfor all data, you tell the block what to do with the data and when to stop.\nReplace the block-interface requirements with the following:\n• The block should implement a function with a fixed name (called forEach).\n• The forEach function should accept a function as a parameter.\n• The forEach function should apply the received function to every ActionInfo\ninstance that should be emitted by the block.\nThis implies that the function passed to forEach should be able to accept Action-\nInfo objects as a parameter. Note that we don’t specify any types in the requirements,\nso the blocks are free to implement the forEach function and the callback function in\ndifferent ways.\nBlock Implementation Examples\nThis next listing shows what source blocks (blocks that do not require input) look like:\nclass Source {\npublic:\ntemplate <typename F>\nvoid forEach(F f) const {\n...\nf( generateNext() );\n...\n}\n};\nYou want the forEach function to be able to accept any invokable entity, includ-\ning functions and function objects (functors); this is why F is a template parameter.\nNote that the caller of the forEach function does not have to worry about allocat-\ning storage for new ActionInfo objects. The Source block manages this storage and\ncan reuse it for every next ActionInfo. Normally, blocks do not have to store previous\nActionInfo objects, unless it is required by the nature of the block (for example, sort-\ning blocks usually must collect all input ActionInfo before emitting the first output\nActionInfo). This helps minimize the data transfer overhead.\n242\nSection 3\nAI \n\n\nIf all the blocks satisfy these requirements, the blocks that require input can\nexpect that the other blocks implement a similar forEach function. The following\ncode listing shows a typical modifier’s forEach function implementation:\ntemplate <typename F>\nvoid forEach(F f) const {\ninput_.forEach(ApplyToModified<F>(f));\n}\nThe input_.forEach call ensures that ActionInfo objects are retrieved from the\ninput block. The ApplyToModified functor’s purpose is to modify each ActionInfo\nobject received from input_ and apply the original F function to the modified Action-\nInfo, as follows:\ntemplate <typename F>\nclass ApplyToModified {\npublic:\nApplyToModified(F f) : f_(f) { }\nvoid operator()(const ActionInfo& ai) const { f_(modify(ai)); }\nprivate:\nF f_;\n};\nThe following code listing shows a typical filter implementation:\ntemplate <typename F>\nbool forEach(F f) const {\nreturn _input.each(ApplyIfAcceptable<F>(f));\n}\ntemplate <typename F>\nclass ApplyIfAcceptable {\npublic:\nexplicit ApplyIfAcceptable (F f) : f_(f) { }\nvoid operator()(const ActionInfo& ai) const {\nif (isAcceptable(ai)) f_(ai);\n}\nprivate:\nF f_;\n};\nNote that there is no physical copying of data here. The ActionInfo objects created\nby the input block are checked in place, and the original function may be applied. Of\ncourse, you do not know anything about the original function, and it might copy or\nconvert data for its own purposes. But there is no copying or conversions for the Filter\nblock purposes, which means that the performance overhead is completely eliminated.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n243\n\n\nPartial Results\nOne of the most important requirements is the ability to interrupt the workflow\nwhen an acceptable ActionInfo is found, or when there is not enough time to com-\nplete the entire decision-making process. It is easy to implement: just make the func-\ntion passed to forEach return a Boolean value, indicating whether the block is allowed\nto continue emitting ActionInfo objects or not. Each time the function is called, the\nforEach implementation should check the result and exit as soon as possible when\nnecessary. It will effectively stop the whole workflow.\nThe sequence diagram shown in Figure 3.3.4 illustrates the execution flow.\n244\nSection 3\nAI \nFigure 3.3.4\nPull workflow with\ncallback functions.\nFunction Pointers versus Functors\nYou could pass function pointers to forEach functions. However, unlike calls through\nfunction pointers, calls to functors (function objects) can be inlined, efficiently elim-\ninating most or all of the function call overhead [Meyers01]. Furthermore, empty or\n\n\ntrivial implementations may be optimized away altogether. This is a very important\nway to get an abstraction bonus instead of an abstraction penalty. Imagine the imple-\nmentation of the previous workflow where all the called code is implemented “in\nplace,” even for blocks that are located very far from each other in the workflow, so\nthat there is no more need to really call functions and pass parameters. When inlined\nproperly, the whole algorithm in the resulting executable can be merged into one\nhighly optimized chunk of code that implements the necessary data processing only,\nwithout moving and converting data. In the meantime, the developers still deal with\na very high-level, abstract, and decomposed representation of this algorithm.\nThis approach has a caveat, though. Depending on your compiler, the results of\ninlining may vary.\n• First of all, the compiler might not use inlining opportunities fully and might still\nmake real function calls. You may need to experiment, measure, and tweak your\ncode and compiler settings in order to achieve the expected results.\n• Secondly, uncontrolled inlining may lead to code bloat when large functions are\nduplicated. In that case, you can always return to function pointers.\nActionInfo Type\nWe keep mentioning the ActionInfo type, but we still have not shown you its definition.\nThere is a reason: the generic ActionInfo type simply does not exist. Each block has its\nown requirements to the incoming ActionInfo flow, and each block can emit ActionInfo\nobjects having specific properties. Combining all properties in one ActionInfo type is\ninefficient and unsafe. You need to be able to define minimalistic ActionInfo types in\nthe lowest blocks (sources), and add or remove properties moving up the workflow, in\ncompile-time. How can you achieve this? Here is what we did:\n• Only sources define concrete ActionInfo types. These types do not have to be the\nsame. Each Source includes only members relevant to this source.\n• All other blocks make ActionInfo a template parameter. Then, they derive their\noutput ActionInfo type from input ActionInfo types, using inheritance or aggre-\ngation to modify ActionInfo properties. As a result, each block’s output Action-\nInfo type depends on the workflow configuration.\n• All blocks use ActionInfo properties that are directly related to a block’s responsi-\nbility and do not use any other properties. This makes the blocks very adaptable:\nthey accept different input ActionInfo types, but any unknown properties are just\npropagated to the output and can be used by higher blocks.\nAlternative Block Implementations\nAnother key idea of the generic programming idea is that you can provide alternative\nimplementations of the same generic algorithm, specialized for some particular condi-\ntions, in order to make it more efficient when possible. This approach is extremely help-\nful for our application. For example, it allows us to have different versions of the same\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n245\n\n\nblock, optimized for different platforms. These versions can be chosen either manually\nor automatically in compile-time. Also, policy-based design [Alexandrescu01] helps to\ncustomize generic block implementations partially, without re-implementing entire\nblocks and duplicating code.\nConstructing Workflows\nHow do you connect the developed blocks to each other and make the workflow run?\nIn C++, creating and connecting blocks looks as simple as the following listing:\ntemplate <typename Input>\nFilter<Input> makeFilter(const Input& input) {\nreturn Filter<Input>(input);\n}\n...\nmakeFilter( makeModifier( makeSource() ) ).forEach(AcceptFirst());\nHowever, you do not have to always do it manually. Because all blocks follow the\nsame rules, the code has a very regular structure, which makes it possible and relatively\neasy to generate it automatically from scripts or even from the visual representation.\nConstraints\nThanks to static polymorphism, you still can take advantage of C++ compile-time type\nchecking. If, for example, the Filter tries to use members of ActionInfo objects received\nfrom the Modifier, and these members do not exist or have different types, this code can-\nnot be compiled. In that case, the Filter cannot consume data directly from Modifier’s\noutput.\nSometimes it isn’t enough, though. Compiler error messages can be unreadable or\nmisleading, especially for code that makes heavy use of templates. This makes it diffi-\ncult to understand which requirement has been broken. In order to simplify the diag-\nnostics, we use C++ concept checking [Stroustrup03]. Also, we could have used the\n“Red Code, Green Code” approach suggested by [Meyers07].\nThe constraints can also be visualized on the workflow diagram:\n• A set of labels is attached to block inputs. Every label represents a requirement to\nthe incoming ActionInfo flow.\n• Another set of labels is attached to block outputs. Every label represents a prop-\nerty that is added by the block, or a property that is removed by the block.\n• When blocks are about to be connected, first of all the label should be analyzed.\nThe requirements of the higher block should be satisfied by the output of the\nlower block. If they do not contradict, the connection is established. After that, the\nlabels of the lower block’s output can be propagated to the higher block’s output.\n246\nSection 3\nAI \n",
      "page_number": 271,
      "chapter_number": 29,
      "summary": "This\nlanguage provides tools to deal with abstractions, and still allows control over low-\nlevel implementation details in order to achieve high performance Key topics include blocks, functionality, and functions. Covers function.",
      "keywords": [
        "blocks",
        "input",
        "ActionInfo",
        "function",
        "data",
        "ActionInfo objects",
        "Generic Programming",
        "cial Contender SDK",
        "Generic Programming Approach",
        "workflow",
        "make",
        "type",
        "input ActionInfo types",
        "objects",
        "Implementation"
      ],
      "concepts": [
        "blocks",
        "functionality",
        "functions",
        "function",
        "implementation",
        "implement",
        "data",
        "types",
        "typed",
        "input"
      ],
      "similar_chapters": [
        {
          "book": "A Philosophy of Software Design",
          "chapter": 5,
          "title": "Segment 5 (pages 36-43)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 44,
          "title": "Segment 44 (pages 462-467)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 7,
          "title": "Program Structure and Control Flow",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 41,
          "title": "Segment 41 (pages 342-350)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 280-291)",
      "start_page": 280,
      "end_page": 291,
      "detection_method": "topic_boundary",
      "content": "This visual representation makes the process of building workflows quite intuitive\nand straightforward.\nConclusion\nAlthough generic programming is definitely not a new idea, it still takes some non-\ntrivial efforts to grasp and apply it properly. In addition, prepare for a struggle with\nC++ compilers and other development tools. Even when they conform to the contem-\nporary C++ standard, efficiency of C++ templates support leaves a lot to be desired.\nFortunately, compilers and tools are being improved, and the upcoming new C++\nstandard is going to facilitate generic programming and template meta-programming.\nAnd the result is worth it, especially if you need to write abstract and reusable\ncode, but have very strict performance requirements. The approach described in this\ngem makes Artificial Contender very flexible, compact, and fast, all at the same time.\nReferences\n[Alexandrescu01] Alexandrescu, A. Modern C++ Design: Generic Programming and\nDesign Patterns Applied, Addison-Wesley Professional, 2001.\n[Buschmann96] Buschmann, F., Meunier, R., Rohnert, H., Sommerlad, P., and Stal,\nM. Pattern-Oriented Software Architecture Volume 1: A System of Patterns, Wiley\nand Sons Ltd., 1996.\n[Czarnecki00] Czarnecki, K., and Eisenecker, U.W. Generative Programming: Meth-\nods, Tools, and Applications, Addison-Wesley Professional, 2000.\n[JLMS98] Jazayeri, M., Loos, R., Musser, D., and Stepanov, A. Report of the Dagstuhl\nSeminar 98171 “Generic Programming,” Schloss Dagstuhl, April 27–30, 1998.\n[Meyers01] Meyers, S. Effective STL: 50 Specific Ways to Improve Your Use of the Stan-\ndard Template Library, Addison-Wesley Professional, 2001.\n[Martin02] Martin, R.C. Agile Software Development. Principles, Patterns, and Prac-\ntices, Prentice Hall, 2002.\n[Meyers07] Meyers, S. “Red Code, Green Code: Generalizing Const,” available\nonline at http://nwcpp.org/Meetings/2007/04.html, 2007.\n[Stroustrup03] Stroustrup, B. “Concept Checking—A More Abstract Complement\nto Type Checking,” Technical Report N1510, ISO/IEC SC22/JTC1/WG21,\nSeptember 2003.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n247\n\n\nThis page intentionally left blank \n\n\n249\n3.4\nAll About Attitude: \nBuilding Blocks for Opinion,\nReputation, and NPC\nPersonalities\nMichael F. Lynch, Ph. D., \nRensselaer Polytechnic Institute, Troy, NY.\nlynchm2@rpi.edu\nT\nhe concept of attitude, a positive or negative evaluation about some attitude\nobject, has a long history in psychology. Several games have used this concept in\nopinion and reputation systems, but the concept of attitude is more general than that.\nAttitude systems can be used to enrich NPC behavior in other ways besides opinion\nand reputation systems, for example, as inputs into decision tree or behavior trees, as\npart of modeling social networks, and to enrich NPC “personalities.” \nAttitude systems are more appropriate for games where NPCs need to exhibit\nbelievable social behaviors toward the player and/or toward one another. These can\ninclude game genres like god games, RPGs, dating games, games about political fac-\ntions or palace intrigue, espionage, and perhaps some types of RTSs. They may also\nbe appealing for use in certain forms of serious games, for example, games that need\nto simulate political processes, media or propaganda effects, or marketing campaigns.\nThis gem presents enough basic attitude theory to get started and suggests some\nlightweight implementations that can be used in conjunction with other parts of the\nAI.\nIntroduction\nAs game consoles and personal computers become more powerful, the prospect of\ndeveloping games with NPCs that behave something like real human (or maybe alien)\nbeings becomes more and more attractive—and more and more demanded by play-\ners. Emotions and attitudes are a large part of what makes us human, and if we are to\ndevelop human-like behaviors in our game characters, developers need to tackle these\nmessier aspects of human behavior.\n\n\nThis gem presents the psychological construct called attitude that can become\npart of your toolkit for building more interesting and human-like NPCs. In present-\ning these ideas, please note that I am going to be doing considerable violence to how\nthese concepts are actually treated in these social sciences, by drastically oversimplify-\ning a number of their more subtle aspects to the point where they will (I hope)\nbecome useful in game development. After all, for these concepts to be practical, they\nultimately have to be implemented in code.\nFortunately, game characters are caricatures of real humans: simpler, more\nextreme, and more over-the-top than real people. This should simplify the challenge.\nSo what you will be doing here is not proper science. Call it cognitive engineering\nor even psychological hacking. One good term for it is “critical technical practice,”\nfirst proposed by Agre in 1997 and quoted by Michael Mateas [Mateas02].\nMy own way of describing critical technical practice is that it is a style of engi-\nneering practice that’s informed by scientific theory but that nevertheless develops\nalong its own trajectory and builds on its own successes. It is nevertheless prepared \nto reexamine its own premises and techniques in the light of new findings. Game AI\ncertainly fills the bill.\nAttitude\nWhat is this thing called attitude? Attitude, as defined by social psychologists, has a\nmeaning much like popular usage, as in “having a positive attitude about something,”\nbut not in the sense of ‘having a bad attitude” or “copping an attitude.” The study of\nattitude in the social sciences has a long history, and there is a vast body of literature\nabout it spanning several disciplines within the social sciences.\nAcademic researchers strive to be precise in their definitions, and this is no differ-\nent for attitude. Many different definitions have been offered over the years, but one\ngood one is by Alice Eagly and Shelly Chaiken [Eagly93], who state that attitude “is a\npsychological tendency that is expressed by evaluating a particular entity with some\ndegree of favor or disfavor.” For excellent and in-depth coverage of this subject, the\ninterested reader is directed to their magisterial text [Eagly93].\nCentral to the attitude construct is the evaluative dimension. Reducing it to the\nbarest essentials, it simply means that the person holding the attitude has made a judg-\nment about the degree to which the holder likes or dislikes the attitude object; that is,\nthe person has judged how appealing or unappealing the target of the attitude is. \nAttitudes can be held for just about anything that can be evaluated; the “target” of\nan attitude is usually termed the attitude object. Attitude objects can be concrete, like\npersons, physical objects, and places; or abstract, like notions of freedom, equality,\nnationalism, or justice. Attitudes about abstract entities that imply a moral dimension\n(like freedom or equality) are usually termed values.\nThe attitude object can be a singular item or an entire category or class of related\nitems. Humans use the tendency to have attitudes about classes of objects, rightly or\nwrongly, to reduce cognitive load and streamline decision-making. The dark side of\n250\nSection 3\nAI \n\n\nthis is that it can lead to unfair prejudice and stereotyping. The bright side is that it\ncan simplify life. If one’s attitudes toward Tide detergent are that it smells nice,\nremoves stains, and cleans well, one can, by collapsing down a long chain of reasoning\nand detergents, lead to the simple behavior outcome “just buy Tide.” Humans are\n“cognitive misers;” we try to keep hard thinking down to the minimum amount\nneeded to get the job done.\nImportantly, attitudes can also be about events, attitudes about other attitudes,\neven attitudes about one’s reactions to having an attitude, and so on. When dealing\nwith human attitudes, it can get complicated very quickly.\nAttitudes are also the basis for other forms of human cognitions. One way to view\na belief, for example, is that it is an attitude about a proposition, that is, a logical state-\nment. One can believe that the “Earth Is Flat” (a proposition that can be true or false),\nor that “Bobby Cheated on Danielle” (which might also be true or false). \nAttitudes demonstrate what is sometimes called dispositional liking. Dispositional\nliking (the basis of attitude) is not the same thing as momentary liking, which is an\nimmediate emotional response to some entity. Attitudes are evaluative beliefs about\nattitude objects; they are formed though a process sometimes called ABC, for affect,\nbehavior, and cognition. Affect (accent on first syllable: AF-fect) is (briefly) the techni-\ncal term for an emotion response. Some attitudes are formed because of an emotion-\nally involving experience with an attitude object. Behavior is of course action; a series\nof pleasant dining experiences in fine restaurants can lead to a positive attitude toward\nfine dining. Cognitions are, of course, thoughts. Thinking about entities or issues can\nlead to the formation of attitudes; by thinking about what effect freedom has in\nhuman affairs may lead to positive attitudes about freedoms, and in turn, about the\npolicies and philosophies that can lead to greater freedom.\nAttitudes accumulate through a lifetime of interacting with the world. Attitudes\nform about attitude objects as you experience them, use them, or think about them.\nImmediate reactions of liking or disliking become attached to prior experiences and,\nby doing so, attitudes form and harden.\nThe distinction between dispositional and momentary liking may seem like a\npetty or unimportant point; however, you’ll see later that attitudes are more or less\nenduring, and although they can be modified by momentary experiences, they persist\nbeyond those experiences.\nStrictly speaking, this discussion is not quite correct. People do not walk around\nsporting attitude meters that we can read off directly; instead we infer that people\nhold these attitudes based on what we can observe. How exposure to an attitude\nobject (observable) leads to the activation of an internally held attitude (not observ-\nable), which can then (although not inevitably) lead to some outward expression of\nthe felt attitude (observable). The expression of an attitude can be through many\nchannels—facial expressions, posture, and other non-verbal communications, spoken\nlanguage, and actual behaviors.\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n251\n\n\nComplex attitude objects, like people, historical events, and organizations, can be\nevaluated along as many attributes about the attitude object that are of interest. You\ncan love her dimples, hate her cooking, be mildly put off by her political views, enjoy\nher taste in movies, and detest her horselaugh, all at the same time. This idea will\ncome up later when we look at the matter of love/hate.\nAlso, because an attitude represents an evaluation held by an individual about\nsomething, it is entirely subjective, and because it is, an attitude is not actually a state-\nment of truth or falseness, even if very strongly felt.\nAttitudes often (although not always) carry an emotional “charge.” Some atti-\ntudes are purely intellectual, but others are learned as a result of an emotionally intense\nexperience, and these can become among the most enduring of attitudes. In such cases,\nthe affective (emotional) reaction (along the axis of appealing/unappealing) is non-\ncognitive—we do not think about our reactions but instead experience them immedi-\nately and spontaneously. This is because such reactions involve more ancient regions\nof the brain where much of the emotional apparatus of the brain resides. Emotion is\nalso bound with memory; highly emotional events are nearly always well remem-\nbered. It is what happens in extreme cases of Post Traumatic Stress Disorder (PTSD),\nand it is also why we usually can easily remember what we were doing during 9/11,\nthe Challenger disaster, or the Kennedy assassination.\nWhat’s in an Attitude?\nFortunately, by taking some careful liberties with the rigorous social science here, one\ncan produce a reasonably lightweight model of attitude that can be used by AI game\nprogrammers. In fact, something very much like attitude has been used in a number\nof games under various guises, perhaps most notably in the Xbox game Fable [Rus-\nsell06]. Greg Alt and Kristen King mention an earlier approach used in the Ultima\nOnline series [Alt02].\nCalculations to update values in the attitude system need not be performed every\nframe; in fact, one nice thing about an attitude system is that attitudes need to be\nrefreshed only when an event in the game that can affect attitudes occurs. Russell\n[Russell06] called these opinion events. More generally, they occur at points in the\ngame story called the dramatic beat. How often do these happen? For Façade, Michael\nMateas [Mateas02] estimated that these occur about every minute or so. An attitude\nsystem is therefore not going to stress the CPU budget very much, unless you are\ndealing with an exceptionally large number of NPCs carrying around a large number\nof attitudes. If so, the updates can be spread out over multiple frames without causing\ntoo many problems or getting them too far out of synch. Memory usage, on the other\nhand, is likely to be much more of a concern. [Alt02] discusses this point at length. \nLet’s begin by assuming that each agent capable of holding an attitude will in fact\nhold a collection of attitudes for as many attitude objects as the game requires. Most\nlikely, the chief attitude object will be the human player or player character (PC), which\n252\nSection 3\nAI \n\n\nthen forms the basis for an opinion system—how the NPCs in the game world regard\nthe player along some number of dimensions that the designers feel is important.\nThe agents here are presumably all the significant NPCs in the game, but may\nalso be collections of agents, such as an entire village or even the entire game world.\nAgents that represent other forms of organizations can also hold attitudes. This was\nthe approach taken in Fable, where attitudinal information about the PC was stored\nglobally (called the hero stats), then at the village level for each of the 10 villages in the\nAlbion game world, then for each of the many significant individual NPCs [Rus-\nsell06]. This approach brought many important implementation benefits that are well\ncovered in that article.\nValence\nWhat data should an individual attitude contain? Obviously, the first item has to be\nthat evaluative dimension of liking/disliking, a value commonly called the valence. So\nto start, each attitude needs to carry at least a single integer or floating point value to\nrepresent this dimension. Most likely it will be a bipolar value, to express the full\nrange “of like/dislike, love/hate, satisfied/dissatisfied, agree/disagree,” and all the rest\nof the ways an evaluation might be expressed. In some cases, a unipolar value may be\nmore appropriate, as you will see later.\nThe valence can be stored as a single precision floating point value, especially if\nthe game requires the ability to model small shifts in attitude taking place over time\n(persuasion). For many uses, however, an integer may be perfectly okay. If there is an\nanticipated need for storing a large number of attitudes for a large number of NPCs,\nthe valence could be stored in as little as four bits, yielding a –7 to +7 range. (The\n16th unused value of –8 (1000b) could be reserved as a sentinel value.)\nHow valence should be scaled, however, is somewhat less clear-cut. The simplest\napproach is to use –1.0 to +1.0 (and normalizing integer values in calculations as if\nthey spanned this range), and further assume that liking/disliking is linear within that\nrange. Indeed many systems implicitly assume this. But is it true? First, it is not clear\nfrom research how many orders of magnitude a liking/disliking reaction can span. If\n“mild dislike” is –0.1, does that mean “blind seething hatred” is a –1.0, or is it more\nlike –10.0, or –100.0? Two possible alternatives are as follows.\n• One is to still place the evaluation value on a scale from –1.0 to +1.0, but have the\nresponse curve be a sigmoid. Chris Crawford suggested this approach for story-\ntelling systems [Crawford04].\n• Another possibility is to allow the evaluation value to range over a small span,\nperhaps {1.0 … 5.0} to represent (for example) five orders of magnitude in loga-\nrithmic fashion, in a manner akin to the decibel scale or the Richter scale. Then\n“mild dislike” is a –1.0, strong dislike –2.0, moderate hatred a –3.0, strong hatred\na –4.0, and blind seething hatred a –5.0. The descriptive phrases used here are\nsimply to give an approximate idea of what each level represents. They are not\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n253\n\n\nintended to express linearity of English descriptions (this is another can of worms\nentirely).\nThere are some occasions when a unipolar representation may be more appropri-\nate, but these are unlikely to arise in most game design situations. For example, in\nRational Emotive psychotherapy, the opposite of love is not hate, but rather indiffer-\nence [Ellis75]. In that view, hate cannot be the opposite of love because it too requires\nan intense entanglement with the attitude object, and simply is a different emotion\nmanifested relative to that attitude object, and not one that is 180° apart. The oppo-\nsite of hate is also indifference.\nUse caution here. If the nature of the game you are developing requires modeling\nthis sort of emotional subtlety, expect to spend quite a bit of time developing a model\nthat is up to the task.\nPotency\nA second value that can be stored in the Attitude class is potency. This is a measure of\nhow strongly held the attitude is. Potency is not the same as the valence. It is possible,\nfor example, to be very strongly politically moderate, having a valence close to 0.0,\nwhile feeling very strongly that way. This occurs because an attitude represents the\naccumulation of a lifetime of exposures to the attitude object, yet it is expressed as a\nsingle snapshot value. As exposures to the attitude object accumulate, the attitude\ntends to become more and more resistant to change, provided that each exposure does\nnot depart too much from where the attitude is now. A person’s first exposure to Brus-\nsels sprouts might result in a momentary liking of +0.2. Eventually, as occasions for\neating Brussels sprouts pile up, the dispositional liking (the attitude) may settle down\nto +0.16. It will tend to stay there unless the person experiences some particularly\ntranscendent or horrid (or even traumatic) experience with Brussels sprouts that forces\na dramatic reevaluation.\nIt is possible to omit this potency dimension, but if you do, you will need some\nother technique to dampen down the large (and not believable) swings in attitude on\nthe part of NPCs that would otherwise result. \nThis leads to another observation. In real life, a person who has occasion to radi-\ncally rethink his position about something highly personal generally goes though con-\nsiderable emotional upheaval along the way. Say, for example, a character learns that\nher brother has been discovered to be the perpetrator of a number of particularly\nheinous murders. All at once, her lifetime of accumulated feelings and attitudes about\nthe brother will begin to collapse. She may move through the well-known sequence of\ndenial/anger/guilt/resignation/reconciliation, sequencing (and possibly skipping)\nthrough these at a rate that is impossible to know beforehand.\nIf your game has moments of serious betrayal or treachery, any practical attitude\nsystem will probably break down under these conditions. Here, the best advice may\n254\nSection 3\nAI \n\n\nsimply be to bail out. For this, the Attitude class needs to implement a method that\ncan forcibly reset the valence and potency to any desired values. Then when the dramat-\nically heavy moment arrives, the game scripting system needs only do two things—use\nthat method to forcibly reset all the affected attitudes held by anyone who needs to be\n“adjusted” to new, more appropriate values, and play the carefully-crafted animations \nof any NPCs that need to be shown going through their emotional upheavals. Periods of\nextreme emotional distress in humans are so complex that attempting to model them in\ngame AI is probably hopeless at present.\nDuration\nOver time, people forget things, and extreme attitudes and bad memories usually soften\nover time. In many situations, we may want to allow attitudes to weaken or fade away.\nThis is more a matter of getting realistic behavior out of an NPC than of remaining\nfaithful to formal theory. As with most of these issues, the situation for real humans is a\nlot more complex than we would like it to be for these models.\nDuration can be expressed in several ways, and since the fade-out generally fol-\nlows a more or less logarithmic shape, a half-life measure is one way to represent it. If\nthe game spans only a short time frame, attitudes and memories will remain fresh,\nand duration can be omitted altogether. But if game time frame is to span many years,\nsome sort of decay function will probably be needed. If the game is supposed to span\neight game years (to pick a convenient duration) and if memories fade by 50% every\ntwo game years, by game’s end, attitudes first picked up near the start of the game will\ndecay to only 1/16th of their original strength. Other approaches for decaying atti-\ntude strengths or valences are also possible, of course.\nAnother thing to consider is personality factors of the NPC. You may want some\nof them to hold grudges, and for them the half-life should be set very long so that very\nlittle (or even no) decay in their negative attitudes occurs. [Russell06] describes how\nthis problem was tackled in Fable.\nThe Model\nFrom these factors, the Attitude construct can be implemented with a lightweight\nclass more or less like this:\nclass CAttitude\n{\nprivate:\nEntity* target;   // attitude object; \n//   points to a game entity \nValence valence;  // typedef Valence as needed\nPotency potency;  // typedef Potency as needed\nint months;         // game months as half-life\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n255\n\n\npublic:\nCAttitude (Entity*);\nDecay;            // compute another month’s decay\nFloat Product;    // valence * potency\n// etc.\n}\nComplex Attitude Objects\nOne useful metric often required from the Attitude class is an overall or aggregate\nevaluation when dealing with complex attitude objects (like the PC) that get evalu-\nated on the multiple attributes that they possess. \nAt first, it might seem that, for linear representations, a normalized SRSS (square\nroot of sum of squares) would work perfectly well. The game engine’s math SDK may\neven already supply a method for computing this value from a vector of attribute-\nbased evaluation values.\nHowever, this is not what theory calls for. Instead a better value is computed as\nthe normalized sum of the products of each attitude’s valence times that attitude’s\nstrength, as in:\n(3.4.1)\nIf you’re using some other representation, like sigmoid or logarithmic curves, gen-\nerating this aggregate value will naturally require more computational overhead. \nA Simple Example\nConsider a game in which the player is battling a small but ferocious tribe over a\nperiod of time. The enemy NPCs are great warriors and smart enough to know when\nto retreat or melt away in order to fight another day. In other words, the individual\nNPCs last a lot longer than the typical 11 or so seconds a typical orc in a typical orc\nhoard survives in games of this type.\nFigure 3.4.1 shows a very simple FSM (Finite State Machine) [Schwab04]. Of\ncourse, if the warriors were as good as just described, this FSM would be totally inad-\nequate. A more modern and convenient algorithm would be the behavior tree (actu-\nally a DAG) [Isla05]. But to make the point, I’ll use this FSM. \nIf there is no opinion or attitude system in use, you could set up an FSM much\nlike this one, choosing and tuning various values of A (approach distance), R (retreat\ndistance), and H (health) to generate slightly different behaviors among the warriors.\nYou can still use this but now add the attitude system.\nA\n=\nA valence* A strength\nA strength\ntot\ni\ni\ni\ni\ni\n∑\n∑\n256\nSection 3\nAI \n\n\nIn that case, each warrior hates the player (to some extent) and fears the player (to\nsome extent) because the attitude system has in some fashion supplied each warrior\nwith hate and fear values about the player to hold as attitudes. But then the player\nkills off a warrior who is the brother of one of the other warriors. The surviving war-\nrior will now strengthen his hate attitude toward the player, and possibly also change\nthe fear value (depending, perhaps, on how valiantly or cowardly the player fought\nwhile killing the brother). From there on, the surviving warrior can communicate his\nheightened hatred and fear, first of all to his buddies (affinity group), and then, like\nripples in a pond, to more remote members of the tribe with ever lessening intensity.\nHow this might be done is discussed briefly in a bit.\nYou now have a basis for dynamically changing the behavior of the FSM by mod-\nifying A, R, and H with attitude values that modify the initial settings upward or\ndownward. Fearful warriors can now retreat at higher values of H. Hate-filled warriors\nmight now require longer R distances before breaking off an attack, and also require\nlonger A distances, on the theory that these hate-filled warriors are much more likely\nto monitor the environment awaiting the player’s return.\nEven this simple use of an opinion system can enrich an otherwise simple model.\nBut you need not stop here. Figure 3.4.2 shows an extended FSM with extra nodes\nadded to model the warrior who becomes an implacable foe. Now, if the level of hate\n(D) rises high enough and the level of fear (F) drops low enough, a tipping point is\nreached, and the FSM for this warrior now transitions to a new mode, whereby the\nwarrior will pursue, fight, and only briefly retreat from combat with the player, until\none or the other is killed. By doing this, you can further add to the believability of the\nNPC, because it is certainly believable for a warrior to reach a point where the battle\nwith the player becomes personal. Note that you now begin to encroach on the design\nof the game itself. Once a warrior transitions into “implacable,” the nature of the\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n257\nFigure 3.4.1\nA simple FSM for a warrior. A is approach distance, R is retreat dis-\ntance, and H is health.\n\n\ngameplay itself changes, and this sort of decision is probably better handled at the\nscript level and not coded into an NPC’s FSM.\nSo, you might be thinking, “You are creating a nightmare not only for the Q/A\ntesters but for the script and level designers who have to handle all this added complex-\nity!” Sorry, this is true, but cannot be helped as more human-like NPCs are sought.\nClearly, better strategies for designing, testing, and tuning these NPCs are part of the\nchallenge.\nAttitude and Behavior\nThis is another area having an extensive body of theories (and their attendant contro-\nversies). A human, of course, holds literally millions of attitudes about just about\neverything she has ever encountered, and only in some cases is a particular attitude—\nheld at a given time and place—followed by an observable behavior. But storing atti-\ntudes that don’t lead to behaviors that the player can observe in a game environment\nare wasted.\nYou want to use attitude data structures only where they are useful, and this\nmeans once again cutting through a lot of theory to get at a minimal configuration\nthat gets the job done.\n258\nSection 3\nAI \nFigure 3.4.2\nAn extended FSM where attitude influences transitions\nbetween the nodes. D is the level of hate and F is the level of fear.\n",
      "page_number": 280,
      "chapter_number": 30,
      "summary": "This chapter covers segment 30 (pages 280-291). Key topics include attitude, games, and emotions. Conclusion\nAlthough generic programming is definitely not a new idea, it still takes some non-\ntrivial efforts to grasp and apply it properly.",
      "keywords": [
        "Attitude",
        "attitude object",
        "attitude system",
        "game",
        "Complex attitude objects",
        "NPCs",
        "NPC",
        "generic programming",
        "object",
        "NPC Personalities",
        "Complex attitude",
        "FSM",
        "Valence",
        "Attitude class",
        "player"
      ],
      "concepts": [
        "attitude",
        "games",
        "emotions",
        "emotional",
        "emotion",
        "emotive",
        "values",
        "likes",
        "liking",
        "personalities"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 8",
          "chapter": 30,
          "title": "Segment 30 (pages 281-288)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 42,
          "title": "Segment 42 (pages 849-853)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 2,
          "title": "Segment 2 (pages 10-17)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 28,
          "title": "Segment 28 (pages 261-272)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 292-300)",
      "start_page": 292,
      "end_page": 300,
      "detection_method": "topic_boundary",
      "content": "So first, let’s introduce two new wrinkles into the model. The first is the notion of\nan attitude toward a behavior. Yes, that is possible. Any action that an NPC is able to\ntake can have associated with it an attitude, which represents the direction and degree\nthe NPC feels that behavior is desirable. In a social game, murder may be possible,\nbut a particular character may view it sufficiently negatively to entirely rule it out, or\nto commit murder only when the provocation reaches some extreme threshold.\nThe next notion is behavioral intention (abbreviated to BI). BIs sit between attitudes\nand behaviors, and in this model a behavior has to have a BI connected to it before the\nbehavior can occur. This can turn out to be a welcome simplification, however, and one\nthat lends itself for use in behavior trees or other models of NPC behavior. Let’s see how\nthis might work in practice.\nLet’s say an NPC has developed a positive enough attitude toward the player that\nhe or she wants to help. This NPC has an elaborately scripted behavior tree that con-\ntains a number of possible “helping” behaviors—for example, Give Gold, Share\nSecret, or Arrange Lodging. However, these can only fire when conditions are right.\nGive Gold and Share Secret can only occur when the player is in proximity with the\nNPC, whereas Arrange Lodging can be done beforehand and at a distance. (The\nplayer can show up at the tavern, expecting to have to beg for a place to stay, only to\nbe told that a room and meal have already been provided for.) Give Gold or Arrange\nLodging can happen only if the NPC has sufficient gold to make either of these gifts.\nShare Secret can happen only if the NPC has a valuable secret to share. \nBIs can be used here to “prime” those helping behaviors, which are then further\nsubject to the previous test conditions. So, if the NPC has developed a positive\nenough attitude toward the PC, the NPC can form a BI to help the PC. The BI can\nthen test the IF portions of the stack of rules in the behavior tree, to determine\nwhether the other conditions are also met. If so, the behavior can actually occur.\nPersuasion and Influence\nThis is yet another complex and messy body of theory, which you need to pare down\nas well. How much paring is required depends on the kind of game you are building.\nSerious games that need to model political struggles, or advertising or propaganda\ncampaigns, may need more of the model than games that don’t.\nIn theory, how effectively a person or group A can persuade B to adopt or shift\nsome position (that is, adopt or shift some attitude) depends on a large number of\nfactors arranged in a causal chain. The persuader, A, is usually called the sender, and\nthe target of the persuasive message the receiver. At a minimum, the sender needs to be\ncredible (to the receiver), likeable (by the receiver), and similar (to the receiver), in\ncombination to some degree. The receiver, for his part, has to attend to the message,\nbe able to process the message, and be sufficiently involved with what the message is\nabout. The message itself might appeal to reason, emotion, or both. If the appeal is to\nreason, it has to be logically sound, as the receiver interprets it. Also, the message can\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n259\n\n\neasily fail to persuade if it advocates a position too extreme relative to the receiver’s\ncurrent attitude and falls outside of positions the receiver is willing to accept.\nIn a game, you can do away with a lot of this. First, let’s assume that all persuasive\ncommunications in a game are important, that receivers will always find them impor-\ntant, attend to them, and are able to understand them. This immediately removes four\nvariables. Categorizing senders into a much smaller number of groups can collapse\ndown credibility, liking, and similarity. It will then be necessary to maintain a matrix\nof how much Group A trusts messages from Group B.\nNote that this matrix is itself a representation of N-by-N attitudes, and could\nthus also see its values shift over time as groups interacted with each other over the\ncourse of the game. If this is too complex for the game, and such attitudes are not\ngoing to shift, you should compose the matrix with static values and leave it alone\nduring the game.\nThis matrix can also be augmented by adding entries for particular individuals\n(who are also members of groups) to the matrix. This permits members of Group A to\nmostly distrust messages from Group B, while allowing members of Group A to\ngrudgingly accept messages from Person P (who otherwise happens to be a member of\nGroup B). [Alt02] provides details of a nice implementation of this idea.\nFinally, let’s assume that you aren’t going to worry about how the message is com-\nposed; simply that it carries content and persuasive strength. To continue with the\nexample, the surviving warrior who now passionately hates the PC can be shown in an\nin-game cutscene haranguing his fellow warriors as to why it’s now important to make\nslaughtering the PC top priority. Internally, the message is simply conveyed, and the\nattitudes of the warrior’s buddies are suitably adjusted.\nSocial Exchanges of Attitudes\nAs mentioned, the warrior personally affected by the outcome of a battle can communi-\ncate his heightened hate and fear about the player to his affinity group. Moreover, if \nhe was close to his brother, and is close to his affinity group, the other members of that\ngroup presumably also have liking toward the deceased brother, and will be responsive\nto the surviving warrior’s messages. This gets into balance theory [Wikipedia07]. Balance\ntheories are a powerful part of modeling social networks, which is another aspect of\nhuman behavior that is outside the scope of this gem. Using the matrix approach in the\nprevious section, possibly with enhancements, can handle much of this.\nIn many games, all that is important is for the various NPCs to hold attitudes\ntoward the player only, and not hold attitudes about each other. If there is no reason\nin the design of the game to keep information about inter-NPC attitudes, leaving\nthem out greatly simplifies the design. In fact, [Russell06] was quite explicit about the\nfact that Fable stored opinion data only about the PC and no one else to get complex-\nity under control. The opinion system used in Fable is essentially based on the attitude\nconstruct, just not by that name.\n260\nSection 3\nAI \n\n\nHowever, some game designs benefit by having such a system if the gameplay\nrevolves around alliances, treachery, and betrayal. Doing so uncovers a hornet’s nest of\nadded complexity.\nOne such complexity is sheer algorithmic cost. Of course for N characters that\ncan hold attitudes, there are N * (N – 1) pairings, yielding an O(N2) complexity. You\ndo not even get to divide this by two, because it is unsafe to assume that A’s attitude\ntoward B will be symmetrical with B’s attitude toward A. One strategy is to reduce the\nsize of N by assigning less critical NPCs to a much smaller number of groups and\ntracking those instead.\nAnother Example\nConsider how this can be used in a hypothetical open-world action-adventure game\nabout warring crime syndicates and the sleazy yet colorful NPCs who inhabit this game\nworld. The object of the game is to compete against the NPCs by doing dirty deeds,\nenforcing the rules laid down by the mob, expanding turf, making money, switching\nallegiances, and, on occasion, even betraying or killing an NPC when it will do you the\nmost good.\nAs the PC, you engage in acts that earn the respect or disgust of the NPCs, who\nform opinions about you as you claw your way to the top. In a manner following the\napproach used in Fable (which used five dimensions of Morality, Renown, Scariness,\nAgreeableness, and Attractiveness), let’s use several dimensions of opinion scale for\nthis example—Competent, Honorable, Ruthless, Charismatic, and Loyal. \nIdeally, the selected attributes should be as orthogonal as possible, that is, each\nattribute is as close to statistically independent of the others as possible. These five\nseem to meet this requirement. It may seem strange to include Honorable, but con-\nsider it as the code of conduct that dictates that innocent “civilians” are not to be\nharmed (especially one’s mother), but that shopkeepers, gamblers, druggies, and johns\nare fair game, as is anyone who crosses you. \nAssume the game features several different competing crime organizations, each\nwith its own style of achieving dominance. One is led by a capo who favors brazen,\novert violence (he will probably go down in flames early on); another prefers corrupt-\ning police and judges, a third likes to insinuate his organization into legitimate enter-\nprises. Each capo is likely to value a different pattern of attributes of the player or any\nNPC as best suited for his style of organization. \nAs the player continues to work up the ranks, different bosses will tend to value\ndifferent combinations of these perceived attributes. A capo who prefers to keep vio-\nlence hidden away, used only as a last resort, might well bypass a player who is too\nhot-headed for that capo’s style. A player who botches too many jobs will find that his\nperceived Competence has deteriorated and so be left off the important missions that\ncould best advance his career. This could lead to the player having to do many more\nlow-level missions in order to get back into the good graces of his superiors in the\ngang hierarchy. \n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n261\n\n\nJust a handful of attribute/attitude dimensions are needed to make this sort of\ngameplay possible. In Fable, the opinion system and hero stats operated using only\nthe five dimensions listed previously. Even if you only consider high/low on each (on\nthe premise that extreme characters are more fun), that leads to 32 possible combina-\ntions of how the player is perceived and the player’s reputation is built. \nOther combinations of these dimensions can be part of the personalities the\ndesigners build into the rest of the NPCs. After all, even a dim-witted but ruthless and\nhighly loyal NPC can be useful to a crime syndicate. \nCautions and Conclusion\nAt this point you may be thinking, “Why not just make games that need NPCs like\nthis be multiplayer games and let real human players handle all this complexity?” To\nsome extent a multiplayer approach will work and already does in many games, but\nthe problem with this approach is analogous to the problem with player-created con-\ntent—most of it (perhaps 95%) is too poor in quality to be of much use, let alone fun.\nNot that many people are excellent storytellers, role players, or improvisational actors,\nand, moreover, it is a lot of work. It seems like it will be the fate of game designers and\nother talented folk to create the rich game worlds and believable characters that play-\ners demand. \nThis gem introduced a few fundamental concepts about the psychology of attitude.\nAttitude is only one aspect of the psychology of those most complex of organisms,\nhuman beings, but is a useful start and one within reach. Fortunately, the representation\nof a single attitude can be quite lightweight, although in a game with any complexity\nthey can become quite numerous.\nAs more and more CPU and RAM budgets are allocated to game AI, game devel-\nopers will increasingly need to mine the very large body of knowledge about human\nbehavior from psychology, social psychology, and cognitive science. The good news is\nthat game developers need to build caricatures and not real humans, and the challenge\nis one of adopting what we know of real humans and re-engineering that knowledge\ninto practical implementations. \nReferences\n[Agre97] Agre, Phil. Computation and Human Experience, Cambridge University\nPress, 1997.\n[Alt02] Alt, Greg and King, Kristen. “A Dynamic Reputation System Based on Event\nKnowledge.” AI Game Programming Wisdom, Charles River Media, 2002: pp.\n425–435.\n[Crawford04] Crawford, Chris. Chris Crawford on Interactive Storytelling, New Riders\nBooks, 2004.\n[Eagly93] Eagly, Alice and Chaiken, Shelly. The Psychology of Attitudes, Harcourt\nBrace Jovanovich, 1993: pp. 1.\n262\nSection 3\nAI \n\n\n[Ellis75] Ellis, Albert and Harper, Robert A. A Guide to Rational Living, Wilshire\nBook Company, 1975.\n[Isla05] Isla, Damien. “Handling Complexity in the Halo 2 AI,” GDC 2005 Proceed-\nings, \navailable \nonline \nat \nhttp://www.gamasutra.com/gdc2005/features/\n20050311/isla_01.shtml.\n[Mateas02] Mateas, Michael. Interactive Drama, Art and Artificial Intelligence, (Ph. D.\nDissertation), Carnegie Mellon University, School of Computer Science, 2002,\nreport CMU-CS-02-206.\n[Russell06] Russell, Adam. “Opinion Systems,” AI Game Programming Wisdom 3,\nCharles River Media, 2006: pp. 531–554.\n[Schwab04] Schwab, Brian. AI Game Engine Programming, Charles River Media,\n2004.\n[Wikipedia07] “Balance Theory,” available online at http://en.wikipedia.org/wiki/\nBalance_theory.\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n263\n\n\nThis page intentionally left blank \n\n\n265\n3.5\nUnderstanding Intelligence\nin Games Using Player\nTraces and Interactive \nPlayer Graphs\nG. Michael Youngblood, UNC Charlotte\nyoungbld@uncc.edu\nPriyesh N. Dixit, UNC Charlotte\npndixit@uncc.edu\nA\ns the field of game AI has grown, the ability to create characters and game reactions\nthat impart reasonable, challenging, and even insightful actions in their control\nhas improved. However, the basic ability to describe what makes something truly\nseem generally intelligent has lagged behind. This gem provides some insight into a\nspecific visualization and graph-based AI analyses mindset with some tools and tech-\nniques that forward a goal of better understanding both artificial and human intelli-\ngence in games. This gem shows how logging player-centric game data can be used to\nbetter understand both natural and artificial player behavior through the use of visual\ndata-mining, graph-based interaction representations, and clustering tools. \nIntroduction\nThe game AI developer is focused on the creation of an entity, multiple entities, or\npossibly just a system which the engaged human player must perceive as a challenge in\nsome form or fashion. Satisfying the cognitive needs of human players is what makes\nthe game interesting, fun, and challenging, and it is what ultimately makes the game\nsell. The interesting problem is that not every human player perceives the same, or\neven plays the same. What is intelligent to one player is dumb to another. This makes\ngame AI development very difficult, and it leads to discussions about the tradeoffs\nbetween focusing on games that pit human versus human or human versus machine\n(AI). Fortunately, there is a strong desire for a better single player experience and\n\n\nhence a strong need for better AI—or as the need is often stated, there is a need for\nhuman-level AI. There have been numerous discussions about the pursuit of human-\nlevel AI [Heinze02, Laird01, Laird02], but there still remain many open questions\nabout how to determine whether you have achieved it. \nIs a game AI turing test [Russell03] a valid way to determine whether an entity is\nhuman or machine controlled? Although it may prove to be an interesting endeavor, it\nwould undoubtedly be mired in much debate and conjecture as with all rather subjective\nmethods. What is needed is a method for objective evaluation of game AI to a human\nbaseline of performance on the same or similar task or scenario. This act requires data. \nThe current trend in modern computer games is to leave out detailed logging in\norder to free up system resources for other material; however, this data could be used\nto enhance the interactive game experience by providing insight into the behaviors of\nboth human and machine players. \nThe Value of Information\nLogging in games is often tied to the game’s save features because these subsystems\ncommonly track the progress of the player. However, most games do not log player\ninformation; this trend is getting worse, as you can note by the various number of\ngames that now use a checkpoint system for saving rather than being able to save the\ngame at any point. This means that the game will only save when a player reaches a\ncertain location in the level. This way, the developers do not need to track where the\nplayer is going at all times—just when they reach certain milestones. As argued by Eil-\ners [Eilers05], this is fine for the first few times that a player plays the level; however,\nit becomes a hindrance later on once they have already memorized the level. It creates\nwork for the player by them having to drudge through mastered areas to reach the\nchallenge. It also hinders the ability for real logging because there is now no in-game\nprogress monitoring which is a nice place to invoke logging. \nLogging is very useful during the play-testing phase because it can make the\nprocess more efficient. The development team could capture the tester’s session in\nvideo form but it is often too time consuming to watch all the videos, analyses may end\nup very subjective, videos consume a lot of disk space, and objective, automated video\nprocessing is a difficult process. Watching the session firsthand can introduce bias from\nthe observer’s opinion. Another problem with video or screen capture playback is that\nit is often difficult to get a complete picture of play just from the player’s perspective.\nSeeing the entire path or desired sections of the player’s interaction at once or in a spe-\ncific focused view in an interactive analysis tool would be very useful in understanding\nthe behavior of that player—whether it was a human or machine.\nA good set of logged data and some analysis tools (including visualization tools)\nalso helps to find emergent behaviors, or behaviors that are not expected by the devel-\nopers. These are not necessarily errors but interesting “accidents” that differentiate\ngames from non-interactive forms of entertainment [Consalvo06]. \n266\nSection 3\nAI \n\n\nThe most common reason for excluding or oversimplifying logging is that it can\nslow down the game. For instance, during the development for Age of Empires II: Age\nof Kings, the developers used faster machines for play testing than those targeted for\ndeployment due to the slow down induced by logging play test data [Marselas00].\nThis does not always have to be the case because one of the most machine expensive\nsteps in logging is file IO, which can be done more opportunistically at periods of\nlessened hardware need, at cutscenes or designed lulls, or by using multiple threads\ninstead of constantly writing to a file during gameplay. There is also an issue of the\nneeded fidelity for logging. Plenty of information can be gained from 1Hz or slower\nlogging, so 10Hz or better is not always necessary.\nOver the past few years, we have been performing AI research with a number of\ngame testbeds of our own creation, some built from the ground up and others modifi-\ncations of commercial games. We often use the Urban Combat Testbed (UCT), which\nhave been made freely available at www.urban-combat.net. UCT is a total conversion\nmod of the popular Quake 3 game by Id Software. The installed base of Quake fans has\nmade it easy to find study participants (especially at Quakecon, which is always a great\ntime), and we have captured hundreds of players interacting in our game scenarios. (If\nthe reader is interested in being a study participant for our game studies, please visit\nour play testing Website at playground.uncc.edu/PlayTesting.)\nWe often capture logs at 10Hz, but as you can see in Figure 3.5.1, the information\nfrom a 1Hz capture can often be just as informative and useful. However, in Human\nComputer Interface (HCI) analysis work, 10Hz sampling is typically conducted because\nit is the fastest normal response time for a human using an interface device. \n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n267\nFigure 3.5.1\nPlayer trace from UCT (Urban Combat Testbed) player data showing player\nmovements from logging at (a) 1Hz and (b) 10Hz. The thin line represents the spatial move-\nment of the focused entity over time in the environment.\n",
      "page_number": 292,
      "chapter_number": 31,
      "summary": "Any action that an NPC is able to\ntake can have associated with it an attitude, which represents the direction and degree\nthe NPC feels that behavior is desirable Key topics include game, player, and human.",
      "keywords": [
        "game",
        "player",
        "NPC",
        "attitude",
        "human",
        "behavior",
        "NPCs",
        "NPC Personalities",
        "group",
        "Game Programming Wisdom",
        "human players",
        "Interactive Player Graphs",
        "logging",
        "Opinion",
        "Game Programming"
      ],
      "concepts": [
        "game",
        "player",
        "human",
        "attitude",
        "behavior",
        "behavioral",
        "interacted",
        "interaction",
        "logging",
        "log"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 8",
          "chapter": 30,
          "title": "Segment 30 (pages 281-288)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 23,
          "title": "Segment 23 (pages 214-221)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 20,
          "title": "Segment 20 (pages 160-167)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 31,
          "title": "Segment 31 (pages 289-299)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 18,
          "title": "Segment 18 (pages 147-158)",
          "relevance_score": 0.48,
          "method": "api"
        }
      ]
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 301-308)",
      "start_page": 301,
      "end_page": 308,
      "detection_method": "topic_boundary",
      "content": "Our analysis tool whose interactive output is shown in Figure 3.5.1, PlayerViz,\nrequires data files that contain the following information at each timestep:\n• Position (x, y, and z)\n• Orientation (yaw, pitch, and roll)\n• Speed\n• Elapsed time\n• Time-score\n• Health\n• Shots fired\n• Whether a flag is captured\nThe PlayerViz tool is included on the book’s CD-ROM and is freely available\nonline at playground.uncc.edu/GameIntelligenceGroup/Projects/CGUL.\nThe frequency of the timesteps is not fixed and can be adjusted depending on the\ndetail required. Ideally, a logging system could dynamically adjust logging frequency\nto minimize impact on the game by scaling to the available hardware capabilities.\nHowever, even if you log once per second it is enough to do some analysis on player\nbehavior. The implementation of the logging and recorded aspects will vary based on\nthe game, but consideration should be given as to what information is easily available\nand useful for knowledge discovery and understanding the intelligent actions of enti-\nties in the game. \nCapturing interaction is the key to understanding the intelligent actions taken in a\ngame by all rational agents participating. So, you log player interactions with the inter-\nactive feature points of the environment [Youngblood02]. Interactive feature points are\nelements in a game upon which an entity can perform an action (for example, open,\nclose, push, jump over, stand in, shoot, and lasso). These elements might occupy posi-\ntive space and represent a real world or fantasy object (for example, window, door, tree,\nstage coach, crate, and magical potion), they might be negative space areas that can\ncontain other game elements or even the player (for example, a courtyard, inside a\nroom, or in a vehicle), or they might be other agents within the game (for example, an\nopposing force, a horse you can ride, or a dragon you may fly). Anything with which a\nplayer can interact in any fashion can be considered an interactive feature point. \nIn the real world, the number of interactive feature points is infinite, but in a\ngame world they are finite and determined by the designers and the capabilities of the\ngame engine. All of the interactive feature points in a game or game scenario can be\ndescribed in a interaction possibilities graph, as shown for a simple example FPS envi-\nronment in Figure 3.5.2a. The vertices represent interactive feature points and the\nedges indicate that an interaction may occur next from the current interaction—an\nextension would be to enumerate the types of interactions possible with each interac-\ntive feature point. This graph can be used to look for invalid interactions or design\nissues when analyzing player traces from logged data. \n268\nSection 3\nAI \n\n\nGenerating information for validation from design, capturing information from\nplay testing and agent interaction testing, and creating insightful knowledge as new\ninformation from analyses emphasizes how valuable this information is to the design\n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n269\nFigure 3.5.2\nThe interaction feature points of a game can be used to generate an interac-\ntion possibilities graph, shown in (a). This illustrates the interaction possibilities in the FPS\ngame scenario shown in (b) from the top and from a 3D perspective in (c).\n\n\nof games and the desired interaction in games, which largely consists of the human\nand AI driven behaviors. A very important factor in understanding behaviors in\ngames is being able to visualize these behaviors. See Figure 3.5.3.\n270\nSection 3\nAI \nFigure 3.5.3\nA screen capture of the PlayerViz tool used for visual-\nizing single or multiple player traces. It shows the path of a player\nover time, where the spheres represent position and the line segments\nprotruding from the spheres represent orientation.\nA Picture Is Worth a Thousand Words \nThe information gathered from in-game logging of player actions can easily become\noverwhelming. There are several variables that are all tracked simultaneously, including\ntime, position, orientation, and interaction. This data is easier to comprehend if it is rep-\nresented visually with a player trace. A player trace shows the actions of the player from\nstart to finish in a visual manner. The PlayerViz tool is designed to allow interactive visu-\nalization of player trace data from one or more players. As seen in Figure 3.5.3, a player\ntrace is represented as a series of spheres with connecting lines. The color of the spheres\ncycles through a rainbow color-map, blue to green to red over time, in a way that the col-\nors appear to change faster when moving slower and vice versa. The position of the\n\n\nspheres represents the position of the player, and the player’s orientation at that time is\nportrayed as an oriented line segment coming out of the sphere. A wireframe white\nsphere represents that the player found the goal and it is usually located at the end of a\ntrace. A red wireframe sphere around the player indicates lost health, and a red solid\nsphere at the end of the line segment means the player fired a shot. A player trace provides\na great overview of physical interactions, but it may not always apply to all games—some\ngames do not have a clear spatial component to them, such as puzzle games. \nPlayerViz can also be used to examine multiple player traces at once, which can\nprovide some interesting results. For example, if the players were playing together,\ntheir group dynamics can be studied. You could also take the average of their player\ntraces to get an overview of their composite performance. If the players are adver-\nsaries, you can examine the strategies taken by each and how they responded to each\nother’s actions. For example, if two players are simultaneously pursuing the same goal,\nit would be interesting to see the paths taken by each player and where the paths\nintersect. Player traces can also be utilized to show AI agent behavior and compare it\nto human behavior. The composite trace of all humans and all agents could be com-\npared to give a general overview of how human-like the AI is for the game. \nSimplification of World Data\nIn order to provide context for the player traces, the world geometry must also be visual-\nized. However, the geometry does not need to be shown in great detail and can be greatly\nsimplified. For PlayerViz, the world is broken down into positive and negative space\nregions. The 3D modeler manually specifies positive space. It represents an approximation\nof the external world geometry for an object (for example, a box around a building). These\nregions are not exact, but you need only a rough estimate of geometry to provide spatial\ncoherence between the player trace and the world. The negative space regions can be man-\nually specified or automatically calculated using methods such as cell decomposition.\nIn order to simplify the negative space geometry, you must break the world down\ninto a series of convex regions. There are several techniques for decomposing the\ngeometry of a world into regions. The technique we used is called key vertex cell\ndecomposition [Youngblood06]. This method involves creating polyhedrons by con-\nnecting key vertices on the positive space objects (such as the corners), avoiding cross-\ning segments, and combining adjacent regions to increase size as long as the\npolyhedrons remain convex.\nThese regions have a lot of extraneous polygons because negative space generally\ncontains mostly empty regions. However, if you render only the polygons that are\nroughly parallel to the ground, you generally get useful geometry. You can also identify\ngateways between spatial regions of the geometry. These gateways are identified by\ncoplanar boundaries between regions. However, the boundaries must be completely\ncoplanar for a gateway to be easily detected. These regions and their connecting gate-\nways can also be used to assist path planning and other spatial tasks for AI agents. The\nagent can learn about the world around it by keeping track of the regions it has visited.\nLooking at the player trace for such an agent can be useful in machine learning studies.\n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n271\n\n\nA Pixel for Your Thoughts \nEven with the help of player trace visualization, it can still be a difficult task to examine\nall of the player traces if there are several hundred players. One way to make it easier is\nto use visual data mining [Keim02]. PlayerViz can be used to generate a set of Web\npages showing a table of thumbnails of each player trace from different angles. This\nallows the user to quickly scan through the dataset and find interesting traces, which can\nthen be loaded into PlayerViz to examine further. Once the desired or anomalous arti-\nfacts are identified visually, the user can then implement ways to automatically find\noccurrences of these artifacts, often through the creation of specific feature-finding algo-\nrithms. The visual data mining is mainly a bootstrap method to guide the creation of\nmore specific tools for a particular game, but it can be powerful in helping to define\nwhat you should be looking for, which is often difficult to determine a priori.\n272\nSection 3\nAI \nFigure 3.5.4\nA table of interesting artifacts found through visual\ndata mining of player traces, as follows—(a) jumper, (b) fluster, \n(c) positive learning, (d) emergent behavior, and (e) crazy Ivan.\n\n\nIn our own work using these player trace images, as shown in Figure 3.5.4, we can\nfind several interesting artifacts that would be near impossible to find by looking at\nthe numbers alone. For example, Figure 3.5.4a shows a jumper. This is a person who\nlikes to jump continuously even when he or she is walking on a flat surface. Another\nexample, shown in Figure 3.5.4b, is called a fluster. A fluster is an artifact in the player\ntrace where the player seemingly loses control of his or her cursor. It can be caused by\nlack of experience with mouse aiming or also by a faulty mouse. These occurrences\nwould be very hard to track without visual data mining.\nFigure 3.5.4c shows two player traces of the same player. The left image is the\nsource, or first attempt, and the right image is the target, the second attempt. It can be\nclearly seen that the player learned from his previous attempt how to get out of that\nclosed-off area. The opposite can also be true when a player finds the goal the first\ntime but forgets it in the second attempt. Thus, you can use this technique to observe\nboth positive and negative learning.\nYou can also find emergent behaviors using this technique, such as the player\ntrace shown in Figure 3.5.4d. The intent was for the player to climb the wall and find\nthe goal, but instead this player climbed the side of a house and jumped down from\nthe roof. Such behavior is also difficult to track without visual reference. The last\nexample is a crazy Ivan, or an instance where a player turns a full 360° to survey his or\nher surroundings, as seen in Figure 3.5.4e. \nUsing this visual data, you can also analyze the areas that are the most visited (or\nleast visited) by the players. This allows the game designers to determine whether their\ndesign for the level matches the player experience. For example, if the designer places\na clue in an area of the map that very few players ever go to, the designer will know\nthat most players are not likely to find it.\nThere are several future additions to the PlayerViz tool planned, such as calculating\nthe average player trace using a set of several player traces. This average trace can be used\nto quickly and easily see the general path taken by most of the players. Another feature\nmight be to generate a congestion map using a set of several player traces to highlight\nareas that were the most visited. This feature would give game designers a good idea of\nwhere future players are more likely to explore, so that they can place interactive feature\npoints accordingly.\nInteractive Player Graphs\nUnderstanding the spatial trajectory and observing simple spatiotemporal interactions\nprovides a great deal of understanding about the intelligence of the observed rational\nagent, but interactive visualization tools still require a lot of manual work for the analyst.\nA representation of interaction that could be used for comparison with other players\nwould be useful in this case. The trajectory of interaction in the context of a game is a\nrepresentative of the strategy taken by the individuals playing. The ability to capture and\ncompare strategies could be very useful. The establishment of a finite set of interactive\n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n273\n\n\nfeature points and the introduction and enforcement of a designed interaction possibil-\nities graph along with proper in-game logging come together to allow for the generation\nof an interactive player graph for each player in a game. \nAn interactive player graph (IPG) is built either during play or in post-play pro-\ncessing of the running or completed player log file. The game environment needs to\ncapture the desired interactions to make the graph. The vertices of the graph represent\nan interaction event (for example, pushing a button, picking up a health item, stun-\nning an opposing force, picking up a key, going through a door, or standing in a new\nregion of the map). IPGs can be very detailed, but it is less important and more com-\nputationally feasible to reduce the data stream and ignore the minor noise generated\nfrom movement, orientation, and other esoteric changes in state. What you need is a\nsomewhat higher abstraction of player activity that gives you the proper resolution for\nunderstanding while reducing the extraneous information that can bog down analysis\nand obscure intention.\nTypically, you track spatial movement within the qualitative convex regions deter-\nmined from cell decomposition methods as described in the previous section and report\nposition by those numbered regions when entered—in many game types, especially\nFPS/3PS, spatial interactions by normal movement can dominate an IPG if tracked at\ntoo high a resolution. Other interactions are typically recorded as they occur. So, you\nbuild an IPG from a combination of interactions, which come from rough spatial move-\nments through environmental regions and player interactions with specific objects in\nthose regions. However, IPGs do not have to incorporate spatial interaction and are\ntherefore also very useful for analyzing games that do not have a clear spatial component\nto them. \nOne problem with the graph representation is that there is a definite issue of tim-\ning in many games, and often the real difference between performances in a scenario,\nespecially ones with few paths of choice through the environment, is the time it takes\ndifferent players to accomplish the same task. Gonzalez [Gonzalez99] and Knauf et al.\n[Knauf01] also note the importance of time in validation of human models. IPGs\ncapture time by weighting the edges between interaction feature points with the time\nit had taken the player between interactions. IPGs abstract a player’s performance in a\ngame or game scenario, removing the detailed state change noise through the environ-\nment while capturing their approach of interaction, moving from one interaction fea-\nture point to another and also capturing the time aspect of their performance. Figure\n3.5.5 shows a player trace converted to an IPG using the associated spatial decompo-\nsition map associated with the scenario—note that the only interaction represented is\nmap traversal; other interactions would merely extend the graph with additional\ninteraction vertices.\nTo be noted is that there are a number of extensions or variations to the base IPG\nformat described here. For example, it may be useful to associate the type of interac-\ntion with an interaction feature point representing each as a separate vertex. Agents\nmay tag internal state to the transitions or interactions as well. \n274\nSection 3\nAI \n\n\nClustering the IPGs\nThe usefulness of a graph format is that it can be used to compare to other graphs. In\norder to compare graphs, a measurement of graph similarity is needed. We suggest using\ngraph edit distance, which is defined as the minimum number of changes required to\nchange one graph into another. A change is defined as the insertion of an edge, the dele-\ntion of an edge, the insertion of a vertex, the deletion of a vertex, an increase in time on\nan edge by 0.1 seconds, or the decrease in time on an edge of 0.1 seconds. Each change\ncarries an equal weight of one—this can be changed to reduce bias. This scheme gives\npreference to time since it is the major factor of difference, but in many of the games\nyou’ll evaluate, time performance is the major differentiator between players. You can\nalso evaluate graphs without time weights using the same metric, but without the\nincrease/decrease in time weighting. \nOne goal of comparison is to be able to group or cluster players by their IPGs,\nwhich essentially represents their strategic choices and performance in the game or\ngame scenario. Figure 3.5.6 illustrates the many different interaction trajectories play-\ners may take even in the same game scenarios. If you cluster player performance, you\nshould see clusters of players grouped by their relative skill level [Youngblood02]. If\nplayers cluster into their skill levels, this technique can be used for player classifica-\ntion. More interestingly from an AI perspective is that if you evaluate machine-driven\n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n275\nFigure 3.5.5\nThe interactive player graph in (a) was created from logged data from the\nplayer whose same player trace is shown in (b) using PlayerViz. The vertices represent inter-\nactive feature points (entry into new spatial map regions in this case), and the edges are\nweighted by the time between interactions.\n(a)\n(b)\n",
      "page_number": 301,
      "chapter_number": 32,
      "summary": "This chapter covers segment 32 (pages 301-308). Key topics include interactive, interactions, and player. This graph can be used to look for invalid interactions or design\nissues when analyzing player traces from logged data.",
      "keywords": [
        "player",
        "player traces",
        "Interactive Player Graphs",
        "Interactive Player",
        "game",
        "Player Graphs",
        "Interactive feature points",
        "trace",
        "interaction",
        "feature points",
        "interactive",
        "player trace data",
        "graph",
        "multiple player traces",
        "Interactive feature"
      ],
      "concepts": [
        "interactive",
        "interactions",
        "player",
        "spatial",
        "game",
        "graph",
        "traces",
        "visualize",
        "visual",
        "visually"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 8",
          "chapter": 32,
          "title": "Segment 32 (pages 300-308)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 26,
          "title": "Segment 26 (pages 238-247)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "makinggames",
          "chapter": 27,
          "title": "Segment 27 (pages 239-248)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 11,
          "title": "Segment 11 (pages 111-118)",
          "relevance_score": 0.58,
          "method": "api"
        }
      ]
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 309-320)",
      "start_page": 309,
      "end_page": 320,
      "detection_method": "topic_boundary",
      "content": "agent data with human data and the agents cluster into groups with human players,\nyou can assert that the agent played consistent with that group of humans, or that the\nagent behaved in a human-consistent manner. \nWe typically use K-medoids clustering [Friedman99]. K-medoids is an iterative algo-\nrithm where based on a given set of data points and k clusters with centers approximated\nby initial representative objects (we seed ours initially with random members of our data\nset), all members are initially assigned to their nearest k cluster representative (or medoid ).\n276\nSection 3\nAI \nFigure 3.5.6\nInteractive player graphs should use different approaches (strategies), as seen in these four\nexamples from the same game scenario and constraints as the previous example in Figure 3.5.5.\n(a)\n(b)\n(c)\n(d)\n\n\nThen, for each step the cluster medoids are recalculated based on whether one of the non-\nmedoids improves the total distance of the cluster and then members are re-clustered\nbased on their distances to the new centers. This process continues until there is no dif-\nference (no improvement) between two consecutive iterations. A clustering criterion\nfunction is applied to evaluate clustering quality for that k clustering. Due to the high\ndimensionality of IPGs and seeding with existing members, we iterate our clusters over all\npossible initial seed values. Due to the abstracted dimensionality of the data, we also\nsuggest exercising k from 2 to (n–2), where n is the number of IPGs being compared, \nto ensure the discovery of the best clustering in accordance with the clustering quality\ncriterion function. \nIn clustering IPGs, the clustering criterion functions utilize the distance measure\nd(xij,xpq), which is the distance between the jth member of cluster i and the qth mem-\nber of cluster p, where the distance d represents the graph edit distance between two\nmembers. The mean intra-cluster distance of cluster i is as follows, where ni is the\nnumber of members in cluster i:\n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n277\nAchieving a desired clustering of data is moreover a result of optimizing the clus-\ntering criterion function [Zhao02]. In our experience clustering IPGs, we have found\nthat the following clustering criterion function to work the best [Youngblood02,\nYoungblood03]. \nMinimize, as follows:\nUtilizing K-medoids on IPGs should produce clusters based on similar strategies\nand performance in the observed game. Clustering human players and agents can be\nhelpful in determining whether the agents are behaving similar to a known group of\nhuman players or even sets of other agent players. Clustering can be used to determine\nplayer skill level and distance from the next skill level. Evaluated in-game dynamically,\nclustering of the current human player could be used to determine the actions of the\ngame AI (based on perceived player skill or strategy). Although K-medoids may be\nunsuitable for real-time processing in-game, building player type profiles from play\ntesters and utilizing simple and fast methods such as the K-Nearest Neighbors (kNN)\nto match and respond appropriately can be effective.\n\n\nDigging in the Graphs\nIn addition to clustering techniques, there are other methods for discovering knowl-\nedge in graph-based data such as that presented by IPGs. The area of graph-based data\nmining offers tools such as SUBDUE (www.subdue.org) by Larry Holder. SUBDUE\nhas been used to discover common patterns in IPGs for UCT data [Cook07]. Utiliz-\ning compression techniques and the minimum description length principle, SUBDUE\ncan find common substructures in IPGs. These represent common strategies taken by\nplayers regardless of their actual cluster similarity. Game AI using appropriate responses\nfor the anticipated actions could exploit these common sub-strategies. \nA Deeper Understanding of Behavior\nData logging and player traces can also be used for other purposes. Players’ look direc-\ntions can be just as useful as their positions. By using the view directions of a player over\ntime, you can derive the surfaces that received maximum or minimal exposure. We have\ndeveloped a tool called HIIVVE (Highly Interactive Information Value Visualization and\nEvaluation), designed for this purpose [Dixit07]. The tool, as shown in Figure 3.5.7, uses\nplayer trace data to calculate intersections and find the information value for each surface\nin the world geometry. The information value of a surface represents the likelihood that a\nplayer will see information placed on that surface. This data can be used to make design\ndecisions about the placement of art assets or interactive feature points. \n278\nSection 3\nAI \nFigure 3.5.7\nThe Highly Interactive Information Value Visualization and Evaluation\n(HIIVVE) tool helps determine the information value of game surfaces. Another example\nof useful in-game logging and knowledge gained from analysis of play testing data.\n\n\nPlayerViz could also be used to track nearly any type of captured information.\nFor example, something useful for better understanding AI agents and potentially\ndebugging issues within their intelligence mechanisms would be to reflect agent inter-\nnal state changes (for FSM and FuSM agents) or subsumption level firings. Agent\naction decisions could also be explicitly represented. \nOur group at UNC Charlotte continues research in better understanding both\nhuman and agent intelligence in games. We offer a full set of analysis tools and meth-\nods for improving game AI through the CGUL (pronounced “seagull”) Toolkit—the\nCommon Games Understanding and Learning Toolkit. CGUL is available online for\nfree at playground.uncc.edu/GameIntelligenceGroup/Projects/CGUL.\nConclusion\nThere are some strong and compelling reasons to include good logging capabilities in\ngames. Data collected from logging human players and AI interacting in a game envi-\nronment can be used to perform visual data mining with tools such as the provided\nPlayerViz, which can be used as a bootstrapping process to guide the development of\na repertoire of tools for game specific analysis and better understanding of intelligent\nactions in the game. Player traces tell only half of the story, though. By tracking and\nconstructing interactive player graphs generated from the observed trajectory of\nplayer/agent interactions that are analyzed with clustering and knowledge discovery\ntechniques, developers can garner new insights into player performance and classifica-\ntion. This information can then be exploited to develop better game AI. \nReferences\n[Consalvo06] Consalvo, Mia and Dutton, Nathan. “Game Analysis: Developing a\nMethodological Toolkit for the Qualitative Study Of Games,” The Interactive\nJournal of Computer Game Research, Vol. 6, No. 1, December 2006. \n[Cook07] Cook, Diane J., Holder, Lawrence B., and Youngblood, G. Michael.\n“Graph-Based Analysis of Human Transfer Learning Using a Game Testbed,”\nIEEE Transactions on Knowledge and Data Engineering, 2007.\n[Dixit07] Dixit, Priyesh and Youngblood, G. Michael. “Optimal Information Place-\nment in 3D Interactive Environments,” Sandbox Symposium, 2007.\n[Eilers05] Eilers, Michael M. “Soapbox: Difficulty and the Interstitial Gamer,” avail-\nable online at http://www.gamasutra.com/features/20050809/eilers 01.shtml,\nAugust, 2005.\n[Friedman99] Friedman, Menahem and Kandel, Abraham. Introduction to Pattern\nRecognition: Statistical, Structural, Neural, and Fuzzy Logic Approaches, Imperial\nCollege Press, London, 1999. \n[Gonzalez99] Gonzalez, Avelino. “Validation of Human Behavioral Models,” Twelfth\nInternational Florida AI Research Society Conference, Menlo Park, AAAI Press,\n1999, pp. 489–493.\n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n279\n\n\n[Heinze02] Heinze, Clinton, Goss, Simon, Josefsson, Torgny, Bennett, Kerry, Waugh,\nSam, Lloyd, Ian, Murray, Graeme, and Oldfield, John. “Interchanging Agents\nand Humans in Military Simulation,” AI Magazine, Vol. 23, No. 2, 2002, pp.\n37–47.\n[Keim02] Keim, Daniel. “Information Visualization and Visual Data Mining,” IEEE\nTransactions on Visualization and Computer Graphics, Vol. 8, No. 1, (March 2002).\n[Knauf 01] Knauf, Rainer, Philippow, Ilka, Gonzalez, Avelino, and Jantke, Klaus.\n“The Character of Human Behavioral Representation and Its Impact on the Val-\nidation Issue,” Fourteenth International Florida AI Research Society Conference,\nMenlo Park, AAAI Press, 2001, pp. 635–639.\n[Laird01] Laird, John E. and van Lent, Michael. “Human-Level AI’s Killer Applica-\ntion: Interactive Computer Games,” AI Magazine, Vol. 22, No. 2 (2001), pp.\n15–25.\n[Laird02] Laird, John. “Research in Human-Level AI Using Computer Games,”\nCommunications of the ACM, Vol. 45, No. 1, 2002, pp. 32–35.\n[Marselas00] Marselas, Herb. “Profiling, Data Analysis, Scalability, and Magic \nNumbers, Part 1: Meeting the Minimum Requirements for Age of Empires II:\nThe Age of Kings,” available online at http://www.gamasutra.com/features/\n20000809/marselas 01.htm, August 2000.\n[Russell03] Russell, Stuart and Norvig, Peter. Artificial Intelligence: A Modern\nApproach, Prentice Hall, 2003. \n[Youngblood02] Youngblood, G. Michael. “Agent-Based Simulated Cognitive Intelli-\ngence in a Real-Time First-Person Entertainment-Based Artificial Environment,”\nMaster’s thesis, The University of Texas at Arlington, 2002.\n[Youngblood03] Youngblood, G. Michael and Holder, Lawrence B. “Evaluating\nHuman-Consistent Behavior in a Real-Time First-Person Entertainment-Based\nArtificial Environment,” Proceedings of the Sixteenth International FLAIRS\nConference, 2003, pp. 32–36.\n[Youngblood06] Youngblood, G. Michael, Nolen, Billy, Ross, Michael, and Holder,\nLawrence. “Building Test Beds for AI with the Q3 Mod Base,” Artificial Intelli-\ngence in Interactive Digital Entertainment (AIIDE), June 2006.\n[Zhao02] Zhao, Ying and Karypis, George. “Evaluation of Hierarchical Clustering\nAlgorithms for Document Datasets,” Proceedings of the 11th Conference of\nInformation and Knowledge Management (CIKM), 2002, pp. 515–524.\n280\nSection 3\nAI \n\n\n281\n3.6\nGoal-Oriented Plan Merging\nMichael Dawe\nU\nsing goal-oriented action planning systems to create and manage behavior in\nautomated agents is a powerful technique that has quickly found acceptance\namong game developers. In game development, planning systems are a relatively new\ntechnology, whereas academia has been using planning to solve problems for well over\n50 years. Thus, it isn’t surprising to find a large base of research that game developers\ncan use to improve their planning systems.\nOne way planners can be improved is through the use of plan merging, a tech-\nnique used in several ways under academic settings but not yet applied to games.\nUsing plan merging can allow a broader range of behaviors for automated agents and\neven let them attempt to pursue multiple goals at once. This gem examines one way\nof implementing a plan-merging system in the context of a real-time game and dis-\ncusses the implications of using such a system.\nReview of Goal-Oriented Planning Systems\nGoal-oriented action planning systems are decision-making algorithms designed to\ntake the burden of choosing particular agent behaviors off the programmer and put\nthem into the agent’s own sense-think-act cycle. The primary benefit of using these\nsystems is the reduced complexity of designing individual actions for artificial agents\nwhile retaining a high level of realism in the agent’s total behaviors.\nGoal-oriented planning lets particular agents decide their own actions through\nthe pursuit of particular goals. An agent’s goals might include destroying a target or\nobtaining an item. Goals are represented as desired world states in whatever system\nthe agent uses to keep track of the state of the world. In traditional planning systems,\nthe agent is restricted to picking one goal as being most important at any given point\nin time. Once this goal is picked, an agent can create a plan by stringing together a\nsequence of atomic actions, sometimes also known as operators.\nFor example, if your agent has decided on the DestroyTarget goal, an action it\ncould pick to accomplish that goal might be the Attack action. Actions have precon-\nditions, which describe conditions on the world that must be true before the action\nexecutes, and effects, which describe necessary conditions on the world after the\n\n\naction has completed. In the case of the Attack action, a precondition might be that\nthe agent’s weapon is loaded. An effect would be the destruction of the target.\nUsing the effects and preconditions as guides, any heuristic search can create a\nplan by listing a sequence of actions an agent can use to achieve the desired goal. Jeff\nOrkin describes how to use the A* algorithm for planning purposes in [Orkin04].\nThe completed plan is then just that sequence of actions the agent executes to accom-\nplish its goal.\nSome final terminology is needed before discussing plan merging. Totally-ordered\nplans are plans in which the order of each action is completely specified, such that one\nparticular action is first, another occurs second, and so on. Partially-ordered plans may\nspecify individual orderings of actions but leave the precise ordering of all actions as\nunspecified as possible. In other words, a partially-ordered plan doesn’t specify the order\nof actions unless an action satisfies the precondition of another. Totally-ordered plans can\nbe made from partially-ordered plans by giving a specific order to the actions in the plan.\nFigure 3.6.1 shows an example of some partially- and totally-ordered plans for\nmaking a sandwich. In the partially-ordered version, notice that independent actions\n(obtaining the meat, cheese, and bread) are unordered relative to each other. Actions\ncan have a relative ordering, though; all the ingredients must be obtained before mak-\ning the sandwich. In general, the only orderings given to actions are those required by\nthe actions’ preconditions. Totally-ordered plans, on the other hand, enforce a specific\nordering of all actions, regardless of whether the individual actions satisfy precondi-\ntions for other actions. Although you could obtain the meat, cheese, and bread for\nyour sandwich in any order, a totally-ordered plan specifies an order in which to per-\nform these actions. It stands to reason that any partially-ordered plan can be expressed\nas a totally-ordered one.\n282\nSection 3\nAI \nFigure 3.6.1\nPartially- and totally-ordered plans. Not all totally-ordered instantiations of\nthe partially-ordered plan are given.\n\n\nAlthough the vast majority of academic-based planning algorithms produce\npartially-ordered plans, these types of planners have not yet found widespread use in\ngames. There are a few reasons why totally-ordered plans are of more immediate use\nto an NPC:\n• First, given a partially-ordered plan, an agent will at some point have to define,\neither explicitly or implicitly, a totally-ordered plan in order to execute the actions\nof the plan. In other words, the agent still needs to choose one action to perform\nfirst among any number of unordered actions. Given this, there are some reasons\nwhy executing one action before another might be advantageous, but the reasons\nfor which the agent might put one action first could easily be abstracted into the\nplanner itself.\n• The second major reason that games have traditionally dealt with totally-ordered\nplans is the ease with which A* is adapted to creating plans. Because A* is such a\nwell known and versatile algorithm, it is an easy choice for use in a goal-oriented\nplanning system, and A* produces totally-ordered plans by its nature.\n[Orkin04a], [Orkin04b], and [Orkin06] cover the many practical details of\nimplementing an A* planning system for games. \nPlan Merging for Goal-Oriented Plans\nPlan merging refers to the process of taking several independently generated plans and\ncreating a single plan out of them, usually with the intention of reducing the overall\ncost of the plan. Often, a reduced-cost plan has the benefit of also producing more\nrational-looking behavior. To demonstrate the power of plan merging, let’s look at an\nexample before getting into the details of the algorithm.\nSuppose an agent has the task of collecting items from around the world and\nreturning those items to a home base. If the agent can carry only one item at a time, it\nis apparent that it has no better choice than to go to an item, collect it, and return to\nbase. However, if the agent can carry multiple items, it is also evident that many situa-\ntions exist where the agent could reduce its total distance traveled by collecting several\nitems at once.\nThere are several ways you could accomplish this behavior utilizing a planning\nsystem. Suppose that your goal of collecting items and returning them to base was the\nReturnItems goal. You could write a GatherItems action that accomplishes that goal.\nAn agent executing the GatherItems action would look for the nearest items, gather as\nmany as it could, and return them to base. Although this would be a solution, it is\nclear that the GatherItems action would be quite complicated. It would need to\ninclude code to pathfind and travel between items, pick up items, pathfind and travel\nto base, and drop off the items once arrived. The increased functionality contained\nwithin one action works to defeat the purpose of having a flexible planning system.\nIt is much easier to write smaller, reusable, atomic actions, such as GoTo for\npathfinding, GetItem to gather the item from the world, and ReturnItem to drop off\n3.6\nGoal-Oriented Plan Merging\n283\n\n\nthe item at base. These multiple actions allow the planner to do the complicated work\nof stringing together the actions into the right order, and further allow you to reuse\nthe actions among many types of NPCs. Yet none of these actions can communicate\nto the agent that it should try to gather multiple items at a time. Instead, you can\naccomplish the desired behavior through plan merging.\nThe general idea is to take two plans with some overlapping actions and combine\nthe plans to produce a single plan with a lower cost than independently executing\neach of the original plans. In the current example, the agent could plan to gather each\nitem independently, producing two plans that were unrelated but very similar, as\nshown on the left in Figure 3.6.2. A possible result from a merge of those two plans\nwould combine as many actions as possible together, producing the single plan shown\non the right in Figure 3.6.2. When the agent executes this plan, it collects both items\nbefore making the return trip to base.\n284\nSection 3\nAI \nFigure 3.6.2\nTwo totally-ordered plans and the result of a possible merge between them.\nImplementing a Plan-Merging Algorithm\nAcademically, the interest in plan merging centers mostly on plan optimization.\n[Foulser92] points out two major components to optimizing a plan: finding actions\nthat can be merged and then computing the optimal way to merge the actions if there\nexists more than one way to put the operators together. It is easiest to deal with these\nproblems separately, so that is the approach taken in this article.\nThe first challenge in finding mergeable actions is discovering precisely what\nkinds of actions can be merged. Put simply, any number of actions can be merged if\nthere is another action that can replace the merged actions with these results:\n• If the action has the same useful effects.\n• If the replaced action costs less than the sum of the merged actions it is replacing.\n\n\nEffects are “useful” if they directly establish a precondition of another action in\nthe plan, or a precondition of the goal itself. For example, suppose an agent has a plan\nto destroy a target using the FireWeapon and ReloadWeapon actions. The Reload-\nWeapon action has a couple of effects. First, it makes the weapon be loaded, and sec-\nond, it reduces the agent’s ammunition store. The first effect would be a useful effect,\nbecause it accomplishes a precondition of another action in the plan. The second\neffect isn’t useful, because it has no bearing on the execution of the plan. \nSearching plans for mergeable actions would be incredibly expensive without\nknowledge of the actions themselves, so it is best to specifically look for actions that are\nknown to be mergeable. In an implemented system, this means either looking for a\nspecific action that can be merged with itself, or looking for a known combination of\nactions that could be merged. In the earlier resource-gathering example, you know that\nthe agent is likely to have multiple plans, each with an instance of the ReturnItems\naction. This is an excellent candidate action to look for, because you know it’s possible\nto merge two ReturnItems actions. In this specific case, you might even start the search\nat the end of the plan, because it is likely to be the last action in each of the plans that\nare being merged. GoTo(Base) can also be merged with itself, because it obviously\naccomplishes the same effect. \nThe second challenge is creating an optimal plan once a possible merge has been\ndiscovered. [Foulser92] deals with the difficulties of creating an optimal plan, noting\nthat creating an optimal plan quickly becomes computationally expensive, and probably\noverkill for games. For the resource-gathering NPC, you’ve already improved behavior\nby allowing the agent to collect multiple resources at once. Rather than spend much\ntime worrying about the optimality of the plan, you could just place the rest of the two\nplans together, as was shown in Figure 3.6.2.\nHowever, to make the agent appear even more intelligent, you could employ crit-\nics, special-purpose checks used to help order the remaining actions. For example, you\nknow you have two pairs of GoTo(Item) and Get Item actions to be placed before the\nmerged actions, so you could write a critic to make sure the agent goes to the closest\nitem first. Critics are then general rules written to enforce a desired behavior in plan\nmerges.\nAt its simplest, then, the plan-merging algorithm accepts two plans generated\nthrough the general-purpose A* planning system. The agent could send its two most\nimportant goals to the planner, for example, and then send those two independent\nplans to the plan merger. For every action in the first plan, the algorithm checks to see\nwhether it can be merged with an action in the second. If a merge can be performed,\nthose two actions are put together into a single plan, being careful to put preceding\nactions from both plans before the merged action, and likewise putting any actions\noccurring after the merged action afterward. If more precise control over the order of\nthe non-merged actions is needed, critics can be employed to determine the best\nordering and rearrange the actions as necessary. For a wider range of possible merges,\na complete plan-merging algorithm should examine the net effects of every possible\n3.6\nGoal-Oriented Plan Merging\n285\n\n\ngroup of actions in each plan, looking for situations where a sequence of actions could\nbe replaced by a single cheaper action. Such an algorithm produces the most impres-\nsive improvements to mergeable plans, but is also expensive to run.\nBeyond Single-Agent Merges\nAlthough merging two plans for a single agent certainly offers opportunities for\nimproved behavior, plan merging also offers remarkable benefits in the areas of squad-\nbased planning. For instance, an agent utilizing plan merging could merge an individ-\nual goal (picking up a weapon or health power-up) with a squad-issued goal (providing\ncover fire). Utilizing plan merging in these situations allows an agent to maintain its\nown goals and personality in the face of squad-issued orders and even allows for situ-\nations where the agent can accomplish many goals at once.\nStrategies for Improving Action Searching\nSearching two or more plans for actions with similar effects is expensive, especially if\nyou consider replacing groups of actions with different net effects. If the game is fast-\npaced, typical of many FPSs, the agent’s primary and secondary goals could change\nmore quickly than it could even devise a plan for its secondary goal. Clearly, plan\nmerging is of no use unless you can quickly perform the merge.\nOne possible strategy to reduce the time needed to search through actions is to\nlook for mergeable actions only when specific actions are present in the plan, some-\nthing that can be determined in the middle of the plan-making process. For extremely\nlong plans, hooks direct to possibly-mergeable actions could be included in the plan\nstructure itself, directing the algorithm not only into the correct places immediately,\nbut also informing it if a merge is worth looking for at all. In specific kinds of agents,\nit might even be worth only looking for a specific action to merge in each plan.\nSimilarly, you might attempt a merge only when the goals being planned for are\ncompatible. Conversely, it makes sense to not even bother to attempt a merge if the\ntwo intended goals are incompatible. Indeed, even making a plan for a secondary goal\nis wasted time if the goals are incompatible. This determination is probably best made\nby the programmer. It might be obvious to you that an Attack and a Retreat goal will\nnever produce mergeable plans, but a generically written algorithm would search\nthrough every action of each plan before reporting that no mergeable actions exist.\nConclusion\nPlan merging offers a way to improve the perceived intelligence of an agent acting\nindependently or within a squad. Although potentially a very expensive process, with\nsome careful consideration, it can be accomplished with little extra time spent exam-\nining the plans generated.\n286\nSection 3\nAI \n\n\nIt should be noted that this is only one way of performing plan merging.\n[Thangarajah02] and [Thangarajah03] present different systems and ways of per-\nforming plan merging that may be more appropriate for agents acting over a longer\nterm than the agent described here. For example, the plan-merging algorithm\ndescribed in [Thangarajah03] would be especially well suited to a strategy game AI\nopponent, able to accomplish goals in a variety of different ways and potentially delay\nactions to take advantage of positive merge opportunities.\nPlanning is a versatile AI system, with many opportunities for expansion and\nimprovement. Even if plan merging is not useful in a given situation, the ideas it suggests\nare applicable to other AI systems, or even other planning systems such as hierarchical\ntask networks (HTNs). Thinking about the behavioral improvements afforded by such\ntechniques lends the agents greater intelligence and the players a better experience.\nReferences\n[Foulser92] Fousler, David, Li, Ming, and Yang, Qiang. “Theory and Algorithms for\nPlan Merging.” Artificial Intelligence, 57(2–3): pp. 143–181, 1992.\n[Orkin04a] Orkin, Jeff. “Applying Goal-Oriented Action Planning to Games,” AI\nGame Programming Wisdom 2, Charles River Media, 2004.\n[Orkin04b] Orkin, Jeff. “Symbolic Representation of Game World State: Toward\nReal-Time Planning in Games.” AAAI Challenges in Game AI Workshop Technical\nReport, 2004.\n[Orkin06] Orkin, Jeff. “Three States and a Plan: The A.I. of F.E.A.R.,” Proceedings\nfrom Game Developers Conference, 2006.\n[Thangarajah02] Thangarajah, John, Winikoff, Michael, Padgham, Lin, and Fischer,\nKlaus. “Avoiding Resource Conflicts in Intelligent Agents,” Proceedings of the\n15th European Conference on Artificial Intelligence 2002 (ECAI 2002).\n[Thangarajah03] Thangarajah, John, Padgham, Lin, and Winikoff, Michael. “Detect-\ning & Exploiting Positive Goal Interaction in Intelligent Agents,” AAMAS ’03,\nJuly 14–18, 2003.\n3.6\nGoal-Oriented Plan Merging\n287\n",
      "page_number": 309,
      "chapter_number": 33,
      "summary": "In clustering IPGs, the clustering criterion functions utilize the distance measure\nd(xij,xpq), which is the distance between the jth member of cluster i and the qth mem-\nber of cluster p, where the distance d represents the graph edit distance between two\nmembers Key topics include plan, planning, and actions.",
      "keywords": [
        "Plan Merging",
        "Plan",
        "actions",
        "agent",
        "game",
        "Interactive player graphs",
        "Goal-Oriented Plan Merging",
        "Totally-ordered plans",
        "Merging",
        "player",
        "planning",
        "data",
        "action planning systems",
        "goal-oriented action planning",
        "clustering"
      ],
      "concepts": [
        "plan",
        "planning",
        "actions",
        "game",
        "goal",
        "clustering",
        "clusters",
        "based",
        "base",
        "merging"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 1",
          "chapter": 27,
          "title": "Segment 27 (pages 248-255)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 23,
          "title": "Segment 23 (pages 220-230)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 35,
          "title": "Segment 35 (pages 304-312)",
          "relevance_score": 0.41,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 27,
          "title": "Segment 27 (pages 536-554)",
          "relevance_score": 0.41,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 54,
          "title": "Segment 54 (pages 513-523)",
          "relevance_score": 0.41,
          "method": "api"
        }
      ]
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 321-328)",
      "start_page": 321,
      "end_page": 328,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank \n\n\n289\n3.7\nBeyond A*: IDA* and \nFringe Search\nRobert Kirk DeLisle\nG\nraph search techniques are ubiquitous in game programming. Regardless of \nthe game genre, methods of graph search inevitably form a basis for game AI.\nThe currently leading genre of 3D FPS games is heavily dependent on pathfinding\napproaches that enable non-player characters to move within the environment for the\npurpose of self-defense or aggressive action. This can also be extended to 2D (or\n2.5D) games in which maze or terrain traversal is an integral part of gameplay. Fur-\nthermore, games such as checkers, chess, Othello, and even tic-tac-toe involve some\nlevel of evaluation of game trees or state graphs in order to develop convincing and\ncompetitive artificial intelligence.\nWithin the realm of path-planning, problems typically take on the form of trees,\nwith the start being considered the root of the tree (see Figure 3.7.1).\nThe root node can then be expanded into a number of child nodes that represent\nall the next possible steps in the search. The typical 2D pathfinding process is most\nobvious with the children representing each of the directions of allowed movement.\nFor example, if the four cardinal directions are allowed, the root node expands into\nfour child nodes, one for each direction, north, south, east, and west. Diagonal move-\nments increase this to eight child nodes with the additional four representing north-\nwest, northeast, southwest, and southeast. Child nodes can be further expanded as\nyou extend the path in search of the goal. This type of problem formulation can also\nbe applied to problems such as searching for the shortest possible solutions for a\nscrambled Rubik’s Cube. The initial, scrambled state of the cube is the root, and each\npossible turn of a cube face corresponds to a child of that root node. By formulating\nproblems in this way, as graph traversal problems with starting states and goal states\nwithin the graph, you open the door to a number of algorithms.\n\n\nA* and Dijkstra\nA* has emerged as the most common search algorithm for pathfinding within game\nAI. The history of A* begins with breadth first search, in which all the children of the\nroot node are expanded and evaluated before moving on to the next level of tree\ndepth. When the goal is not identified at the current depth, the next layer of children\nare expanded and evaluated. Dijkstra modified this algorithm by adding an “open\nlist” and “closed list” to provide two fundamental capabilities:\n• Each node keeps track of the cost of the path to that point, and the open list can\nbe sorted based upon this cost allowing a “best first” search strategy. This is par-\nticularly useful when transitions from one node to another do not have the same\ncost, such as choices in moving through swamp versus dry ground. Allowing\nexpansion of the best path thus far can bias the search away from costly paths.\n• Together, the open and closed lists act as a catalog of previously evaluated nodes,\nthus preventing the re-expansion of already visited nodes. These additions\n290\nSection 3\nAI \nFigure 3.7.1\nA search grid and its associated search\ntree are shown with corresponding grid and tree nodes\nsimilarly colored. A path is shown in both with arrows.\nPaths that result in redundantly visiting the same node\nhave been removed.\n\n\nimprove upon Breadth First Search considerably; however, further improvements\nwere to be found by incorporating an “Informed Search” strategy through the use\nof heuristics.\nUp to this stage, the cost of any particular node is considered to be the cost from the\ngoal to this point, and is commonly referred to as g(). An uninformed search of this sort\nis significantly improved upon if you include an estimate of the cost from this point to\nthe goal. This heuristic calculation, typically referred to as h(), gives you another method\nto estimate the total path cost and once again significantly biases the search toward the\ngoal. The resulting overall cost of any particular node becomes f() = g() + h(), and it\ndeserves mentioning that h() should always be admissible, or an underestimate of the\ncost from the node to the goal. If h() ever overestimates the cost to the goal, searching\npotentially fruitful paths may be delayed or missed completely. A* follows the same gen-\neral algorithm as Dijkstra’s, but the cost associated with any particular search node now\nincludes the heuristic cost, that is, the estimated cost to the goal. Algorithm 3.7.1 shows\na comparison of Dijkstra’s algorithm and A*.\nAlgorithm 3.7.1\nDijkstra’s Algorithm and A*\nopen – priority queue of search nodes\nclosed – searchable container of search nodes (such as an associative\narray)\nroot = start node\npush root onto open\nwhile goal not found and open not empty\nsort open by f() of each search node\nremove top of open and set to current node\nif current node = goal\nstop\nelse\npush current node onto closed\nfor each child of current node\nif child present in closed\ncontinue\nelse\nset child’s f() = g() + h()(for Dijkstra’s, h() = 0)\npush child onto open\nIt comes as no surprise that A*’s most fundamental weakness is the management\nof the open and closed lists. The open list must be maintained in sorted order with the\ntop of the list being the node with the lowest cost. Perhaps more significantly, the\nopen and closed lists are continuously polled to determine whether a node has already\nbeen evaluated, and this can lead to a high computational burden. Although various\noptimizations of A* have been developed, the overall costs associated with the open\n3.7\nBeyond A*: IDA* and Fringe Search\n291\n\n\nand closed lists can lead to loss of performance or even to the complete lack of func-\ntion if the search space is extremely large. Although simple 2D pathfinding problems\nmay not suffer significantly enough from these drawbacks to warrant different\napproaches, pathfinding within a complex 3D environment can easily present situa-\ntions that hinder A*’s capabilities.\nOther problems, such as searching for the shortest solution for a scrambled Rubik’s\nCube, present search spaces so large that A* quickly exceeds the memory capacity of\nthe computer within a few minutes. The 3 \u0002 3 \u0002 3 Rubik’s Cube, for example, has as\nmany as 18 child nodes for any particular search node. Even if you restrict the next\nmove to not include certain manipulations (for example, you should not allow the\nsame side to be turned twice), you can only reduce the number of children in the next\nlayer to approximately 13. This leads to over 1 billion possible states after only eight\nturns! Clearly in such complex search trees, other methods must be used in order to\nsimply enable the identification of a solution.\nThe Iterative Deepening A* (IDA*)\nAn extension of A* is Iterative Deepening A* (IDA*), as described in Algorithm 3.7.2.\nIn its most basic form, this algorithm eliminates the open and closed lists. It is true\nthat this imposes the risk of repeated evaluation of states, but this may be accommo-\ndated by properly structuring the way in which nodes are expanded (specific ordering,\nprevention of backtracking, and so on).\nAlgorithm 3.7.2\nIterative Deepening A* (IDA*)\nroot = start node\nthreshold\n= root’s g()\nperform a depth-first search starting at root\nif goal not found,\nset threshold = minimum g() found that is higher than current\nthreshold\nrepeat depth-first search starting at root\ndepth-first search(node):\nif node = goal\nreturn goal found\nif node’s f() > threshold\nreturn goal not found\nelse\nfor each child of node, while goal not found, depth-first\nsearch(child)\n292\nSection 3\nAI \n\n\nIt may also be self-accommodating due to the fact that a node expanded early will\nhave a lowered value for g() than if it is expanded later, and should always have the\nsame value for h() regardless of when it was evaluated. In IDA*, a cost threshold is\nestablished for f() defining the maximum allowable cost above which a node will not\nbe evaluated. All nodes are expanded below this threshold and if the goal node is not\nfound, the threshold is increased. As you have no history maintained, you must reini-\ntiate the search from the original start node and expand all nodes allowed given the\nnew threshold. It may seem counterintuitive to repeat the evaluation of all previous,\nnon-goal nodes, but the cost of expanding and evaluating a node is typically much\nlower than the cost of maintaining the open and closed lists. In addition, the frontier\nnodes, those at the edge of the search that were not explored before, will always be\ngreater in number than the number of expanded nodes below the threshold. This fact\neffectively reduces the cost of re-investigation of previous nodes to a fraction of the\ncost to expand the new frontier. The ultimate result is minimal overhead in terms of\nmemory at the expense of time required for the search.\nThe Fringe Search Algorithm\nBetween A* and IDA* is an algorithm called Fringe Search (see Algorithm 3.7.3), in\nwhich nodes are expanded given a cost threshold as in IDA*, but in this case the fron-\ntier nodes are not lost. Rather, the frontier nodes are maintained in now and later lists.\nAlgorithm 3.7.3\nFringe Search\nnow – linked list of search nodes, list order determines order of\nevaluation\nlater – linked list of search nodes\nroot = start node\nthreshold = root’s g()\npush root into now\nwhile now not empty\nfor each node in now\nif node = goal\nstop\nif node’s f() > threshold\npush node onto end of later\nelse\ninsert children of node into now behind node\nremove node from now and discard\npush later onto now, clear later\nset threshold = minimum g() found that is higher than current \nthreshold\n3.7\nBeyond A*: IDA* and Fringe Search\n293\n\n\nThe node at the top of the now list is evaluated and if its f () value is greater than\nthe threshold, it is moved to the later list. If the f() value is lower than the threshold,\nthe node’s children are expanded and the current node is discarded. The newly\nexpanded child nodes are added to the top of the now list and are thus next in line for\nevaluation.\nThis procedure maintains the list in a weakly sorted order and effectively expands\nthe nodes in a depth-first fashion much like IDA*. If the goal is not found after the\ncompletion of one pass through the now list (one iteration), the threshold is increased\nas it was in IDA*, the later list is transferred to the now list, and search is resumed\nfrom the top of the now list. Although the fringe-search process does require mainte-\nnance of the now and later lists, there is no sorting cost. Furthermore, this extra mem-\nory cost is lower than that of A*, because there is no need to store all previously\nevaluated nodes. Fringe search also does not suffer from speed losses seen with IDA*\ndue to repeated search from iteration to iteration.\nIn a study by Bjornsson, Enaenberger, Holte, and Schaeffer [Bjornsson05], these\nalgorithms were compared using game maps extracted from Baldur’s Gate II. It was\nfound that search times for fringe search were reduced as much as 25–40% compared\nto A* and 10 times compared to IDA*. These improvements in search times over\nIDA* were maintained even though IDA* was optimized to accommodate repeated\nvisits to nodes in the search tree. Overall, the gains in speed were attributed to the lack\nof needing to maintain an open list in sorted order. The cost for fringe search’s perfor-\nmance is obviously increased memory usage due to the requirement to maintain some\ndegree of the search’s history in the now/later lists. In this way, fringe search seems to\nrepresent a useful intermediate between A* and IDA*.\nConclusion\nThere is no paucity of algorithms for graph search and pathfinding. Although A* rep-\nresents the most widely used algorithm, the degree of specialization and optimization\nof the A* algorithm for individual cases expands the set tremendously. The driving\nforce behind algorithm selection has always been and will always be defined by mem-\nory and time constraints, and in nearly every case one must be traded for the other.\nIDA* and fringe search represent useful modifications of the A* family of algorithms\nand may ultimately provide advantages over traditional approaches to pathfinding.\nReferences\n[Bjornsson05] Bjornsson, Yngvi, Enzenberger, Markus, Holte, Robert, and Schaeffer,\nJonathan. “Fringe Search: Beating A* at Pathfinding on Game Maps,” IEEE Sym-\nposium on Computational Intelligence and Games (2005), pp. 125–132.\n294\nSection 3\nAI \n\n\n295\nS E C T I O N\n4\nAUDIO\n",
      "page_number": 321,
      "chapter_number": 34,
      "summary": "This chapter covers segment 34 (pages 321-328). Key topics include search, node, and cost. Regardless of \nthe game genre, methods of graph search inevitably form a basis for game AI.",
      "keywords": [
        "Search",
        "Fringe Search",
        "node",
        "IDA",
        "search nodes",
        "cost",
        "child nodes",
        "algorithm",
        "goal",
        "list",
        "search nodes root",
        "open",
        "root node",
        "Fringe Search Algorithm",
        "threshold"
      ],
      "concepts": [
        "search",
        "node",
        "cost",
        "algorithms",
        "threshold",
        "expanded",
        "game",
        "allowed",
        "push"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 25,
          "title": "Segment 25 (pages 240-248)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "Segment 32 (pages 270-277)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 19,
          "title": "Segment 19 (pages 158-165)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 27,
          "title": "Segment 27 (pages 248-255)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 32,
          "title": "Segment 32 (pages 296-313)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 329-336)",
      "start_page": 329,
      "end_page": 336,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank \n\n\n297\nIntroduction\nAlexander Brandon\nG\name audio programming is becoming more complex than ever. As game audio is\ngetting closer and closer to film post production, a great deal of factors come\ninto play. Where will point-sourced specialized sounds be in relation to the camera?\nWill they be looped? Will there be a delay on them, or will they be in sequence? How\nare they triggered? How are they categorized and mixed? The authors in this section\nhave provided some very impressive fresh ways to tackle new issues like these so that\nyour titles can remain competitive with high audio quality.\nJason Page from Sony Computer Entertainment Europe provides some insights\ninto the revolutionary cell processing power of the Playstation 3 and the MultiStream\ntool Jason and his team has developed. Robert Sparks provides a vital yet elegant solu-\ntion for multiple layers of mixing groups. Especially with the different hardware play-\nback settings on multiple platforms, this particular gem is a godsend. You might have\nread recently that the pro studio standard for effects, Waves, are in use in Halo 3.\nCheck out Mark France’s article for more information on how to get this kind of real-\ntime effect functionality. Ken Noland also provides even deeper tips for optimizing\nthese effects. Finally, Stephan Schütze bangs his head against the wall of repetition,\nwhich is still all too present in games today.\nAll of these authors are respected pros in the field with ideas that could make your\nnext game the next award-winning title for audio. Enjoy the gems!\n\n\nThis page intentionally left blank \n\n\n299\n4.1\nAudio Signal Processing\nUsing Programmable\nGraphics Hardware\nMark France\nmark@raccoongames.com\nR\neal-time modern audio processing can sometimes be very compute-intensive, as\nmany algorithms often need to be performed simultaneously. Programmable digital\nsignal processors are usually available only to developers and are much too expensive for\nconsumers. Also, modern sound cards are still only fixed function implementations that\nevolve at a slower pace and can be limiting for audio programmers. This gem suggests\ntechniques that enable you to offload audio routines from the CPU and benefit from the\nGPU’s relatively huge SIMD (Single Instruction Multiple Data) parallel stream process-\ning power (see Figure 4.1.1). This increased flexibility allows creation of customizable,\nhigh-quality reverb models that can be calculated in real-time from scene geometry,\nrather than relying on the use of simple presets found in previous generation hardware. \nFIGURE 4.1.1\nComparison of computational power for GPUs and CPUs\n[Owens07].\n\n\n300\nSection 4\nAudio \nGPGPU Programming Overview\nThe GPU’s shift to a programmable pipeline and its increasing programmability has\nallowed it to be used as a powerful general purpose coprocessor. The pipeline shown\nin Figure 4.1.2 can be programmed for use in applications other than the specific\ngraphical ones it was designed for. This is known as GPGPU (General Purpose com-\nputation on a Graphics Processing Unit) programming and has been successfully used\nfor applications from artificial neural networks [Rolfes04] to cloth physics simula-\ntions [Zeller05].\nFIGURE 4.1.2\nThe recent graphics pipeline.\nFor GPGPU, fragment shaders are more useful because there are more fragment\npipelines than vertex pipelines and, because the fragment processor is at the end of \nthe pipeline, it allows for direct output. Shader programs can be written in assembly\nlanguage or high-level shader languages such as Cg, HLSL, and GLSL. My preferred\nlanguage for this purpose is Brook for GPUs, which is specifically designed for stream\nprocessing and runs directly on GPUs by generating Cg code with a C++ runtime.\nMore information on GPGPU programming can be found at [GPGPU07].\nGPU Audio Optimization\nGPU features such as multiple execution units or multiply-accumulate instructions\nare similar to those of professional audio DSP hardware [Gallo04], therefore it can be\nsuggested that the ubiquitous GPU can be used as an efficient DSP substitute.\nGPUs operate on vectors containing four floats, often representing the RGBA\ncomponents. Therefore, audio sample data is often stored in one of these components\nand 1D arrays of samples are mapped to 2D square textures before being processed on\nthe GPU.\nDoes using the GPU for audio calculations significantly optimize performance?\nWhalen [Whalen05] asked this question by using shading languages to process an\narray of DSP effects on both graphics hardware and CPU. The point was to discover\nwhich was fastest. It was discovered that algorithms such as chorus and compression\n\n\nhad a significant decrease in execution times when processed on the GPU; others such\nas Filter and Delay effects were slightly slower. The GPU excels at certain tasks that\nare suited to its model of stream processing—that is, many processors executing the\nsame code in parallel—therefore, not all audio programming techniques may be opti-\nmized by running them on the GPU.\nAudio Effects\nThis section concentrates on describing algorithms for chorus and compression audio\nprocessing effects. A chorus effect introduces a short delay and slight pitch change to\nan audio signal in order to add an audible “thickness” to the sound. The chorus effect\ncan be used in games to help create a surreal “dreamy” effect. The processing of this\neffect requires two texture lookups; interpolation between them is shown here:\nlookahead(coord, index)\n{\ncoord.x = coord.x + index * step;\nif(coord.x > 1.0)\n{\nrowsUp = floor(coord.x / rowSize);\ncoord.x = coord.x - rowsUp * (1 + step);\ncoord.y = coord.y + rowsUp * step;\n}\nreturn coord;\n}\nchorus(coord, texture)\n{\ns1 = lookUp(texture, coord);\ns2 = lookUp(texture, lookahead(coord, 20 * sin(coord.x)));\nreturn interpolate(s1, s2, 0.5);\n}\nAudio compression effects that are unrelated to data compression reduce the\ndynamic range of audio signals and are useful for balancing the game’s overall audio\nmix. This effect needs one texture lookup and the logarithmic compression calcula-\ntion to be performed:\ncompress(coord, texture)\n{\ns1 = lookUp(texture, coord);\nreturn pow(abs(s1), 1 - level / 10);\n}\nMany other audio effects, such as delay and normalization, can be optimized\nusing similar techniques.\n4.1\nAudio Signal Processing Using Programmable Graphics Hardware\n301\n\n\nRoom Acoustics\nAnother type of audio-processing technique that could be made more efficient using\nGPU is calculating real-time room acoustics, as demonstrated by [Jedrzejewski06]. \nCalculating echoes, occlusions, and obstructions in real-time from environment\ngeometry requires a lot of computation; a ray tracing method can be used to imple-\nment this, which is well suited to GPU processing. Ray tracing for acoustics is differ-\nent from graphical ray tracing because the scenes that are computed don’t need to be\nvisually accurate and smaller render targets are often used. Rays are traced from the\nsound source until they reach the listener’s position.\nPrecomputation\nThe scene geometry consists of polygons that represent walls; other game objects that\nare considered large enough to affect the audio environment can be approximated as\nboxes. During the precomputation stage of this algorithm, the geometry is partitioned\nto a BSP tree with solid convex regions for leaves. After the BSP tree is computed, it is\nused to create a portal graph that shows the paths between each leaf. If a portal and\npolygon lay on the same plane in a certain leaf, additional leaf splits need to be made;\nnew portal and paths computation might be needed if there is need for additional leaf\nsplitting. Information on portals and planes is stored in separate 1D textures that con-\ntain data such as whether it is a portal or plane, and its absorption values. The leaf data\ncontains indexes into the plane texture and how many planes it contains. This stage\ncan be performed every time the scene geometry is changed.\nReal-Time Rendering\nFragment shaders are executed that first compute intersections in the current leaf for\neach ray, and then the propagation to new leaf, and then the reflected ray and intersec-\ntion with listener. The listener’s position can be approximated as a bounding sphere;\noften the bounding volume for the player’s avatar is used if the listener object is\nintended to represent the player. Pseudocode for the shader programs is shown here:\nLeafPlaneIntersect(Ray)\n{\nGet plane index for current Ray\nfor (i=1; i<=6; i++)\n{\nIntersect with plane for current leaf\nStore data for closest intersected rays\n}\n}\nPropagateRay(Ray)\n{\nCheck if currentLeaf contains more planes\n302\nSection 4\nAudio \n\n\nif(currentLeaf == listener.leaf)\nIntersect Ray with boundingsphere\nif(intersection with plane)\nReflect ray and its absorption\nIf(intersection with portal)\nSet new leaf for ray\n}\nThe environmental reverberation model is then constructed. It involves retrieval\nof the render target texture and final ray data. Three render target textures are used,\none for state information, one for the ray origin, and one for the ray direction.\nConclusion\nNot all audio algorithms can take advantage of the GPU’s parallel computation; how-\never, certain tasks such as some audio effect algorithms and acoustical ray tracing excel\nwhen executed on graphics hardware. Other than the audio techniques described in\nthis gem, GPUs have also been shown to greatly outperform CPUs for techniques\nsuch as FFT (Fast Fourier Transforms), which are ubiquitous in audio processing.\nWith PCI-Express cards becoming common, transfering large amounts of data from\nvideo memory to system is no longer a significant bottleneck. These techniques show\nthat the GPU can be utilized as a practical optimization for many audio algorithms\nand even a feasible replacement for specialized audio DSP hardware.\nReferences\n[Buck04] Buck, Ian, et al. “GPGPU: General Purpose Computation on Graphics\nHardware,” SIGGRAPH, 2004.\n[Gallo04] Gallo, Emmanuel, and Tsingos, Nicolas. “Efficient 3D Audio Processing\nwith the GPU,” Proceedings of the ACM Workshop on General Purpose Com-\nputing on Graphics Processors, ACM, 2004.\n[GPGPU07] “General Purpose Computing Using Graphics Hardware,” available\nonline at www.gpgpu.org.\n[Jedrzejewski06] Jedrzejewski, Marcin, and Krzysztof, Marasek. “Computation of\nRoom Acoustics Using Programmable Video Hardware,” Computer Vision and\nGraphics, Springer Netherlands, 2006.\n[Owens07] Owens, John D., Luebke, David, Govindaraju, Naga, Harris, Mark,\nKrüger, Jens, Lefohn, Aaron E., and Purcell, Tim. “A Survey of General-Purpose\nComputation on Graphics Hardware,” Computer Graphics Forum, 26(1), pp.\n80–113, March 2007.\n[Rolfes04] Rolfes, Thomas. “Artificial Neural Networks on Programmable Graphics\nHardware,” Game Programming Gems 4, Charles River Media, 2004.\n4.1\nAudio Signal Processing Using Programmable Graphics Hardware\n303\n",
      "page_number": 329,
      "chapter_number": 35,
      "summary": "Jason Page from Sony Computer Entertainment Europe provides some insights\ninto the revolutionary cell processing power of the Playstation 3 and the MultiStream\ntool Jason and his team has developed Key topics include effects, rays.",
      "keywords": [
        "audio",
        "Graphics Hardware",
        "GPU",
        "Programmable Graphics Hardware",
        "Audio Signal Processing",
        "Graphics",
        "hardware",
        "ray",
        "processing",
        "audio DSP hardware",
        "audio processing",
        "Programmable Graphics",
        "general purpose",
        "Audio Signal",
        "GPUs"
      ],
      "concepts": [
        "effects",
        "ray",
        "rays",
        "hardware",
        "processing",
        "process",
        "graphics",
        "graphical",
        "leaf",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 8",
          "chapter": 53,
          "title": "Segment 53 (pages 511-519)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 52,
          "title": "Segment 52 (pages 506-515)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 18,
          "title": "Segment 18 (pages 348-366)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 11,
          "title": "Segment 11 (pages 104-112)",
          "relevance_score": 0.43,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 50,
          "title": "Segment 50 (pages 481-489)",
          "relevance_score": 0.43,
          "method": "api"
        }
      ]
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 337-344)",
      "start_page": 337,
      "end_page": 344,
      "detection_method": "topic_boundary",
      "content": "[Whalen05] Whalen, Sean. “Audio and the Graphics Processing Unit,” available at\nhttp://www.node99.org/projects/gpuaudio/, 2005.\n[Zeller05] Zeller, Cyril. “Cloth Simulation on the GPU,” SIGGRAPH, NVIDIA\nCorporation, 2005.\n304\nSection 4\nAudio \n\n\n305\n4.2\nMultiStream—The Art of\nWriting a Next-Gen Audio\nEngine\nJason Page, Sony Computer Entertainment,\nEurope\nJason_Page@scee.net\nF\nor the past three years, the SCEE R&D’s audio team has been writing a “next-\ngen” audio engine for the Playstation 3 that was to be part of the official SDK. My\naim for this gem is, having been a part of the SCEE engine project, to give you an idea\nof the work involved in designing and creating your own audio engine. In turn, this\ninformation may be useful in allowing you to create your own audio engine, or by\nknowing the magnitude of the job depending on your goals, you might decide to use\nsomething from an SDK or middleware provider instead.\nI’m not going to cover the MultiStream function calls in detail—any licensed PS3\ndeveloper can look at the docs at any time, but I would like to bring your attention to\nthe issues that my team had to overcome. At the end of this gem, I will look at new\nproblems that might also need resolving due to the expectations of next-gen audio.\nAt the very beginning of the project, we had no idea what hardware would be avail-\nable for us; no idea of how much RAM we would require or the expected performance.\nWe decided to take the approach of creating everything in software and to expect\nthere to be no help from hardware DSPs. Later, we found that this was indeed the cor-\nrect choice, as there was to be no audio hardware in the PS3. I also find that planning\naudio engines around known hardware or game requirements can mean that the final\nresult is rather mediocre. If you know that a game requires a low-pass filter for occlu-\nsion and obstruction but don’t allow the capability for such a filter to handle high-\npass, band-pass, notch—or indeed the many other filter types—how many creative\npossibilities have you lost? Thinking big means you can trim down areas that might\nnot be feasible in the long term, and also means you’ve got something different from\neveryone else. \nA “stream” in MultiStream consists of audio data to play (up to eight channels),\nplayback frequency, volume parameter/surround sound position, amplitude envelope,\nDSP effects, and output routing locations.\n\n\nHow It All Began\nApart from the technical aspects of creating an audio engine, we also had to decide on\nother features to include to make MultiStream “next-gen.” The following sections\nexplain a little more about the questions we had to answer. Again, the idea of creating\nyour own audio engine might be appealing, but such a project could end up taking years\nto complete, and there are many things that you need to have planned for in detail first.\nHaving a team working on an audio engine for three years does not come cheap. The\nfollowing sections describe the design process we used before writing any code.\nUnderstanding “Next-Gen” Audio\nAlthough it should seem simple, creating an audio engine that makes games sound\nbetter than ever before isn’t as straightforward as it may seem. The ability to play CD\nquality audio tracks has been available to game developers for over a decade. The abil-\nity to add high-quality reverb (although perhaps not to the standard of professional\naudio plug-ins by companies such as Yamaha or Lexicon) to hundreds of audio chan-\nnels has been with us since the late 90s. Would just upping these values create a “next-\ngen” feel? After all, it is not we who decide this—it’s Jimmy and Jenny who just spent\n$60 on a game and need to be impressed.\nIf you are thinking of writing your own audio engine, first ask yourself what it\nneeds to do. For MultiStream, one purpose was to allow game audio to sound better\nthan, and in a certain sense, different than current generations. More channels but with\nthe same audio capabilities as previous audio engines—this might make a game sound\nbetter by creating a richer environment, but it’s unlikely that it will really stand out of\nthe crown as being “next-gen.” Sure, you can do what you want with offline processing,\nbut the real power of next-gen is to do it all in real-time. There’s a whole load of great\nsounding effects like vocoders or convolution reverb that have never been done before in\nreal-time, but these all need frequency domain processing. This has previously been seen\nas impractical to run in real-time along with a full game, but we wanted it. It soon\nbecame obvious that next-gen audio means gaining expertise in a number of areas we’d\nnever had to worry about before.\nWish Lists\nFrom my experience with audio on the PS3, it seems that a good approach for anyone\nto take is to make a wish list of what kind of audio processing they require. At the\ntime of writing, it seems like just about any type of audio process is not only possible,\nbut is also possible in real-time. To give you an example, MultiStream can process over\n50 mono convolution reverb effects in real-time. However, this also means that there\nis no processing left for any audio channels! But, it does mean that even if you require\none convolution reverb, which was previously thought of as not being possible for\ngames, it is now a reality. Programming audio on the PS3 does literally allow for new\napproaches to audio, where techniques that had previously only been seen in profes-\nsional music packages can now be used.\n306\nSection 4\nAudio \n\n\nHow Many Audio Channels?\nIt is presumed that for any audio engine, it must be able to process enough audio chan-\nnels of data to meet expectations. Yes, more channels do help produce “better audio,” but\nas Mozart once said, “The silences between the notes are as important as the notes them-\nselves.” Even so, today’s (and tomorrow’s) game requirements mean more audio channels\nare required for creating the same game sounds as before. It would be reasonable to think\nthat a car engine sound might be created with at least 25–30 audio channels:\n• Car engine rev loops * eight (each for, say a 1000 RPM rev range)\n• Car exhaust loops * eight (recorded at the same time as the car engine)\n• Skid sounds (four looping skid sounds, one for each wheel)\n• Road rumble sounds (four sounds, one for each wheel)\n• Gear changing noises\nDue to the number of channels required for a single car in the standard race\ngame, it was previously only the player’s car that used such a detailed model. All other\nAI cars might be using a far lower channel count, due to hardware constraints, CPU\nand/or RAM constraints.\nToday, this is not such a problem. MultiStream has limited its maximum channel\ncount to 512. In a racing game, this would make it possible to handle 20 race cars, all\nwith the same audio capabilities as the player’s own car. Of course you could go beyond\n512 voices (depending on platform you’re developing for), but you have to draw the line\nsomewhere. In our case, sticking with this limit still means there’s plenty of processing\ntime to spare for DSP effects, buss routing, re-sampling, and amplitude envelopes. \nFinally, in the case of car engines, it must also be noted that by the time you read\nthis, the method of cross-fading loops for engines may well be a thing of the past.\nMethods that use granular synthesis techniques, whereby playback of small sections—\nor “grains”—of a car engine sample, create a far more realistic engine sound than loops\nalone. Again, this was not really possible until now. The RAM footprint required for\nsuch samples without the ability to use file formats such as MP3 or ATRAC3 meant\nthat these techniques, although tried and tested in theory, used too much RAM.\nSample Formats\nPlayback of an audio file must also take into consideration the format of the sample\ndata and the number of channels. Note that stereo files do not necessarily take twice\nas much processing or RAM as mono files; this depends on the file format. MP3 joint\nstereo mode for example records some of the audio in mono, where if the left and\nright channels contain the same frequencies, there’s no need to store them twice.\nActually, there are indeed many books and Websites explaining why you should store\nthem twice, but again, explaining this would take too many pages! \nFor game audio, one of the main issues is accessibility. Sample accurate playback\nis something you really need to aim for. For the MP3 format, it is relatively simple to\nplay back audio approximately +/–1000 samples from where is required. Therefore,\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n307\n\n\nextra work (and RAM and CPU) is required to make MP3 fully sample accurate, or as\nI like to call it, “game-compatible.”\nThe file formats your audio engine accepts also have to be considered, as shown\nin Table 4.2.1.\nTable 4.2.1\nAudio Engine File Formats\nFormat\nPros and Cons\nNotes\nFloat32 PCM\nPros:\nNo decoding required\nBest quality audio\nEasy to loop to sample boundaries\nCons:\nLarge memory footprint\n16-bit PCM\nPros:\nGood “CD-quality” audio\nEasy to loop to sample boundaries\nSmaller memory footprint than Float32\nCons:\nStill quite a large memory footprint.\nUnlikely that a game will have enough \nRAM to store all samples in this format.\nADPCM\nPros:\nPassable quality\nSmaller memory footprint than PCM\nQuite fast to decode\nCons:\nPossible that decoders only handle \nmono input files\nHigher CPU overhead required for \ndecoding\nLooping to sample boundaries may \nnot be possible\nMP3\nPros:\nGood quality\nExcellent compression\nMany decoders can handle multiple \nchannel data\nCons:\nHigh CPU overhead\nNot easy to seek to sample accurate \npositions\n308\nSection 4\nAudio \nFaster processing means more \nCPU spare for other tasks.\nMore usable in games than float32\nformat, but in many cases, the \nlisteners aren’t going to notice \nthe difference.\nIn many cases, this is still used as \na standard game audio format. It \noffers compression and a fast decode.\nSample accurate seeking or looping\nmight not be possible; it does not\nrequire too much tweaking of the\ninput data to align loop markers to\nboundaries.\nBest for getting as many sounds in\nRAM at one time, but you must\nconsider the processing required to\ndecode such formats.\n\n\nIt also has to be noted that codecs such as MP3 require data buffers per audio\nchannel too, where decoded data and other information needs to be stored. Consider-\ning that MultiStream can play 512 MP3s at once, even if each audio channel only\nrequired a 2KB buffer, the codec still requires 1MB. Although this may seem obvious,\nit is areas such as this that are best explained to game producers and designers early on\nin the development cycle when they request such codecs. \nAlso, as shown in Table 4.2.1, although float32 input would offer the best qual-\nity, another issue can be DMA bandwidth (a method for transferring data around a\nsystem). In which case, using 16-bit data could half the bandwidth, while still produc-\ning audio of CD quality.\nLoop markers need to be considered too. The loop markers may be stored in the\nfile header (such as .WAV), within the sample data (such as the SCE’s .VAG ADPCM\nformat), or not at all (such as .MP3s converted from .WAVs, where the loop informa-\ntion is lost). Handling looping of audio is not as simple as it may seem. If the sample\nis memory resident, you just play the sample and know where in RAM the address of\nthe loop position is. If you are streaming audio content, care needs to be taken so the\nloop point is in memory when it comes time to loop.\nTo Stream or Not to Stream\nMost audio systems need to be aware that data may be streamed. Here, your audio\nengine has to cater for some kind of buffer mechanism, where data is copied into an area\nof RAM for playback (this data is normally loaded from disk, but there could also be\nPCM data obtained from a decoded .MP3 via a codec outside of your audio engine).\nFrom experience I would not recommend handling data-loading functions\nwithin your audio engine. If the audio engine requires more data for a streaming\nbuffer, it should request this. (In MultiStream, this is handled via a callback function.)\nIf you start handling data loading in your audio engine, expect a world of pain later\non. Here’s why:\n• You need to sync other game data-loading with your audio engine loading.\n• You need to handle all cases of corrupt data loads (disk removed during loading or\na damage disk). \nEssentially, your audio engine becomes far trickier to optimize and maintain. It\nmust also be noted though, that any audio streaming needs to take priority over any\nother data loading. Why? Simply due to the fact that if audio is not streamed in time,\nyou will have to either repeat playback of the last buffer of data (which sounds like a\nbroken CD player) or play silence. Either of these may well cause your title to fail dur-\ning any QA process.\nEven if your audio system is not going to handle streaming of data from disk\ndirectly, the programmer(s) in charge of the IO systems must be aware of the follow-\ning priorities in order of importance:\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n309\n\n\n• Currently playing streams must be updated first\n• Newly requested streams can be updated next\n• Data load for the game happens last\nUsing this method, if a player keeps requesting more audio to be streamed, say,\neach game frame, any currently playing music will still play correctly without skipping\nor jumping. In many cases, streaming is used for areas such as sports commentary. This\nis also usually in context with the current action on-screen, which is why the playing of\nsuch audio is, as far as I am concerned, more important than game data loading. There\nis nothing worse than a commentator saying the wrong thing at the wrong time. \nThe size of stream buffers is not an exact science. This will depend on the sample\nrate and format of the data you are streaming, along with how often you expect to be\nloading data. Streaming data from hard disk is by no means as tricky as loading from\nDVD, where the time taken for the DVD head mechanism to physically move to the\ncorrect place and the time taken for data to load is also a factor. In many cases, having\nmultiple copies of the same file on a disk is a common technique for speeding up\nDVD loading. The current head position is kept track of in software, and then the\nstreaming engine (note that the streaming engine and the audio engine are separate\nengines) will choose which file on the disk is closest to this position.\nThis method can also help with prioritizing audio data streaming. If multiple\naudio channels need more data, you need to choose which should be the first to be\nloaded. Again, this is not an exact science. If multiple streams require more data, then\nyou need to make sure that they all get that data as soon as possible. If you can’t \nload the data in time, a simple solution is to either increase the stream buffer sizes or\nreduce the sample rate of the audio. Halving the sample rate has the same effect as \ndoubling the stream buffer size. For example, playing 48000 samples at 24kHz will\ntake twice as long as playing 48000 samples at 48kHz.\nThe method of reducing the playback frequency of a stream is also very useful for\ndetermining whether pops or clicks in audio playback are caused by the system run-\nning out of data to process. This modification is normally very simple to make to any\naudio calls, compared to increasing streaming buffer sizes, which can be limited in size\ndue to the restrictions set by other non-audio game requirements. \nFor streaming audio with loop markers, depending on the sample data format,\nthe only time you might know that you need to loop the data is when you’ve reached\nthe loop marker, which is too late. For MultiStream, we decided to ignore all loop\nmarkers within the audio engine. It is therefore up to the user to either decode .WAV\nheaders, or stream correctly to the required data. Not only does this allow the user to\nfeel in control, but it also takes care of any of the issues mentioned previously.\nSo if we are required to loop to a certain offset within a file, we can first check to\nsee if that portion of the file is indeed in RAM. If it isn’t, we need to load this portion\nfirst so that playback will continue as desired. \n310\nSection 4\nAudio \n\n\nVolume Parameters\nSetting volume levels of an audio channel, along with setting its frequency, are the\ntwo most basic audio DSP effects.\nFor MultiStream, you still had a number of issues to decide upon: As the PS3 can\noutput audio up to 7.1, you needed to allow any audio channel to be routed to any\nnumber of speakers. For example, a mono audio signal may want to be heard on both\nthe front-left and the rear-right speakers. This means that any single audio channel\nrequires eight volume parameters. Furthermore, as MultiStream can play data con-\ntaining up to eight audio channels, there are a total of 64 volume parameters available\nper stream. Finally, MultiStream can process up to 512 channels and these volume\nparameters can be Float32s. This means that 512 \u0002 64 \u0002 4 bytes are required just for\nvolume parameters alone.\nYou could reduce the memory footprint if you used 16-bit volume parameters, or\nmaybe even less. Imagine that MIDI volume parameters range from 0–127, giving\nyou 128 possible settings. Why do you need to use floats that give you millions of\npossible settings? First, you must ask when you set a volume parameter in MIDI, is\nthe hardware (or software plug-in) using this value directly? It could be that this value\nis then scaled to work in a floating point system where volumes are ramped toward\nthe required volume level. Secondly, for ease of use, having a system that uses floats\ncan make the rest of the audio engine quicker in general. There will be less need for\nconversion of volume levels between various formats, for a start. \nPlayback Frequency\nAs stated in the “Volume Parameters” section, volume and frequency are the two most\nbasic audio DSP effects.\nWith a purely software-based system, even setting the playback frequency of a\nsample needs consideration. Any resampling is going to take CPU time and it would\nbe foolish to waste this time on such basic functionality. Not only does the resampling\nalgorithm need to be considered, but also the fact that playing back audio at high fre-\nquencies can in turn take longer to process. Therefore, a system that can process 4000\naudio channels may only be able to do so at a maximum playback sample rate of, say,\n48kHz.\nTo explain a little more, if you need to create one second worth of audio data for\nplayback at 48kHz, you need to process 48000 samples to do so. If you need to play\nback at 96kHz, you need to process 96000 samples. “Processing” in this sense could\nmean decoding of MP3 files. So again, playing back a 48kHz MP3 at 96kHz means\nyou need to decode twice as much of the MP3 file.\nPerhaps this processing time could have been spent more wisely? If you require a\nsample to be played at an octave higher than its original pitch, it may make sense to\nresample it down to 24kHz, which in turn means that going one octave higher would\nput it at 48kHz. Simply put, by halving your audio files sample rate, you can cut the\nprocessing required to resample in half.\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n311\n",
      "page_number": 337,
      "chapter_number": 36,
      "summary": "This chapter covers segment 36 (pages 337-344). Key topics include audio, engine, and data. I’m not going to cover the MultiStream function calls in detail—any licensed PS3\ndeveloper can look at the docs at any time, but I would like to bring your attention to\nthe issues that my team had to overcome.",
      "keywords": [
        "Audio",
        "audio engine",
        "Engine",
        "data",
        "audio channels",
        "Next-Gen Audio Engine",
        "sample",
        "Next-Gen Audio",
        "game",
        "RAM",
        "audio data",
        "channels",
        "Graphics Processing Unit",
        "MultiStream",
        "time"
      ],
      "concepts": [
        "audio",
        "engine",
        "data",
        "sample",
        "stream",
        "processing",
        "process",
        "games",
        "channel",
        "require"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 55,
          "title": "Segment 55 (pages 536-543)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 50,
          "title": "Segment 50 (pages 490-497)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 52,
          "title": "Segment 52 (pages 506-515)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 52,
          "title": "Segment 52 (pages 499-510)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 345-352)",
      "start_page": 345,
      "end_page": 352,
      "detection_method": "topic_boundary",
      "content": "Frequency Domain Processing\nFor any frequency domain processing, you are looking at requiring a FFT/iFFT rou-\ntine. Understanding the fine detail of the FFT is not as important as it may seem. Yes,\nthere’s a lot of math involved here, but once written, it’s not something that you really\nhave to worry about again. Using it is a different issue.\nThe main problem with frequency domain processing is choosing the correct win-\ndow size. If the effect you’re working on needs high-frequency resolution (for example,\nsay you’re implementing some kind of parametric EQ), you need a large window. Large\nwindows give very poor time resolution, so it’s not possible to change parameters\nquickly. Large windows also result in greater latency, require more memory, and use\nmore CPU. Although shorter windows do not suffer from these problems, they lead to\npoor frequency domain resolution, which defeats the objective of trying to implement\na frequency domain effect.\nThe answer really lies in finding the right window size for your application, tuning\nit to make the best use of the available resources, and listening to hear if it sounds right.\nEven then, different effects may require different window sizes. Are you prepared to\nswitch window sizes in the signal path, or is a “one-size-fits-all” solution good enough?\nBasics of FFT\nThere are a number of issues to consider when using FFT. First, the number of input\nsamples needs to be double the number of output samples. So, for example, you need\nto feed the FFT 1024 samples for it to output 512. They have implications for other\nroutines too. For things like amplitude envelopes, you may need to actually process all\n1024 samples but then rewind the envelope parameters by 512 samples so that the\nnext time the amplitude envelope is processed, you are using the correct values.\n312\nSection 4\nAudio \nFIGURE 4.2.1\nSimple amplitude envelope\n(fade in/fade out) over 2048 samples.\nThe very big plus point of FFT (and windowing) is that you’ll find it can remove\na lot of possible pops and clicks normally heard with large volume changes or looping\nto boundaries where samples do not match up.\nAs you can see in Figure 4.2.2, on each step, the envelope needs to be re-calculated\nfor the first half of the data packet, even though it has already just calculated it for the\nsecond half of the previous packet.\nNote that some systems would also include a step before this, where, being the\ninverse of the last step, the fade-in part of the amplitude envelope would only process\nthe first 512 samples. This is illustrated in Figure 4.2.3.\n\n\nThis step adds a lot of latency to the final output, which we found to be far too\nnoticeable in real-time applications.\nLatency\nReal-time applications obviously require a low latency. For MultiStream, we decided\nthat it should generate 512 samples per channel each time the update routine is\nprocessed (this technique is known as granularity). When using FFT, outputting 512\nsamples requires 1024 input samples due to windowing functions (see the section\ncalled “Basics of FFT”).\nWith 512 sample granularity, this gives the FFT function enough data to meet\ntwo goals:\n• Latency is low enough for most game requirements.\n• 512 bands (where MultiStream requires 1024 samples as input data) gives enough\nscope for many FFT-based DSP effects but keeps the quality high. If you drop to\n256 bands (512 samples as input data), you would find the audio quality to be too\npoor to be of any use for just about any application.\nProcessing of 512 samples means that the update routine will need to be called\n93.75 times per second (every 10.66 milliseconds):\n48000 samples = 1 second of playback\n48000 / 512 = 93.75 Hz (Number of audio updates required per second)\n1000/93.75 = Audio engine will be called every 10.66 milliseconds\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n313\nFIGURE 4.2.2\nAt least four passes of the data are required when processing in 1024 sample\npackets if you’re using windowing techniques.\nFIGURE 4.2.3\nSome\nsystems use a fading that\nprocesses only the first\n512 samples.\n\n\nThis is generally fast enough to keep the audio in sync with any graphic updates\nrunning at a maximum of 60 updates per second. Even though outputting 512 sam-\nples may seem like an easy task (remember that there are 48000 samples required for\none second of playback), processes such as MIDI sequencers run at a faster rate than\nthis. In many cases, they run up to 240Hz or even 384Hz (between 2–4 milliseconds!)\nTherefore, the problem may be that if a MIDI sequence requires an instrument to start\nplaying, it will not actually start until the next audio update. Now, many people will\nnot notice this, but those who have very good hearing (such as the audio engineers who\nare going to be listening to their work played through your audio engine) will notice. If\na lower latency than 512 samples is required, FFT processing may not be the one for you. \nFor MultiStream, we have both frequency and time domain processing modes. So\nif you do not require frequency domain effects, it is possible to process totally in time\ndomain. This means window size is no longer an issue and we can offer optional gran-\nularity settings of 128 or 256.\nPacket Smoothing\nAs discussed in the “Latency” section, granularity is the number of samples generated on\neach audio update. On the simplest level, each update would use the settings the user\nhas required for each audio channel, such as what frequency and volume with which to\nplay back. One issue to consider here is that if each packet just uses the required volume,\nit is possible to get aliasing artifacts due to the sudden jump of volume. Another artifact\nis clicking or popping, which is noticeable on audio such as car engines where multiple\naudio channels would be cross-faded depending on the motor rev required.\nFor time-domain processing, a filter process is required so that volume changes\nare smoothed, whereas for frequency domain processing, you will find that the win-\ndowing which is required for FFT (such as a hamming or hanning window) does all of\nthe hard work for you.\nAs first discussed in the “Frequency Domain Processing” section, windowing\ntechniques are used when processing frequency data. The reason for windowing when\nconverting to frequency domain is that when you process the data, you only focus on\na single portion of the data. Analysis therefore knows nothing about what audio sig-\nnals proceeded or follow this data and if you don’t take this into consideration, there\nwill be discontinuity between each data packet (known as pops and clicks to you and\nme). Window types such as hanning or hamming are essentially just algorithms used\nto modify each packet of data. Each packet of data is then processed and mixed with\nthe previous packet, producing an output which resembles the desired data. This is a\nreally simplified paragraph on what would normally require chapters in other books,\nbut hopefully there’s enough information here to give you something to Google with!\nWindowing may also mask a multitude of sins that normally cause pops and\nclicks to be output, such as looping samples whose start and end samples do not\nmatch. Note that care must be taken here still. Although looping to any sample might\nsound fine when using window techniques, if for any reason you need to move your\n314\nSection 4\nAudio \n\n\naudio engine to a pure “time domain” mode, where no such windowing or filtering\ncolors the audio output, you will hear these pops again. Source data that loops per-\nfectly by default is always preferred.\nSurround Sound\nConsideration must be taken on how to handle surround sound. There are two main\napproaches to take:\n• User supplies X, Y, and Z coordinates of both the source and listener positions\n• User supplies an angle and distance for the source compared to the listener position\nMultiStream uses the X, Y, Z approach, using OpenAL 1.1 algorithms, although\nit would also be sensible for such a routine to accept either approach considering that\nthe X, Y, Z system creates a surround sound panning position (angle) and an overall\nvolume (distance) from this position anyway.\nProcessing multi-channel audio in surround sound must also be considered.\nAgain, MultiStream will fold multi-channel audio down to a single point source\nif it requires positioning in surround sound. Another way to handle multi-channel\naudio is to play each channel as mono (for example, channel 0 = front-left and chan-\nnel 1 = front-right for a stereo channel), and set the surround sound X, Y, Z position\nfor each speaker. Figure 4.2.4 shows six channels of audio.\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n315\nFIGURE 4.2.4\nSix channels of audio.\nSplitting the .WAV into separate\nchannels, you can position each\nspeaker’s position in the game world\nto replicate the desired effect.\nThis approach can be also used for car race games, where moving the camera\nposition from behind to inside a player’s car means all the audio playback works cor-\nrectly, such as the exhaust being heard from behind the player (see Figure 4.2.5).\nFor certain types of games, a common approach for game audio is to keep non-\nplayer audio as mono (point source) and player-specific audio can be multi-channel if\ndesired. As the player is always in front of a camera, it is safe to presume that no surround\n\n\nsound processing of their audio is required and just playing their audio as stereo will be\nfine. Not only does this allow for higher quality samples, but it also reduces processing\noverheads because there are fewer surround sound objects in the game world.\nSyncing Channels\nOne problem that often occurs in audio programming is being able to sync multiple\nchannels. This allows the starting, stopping, and pitch changing of multiple channels to\nhappen at the same time. You might hear phasing or chorus effects if this is not taken\ninto consideration.\nThe reason for this can be seen in Figure 4.2.6.\n316\nSection 4\nAudio \nFIGURE 4.2.5\nChannel location relative to the player.\nFIGURE 4.2.6\nChannels can become out of sync if the audio engine updates between play\naudio commands.\nIn Figure 4.2.6, you can see that two audio channels have been requested to play,\nbut due to the audio engine’s update routine firing in between the initialization of\nthese two audio channels, the output of “Audio 1” is now one data packet ahead of\n“Audio 2.” In real life MultiStream terms, this means that “Audio 1” is 512 samples\nahead of “Audio 2.” This can also occur if you pause and resume channels, or set the\npitch of multiple channels, except that in both of these cases it is possible for the audio\nto drift farther and farther out of sync!\n\n\nFor a solution to this problem, you need to make sure that any phase-causing\nfunctions (the Play or Pitch Change functions) are not split by the audio update rou-\ntine. The simplest method for this is to have two functions:\nVoid Sync_On(void)\nVoid Sync_Off(void)\nHere, any Play or Pitch functions called between these calls are “remembered”\nand are processed in the next audio update function after Sync_Off, as illustrated in\nFigure 4.2.7.\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n317\nFIGURE 4.2.7\nThe Play Audio commands are queued up to be synchronously started during\nthe same audio engine update.\nAs you can see in Figure 4.2.7, “Audio 1” has now waited until the “Sync Off”\nfunction has processed, which means both channels are now playing in sync as desired.\nDSP Effects\nDSP effects separate the “next-gen” from current or last-gen titles. The processing\npower available, again from my experience on the PS3, means that it is possible to\nprocess audio in real-time, and using a minimal amount CPU at the quality normally\nonly experienced in professional effect units.\nThe purpose of this gem is not to discuss each DSP effect. There are many books\nalready available covering filter design, FFT and so on. Therefore, I will leave it to you\nto research this area.\nRest assured, having only a low-pass filter to use as occlusion/obstruction is not\ngoing to make your title sound next-gen. You will need to go a little further to impress\npeople! Just think about the amount of DSP effects available for general music or\nsound effect creation and then think of how any of these effects could be used within\nyour game title. Think about every room in every level having its own reverb type, for\nexample. As “anything is possible,” a good start is having programmers communicate\nto audio engineers about what effects they would like to see in real-time and why.\nOf course, processing DSP effects in real-time also means that there is less pre-\nprocessing required for audio samples. Considering that a game title may contain tens\nof thousands of samples, it can make sense to process these in-game, allowing the\n\n\ndeveloper to tweak and change parameters at will, rather than needing to go back to\nthe audio engineer and ask for changes or just put up with an effect that’s close\nenough to what you want. Imagine a sample of a human voice that you decide would\nsound better if it were talking through a radio headset. Having the ability to test these\neffects without the need to waste time pre-processing data not only speeds up devel-\nopment, but also allows for far more creativity when creating your audio.\nRouting\nThe number of busses an audio channel can be mixed to cannot be underestimated.\nFor MultiStream, we currently have 31 sub-busses and one master buss. It is already\nbecoming apparent that these values should be increased in the future. The grouping\nof sound sources has previously been used for volume scaling. For example, all SFX\nwould route to one bus, all music to another, and all commentary to another. The vol-\nume parameters can then be modified in, say, game “option” menus and will then just\nset the volumes for these busses, scaling all audio playing through them.\nToday, with the number of audio channels required for creating things like car\nengines, busses can be used for far more than just volume scaling. By adding DSP\neffects to busses, it is easier and less CPU intensive to set such effects for all of these\ncomponents in one go (see Figure 4.2.8). Imagine a car game where you see a car go\nbehind an object. Instead of processing low-pass filters for 30 or more audio channels,\nyou could just do it once.\n318\nSection 4\nAudio \nFIGURE 4.2.8\nPutting DSP effects into the buss can reduce the amount of processing\ndone per channel.\n\n\nConclusion\nCreating a good master mix is still seen as something of a black art. Indeed, it can be seen\nas something that you will never get right. It should go without saying that games, unlike\nfilm, are unpredictable. You can never be sure where the camera is pointing or which\nsituation the player is in. Trying to work out what audio should be heard is not simple.\nDucking techniques have been used previously to provide a little clarity. Most\nsports titles will automatically reduce the volume of all other audio whenever com-\nmentary is played. This has previously been handled by a simple “if I am playing\ncommentary, reduce all other volume by x percent” approach. In the real world, a\nducker (or side chain compressor) would be used. This analyzes the audio input signal\n(in this case, the voice of the commentator) and then reduces another input signal (all\nother audio) accordingly.\nThis technique can now easily be introduced into a next-gen title and gives a far\nmore realistic result. The previous method does not check for what commentary is\nplaying, it just knows it is. If there is a long silence in the commentary audio sample,\nall other audio volume will still be reduced. Using a ducker DSP effect will not cause\nthis problem.\nPriority systems can also be used to make sure that you hear audio that’s more impor-\ntant to a scene. The choice of what is important in a scene is still really up to the game\nengine. For example, imagine a game where 10 enemies who are all the same distance\nfrom the player are shooting; you may need to choose which ones are more important.\nPerhaps you need to order this by the direction the enemies are shooting or by what kind\nof weapons they are shooting (laser rifles being more powerful than pistols perhaps).\nThe number of priority levels is also a factor (where, say, a higher level will give\none sound priority over another). I have previously written systems that give the user\n256 priority levels for any SFX. Although this feels like a good idea, in practice it is\nnot common for there to be any noticeable difference between using a priority level of\n122 compared to 121. A smaller range of something like 0–7 is far more usable.\nMixing the two techniques of both the ducker and a priority system can allow\nyou to automate a master mix. Here, a number of busses are used—one buss for each\npriority level. On each buss apart from one (which has the highest priority), a ducker\nis placed and each buss also feeds into the adjacent buss. Buss 0 will duck busses 1–6.\nBuss 1 will duck busses 2–6. Buss 2 will duck busses 3–6, and so on. Simply by mak-\ning sure your audio routes to the selected buss, it should be possible that volume lev-\nels are controlled correctly. This requires minimum input from the users; they just\nselect the buss for audio to route to in the same way as you select the sound’s priority.\nUnder MultiStream, this would be a feasible routing and DSP setup, although I\nadmit that there are still other considerations. Other busses may contain reverb effects\nand you will need to know how to route from the six priority busses to these other\nbusses. Even so, I believe this is an area that may well make games feel far more “film-\nlike” with regard to post-production values, and it is only possible to do this now,\nunder the next-gen banner.\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n319\n",
      "page_number": 345,
      "chapter_number": 37,
      "summary": "This chapter covers segment 37 (pages 345-352). Key topics include audio, processing, and process. Although shorter windows do not suffer from these problems, they lead to\npoor frequency domain resolution, which defeats the objective of trying to implement\na frequency domain effect.",
      "keywords": [
        "Audio",
        "Audio engine",
        "samples",
        "Next-Gen Audio Engine",
        "audio channels",
        "DSP effects",
        "FFT",
        "Audio FIGURE",
        "Frequency Domain Processing",
        "Processing",
        "Frequency Domain",
        "channels",
        "audio update",
        "effects",
        "Domain Processing"
      ],
      "concepts": [
        "audio",
        "processing",
        "process",
        "processes",
        "requiring",
        "requires",
        "requirements",
        "sample",
        "sounds",
        "game"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 55,
          "title": "Segment 55 (pages 536-543)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 50,
          "title": "Segment 50 (pages 481-489)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 51,
          "title": "Segment 51 (pages 490-498)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 52,
          "title": "Segment 52 (pages 499-510)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 53,
          "title": "Segment 53 (pages 511-519)",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 353-360)",
      "start_page": 353,
      "end_page": 360,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank \n\n\n321\n4.3\nListen Carefully, You Probably\nWon’t Hear This Again\nRemoving Repetition from Audio Environments \nin Games and Discussing a New Approach to\nSound Design\nStephan Schütze\nBeing REALLY Different\nD\nrop a coin on a table and listen to the sound it makes. Drop the same coin a sec-\nond, third, or hundredth time and the chance of the sound it makes being the same\nis incredibly unlikely. The creation of sound is influenced by a staggering number of\nfactors and apart from scientifically measurable sounds, such as a sine wave, is extremely\nvariable. Sounds used in most games, however, are generally static or limited in their\nvariation. In some cases this may be desirable. For the most part, though, having the\nsame sound effect repeat with little or no change not only reduces the realism of a game\nenvironment but, more importantly, it is often a source of frustration or annoyance for\nthe players.\nThe technology to create real-time variable in-game sound effects has been avail-\nable for some time. These techniques not only remove the issue of repetitive sounds,\nbut they also allow for far more complex audio assets to exist in a game than would\nhave generally been possible with the limited resources of some game consoles. With\nthe advance into the newest generation of game consoles, these methods can allow an\naudio designer to create rich audio environments featuring complex reactive and truly\ninteractive sound and music. At last developers can achieve a level of sound design\ncomparable to the incredible levels of graphics that have been achieved in interactive\nentertainment in the past few years.\nThis gem discusses the methodology behind creating these more complex and\nvariable sound environments, as well as illustrates a need to shift our thinking as cre-\nators of audio assets. I will also look at some of the tools available to asset creators.\nThe goal of this gem is to inform about the techniques available but also to generate\nthought amongst sound designers about how we practice our craft. I also hope to\ninform producers of the potential that exists for incredible audio environments.\n\n\nHow It Works; Thinking Differently\nThe first step is to move away from the traditional static linear audio used in film and\ntelevision. Games do not function in a linear fashion, but for want of a better role\nmodel the industry has often strived to achieve movie industry standards of quality\nand production. Initially as game technology was developing, this was a useful bench-\nmark, but the closer games come to meeting the standards of big budget film and tele-\nvision productions, the more we should look at exceeding them. It is apparent now\nthat in the very near future games will surpass film and television in the potential to\ndeliver entertainment. As a result, the benchmarks for production quality may also\nmove beyond those of linear media. Audio can and should be one of the leading areas\nin which interactive entertainment production methods surpass film standards. A\nselection of static pre-made sounds to be triggered as required in-game, although ade-\nquate, completely fails to utilize the creative possibilities available to designers and\ndevelopers. \nThe basic principle of this technique is to construct complex sounds from their\nindividual raw component sounds. Although this may be inefficient on a sound-by-\nsound basis, when implemented for the entire audio environment it often actually\ntakes less memory and fewer resources to create sounds that are infinitely variable and\noften far more interesting than pre-made sound effects. It also provides the sound\ndesigner with a much bigger selection of possibilities for sounds in-game. So you can\nactually have more sounds in-game with no repetition and for less memory. Initially\nthis process has a steeper learning curve for designers, and may take longer to set up.\nHowever, the resources gathered will provide ongoing material for future projects\nwithout the risk of sounding like you are simply reusing the same sound library.\nGoing Bang!\nTo begin with, it is useful to think of the sounds we record and add to the engine as\nbeing the core building blocks from which we will create all in-game sounds. This is no\ndifferent than going out and recording raw source material, preparing the source sounds\nand mixing them together to produce a finished sound effect. The difference here being\nthat creating the actual sound happens in-game each time a sound is needed. This\napproach does preclude your ability to simply drop in pre-made library sound effects,\nbut the benefits are worth the effort.\nExplosions are common sounds required in a great many games. I will refer to them\nas “pops.” I use the term “pop” because it encompasses a lot more than simply saying\nexplosion. Pops appear in most shooter-style games as sounds for grenades, missiles, or\nrockets detonating or for objects in the world exploding. Pops however also exist in\nmany platform games to represent an adversary being defeated, an item being collected,\nor a special effect such as teleporting, turning invisible, or gaining invulnerability. An\nactual explosion effect is very similar in structure to a literal “pop” sound or many of the\nother sounds I have mentioned, as they contain many or all of the same elements. \n322\nSection 4\nAudio \n\n\nIt is important to understand that a real explosion is a release of energy, usually\nthrough some kind of chemical reaction. The actual release of energy will create a basic\nBANG, which will then echo or reverberate with a fading effect. The extraordinary\nexplosions heard in Hollywood films are a result of the initial energy affecting other\nthings in the world. So smashing glass, splintering wood, bending metal, and so on are\nnot actually apart of the initial explosion of energy; they are consequences of this\nenergy rushing out and meeting wooden, metal or glass objects and having an effect on\nthem, in some destructive way. Those items then react in a similar manner; you get an\ninitial sharp attack sound followed by a drop-off. When the item affected is a plate-\nglass window, the result is of thousands of small attacks and drop-offs combined to\nspectacular effect.\nOften, when creating sounds, the recorded material alone can sound dull or life-\nless. The recorded sound of a real gun being fired can be quite unsatisfying in its raw\nstate. Sound designers will often combine several raw sounds together to create a single\nnew sound. Sometimes the raw material used is to accentuate certain frequency ranges\nto add depth to the final assets. A low frequency impact can add considerable weight to\na sound, whereas high frequencies can make a sound seem much louder and brighter.\nEQing can add further depth to the final sound and is often helpful if you want a par-\nticular sound to stand out from the rest of the audio environment. Balancing the final\naudio environment should consider the mix of frequencies used as well as the ampli-\ntude levels of the sounds. Too much of any particular frequency range can quickly tire\nthe listener and become annoying.\nTo better understand how to construct a sound, it helps to first deconstruct it:\n• An initial sharp attack sound/surge of energy. A very short, hard attack, zero\ndrop-off sound. Think of a handclap or gunshot.\n• A drop-off and fade sound. Think of the echo of a handclap in a church or a\ngunshot. This is actually part of the initial sound, but it is useful to think of it as\na separate element when deconstructing sounds.\n• Affected elements. These are the sounds of everything that are affected by the\ninitial surge of energy.\n• The drop-off of every affected element.\n• Major subsidiary effects. Elements returning to a state of rest. Think large\nfalling debris.\n• Minor subsidiary effect. As the previous entry, but smaller debris, such as dust,\nand so on.\nThis example deconstructs a traditional explosion into its basic sound elements.\nSometimes the inclusion of extra sound material can significantly improve the final\nresult. The same thing can be done for any game pop. For example, a musical pickup\nsound in a children’s platform game.\n• An initial sharp attack sound/surge of energy. A very short, hard attack zero\ndrop-off sound, such as striking a chime or bell tree.\n4.3\nListen Carefully, You Probably Won’t Hear This Again\n323\n\n\n• A drop-off and fade sound. The actual ring of the bell and its fade over time.\n• Affected elements often occur in a cascade of sounds. The bell chime moves\nand hits the surrounding chimes, but with less energy and in a random pattern.\n• The drop-off of every affected element. The other bells all ring.\n• Major subsidiary effects. The overtones or harmonics of the initial bell and fur-\nther minor contact between chimes.\n• Minor subsidiary effect. The fading rings of all chimes as they return to a state\nof rest.\nAfter deconstructing explosions, bubble pops, or chimes ringing, you can then\nreconstruct those sounds from their individual components. When you understand\nexactly how these components sound in their raw state, you can construct a convincing\npop using a very limited number of raw components and cleverly combining them.\nSo, let’s actually make a sound effect. Previously, I deconstructed a sound so that\nyou can understand the elements you need to construct the same type of sound effect.\nLet’s use the following elements: \n• Big_Bang01–03: A short sharp metallic impact sound\n• Stone_Fall01–02: Stone objects affected by the energy\n• Debris01–02: Small objects returning to a state of rest\nThese base sounds are included on the CD-ROM in standard PCM .wav file for-\nmat. Also included are seven in-game sounds (Ingame_Sound01a–Ingame_Sound03)\ncreated using only the seven base sounds.\nSeven wav files totalling 629KB were combined to create seven new in-game sounds\ntotalling 1.38MB. All the new sounds were created and recorded directly out of the\nMicrosoft XACT (Cross-Platform Audio Creation Tool) authoring tool using the initial\nseven base sounds. The three variations of Ingame_sound01 and Ingame-Sound02 \nare examples to show the variation, which is essentially limitless. Ingame_Sound03 was\nconstructed simply to illustrate an entirely different result from the base material.\nI allowed myself only one hour for gathering the base sounds, setting up the\nXACT project and creation, audition and recording of the new sounds. This was an\nintentional limitation to demonstrate the speed at which the tool can be used. I’m not\nsaying these new sounds are going to win any awards, but they show how a few sim-\nple definitions allow you to create infinite realistic variations quickly in real-time. I\npurposely did not descriptively name the sounds, as I did not want to influence the\nlistener’s thoughts when they were first played.\nThe Old and the New\nFigure 4.3.1 illustrates various files laid out as they might be in a traditional linear\nsound-editing program to create an explosion sound effect. The tracks allow for\nsounds to be triggered with varying degrees of overlap and the horizontal axis is used\nto position the sounds relative to each other in time. The sounds themselves can be\nany combination that produces the desired final sound effect.\n324\nSection 4\nAudio \n\n\nThis is a traditional linear editing method as used for audio and video; once the\ndesigner is happy with the result the sounds are combined by rendering them together\nto produce a new file in the desired file format.\nFigure 4.3.2 illustrates the same layout of sounds events with the same temporal\npositioning and overlap as Figure 4.3.1. In Figure 4.3.2 however, the layout is just a\nrepresentation of how you would like the sounds to be combined in real-time by the\ngame engine; there is no rendering process. The sound events are also not limited to\nan individual sound file. The number in brackets in each sound event represents a\npool of sound files that are drawn from randomly to create the desired final output\nsound. The number of sounds available for each sound event is limited only by the\nphysical memory available on the end platform. \n4.3\nListen Carefully, You Probably Won’t Hear This Again\n325\nFIGURE 4.3.1\nStandard editing software shows linear progression.\nFIGURE 4.3.2\nSound tool layout.\n\n\nAnother difference with this method is the ability to alter the sound’s position\nrandomly in time. The black arrows represent a time-offset value. Each time a sound\nis played, each of the tracks will count its time offset before the sound is triggered.\nThe gray arrows represent a variable time offset. In this case, the time before the\nsound is triggered is randomized up to the maximum value set. For example, sound\n01 will randomly wait a short period of time each time it is played. By comparison,\nsound 02 will wait a set time approximately twice that of sound 01 each time it is\nplayed. Sound 03 combines a set wait time with a further randomized wait time. This\nmeans it can sometimes play almost directly after sound 02 triggers, and sometimes as\nlate as halfway through sound 02 triggering.\nThe main tools used to create sound effects are amplitude, pitch, and time manip-\nulation. Combinations of these three factors can change an original source sound into a\nnew sound completely unrecognizable from the original. Sound designers in all media\nuse these tools to create the sounds they want to use and render out a new altered sound\nin the required format. This method replaces the tools that manipulate the sounds. The\nmanipulation occurs in real-time in the game. No permanent rendering occurs; a sound\nis created as it is needed according to the parameters provided using a source sound and\nthen it is discarded. Each time the required sound is called, the process is repeated, the\nvariable parameters are applied, and a unique sound is created. \nNew Tools for a New Approach\nFigure 4.3.3 shows the FMOD sound designer interface. In many ways it appears sim-\nilar to the two previous diagrams. There are sound events arranged horizontally on two\ntrack layers. FMOD’s use of sound events rather than actual wave files in the design\ntool allows for a sound event to include multiple sound files as described in Figure\n4.3.2. In Figure 4.3.3, the sound events overlap to allow for a cross-fade between them.\nA significant feature in FMOD is that the horizontal axis is not limited to repre-\nsenting time alone. This is another way in which moving away from traditional meth-\nods can be extremely effective. In Figure 4.3.3, movement along the horizontal axis\nrepresents the RPM of an engine, but it could just as easily represent altitude, speed,\nor number of hit-points. As any of these parameters are affected, the sounds change as\ndefined. The strength of these systems is that they allow the content creator to set the\ndesired parameters and how they will affect the audio environment. This frees up\ncoder time considerably, because the coder can be provided with a few simple tags to\nlink up. In the case of the car example, once the sound is added, all that is needed in\ncode is for the RPM data from the game to be linked to the RPM tag from FMOD. \nMicrosoft’s XACT audio tool in Figure 4.3.4 has a considerably different interface\nthan FMOD’s Sound Designer, but many of the same features and strengths. XACT\nuses wavebanks and soundbanks that are defined by the designer. The soundbanks are\nrepresentative of the end sound that is desired, and each sound event can consist of\nmultiple sound files in the same way as FMOD. Parameters for randomizing pitch and\nvolume are accessible at multiple levels when creating a sound. As such, it is possible to\n326\nSection 4\nAudio \n\n\nrandomize each smaller component making up a sound event, and then pitch or alter\nthe final event as needed. XACT works in the same way as the example in Figure 4.3.2,\nit just does not use a traditional linear type of editing window.\n4.3\nListen Carefully, You Probably Won’t Hear This Again\n327\nFIGURE 4.3.3\nFMOD Designer.\nFIGURE 4.3.4\nMicrosoft XNA XACT audio tool.\n",
      "page_number": 353,
      "chapter_number": 38,
      "summary": "This gem discusses the methodology behind creating these more complex and\nvariable sound environments, as well as illustrates a need to shift our thinking as cre-\nators of audio assets Key topics include sound, audio, and games.",
      "keywords": [
        "Sound",
        "sound effect",
        "Audio",
        "time",
        "sound event",
        "effect",
        "sound designers",
        "game",
        "base sounds",
        "create",
        "FMOD sound designer",
        "in-game sounds",
        "sound files",
        "FMOD",
        "attack sound"
      ],
      "concepts": [
        "sound",
        "audio",
        "games",
        "explosions",
        "explosion",
        "time",
        "effect",
        "pops",
        "pop",
        "tool"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 8",
          "chapter": 50,
          "title": "Segment 50 (pages 481-489)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 55,
          "title": "Segment 55 (pages 536-543)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 61,
          "title": "Segment 61 (pages 575-584)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 64,
          "title": "Segment 64 (pages 602-609)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 361-371)",
      "start_page": 361,
      "end_page": 371,
      "detection_method": "topic_boundary",
      "content": "In some ways this is a good thing, because it forces the user to approach asset creation\nin a different way. Although FMOD supports nearly all currently available platforms,\nXACT is limited to the Microsoft platforms and PC. Hardly surprisingly though, it does\ninterface extremely well with the supported platforms and is easy to use.\nMicromanagement\nAll but the simplest sounds (such as a sine wave) are made up of many smaller sounds.\nBy dividing sounds into their smaller components, you increase their usefulness to the\noverall sound environment. For example, the click/clunk sound of a car door being\nclosed is reasonably characteristic, and will provide only so much usefulness as a\nsound for another purpose, even with some pitch shifting of the sound. If, however,\nthe sound is divided into the separate elements that create the final sound (click and\nclunk), not only do you have two new source sounds that can be combined into other\ncomplex sounds, but you can also add some slight variation to the original car door\nsound by subtle pitch shifting or varying slightly the time between the click and the\nclunk.\nThis is a relatively basic example, and a non-repetitive car door sound will probably\nnot win you any awards, but it is certainly relevant when thinking about how to\napproach sound design for greater realism. Go and open and close a car door a few\ndozen times and see how different the sounds are each time. It is also worth noting that\ndividing the two sounds will not add significantly to memory. The combined wav data\nis the same length. \nThis method will however drastically increase the number of files you will be\ndealing with and as a result there will be increases in resources. If nothing else, your\nheader files or wherever you have your assets listed will be bigger. These changes are\nquite small and with third-generation consoles they should be completely ignorable.\nThe benefits of a more dynamic audio environment far outweigh the issues of having\nto wrangle more files. That is our job, after all.\nWhy Are We Doing This Again?\nThe ultimate goal with this system is to have every sound rendered in-game and to\navoid repetition and create a dynamic and effective audio environment. Implementa-\ntion time can take longer, especially initially as the designer learns to get the most out\nof the system, depending on the level of complexity of the audio environment. Obvi-\nously spending a lot of time on very minor sounds may not be cost effective, but the\nfreedom exists in the system for the designers to choose how detailed they want to be\nin creating sounds.\nThe time it would take to randomize simple footsteps by separating the foot impact\nand gravel crunch underneath, and then replacing the gravel sound as required when\ndifferent surfaces are walked on is trivial when compared to the benefits of not having\n328\nSection 4\nAudio \n\n\nannoyingly repetitive footsteps. Add in pitch and volume randomization and it might\neven sound real. The player will probably never notice; that’s often a sign of good sound\ndesign.\nThe designer creates the sounds by choosing the raw material and setting the vari-\nables that will control how the sound is created in real-time. Because of the random\nnature of the sounds, it is important that the designer audition a considerable selection\nof each sound to ensure it doesn’t output undesirable results. Often regular tweaking\nmight be necessary as more sounds are added to the audio environment and they need\nto balance with each other. One of the best aspects of this method is that once a sound\nis in the game it can be tweaked using the parameters in the tools.\nThis means often drastic changes can be made to the sound environment with\nnothing more than the changing of a single data file. This should not require a full\nrebuild of the game engine. As a result, the sound department should be able to work\nwith considerably less support from the code team, balancing and changing the audio\nenvironment regularly and easily. This method is also incredibly useful for online con-\ntent, as new sounds could be included in game updates without the need to download\nlarge amounts of data. The designer uses the available assets that each player will\nalready have installed and creates new sound assets by making new definitions only.\nAn MMORPG could have hundreds of new sounds added to it by simply download-\ning a new definitions file and a new EXE file of only a few hundred kilobytes.\nGoing Further\nThis gem has focused on the most basic tools for sound production and manipulation:\ntime, pitch, and amplitude, and their most basic uses. The available software tools do,\nhowever, offer far more advanced tools such as filtering, effects, and implementation\ntools. More importantly, though, these tools can allow you to create incredibly com-\nplex audio environments. A series of musical motifs or even individual note events\ncould be combined in real-time to predefined parameters and played in-game to react\nand interact with a player’s actions. If you want an ascending and descending musical\npattern as Doofy Duck runs up and down the stairs, you can do it. If the player wants\nto test you by stopping halfway and jumping up and down, that’s okay too; the music\ncan respond appropriately.\nAlthough this method certainly isn’t limitless, it allows a freedom of creativity\nthat benefits greatly from thinking outside the box. An entire game could center on a\nmusical score that grows organically from the actions of the player, or where every\npossible interaction in the game world was supported by a unique audio representa-\ntion. Insert your idea here and go and make it happen!\nEven though I refer to this method as rendering or creating the sounds in real-\ntime, these ideas will not reduce or replace the work of a sound designer. In fact, it\nmakes the role even more critical and requires the sound designer to work far beyond\nsimply using library sounds. This method will very quickly expose a designer with\nweak skills or poor imagination. Conversely, a great designer could use this system to\n4.3\nListen Carefully, You Probably Won’t Hear This Again\n329\n\n\ncreate an audio experience worthy of the best titles in the industry. This method will\nhave an impact on the time required to create and implement audio at least for the\nfirst project on which it’s utilized. However, once developers overcome the initial\nlearning curve, this method can be extremely flexible. The method allows for last\nminute changes and alterations to the sound assets far more easily than traditional\nmethods of game sound design. \nConclusion\nGame production standards have increased dramatically in the last five years, and as\nstudios better understand the importance of good tools and production processes, the\nincrease in quality should continue. In the past, game audio was often overlooked or\ngiven minimal attention. The development of new middleware software and produc-\ntion tools such as XACT allows audio content producers to approach content design\nand creation in a whole new way. Once designers unlearn some of the traditional\napproaches to sound construction, these new methods can allow for incredible flexibil-\nity and variety. The ability to create audio environments never before possible is not\nonly a great opportunity for talented audio teams, but will hopefully provide entertain-\nment for players that exceed the experiences available through any other media.\n330\nSection 4\nAudio \n\n\n331\n4.4\nReal-Time Audio Effects\nApplied\nKen Noland\nT\nhe purpose of this gem is to outline some of the more basic fundamentals of audio\nprocessing from a high-level perspective, taking into account all the tips and tricks\nI’ve learned over the years in designing an audio engine for video games. Some of these\ntips are straightforward and others require a little more thought to work around.\nA quick search on the Web will show you how to efficiently create a graphics ren-\ndering pipeline or perhaps an AI framework. However, when it comes to creating\nyour own sound system, a large portion of articles are, in my opinion, too API specific\nor too general and don’t cover the niche cases that always tend to show up with audio\nprogramming. Lately this has been changing, and a much larger focus has been put on\naudio programming from the perspective of a digital signal analysis perspective.\nThis gem is less API specific, although I do mention a couple APIs available and\nsome of the more interesting features, but instead this gem is focused on the general\nprinciples of building an audio system. As a note of caution though—as the gem pro-\ngresses, I will go into more and more advanced topics that will likely require further\nreading.\nBefore I begin, I want to introduce a very basic concept. Sound is perceived as a\ndifference in samples. Be very mindful of this. If you’ve seen a waveform, you know\nthat it consists of mostly oscillating values that are constantly changing. Those changes\ndenote the frequency over time. If the signal is flat, there is no frequency. If a signal\nchanges very rapidly, there is a very high frequency.\nThis is a very important concept to know. Keeping in mind that the values are\nconstantly oscillating, if you drop from one high value to another, because say you\nwant to clear the buffer and fill it with all zeros during the middle of a peak oscillating\nvalue, you introduce a frequency change that can be perceived as a tick or a pop. A\nmuch more accurate way to deal with clearing a sound buffer is covered in the follow-\ning sections.\n\n\nA quick note about the two primary APIs available—you have DirectSound (for\nWin32 and Vista) and OpenAL (available on most platforms, including Win32,\nVista, Linux, and most consoles). Both APIs do what they do well and support a wide\nvariety of formats and effects. I have no preference when it comes to choosing one\nAPI over another and it depends on what environment you are developing for.\nWith that being said, both sound APIs have their benefits and drawbacks.\nBecause of the distinct difference in drivers for DirectSound and OpenAL, I recom-\nmend writing a sound system that is abstract enough that the end user can readily\nswitch between the two different sound APIs depending on the card and drivers they\nhave installed. I also recommend including an option for software processing for both\nAPIs; that way any driver-related problems are addressed.\nOpenAL and DirectSound have two very distinct design methodologies and are\nmuch like their graphical counterparts. If you have worked with OpenGL, OpenAL\nwill come very naturally to you. If you’ve worked primarily with DirectX, Direct-\nSound is going to be very straightforward.\nOverview of a Sound System\nThere are four concepts to understand when dealing with a high-level overview of a\nsound system—the primary buffer, the listener, the sound, and any effects applied to\nthe sound or the listener.\nThe Primary Buffer\nThe primary buffer is the final resting place for the PCM samples you send to it. Under\nmost sound systems you won’t be filling the primary buffer directly, but you will be\ndealing with it from the perspective of the listener. The only thing that you are con-\ncerned about with the primary buffer is how much it advances from frame to frame. \nThe Listener\nThe listener is a special object that exists in 3D space. It listens to the incoming\nsounds and applies any special transformations and effects such as panning and falloff,\nand advanced filters like Doppler Shifting and Head Relative Transfer Delay.\nYou should always assume that under any given API you are going to have only\none listener. Normally this is not a problem, but for those of us who write games that\nhave multiple viewports or monitors, it represents a slight challenge. The solution to\nthis problem is actually very easy. Simply transform all sounds to the listener and\nrecord things such as velocity in the sound properties so that effects, such as Doppler\nshifting, can still be correctly calculated. Things get a little more complex when listen-\ners have effects applied to them and those effects are different from listener to listener,\nbut I’ll explore effects in a little while.\n332\nSection 4\nAudio \n\n\nThe Sound Sources\nThe sound sources themselves are typically mono channel signals coming from within\nthe world. Sound sources typically have properties such as position, falloff, and veloc-\nity. Those properties are then used by the listener and the effects to process the sound.\nUnder any sound system, you should differentiate between sound sources and the\nactual sound data. Sound sources contain a reference to the sound data as well as the\nposition and orientation of the particular sound and the current play position within\nthe actual sound data. The actual sound data is merely the container for the PCM\ndata as well as any other audio designer related properties, such as falloff reference,\nmaximum number of instances, and any general effects to be applied to all instances\nof the sound itself.\nThe Sound Effects\nSound sources also contain effects. Some of those effects are inherited from the sound\ndata and other effects are applied from its position within the world. Either way, it is\na good idea to stack up the effects so that you can easily collapse them upon request.\nPutting these concepts together, you’ll see that the primary buffer requests data\nfrom the listener, the listener then goes out and determines what sounds to play and\nrequests the samples from the sound sources. Upon getting that request, the sound\nsources collapse the effect stack and fill the listener with the correct data. The listener\nthen runs a digital signal peak limiter on the sound effects and collapses its own\neffects stack; then it presents the contents to the final buffer.\nOne thing to note in this entire example of a sound system is that it uses a model-\nview-controller architecture. The data is encapsulated in the sound data (the model)\nand is requested by the sound source (the controller), which then applies the individ-\nual sound effects (more controllers), which in turn is requested by the listener and\nthen finally outputted to the primary buffer (the view).\nSound Buffers\nOn almost all machines you are limited to the amount of sounds the hardware can\nplay. Even when processed in software mode, you should still clamp the number of\navailable sound buffers to something within the range of your performance targets. As\nof writing this gem, the maximum available hardware accelerated sounds on the aver-\nage top-of-the-line consumer sound card is 128 sounds. Keep this in mind for later. \nThis does not take into account that Vista will force you to use software process-\ning under DirectSound at the time of writing this gem. The only alternative is\nOpenAL if you want to utilize the hardware processing under Windows Vista.\nIn most cases, you will want to allocate enough sound samples in your sound buffers\nso that continuous playback is possible, even in the most dramatic frame rate drops. I\n4.4\nReal-Time Audio Effects Applied\n333\n\n\ntypically create my buffers with enough room for one second’s worth of sound data at\n44100kHz.\nOne thing I’ve picked up is that it can actually take more time and more resources\nto stop and start sound buffers than to let them play beyond the duration of their\nsound (making sure to clear the sound buffer so that they are not heard!). But do this\nwithin reason. For instance, using the previous example, where you have 128 sound\nbuffers, you should start playing eight of them. As soon as all eight sounds are occu-\npied by sound data, you start playing eight more. Once it drops below a certain\nthreshold and the sound buffers haven’t been accessed in a while, you go ahead and\nstop them. This kind of balancing is not necessary, but I found it to help in situations\nwhen I had a lot of short sounds playing one right after the other.\nOnce you get a request to play a sound, populate as much of the sound buffer as\nyou can at the current write position, remembering that you’re likely to have already\nstarted playing this buffer. You can get the playback advancement by recording where\nyour previous playback cursor was to where it is now from frame to frame. One thing\nto note here is that drivers will sometimes give you the wrong playback position. The\nposition is sometimes off by only a couple of samples, but other times it can be signif-\nicantly off. In order to compensate for that, take half the size of the buffer and fill that\non the first request. Thus, the one second buffer actually only contains half a second\nof data.\nSo let’s say you’ve got a 44100 sample size buffer and the write position is at\n44000 and your playback has advanced 150 samples in the last frame. Using this\nknowledge, you can request 22050 samples (1/2 buffer size) from the sound source on\nthe first pass. Now that you’ve got those samples from the sound source, you need to\nwrite 100 samples to fill the current write position to the end of the buffer and then\nthe remaining 21950 samples go to the start of the buffer at offset 0. This is simply\nknown as a circular buffer.\nOn the next update, all you have to do is continue to fill the buffer at the last\nwritten position with the amount of samples that playback has progressed. In the last\nexample, you’d then be writing 150 samples to buffer position 21950.\nAs a safety precaution, you also want to clear out the previously played samples.\nWhen you do this, you’ll want to stay three to four frames behind the playback cur-\nsor’s current position, remembering that the playback cursor could be off as well.\nAnother safety precaution is to set a callback at the last written position, clearing any\nprevious callbacks. When the play cursor gets to that position, it triggers the callback,\nwhich then should fade the entire buffer into silence. Because sound is generally\nprocessed on a separate thread, this should work in all cases. This way, you’ll never get\nthose repeating sounds looping in the event that the main update thread locks up.\nRank Buffers\nUsing the example of the sound card from before, I’m going to say that you will have\n128 sounds in total that you can play at any given time. The problem is that within\n334\nSection 4\nAudio \n\n\nyour 3D world you have hundreds, perhaps even thousands, of sounds coming from\nall different directions. This is where you want to implement a special kind of buffer\nknown as a rank buffer.\nA rank buffer is a very simple concept. You algorithmically generate a rank and\nthen you request a buffer. If all buffers are full and the rank exceeds another already\nplaying sound buffer then the lowest ranking sound is booted out and the new sound\nis played.\nThe rank can be calculated any number of ways. The most general way to calcu-\nlate the rank is to determine the attenuation (distance, falloff, and volume), and then\nmultiply that by a value given to you by the audio designer. This works in most cases,\nbut not all. It’s best to take into account all properties of the sound such as distance,\nfalloff, effects, and other items associated with the sound so you can get a clear idea of\nthe sound rank. It’s not acceptable to have a high priority sound just repeating its sub-\ntle echo effect, as another lower priority, but potentially more noticeable, sound is get-\nting skipped.\nAlso worth noting is that audio designers like to specify how many instances of a\nparticular sound or sound category can be played. For instance, if you’re in a room with\ntons of machine gun fire going off, it only makes sense to play 10 or so of these types\nof sounds. Be sure to take this into consideration when building your rank buffer\nalgorithm. One thing I did was to allow the sound data to figure out its rank given its\nparticular context by abstracting a simple function that took in the parameters passed\nto the sound, like its position relative to the listener and the general world data.\nThere are some catches to the rank buffer solution that you must address specific\nto audio processing. The primary catch is that you can’t just stop a sound and then\nfollow that up with another sound. Remember earlier when I stated that sound is per-\nceived as the difference in samples. If you stop playing one sound abruptly, you’ll hear\na tick or a pop. Instead, you have to transition one sound to the next, fading out the\nprevious sound.\nThings get even more complex when the previous sound has effects applied to it.\nBecause of the way audio drivers handle the effects applied to the sound buffer, you\nshould not just linearly transition the effect, but instead you have to wait until the\nprevious sound has finished fading and then you can switch the effects properties\nover. Thus, once your gain (volume) has reached zero for the previous sound source,\nyou can apply the new effects and start copying over the new sound. One thing to\nnote is that you do not want to commit the switch in effects until the playback cursor,\nnot the write cursor, reaches the desired switch point.\nRemember previously when you copied half a second of sound samples into the\nbuffer to accommodate for drops in frame rate? You want to be able to transition\nimmediately. Keep in mind that once you send the data to the sound buffer, it’s up to\nthe driver’s implementation if it wants to keep that data around, so I wouldn’t count\non it still being there. To get around this, you should keep copies of all the samples\nyou copy over to the sound buffer so that you can go back in time and fade out at the\nplayback buffer’s current write position.\n4.4\nReal-Time Audio Effects Applied\n335\n\n\nThe fade sample amount varies, but I generally keep it at around five millisec-\nonds, or roughly 220 samples at 44100kHz playback. You can set this up to be a\nproperty of the sound data so the audio designer can adjust this value. \nEffects and Filters\nEffect objects should be created through an abstract factory method generated by\nyour individual sound system API, so that hardware processing is possible, and then\nattached to the sound source or listener so that they can be collapsed when requested.\nEffects are different from filters. An effect can contain multiple filters or simply\ngenerate sound data or perhaps contain a wrapper for a hardware accelerated feature.\nIn any case, think of the effect as the middleman between the sound source and, if the\neffect calls for it, the filter. When designing filters, keep in mind that filters should be\nas generic as possible and that any implementation details should be gathered and\nstored in the effect object itself, thus allowing you to abstract new effects quickly. To\nput it simply, effects are implementation specific and filters are not.\nThere are two types of filters to be concerned with, as follows:\n• Infinite impulse response (IIR) filters, which recursively work on the sound samples.\n• Finite impulse response (FIR) filters, which just deal with transforming the sound\nsamples in some manner without regard to prior output.\nYou’ll have to differentiate the two filters when designing the effect.\nWithin the filters, there is a concept known as wet/dry mix. Wet samples are\nsamples that have been previously transformed and dry samples are the raw incoming\nsamples without any transformation. You should have a distinguishing factor of\nwet/dry mix and allow for your effects to change that ratio.\nTo complicate matters even more, there are multiple ways of transforming the\nsamples. One of the most common methods is through the use of Fast Fourier Trans-\nform (FFT). This type of calculation, although extremely useful and applicable, is\nvery time- and processor-consuming and much research has been done to improve the\nspeed of this operation. Be sure to run this type of operation only when absolutely\nneeded, caching any data that you can from it. This means that you should be able to\ntransform the sound in the effect object to the frequency domain, run all of the filters\nin the transformed frequency domain (ensuring that the filters can use the frequency\ndomain data), and then transform back to sample space in the effect itself when all fil-\nters have been processed, if the effect calls for it.\nFIR filters are the easiest to deal with because all you have to do is feed it the data\n(dry mix) and it spits out the result. IIR filters are a little more complex because they\nrely on the previously generated result (wet mix). The easiest way to deal with this is\nto have a separate buffer set up within the effect that records the output from the fil-\nter (the wet mix buffer). The size of that buffer is specified when the effect is created,\nthus setting the delay line. In some effects, this delay line can be set up using the\ninputs for the effect, such as feedback delay, which can then be translated to buffer\n336\nSection 4\nAudio \n\n\nsize. Otherwise, you can optionally explicitly set the buffer size, thus clamping your\nwindow.\nIIR and FIR filters can also be arranged in a directed graph, allowing the filter to\nreference other filters in a cyclical manner, running until it has reached the extents as\nspecified by delay line. This is the style of filter design outlined in Game Programming\nGems 5 in the article entitled “Fast Environmental Reverb Based on Feedback Delay\nNetworks” [Schüler05]. Using these types of filters is very handy, because you can\ndesign new effects quickly as well as extend those effects to simulate audible character-\nistics of the world around you.\nI have yet to talk about signal timing—all those cases where you have looping\nsounds combined with effects that elongate a sound beyond the original sound\nlength, such as with an echo. I’ve outlined a system where you request samples and\nthose samples are filled via collapsing a stack of effects resulting in the final data, thus\na sound is finished when no samples are returned via the listener. However, there is a\ncaveat to this. Simply waiting for the request to return no samples on a looping sound\nsource with an IIR filter will result in a sound that never loops. Therefore, you do\nhave to push a separate flag that informs the sound source that it is looping and that\nwhen an effect reaches the end of reading the sound data, it should loop back to the\nbeginning.\nCompression and Streaming\nThere are many audio compression formats available, each one focusing on a particu-\nlar need. Some formats, such as ADPCM, are focused on performance and quick\ndecoding, whereas others, such as MP3 and OGG, are focused heavily on compres-\nsion ratio, giving you small file size while maintaining quality. Here’s a quick compar-\nison between those three formats.\n• ADPCM is the simplest of the three formats. It uses a simple predictive algorithm\nto generate deltas on blocks of audio. Those deltas are stored in four-bit values,\nthus making the decompression algorithm as simple as two table lookups and\ndecoding a four-bit delta, coupled with two multiplies and an add makes this the\nleast CPU intensive algorithm with the highest payoff in compression. However,\nthe compression ratio is a measly 4:1 compared to the other formats and the sig-\nnal restoration at low sampling is not nearly as good as the other formats.\n• MP3 is a common format, widely known and used across multiple platforms.\nMP3 uses frames, similar to chunks used in ADPCM. These frames contain infor-\nmation on the acoustical makeup of the sound signal in transformed frequency\ndomain, which then is broken down into a quantized lookup table [MP307a]\n[MP307b]. MP3 allows for many encoding options such as variable bit rate and\nID3 tags.\n• OGG Vorbis uses the modified discrete cosine transform to convert from signal\nspace to frequency domain, similar to MP3, and then clamps the floor value.\n4.4\nReal-Time Audio Effects Applied\n337\n\n\nAfterward it quantizes the entropy coding and then stores the delta into a lookup\ntable [OGG07]. This kind of encoding allows for lossy compression at variable\nbit rates and is specially tuned for fast decompression, but still not as efficient as\nADPCM.\nYou might be wondering why I’m going into detail about these three compression\ntechniques. There are many libraries out there that will handle the conversions for you\n([MP307a], [MP307b], [OGG07]), and aside from the performance related data,\nthere’s really no need to go into detail about each format. But then again, there’s\nsomething there that you may have picked up. OGG and MP3 store their informa-\ntion in the frequency domain, which means that the really expensive FFT that I men-\ntioned earlier is already present.\nWhat this means is that using the libraries from each respective format, you can\nextract the frequency data and use that information to run your frequency domain\neffects, and then translate into the signal space for final presentation!\nAnother reason for going into detail about each respective format is that you’ll\nnotice each compression scheme has “frames” or “blocks” that they work with. Using\nthis information, you can create a separate rank buffer mechanism for caching\ndecoded PCM samples or decoded frequency data. When you’re streaming from disk,\nit means that you can cache certain frames or blocks in an already decoded fashion as\nopposed to having to store the entire decoded file. For music, this is extremely impor-\ntant. You want to read ahead as much as you can and cache the decoded data, but you\ndon’t want to dedicate 300MB or more of memory just to your sound track. By\ndecoding on a frame-by-frame basis, you can limit your memory usage to any arbi-\ntrary number and by utilizing the rank buffer (without the need for fading samples),\nyou have a mechanism for streaming files from disk efficiently.\nConclusion\nBuilding an entire audio system from scratch seems like a daunting task at first look,\nbut by utilizing the methods you know as a programmer and using the concepts out-\nlined here, you should be able to get up and running fairly quick. There are many\nother topics to learn about and a ton of resources to get you started—a few of which\nI’ve noted in the references section. I would also go so far as to suggest reading up on\nthe many dedicated forums and newsgroups. They contain some of the best informa-\ntion available.\nAudio programming is both rewarding and challenging. After you develop your\nown sound system, tailored to your game’s needs and performance requirements,\nexpanding upon that knowledge and implementation to facilitate design decisions\nand extended effects makes a difference in the overall playability of the final video\ngame. That difference is then perceived by the players, and they leave the game with a\nbetter sense of immersion, so in my opinion, it is one of the most important areas of\nvideo game programming.\n338\nSection 4\nAudio \n",
      "page_number": 361,
      "chapter_number": 39,
      "summary": "This chapter covers segment 39 (pages 361-371). Key topics include sounds, audio, and effective. This method will however drastically increase the number of files you will be\ndealing with and as a result there will be increases in resources.",
      "keywords": [
        "sound",
        "sound data",
        "sound buffer",
        "effects",
        "buffer",
        "audio",
        "Sound Sources",
        "sound system",
        "Sound Effects",
        "Audio Effects Applied",
        "data",
        "samples",
        "sound samples",
        "Real-Time Audio Effects",
        "Audio Effects"
      ],
      "concepts": [
        "sounds",
        "audio",
        "effective",
        "buffer",
        "design",
        "samples",
        "sampling",
        "data",
        "available",
        "game"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 55,
          "title": "Segment 55 (pages 536-543)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 61,
          "title": "Segment 61 (pages 575-584)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 51,
          "title": "Segment 51 (pages 490-498)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 50,
          "title": "Segment 50 (pages 490-497)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 51,
          "title": "Segment 51 (pages 498-505)",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 372-380)",
      "start_page": 372,
      "end_page": 380,
      "detection_method": "topic_boundary",
      "content": "References\n[dsnd07] Microsoft “DirectSound,” available online at http://msdn2.microsoft.com/\nen-us/library/bb219818.aspx, August 1, 2007.\n[MP307a] Underbit Technologies. “MAD: MPEG Audio Decoder,” available online\nat http://www.underbit.com/products/mad/, August, 2007.\n[MP307b] Mike Cheng. “The LAME Project,” available online at http://lame.\nsourceforge.net/index.php, August, 2007.\n[OGG07] Xiph.Org “Vorbis audio compression,” available online at http://xiph.org/\nvorbis/, August, 2007.\n[openal07] Creative. “OpenAL: A Free (LGPL-ed) and Open Source, Cross-Platform\nAudio Library Used for 3D and 2D Sound,” available online at http://www.\nopenal.org, August 1st, 2007.\n[Schüler05] Schüler, Christian. “Fast Environmental Reverb Based on Feedback\nDelay Networks,” Game Programming Gems 5, Charles River Media, 2005.\n4.4\nReal-Time Audio Effects Applied\n339\n\n\nThis page intentionally left blank \n\n\n341\n4.5\nContext-Driven,\nLayered Mixing\nRobert Sparks\nsparks.robert@gmail.com\nT\nhe technical quality of sound in our industry is beginning to approach that of the\nfilm industry. Next-generation consoles are here. Games support Dolby Digital\nand DTS; they use high sampling rates; they have virtually unlimited numbers of\nvoices; and they use perceptually lossless compression algorithms. That said, the film\nindustry still has great advantages over us when it comes to overall control of the final\nproduct.\nConsider the process of sound mixing. A film can be mixed with total control of\nevery sound effect. Each scene can be mixed with purpose and deliver a specific emo-\ntional experience. A game is mixed with much less control. For the most part, we can’t\nchange much from scene to scene. We rely on positional and environmental simula-\ntion to do the rest. \nThis gem presents a mixing system that brings the overall sound of a game under\ngreater human control. A similar system was used with great success in developing\nScarface: The World Is Yours and supported a three-week final mixing session of the\ngame at Skywalker Sound.\nOverview\nThis mixing system takes for granted the idea that game parameters can be tuned in\nreal-time. It concerns itself with organizing that tuning experience into an effective\nworkflow—a workflow based on the mixing of films.\nThe system presents sound parameters (for example, volume, pitch, and filter set-\ntings) as if they were the rows of faders and knobs on a mixing board. Each of the rows\ncontrols whole groups of related sounds (for example, music, dialogue, or footsteps).\nThe system also divides the action of the game into logical scenes. Unlike the\nscenes of a film, which can be defined chronologically, the scenes of a game must be\ndefined by actions of the player. \n\n\nAssociating a set of mixing parameters with each logical scene allows precise con-\ntrol of the overall sound (see Figure 4.5.1). It also allows each scene to be mixed inde-\npendently in real-time in a series of mixing sessions.\nThe scene-by-scene approach of this system makes it context-driven. Later, you’ll see\nthat scenes can overlap and modify each other, making it also a layered mixing system.\n342\nSection 4\nAudio \nFIGURE 4.5.1\nAn example of context-driven mixing in which the player enters a dark alley\nand activates “invincibility mode.”\nImplementation\nWhat follows is a high-level description of the mixing system logic and its main\nclasses. A more detailed C++ implementation is available on the CD-ROM.\nMixing System\nThe mixing system provides a central mixing interface to other systems in the game. It\nmanages component lifetimes and performs calculations.\nMixing Categories\nThe mixing system groups related sounds into mixing categories. The system works\nonly in terms of these categories rather than in terms of individual sounds.\nPossible mixing categories include music, ambience, explosion, glass, footsteps, or\nbirds. When a sound plays in the game, it is assigned a mixing category. \nThe Central Mix\nThe mixing system centralizes the mixing (or tuning) parameters for all sounds into a\nsingle logical object, the central mix. Parameters may include volume, pitch, LFE\ngain, auxiliary effect gain, or parameters related to positional simulation. The central\nmix provides a set of parameters for each mixing category. \n\n\nConceptually, the central mix is like a mixing board through which all sounds in\nthe game are routed. As sounds play in the game, they do so according to the parame-\nters assigned to their mixing category in the central mix (see Figure 4.5.2). \n4.5\nContext-Driven, Layered Mixing\n343\nFIGURE 4.5.2\nThe central mix acts as a mixing board for the game,\ncontrolling the playback of groups of sounds.\nMixing Snapshots\nThe sound designer works with the central mix in terms of sets of parameter values\nknown as mixing snapshots. The state of the central mix is calculated using these snap-\nshots (see Figure 4.5.3). Mixing snapshots are like mixing board presets or fader\nautomation controls for the central mix.\nFIGURE 4.5.3\nThe mixing system calculates the central mix using mixing snapshots pro-\nvided by the sound designer.\nThe sound designer defines a mixing snapshot for each logical scene of the game.\nWhen the scene begins, a mixing event triggers, adding the associated snapshot to the\ncentral mix calculations. When the scene ends, another mixing event triggers, remov-\ning the snapshot. The snapshots provide fade-in durations and fade-out durations that\nsmooth transitions as the snapshots are added and removed from the calculations. \n\n\nMixing snapshots give the designer complete control over each scene. The granular-\nity of this control depends on the number of scenes and the number of mixing categories. \nScenes can be very general and appear throughout the game or they can be very\nspecific. Scenes may overlap and be defined as modifications of other scenes. Table\n4.5.1 defines some example mixing snapshots and scenes.\nTable 4.5.1\nExample Mixing Snapshots\nSnapshot Name\nScene Description\nSound Highlights\non_foot_night\nActive when the player is \nFootsteps and foley.\non foot at night.\nNighttime ambient sounds.\nNighttime reverb settings and\nroll-off settings.\non_foot_day\nActive when the player is \nDaytime ambient sounds\non foot in the daytime.\nFootsteps and foley.\nDaytime reverb settings and\nroll-off settings.\nin_car\nActive when the player is \nPlayer’s vehicle sounds.\ndriving a vehicle.\nReduced ambient sounds.\nIn car reverb settings.\nTraffic levels increased.\ninterior\nActive when the player enters \nReduced outdoor sounds.\na building. This snapshot may \ninstall at the same time as \non_foot_day or on_foot_night.\ndialogue_duck\nActive when the player speaks. \nEmphasis on dialogue clarity.\nThis may install at the same \nReduction of music and other \ntime as almost any other snapshot.\ninterfering sounds.\ninvincible\nActive when the player enters \nPitch lowering of specific \na special invincibility mode. \nsound effects.\nThis may install with almost any \nIncreased volume of the \nother snapshot.\nsub-woofer.\nnis_2\nActive during the cinematic, \nAll in-game sound effects \nnon-interactive sequence (NIS) \nremoved from the mix except \nnamed nis_2.\nthose required by the NIS.\npre_mix\nActive at all times.\nAllows for global adjustments\nin all sounds.\nMixing Layers\nMixing layers organize the mixing snapshots that are active. The mixing snapshots are\nassigned to layers by the sound designer. Three mixing layers exist, each exhibiting a\nspecific behavior:\n344\nSection 4\nAudio \n\n\n• The pre-mix layer contains one snapshot that is always present and never changes.\nThis layer allows sound properties to be changed globally in all contexts.\n• The base layer always contains one snapshot and never more than one. As new\nbase layer snapshots become active, they replace previous base layer snapshots.\n• The modifying layer contains any number of snapshots at a time, allowing scenes\nto overlap. These snapshots act as modifiers to other snapshots, typically reducing\nspecific volumes and applying special filters or pitch effects. For example, a mod-\nifying snapshot will duck music during dialogue or apply special filters during key\ngame play moments.\nFigure 4.5.4 illustrates the three mixing layers.\n4.5\nContext-Driven, Layered Mixing\n345\nFIGURE 4.5.4\nThree mixing layers organize the active mixing snapshots.\nExtending the Concept with Live Tuning\nA remote tuning application is essential for achieving a truly efficient mixing work-\nflow. Only live tuning enables the sound designer to fix problems as they are heard\nand to precisely adjust volume levels and other settings.\nThe tuning application can present parameters with simple arrays of numbers or\nwith a graphical representation of a mixing board. It is useful for the application to\ndisplay both the active mixing snapshots and the state of the central mix. This allows\nmixing snapshots for each scene to be selected and tuned individually.\n\n\nThe resulting workflow is very sophisticated. Typically, the process involves\nteleporting to the location or mission that requires mixing, selecting the appropriate\nmixing snapshot in the remote tuning application, and then mixing the scene while\nplaying through it. \nFor Scarface: The World Is Yours, our team implemented a MIDI interface between\nour tuning application and a physical mixing board. This interface made it easy for\npeople from outside the video game industry to work on our project (see Figure 4.5.5).\n346\nSection 4\nAudio \nFIGURE 4.5.5\nLive mixing through a\nMIDI control surface. \nPerformance\nCPU requirements of the mixing system are low. Calculating the central mix consumes\nmost of its energy, which involves combining the parameters of the active mixing snap-\nshots. Typically there may be four active mixing snapshots and 20 sound categories\nwith four parameters each. The parameters are often combined using addition or\nmultiplication.\nMemory requirements grow with the number of mixing snapshots and the num-\nber of sound categories. Large games may require several hundred mixing snapshots\nand a few dozen sound categories. An un-optimized mixing snapshot may require 512\nbytes. As a result, 200 snapshots will consume 100KB of memory. Optimization\nreduces the memory footprint of the system considerably. \nThe most effective optimization reduces the number of mixing snapshots held in\nmemory at a time by loading snapshots only when needed (for example, bundling\n\n\nsnapshots with art for a mission, a location, a cinematic sequence, or a character).\nThis requires pipeline work and coordination with other content loading systems. \nAnother optimization stores mixing snapshot parameters as shorts instead of\nfloats, which halves the size of a mixing snapshot.\nCombining these optimizations, a 512 byte mixing snapshot becomes 256 bytes;\n200 snapshots in memory become 10 in memory. Therefore, a 100KB footprint\nreduces to 2.5KB.\nSample Program\nThe CD-ROM includes a sample program for this article. The program presents a\nvery simple game and an equally simple mixing environment. Click the buttons to\ntrigger sound effects and mixing events. Use the mixing board and related controls to\nselect and tune mixing snapshots and experience context-driven, layered mixing.\nConclusion\nThis gem discussed a powerful approach to sound mixing that has proven itself prac-\ntical and effective in the field. \nWorkflow is paramount when it comes to delivering quality sound. Well-defined,\nintuitive processes enable creative and polished work. Established, effective processes\nare available to be borrowed from the film industry. Technical decisions should focus\non establishing these processes in the gaming industry.\n4.5\nContext-Driven, Layered Mixing\n347\n",
      "page_number": 372,
      "chapter_number": 40,
      "summary": "This chapter covers segment 40 (pages 372-380). Key topics include mixing, mixed, and sound. “MAD: MPEG Audio Decoder,” available online\nat http://www.underbit.com/products/mad/, August, 2007.",
      "keywords": [
        "Mixing",
        "Mixing Snapshots",
        "mixing system",
        "Snapshots",
        "Sound",
        "active mixing snapshots",
        "Central Mix",
        "Layered Mixing",
        "mixing board",
        "Mixing Layers",
        "Game",
        "system",
        "active mixing",
        "Mix",
        "central"
      ],
      "concepts": [
        "mixing",
        "mixed",
        "sound",
        "snapshots",
        "control",
        "controlling",
        "game",
        "gaming",
        "scene",
        "effects"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 55,
          "title": "Segment 55 (pages 536-543)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 51,
          "title": "Segment 51 (pages 490-498)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 50,
          "title": "Segment 50 (pages 490-497)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 52,
          "title": "Segment 52 (pages 499-510)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 53,
          "title": "Segment 53 (pages 511-519)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 381-391)",
      "start_page": 381,
      "end_page": 391,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank \n\n\n349\nS E C T I O N\n5\nGRAPHICS\n\n\nThis page intentionally left blank \n\n\n351\nIntroduction\nTimothy E. Roden, Angelo State University\ntroden@angelo.edu\nI\nn the early days of 3D computer games, developers were generally concerned with\nkeeping polygon counts low and reducing scene complexity. Graphics engines had\nfixed function pipelines that allowed very narrow creative freedom in terms of render-\ning and animation. It is amazing how things have changed. The graphics section of\nthis edition of Game Programming Gems presents a wide range of articles covering top-\nics as diverse as content creation, animation, and rendering.\nJeremy Hayes of Intel expands on the work Jason Shankel did to show advanced\nmethods of procedural terrain generation using a method called particle deposition.\nNew techniques are described for volcano placement, mountain ranges, dunes, and\noverhanging terrain. These new methods add more control, which enable a level\ndesigner to better define the placement of important terrain features. Because crafting\ninteresting and useful terrain is not only a function of geometry, another gem explores\nthe mapping of textures onto terrain. Antonio Seoane, Javier Taibo, Luis Hernández,\nand Alberto Jaspe present a method for mapping very large textures onto outdoor ter-\nrain and Ben Garney provides an implementation of that idea with pointers for\nenabling the technique on SM 1.0-level graphics cards.\nThe graphics section features several excellent gems that cover rendering. Joris\nMans and Dmitry Andreev of 10Tacle Studios describe an advanced decal system that\nproperly blends bump and diffuse maps under a decal, thereby removing the “on top\nof” look that decals can sometimes exhibit. A system for real-time rendering of diffuse\nlighting for rough materials is presented in the gem by Tony Barrera, Anders Hast,\nand Ewert Bengtsson. Chris Lomont presents a comprehensive overview of high-\nperformance subdivision surfaces. Joshua Doss demonstrates the use of graftal imposters\nin rendering cartoon-style plants and fur effects.\nAnimation receives a good treatment in this edition of the Gems series. Bill\nBudge of Sony Entertainment of America explains techniques for dealing with cumu-\nlative errors in skeletal animation sequences. A technique for animating relief impos-\ntors is described by Vitor Fernando Pamplona, Manuel M. Oliveria, and Luciana\nPorcher Nedel. Finally, I have contributed a gem on procedural generation of lipsync\ndata for human models using a freely available phonetic dictionary.\n\n\nThis page intentionally left blank \n\n\n353\n5.1\nAdvanced Particle\nDeposition\nJeremy Hayes, Intel Corporation\narmyofzin@gmail.com\nP\narticle deposition is a procedural terrain generation technique that has so far been\nlimited to creating topography for volcanic mountain ranges. However, the\nbeauty of particle deposition lies within its versatility. This gem demonstrates several\nadvancements to particle deposition that allow the creation of new types of terrain\ntopography as well as improved volcanic mountain ranges. These advances to particle\ndeposition also improve artistic control by allowing a level designer to preview and\nrefine the position and size of terrain features.\nWhy Particles?\nThe surface layer of the earth is called the continental crust. The continental crust floats\non another layer of the earth, called the mantle, because it is less dense than the mantle.\nOver very long periods of time, the continental crust behaves like a ductile solid (like\nhot wax) [Grotzinger07]. The earth’s topography is created by forces above and below\nthe surface. The continental crust is fractured, rippled, and twisted by plate tectonics,\nwhich are powered by geothermal forces inside the earth. Above the surface, the earth’s\nclimate also molds the topography. Erosion by wind, water, and ice can cause dramatic\nchanges over time.\nParticles can be used to naturally simulate the deformation of terrain by plate tec-\ntonics and erosion. Particles can be used to simulate the flow of material and they can\nbe joined to form solids. In other words, particles provide a simple and versatile\nmechanism to generate the topography of virtual terrain.\nParticle Deposition\nShankel proposed the original particle deposition algorithm as a way to generate terrain\nthat looks similar to volcanic mountain ranges [Shankel00]. Particle deposition traverses\na height field with a random walker. The random walker drops at least one particle at\neach location it visits. The particle must check the height of the adjacent positions after\n\n\n354\nSection 5\nGraphics \nit lands on the height field. If a lower adjacent position is found, the particle moves to\nthat position. The particle repeats this process until it can no longer move to an adjacent\nposition of lower elevation. Figure 5.1.1 demonstrates a single particle descending a\none-dimensional height field. The algorithm can be stopped when a predetermined\nnumber of particles have been dropped or when the user is content with the results. Fig-\nure 5.1.2 shows an example of terrain created with particle deposition.\nFIGURE 5.1.1\nDepositing a single particle.\nImproving Particle Deposition\nAlthough particle deposition does create interesting topography for volcanic moun-\ntain ranges, it is easy to see it has some limitations. Notice that the slope of the terrain\nFIGURE 5.1.2\nA screenshot of terrain generated with the original particle deposition algorithm.\n\n\nformed by the particles is almost always 45°, which is a consequence of the heuristic\nused to settle the particles on the terrain. Particles are not allowed to stack if there is a\nlower adjacent position, so the slope will never be greater than 45°. Sometimes parti-\ncles will briefly form slopes less than 45°. This usually occurs when particles are accu-\nmulating in a valley or near an existing peak. Unfortunately, these gentler slopes will\nnever span more than a few positions. Developers would like to be able to create more\ninteresting terrain slopes composed of various angles that span small and large dis-\ntances, as shown in Figure 5.1.3.\n5.1\nAdvanced Particle Deposition\n355\nFIGURE 5.1.3\nAn example of ideal terrain composed of various angles.\nAnother limitation of particle deposition is there is no control over the placement of\nmajor terrain features such as the volcano’s peak. It is also hard to control how many\npeaks to create. The outcome of the terrain is almost entirely random. This is a big dis-\nadvantage if a level designer wants to create a certain number of volcanoes at specific\nlocations. It would be nice to give a level designer more control over the major terrain\nfeatures (for example, size, general shape, and placement). Perhaps the biggest limitation\nto particle deposition is that it only creates topography that is suitable for a volcanic\nmountain range. What if you want to create other types of terrain? Fortunately, all of\nthese limitations can be overcome with simple modifications to particle deposition.\nNotice that particle deposition can be broken into two main steps. The first step\ndefines where to initially drop the particles. The second step defines where the parti-\ncles settle after they have been dropped. Let’s refer to the first step as particle place-\nment, and the second step as particle dynamics. In order to overcome the limitations\nof particle deposition, you need to improve both particle placement and particle\ndynamics. Let’s start by examining particle dynamics.\nImproving Particle Dynamics\nParticle dynamics are required to simulate the effects of erosion. After a particle is\ndropped on the height field, it begins randomly searching the adjacent positions to\ndetermine if the particle can move to a lower elevation. The slope of the terrain is\nimplicitly defined by how far away the particle is allowed to search. The monotony of\n\n\nthe terrain’s slope can be broken up by varying the search radius and elevation thresh-\nold of the particles placed on the slope. If the search radius is large, the slope will be\nshallow, as shown in Figure 5.1.4.a. Conversely, if the search radius is small, the slope\nwill be steep. Figure 5.1.4.b shows how particles can accumulate to form a very steep\nslope. To make this possible, the particle dynamics need to be changed so that parti-\ncles will not move to an adjacent position until the difference in elevation reaches a\ncertain threshold.\n356\nSection 5\nGraphics \nFIGURE 5.1.4\nIn (a), particles that search a large radius form a gentle slope. \nIn (b), particles with elevation thresholds larger than 1 form very steep slopes.\nThe search radius and elevation threshold of each particle can be chosen randomly,\nbut this will cause only small changes in the terrain’s slope. Better results can be\nachieved using a noise function. Noise will allow smooth transitions between gentle\nand steep slopes. There are several widely known noise functions but for simplicity the\nresults in this gem were obtained using value noise. Refer to [Ebert03] for a thorough\n(a)\n(b)\n\n\ndiscussion of noise functions. Figure 5.1.5 demonstrates the difference between using a\nconstant search radius and a search radius defined by a noise function. In Figure 5.1.5,\nall of the particles were dropped at the same location to emphasize the change in slope\ncharacteristics. In a similar manner, the elevation threshold can be varied to create ter-\nrain with even more extreme slopes. The following pseudocode represents the particle\ndynamics used in this article:\nfor each dropped particle:\ndetermine the search radius using a 2D (or 3D) noise function\nwhile there is a lower position (within the search radius):\nmove to the closest position that is lower\nincrement the height field at the final position\n5.1\nAdvanced Particle Deposition\n357\nFIGURE 5.1.5\nThe terrain on the left was created using a constant search radius equal to 1,\nand terrain on the right was created using value noise to vary the search radius between 1 and 4.\nImproving Particle Placement\nThe particle placement heuristic defines where particles are initially dropped on the\nheight field. This is a very important step in particle deposition. If particle placement\nis random, the terrain features are going appear random. Different particle placement\nheuristics will generate different types of terrain. The next three sections investigate\ndifferent particle placement heuristics—each one designed to create a specific type of\nterrain.\nVolcanoes\nBefore you consider a suitable particle placement heuristic for volcanoes, it helps to\nknow how real volcanoes are formed. A volcano is formed by layers of ash and lava\nthat are ejected from its central vent. The layers of ash and lava accumulate, over\nmany years, to create a cone-like shape. The exact shape of the cone is determined by\nthe type of magma ejected from the volcano. Different magma types result in differ-\nent types of eruptions and landforms. Some volcanoes also have side vents and radiat-\ning fissures, which create more asymmetric shapes. Volcanoes can have gentle slopes\nor steep slopes, and they can have symmetric shapes or asymmetric shapes. Like all\nlandforms, the shape of a volcano is also defined by erosion of the surface.\n\n\nOne possible particle placement heuristic for volcanoes would be to dump a lot of\nparticles at a single location until the stopping criteria are met. Although this might\nbe adequate, the resulting shape would be fairly symmetric and somewhat boring. A\nmore interesting particle placement heuristic, as demonstrated in Figure 5.1.6, is to\nloosely simulate the lava streams that wander radially from the central vent of the vol-\ncano. The pseudocode for this particle placement heuristic follows:\nchoose a position for the central vent\nchoose the number of streams\nchoose a random length and direction for each stream\nwhile the stopping criteria has not been met:\nfor each stream:\nstart at the central vent\nwhile the end of the stream has not been reached:\ndrop a particle and compute particle dynamics\nmove in the direction of the stream (+/- small random angle)\nUsing this heuristic, the shape of the volcano is defined by the number of streams,\nthe length of each stream (which doesn’t have to be the same for every stream), and\nthe total amount of particles dropped. The stopping criteria can be when a certain\nnumber of particles have been dropped or when the peak of the volcano has reached a\ncertain elevation. If you implement particle deposition in a way that allows users to\nwatch the terrain being generated in real-time, the users can stop the algorithm when\nthey are content with the results.\nIf a caldera at the peak of the volcano is desired, you can use the same inversion\nalgorithm used in [Shankel00] to invert the peak of the volcano. Begin by arbitrarily\nchoosing the elevation of the caldera plane, and invert the elevation at the central\nvent’s position across the caldera plane. Then check the neighboring positions, invert\nthem if they lie above the caldera plane, and check their neighbors. Repeat this\nprocess until there are no more neighbors to invert.\nNotice the shape of the volcano can now be more easily defined by a level designer.\nA level designer can choose the location of the volcano by deciding where to place the\ncentral vent. In addition, the paths of the lava streams can be precomputed, as shown\nin Figure 5.1.7, and overlaid on the height field. This would allow a level designer to\npreview the size and general shape without dropping a single particle.\nMountains\nParticle deposition can also create realistic mountains by using a clever particle place-\nment heuristic. The ridges between mountain peaks form a very distinct tree-like struc-\nture. For obvious reasons, this will be referred to as the mountain’s ridge structure. This\nis the result of many years of erosion, and the tree-like structure of the surrounding\nriver network is correlated to the mountain’s ridge structure. The ridge structure is\nimportant because it provides ideal locations where particles should be dropped, hence\nthe particle placement heuristic to generate mountains.\n358\nSection 5\nGraphics \n",
      "page_number": 381,
      "chapter_number": 41,
      "summary": "The graphics section of\nthis edition of Game Programming Gems presents a wide range of articles covering top-\nics as diverse as content creation, animation, and rendering Key topics include particle, random, and deposition.",
      "keywords": [
        "particle deposition",
        "particle",
        "particle placement",
        "particle placement heuristic",
        "terrain",
        "deposition",
        "particle dynamics",
        "Advanced Particle Deposition",
        "particle deposition algorithm",
        "placement",
        "original particle deposition",
        "search radius",
        "slope",
        "search",
        "Advanced Particle"
      ],
      "concepts": [
        "particle",
        "random",
        "deposition",
        "depositing",
        "slope",
        "animation",
        "animating",
        "position",
        "positions",
        "graphics"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 8",
          "chapter": 1,
          "title": "Segment 1 (pages 1-13)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 56,
          "title": "Segment 56 (pages 537-549)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 3,
          "title": "Segment 3 (pages 42-63)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 26,
          "title": "Segment 26 (pages 503-525)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 49,
          "title": "Segment 49 (pages 477-489)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 392-403)",
      "start_page": 392,
      "end_page": 403,
      "detection_method": "topic_boundary",
      "content": "Now you need a way to procedurally create a realistic ridge structure. Fortunately,\na suitable algorithm already exists. Diffusion limited aggregation (DLA) is a physical\nprocess that forms dendrite-like structures known as Brownian trees. Figure 5.1.8\nshows a Brownian tree that was created using DLA on a two-dimensional lattice.\n5.1\nAdvanced Particle Deposition\n359\nFIGURE 5.1.6\nA volcano created using advanced particle deposition. See the color insert\nsection for color versions of the photos from this gem.\nFIGURE 5.1.7\nAn example of the paths taken by simulated\nlava streams.\n\n\nQualitatively this looks similar to the ridge structure of mountains. The pseudocode\nto create a Brownian tree on a two-dimensional lattice follows:\nchoose one or more seed positions\nwhile stopping criteria has not been met:\nplace a random walker at a random position\nmove the random walker until it is adjacent to a seed \n(i.e. touching)\nplace a new seed at the random walker’s position\nThe most obvious stopping criteria for a Brownian tree are when a desired num-\nber of particles have been dropped or when the Brownian tree covers a desired area or\nvolume. After the Brownian tree has been generated, it is straightforward to define the\nparticle placement heuristic to generate mountains. First, overlay the Brownian tree\non the height field. Then traverse the entire height field and drop a particle at every\nlocation covered by the Brownian tree. You’ll need to traverse the height field several\ntimes until the terrain features reach a desired size. Figure 5.1.9 shows the results\nusing this particle placement heuristic with the particle dynamics discussed earlier.\n360\nSection 5\nGraphics \nFIGURE 5.1.8\nAn example of a Brownian tree created\nwith DLA.\nNotice the Brownian tree provides a nice way to preview the shape of a mountain\nrange, like the lava streams of volcanoes, without needing to drop a single particle. A\nlevel designer can use the Brownian tree to easily decide the position and size of the\nmountains. The general shape of the Brownian tree can also be controlled by starting\nthe random walkers at positions that lie in the direction of desired growth. \n\n\nIf you are familiar with L-systems (see [Prusinkiewicz96]), you may wonder if L-\nsystems can be used to generate a tree-like structure suitable for the particle placement\nheuristic. The answer is yes. However, L-systems require a grammar to define the tree’s\nstructure. The simplicity of Brownian trees was preferred for this article, but the\npotential of L-systems should not go unmentioned.\nDunes\nDunes are very interesting landforms that are usually associated with deserts, but can\nalso form underwater. Dunes found in the desert are formed by the wind so they are\nconstantly moving and changing shape. In fact, dunes have been recorded moving as\nmuch as 20 meters per year. There are several types of dunes, but this gem focuses on\na common type of dune called a traverse dune. Traverse dunes form a ridge that is per-\npendicular to the direction of the prevailing wind. As shown in Figure 5.1.10, a dune\nis formed as the wind rolls and tosses particles up the windward slope, and deposits\nthe particles on the leeward slope. This motion can be easily simulated using particle\ndeposition.\nOne obvious solution is to randomly pick particles off the height field and dis-\nplace them by a small random distance in the direction of the wind. However, this\ndoes not quite work. The missing key is that particles are more likely to be deposited\non the leeward slope than they are on windward slope because of wind’s “shadow” on\nthe leeward slope. To simulate this, you can assign a cost to the distance each particle\ntraverses. The cost of traveling up the windward slope should be less than the cost of\n5.1\nAdvanced Particle Deposition\n361\nFIGURE 5.1.9\nMountains created using advanced particle deposition. (Also shown in color\nin the color insert section.)\n\n\ntraveling down the leeward slop. The following pseudocode implements a suitable\ncost function, and Figure 5.1.11 demonstrates the results:\nwhile stopping criteria has not been met:\nchoose a random position and remove a particle\ndisplacement = small random number\nwhile displacement >= 0:\nmove the particle one position in the direction of the wind\nif the particle moves up\ndisplacement -= 1\nelse\ndisplacement -= 2\ndrop the particle and compute particle dynamics\n362\nSection 5\nGraphics \nFIGURE 5.1.10\nDunes are formed as particles are carried up the windward\nslope and deposited on the leeward slope.\nFIGURE 5.1.11\nDunes created using advanced particle deposition.\nIn this example the cost of traveling up the windward slope is only the horizontal\ndistance traveled (i.e. no vertical cost), and the cost of traveling down the leeward slope\nis the horizontal and vertical distance traveled. This is a very simple, yet effective, cost\nfunction. Different cost functions will yield different dune shapes and dynamics so\nexperimentation is encouraged.\nOverhanging Terrain\nAs shown in Figure 5.1.12, overhanging terrain is terrain that protrudes over other\nterrain. Particle deposition can create this type of terrain with some minor modifica-\n\n\ntions to the particle dynamics. Assign a stickiness attribute to each particle that is\ndropped on the terrain, and look at the path a particle takes as it falls toward the\nterrain. If a particle touches another particle at an adjacent position before landing on\nthe terrain, the stickiness of the falling particle will determine if the particle stops or\ncontinues to fall. As shown in Figure 5.1.13, when very sticky particles brush the face\nof a steep slope they will accumulate to form an overhang. The stickiness of a region\ncan be user-defined or it can be defined by a noise function. The following pseudocode\nprovides more details:\nchoose an arbitrary threshold, S\nfor each dropped particle:\ndetermine the particle’s stickiness, Sp (3D noise)\ncheck the particle’s path as it falls toward the height field\nif the particle touches an adjacent particle\ndetermine the adjacent particle’s stickiness, Sa (3D noise)\nif (Sp >= S) and (Sa >= S)\nleave the particle at this position\nelse\nuse the heuristic discussed in the particle dynamics section\n5.1\nAdvanced Particle Deposition\n363\nFIGURE 5.1.12\nAn example of overhanging terrain.\nFIGURE 5.1.13\nSticky particles attach\nto steep slopes as they fall to the surface.\n\n\nNotice that traditional height fields cannot be used to define overhanging terrain\nbecause a height field is a two-dimensional lattice with elevation assigned to each\nposition (that is, a two-dimensional scalar field). Voxels can represent volumes in a\nthree-dimensional lattice (that is, a three-dimensional scalar field), and they are ideal\nfor modeling overhanging terrain because they can be polygonalized using a marching\ncubes/tetrahedrons algorithm. Voxel representations will increase the space and time\ncomplexity of particle deposition. Hybrid representations, which only use voxels\nwhere they are needed, can ameliorate some of this cost.\nConclusion\nParticle deposition is a powerful tool for creating various types of realistic terrain. The\nterrain types shown here do not represent an exhaustive list of what is possible with\nparticle deposition. Canyons, craters, caves, plateaus, terraces, and various outcroppings\nare just a few other examples of what might be possible using particle deposition.\nReferences\n[Ebert03] Ebert, David S., Musgrave, F. Kenton, Peachey, Darwyn, Perlin, Ken, and\nWorley, Steven. Texturing & Modeling: A Procedural Approach, Morgan Kauf-\nmann Publishers, 2003.\n[Grotzinger07] Grotzinger, John, et al. Understanding Earth, W. H. Freeman and\nCompany, 2007.\n[Prusinkiewicz96] Prusinkiewicz, Przemyslaw, and Lindenmayer, Aristid. The Algo-\nrithmic Beauty of Plants, Springer Verlag, 1996.\n[Shankel00] Shankel, Jason. “Fractal Terrain Generation—Particle Deposition,”\nGame Programming Gems, pp. 508–511. Charles River Media, 2000.\n364\nSection 5\nGraphics \n\n\n365\n5.2\nReducing Cumulative Errors\nin Skeletal Animations\nBill Budge, Sony Entertainment of America\nbill_budge@playstation.sony.com\nT\nhis gem describes a simple trick to reduce the amount of cumulative error during\nplayback of skeletal animations. It is applied during the offline processing of anima-\ntion data, as part of the normal animation tool chain, and doesn’t affect the size of the\nanimation data. No changes are required in the runtime animation playback engine.\nA Quick Tour of Game Animation Systems\nIn the pioneering 3D game Quake, characters were animated by storing a separate\nmesh for each pose and rendering a different one each frame [Eldawy06]. This kind of\nanimation is simple and in theory capable of the highest quality, but it is difficult to\nmodify and blend animations, and a lot of memory is needed to store the meshes. For\nthese reasons, skeletal animation is now the standard in 3D games.\nSkeletal animation works by attaching the vertices of a single character mesh to a\ncollection of coordinate transforms—the bones of a “skeleton”—and animating the\nbones to deform the mesh. The vertices of the character mesh are positioned in a sin-\ngle coordinate space, together with the transforms that align the bones with the mesh.\nThese transforms make up what is called the “default” or “rest” pose of the skeleton.\nTo deform the mesh into a new pose, you first use the inverses of the rest transforms\nto take the vertices from their original space to “bone” space and then use the new\nbone transforms to move the vertices to their final positions. The equation for each\nvertex is as follows:\n(5.2.1)\nSkeletal animation works well because most game characters and objects are not\nshapeless blobs, but can be closely approximated by collections of rigid bodies. This\nleads to a great reduction in data because there are far fewer bones than mesh vertices\nto animate, and a great increase in flexibility, because it is much easier to modify and\nblend bone transforms than meshes.\nV\nM\nM\nV\n'=\n(\n)\n−\npose\nrest\n1\n\n\nA further observation leads to a trick that reduces the animation data by almost\nhalf again. Game characters and objects are not just random collections of rigid bod-\nies; they are jointed (at least until you blow them to pieces!). Each bone is connected\nto others at these joints, so you can organize the transforms into a hierarchy and make\nall but the root transform relative to its parent transform. Because jointed bones don’t\nmove relative to each other, all of the child translations reduce to constant vectors,\nwhich can be removed from the animation and stored with the skeleton. In fact,\nthey’re already there in the rest pose. Thus animations need only have a single transla-\ntion track for the root and rotation tracks for every bone.\nPlayback of the parent-relative transforms is straightforward. First, you construct\nthe root transform from the root translation and rotation tracks. Next, for each child\nof the root, you construct the child transform by concatenating the child rotation\nwith the parent transform. This process is repeated for the children’s children, and so\non, until you have reconstructed the transform for every bone in the hierarchy.\nFor a more in-depth description of skeletal animation and an introduction to\nskinning techniques to improve mesh deformation around joints, see [Lander98].\nCumulative Error\nUnfortunately, there is a price to pay for the data reduction that you achieved by mak-\ning the transforms parent-relative. It’s not so much the extra work—by doing the\nreconstruction in breadth-first order as described, you only require one additional\nmatrix concatenation per bone. The real problem is that Equation 5.2.1 has effec-\ntively become:\n(5.2.2)\nThe reconstruction of the transforms using Equation 5.2.2 is less robust than\nEquation 5.2.1. There are two reasons for this. First, any error at a transform higher\nin the hierarchy will affect every transform below it. An error at the root transform,\nfor example, will affect every other transform. Second, the error at each step will\nnaturally tend to accumulate, creating a larger error. For the purposes of this gem, we\nassume that the error at each transform behaves like a random variable (otherwise,\nyou would be able to compensate for it). Therefore, the second effect due to concate-\nnating rotations is like adding random variables\nThese two effects mean that the greatest error will be at bones that are furthest\nfrom the root. These are usually the character’s hands and feet. Such artifacts can be\nseen in many games. The classic example is a standing idle animation where the feet\nappear to slide over the ground. An even worse situation is when a character is grip-\nping a bat or sword with two hands. The gripped object is a leaf bone, parented to one\nof the hands. The other hand, being at the end of a different transform chain, will\nappear to be swimming around and through the object.\nV\nM\nM\nM\nM\nV\n'\n...\n=\n(\n)\n−\nroot\nparent\nchild\nrest\n1\n366\nSection 5\nGraphics \n\n\nFor parent-relative animations, you should be mostly concerned with rotation\nerror. Where does this error come from? A small amount is due to the use of floating\npoint arithmetic, which introduces round-off and precision error. However, by far the\nmost important sources of error are lossy compression schemes that most games use to\nfurther reduce the size of animation data.\nThere are many ways to compress rotation data. Some are lossless. For example,\njoints like the knees and elbows have only a single degree of freedom. Storing a full\nrotation such as a quaternion for each pose is wasteful. Instead, the axis of rotation\ncan be stored, and the animation data reduced to a series of angles.\nLossy compression algorithms can lead to even greater reductions in storage cost.\nOne of the simplest techniques is key frame reduction, which looks at the rotation val-\nues and tries to remove those values that can be interpolated from neighboring values\nwithout exceeding some error threshold. The problem with key frame reduction is that\nit is difficult to know which values to keep and which to reject. A better technique is to\nuse a curve fitting algorithm to convert the rotation values into a multidimensional\nspline curve that approximates the data to some tolerance [Muratori03]. Spline curves\nare a good fit to real-world rotational data, are very compact, and are easy to evaluate at\nruntime. Wavelet compression is another popular technique [Beaudoin07].\nEven if an efficient representation is found, storing lots of floating point data can\nbe inefficient if the numbers are in a known small range. If you are using quaternions,\nall numbers are in the range [–1…1], so the eight bits of exponent for each is waste-\nful. You can compress the numbers to 12- or 16-bit fixed point form. For an entire\ngem on quaternion compression, see [Zarb-Adami02].\nIt is common to push the compression algorithms to the point where artifacts due\nto cumulative errors just become visible. In this case, you will always have significant\nerrors at each rotation.\nFigure 5.2.1 shows a simple 2D hierarchy of two bones in the original pose and\nsome possible reconstructed poses, given the same random error at each transform.\nThe parent transform is shown with its error range as a gray region. Three child trans-\nforms are shown with their error ranges, one aligned with the actual child transform,\nand the other two where the parent has the greatest error. Note how the error at the\nends of the bones increases from parent to child.\nEliminating Cumulative Rotation Errors\nThe conventional algorithm for processing the animation begins by extracting all of\nthe transform data from the authored representation into a common coordinate space.\nNext, all child transforms are made parent-relative by concatenating with the parent’s\ninverse transform. Finally, the relative transforms are compressed and formatted for the\nruntime playback engine. Let’s call this the naive algorithm, because it assumes that\nthere is no error introduced by compression and reconstruction by the runtime.\n5.2\nReducing Cumulative Errors in Skeletal Animations\n367\n\n\nThe idea of this gem is to take reconstruction errors into account and use the recon-\nstruction algorithm to get the parent transform with error, and make the child trans-\nforms parent-relative to that transform rather than the original.\nThis leads to the following procedure, which we call Algorithm 1:\n1. Compress and format the root transform data.\n2. Run the decompression algorithm on the result of Step 1 and replace the\noriginal transform with the results.\n3. For each child of the root, make its transform data relative to the decompressed\nroot transform data. Compress and format the parent-relative rotation data.\n4. Run the decompression algorithm on each child from Step 3 to get its final\ntransform data and replace the original child data with the result.\n5. Continue down the hierarchy until all bones have been processed.\nThis completely eliminates the accumulation of rotational error because for each\nchild transform, Step 3 subtracts the rotational error of the parent transform. However,\nthe parent’s rotation error does more than just rotate the child. It also translates the\nchild (unless the child’s origin is at the parent’s origin). That means that the parent-\nrelative transform resulting from Step 3 will generally have a translation that is differ-\nent from the constant one you store with the skeleton. This translation error can’t be\neliminated by any child rotation. Although you could correct it by adding a new trans-\nlation, that would defeat the whole purpose of making the transforms parent relative,\nwhich was to eliminate these translations in the first place! So translation error is still\naccumulating, although total error is less than with the naive algorithm because the\nrotation error is less.\nFigure 5.2.2 shows the results of the naive algorithm and Algorithm 1. Note how\nthe child bones all have the same orientation as in the true pose (although still with\nlocal error), and how they are offset by the translation error as a result of the rotational\nerror of the parent bone.\n368\nSection 5\nGraphics \nFIGURE 5.2.1\nCumulative error increases from parent to child.\n\n\nIt is possible to reduce this translation error, but to do this you have to rotate the\nchild bone away from its true orientation. To calculate this rotation, you first select a\nfixed point on the bone where you would like to minimize the translation error. Let’s\ncall it a significant point. A significant point could be the origin of a child bone, or\nsome arbitrary point that identifies the “end” of the bone. You rotate the reconstructed\nbone from Algorithm 1 so as to move the significant point closest to its true position.\nFigure 5.2.3 shows the geometry.\n5.2\nReducing Cumulative Errors in Skeletal Animations\n369\nFIGURE 5.2.2\nRemoving cumulative rotational error.\nFIGURE 5.2.3\nReducing translation error at\nthe significant point.\nThe rotation is computed by the following equations:\n(5.2.3)\n(5.2.4)\nangle=\n⋅\n(\n)\n−\ncos 1 O'S'\nO'S\nAxis\nO'S'\nO'S\n=\n×\n\n\nModify Step 3 of Algorithm 1 to get Algorithm 2:\n1. Compress and format the root transform data.\n2. Run the decompression algorithm on the result of Step 1 and replace the\noriginal transform with the results.\n3. For each child of the root:\na. Make its transform data relative to the decompressed root transform data.\nb. Concatenate this with the decompressed parent transform to get the\nreconstructed transform, without error.\nc. Compute the rotation that takes the significant point in this reconstructed\ntransform closest to its actual position, and add it to the transform.\nd. Compress and format the parent-relative rotation data.\n4. Run the decompression algorithm on each child from Step 3 to get its final\ntransform data and replace the original child data with the result.\n5. Continue down the hierarchy until all bones have been processed.\nFigure 5.2.4 shows the results of the naive algorithm and Algorithm 2. Note how\nthe child bones now have slight rotation errors (although they don’t accumulate, as\neach step still corrects for the parent’s error) and how the translation error has been\nreduced.\n370\nSection 5\nGraphics \nFIGURE 5.2.4\nReducing cumulative translational error.\nAlgorithm 2 does not completely eliminate translational error. One way to address\nthis is to add translation tracks at leaf bones to combat any objectionable artifacts.\nAnother way is to employ an inverse kinematics system to make sure that bones are\nwhere they should be. Even if a game uses an IK system, these error reduction tech-\nniques are useful because they improve the quality of the pose reconstruction so that it\nis closer to the artist’s original version.\n",
      "page_number": 392,
      "chapter_number": 42,
      "summary": "touching)\nplace a new seed at the random walker’s position\nThe most obvious stopping criteria for a Brownian tree are when a desired num-\nber of particles have been dropped or when the Brownian tree covers a desired area or\nvolume Key topics include particle, errors, and transform.",
      "keywords": [
        "Particle",
        "Particle Deposition",
        "error",
        "Brownian tree",
        "transform",
        "Advanced Particle Deposition",
        "Brownian",
        "transform data",
        "child",
        "algorithm",
        "data",
        "root transform",
        "translation error",
        "rotation",
        "root transform data"
      ],
      "concepts": [
        "particle",
        "errors",
        "transform",
        "animations",
        "animation",
        "animated",
        "animate",
        "bone",
        "rotation",
        "rotations"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 37,
          "title": "Segment 37 (pages 368-376)",
          "relevance_score": 0.42,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 41,
          "title": "Segment 41 (pages 389-403)",
          "relevance_score": 0.41,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 28,
          "title": "Segment 28 (pages 264-271)",
          "relevance_score": 0.35,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 45,
          "title": "Segment 45 (pages 444-451)",
          "relevance_score": 0.35,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 15,
          "title": "Segment 15 (pages 132-139)",
          "relevance_score": 0.33,
          "method": "api"
        }
      ]
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 404-412)",
      "start_page": 404,
      "end_page": 412,
      "detection_method": "topic_boundary",
      "content": "Conclusion\nYou have seen how skeletal animation systems suffer from cumulative error, and how\nconventional processing of animation data can lead to noticeable artifacts on playback.\nWith a simple modification to the processing algorithm, however, you can eliminate\ncumulative rotational error and reduce the translation errors.\nOnly the preprocessing of animation is changed, so it has no performance or\nmemory impact on the game runtime. Finally, translation tracks can be added at\nimportant bones to eliminate any remaining artifacts.\nReferences\n[Beaudoin07] Beaudoin, Philippe. “Adapting Wavelet Compression to Human\nMotion Capture Clips,” available online at http://www.cs.ubc.ca/~van/papers/\n2007-gi-compression.pdf.\n[Eldawy06] Eldawy, Mohamed. “Trends of Character Animation in Games,” available\nonline at http://adlcommunity.net/file.php/23/GrooveFiles/Games%20Madison/\ncharAnimation.pdf.\n[Lander98] Lander, Jeff. “Skin Them Bones: Game Programming for the Web Gener-\nation,” Game Developer Magazine, May 1998, pp. 11–16, www.gamasutra.com/\nfeatures/gdcarchive/2000/lander.doc.\n[Muratori03] Muratori, Casey. “Discontinuous Curve Report,” available online at\nhttp://funkytroll.com/curves/emails.txt.\n[Zarb-Adami02] Zarb-Adami, Mark. “Quaternion Compression,” Game Program-\nming Gems 3, Charles River Media Press, 2002.\n5.2\nReducing Cumulative Errors in Skeletal Animations\n371\n\n\nThis page intentionally left blank \n\n\n373\n5.3\nAn Alternative Model for\nShading of Diffuse Light \nfor Rough Materials\nTony Barrera, Barrera Kristiansen AB\ntony.barrera@spray.se\nAnders Hast, Creative Media Lab, \nUniversity of Gävle\naht@hig.se\nEwert Bengtsson, Centre For Image Analysis,\nUppsala University\newert@cb.uu.se\nT\nhis gem shows how it is possible to improve the shading of rough materials with\na rather simple shading model. This gem discusses both the flattening effect,\nwhich is visible for rough materials, as well as the possible methods for creating the\nbackscattering effect.\nIntroduction\nUsually Lambert’s model (cosine law) [Foley97] is used to compute the diffuse light,\nespecially if speed is crucial. This model is used for both Gouraud [Gouraud71] and\nPhong [Phong75] shading. However, this model is known to produce plastic looking\nmaterials. The reason for this is that the model assumes that the object itself is perfect\nin the sense that the surface scatters light equally in all directions. However, in real life\nthere are no such perfect materials.\n\n\nA number of models have been introduced in literature that can be used for met-\nals [Blinn77, Cook82]. These models assume that the surface consists of small v-\nshaped cavities. Oren and Nayar proposed a model for diffuse light suitable for rough\nsurfaces [Oren94, Oren95a, Oren95b]. This model can be used for rough surfaces,\nlike clay and sand. However, this model is quite computationally expensive, even in\nits simplified form. Nonetheless, the benefit of using their model is that it produces\nmore accurate diffuse light for rough surfaces. They showed that a cylindrical clay vase\nwill appear almost equally bright over the entire lit surface except for the edges where\nthe intensity drops quite suddenly.\nThe Lambert model will produce shadings which drop off gradually and this is\nseldom the case in real life. This effect is shown in Figures 5.3.1 and 5.3.2. Note that\nthe intensity is not scaled down for the Lambert shaded teapot in Figure 5.3.1 and\ntherefore it appears brighter than the teapot rendered with the Oren-Nayar model in\nFigure 5.3.2. Nonetheless, it is apparent that the intensity is almost equally bright\nover the surface for the Oren-Nayar model.\n374\nSection 5\nGraphics \nFIGURE 5.3.1\nA Lambert shaded teapot.\nWe proposed earlier in a poster a model that simulates the same behavior, but is\nmuch simpler and faster to compute [Barrera05]. This gem develops the idea further.\n\n\nThe Flattening Effect\nOne of the main differences between Lambert’s model and the Oren-Nayar model is\nthat the Oren-Nayar model produces diffuse light that is almost equally bright over the\nsurface. This flattening effect can be modeled by forcing the diffuse light to be closer to\nthe maximum intensity, except on the edge where it should drop down rather quickly\nto zero. Thus, the shading curve will be horizontally flat over a large portion of the\ninterval. The following function could be used for this purpose:\n(5.3.1)\nwhere cosθ= n·l is the Lambert’s law, ρ is the surface roughness property that tells how\nflat (or close to one) the function should be, and k is a constant.\n(5.3.2)\nThe constant k makes sure that Id=1 for cosθ=1. Note that k can be precomputed\nand can also contain surface color.\nThe roughness property ρ is not derived in a way that it describes the physical\nbehavior in the way that Oren and Nayar does for the distribution of cavities. Instead\nk = +\n1 ρ\nρ\nI\nk\nd =\n−\n+\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n1\n1\n1 ρ\nθ\ncos\n5.3\nAn Alternative Model for Shading of Diffuse Light for Rough Materials\n375\nFIGURE 5.3.2\nAn Oren-Nayar shaded teapot.\n\n\nit can be used to adjust the slope of the curve, thus simulating different degrees of\nroughness. Figure 5.3.3 shows Lambert (cos θ), the steepest curve, compared to the\nnew model with ρ = {0.75, 1.5, 3.0, and 6.0}. The larger the value for ρ that is used,\nthe closer to 1 the curve will be over the interval.\n376\nSection 5\nGraphics \nFIGURE 5.3.3\nIntensity for different angles between the normal\nand the light source vector for ρ = {0.75, 1.5, 3.0, and 6.0}.\nIn Figures 5.3.4 through 5.3.7, the effect of using the method is shown for a\nshaded teapot. Notice how the surface appears flatter when ρ increases.\nThe shader code in GLSL looks like this:\nuniform float shininess;\nvarying vec3 normal, color, pos;\nvoid main()\n{\nvec3 l = normalize(gl_LightSource[0].position.xyz - pos);\nvec3 n = normalize(normal);\nfloat nl=max(0.0, (dot(n,l)));\n// Flattening \nfloat rho=6.0;\nfloat k=(1.0+rho)/rho;\nfloat diff = k*(1.0-1.0/(1.0+rho*nl));\ngl_FragColor = vec4(color * diff, 1);\n}\n\n\nBackscattering\nThe backscattering effect is visible in many materials and it is a contributing reason to\nwhy you can see things quite well in the dark using a flashlight. However, it is a rather\nsubtle effect and it is quite hard to notice it in real life and it should therefore be used\nwith care. Because it is visible only when the light source is in the same direction as\nthe viewer, it could be modeled using l·v. The following equation was used as an\nattenuation factor that is multiplied with the diffuse light:\n5.3\nAn Alternative Model for Shading of Diffuse Light for Rough Materials\n377\nFIGURE 5.3.4\nρ is the surface roughness property\nthat tells how flat (or close to one) the function\nshould be; here ρ is 0.75.\nFIGURE 5.3.5\nHere the surface roughness prop-\nerty, ρ, is 1.5.\nFIGURE 5.3.6\nHere, ρ is 3.0. Notice how the\nsurface appears flatter when ρ increases.\nFIGURE 5.3.7\nHere, ρ is 6.0. The flattest surface\nof all.\n\n\n(5.3.3)\nWe used the power function for f but the Schlick model [Schlick94] can also be\nused. This function determines how the effect will be distributed over the surface in a\nsimilar manner as for the specular light.\nThe constant b will determine how much impact the backscattering effect should\nhave on the diffuse light. A large b will yield a small effect and vice versa. In Figure\n5.3.8, a small b is used only to demonstrate the effect.\nIn Figure 5.3.9, it is clear that the backscattering effect vanishes as the viewer is\nlooking at the object from a different direction than the light source direction.\nF\nf\nb\nb\nbs =\n•\n(\n)+\n+\nl v\n1\n378\nSection 5\nGraphics \nFIGURE 5.3.8\nNote how the center of the teapot\nappears brighter because the light source is in the\nsame direction as the viewer.\nFIGURE 5.3.9\nThe light source is no longer in\nthe direction of the viewer and the backscattering\nis not visible.\nThe extra code needed for computing the backscattering is as follows:\nvec3 v = normalize(-pos);\nfloat lv=max(0.0, (dot(l,v)));\n// Backscattering\nfloat b=1.00000;\nfloat bs=(pow(lv,80.0)+b)/(1.0+b);\ngl_FragColor = vec4(color*diff*bs, 1);\nAnother possibility is to add the effect as a term of its own to the Phong-Blinn\nmodel. The following equation was used for Figure 5.3.10.\n(5.3.4)\nI\nK f\nbs\nbs\n=\n•\n(\n)\nl v\n\n\nThe constant Kks determines how much the effect will be visible and once again\nthe function f determines how the effect will be distributed over the surface. It should\nbe mentioned that the backscattering intensity was multiplied with the color of the\nsurface in the picture.\n5.3\nAn Alternative Model for Shading of Diffuse Light for Rough Materials\n379\nFIGURE 5.3.10\nOnce again the teapot appears brighter in\nthe center because the light source is in the same direction\nas the viewer.\nChange the code as follows:\nfloat kks=0.3;\nfloat bs=kks*pow(lv,80.0);\ngl_FragColor = vec4(color *(diff+bs), 1);\nConclusion\nThe Oren-Nayar model is rather complex while the proposed model is quite simple\nand easy to use. Still it produces a result that mimics behavior typical for rough mate-\nrials. You saw two possible ways of computing the backscattering effect and it is hard to\ntell which one is the better method. You can used large values for the power function to\nmake the difference visible in the images, but when an object is rotated interactively it\nis clear that a much lower value gives a more pleasing result.\n",
      "page_number": 404,
      "chapter_number": 43,
      "summary": "This gem discusses both the flattening effect,\nwhich is visible for rough materials, as well as the possible methods for creating the\nbackscattering effect Key topics include model, game, and surface.",
      "keywords": [
        "Diffuse Light",
        "Model",
        "Light",
        "Rough Materials",
        "effect",
        "surface",
        "Alternative Model",
        "Oren-Nayar model",
        "Diffuse",
        "light source",
        "backscattering effect",
        "Rough",
        "rough surfaces",
        "Materials",
        "backscattering"
      ],
      "concepts": [
        "model",
        "game",
        "surface",
        "float",
        "figures",
        "light",
        "curve",
        "shading",
        "differences",
        "different"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 40,
          "title": "Segment 40 (pages 394-404)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 60,
          "title": "Segment 60 (pages 581-588)",
          "relevance_score": 0.42,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 57,
          "title": "Segment 57 (pages 550-561)",
          "relevance_score": 0.4,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 44,
          "title": "Segment 44 (pages 432-439)",
          "relevance_score": 0.4,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 47,
          "title": "Segment 47 (pages 458-468)",
          "relevance_score": 0.4,
          "method": "api"
        }
      ]
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 413-428)",
      "start_page": 413,
      "end_page": 428,
      "detection_method": "topic_boundary",
      "content": "References\n[Barrera05] Barrera, T., Hast, A., and Bengtsson, E. “An Alternative Model for Real-\nTime Rendering of Diffuse Light for Rough Materials,” SCCG’05 Proceedings\nII, pp. 27–28, 2005.\n[Blinn77] Blinn. J.F. “Models of Light Reflection for Computer Synthesized Pic-\ntures,” Proceedings of the 4th Annual Conference on Computer Graphics and\nInteractive Techniques, 1977, pp. 192–198.\n[Cook82] Cook, R.L., and Torrance, K.E. “A Reflectance Model for Computer\nGraphics,” ACM Transactions on Graphics (TOG), 1, 1, 1982, pp. 7–24.\n[Foley97] Foley, J.D. van Dam, A., Feiner, S.K., and Hughes, J.F. Computer Graphics:\nPrinciples and Practice, Second Edition in C, 1997, pp. 723–724.\n[Gouraud71] Gouraud H. “Continuous Shading of Curved Surfaces,” IEEE Transac-\ntions on Computers, Vol. c–20, No. 6, June, 1971.\n[Oren94] Oren, M., and Nayar, S.K. “Generalization of Lambert’s Reflectance\nModel,” Proceedings of the 21st Annual Conference on Computer Graphics and\nInteractive Techniques, 1994, pp. 239–246.\n[Oren95a] Oren, M., and Nayar, S.K. “Generalization of the Lambertian Model and\nImplications for Machine Vision,” International Journal of Computer Vision,\n1995, pp. 227–251.\n[Oren95b] Oren, M., and Nayar, S.K. “Visual Appearance of Matte Surfaces,” Science,\n267, 5201, 1995, pp. 1153–1156.\n[Phong75] Phong, B.T. “Illumination for Computer Generated Pictures,” Commu-\nnications of the ACM, Vol. 18, No. 6, June, 1975.\n[Schlick94] Schlick, C. “A Fast Alternative to Phong’s Specular Model,” Graphics\nGems 4, 1994, pp. 385–387.\n380\nSection 5\nGraphics \n\n\n381\n5.4\nHigh-Performance\nSubdivision Surfaces\nChris Lomont\nchris@lomont.org\nS\nubdivision surfaces are a method of representing smooth surfaces using a coarser\npolygon mesh, often used for storing and generating high-detail geometry (usu-\nally dynamically) from low-detail meshes coupled with various scalar maps. They have\nbecome popular in modeling and animation tools due to their ease of use, support for\nmulti-resolution editing, ability to model arbitrary topology, and numerical robust-\nness. This gem presents extensions to Loop subdivision, blending results from numer-\nous places and adding useful implementation details. The result is a complete set of\nsubdivision rules for geometry, texture, and other attributes. The surfaces are suitable\nfor terrain, characters, and any geometry used in a game.\nThis gem also presents a general overview of methods for fast subdivision and\nrendering. After learning the material presented, you’ll be able to implement subdivi-\nsion surfaces in a production environment in either tools or in the game engine itself.\nIntroduction to Subdivision Schemes\nThere are many types of subdivision schemes, with varying properties. Some of the\nproperties are as follows:\n• Mesh type—Usually the mesh is made of triangles or quads.\n• Smoothness—This is the continuity of the limit surface, and is usually denoted\nC1, C2…, and so on, or G1, G2…., and so on.\n• Interpolating [Zorin96] or approximating—Interpolating schemes go through\nthe original data points, whereas approximating schemes may not.\n• Support size—This is the amount of neighboring geometry affecting the final\nposition of a given surface point.\n• Split—Some schemes work by replacing faces with more faces, others work by\nreplacing vertices with new sets of vertices. A few more work by replacing the\nentire previous mesh, making a “new” mesh.\nTable 5.4.1 lists common schemes and some data about them.\n\n\nTable 5.4.1\nSubdivision Schemes\nMethod\nMesh\nSmoothness*\nSplit\nScheme\nCatmull-Clark\nQuads\nC2\nFace\nApproximating\nDoo-Sabin\nAny\nC1\nVertex\nApproximating \nLoop\nTriangles\nC2\nFace\nApproximating\nButterfly\nTriangles\nC1\nFace\nInterpolating\nKobbelt\nQuads\nC1\nFace\nInterpolating\nReif-Peters\nAny\nC1\nNew\nApproximating\nSqrt(3) (Kobbelt)\nTriangles\nC2\nFace\nApproximating\nMidedge\nQuads\nC1\nVertex\nApproximating\nBiquartic\nQuads\nC2\nVertex\nApproximating\n*Smoothness generally has one degree less of continuity at exceptional points.\nAlthough this gem focuses mainly on generating geometry and rendering issues,\nsubdivision surfaces have many other uses, including:\n• Progressive meshes\n• Mesh compression\n• Multi-resolution mesh editing ([Zorin97])\n• Surface and curve fitting ([Lee98] and [Levin99])\n• Point set to mesh generation\nMost 3D animation and rendering packages support subdivision surfaces as a\nprimitive, although there is no standard type used throughout the industry. (See\nhttp://www.et.byu.edu/~csharp2/#A_SubD [as of 2007] for a partial list of toolsets\nsupporting subdivision.) Catmull-Clark and Loop subdivision are the most commonly\nused, because they are arguably the simplest, are well documented, and are well suited\nto real-time rendering. \nA related topic is PN triangles [Vlachos00], which provide a way to replace trian-\ngles at the rendering level with a smoother primitive. The basic idea is to quadratically\ninterpolate surface normals, similar to Phong shading, and to use this cubically to\ninterpolate new geometry. A good overview of subdivision is [Zorin00].\nSubdivision Schemes Usage\nFor a production tool chain for interactive games, one method for using subdivision\nsurfaces is to create art using high-resolution geometry and textures (and the geome-\ntry might be modeled in whatever subdivision flavors the tools support). The art is\nthen exported as high-density polygon models and associated data. Tools then reduce\nthe assets to a low poly count mesh with associated displaced subdivision maps, tex-\ntures, and animation data. A subdivision kernel in the GPU dynamically converts\nassets back to needed poly counts at runtime based on speed, distance from camera,\n382\nSection 5\nGraphics \n\n\nhardware support, and so on. This allows different subdivision surfaces to be used in\nthe asset creation and asset rendering stages, which has advantages.\nA tool along these lines is ZBrush, which allows you to edit meshes using sub-\ndivision surfaces in order to add geometry at multiple resolution levels, and then con-\nverts the resulting high-detail geometry to low-detail meshes and displacement maps.\nChoice of Subdivision Type\nThis gem covers implementing Loop subdivision [Loop87]. Some reasons are that it is\ntriangle based, making it (perhaps) easier to implement on GPUs, most artists and\ntools already work with triangle meshes, it is well studied, and it produces nice-looking\nsurfaces. Another common choice is Catmull-Clark subdivision [Catmull78], but\nbeing quad-based, it seems less suitable for gaming. Pixar uses Catmull-Clark subdivi-\nsion for animating characters. Many ideas presented in this article are applicable to\nquad-based subdivision as well as other schemes.\nLoop Subdivision Features and Options\nA single iteration of the original Loop Subdivision algorithm applied to a closed trian-\ngle mesh returns another closed triangle mesh with more faces. Repeated applications\nresult in a smooth limit surface. Extensions to the original method are needed to\nmodel more features; a full-featured subdivision toolset includes the following:\n• Boundaries—Allow non-closed meshes.\n• Creases—Allow sharp edges and surface ridges. Adding boundaries gives creases.\n(Creases technically should have the techniques in [Biermann06] to prevent minor\ncorner errors, but [Zorin00] claims these errors are visually minor. The corrections\nrequire more computation than what is presented and intended: a technique\nsuited for real-time rendering.)\n• Corners—Useful for making pointed items.\n• Semi-sharpness—Modifies the basic rules for boundaries, creases, and corners to\nget varying degrees of sharpness.\n• Colors and textures—Easy extensions of the subdivision process needed for ren-\ndering and gaming.\n• Exact positions—After a few subdivisions, vertices can be pushed to what would\nbe their final position if the subdivision were carried out to the limit. This com-\nputation is not very expensive.\n• Exact normals—Computing exact normals for shading is not very expensive, and\nis less costly than face normal averaging.\n• Displacement mapping—Adds geometry to the subdivided surface, and is a very\nnice feature to have, but not implemented in this gem. Instead, see [Lee00] and\n[Bunnell05].\n5.4\nHigh-Performance Subdivision Surfaces\n383\n\n\n• Evaluation at arbitrary points—Allows for computing the limit surface at an\narbitrary position on the surface [Stam99]. This is useful for ray tracing or very\ndetailed collision detection, but for game rendering is not likely to be needed.\n• Prescribed normals—Allows for requesting specific normals on the limit surface\nat given vertices [Biermann06], and is useful for modeling. However, it is more\nexpensive to implement than what is presented in this gem, and for this and space\nreasons details, it is omitted. \n• Multi-resolution editing support—By storing all the levels of the subdivided\nmesh, users can work on any level of the subdivision, making many editing fea-\ntures easier [Zorin97].\n• Collision detection—Needed for game dynamics; one method is in [DeRose98].\n• Adaptive subdivision—Subdivides parts of the mesh into different amounts based\non some metric, patching any holes formed in the process. Adaptive subdivision is\nuseful for keeping polygon counts low while still giving nice curves, silhouettes,\nand level of detail. The decision on where to subdivide the mesh is usually based\non curvature.\nFeatures are added to the mesh by tagging vertices, faces, and edges with parame-\nters to direct the subdivision algorithm. Tag combination restrictions can be enforced\nin software to prevent degenerate cases if needed.\nGeometry Creation\nTo implement boundaries, creases, corners, and semi-smooth features, each vertex\nand edge is tagged with a floating-point weight 0 ≤w < ∞. A weight of 0 denotes stan-\ndard Loop subdivision, and ∞denotes an infinitely sharp crease or boundary. Infinity\nneed not be encoded in the data structures, because the weight is really a counter for\nthe levels of subdivision affected. Any number larger than the highest level of subdivi-\nsion performed will suffice. For example, 32767 should suffice, because it is unlikely\nthat any mesh will be subdivided this many times. \nLoop subdivision takes a mesh and creates a new mesh by splitting each old trian-\ngular face into four new faces, as shown in Figure 5.4.5. This is done in two steps. The\nfirst step inserts a new vertex on each existing edge, and the second step modifies old\nvertices (not those inserted on the edges).\nMost of these geometry rules are from [Hoppe94a] and [Hoppe94b], with some\nideas merged from [DeRose98] and [Schweitzer96].\nEdges\nThe first step inserts a new vertex on each edge using a weighted sum of nearby vertices.\nThe edge weights and the types of vertices at each endpoint of the edge serve to catego-\nrize the edges. Vertex categories are listed in the next section. \n384\nSection 5\nGraphics \n\n\nEach (non-boundary) edge has two adjacent triangles; the new vertex has the posi-\ntion (v0 + v1), where v0 and v1 are the vertices on the edge to split, and the other two \nvertices are the remaining vertices on the two adjacent triangles. This is illustrated in\nFigure 5.4.1, where the circle denotes the new vertex on the edge between the triangles.\nThe weights can be written \n, where position j corresponds to vertex j \n(0-indexed).\n3\n8\n3\n8\n1\n8\n1\n8\n,\n,\n,\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n3\n8(\n5.4\nHigh-Performance Subdivision Surfaces\n385\nFIGURE 5.4.1\nEdge mask.\nThe weights used to create a new edge depend on the edge weight and the vertex\ntypes of the two vertices that define the edge: v0 and v1. Given the two vertices on an\nedge, Table 5.4.2 shows which type of weights to use to create the new edge vertex.\nWeights are as follows:\n• Type 1 weights : \n.\n• Type 2 weights :\n.\n• Type 3 weights : \n, where the \nweight goes with the corner edge. \nAn edge is smooth if it has weight w = 0. An edge is sharp if its weight is w ≥0. If\nan edge has weight 0 < w < 1, the new vertex is linearly interpolated between the two\ncases w = 0 and w = 1, keeping the end vertex types fixed.\n3\n8\n3\n8\n5\n8\n0 0\n,\n, ,\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n1\n2\n1\n2\n0 0\n,\n, ,\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n3\n8\n3\n8\n1\n8\n1\n8\n,\n,\n,\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n\n\nTable 5.4.2\nEdge Mask Selection\nDart\nRegular Crease\nNon-Regular Crease\nCorner\nDart\n1\n1\n1\n1\nRegular crease\n1\n2\n3\n3\nNon-regular crease\n1\n3\n2\n2\nCorner\n1\n3\n2\n2\nWhen an edge is split, each new edge gets weight ˜w = max{w–1,0}. This gives finer\ncontrol over sharpness because the crease rules are applied to a few levels, and then the\nsmooth rules are applied, with possible interpolation on one step. An option for more\ncontrol is to tag each end of an edge with a weight, giving two weights per edge, to\ninterpolate the new edges, and then to make the corresponding changes throughout.\nNote in all cases the total weight sums to 1 (also true for vertex masks).\nVertices\nThe type of a vertex depends on the vertex weight and the types of incident edges. A\nsmooth vertex is one with zero incident sharp edges and weight 0. A dart vertex has one\nsharp incident edge and weight 0. A crease vertex has two sharp incident edges and\nweight 0. A corner vertex has > 2 sharp incident edges or has weight w ≥1. An interior\ncrease vertex is regular if it has six neighbors and exactly two non-sharp edges on each\nside of the crease; a boundary crease vertex is regular if it has four neighbors. Otherwise,\ncrease and boundary vertices are non-regular. If an edge has weight 0 < w < 1, it suffices\nto call it smooth for vertex classification.\nThe second step of Loop subdivision modifies all the original vertices (not the\nvertices inserted on each edge in step one) using a weighted sum of the original vertex\nand all neighboring vertices.\nThe weighting is dependent on the number n of neighboring vertices. For\nsmooth and dart vertices, this is illustrated in Figure 5.4.2. The value of b is usually\n, although other values are in the literature. (For example, \n[Warren95] proposes b(n) = 3/(8n) for n > 1 and b(3) = 3/16, but this has unbounded\ncurvature for a few valences.) The old vertex is given weight 1 – b(n)  and each old\nneighbor (not the vertices created in step one!) is given weight b(n)/n to determine\nthe new vertex position, which is then the weighted sum of all these vertices: \n.\nv\nb n\nv\nb n\nn\nv\nnew\nold\nj\nj\nn\n=\n−\n(\n)⋅\n+\n=∑\n1\n1\n( )\n( )\nb n\nn\n( )\ncos\n=\n−\n+\n⎧\n⎨\n⎩\n⎫\n⎬\n⎭\n⎛\n⎝\n⎜\n⎜\n⎞\n⎠\n⎟\n⎟\n1\n64\n40\n3 2\n2\n2\nπ\n386\nSection 5\nGraphics \n\n\nFor corner vertices, the vertex position does not move, so vnew = vold.\nFor crease vertices, the new vertex is the sum of \nof the original vertex and\nof each of the two neighbors on the crease.\nIf a vertex has weight 0 < w < 1, the new vertex is linearly interpolated between\nthe two cases w = 0 and w = 1. A new vertex also has a new weight ˜w = max{w–1,0}.\nThe final case is when a vertex has weight 0 < wv < 1 and some neighboring edge\nhas weight 0 < we < 1, leading to many possible combinations of interpolation. In this\ncase evaluate each with weights 0 and weights 1, and interpolate on wv, instead bi-\nlinearly interpolating the four cases of the weights (0,0), (0,1), (1,0), and (1,1).\nAnother option is to require integer weights, avoiding interpolation cases entirely\nat the loss of control on semi-sharp creases.\nDisplacement-mapped surfaces are implemented by moving the vertices as needed\naccording to a displacement map. Vertices are also modified using [Biermann06] to\nimplement prescribed normals, and this also splits crease rules into convex and concave\ncases, avoiding certain degenerate cases.\nLimit Positions\nVertices can be projected to the position they would take if the surface were sub-\ndivided infinitely many times. This is often done after a few subdivision levels have\nbeen applied. This is optional and often doesn’t modify the surface much.\nLimit positions v∞are computed from a weighted sum of the current vertex v0\nand n neighbors vj. Corner vertices stay fixed, that is v∞= v0. Smooth vertices are pro-\njected using \n. A regular crease uses weights \nwith v0,\n1\n6\n2\n3\n1\n6\n,\n,\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\nv\nb n n\nv\nn\nv j\nj\nn\n∞\n=\n=\n+\n+\n+ ∑\n3\n8\n1\n1\n1\n0\n1\n( )(\n)\n1\n8\n3\n4\n5.4\nHigh-Performance Subdivision Surfaces\n387\nFIGURE 5.4.2\nVertex mask.\n\n\ngetting . The two crease neighbors get weight , and the rest of the neighbors get\nweight 0. Similarly, non-regular creases use weights \n.\nVertex and Crease Normals\nTrue normals can be computed for each vertex, which should be done after comput-\ning limit positions for each final vertex. Surprisingly this is faster than computing\napproximate normals by averaging each adjacent face normal (partitioned to each side\nof a crease).\nComputing two true tangents and taking a cross product computes true normals\nat each vertex.\nFor a smooth or dart vertex, the two tangents are \nand\n.\nCrease and boundary vertices require more work. Normals are not defined per\nvertex for corners, and must be done for each face. Tangents need to be computed for\neach side of the crease. Along a crease (or boundary), one tangent is –1 times one\ncrease neighbor plus 1 times the other crease neighbor. The second tangent is more\ncomplicated to compute and is done as follows. Weights wj are computed for each ver-\ntex, with j = 0 being the vertex where a normal is desired. Then the other indices are\nnumbered j = 0,2...,n from one crease to another. The weights depend on the number\nof vertices and for each case are as follows:\nw0 = 0, w1 = wn = sin(z), wi = (2 cos(z) – 2)(sin(i–1)z),  \nfor n ≥5.\nThis creates over four subdivision levels from a tagged cube, with one face missing\nand marked as boundary, as in Figure 5.4.3. Notice that some corners have varying\nsemi-sharpness.\nFeature Implementation\nBesides geometry, a full solution needs colors, textures, and other per-vertex or per-face\ninformation.\nFace parameters like color and texture coordinates can be interpolated using the\nsame subdivision methods when new vertices are added. A simple method is to inter-\npolate by distance after the old vertices are modified, giving new values for the new\nfaces. Many features can be subdivided per vertex except at exceptional places, like\nalong an edge where texture coordinates form a seam. Some details for Catmull-Clark\nz\nn\n=\n−\nπ\n1\nw\nw w\nw w\n0\n1\n2\n3\n4\n2 1 2 2 1\n,\n,\n,\n,\n,\n, , ,\n(\n) = −−\n−\n(\n)\nw\nw w\nw\n0\n1\n2\n3\n1 0 10\n,\n,\n,\n, ,\n(\n) = −(\n)\nw\nw w\n0\n1\n2\n2 1 1\n,\n,\n, ,\n(\n) = −(\n)\nt\nv\nj\nn\nj\nj\nn\n2\n1\n2\n1\n=\n∗\n+\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n=∑\ncos\n(\n)\nπ\nt\nv\nj\nn\nj\nj\nn\n1\n1\n2\n=\n∗\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n=∑\ncos\nπ\n1\n5\n3\n5\n1\n5\n, ,\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n1\n6\n2\n3\n388\nSection 5\nGraphics \n\n\nsurfaces (but applicable to Loop surfaces) are in [DeRose98]. Basically, per-vertex\nparameters are interpolated like vertex coordinates, so adding (u,v) texture coordi-\nnates is as simple as treating vertex points as (x,y,z,u,v) coordinates. Per-vertex textures\ndon’t allow easy handling of seams, in which case per-face texture coordinates are use-\nful. However, all internal points on a subdivided face become per-vertex parameters.\nPossible features to add but not covered in this article for lack of space are adaptive\ntessellation (where only part of the mesh is subdivided as needed for curvature, such as\nwith silhouettes, clipping, and so on, and making sure cracks aren’t introduced), and\ndisplaced subdivision surfaces (which add geometry by using a “texture” map to offset\ngenerated vertices as they are computed). Adaptive tessellation is covered in [Bunnell05]\nand displaced subdivision surfaces are covered in [Bunnell05] and [Lee00]. \nCollision Detection\nIf prescribed normals are not implemented, the surface has the convex hull property;\nthat is, sits inside the convex hull of the bounding mesh. This can be used for coarse\n5.4\nHigh-Performance Subdivision Surfaces\n389\nFIGURE 5.4.3\nExample geometry.\n\n\ncollision detection. More accurate (and more expensive) collision between subdivision\nsurfaces is covered in [Wu04] using a novel “interval triangle” that tightly bounds the\nlimit surface. [Severn06] efficiently computes the intersection of two subdivision sur-\nfaces at arbitrary resolutions. Collision detection will not be covered here further.\nIn the next section, a data structure is presented that accommodates Loop subdivi-\nsion on a triangle mesh. An algorithm follows that performs one level of subdivision,\nreturning a new mesh. This structure supports most of the features described in this\ngem, and is extensible to many of the other features.\nSubdivision Data Structure\nThere are many approaches in the literature for data structures used to store and\nmanipulate subdivision surfaces including half-edge, winged-edge, hybrid, and grids\n[Müller00]. For Loop subdivision, the data structure should allow finding neighbor\nvertices and incident edges easily, and preserve this ability on each level of subdivision. \nThere are many factors when designing the data structure. Converting a mesh to\na Loop-subdivided mesh is the main goal, leading to certain structures, but other\ntimes the end purpose is GPU rendering, in which case optimizing data structures for\nthis use makes sense. The approach presented here is somewhat of a hybrid, resulting\nin a data structure that ports easily to a GPU. A later section covers performance\nissues when moving to a GPU.\nThe following data structure is easy to read/write from files or elsewhere, fast to\nuse internally, and does not use pointers. Avoiding pointers makes it easier to move to\nGPUs or languages not as pointer friendly as C/C++, and makes the memory foot-\nprint smaller than the previously mentioned schemes (useful for large mesh tools),\nbecause instead of storing connectivity information explicitly, it is deduced from\nindex positions. This structure also makes sending meshes to a GPU easier because\nitems are arranged into vertex arrays, normal arrays, and so on, using indices to render\npolygons.\nData Structure\nSee Figures 5.4.4 and 5.4.5 for insight. Extensions allow storing all the levels of sub-\ndivision for multi-resolution editing.\nThe mesh and supported features are stored in various arrays. Each array is \n0-based. There is one array for each of the following:\n• Vertices array VA—Each vertex is a three-tuple x, y, z of floats, and a float sharp-\nness weight 0 ≤w < ∞, with 0 being smooth, and a half-edge index vh of a half-\nedge ending on this vertex (for fast lookup later). If the vertex is a boundary, vh is\nthe boundary half-edge index ending on the vertex. Optional per-vertex color,\ntexture, or index to a normal can also be stored.\n• Faces array FA—Each face represents a triangle, stored as three indices v0,v1,v2\ninto the vertex array. Also stored are three indices n0,n1,n2 into the normal array\n390\nSection 5\nGraphics \n\n\nNA, corresponding to the three vertex indices. Each face can optionally store\ncolor, texture, and other rendering information, per face or per vertex as desired.\nFaces are oriented clockwise or counter-clockwise as desired, but all must be ori-\nented the same way.\n• Half-edge array HA—Each face has three (half) edges in the half-edge array,\nstored in the same order. Thus, a face with index f and (ordered) vertex indices\n{v0,v1,v2} has ordered half-edges with indices 3f, 3f+1, and 3f+2, denoting half-\nedges from vertex v0 to v1, v1 to v2, and v2 to v0, respectively. Note that half-edges\nare directed edges, with the two half-edges of a pair having opposite directions. A\nhalf-edge entry is two values: an integer marking the pair half-edge index or a –1\nif it’s a boundary, and a floating-point sharpness weight 0 ≤w < ∞ denoting the\ncrease value, 0 being smooth, and larger values denoting sharpness. Each match-\ning half-edge pair must have the same crease values to avoid ambiguity. Note that\na half-edge index determines the corresponding face index, which in turn deter-\nmines a start and end vertex for the directed half-edge.\n• Normals Array NA—Normals can be included in the scheme in numerous ways\nwith varying tradeoffs. In order to handle creases, boundaries, and semi-sharp\nfeatures cleanly, you need one normal per vertex per face, but for many vertices\n(for example, smooth and regular) only a single normal is needed. An array of\nnormals accommodate this; each has a unit vector and a weight 0 ≤w < ∞telling\nhow fast a vertex normal converges to a prescribed normal, with 0 meaning no\nprescribed normal. Normals are referenced by index, thus avoiding redundant\nstores.\nBesides storing the size of each array, the number of edges E (where a matching\npair of half-edges or a boundary edge constitutes a single edge) is stored. This is not\ntoo costly to compute if the mesh has no boundary (E=# half-edges/2 = #faces*3/2),\nand can be computed otherwise by scanning the half-edge array and setting E=(size of\nHA+# of boundary edges in HA)/2.\nInformation about vertex types (smooth, crease, and so on) may also be stored on\na vertex tag for speed. Other items may also be tagged, but the algorithm described\nhere needs to be modified to maintain the invariants across subdivisions.\nUnneeded features can be dropped, such as three normals per face, prescribed\nnormals, or semi-sharp creases, but this loses finer grained control.\nA well-formed mesh requires a few rules. If a half-edge is not paired (it is on a\nboundary), it has pair index –1, and must have infinite crease weight. Otherwise, the\nedge will shrink. Each half-edge of the same edge must have the same weight; other-\nwise the edges will subdivide differently, creating cracks.\nFile Format\nBased on the data structure described here, a file format is defined as an extension to\nthe popular text based Wavefront *.OBJ format. An entry is a line of text, starting\nwith a token denoting the line type followed by space-separated fields. Various pieces\n5.4\nHigh-Performance Subdivision Surfaces\n391\n\n\nof data are stored to speed up loading so all items such as paired edges do not need to\nbe recomputed each load. The format in order of file reading/writing is in Table 5.4.3.\nTable 5.4.3\nFile Format Entries\nEntry\nDescription\n#SubdivisionSurfL 0.1\nDenotes a non-standard OBJ file, versioned.\nsi v f e n\nOptional subdivision info giving number of vertices, faces, edges,\nand normals. Allows pre-allocation of arrays.\nv x y z \nOne entry per vertex with floating point position.\nf v1 v2 v3\nOne entry per face with one-based vertex indices, oriented.\nhd j wt \nHalf-edge data, one entry for each half-edge, in the order de-\nscribed by the faces, in half-edge order v0 →v1, v1 →v2, v2 →v0.\nEach entry is a one-based integer edge pair index j (or –1 for a\nboundary) and a floating-point weight wt.\nfc r1 g1 b1 a1 r2 g2 \nOptional face colors, one per vertex, RGBA, [0,1] floats. Not \nb2 a2 r3 g3 b3 a3\nallowed with per-vertex colors vc.\nvc r g b a\nOptional per-vertex color data, RGBA, [0,1] floats. Not allowed\nwith per-face colors fc.\nft u1 v1 u2 v2 u3 v3 texname\nOptional face textures with (u,v) floats in [0,1]. texname is\napplication dependent.\nfn nx ny nz w\nOptional normal data, with weights for prescribed normals. 0 is\ndefault weight.\nfni n1 n2 n3\nOptional face normal indices into the normal table, one normal\nper vertex. Requires fn entries.\nvs wt\nOptional vertex sharpness,[0,∞), with 0 being smooth and\ndefault; one per vertex.\nSubdivision Algorithm Details\nThis is an overview of the Loop subdivision algorithm. Let V = # old vertices, F = # old\nfaces, H = 3F = # of half-edges, and #E = number of edges = (H + # boundary edges)/2.\nOne level of subdivision consists of six steps:\n1. Compute new edge vertices.\n2. Update the original vertices.\n3. Split the faces.\n4. Create new half-edge information.\n5. Update the other features.\n6. Replace the arrays in the data structure with the new ones, and discard,\nstore, or free the old ones as desired.\nThese steps are described in detail in the following sections.\n392\nSection 5\nGraphics \n\n\nComputing New Edge Vertices\nFollow these steps to compute the new edge vertices:\n1. Because each existing vertex will soon be modified (and originals need to be\nkept around until all are done), and because new vertices are going to be\nadded per edge, you allocate an array NV for all new vertices of size (# old\nvertices + # edges). When creating new edge vertices, the first V positions in\nthe array are skipped so the original vertices can be placed back in the same\npositions as they are modified.\n2. Allocate an array EM (edge map) of integers of size (# half-edges) to store\nindices mapping half-edges to new vertex indices. Initialize all to –1 to indi-\ncate half-edges not yet mapped.\n3. For each half-edge h, if EM[h] = –1, insert a vertex on the edge using the\nedge split rules. Store the new vertex in an unused slot in NV past the orig-\ninal V, and store the resulting NV index in EM[h]. If h2=E[h] is not –1, h\nhas a paired half-edge h2, so store the NV index in EM[h2] also. \nUpdating the Original Vertices\nNow you must move each original vertex to a new position, placing the new vertex in\nthe new vertex array NV, in the same order and position as before to make splitting\nfaces easy. This is done using the vertex modification rules from before. During updat-\ning, reduce vertex weights by 1, clamping at 0. New vertices have weight 0. Each ver-\ntex stores a half-edge index vh ending on the vertex, which is used to quickly walk\nneighboring vertices and determine edge types, as shown in Figure 5.4.4. Given a half-\nedge index h ending at the vertex, the joined neighbor vertex is VA[FA[Floor[h/3]].ver-\ntexIndex[h mod 3]]. Given eA, the next half-edge of interest is found by eB = EA[eA] and \n. With this information, the edges and neighboring vertices\ncan be queried rapidly.\nThe reason for requiring a boundary vertex to be tagged with an incoming crease\nis so traversal only needs to go in one direction, thus making the code simpler.\nAfter all updates, change all vertices (new and old) to have a half-edge index of\n–1, which denotes no incoming matching half-edge. These will be filled in during the\nface splitting.\nSplitting the Faces\nEach old face will become four new faces, split as shown in Figure 5.4.5. Figure 5.4.5\nshows the original triangle with edge and face orientations, and how this maps to new\nedge and face orientations, along with the order (0, 1, 2, 3) in which the new faces are\nstored.\ne\nFloor e\ne\nC\nB\nB\n=\n⎡\n⎣\n⎢\n⎤\n⎦\n⎥+\n+\n(\n)\n(\n)\n3\n3\n2\n3\nmod\n5.4\nHigh-Performance Subdivision Surfaces\n393\n\n\n1. Allocate an array NF for new faces of size 4F.\n2. For each face f, with vertex indices v0,v1,v2, look up the three edge vertex\nindices as j0=NV[3f+V], j1=NV[3f+1+V], and j2=NV[3f+2+V]. \n3. Split the faces in the order shown in Figure 5.4.5. To NFm add faces\n{j2,v0,j0}, {j0,v1,j1}, {j1,v2,j2}, {j2,j0,j1} at positions 4f, 4f+1, 4f+2, and 4f+3.\nThis order is important! Each parent half-edge ek is conceptually split into\ntwo descendent half-edges ekA, followed by ekB.\n4. During the face split, tag each vertex (which still has a –1 tag from the pre-\nvious steps) with an incident half-edge index ending on the vertex, giving\npreference to an incoming boundary.\n394\nSection 5\nGraphics \nFIGURE 5.4.4\nVertex neighbors.\nFIGURE 5.4.5\nFace splitting.\n\n\nCreating the New Half-Edge Information\nThis step could be merged with the split-face step, but is separated for clarity. A new list\nof half-edges is needed, correctly paired and weighted. Create a new half-edge array NE\nof size 12*F (three per new face, each old face becomes four new faces, producing 12). \nDefine a function nIndex(j,type) to compute new half-edge pair indices, where\nj is the old half-edge index, and type is 0=A or 1=B, denoting which part of the new\nhalf-edge is being matched. This function is as follows:\nfunction nIndex( j, type)\n/* data table for index offsets - matches new half-edges */\noffsets[] = {3,1,6,4,0,7}\n/* original half-edge pair index */\nop = EV[ei]\nif (op == -1) \nreturn –1   /* boundary edge */\n/* new position of the split-edges for the face with pair op */\nbp = 12*Floor[op/3]\n/* return the matching new index */\nreturn bp + offsets[2*(op mod 3) + type]\nThe {3,1,6,4,0,7} array comes from matching half-edges to neighboring half-\nedges and is dependent on inserting items in arrays as indicated. For each original face\nindex f, do the following:\n1. Let b=12*f be the base half-edge index for a set of new half-edges, which will\nbe stored in NE at the 12 indices b through b+11.\n2. Store the 12 new half-edge pair indices at b,b+1,…,b+11 in the following\norder: {e2B,e0A,b+9,e0B,e1A,b+10,e1B,e2A,b+11,b+2,b+5,b+8}, where eiT is\nnIndex(ei,type) with ei being the edge index. type = 0 for T = A and type\n= 1 for T = B. These are grouped three per face in the order of faces created\nin Figure 5.4.5.\n3. In the previous 12 entries, update the half-edge weights, with descendent\nhalf-edges getting the parent half-edge weights –1, clamped at 0. New half-\nedges with no parent get weight 0.\nUpdating Other Features\nPer-vertex colors and per-vertex texture coordinates can be updated during the edge\nvertex creation and during the vertex re-positioning steps by simple interpolation. Per\nface per vertex colors and textures coordinates can be interpolated in the previous\nsteps also, or can be done as a final step.\nDisplaced subdivision surface modifications can also be applied here by modify-\ning the current vertex positions using a displacement map.\n5.4\nHigh-Performance Subdivision Surfaces\n395\n",
      "page_number": 413,
      "chapter_number": 44,
      "summary": "This chapter covers segment 44 (pages 413-428). Key topics include vertex, edge, and subdivision.",
      "keywords": [
        "Vertex",
        "Subdivision Surfaces",
        "Subdivision",
        "edge",
        "vertices",
        "Loop subdivision",
        "High-Performance Subdivision Surfaces",
        "Face",
        "half-edge",
        "Surfaces",
        "weight",
        "crease",
        "mesh",
        "normals",
        "half-edge index"
      ],
      "concepts": [
        "vertex",
        "edge",
        "subdivision",
        "subdivisions",
        "faces",
        "surfaces",
        "mesh",
        "meshes",
        "weight",
        "vertices"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 36,
          "title": "Segment 36 (pages 347-355)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 34,
          "title": "Segment 34 (pages 326-336)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 43,
          "title": "Segment 43 (pages 424-431)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 25,
          "title": "Segment 25 (pages 231-249)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 36,
          "title": "Segment 36 (pages 360-367)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 429-442)",
      "start_page": 429,
      "end_page": 442,
      "detection_method": "topic_boundary",
      "content": "If the surface is about to be rendered, temporary normals can be computed using\nstandard averaging techniques or by using the exact normals. Exact normals are more\nappropriate on a surface with vertices at limit points. Normals don’t usually need to be\ncomputed until render time.\nFinal Step\nThe final step is to replace the arrays in the data structure with the new ones, and dis-\ncard, store, or free the old ones as desired.\nFinally, if the mesh is not going to be subdivided further, vertices can be pushed\nto limit positions, and true normals can be computed. In a rendering engine where\nthe object is about to be drawn, this is an appropriate step.\nPerformance Issues\nThis section contains an overview of hardware rendering techniques for this algorithm.\nPerformance Enhancements\nThere are many places to improve the performance of the algorithm itself, especially if\nall the features are not needed. If all you need is a simple, smooth, closed mesh, you\ncan remove all the special cases, making subdivision very fast.\nConsider these implementation tips:\n• Use tables for the b(n) based weights, the tangent weights, normal weights, limit\nposition weights, as well as any other items. A given mesh has a maximum valence\nvertex and all new vertices have valence at most, which makes using tables feasible.\n• Make the half-edge array spaced out by four entries per face instead of three,\nallowing many divide by three and mod three operations to be replaced with\nshifts. This is the traditional space-for-speed tradeoff.\n• Most interior vertices will have valence 6 and be smooth, so make that code fast,\nwith special cases for the other situations. Most boundary vertices will be regular\nwith valence 4. Most edges will be weight 0 and connect valence 6 smooth vertices.\n• Tag edges and vertices for whether they are smooth or need special case code,\nallowing faster decisions, instead of determining vertex and edge types by walking\nneighbors. Once a vertex or edge type is determined it is easy to tag descendents.\n• Pre-compute one level of subdivision to isolate the special case vertices, and then\nat runtime use a simpler version of the algorithm since there are fewer cases. This\nis a minor speed improvement, and is used for some hardware implementations.\n• A pointer-based data structure like a half-edge structure can speed up subdivision\nat the cost of using more (and likely less contiguous) memory and making reading/\nwriting harder. It is not clear which is really faster until you do some tests.\n396\nSection 5\nGraphics \n\n\n• Move per-vertex per-face parameters to per-vertex when possible. For example,\ncreases require per-vertex per-face normals because neighboring faces require dif-\nferent normals along the crease, but once a face has been subdivided, the interior\nsmooth new vertices can (and should) use per-vertex normals.\n• Do multiple subdivision steps at once if desired, storing only the resulting triangles,\nand not updating all the connectivity info. This is detailed in the GPU section.\n• Implement adaptive subdivision. Having fewer triangles to split after a few steps\nwill speed things up a lot (but will break the simple algorithm operating on the\ndata structure presented).\nGPU Subdivision and Rendering\nI dropped my original plan to present a state-of-the art GPU subdivision renderer\nonce I reviewed the literature and learned how fast such articles become obsolete.\nInstead the focus here is putting in one place unified rules for a subdivision scheme.\nThis will assist future hardware and software implementations, making this gem use-\nful for a longer period.\nFor GPU rendering, the following is a chronological review of several papers,\nmost of which can be found on the Internet. The papers are roughly evenly divided\nbetween Catmull-Clark methods, Loop methods, and universal methods:\n• [Pulli96] presents an efficient Loop rendering method. It works by grouping tri-\nangles into pairs during a precomputation phase, effectively passing squares and a\n1-neighborhood to a rendering function, which then renders the two triangles to\nan arbitrary subdivision depth.\n• [Bischoff00] presents a very memory efficient and fast Loop rendering solution.\nThe main concept is using multivariate forward differencing to generate triangles\nseveral subdivision levels deep without having to generate the intermediate levels.\nRendering is done patch by patch.\n• [Müller00] presents an extension to [Pulli96], and details a triangle paring algo-\nrithm and a sliding window method. Details are also presented for adaptive sub-\ndivision and crack prevention.\n• [Leeson02] covers a few subdivision methods, and gives an overview of some ren-\ndering tips such as hierarchical backface culling.\n• [Bolz02] implements Catmull-Clark subdivision, using a static array to hold the\nresults. The methods are good for SIMD implementation.\n• [Bolz03] implements Catmull-Clark subdivision on a GPU, with special attention\ngiven to avoiding cracks and pixel dropout caused through floating point errors.\n• [Boubekeur05] presents a general method useful for rendering many types of\nsubdivision surfaces. The main idea is to implement a “refinement pattern” on\nthe GPU. Each triangle or other primitive passed to the GPU is then refined\nusing the pattern. \n• [Bunnell05] and [Shiue05] both implement Catmull-Clark subdivision on a\nGPU, with ample details. \n5.4\nHigh-Performance Subdivision Surfaces\n397\n\n\nFast Subdivision Surface Rendering\nA fast subdivision routine suited for a GPU is based on the following observation. For\neach triangle, upon subdividing, the new items (vertices, edges, faces, colors, and so\non) are a linear combination of a 1-neighborhood of the triangle. Second subdivision\nitems are then linear combinations of first subdivided items, hence a linear combina-\ntion of the original neighborhood. This is exploited in various ways in the preceding\nreferences, and will be explained in a simple case.\nA patch is single triangle T and the surrounding triangles (those that influence\ndescendent triangles from the triangle T). See Figure 5.4.6 for a patch illustration—\non the left, T is shaded and a 1-neighborhood is included. The right side shows T sub-\ndivided once, with a new 1-neighborhood (without all edge lines drawn). \n398\nSection 5\nGraphics \nFIGURE 5.4.6\nSubdividing a patch.\nAssume for the moment there are no creases or boundaries (which can be added\nback in later). All the subdivision levels beneath T can be generated from linear com-\nbinations of existing vertices, so for each level of desired subdivision a mask can be\ncomputed in terms of neighboring vertices that outputs all the triangles descended\nfrom T, without needing to compute intermediate levels. Connectivity information does\nnot need to be computed or stored either—all that is desired are the vertices of the faces,\nwhich naturally fall into a grid arrangement and are suitable for GPU rendering.\nMesh precomputation gathers needed data for each patch, stored per triangle. At\nrender time, a subdivision level is selected, and each patch is passed to a GPU kernel.\nThe GPU kernel then takes the low-resolution triangle, creates subdivided triangles in\none pass, and renders the resulting triangles. In order to incorporate all the features\nfrom the gem, different kernels should be implemented. Alternatively preprocessing\ncould simplify the numbers of cases, resulting in fewer GPU kernel variations.\nA final point is this method might result in pixel dropout or cracks, because\nneighboring triangles may be evaluated using floating point operations in different\norders. This is addressed in [Bolz03] for Catmull-Clark surfaces.\n\n\nConclusion\nThis article showed details of how to implement Loop subdivision surfaces with addi-\ntional features and provides a starting point for the literature on subdivision surfaces.\nGeometry features such as creases, boundaries, semi-sharp items, and normals were\ncovered, as well as surface tags like colors and textures. Future directions would be to\nadd displaced subdivision surfaces and adaptive subdivision to the algorithm.\nReferences\n[Biermann06] Biermann, Henning, Levin, Adi, and Zorin, Denis. “Piecewise Smooth\nSubdivision Surfaces with Normal Control,” Proceedings of the 27th Annual Con-\nference on Computer Graphics and Interactive Techniques, pp. 113–120, 2006. \n[Bischoff00] Bischoff, Stephan, Kobbelt, Leif, and Seidel, Hans-Peter. “Towards\nHardware Implementation of Loop Subdivision,” Eurographics SIGGRAPH\nGraphics Hardware Workshop, 2000 Proceedings. \n[Bolz02] Bolz, Jeffery, and Schröder, Peter. “Rapid Evaluation of Catmull-Clark \nSubdivision Surfaces,” Web3d 2002 Symposium, available online at http://\nwww.multires.caltech.edu/pubs/fastsubd.pdf.\n[Bolz03] Bolz, Jeffery, and Schröder, Peter. “Evaluation of Subdivision Surfaces on\nProgrammable Graphics hardware,” available online at http://www.multires.cal-\ntech.edu/pubs/GPUSubD.pdf.\n[Boubekeur05] Boubekeur, Tamy, and Schlick, Christophe. “Generic Mesh Refine-\nment on GPU,” ACM SIGGRAPH/Eurographics Graphics Hardware, 2005.\n[Bunnell05] Bunnell, Michael. “Adaptive Tesselation of Subdivision Surfaces with\nDisplacement Mapping,” GPU Gems 2, 2005, pp. 109–122.\n[Catmull78] Catmull, E., and Clark, J. “Recursively Generated B-Spline Surfaces on\nArbitrary Topological Meshes,” Computer Aided Design 10, 6(1978), pp. 350–355.\n[DeRose98] DeRose, Tony, Kass, Michael, and Truong, Tien. “Subdivision Surfaces\nin Character Animation,” International Conference on Computer Graphics and\nInteractive Techniques, SIGGRAPH 1998, pp. 85–94. \n[Hoppe94a] Hoppe, Huges. “Surface Reconstruction from Unorganized Points,”\nPhD Thesis, University of Washington, 1994, http://research.microsoft.com/\n~hoppe/.\n[Hoppe94b] Hoppe, Huges, DeRose, Tony, DuChamp, Tom, et. al. “Piecewise\nSmooth Surface Reconstruction,” Computer Graphics, SIGGRAPH 94 Proceed-\nings, 1994, pp. 295–302.\n[Lee98] Lee, Aaron, Sweldens, Win, et. al. “MAPS: Multiresolution Adaptive Para-\nmeterization of Surfaces,” Proceedings of SIGGRAPH 1998. \n[Lee00] Lee, Aaron, Moreton, Henry, and Hoppe, Huges. “Displaced Subdivision\nSurfaces,” SIGGRAPH 2000, pp. 95–94.\n[Leeson02] Leeson, William. “Subdivision Surfaces for Character Animation,” Game\nProgramming Gems 3, 2003, pp. 372–383.\n5.4\nHigh-Performance Subdivision Surfaces\n399\n\n\n[Levin99] Levin, Adi. “Interpolating Nets of Curves by Smooth Subdivision Sur-\nfaces,” Proceedings of SIGGRAPH 99, Computer Graphics Proceedings, Annual\nConference Series, 1999.\n[Loop87] Loop, Charles. “Smooth Subdivision Surfaces Based on Triangles,” Master’s\nThesis, University of Utah, Dept. of Mathematics, 1987, available online at\nhttp://research.microsoft.com/~cloop/thesis.pdf. \n[Müller00] Müeller, Kerstin, and Havemann, Sven. “Subdivision Surface Tesselation\non the Fly Using a Versatile Mesh Data Structure,” Comput. Graph. Forum, Vol.\n19, No. 3, 2000, available online at http://citeseer.ist.psu.edu/. \n[Pulli96] Pulli, Kari, and Segal, Mark. “Fast Rendering of Subdivision Surfaces,” Pro-\nceedings of 7th Eurographics Workshop on Rendering, pp. 61–70, 282, Porto,\nPortugal, June 1996. \n[Schweitzer96] Schweitzer, J.E. “Analysis and Application of Subdivision Surfaces,”\nPhD Thesis, University of Washington, Seattle, 1996, available online at\nhttp://citeseer.ist.psu.edu/.\n[Severn06] Severn, Aaron, and Samavati, Faramarz. “Fast Intersections for Subdivi-\nsion Surfaces,” In International Conference on Computational Science and its\nApplications, 2006. \n[Shiue05] Shiue, L.J., Jones, Ian, and Peters, Jörg. “A Realtime GPU Subdivision Ker-\nnel,” ACM SIGGRAPH Computer Graphics Proceedings, 2005. \n[Stam99] Stam, Jos. “Evaluation of Loop Subdivision Surfaces,” SIGGRAPH 99\nCourse Notes, 1999, http://www.dgp.toronto.edu/people/stam/. \n[Vlachos00] Vlachos, Alex, Peters, Jörg, Boyd, Chas, and Mitchell, Jason. “Curved\nPN Triangles,” ID3G 2001, http://www.cise.ufl.edu/research/SurfLab/papers/. \n[Warren95] Warren, J. “Subdivision Methods for Geometric Design,” Unpublished\nmanuscript, November 1995.\n[Wu04] Wu, Xiaobin, and Jörg Peters, “Interference Detection for Subdivision Sur-\nfaces,” EUROGRAPHICS, 2004. Vol. 23, 3. \n[Zorin96] Zorin, Denis, Schröder, Peter, and Sweldens, Wim. “Interpolating Subdivi-\nsion for Meshes with Arbitrary Topology,” Proceedings of SIGGRAPH 1996,\nACM SIGGRAPH, 1996, pp. 189–192. \n[Zorin97] Zorin, Denis, Peter Schröder, and Wim Sweldens. “Interactive Multi-\nResolution Mesh Editing,” CS-TR-97-06, Department of Computer Science,\nCaltech, January 1997, available online at http://graphics.stanford.edu/~dzorin/\nmultires/meshed/. \n[Zorin00] Zorin, Denis and Schröder, Peter. “Subdivision for Modeling and Anima-\ntion,” Technical Report, ACM SIGGRAPH Course Notes 2000, available online\nat http://mrl.nyu.edu/~dzorin/sig00course/. \n400\nSection 5\nGraphics \n\n\n401\n5.5\nAnimating Relief Impostors\nUsing Radial Basis Functions\nTextures\nVitor Fernando Pamplona, \nInstituto de Informática: UFRGS\nvfpamplona@inf.ufrgs.br\nManuel M. Oliveira, \nInstituto de Informática: UFRGS\noliveira@inf.ufrgs.br\nLuciana Porcher Nedel, \nInstituto de Informática: UFRGS\nnedel@inf.ufrgs.br\nG\names often use simplified representations of scene elements in order to achieve\nreal-time performance. For instance, simple polygonal models extended with\nnormal maps and carefully crafted textures are used to produce impressive scenarios\n[IdSoftware], while billboards and impostors replace distant objects. More recently,\nrelief textures [Oliveira00] (textures containing depth and normal data on a per-texel\nbasis) have been used to create impostors of detailed 3D objects using single quadri-\nlaterals and preserving self-occlusions, self-shadowing, view-motion parallax, and\nobject silhouettes [Policarpo06].\nRelief rendering simulates the appearance of geometric surface detail by using the\ndepth and surface normal information to shade individual fragments. This is obtained\nby performing ray-height-field intersection in 2D texture space, entirely on the GPU\n[Policarpo05]. The mapping of relief details to a polygonal model is done in the con-\nventional way, by assigning a pair of texture coordinates to each vertex of the model.\n\n\nRelief impostors are obtained by mapping relief textures containing multiple layers of\ndepth, normals, and color data per texel onto quadrilaterals [Policarpo06]. \nIntroduction\nTextures in general can be used to represent both static and animated objects, and\ntexture-based animation traditionally uses techniques such as image warping or a set\nof static textures cyclically mapped onto some polygons. Although conventional\nimage warping techniques are limited to some planar deformations, the second\napproach requires as many textures as frames in the animation sequence, which, in\nturn, tends to need a significant amount of artwork. \nThis gem describes a new technique for animating relief impostors based on a sin-\ngle multilayer relief texture using radial basis functions (RBF). The technique preserves\nthe relief-impostor properties, allowing the viewer to observe changes in occlusion and\nparallax during the animation. This is illustrated in Figure 5.5.1, which shows three\nframes of a dog walking animation sequence created from a dog relief impostor. Note\nthe changes in the positions of the dog’s legs. \n402\nSection 5\nGraphics \nFIGURE 5.5.1\nThree frames of a dog walking animation created by warping a relief impos-\ntor. Note the changes in the positions of the legs.\nIn order to produce these animations, during a pre-processing step, the user spec-\nifies a set of control points over the texture of the relief impostor. Moving the control\npoints in 2D warps the texture, thus bringing the represented objects into new poses.\nSuch poses are the key poses to be interpolated during the animation. Note that these\nposes are only implicitly represented by the control points and by a single texture.\nThis situation is illustrated in Figure 5.5.2.\nAs part of the pre-processing, the algorithm also interpolates the positions of\nthese control points for the desired number of frames in the animation and, for each\nof them, solves a linear system to obtain a set of RBF coefficients. The control points\nand their corresponding RBF coefficients define a series of warping functions that\nproduce the actual animation. For efficiency reasons, these control points and coeffi-\ncients are stored in a texture (usually 16 \u0002 16 or 32 \u0002 32 texels). At runtime, this\ndata is used to recreate the animation on the GPU. \n\n\nThe proposed technique can be used to animate essentially any kind of texture-\nbased representations, such as relief textures [Oliveira00], billboards with normal\nmapping, and displacement maps [Cook84]. Note that it is also possible to replace\nthe RBFs with any other method that describes the desired transformation and that\ncan be evaluated on a GPU. The proposed technique produces real-time realistic ani-\nmations of live and moving objects undergoing repetitive motions.\n5.5\nAnimating Relief Impostors Using Radial Basis Functions Textures\n403\nFIGURE 5.5.2\nControl points (dark dots) placed over the texture of the relief impostor (top\nrow) warp the texture, changing the pose of the rendered dog (bottom row). All poses are\nimplicitly represented by a single texture and the sets of control points. \nImage Warping\nWarping-based texture animation evaluates a function over the source image in order\nto compute each frame of the sequence. Given a source image, a warping function\nproduces an output image by computing new coordinates for each source pixel. Image\nwarping then comprises two steps:\n• A mapping stage that associates source and target pixels’ coordinates.\n• A re-sampling stage.\nThe mapping is usually computed using a global analytic function built from a\nset of correspondences involving control points in the source and the target images.\nMany techniques, such as triangulation based, inverse-distance weighted interpola-\ntion, radial basis functions, and locally bounded radial basis functions, are available to\ngenerate the mapping function from a set of corresponding points [Ruprecht95]. \n\n\nRadial Basis Functions\nRadial basis function (RBFs) methods are a mathematical way to produce multivari-\nate approximation and one of the most popular choices when interpolating scattered\ndata [Buhmann03]. In computer graphics, RBFs have been used for surface recon-\nstruction from point clouds [Carr01], for image warping [Ruprecht95], and for ani-\nmation [Noh00]. An RBF is defined in Equation 5.5.1: \n(5.5.1)\nHere, N is the number of centers, φ is a basis function, λi is the i-th coefficient for\nthe RBF representation, ci is the i-th center, and x is a point for which the function\nwill be evaluated. In the case of image warping, ci represents the pixel coordinates of\nthe control points, and x represents the pixel coordinates of any pixel in the image. In\nthis case, a good choice of φ is the multiquadrics, originally proposed by Hardy\n[Hardy71]: \n(5.5.2)\nwhere di = \nand r is a positive arbitrary characteristic radius that can be a con-\nstant or a different value per control point. In Equation 5.5.2, r represents the\nsmoothness of the interpolation and is critical for good image warping results. In our\nexperiments, we used r = 0.5, as suggested in [Ruprecht95].\nThe warping problem can be modeled using the linear system shown in Equation\n5.5.3, where φij is the distance between control points ci and cj expressed in pixel coor-\ndinates, fkx and fky are, respectively, the x and y image coordinates of control point ck.\nλkx and λky are the RBF coefficients that you want to solve for. Once such coefficients\nhave been obtained, you can use RBFs as warping functions.\n(5.5.3)\nInterpolating the Warping Functions\nGiven two sets of control points St and St+k specified by the user for two key poses at\ntimes t and (t+k), respectively, the RBF coefficients for the intermediate poses are\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nφ\n11\n12\n13\n1\n21\n22\n23\n2\n31\n32\n33\n...\n...\n...\nm\nm\n3\n1\n2\n3\nm\nn\nn\nn\nnm\n...\n...\n...\n...\n...\n...\nφ\nφ\nφ\nφ\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\nλ\nλ\nλ\nλ\nλ\nλ\nλ\nλ\n1\n2\n3\n1\n2\n3\nx\nx\nx\nnx\ny\ny\ny\nny\n... ...\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥\n=\nf\nf\nf\nf\nf\nf\nf\nf\nx\nx\nx\nnx\ny\ny\ny\nn\n1\n2\n3\n1\n2\n3\n... ...\ny\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥\nx ci\n−\nφ( )\nd\nr\nd\ni\ni\n=\n+\n2\n2\nf x\nx c\ni\ni\ni\nN\n( ) =\n−\n(\n)\n=∑λ φ\n1\n404\nSection 5\nGraphics \n\n\nobtained by interpolating the coordinates of the corresponding pairs of control points\nin St and St+k, and solving Equation 5.5.3 for the interpolated λs. To achieve some\nsmooth interpolation, we used a cubic Hermite spline, where the end points of the\ntangents are given by the vector 0.5(St + St+k). This is illustrated in Figure 5.5.3.\nWhen using normal maps, the same warping approach has to be applied to the nor-\nmal map as well. Thus, both textures must be evaluated using the same RBF for each\nframe.\n5.5\nAnimating Relief Impostors Using Radial Basis Functions Textures\n405\nFIGURE 5.5.3\nThe light gray frames between time\n0.0 and 0.5 used a cubic Hermite spline to interpolate\nthe control point. \nEvaluating the Warping Function Using Shaders\nModern GPUs can execute programs called shaders. As the warping function needs to\nbe executed for each texel, it is clear that the RBF should be evaluated on a fragment\nshader. But for this, it is necessary to invert the warping functions because, given a\nfragment f, you must be able to obtain the texture coordinates that were mapped to f\nunder the warping transformation. Fortunately, inverting the warping function using\nEquation 5.5.3 only requires two steps:\n• Compute φij using the coordinates of the control points of the current (desired)\npose.\n• Use the x and y coordinates of the unmodified (before moving) control points as\nfkx and fky.\nFor the example shown in Figure 5.5.2, the RBF coefficients used for rendering\nthe image in the bottom center were computed as follows: φij are the distances\nbetween the control points ci and cj shown in the top center, whereas fkx and fky are the\ncoordinates of the k-th control point shown in the top left. Note that the re-sampling\nneeded as the second step of an image warping operation is provided for free by the\ntexture filtering hardware. \n\n\nAs previously mentioned, you store the RBF data (coordinates of the control\npoints and lambda values) into a texture for access by the shader during runtime. The\nj-th row of this a texture represents the j-th frame of the animation. The RGBA chan-\nnels of the i-th texel store the (x,y) coordinates as well as the λix and λiy coefficients of\nci, respectively (see Figure 5.5.4). We used a float32 non-normalized texture, because\nthe values of λ may not be in [0,1] range.\n406\nSection 5\nGraphics \nFIGURE 5.5.4\nThe animation data is stored in single texture. Each row of\nthe texture represents a frame of the animation. Along a row, the i-th texel\nstores the (x,y) coordinates of the control point ci as well as its λix and λiy\ncoefficients.\nThe shader for evaluating the RBF-based warping function is shown next. It maps\ntexture coordinates of the current fragment (obtained after rendering a texture-mapped\nquadrilateral) into texture coordinates on the original texture. \nListing 5.5.1\nEvaluating the RBF-Based Warping Function\n// Computes the Phi function.\nfloat multiquadric(float r, float h) {\nreturn sqrt(r*r+h*h);\n}\n// Evaluates the RBF for texCoord with a pre-defined number\n// of control points, the actual time and smoothness. \nfloat2 evaluateRBF(float2 i_texCoord, float points, float keyTime,\nfloat smoothness, samplerRECT rbfTexture) {\nfloat2 newTexCoord;\nnewTexCoord.xy = float2(0.0, 0.0);\nfor (int i=0; i<points; i++) {\nfloat2 access = float2(i, keyTime);\nfloat4 rbf = texRECT(rbfTexture, access);\n\n\nfloat distance = sqrt((pow(rbf.x - i_texCoord.x,2)) \n+ (pow(rbf.y - i_texCoord.y,2)));\nfloat temp = multiquadric(distance, smoothness);\nnewTexCoord.xy += temp * rbf.zw;\n}\nreturn newTexCoord;\n}\nAnimating Relief Maps\nYou can produce RBF-based animations of relief maps by adding a couple of extra\nlines to a relief mapping pixel shader [Policarpo05]. Just before the call to the linear\nsearch, you should clamp the original texture coordinates to the [0,1] range. This is\nrequired if the entire relief map covers only part of the polygon. In this case, the tex-\nture coordinates for some fragments will be out of the [0,1] range needed for the RBF\nevaluation. This clamping does not hurt the animation because there is no depth or\nnormal information outside the region not covered by the texture. You then need to\nadd the code in Listing 5.5.2 to a relief mapping shader, immediately before calling\nthe linear search. \nListing 5.5.2\nActions Required for Relief Warping That Need to Be Executed Before\nCalling the Linear Search\n// s is the texture coordinate used in the relief mapping shader\nfloat2 sZeroOne = clamp (s.xy, 0.0, 1.0);\n// Evaluating RBFs\nfloat2 sRBFEval = evaluateRBF(sZeroOne.xy, points, keyTime,\nsmoothness, rbfTexture);\n// Compensating the clamp. \ns.xy += sRBFEval - sZeroOne;\n... // Call linear search. \nAnimating Relief Impostors\nRelief impostors [Policarpo06] are rendered using multilayer relief representations.\nFigure 5.5.5 (right) shows a dog impostor modeled as a quad-layer relief texture,\nwhose depth values are shown on the left. For the case of relief impostors, the warping\nstrategy described earlier will cause all layers to be subject to the same warping func-\ntion and, consequently, undergo the same motion. Thus, although a single warping\nfunction can be used to animate a running dog, it would not produce a convincing\ndog motion. In this case, for instance, the two front legs would always move together\ninstead of moving in opposite directions.\n5.5\nAnimating Relief Impostors Using Radial Basis Functions Textures\n407\n\n\nThus, for multilayer relief representations, you might want to animate each indi-\nvidual layer independently. A walking dog motion is illustrated in Figure 5.5.1. In this\nexample, however, the animation was created using a single warping function by\nexploiting the symmetry of the walking motion of bipeds and quadrupeds—for each\nframe f at time t, the first two layers were rendered using time t, while the last two lay-\ners were rendered using time (1–t). Thus, while the right front (back) leg is moving\nforward, the left front (back) leg is moving backward. t is used in the evaluation of the\nfunction evaluateRBF as the parameter keyTime in the code fragment shown in List-\ning 5.5.2.\nIn this case, the linear and binary search calls in Listing 5.5.3 receive two new para-\nmeters: sFront and sBack. These parameters represent the warped texture coordinates\nfor the front and back layers, respectively. These coordinates are used to sample both the\ndepth and normal maps from different layers. This is illustrated in Listing 5.5.4 for the\ncase of the x-component of the normal map, where the retrieved values are combined in\na single RGBA variable (normal_x). (The x and y components of the normal map are\nstored in separate textures, normal_map_x and normal_map_y, respectively. The z compo-\nnent is computed on the fly from the other two components [Policarpo06].)\n408\nSection 5\nGraphics \nFIGURE 5.5.5\nA dog impostor modeled as a quad-layer relief texture.\nThe depth values of the progressing layers are stored in the R, G, B, and\nA channels, respectively (left). A view of the rendered dog impostor is\nshown on the right. See Color Plate 8 for a color version of this image.\nListing 5.5.3\nUsing a Single Warping Function to Produce the Walking Motion Shown\nin Figure 5.5.1\nfloat2 sZeroOne = clamp (s.xy, 0.0, 1.0);\nsFront = evaluateRBF(sZeroOne, points, keyTime, \nsmoothness, rbfTexture);\nsFront = s.xy + (sFront - sZeroOne);\nint keyTimeBack = (int)(keyTime+maxKeyTime/2) % (int)maxKeyTime;\n\n\nsBack  = evaluateRBF(sZeroOne.xy, points, keyTimeBack, \nsmoothness, rbfTexture);\nsBack  = s.xy + (sBack - sZeroOne);\n... // Call the linear search with sBack and sFront. \nListing 5.5.4\nSampling the Multilayer x-Component of the Normal at Two Positions,\nUsing Texture Coordinates sFront and sBack\nfloat4 normal_x;\nnormal_x.xy=tex2D(normal_map_x,sFront.xy).xy;\nnormal_x.zw=tex2D(normal_map_x,sBack.xy).zw;\nA similar operation is performed for the y-component of the normal.\nThe following code fragment uses sFront and sBack to sample the color texture. \nListing 5.5.5\nSampling the Color Texture Using Texture Coordinates sFront and sBack\nand Checking the Relative Position of the Viewing Ray with Respect to Several Layers\n// get color at intersection\nfloat4 c;\nfloat4 cFront = tex2D(texture,sFront.xy);\nfloat4 cBack  = tex2D(texture,sBack.xy);\nfloat4 z=abs(s.z-q); // q is the quad-depth value joined. \nfloat zt=z.x;\nc = cFront;           // hits the first layer.\nif (z.y<zt) c=cFront; // hits the second layer.\nif (z.z<zt) c=cBack;  // hits the third layer.\nif (z.w<zt) c=cBack;  // hits the fourth layer.\nResults\nWe have implemented the described algorithms using C++ and Cg and used them to\nanimate several textures and relief impostors. In all our experiments, the textures had\n400 \u0002 400 texels. On a 2.21GHz PC with 2.0GB of memory and an NVIDIA\nGeForce 8800 GTX with 768MB, our implementation achieves 3,000fps, 710fps,\nand 500fps, when rendering animations of textures with normal maps, relief maps,\nand relief impostors, respectively. \nFigure 5.5.6 depicts the control points (small dark dots) used to define the walk-\ning dog animation shown in Figure 5.5.1. The user defined a set of control points\npositioned on top of the dog image (left). Some of these points were then interactively\nmoved defining the configurations shown in Figure 5.5.6 (center) and (right). As the\nuser moves a control point, the underlying texture is automatically warped, providing\nimmediate visual feedback that allows the user to plan and define the animation (see\nFigure 5.5.2). \n5.5\nAnimating Relief Impostors Using Radial Basis Functions Textures\n409\n",
      "page_number": 429,
      "chapter_number": 45,
      "summary": "This chapter covers segment 45 (pages 429-442). Key topics include textures, subdivision, and points. Final Step\nThe final step is to replace the arrays in the data structure with the new ones, and dis-\ncard, store, or free the old ones as desired.",
      "keywords": [
        "subdivision surfaces",
        "subdivision",
        "Radial Basis Functions",
        "control points",
        "Loop subdivision surfaces",
        "Basis Functions Textures",
        "Relief Impostors",
        "texture",
        "Relief",
        "warping function",
        "surfaces",
        "Animating Relief Impostors",
        "points",
        "GPU Subdivision",
        "Subdivision Surface Rendering"
      ],
      "concepts": [
        "textures",
        "subdivision",
        "points",
        "float",
        "animation",
        "animating",
        "animations",
        "animate",
        "surface",
        "warps"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 48,
          "title": "Segment 48 (pages 469-476)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 48,
          "title": "Segment 48 (pages 456-465)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 62,
          "title": "Segment 62 (pages 597-603)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 43,
          "title": "Segment 43 (pages 424-431)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 5,
          "title": "Segment 5 (pages 41-49)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 443-452)",
      "start_page": 443,
      "end_page": 452,
      "detection_method": "topic_boundary",
      "content": "Figure 5.5.7 shows a few frames of a horse animation. On the left is the original\nrelief impostor. The images to its right show different poses, seen from the same view-\npoint, obtained with RBF-based warping functions. The accompanying video on the\nCD-ROM shows these animations. \n410\nSection 5\nGraphics \nFIGURE 5.5.6\nControl points (dark dots) used to define the RBF-based warping functions\nused to create the dog walking animation illustrated in Figure 5.5.1. Besides these 12 control\npoints, four extra control points were positioned at the corners of the texture to anchor it.\nFIGURE 5.5.7\nHorse animation. The image on the left shows a view of the original relief\nimpostor. The three images to its right show frames from an animation seen from the same\nviewpoint. Note the changes on the horse’s body and tail. A total of 27 control points were\nused to produce this animation, including 4 anchors at the corners of the texture.\nConclusion\nThis gem presented a technique for animating relief impostors in real-time using RBF-\nbased warping functions. This approach produces realistic animations of live and\nmoving objects undergoing repetitive motions. Given its generality, it can be used to\nanimate essentially any kind of texture representation. During a pre-processing stage,\nthe user specifies a set of control points, which are the centers for an RBF representa-\ntion. By moving such control points around in 2D, the user obtains immediate feed-\nback on the resulting animation. Once the key deformations have been specified, the\n\n\nsystem interpolates the control points for intermediate frames and solves the linear\nsystem defined by Equation 5.5.3 to find a set of RBF coefficients (λs), which are saved\ninto a texture with the 2D coordinates of the control points. The stored information is\nthen read during runtime by a shader that performs the actual animation via texture\nresampling. \nOur technique can be used to define separate warping functions to individual layers\nof a relief texture. As a result, it supports the definition of complex animations using a\nsimple interface, thus reducing the amount of time and artwork usually associated with\ntexture animation. As any other technique, this approach has some limitations: large\ndeformations tend to distort the texture too much, leading to poor results. Also, the use\nof the clamping function shown in Listings 5.5.2 and 5.5.3 may introduce some arti-\nfacts when the polygon used to render the impostor is seen at a grazing angle. Under\nsuch viewing configurations, these artifacts can be avoided by calling the function \nevaluateRBF at each step of both the linear and binary searches, at the cost of some\nperformance penalty.\nThe accompanying CD-ROM contains a video and a demo (including source\ncode and shaders) for animating normal maps and multi-layer relief maps. \nAcknowledgements\nWe would like to thank NVIDIA for donating the GeForce 8800 GTX video card\nused in this work.\nReferences\n[Buhmann03] Buhmann, Martin. Radial Basis Functions, Cambridge University Press,\n2003.\n[Carr01] Carr, Jonathan et al. “Reconstruction and Representation of 3D Objects\nwith Radial Basis Functions,” Proceedings of SIGGRAPH 2001, ACM Press,\nNew York, NY, pp. 67–76.\n[Cook84] Cook, Robert L. “Shade Trees,” In Computer Graphics (SIGGRAPH 84)\n18(3), pp. 223–231, 1984.\n[Donnelly05] Donnelly, William. “Per-Pixel Displacement Mapping with Distance\nFunctions,” GPU Gems 2, 2005.\n[Hardy71] Hardy, Roland L. “Multiquadric Equations of Topography and Other\nIrregular Surfaces,” J. Geophys. Res., 1971, Vol. 73, pp. 1905–1915.\n[IdSoftware] Id Software. DOOM 3, available online at http://www.idsoftware.\ncom/games/doom/doom3/.\n[Litwinowicz94] Litwinowicz, Peter, and Williams, Lance. “Animating Images with\nDrawings,” Proceedings of the 21st Annual Conference on Computer Graphics\nand Interactive Techniques SIGGRAPH, 1994, ACM Press, New York, NY, pp.\n409–412.\n5.5\nAnimating Relief Impostors Using Radial Basis Functions Textures\n411\n\n\n[Noh00] Noh, Jun-yong, et al. “Animated Deformations with Radial Basis Func-\ntions,” Proceedings of the ACM Symposium on Virtual Reality Software and\nTechnology, Seoul, Korea, October 22–25, 2000, VRST ’00. ACM Press, New\nYork, NY, pp. 166–174. \n[Oliveira00] Oliveira, Manuel M., Bishop, Gary, and McAllister, David. “Relief\nTexture Mapping,” Proceedings of  SIGGRAPH 2000, New Orleans, LA, July\n23–28, 2000, pp. 359–368.\n[Policarpo05] Policarpo, Fabio, Oliveira, Manuel M., and Comba, João. “Real-Time\nRelief Mapping on Arbitrary Polygonal Surfaces,” ACM SIGGRAPH, 2005,\nSymposium on Interactive 3D Graphics and Games, Washington, DC, April\n3–6, 2005, pp. 155–162.\n[Policarpo06] Policarpo, Fabio, and Oliveira, Manuel M. “Relief Mapping of \nNon-Height-Field Surface Details,” ACM SIGGRAPH 2006 Symposium on\nInteractive 3D Graphics and Games, Redwood City, CA, March 14–17, 2006,\npp. 55–62. \n[Policarpo06b] Policarpo, Fabio, and Oliveira, Manuel M. “Rendering Surface\nDetails in Games with Relief Mapping Using a Minimally Invasive Approach,” In\nWolfgang Engel (Ed.), SHADER X4: Lighting & Rendering. Charles River Media,\nInc., Hingham, Massachusetts, 2006, pp. 109–119.\n[Ruprecht95] Ruprecht, Detlef, and Müller, Heinrich. “Image Warping with Scat-\ntered Data Interpolation,” IEEE Computer Graphics and Applications, Vol. 15,\nNo. 2, 1995, pp. 37–43.\n412\nSection 5\nGraphics \n\n\n413\n5.6\nClipmapping on SM1.1 \nand Higher\nBen Garney\nGarageGames\nC\nlipmaps are a fast and robust technique for texturing terrains. This gem provides\na brief introduction to the theory behind clipmaps, and discusses their imple-\nmentation on Shader Model 2.0 hardware. Finally, it explores some advanced topics,\nsuch as support on fixed function, SM1.x, and SM3.0+ hardware, as well as different\nsources for image data.\nBasic Concepts of Clipmaps\nWhen rendering to a 1024 \u0002 768, 32bpp display, only 786,432 pieces of color infor-\nmation are necessary at any given moment to give a fully detailed, unique view. This is\nexactly 3MB of data. If you want to run at 60Hz, you need to transfer only 180MB/sec\nto the display, which is well within the capabilities of most game platforms.\nSuppose you draw a model on-screen. Regardless of how much detail its texture\nmight contain, you cannot display more texels than the screen has pixels. For the case of\na small object, like a character or power-up, you can discard more detailed mip levels of\nthe texture (see [Forsyth07]). However, for environments where the camera spends most\nof its time looking only at a small portion of the mesh, dropping mip levels is impracti-\ncal. If any part of the model requires high detail, you need all or most of the mips, and\nwith a terrain or other environment, you’ll almost always need high detail on some part.\nWhat can be done to deal with terrain textures efficiently? Only load partial\nmipmaps! Ideally, you would only load texels onto the GPU that are needed for the\ncurrent frame—meaning that you could have an arbitrarily detailed terrain that fits in\nonly 3MB of VRAM. Unfortunately, GPU manufacturers haven’t built their hardware\nto support this kind of operation.\nClipmaps are a generalization of mipmapping that allow you to only load subsec-\ntions of each mip level. If a texel that is sampled isn’t loaded, lower-resolution data\nthat’s already loaded is used instead. This means you can upload a relatively small\ndataset, get efficient rendering, and degrade gracefully if the viewpoint manages to\noutrun your texture paging. Although clipmaps aren’t directly supported by current\ngraphics hardware, you can emulate them efficiently using shaders.\n\n\nImplementation of Clipmaps\nBuilding on the concept of mipmapping, SGI developed clipmapping for the pur-\npose of virtualizing a single large texture [Tanner96].\n414\nSection 5\nGraphics \nFIGURE 5.6.1\nImage of a clipped mipmap stack.\nRecall that a mipmap is used to reduce aliasing and localize memory accesses.\nConceptually, when a given pixel of a triangle is rendered, the pixel’s bounds are pro-\njected into texture space, and based on its size, a mipmap is selected and from it a\npoint is sampled. The result of this is that as a triangle becomes more distant it selects\nfrom less detailed mip levels and the memory accesses are less scattered than they\nwould otherwise be.\nClipmaps take the same basic idea, but the more detailed levels of the mip pyra-\nmid are clipped to limit memory usage. This means that a 32KB px texture, which\nwould normally have 15 mip levels and consume half a gig of memory, if put into a\nclipmap with a maximum level size of 512px, would only use six 512px textures’\nworth of memory, plus a “cap” 512px texture with a full mip chain. The memory\nfootprint for this is only 7.3MB.\nIn SGI’s InfiniteReality2 hardware platform, the hardware, when accessing mip\nlevels, checked to see if the cached clipmap region in memory covered the area of the\nmip level it wanted to read from. If so, it would sample as normal. If not, it would\nbump up to the next less detailed mip level and try again, with the result that if\ndetailed data was not available for an area, less detailed data would be used instead.\n\n\nOn the CPU, SGI’s Performer scenegraph was responsible for adjusting the data\nin each layer of the clipmap by purging old data and uploading new data from a data-\nbase on disk.\nAdvantages of Clipmaps\nClipmaps bring several major benefits compared to the other strategies discussed, as\nfollows:\n• They always have smooth transitions between LOD levels, and no specific LOD\nlevel is required to render geometry—worst case, things will just be a bit blurry.\n• They have a fixed memory cost; no dynamic allocation of GPU resources is\nneeded either during rendering or updating. This is important as most GPU dri-\nvers don’t deal well with frequent allocations and deallocations.\n• They are view independent provided you place the focal point for detail at the\ncamera position; data is available with a smooth fall-off in all directions, meaning\nthat spinning in place has no effect on performance.\n• Clipmaps are straightforward to implement on any hardware with programmable\nshaders. Even on fixed-function hardware, it’s possible to emulate them. Imple-\nmenting update region determination is a bit tricky, but the system as a whole is\nstraightforward to work with, with no complex caching logic.\n5.6\nClipmapping on SM1.1 and Higher\n415\nFIGURE 5.6.2\nUpdating clipmaps affect which areas are\ncontained in each detail level.\n\n\n• They have well-defined relationships between image quality and resources allocated\nto the clipmap, so it’s easy to tune the visual experience based on user preference.\n• They can receive data from many sources. Unique data from files, CPU synthesis\nof data, or GPU synthesis are all supported.\n• Finally, they have excellent update characteristics, because the quantum of update\nis variable. The minimal amount of update required is usually quite small and\nbounded—just the new texels that need to be uploaded for a single clipmap level,\nwhich is often only a few thousand. The worst case is a full upload of the whole\nclipmap, which is only a dozen megabytes or so.\nDrawbacks of Clipmaps\nThe major drawback of clipmapping is that it cannot deal with varying detail levels.\nDetail simply falls off linearly in texture space from the focal point. This makes them\nunsuitable for dealing with a complex interior environment, where multiple regions\nin texture space may need to be high detail (for instance, the floor and walls may have\ndifferent UV regions that they use). If you can require mid-range SM2 or higher,\nthere are some good options to check out, like [Lefebvre04].\nThe full un-optimized shader for clipmapping is also expensive and requires at\nleast SM2. However, with some geometry conditioning, this can be optimized signif-\nicantly, as you’ll read later on. It might also be possible to use the gradient operators in\nSM3 and higher to write a more efficient clipmapping shader.\nDetails of Clipmaps\nThe following sections explain and describe the details related to clipmaps, including\nclipstack size, the focus point, and methods for updating clipmaps.\nClipstack Size\nThe size of the textures in the clipmap stack is the main variable when working with\nclipmaps, and it can be controlled quite simply—it should be the power of 2 nearest\nto the display resolution. For higher-quality results, bias up, and for lower quality, bias\ndown. The reason for this goes back to the original discussion of the amount of texel\ndata needed for an optimal renderer; the most demanding situation possible, texture-\nwise, is for the view to be looking straight on at a clipmapped surface, zoomed in as\nmuch as possible without magnification of the original texture. In this case, a texture\nequal in size to the screen would be needed to give the illusion of full detail.\nThe Focus Point\nSelecting the focus point, which is the location in UV space of the clipmap where detail\nshould be highest, is another open question when working with clipmaps. There are\nmany possible heuristics, but the one that gives the most consistent results is to simply\nproject down from the camera position onto the plane of the clipmapped geometry\nand use those coordinates as the new focal point.\n416\nSection 5\nGraphics \n\n\nMethods for Updating\nThere are three broad paths for getting data into the style of clipmap discussed here.\nThey all do roughly the same thing: updated regions are identified by the clipmap\ncode and data is supplied to fill them.\nFirst, you can blast data into them from files on the disk. In this case, once the\ndata is in system memory, you only need to directly upload it to the GPU. Second,\nyou can synthesize data on the CPU and upload it. I found this to be inferior to the\nnext method, but if you have an existing fast synthesis routine, it might be useful.\nFinally, you can perform synthesis on GPU by doing render to texture operations.\nThis requires allocating the clipstack textures as render targets to begin with, but oth-\nerwise operation is identical to the other modes.\nImplementing Clipmaps\nThe following sections cover the details related to implementing your clipmaps.\nThe SM2.0 Path\nFor simplicity’s sake, this gem discusses only the SM2.0 clipmap path in-depth. Once\nyou have the 2.0 path done, the majority of work is done, so getting the SM1.x and\nSM3.0+ paths going is straightforward.\nThis is the core pixel shader code that drives the clipmap effect in the demo app\non the CD-ROM:\nPS_OUTPUT Output;\n// The base level can always be sampled as there’s nothing behind\n// it... so save some math.\nfloat3 colAccum = tex2D(clipSamplers[0], In.TextureUV[0]);\n// Grab the rest, fading based on distance from each layer’s center.\nfor(int i=1; i<CLIP_LAYER_COUNT; i++)\n{\nfloat fade = smoothstep(0.4, 0.5, distance(In.TextureUV[i],\ng_clipLayerAndCenter[i].xy));\nfloat4 curColor = tex2D(clipSamplers[i], In.TextureUV[i]);\ncolAccum = lerp(curColor, colAccum, fade);\n}\n// Store accumulated result and return.\nOutput.RGBColor = float4(colAccum,1); \nIn the SM2.0 path, you do all the clipmap level selection calculations per-pixel.\nAt each pixel, you must determine the UV coordinate and, using information passed\nvia uniform shader constants, produce a texture coordinate for each clipstack entry by\nscaling and offsetting the original UVs. You also generate a “fade” value for each clip-\nstack entry based on distance in texture space from its focal point. Then, using the\n5.6\nClipmapping on SM1.1 and Higher\n417\n\n\nfade values as coefficients, you lerp the colors from each clipstack entry in order from\nleast to most detailed.\nToroidal Updates and the Rectangle Clipper\nA major optimization in the clipmap scheme is to treat the clipstack textures as\ntoroidal buffers. This means that shifting the stack is done as efficiently as possible—\nyou only do work to upload new data. However, determining the regions that need to\nbe updated is a little tricky.\nConsider a level of the clipstack. At any given moment, there’s a rectangle of data\nthat’s contained in the clipstack’s texture. Let’s call this rectangle currentRect. It’s the\ncurrently loaded subset of the full set of data available at some miplevel of the virtual-\nized texture. As you move the focus point, this rectangle shifts around to center on it.\n418\nSection 5\nGraphics \nFIGURE 5.6.3\nThe size of the clipstack texture is only 1/16th the size of the full source level.\nSuppose you’re on the third level of the clipmap from the top. This means that\nthe size of the clipstack texture is only 1/16th the size of the full source level. The sit-\nuation is illustrated in Figure 5.6.3. The grid shows how the texture is mapped to the\ngeometry. You scale unit UV coordinates by four, so the texture is repeated four times\nin each direction. However, the currentRect isn’t aligned to this grid; it’s somewhere in\nthe middle. By uploading the texture data in the pattern shown to the right, you end\nup with every piece of data where you want it on the geometry.\nYou then clip the currentRect against this grid, which is spaced equal to the size of\nthe clipstack textures. You’ll always end up with different pieces (in the common case,\nfour, but if you’re aligned to the grid in various ways, it can be less). This gives you the\nbasic idea of what’s going on and how uploaded data has to map into the texture to be\ndisplayed properly. ClipMap::fillWithTextureData implements this to refill the clip-\nstack entirely.\n\n\nWhat about updating? When you move the rectangle, you tend to get something\nthat looks like Figure 5.6.4.\n5.6\nClipmapping on SM1.1 and Higher\n419\nFIGURE 5.6.4\nThe inverted L-\nshaped region is what needs to be\nuploaded during an update.\nThe inverted L-shaped region is what you need to upload. So when you’re updat-\ning what currentRect contains, you determine what this update region is, and then clip\nand wrap it just as you did above with the currentRect itself, ending up with rectangu-\nlar regions in the texture where data is to be uploaded. This allows very efficient\nclipmap updates—moving the focus point one pixel means a rectangle only one pixel\nwide would be uploaded. See ClipMap::recent for the implementation of this.\nBasic CPU Synthesis\nA helpful aid in debugging is a simple checkerboard synthesizer. A 1px checkerboard\nmakes it easy to spot any sampling issues or other problems. Applying a gradient\nmakes it simple to spot any incorrect updates—the colors won’t match.\nLook in ClipMap::uploadToTexture for an example of this. The #if block can be tog-\ngled to 1 to enable a simple CPU synthesis that chooses a random color for each clipmap\nupload. This is useful for seeing how updates happen and what regions they cover.\nBasic CPU Upload\nBy default, the example app loads data into the clipmap from a large image stored in\nsystem memory. (This allows you to avoid the complexity of a paged loader.) This is a\nstraightforward bitblt operation.\nAdvanced Clipmapping\nThe following sections cover some advanced issues related to clipmaps, including\nadding background paging, budgeting updates for better performance, optimizing fill-\nrate, and more.\n",
      "page_number": 443,
      "chapter_number": 46,
      "summary": "This chapter covers segment 46 (pages 443-452). Key topics include texture, data, and animation. The images to its right show different poses, seen from the same view-\npoint, obtained with RBF-based warping functions.",
      "keywords": [
        "texture",
        "clipmap",
        "Data",
        "Control points",
        "Radial Basis Functions",
        "level",
        "Graphics",
        "point",
        "relief",
        "mip levels",
        "Basis Functions Textures",
        "Graphics FIGURE",
        "clipmap level",
        "GPU",
        "size"
      ],
      "concepts": [
        "texture",
        "data",
        "animation",
        "animations",
        "animating",
        "animate",
        "updating",
        "update",
        "functions",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 48,
          "title": "Segment 48 (pages 469-476)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 27,
          "title": "Segment 27 (pages 526-548)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 43,
          "title": "Segment 43 (pages 424-431)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 42,
          "title": "Segment 42 (pages 401-412)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 48,
          "title": "Segment 48 (pages 456-465)",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 453-467)",
      "start_page": 453,
      "end_page": 467,
      "detection_method": "topic_boundary",
      "content": "Background Paging\nConceptually this is simple (although implementing a performant pager takes work!).\nBreak your source image and its mip levels into tiles. Maintain a cache in system\nRAM of all the tiles that the clipstack levels overlap, plus a border of data so that\nadjustments to the focal point can be fulfilled rapidly.\nMake sure you have a fast copy from your tiles into the clipstack textures. If data\nisn’t available to update a clipstack level, make sure the data has been requested, and\ndefer the update until a later time. I find that working from the bottom up gives good\nresults—high detail follows the user around, whereas mid-level data sometimes takes\na while to appear.\nBudgeting Updates\nThis is one of the major victories of clipmaps as opposed to other techniques. Most\nsurface caching approaches require a fixed quantum of work to be done—say, synthe-\nsizing a 128 \u0002 128px tile. As texel density increases the quantum does, too, until\nyou’re looking at a minimum of doing a 512px or 1024px tile! No good.\n420\nSection 5\nGraphics \nFIGURE 5.6.5\nAn image from the clipmapping app provided on the CD-ROM. See Color\nPlate 9 for a color version of this image.\n\n\nInstead, clipmaps generally require frequent small updates as the focal point moves.\nThe typical update is just a few slices along the horizontal or vertical edges of a clipstack\nlayer—for a 512px clipmap, this might be only a thousand pixels to upload.\nThus, you can set a texel upload budget and, after each level is updated, check to\nsee if you’ve overrun it. If so, just stop updating, and let the next frame’s update take\ncare of it. This is also helpful in cases when the camera is moving—you don’t waste\nmuch time on detail that will only be visible for a frame. It’s also possible to budget\nbased on available time until the next present, using the same methodology. \nAs long as you always update at least one level each time through before aborting,\nall the required data will eventually make it into the clipmap, and you may avoid lots\nof work that would be seen for only a frame or two.\nOptimizing Fillrate/Low-End Support\nBy conditioning your geometry into chunks with known texture coordinate bounds,\nit’s possible to determine efficiently at runtime what clipstack levels are needed to tex-\nture that chunk, thus allowing you to reduce the number of textures that have to be\nbound to the clipmap shader.\nThis also begins to enable SM1.0 support, because you can get down to four or\nfewer active textures, under the four texture sampler limit of SM1.0. By then, moving\nthe level fade calculations into the vertex shader (and ensuring a certain minimal ver-\ntex density!), you can fit the clipmap logic into an SM1.x pixel shader.\nUsing an SM1.x-compatible path is a good idea even on higher end cards because\nit’s much faster than the naive SM2.0 shader. Especially on cards that report high\ncapabilities but can’t do them quickly, like the X300, this can be a huge win.\nBy extending this idea, it’s also possible to target FF cards. You can either bind the\nsmallest clipstack level that contains the chunk’s texcoords, or you can hack up transi-\ntions between two or three levels using register combiners and approximating the\nshaders.\nTaking Advantage of the High End\nIn shader models where the pixel gradient operators are available, you can do the\nmipmap calculations yourself, and look up the exact levels of the clipmap that are\nneeded for the pixel in question. This cuts the fillrate significantly, although the shader\nmay then be costly to evaluate.\nIn higher-end contexts, it’s also feasible to consider maintaining several clipmaps\nfor different attributes. For instance, one for normal maps, another for diffuse, a third\nfor specularity. For “localized” attributes, which tend to average to nothing in the dis-\ntance, like normal maps, it might also be profitable to maintain just the two or three\nmost detailed levels of the clipmap.\nIf You Want To Save Some Time...\nIf you want to just grab an existing implementation off the shelf, Torque Game\nEngine Advanced, which my employer, GarageGames, sells, contains the Atlas terrain\n5.6\nClipmapping on SM1.1 and Higher\n421\n\n\nsystem. TGEA comes with full source and liberal licensing terms, and Atlas has a fully\npaged and optimized clipmapping implementation with support for SM1 and higher.\nTorqueX’s 3D terrain system also includes a comparable implementation in C# on\nXNA. Check them out; they might save you a lot of time and money.\nL3DT, 3d Studio Max, and the Panda DirectX exporter were used to create the\nassets for the demo included on the CD-ROM.\nReferences\n[Forsyth07] Forsyth, Tom. “TomF’s Tech Blog—Knowing Which Mipmap Levels Are\nNeeded,” available online at http://home.comcast.net/~tom_forsyth/blog.wiki.\nhtml, 2007.\n[Lefebvre04] Lefebvre, Sylvain, Darbon, Jerome, and Neyret, Fabrice. “Unified Tex-\nture Management for Arbitrary Meshes,” 2004.\n[Tanner96] Tanner, Migdal, Jones. “The Clipmap: A Virtual Mipmap,” available\nonline at www.cs.virginia.edu/~gfx/Courses/2002/BigData/papers/Texturing/\nClipmap.pdf, 1996.\n422\nSection 5\nGraphics \n\n\n423\n5.7\nAn Advanced Decal System\nJoris Mans\njoris.mans@10tacle.be\nDmitry Andreev\ndmitry.andreev@10tacle.be\nM\nost games these days use decals in one way or another; for instance, to show bul-\nlet marks on the environment, or to add variation on repetitive geometry. Usu-\nally this is done by rendering a transparent polygon on top of the existing geometry.\nThis technique, however, has some drawbacks, especially if you want to apply bump\nmapping in your decals. When rendering a bump mapped decal on top of existing\nbump mapped geometry, the lighting is not correct, because the pixels underneath \nthe decal should also have been lit using a combination of the decal bump map and the\ngeometry bump map. This gem explains how to render decals that actually replace the\nbump and the diffuse map of the geometry (this can be extended to any kind of tex-\nture map you use), thereby giving correct lighting results and a higher image quality.\nRequirements\nAn implementation of this gem can be done on any platform supporting render tar-\ngets and shader logic capable of sampling and interpolating between values obtained\nfrom at least two different maps (if you only want to use it for diffuse textures), or\nfour (if you want to add bump map support). The best image quality is obtained by\nusing render targets that are the same resolution as the screen, but smaller ones will\nalso work, with a decrease in image quality. The demo provided runs on any PC with\na DirectX 9 compatible graphics card supporting pixel shader version 2.0.\n\n\nNormal Decals Method\nIn a traditional engine using a decal system, you first render all the geometry in the\nframe buffer, and on top of that you render polygons containing the decals, usually\nusing some kind of blending. \nAdvanced Decals Method\nIn this example, you’ll do things slightly differently. First, you need to create the neces-\nsary tools in the runtime to accomplish the decal renderer. In this case, we will create\ntwo full-screen render targets. The first one is in 32-bit RGBA format; it’s called the\nDiffuseRenderTarget. The second one will also be in 32-bit RGBA format; it’s called\nBumpRenderTarget. For this second one, you could use a 16-bit per component buffer\nor any other format of render target if your bump maps are stored in higher precision.\nIn our demo, we use DXT5 compressed bump maps so 32-bit RGBA will suffice.\nRendering the scene can be split up into two parts. First, you generate the decal\nbuffers, and next you render the scene with the decals applied. To generate the decal\nbuffers, execute the following steps:\n• Render all depth values of the geometry in the main z buffer (excluding the decals).\nThe depth compare function used is the same as the one you use to do the normal\nscene rendering.\n• Select the DiffuseRenderTarget as the current render target, while still using the\nmain z buffer. The render target is cleared, using black as the clear color. \n• Render all decal geometry into that render target, using the same depth compare\nfunction used previously for the depth pass, but don’t render the complete shader\nas you would in the normal decal case. The rendering uses a special shader that\njust outputs the color of the diffuse texture, pre-multiplied with the opacity tex-\nture (or diffuse alpha depending on your art pipeline). In the alpha component of\nthe render target, output the opacity value used to scale the diffuse value. \n• In the BumpRenderTarget, you do something similar. Render the decal geometry,\nthis time using the bump map texture value in world space as output. This step\ncan be combined with the previous one if your target hardware supports multiple\nrender target rendering. Sample results are shown in Figures 5.7.1 and 5.7.2.\nFinally, you render the scene. The thing you have to do in the shaders used for the\ngeometry is change the code that makes the diffuse texture lookup and the bump tex-\nture lookup. You must actually combine the diffuse value from the texture applied to\nthe geometry and the values found in the DiffuseRenderTarget/BumpRenderTarget\ntextures. This works as follows:\n• Take the screen space position of the pixel you are currently rendering. This will\nbe used as texture coordinates to read out the values of the render targets.\n• Use the texture coordinates to read the diffuse RGB value from the decal diffuse\nmap; call this value drt.\n424\nSection 5\nGraphics \n\n\n5.7\nAn Advanced Decal System\n425\nFIGURE 5.7.1\nThe DiffuseRenderTarget used by the decal system.\nFIGURE 5.7.2\nThe BumpRenderTarget used by the decal system.\n\n\n• Combine it with the RGB value read from the diffuse texture used on the geom-\netry, using the alpha value read from the render target. This gives you the follow-\ning formula, where dt is the diffuse texture of the object, drt is the render target\ntexture containing the decal diffuse value, and d is the resulting diffuse value:\ndrgb = dtrgb *(1 – drta) + drtrgb\n(5.7.1)\n• Read the value stored in the decal bump map; store it in the variable brt.\n• Combine it with the bump map of the object according to the following formula.\nHere bt is the bump texture of the object, wsb is the bump vector in world space,\ndrt is the render target texture containing the decal diffuse value, brt is the render\ntarget texture containing the decal bump value in world space, and b is the result-\ning bump value:\n(5.7.2)\nDecodeBump is the function that converts your RGBA texel into a bump vector,\ndepending on the way you store your bump maps. Of course, interpolating bump\nvectors like this isn’t really mathematically correct, but the visual results in this case\nare fine, so you need not look for a more advanced solution.\nFigures 5.7.3 and 5.7.4 show a comparison of the traditional method and the\ntechnique explained in this gem.\nwsb\nTransformToWorldSpace DecodeBump btrgba\n=\n(\n)\n(\n)\n=\n−\n(\n)+\n(\n)\nb\nwsb\ndrt\nDecodeBump brt\ndr\nxyz\na\nrgba\n*\n*\n1\nt\nb\nnormalize b\na\n=\n( )\n426\nSection 5\nGraphics \nFIGURE 5.7.3\nDecals using the traditional technique.\n\n\nDecodeBump\nAs mentioned in the previous paragraph, this example uses a function called Decode-\nBump to decode the bump maps. There are several ways of storing bump maps. The\nchoice of which one to use depends on hardware support, quality, and speed. Explain-\ning in detail the different approaches is beyond the scope of this gem, but it does\ninclude some examples of how this can be done. The easiest solution is to use an RGB\nrender target with 8bits per component and store the bump maps as color values,\nscaled and biased to fit in the 0..255 range of the pixel color.\nEncoding a bump vector into this format would look like this:\ncolor.rgb = (bumpvector xyz + 1)*127.5\n(5.7.3)\nThe corresponding DecodeBump function would be something similar to this:\nbumpvector.xyz = color .rgb* 2 – 1\n(5.7.4)\nRecall, although you wrote byte values in the range of 0..255 when encoding, in\nthe pixel shader, all values are normalized floats, where 0 maps to 0 and 255 maps to\n1.0. The disadvantage of this way of storing is that the bump maps are uncompressed,\nand that compression using DXT1 gives rather bad visual results.\nAnother approach sometimes used is to store the bump values in a DXT5 com-\npressed surface, using the green component to store the x value of the bump vector,\nand the alpha component to store the y value. When reading the bump map, you can\nreconstruct the z value using x and y.\n5.7\nAn Advanced Decal System\n427\nFIGURE 5.7.4\nDecals using this technique.\n\n\nEncoding the bump vector would look like this:\n(5.7.5)\nThe DecodeBump function reconstructs the third component:\n(5.7.6)\nThis allows you to store bump maps in a compressed format, while still maintain-\ning a good level of quality by exploiting the fact that the green component in a DXT5\ncompressed texture contains six bits of precision, and that the alpha component is\ncompressed separately. The tradeoff is, of course, that there are more calculations\nneeded to recreate the bump vector—calculations that are not available on all plat-\nforms or that might be too expensive. There is a tradeoff between storage require-\nments and pixel shading speed, but on some platforms the fact that the data is\ncompressed gives you fewer cache misses and actually is faster, even with the calcula-\ntion of the z component, than reading decompressed values directly.\nAdvantages of This Advanced Decal System\nThe main advantage of this system is the increased quality of the image, compared to\nnormal bump maps. It also allows decals to be used in different ways. For instance,\nimagine using decals that only contain bump maps to influence the look of a repetitive\nwall by adding cracks, noise, or other variations. Instead of only using decals in the\nruntime to add bullet and explosion marks, you can also use them in the level editor\nwhen building the scene.\nCreating the same diversity in an engine supporting standard decals would require\nan entirely different approach. Because you cannot replace bump maps with normal\ndecals, you would have to actually create bump maps for each part of the scene where\nyou want variations, replacing the original bump map used with the variant one. Not\nonly does this mean that you will have a lot more bump maps in memory, but it also\nrequires more work from artists to create and place those bump maps on the geometry.\nAs shown in Figure 5.7.5, playing with the opacity of the decals can simulate wear\nand tear on geometry over time. Because you have coherent lighting here, you can per-\nfectly blend in an erosion bump map by playing with the opacity value of the decal.\nIt can also be used to create variations in a scene that uses a lot of instancing. You\ncan instance the same geometry all over, using hardware instancing support if that is\nbumpvector x\ncolor b\nbumpvector y\ncolor\n.\n. *\n.\n.\n=\n+\n=\n2 1\na\nbumpvector z\nbumpvector x\nbumpvect\n*\n.\n.\n2 1\n2\n+\n=\n(\n) +\nor y.\n(\n)\n2\ncolor r\ncolor g\nbumpvector x\ncolo\n.\n.\n.\n*\n.\n=\n=\n+\n(\n)\n0\n1\n127 5\nr b\ncolor a\nbumpvector y\n.\n.\n.\n*\n.\n=\n=\n+\n(\n)\n0\n1\n127 5\n428\nSection 5\nGraphics \n\n\navailable, but thanks to the decal system, you can add variations on the surface of each\ninstance, without requiring the memory footprint of having each instance separately in\nmemory. Because you use the depth buffer in the decal system, it supports non-planar\ndecals. The only constraint is that the decals have to follow the geometry underneath\nthem as closely as possible. Apart from that, the topology of the decal has no restrictions\nwhatsoever. An example of decals on non-planar geometry is shown in Figure 5.7.6.\n5.7\nAn Advanced Decal System\n429\nFIGURE 5.7.5\nUses of decals for erosion over time. From left to right and top to bottom,\nthe opacity values are 0, 20, 40, 60, 80, and 100 percent, respectively.\n\n\nIt is also quite easy to implement, and integrating it in existing technology does\nusually not require major changes in the rendering pipeline. If the z pre-pass is already\navailable, you can add the render to the two decal render targets right after that pre-\npass, and you just need to change the shader code that samples the diffuse and bump\nmap textures when rendering the scene in order to read from the decal buffers. (See\nthe color insert for color versions of many of the images shown in this gem.)\nPerformance and Experimental Results\nThis section shows the results of our implementation of the technique described here,\nalong with some performance tests and potential issues. The demo on the CD-ROM\nis just another simplified implementation of advanced decals we are using in our\ngaming engine. It shows the main parts of the technique and yields clear performance\ntendencies. All tests were performed on a 3.0GHz P4 with NVIDIA's GeForce 6800\nGT and GeForce 7800 GT.\nWe used four rendering presets of the demo to show different aspects of performance:\n• Original—A standard lighting model that already exists in almost all modern\nengines, without using decals. In this case, it is based on two per-pixel computed\nlight sources using tangent-space normal maps, diffuse maps, and specular maps. \n• Normal decals—Regular decals rendered on top of the geometry rendered using\nthe original shaders. A decal consists of a diffuse texture and a bump texture.\n• Advanced (Original)—Renders the decals into the decal buffers, but uses the shader\nof the original to render the objects on the scene, so no decals are shown. This\nallows you to see the cost of filling the two decal buffers.\n• Advanced—Renders the decals into the decal buffers and applies them to the\nobjects in the scene.\n430\nSection 5\nGraphics \nFIGURE 5.7.6\nDecals on non-planar geometry.\n\n\nThe primary question is what is the performance difference between normal and\nadvanced decals. Figures 5.7.7 and 5.7.8 show performance in frames per second.\nThere are two 512 \u0002 512 \u0002 32 (diffuse and specular) and one 1024 \u0002 1024 \u0002 32\n(normal map) textures assigned to each object. There are also the same amount of\ntextures of the same sizes assigned to decals.\n5.7\nAn Advanced Decal System\n431\nFor these two tests, we used non-compressed textures with 8x anisotropic filtering\nof normal maps. In the “full scene test” in Figure 5.7.7, the camera was pointed such\nthat almost all decals were visible covering both close and distant scene objects. Whereas\nthe close up test shown in Figure 5.7.8 was completed with the camera pointed only at\none single object covering the full screen, so that all others would be Z culled.\nFIGURE 5.7.7\nFull scene test.\nFIGURE 5.7.8\nClose up test.\n\n\nNo matter what resolution we render in, or whether rendering an entire scene or\nclose up, the “Advanced” technique is about 11% slower than the “Advanced (Origi-\nnal)” technique. The same tests have been done but with compressed textures and\nthey gave us a 23% difference between “Advanced” and “Advanced (Original),” and\nhigher frame rates. So those few additional texture fetches and blending instructions\nin the main shader cost us about 11–23% speed.\nBut how does that difference depend on the complexity of decals? To answer that\nquestion we’ve made two tests showing that dependency. We rendered one full-screen\ndecal multiple times on our scene to see how this would influence performance. As\nthe cost of the lookup in the decal buffers is independent of the number of decals\nused, the test we did previously already showed the performance implications of those\nlookups. The second test shows the cost of actually rendering the decals themselves.\n432\nSection 5\nGraphics \nAs you can see in Figures 5.7.9 and 5.7.10, when only using one decal the stan-\ndard decal technique is faster, but when drawing multiple decals on top of each other,\nthe advanced technique actually renders faster. This is due to the fact that when ren-\ndering multiple standard decals on top of each other, the complex shader, which does\nthe lighting and bump mapping, is executed once for each decal being rendered, while\nin the advanced decal’s case, the complex shader is only executed once, even when\nmultiple decals overlap.\nIf your rendering pipeline is memory or API-call bound, all decal buffers (tex-\ntures) could be filled at once using multiple render targets. In this demo it only shows\nthat it doesn’t cause any additional performance issues. But using it will just minimize\ntexture state changes and API calls, simply because in that case you need to render\ndecals only one time.\nFIGURE 5.7.9\nNon-compressed textures.\n\n\nAlthough we can conclude there is a performance cost of about 12% in our test\nscene, we should not forget that these are test cases and that a rendering engine does\nmuch more rendering than just rendering objects with decals. When taking into\naccount the cost of the other things going on during the rendering (for example, full-\nscreen effects, particle systems, shadowmapping, and so on), the total performance hit\npercentage will be smaller. Something else to consider is that by using these decals,\nyou can build scenes with a smaller amount of different textures, resulting in available\nmemory gains, fewer state changes, and bigger batches, which might actually increase\nrendering performance. So the cost of using those decals ends up being less than 12%,\nwhile gaining available memory. You can even see performance benefits when there\nare lots of overlapping decals.\nDemo\nOn the CD-ROM, you can find a demo of this decal system. There are several param-\neters exposed so you can see and test the differences between this system and tradi-\ntional decals, and tweak certain rendering settings. You can also see how the diffuse\nand bump maps are combined with the decal maps, to get a better understanding of\nthe algorithm. There are some more screenshots found in the “screenshots” subfolder.\nThe demo requires a DirectX 9 compatible video card supporting shader model 2.0.\nConclusion\nThis gem covered a way of rendering decals that has several advantages over the tradi-\ntional approach. It results in better image quality, consistent lighting of the parts\ncovered with decals, and it allows uses of decals that were previously not possible. For\ninstance, decals can be applied that only contain a bump map and no diffuse texture.\nThe method presented here can easily be integrated in existing technology, without\n5.7\nAn Advanced Decal System\n433\nFIGURE 5.7.10\nDXT 1/5 compressed textures.\n\n\nrequiring massive changes to the production or rendering pipeline, and the perfor-\nmance cost is relatively small. Moreover, when pushing this further, you can use decals\nto replace any texture used on the scene geometry. Another possibility is to add decals\non the scene when building the geometry in the editor, thereby allowing for many\nvariations on top of generic, tiled textures without having to resort to using detail\ntextures.\nReferences\n[Jing06] Jing, YingHui, et. al. “A Post-Processing Decal Texture Mapping Algorithm\non Graphics Hardware,” Proceedings of the 2006 ACM International Confer-\nence on Virtual Reality Continuum and Its Applications, pp. 99–104.\n[Lengyel01] Lengyel, Eric. “Applying Decals to Arbitrary Surfaces,” Game Program-\nming Gems 2, 2001, Charles River Media, pp. 411–415.\n434\nSection 5\nGraphics \n",
      "page_number": 453,
      "chapter_number": 47,
      "summary": "This chapter covers segment 47 (pages 453-467). Key topics include rendering, render, and textures. Maintain a cache in system\nRAM of all the tiles that the clipstack levels overlap, plus a border of data so that\nadjustments to the focal point can be fulfilled rapidly.",
      "keywords": [
        "Advanced Decal System",
        "decals",
        "Decal System",
        "Advanced Decal",
        "bump maps",
        "bump",
        "decal bump map",
        "bump map",
        "render",
        "decal buffers",
        "render target",
        "maps",
        "Advanced",
        "decal bump",
        "texture"
      ],
      "concepts": [
        "rendering",
        "render",
        "textures",
        "decal",
        "values",
        "performance",
        "performed",
        "levels",
        "color",
        "support"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 40,
          "title": "Segment 40 (pages 394-404)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 36,
          "title": "Segment 36 (pages 360-367)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 59,
          "title": "Segment 59 (pages 570-580)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 48,
          "title": "Segment 48 (pages 456-465)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 43,
          "title": "Segment 43 (pages 424-431)",
          "relevance_score": 0.58,
          "method": "api"
        }
      ]
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 468-478)",
      "start_page": 468,
      "end_page": 478,
      "detection_method": "topic_boundary",
      "content": "435\n5.8\nMapping Large Textures for\nOutdoor Terrain Rendering\nAntonio Seoane, Javier Taibo, \nLuis Hernández, and Alberto Jaspe\nVideaLAB, University of La Coruña\n(antonio.seoane@videalab.udc.es), (jtaibo@udc.es),\n(lhernandez@udc.es ), and (jaspe@videalab.udc.es)\nT\nexturing highly detailed large terrain areas is a requirement in many games, espe-\ncially flight simulators. Fortunately, hardware supports large textures, up to 8192\ntexels square. Classical techniques are based on the tiling of large textures or blending\ndetail textures. The problem with these techniques is that, in one case, geometry must\nbe divided into segments with borders exactly matching the texture tile boundaries\nand, in the other case, the appearance is repetitive, unnatural, and unrealistic. This\ngem explains a method that allows the use of huge textures, based on clipmaps. The\ntechnique can be used with any geometry algorithm without the need to divide tex-\ntures into tiles adapted to the geometry boundaries. Moreover, it allows dynamic\ngeometry deformation.\nIntroduction\nIn the case of online games huge textures can be stored on game servers, so they can\nbe downloaded in real time. This allows textures that would exceed the storage capac-\nity of the user’s computer and also enables easy updates on the game server to add\nmore detail, new features, and so on. Moreover, allowing huge textures is more nat-\nural and easy for game artists, who can use a large canvas to paint with as much detail\nas possible and also eliminate artifacts due to repeating tiled textures.\nIn order to successfully deal with textures that are larger than system and video\nmemory, some specific techniques are needed. On the Virtual Terrain Project Website,\nthere is a large compilation of papers about the problem of mapping large textures over\nterrain [VTerrain07]. One drawback in the vast majority of existing solutions is the\nstrong coupling between texture and geometry databases. This requires subdivision of\nthe texture in order to adapt it to the geometry or vice versa.\n\n\nClipmapping is one of the best approaches to manage large textures that cannot fit\ninto system memory [Tanner98]. This technique decouples the handling of texture\nand geometry, allowing independence between both databases. The first implementa-\ntion of this technique was made in Silicon Graphics systems and required expensive,\nspecific hardware [Montrym97].\nThe main idea of clipmapping is to handle a large size mipmap pyramid (where\nlarge means larger than the texture size limit and/or the available video memory),\nkeeping only a subset of the pyramid in video memory. The portion of each level that\nis kept resident is limited by a user-specified parameter called the clipsize. The levels\nwith size lower than or equal to the clipsize are always in video memory, and the larger\nlevels are clipped to this limit. The area of incomplete levels that is resident is centered\naround a point called the center of detail or the clipcenter. As the camera moves, the\nclipcenter is dynamically updated and the region cached for each level in video mem-\nory is consequently updated. This way, there is always the best possible quality avail-\nable to map the geometry into the region being visualized. They can be large areas\nwith low resolution or small areas with very fine detail.\nThe main advantage of clipmapping is that a huge texture can be handled using a\nlimited portion of memory. For instance, a 65536 \u0002 65536 texel cliptexture (21.3GB\nusing 32 bits depth in a storage device) using a 1024 clipsize requires only 29.34MB\nof video memory. The system can be adjusted to use any clipsize depending on the\namount of video memory that is allocated to it.\nNext, this gem describes a technique that allows the handling of a large amount\nof texture using current PC and console hardware. The technique stores the image in\ntiles that are not used directly as textures. These tiles are combined in a texture stack\nthat caches the region of interest, following the clipmap idea. Although inspired by\nclipmapping, there are important differences in its structure, video memory manage-\nment, and the way the texture is applied. This allows implementation on any graphics\ncard without special hardware requirements—only OpenGL or Direct3D fixed func-\ntion pipeline is required to implement this technique.\nThe technique described in this gem has been successfully used in several projects\nusing texture details of 0.25 m/texel in geographical areas of about 60,000km2[Santi07].\nIt has also been successfully used with different geometry algorithms, based on grids\nas well as TINs. \nThe main advantages of this technique are as follows:\n• It can be implemented using a fixed-function pipeline API such as OpenGL or\nDirect3D.\n• It maintains independence between geometry and texture databases.\n• Texture coordinates can be automatically computed in the GPU, avoiding their\ntransference to the graphics system. This allows modification of the geometry in\nreal-time, while keeping the right texture mapping.\n• Texture aliasing is avoided using trilinear and anisotropic filtering hardware\ncapabilities.\n436\nSection 5\nGraphics \n\n\n• It allows the visualization of high-resolution textures with the possibility of\nincluding higher-resolution regions.\n• It allows the use of several independent large textures that can be combined to\nshow different information types simultaneously on the terrain.\nStructure\nThe technique proposed manages a virtually unlimited texture that we call the virtual\ntexture. It is stored using a pyramidal mipmap scheme [Williams83]. The highest\ndetail level of this pyramid is formed by 2l–1 \u0002 2l–1 texels at most (such as in case of a\nsquare texture), with l being the number of levels in the pyramid. Levels are num-\nbered from 0 to 2 i, the largest side size for level i, as illustrated in Figure 5.8.1.\n5.8\nMapping Large Textures for Outdoor Terrain Rendering\n437\nFIGURE 5.8.1\nVirtual texture.\nThe virtual texture is stored complete on persistent storage, either on a local disk\nor remotely requested from a server. This virtual texture is structured in the persistent\nstorage level in square tiles with a side size in texels power of two. An exception to this\nis those levels of the pyramid in which the texture size is smaller than the tile size.\nTiles are addressed with a vector (column, row, and level).\nThe pre-filtered mipmap levels of the virtual texture increase by a third the stor-\nage space required, but this is needed to map the texture in an efficient way that\navoids aliasing artifacts.\nTexture Cache\nFollowing the clipmap concept, a subset of the full pyramid is cached in texture mem-\nory to apply the adequate detail level to the area being visualized. This virtual texture\nis managed through a two-level cache system. The second-level cache is located in\nmain memory and uses a pool of buffers to store the least recently used tiles. \n\n\nTiles are asynchronously loaded on demand. Requests are prioritized by level with\ncoarser levels given higher priority. This way, larger areas are covered as quickly as pos-\nsible and the detail around the center of interest is progressively refined as higher level\ntiles become available. The tile size is a critical parameter, as it can impact the transfer\nrate from persistent storage to main memory.\nThe first-level cache is a subset of the virtual texture levels that is resident in tex-\nture memory. The virtual pyramid is fully stored in texture memory from the apex to\nthe base level. This set of levels is called the pyramid and it will be managed as a reg-\nular mipmapped texture. The size of the base level is called the clipsize. The base level\n(lb) is calculated from the clipsize (c) as lb=log2(c).\nFrom the base level up, only a subregion of the whole level is stored. The set of\nincomplete levels is called the stack. The levels of the stack are all the same size in tex-\nels and, progressively from the coarser to the finer detail level, half the terrain region.\nThe levels that make up the stack are incomplete subsets (with size c \u0002 c texels) of the\ncorresponding virtual texture levels. These levels are centered on a point of interest,\ncalled the center of detail. These concepts are shown in Figure 5.8.1.\nYou’ll use l–lb+1 independent textures in texture memory, as shown in Figure\n5.8.2. The first one (t0) corresponding to the pyramid’s finest level. Subsequent tex-\ntures ti cache the virtual level lb+i.\n438\nSection 5\nGraphics \nFIGURE 5.8.2\nTexture stack.\nTrilinear Filtering\nIn order to allow the graphics system to perform a trilinear filtering to avoid aliasing,\nmipmap levels for every texture are needed. Let tij be the mipmap level j of the texture\ni; it caches level lb+i–j of the virtual texture.\nAs shown in Figure 5.8.3, it is not necessary to have all mipmap levels in the tex-\ntures corresponding to the stack. This can save valuable bandwidth during cache updat-\ning. Our experience proves that about four or five mipmap levels in the textures of the\nstack are enough to achieve good quality without noticeable artifacts with a clipsize of\n1024 \u0002 1024 texels.\nFigure 5.8.4 illustrates the terrain area covered by different levels of the stack.\nThere, you can see the application of those levels to a real terrain, represented by a\ncolor-coded grid (see Color Plate 11 in the color insert of this book for the full-color\nversion of this image).\n\n\nTexture Memory Usage\nFor an l level virtual texture with a clipsize c, m mipmap levels for the stack textures\nand a texel depth of b bytes, the usage of texture memory for the cache can be com-\nputed as follows:\n(5.8.1)\ntexture\nmemory\nl\nl\nc\nb\ni\ni\nm\n_\n=\n−\n−\n(\n)⋅\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟+\n=\n−\n∑\n1\n2\n2\n2\n0\n1\n2\n0\ni\ni\nlb\nb\n=∑\n⎛\n⎝\n⎜\n⎜\n⎞\n⎠\n⎟\n⎟⋅\n5.8\nMapping Large Textures for Outdoor Terrain Rendering\n439\nFIGURE 5.8.3\nTexture stack with mipmap levels correspondence.\nFIGURE 5.8.4\nRings of detail and an example of a virtual texture applied to the terrain,\nshowing levels of detail using color codes.\n\n\nHigher levels in the pyramid can be incomplete, allowing the inclusion of addi-\ntional detail for special interest areas over a constant overall image detail. It is quite\nusual in games such as flight simulators to have a medium detail satellite texture over\nall the terrain and increase detail in some areas in which the plane is likely to fly low\nor approximate, such as an airport.\nUpdating the Contents of the Cache\nData stored on the texture cache corresponds to a zone of the terrain covered by the\nvirtual texture around the center of detail. As this center of detail is moved, contents\nof the cache need to be updated.\nDetail Center Computation\nEvery frame, the application must place the center of detail in the location where the\nmaximum quality is desired. Several strategies can be used. Typically, you’ll use camera\nposition and orientation. The trivial approach is placing the center of detail in the ver-\ntical projection of the camera location over the ground. Better results can be achieved\nby placing it on a point of the visible terrain close to the camera, and then computing\nthe intersection with the terrain of the eye view direction.\nTexture Stack Update\nWhatever strategy is used, once the center of detail is placed, stack texture levels must\nbe updated. Each level is updated sequentially from coarser to finer.\nTextures corresponding to these levels are considered divided in square blocks\nwith side size power of two. These blocks are called subtiles to differentiate them from\nthe tiles stored in the second level cache. The subtile is the texture updating atomic\nunit. Subtile size (s) must be a divisor of the clipsize (c) and the tile size (t), where\ns = 2i , t = 2 j , c = 2k , with i <= j , i < k\n(5.8.2)\nAs the center of detail is moved, some subtiles will become invalid and will have\nto be updated, whereas others will retain useful data. For each texture, there is a sub-\ntile state matrix indicating the validity of each subtile in the texture. Immediately after\nplacing the center of detail, these matrices will need updating for the new position.\nAfter the state matrices are updated, each texture is processed, from coarser to\nfiner detail. For each invalid subtile, you compute the address of the tile containing\nthe subtile data. This tile is requested from the second level cache. If it is resident the\nsubtile data is uploaded to the texture memory; otherwise, the asynchronous load of\nthe tile will be requested by the RAM tile cache and will be available in the next few\nframes. In case of incomplete levels, invalid subtiles absent from persistent storage will\nnever be updated.\n440\nSection 5\nGraphics \n\n\nThe virtual texture window cached in each stack level is accessed toroidally in the\ncorresponding real texture. This allows partial updates of each level, which drastically\nimproves the efficiency. As seen in Figure 5.8.6, when the center of detail is updated,\nthe window position is changed. Using the wraparound addressing only the new sub-\ntiles not present in the previous window position have to be loaded, while the overlap-\nping area remains in place.\nA subtile update of a texture implies updating the related area in each mipmap\nlevel of this texture. In the mipmap level updating process, consider that subtile size\nfor mipmap level m is s/2 m. Update of levels tij, where j > 0 can be made from coarser\ntextures, because mipmap levels data is replicated, as shown in Figure 5.8.3.\nLoad Control\nTexture upload time is critical for a real-time graphics application like a game to sus-\ntain the frame rate. The render time plus the texture update time must not exceed the\nframe time. For this reason, updating subtiles is limited in duration for each frame.\nThat means that for quick movements of the center of detail, it will not be possible to\nreach the finest detail in one single frame. This usually is not a problem because fast\nmovements do not usually allow the viewer to appreciate details in the image and a\nblurry aspect is normally acceptable.\nWhen deciding the subtile size, it is important to find a tradeoff between an ade-\nquate load control and a high transfer rate. The smaller the subtile size, the higher the\naccuracy to measure the update time. Even though the subtile update time is strongly\ndependent on the hardware used, the smaller sizes typically have a very poor efficiency.\nOur experience has shown that subtile sizes of 128 \u0002 128 give the best performance.\nConcentric Rings Update\nBecause of the previously mentioned update time limit, textures in the stack are not\nalways completely updated. It is necessary to decide when a texture is updated with\nenough data to be applied. The simple approach is to exclude a texture from use in the\nstack until it is completely updated. The problem here is that every time the center of\ndetail is moved the distance of a subtile, it will be invalidated until being completely\nupdated again. This problem is reduced by applying the texture even though only a\npartial area of the full texture is loaded.\nYou update the subtiles of each texture in concentric rings, innermost to outer-\nmost, so the coverage grows as the subtile rings are updated (see Figure 5.8.5). This\nway, the texture is useful from the moment it begins to have some valid subtiles.\nBeginning from the center, the highest interest zone is available sooner. Also, the cen-\nter subtiles are the ones with higher life expectancy.\n5.8\nMapping Large Textures for Outdoor Terrain Rendering\n441\n\n\nPseudocode\nThe virtual texture update is summarized in the following pseudocode:\nCompute the center of detail position\nFor each texture level of the stack\nupdate the subtile validity matrix\nFor each texture level of the stack from coarser to finer detail\nFor each subtile of the level (innermost to outermost)\nand while update time limit is not surpassed\nIf the subtile state is invalid\nCompute the address of the disk tile\nRequest the tile to the RAM tile cache\nIf the tile is cached \nUpdate the subtile in all mipmap levels\nSet subtile state to valid\nRendering Issues\nGeometry management algorithms can be adapted and used with the described tech-\nnique. There are two possible ways to map a virtual texture to a geometry model. The\nfirst way, considering the geometry model is divided into patches, is to apply the\nfinest available texture that covers each geometry patch. In this case, you would follow\nthese steps:\nFor each geometry patch\nApply the finest texture level that covers the patch\nCompute the texture coordinates for the selected texture\nDraw the patch\nThe second way is to select each texture level, asking for its coverage and drawing\nthe geometry covered by the selected level but not for the finer ones. In this case, you\nwould follow these steps:\n442\nSection 5\nGraphics \nFIGURE 5.8.5\nCircular update.\n\n\nFor each texture level\nSelect and apply the texture\nCompute the texture coordinates for the selected texture\nCompute geometry covered by the level but not finer ones\nDraw the geometry set computed\nNo matter which way is used, texture coordinates must be computed for every\nvertex of the geometry. These coordinates are computed for the finest level of the vir-\ntual texture. Because each texture level from the stack covers half the virtual space of\nthe coarser one, you need to scale the texture coordinates computed to translate it to\nthe virtual texture level applied. The scale factor for level i is 2 l-i-1. The toroidal updat-\ning of the textures in the stack assures that the mapping will be correct if the texture\nrepeats (see Figure 5.8.6).\n5.8\nMapping Large Textures for Outdoor Terrain Rendering\n443\nFIGURE 5.8.6\nToroidal update and mapping example.\nThe texture coordinate computation just described can be done in several ways. For\nstatic geometry, texture coordinates can be precomputed and stored statically in texture\ncoordinate arrays. This way, all the computation can be done with the texture matrix\nscaling and texture repeat mode, so no shaders are needed at all. Only the standard fixed\nfunction graphics pipeline available in both OpenGL and DirectX is needed.\nIn the case of dynamic geometry, texture coordinates must be computed every\ntime the vertices are modified. In both cases, but especially with dynamic geometry, it\nis very helpful to automatically compute the texture coordinates in a vertex shader.\nThis way, you avoid their computation in CPU, the transfer from main memory to\nvideo memory, and the storage for texture coordinate arrays in video memory. The\nfollowing pseudocode shows how to calculate texture coordinates:\nL: left texture limit, R: right texture limit,\nT: top texture limit, B: bottom texture limit,\n(x,y,z): vertex position, i: virtual texture level selected\nscale = 2l-i-1\nu = scale * (x-L)/(R-L)\nv = scale * (y-B)/(T-B)\n\n\nTexture coordinate computation can include additional transformations in case\nthere are different coordinate systems for the texture database and the geometry data-\nbase. By using only one GPU texture stage for the mapping of the virtual texture, this\nallows you to easily combine the virtual texture with other virtual or regular textures,\neach one bound to a texture stage.\nResults\nThe presented technique has been tested using a proprietary terrain navigation system\nwith a data set containing a virtual texture of aerial terrain photographs covering an\narea of about 250 \u0002 200Km [Santi07]. The resolution of this image is 0.5m per texel,\nwhich is a stack of 19 levels.\nFigure 5.8.7 shows the results of a stress test executed on a low-end computer\nusing a programmed flight at 3000Km/h over the terrain, using a large clipsize (2048\nsquare texels), and an update time limit of only 1ms.\n444\nSection 5\nGraphics \nFIGURE 5.8.7\nTest results.\nTable 5.8.1\nSystem Configuration for Testing\nGraphics hardware\nAGP 8x NVIDIA GeForce 7800 GS\nClipsize\n2048 texels\nTile size\n512 texels\nSubtile size\n128 texels\nUpdating-time limit\n1ms\nVirtual texture size\n~ 1M  \u0002 1M\nVirtual texture color depth\n24 bits (RGB888)\nFiltering\nTrilinear\nAnisotropic filtering\n4x\nFlight speed\n3000Km/h\n\n\nFigure 5.8.7 shows the graphs for a six-second interval, using the configuration\nshown in Table 5.8.1. The first graph shows the time used by the system to upload\ntexture subtiles to VRAM. The system attempts to limit update times to 1ms in order\nto leave time to process the rest of the application.\nThe second graph shows the completeness of the texture stack which can be used\nas a measure of the quality of the texture shown by the system. In spite of the stress\nconditions of the test, it holds a completeness level of about 75%, that means a tex-\nture detail of 1m per texel.\nTable 5.8.2\nStatistics\nMin.\nMax.\nAvg.\nStd. Dev.\nSubtiles per frame\n0\n4\n2.15\n1.55\nSubtiles load (ms)\n0.19\n1.09\n0.33\n0.09\nCompleteness (%)\n72.83\n77.69\n75.53\n0.86\nTable 5.8.2 shows some interesting statistics, such as an average of two subtiles\nuploaded to video memory per frame or an average of 0.33ms used per frame for updat-\ning. Tests with more favorable conditions, such as reducing the clipsize to 1024 texels,\nmaintain averages over 95% quality (0.5m per texel) during all the executions, even\nusing only 1ms as the update time limit, which proves the efficiency of the technique.\nConclusion\nThe technique described in this gem makes it possible to efficiently manage large tex-\ntures beyond hardware limits. They can be used in a variety of real-time applications due\nto configurable load control. The technique stores the image in tiles that are not used\ndirectly as textures. These tiles are combined in a texture stack that caches the region of\ninterest, following the clipmap idea. You do not need to subdivide the geometry to\nmake the patches match the texture tile boundaries, as occurs in many terrain visualiza-\ntion techniques. Whatever the geometry algorithm, there will always be a texture to map\neach patch.\nThe limitation of this technique is more about patch size than geometry struc-\nture, subdivision, or tessellation. The implementation of this texturing technique has\nbeen successfully used with different geometry algorithms, based on grids as well as\nTINs, with some slight level dropping when using large geometry patches for close\nviews, which can be usually avoided.\n5.8\nMapping Large Textures for Outdoor Terrain Rendering\n445\n",
      "page_number": 468,
      "chapter_number": 48,
      "summary": "This\ngem explains a method that allows the use of huge textures, based on clipmaps Key topics include textures, level, and updates. Classical techniques are based on the tiling of large textures or blending\ndetail textures.",
      "keywords": [
        "texture",
        "virtual texture",
        "Large Textures",
        "texture level",
        "Mapping Large Textures",
        "level",
        "virtual texture levels",
        "Texture coordinates",
        "texture memory",
        "texture stack",
        "Outdoor Terrain Rendering",
        "detail",
        "Virtual",
        "Large",
        "Terrain"
      ],
      "concepts": [
        "textures",
        "level",
        "updates",
        "updated",
        "terrain",
        "geometry",
        "tiling",
        "tiles",
        "tiled",
        "large"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 47,
          "title": "Segment 47 (pages 448-455)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 49,
          "title": "Segment 49 (pages 477-489)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 48,
          "title": "Segment 48 (pages 456-465)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 48,
          "title": "Segment 48 (pages 461-471)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 42,
          "title": "Segment 42 (pages 401-412)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 479-486)",
      "start_page": 479,
      "end_page": 486,
      "detection_method": "topic_boundary",
      "content": "References\n[Montrym97] Montrym, J.S., Baum, D.R., Dignam, D.L. and Migdal, C.J.\n“InfiniteReality: A Real-Time Graphics System,” SIGGRAPH 97, Proceedings of\nthe 24th Annual Conference on Computer Graphics and Interactive Techniques,\npp. 293–302. ACM Press/Addison-Wesley Publishing Co, 1997.\n[Santi07] The SANTI Project Web Page, available at http://videalab.udc.es/santi.\n[Tanner98] Tanner, C.C., Migdal, C.J., and Jones, M.T. “The Clipmap: A Virtual\nMipmap,” In Proceedings of the 25th Annual Conference on Computer Graph-\nics and Interactive Techniques, pp. 151–158. ACM Press, 1998.\n[VTerrain07] The Virtual Terrain Project Website, http://vterrain.org.\n[Williams83] Williams, L. “Pyramidal Parametrics,” SIGGRAPH 83, Proceedings of\nthe 10th Annual Conference on Computer Graphics and Interactive Techniques,\npp. 1–11. ACM Press, 1983.\n446\nSection 5\nGraphics \n\n\n447\n5.9\nArt-Based Rendering with\nGraftal Imposters\nJoshua A. Doss, Advanced Visual Computing,\nIntel Corporation\njoshua.a.doss@intel.com\nG\nraftals are used to express the shape and formation of plants in a formal grammar for\nuse in computer graphics. A close relative of fractals, graftals allow for a compact\nrepresentation of foliage [Smith84]. Graftals have also been used in a non-photorealistic\ncartoon rendering implementation at interactive frame rates [Kowalski98]. Graftal\nimposters are used as a real-time method of drawing cartoon-style plants and fur using\nthe geometry shader in modern GPUs. The particular style we aim to produce is inspired\nby Dr. Seuss’s children’s book illustrations [Seuss71].\nAn artist will provide sketches of graftal imposters along with a set of textures that\nplace the foliage in a scene with a high amount of control over the final look and feel.\nThe imposters are placed along the silhouette edges of the object. See Color Plate 12\nfor a full-color example.\nAssets\nCreating graftal imposters requires a set of assets in addition to the geometry—a tex-\nture atlas, control texture, and a vector field texture. The texture atlas contains three\ntypes of graftal imposters along with several variations of each type. A control texture\nprovides information about what type of graftal imposter should be placed at a certain\nlocation on the mesh. The vector field gives a direction and the color texture provides\ninformation on the coloring of the landscape mesh as well as the graftal imposters. \nTexture Atlas\nThe texture atlas contains the graftal imposter itself. It is created by specifying a few\ndifferent types of graftal imposter types, in different rows. The exterior of each graftal\nimposter should have an RGB and alpha value of zero for all components. As you near\nthe soft edge of the graftal imposter, the alpha value should go smoothly from zero to\none in the middle of the stroke, giving you a smooth transition and reducing any\naliasing effects. The red, green, and blue channels should remain zero until the alpha\nchannel is saturated, as shown in Figure 5.9.1.\n\n\nOnce you reach the middle of the outline, it blends smoothly into solid red, leav-\ning the alpha channel saturated. The inside of graftal imposters will be blended with\nthe color of the underlying geometry while the outside will be blended with the rest of\nthe scene.\nControl Texture\nThe control texture enables the designer to specify where a graftal imposter may \nbe placed and the type of graftal imposter to be drawn. The alpha channel is used to\nindicate areas where no graftal imposters can be drawn. Red should be used where the\ndesigner wants graftal imposters from the first row of the texture atlas, green for \nthe second row, and blue for the third and final row. Using three color channels isn’t\nthe optimal encoding; however, it simplifies the asset-creation process.\nVector Field\nIt is often desirable to indicate a direction, or flow of the graftal imposters. One exam-\nple of this comes when using this technique to create fur (or hair) on a character. You\ncould use the normal as an extrusion direction; however, this limits the amount of\ncontrol the end user has over the final look of the scene. Hair, plants, trees, and so on,\ndon’t always grow at a right angle to the surface from which they protrude. To solve\nthis, you can create the vector field, which gives you the direction. See Figure 5.9.3.\n448\nSection 5\nGraphics \nFIGURE 5.9.1\nThe texture atlas uses red as a color key inside of the graftal\nimposter and the alpha channel to smoothly blend the graftal imposter with\nthe rest of the scene.\n\n\nTo create the vector field, you leverage existing digital content-creation applica-\ntions and plug-ins. After creating a mesh of the desired resolution, the designer saves it\nand substantially reduces the tessellation of the mesh. You want to be sure to preserve\nthe original normals, as they are used in another step and passed with the mesh. Next,\nthe designer manipulates the normals to indicate which direction the graftal imposters\nshould go; this can be done on a per-face basis or by selecting several normals at the\n5.9\nArt-Based Rendering with Graftal Imposters\n449\nFIGURE 5.9.2\nThe control texture indicates coverage and row selection.\nFIGURE 5.9.3\nThe vector field texture indicates the\ndirection of the graftal imposters.\n\n\nsame time. Once the manipulation is complete, these new values can be saved using a\nnormal map plug-in creating the vector field. The result will look like Figure 5.9.3.\nColor Texture and Mesh\nThe color texture is used to indicate the color of the mesh as well as the internal color-\ning of the graftal imposters. Figure 5.9.4 is the color texture for the scene. This tech-\nnique creates the graftal imposters along the edges. Large differences in edge length\nresult in visible irregularities in the width of the graftal imposters; therefore, the mesh\nshould contain triangles of roughly uniform size. In areas with a high level of detail\nwhere extremely small triangles are required, it might be best to use the control texture\nto omit the creation of graftal imposters. \n450\nSection 5\nGraphics \nFIGURE 5.9.4\nThe color texture is used to color the\nbase mesh and the graftal imposters.\nRuntime\nYou can now use the assets created in the previous step during the runtime compo-\nnent of the algorithm. This implementation of graftal imposters requires the use of a\nprogrammable graphics language with a geometry shader. First, you draw the original\nmesh and apply the color texture. Next, you use the geometry shader to determine\nwhere to place the graftal imposters as well as which type of graftal imposter to place.\nFinally, you use the pixel shader to give the final color to the graftal imposters and\nblend them with the rest of the scene.\n\n\nThe control texture was created earlier to dictate where you can create graftal\nimposters as well as what type of graftal imposter to draw in a given area. You need to\ntest the triangle to determine whether the primitive is eligible for a graftal imposter\nand assign a type if it is.\ntexCoordCentroid = ( vertex1uv + vertex2uv + vertex3uv ) / 3;\ncontrolSample = controlTexture.sample( sampler, texCoordCentroid );\nif( controlSample.a == 1 )\nif( controlSample.r == 1)\nglyphType = 0;\nelseif( controlSample.g == 1)\nglyphType = 1;\nelse\nglyphType = 2;\nSampling a texture from within the geometry shader is allowed using the unified\ninstruction set provided with Direct3D 10. You need to choose a point at which to sam-\nple the texture, since you have access to multiple vertices. The previous pseudocode\nshows how you can sample the control texture using the centroid of the triangle cur-\nrently being processed.\nNow that you know the triangle is eligible for graftal imposter(s), you need to test\neach edge to see whether it is a silhouette edge. A silhouette edge is an edge that’s shared\nby both a front and a back facing triangle. In order to test an edge to see whether it is \na silhouette edge, calculate the dot product of the face normal N1,N2 with the view\ndirection V for both faces and test to see if the signs differ [Lake00].\n(5.9.1)\nTo create the new geometry at the silhouette edge, you extrude vertices V0 and V1\nin a direction D obtained from the vector field sampled using the texture coordinates\nat the midpoint M of the edge. See Figure 5.9.5.\n(\n) (\n)\n0\nN\nV\nN\nV\n1\n2\n•\n∗\n•\n≤\n5.9\nArt-Based Rendering with Graftal Imposters\n451\nFIGURE 5.9.5\nNew vertices are created by extruding each vertex\nalong the edge where graftal imposters are desired.\n\n\n//Recreate the original vertex V0\nPosition = Input.Position;\nOutput.Position = mul(Position, ObjectToProjection);\nOutput.GraftalImposterColor = V0Color = ColorTexture.sample\n(sampler, Input.V0.Texcoord);\n. . .\nAppendVertex();\n//Create a new vertex in the appropriate direction\nPosition = Input.Position * Direction + GraftalHeight;\nOutput.Position = mul(Position, ObjectToProjection);\nOutput.GraftalImposterColor = V0Color;\n. . .\nAppendVertex();\n//Recreate the original vertex V1\nPosition = Input.Position;\nOutput.Position = mul(Position, ObjectToProjection);\nOutput.GraftalImposterColor = V1Color = ColorTexture.sample\n(sampler, Input.V1.Texcoord);\n. . .\nAppendVertex();\n//Create final new vertex, finishing the quad\nPosition = Input.Position * Direction + GraftalHeight;\nOutput.Position = mul(Position, ObjectToProjection);\nOutput.GraftalImposterColor = V1Color;\n. . .\nAppendVertex();\nThis pseudocode shows how to create the graftal imposter surface as well as sam-\npling the color texture in order to shade the graftal imposter. Selecting the color once\nper incoming vertex allows you to have a graftal imposter that crosses a color bound-\nary, because the value is interpolated as it is passed to the pixel shader. \nNext, you assign texture coordinates to the newly created geometry to place the\ngraftal imposter on the newly created surface by indexing into the texture atlas. You\nuse the graftal imposter type G to index into the correct row of the texture and a\npseudorandom value such as a sample into a noise texture to determine the column C.\nNC is the number of variations, or columns, the texture atlas contains (see Equations\n5.9.2–5.9.5).\n(5.9.2)\n(5.9.3)\nuv\nC\nN\nG\nV\nc\nNew\n0\n3\n=\n,\nuv\nC\nN\nG\nV\nc\n0\n3\n1\n3\n=\n+\n,\n452\nSection 5\nGraphics \n\n\n(5.9.4)\n(5.9.5)\nThe final step is to sample the texture atlas using the texture coordinates calcu-\nlated in the geometry shader. You want to have a smooth transition from the graftal\nimposter’s black outline to the internal color passed in by the vertex shader. To accom-\nplish this, the red channel of the result is masked off and the sample is linearly inter-\npolated with the color value passed in via the geometry shader. The red channel is\nused as the blending factor in the interpolation.\n//coloring the graftal imposter\nAtlasColor = AtlasTexture.Sample(Sampler, \nGraftalImposterTextureCoords);\nRedZero = float3(0,AtlasColor.gb);\nGraftalImposterColor = lerp(RedZero.rgb, IncomingColor.rgb, \nAtlasColor.rrr);\nGraftalImposterColor.a = AtlasColor.a;\nThe incoming color is interpolated across the two vertices that you sampled within\nthe geometry shader. You blend the graftal imposter’s soft edge by doing a linear inter-\npolation with the red channel masked out. Preserving the alpha value enables you to\nblend the outside of the smooth edge with the underlying landscape color. See Color\nPlate 12 for a full-color example of rendering with graftal imposters.\nAcknowledgements\nThe author would like to thank Jeffery A. Williams, Rahul Sathe, David Bookout,\nNico Galoppo, Adam Lake, and the Advanced Visual Computing team at Intel for\ntheir assistance, support, and contributions. \nConclusion and Future Work\nThis gem has shown how to create a scene in a style similar to that found in Dr. Seuss’s\nchildren’s books. The technique utilizes the new technology capabilities of geometry\nshaders in this GPU-centric technique, leaving more CPU cycles for game logic and\nother tasks. Currently, we are applying graftal imposters only to the silhouette edges. In\nour future work, we would like to automatically generate the vector field used for\nextrusion directions without an artist having to encode it for the entire geometry. \nWhen implementing this technique for production, a couple of additional fea-\ntures may be desired. Adapting the introduction and removal of graftal imposters to\nprovide for inter-frame coherence by scaling or “fading” in the graftal imposters is one\npossible solution. Another important consideration is handling of z-fighting.\nuv\nC\nN\nN\nG\nV\nc\nc\nNew\n1\n1\n3\n=\n+\n,\nuv\nC\nN\nN\nG\nV\nc\nc\n1\n1\n3\n1\n3\n=\n+\n+\n,\n5.9\nArt-Based Rendering with Graftal Imposters\n453\n",
      "page_number": 479,
      "chapter_number": 49,
      "summary": "This chapter covers segment 49 (pages 479-486). Key topics include color, textures, and imposters.",
      "keywords": [
        "Graftal Imposters",
        "Graftal",
        "Imposters",
        "texture",
        "color texture",
        "create graftal imposters",
        "Color",
        "texture atlas",
        "graftal imposters requires",
        "control texture",
        "graftal imposter types",
        "Annual Conference",
        "Creating graftal imposters",
        "Graphics System",
        "graftal imposter surface"
      ],
      "concepts": [
        "color",
        "textures",
        "imposters",
        "creating",
        "create",
        "edges",
        "vertex",
        "channels",
        "direction",
        "direct"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 48,
          "title": "Segment 48 (pages 469-476)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 43,
          "title": "Segment 43 (pages 424-431)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 42,
          "title": "Segment 42 (pages 401-412)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 57,
          "title": "Segment 57 (pages 550-561)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 36,
          "title": "Segment 36 (pages 347-355)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 487-494)",
      "start_page": 487,
      "end_page": 494,
      "detection_method": "topic_boundary",
      "content": "References\n[Doss07] Doss, Joshua A. “Inking the Cube: Edge Detection with Direct3D 10,”\nGame Developer Magazine, June/July 2007, pp. 13–18.\n[Kowalski98] Kowalski, Michael A., et. al. “Art-Based Rendering of Fur, Grass, and\nTrees,” Proceedings of the 26th Annual Conference on Computer Graphics and\nInteractive Techniques, SIGGRAPH 1998, pp. 433–438.\n[Lake00] Lake, Adam, et. al. “Stylized Rendering Techniques for Real-Time 3D Ani-\nmation and Rendering,” Proceedings of the 1st International Symposium on\nNon-photorealistic Animation and Rendering, NPAR, 2000, pp. 13–20.\n[Rost06] Rost, Randi J. OpenGL Shading Language, Addison Wesley, 2006.\n[Seuss71] Seuss, Dr. The Lorax, Random House, Inc., 1971.\n[Smith84] Smith, Alvy Ray. “Plants, Fractals, and Formal Languages,” Computer\nGraphics, Vol. 18, No. 3, SIGGRAPH 1984, pp. 1–10.\n454\nSection 5\nGraphics \n\n\n455\n5.10\nCheap Talk: Dynamic \nReal-Time Lipsync\nTimothy E. Roden, Angelo State University\ntroden@angelo.edu\nG\name developers are increasingly using lipsyncing for in-game 3D characters. One\nproblem is that getting lipsyncing up and running can be both time-consuming\nand expensive. A custom solution may involve valuable programmer time while a\nmore expedient method, involving purchased middleware, can have other drawbacks.\nParticularly for developers wanting to experiment with lipsyncing, perhaps as part of\na proof-of-concept demo, a quicker, less expensive solution is desirable. Fortunately,\nyou can incorporate lipsyncing into a game on the cheap and in a minimum amount\nof time. The result is at least adequate for a proof-of-concept and might be sufficient\nfor a packaged game. This gem explains a method for quick and easy lipsyncing.\nRequirements\nIn order to use this method, several general requirements need to be met. First, you need\na 3D character. Because the example animates the lips, you need at least a pair of lips\nand preferably an entire head. This gem’s examples use a head generated with Singular\nInversion’s FaceGen® software. The head model we are using is shown in Figure 5.10.1.\nThe model consists of 7,341 vertices and 12,960 triangles, not including the hair.\nThe head model needs to have some parametric controls for mouth positions that\ncan be manipulated dynamically. A set of morph targets works great. If you are not\nversed in how morph targets work, Lever provides a good explanation [Lever02]. A\nnice thing about the FaceGen® models is that they come with a large set of morph tar-\ngets for both facial expressions and lip positions, which correspond to various basic\nunits of speech. As shown in Figure 5.10.2, the head used here has 16 morph targets\nfor visemes, which are visual representations of speech such as “aah” and “ee.” Watt\nand Policarpo describe visemes as the basic units of visual speech that are described by\nextreme lip shapes, which correspond to basic auditory speech units [Watt03]. A set\nof visemes constitutes a minimally distinct set representing the sounds in a language.\nYou can probably imagine more lip positions than the 16 shown in Figure 5.10.2.\nHowever, this minimal set is actually quite good for the purposes here and will allow\nyou to generate very convincing lipsync animation.\n\n\n456\nSection 5\nGraphics \nFIGURE 5.10.1\nA head model generated using Singular Inversion’s\nFaceGen® software.\nFIGURE 5.10.2\nThe 16 visemes used here, each\nshown at its extreme (1.0) morph. See Color Plate 13.\n\n\nYou’ll need the ability in your program to independently adjust each of the 16\nvisemes using a float value that ranges from 0.0 to 1.0. Values of 0.0 effectively turn\noff the viseme, whereas values of 1.0 mean the viseme is at full strength. Figure 5.10.3\nillustrates how a value of 0.0 adds nothing to the mouth position, whereas higher val-\nues cause the mouth to morph into the desired shape. This example allows any set of\ncombinations. So, for example, you could have the mouth change shape by applying\nthe “aah” viseme at a value of 1.0 combined with 0.5 of the “ee” viseme. In fact, this\nability is crucial to enabling you to generate realistic dynamic lipsync.\n5.10\nCheap Talk: Dynamic Real-Time Lipsync\n457\nFIGURE 5.10.3\nThe “aah” viseme at varying values (from left: 0, 0.33, 0.66, and 1.0).\nFor audio, you can use pre-recorded speech or audio generated speech at runtime,\nsuch as the output of text-to-speech engine. You will also need the text of what is\nbeing spoken. Using a text-to-speech engine works nicely because the audio is gener-\nated at runtime based on a text string, so you get the text and audio at the same time. \nGeneral Procedure\nFor each lipsynced audio sample, the general runtime procedure is as follows:\n1. Translate each word of the text into its corresponding set of phonemes.\n2. Translate each phoneme into its corresponding viseme.\n3. Generate animation data based on the set of visemes.\n4. Start playing the audio.\n5. Use the animation data to drive the 3D model during audio playback.\nThe companion CD-ROM contains source code written in C++ for a static\nlibrary that implements Steps 1 through 3 of this procedure.\nWord to Phoneme Mapping\nPhonemes are different from visemes. Phonemes are the basic distinctive units of how\nspeech is heard in a language. Individual words can be broken down into phonemes\nbased on the individual sounds that make up a word. Visemes, on the other hand, are\n\n\nthe basic visual units of speech. There is a close correspondence between phonemes and\nvisemes. There are typically more phonemes in a language than visemes. That is because\nseveral different sounds may be represented by the same lip position. For example, “s”\nand “z” are audibly different sounds, but the position of the lips can be similar.\nTranslating words into phonemes couldn’t be easier than using the Carnegie Mel-\nlon Pronouncing Dictionary [CMU07]. The CMU dictionary is available online and\ncan be used for any research or commercial purpose without restriction. It is a text file\ncontaining over 118,000 English words and their corresponding phonetic transla-\ntions. For example, the word “hello” translates to the four phonemes HH, AH, L, and\nOW. There are a total of 39 distinct phonemes in the CMU dictionary. Table 5.10.1\nlists each phoneme and an example word found in the dictionary that uses the\nphoneme. Because the dictionary is already in alphabetical order in the text file, it is a\nfairly simple programming task to read the dictionary into an array and perform a\nbinary search to look up words and retrieve their corresponding phonemes.\nTable 5.10.1\nThe 39 CMU Phonemes\nPhoneme\nExample\nTranslation (of the Example)\nAA\nOdd\nAA D\nAE\nAt\nAE T\nAH\nHut\nHH AH T\nAO\nOught\nAO T\nAW\nCow\nK AW\nAY\nHide\nHH AY D\nB\nBe\nB IY\nCH\nCheese\nCH IY Z\nD\nDee\nD IY\nDH\nThee\nDH IY\nEH\nEd\nEH D\nER\nHurt\nHH ER T\nEY\nAte\nEY T\nF\nFee\nF IY\nG\nGreen\nG R IY N\nHH\nHe\nHH IY\nIH\nIt\nIH T\nIY\nEat\nIY T\nJH\nGee\nJH IY\nK\nKey\nK IY\nL\nLee\nL IY\nM\nMe\nM IY\nN\nKnee\nN IY\nNG\nPing\nP IH NG\nOW\nOat\nOW T\n→\n458\nSection 5\nGraphics \n\n\nPhoneme\nExample\nTranslation (of the Example)\nOY\nToy\nT OY\nP\nPee\nP IY\nR\nRead\nR IY D\nS\nSea\nS IY\nSH\nShe\nSH IY\nT\nTea\nT IY\nTH\nTheta\nTH EY T AH\nUH\nHood\nHH UH D\nUW\nTwo\nT UW\nV\nVee\nV IY\nW\nWe\nW IY\nY\nYield\nY IY L D\nZ\nZee\nZ IY\nZH\nSeizure\nS IY ZH ER\nPhoneme to Viseme Mapping\nTranslating phonemes to visemes is a direct lookup based on a table you need to cre-\nate beforehand. The dictionary contains 39 separate phonemes and the 3D model\nused here has 16 visemes. A little creativity is required here to determine the correct\nviseme for each of the phonemes. Probably the easiest way to do this is in front of a\nmirror. Using Table 5.10.1, pronounce each example word and notice the position of\nyour lips as you sound out the particular phoneme in the word. Match your lip posi-\ntion with the closest viseme in Figure 5.10.2. For the purposes of this gem, we will use\nthe mapping shown in Table 5.10.2. \nTable 5.10.2\nPhoneme to Viseme Mapping\nPhoneme\nViseme\nPhoneme\nViseme\nPhoneme\nViseme\nAA\nBig aah\nF\nF,V\nP\nB,M,P\nAE\nAah\nG\nCh,J,sh\nR\nR\nAH\nAah\nHH\nEh\nS\nD,S,T\nAO\nBig aah\nIH\nI\nSH\nCh,J,sh\nAW\nBig aah\nIY\nEe\nT\nD,S,T\nAY\nAah\nJH\nCh,J,sh\nTH\nTh\nB\nB,M,P\nK\nCh,J,sh\nUH\nOh\nCH\nCh,J,sh\nL\nTh\nUV\nOoh,Q\nD\nD,S,T\nM\nB,M,P\nV\nF,V\nDH\nTh\nN\nN\nW\nW\nEH\nEh\nNG\nD,S,T\nY\nEe\nER\nR\nOW\nOh\nZ\nW\nEY\nEh\nOY\nOoh,Q\nZH\nCh,J,sh\n5.10\nCheap Talk: Dynamic Real-Time Lipsync\n459\n\n\nReal-Time Lipsyncing\nAt runtime, for each audio sample you want lipsynced, you have to convert the string\ncontaining the text into phonemes and then into visemes. Using the code supplied on\nthe companion CD-ROM, this consists of making two functions calls. A third func-\ntion is then called to translate the visemes into lipsync animation data that can be\nused to animate the 3D model during playback of the audio. Let’s first examine how\nthe lipsync data is generated.\nSeveral methods could be used that vary in complexity with more complex meth-\nods providing possibly more accurate data. However, for the purposes of this gem,\nlet’s use an easy approach that gives quite remarkable results given its simplicity. The\nidea is to divide the duration of the spoken audio by the number of visemes and\nassign each viseme a time slot to become active during audio playback.\nFor example, Figure 5.10.4 illustrates the word “hello.” The word consists of four\nvisemes. At time 0, you begin to morph the viseme “eh” from 0 to 1 and then back to\n0. Before “eh” becomes inactive, you must begin to morph the “ahh” viseme, and so on.\nThe idea is to overlap the visemes slightly from one to the next. This results in more\nnatural looking lipsync.\nBy varying the amount of overlap, you can achieve some interesting effects. For\nexample, a long overlap period tends to make the speaker appear to slur words together,\nat least visually. A short overlap period produces very distinct visemes as might be\nexpected when someone is angry. Too short or too long of an overlap produces unnat-\nural looking results. \n460\nSection 5\nGraphics \nFIGURE 5.10.4\nThe word “hello” and its corresponding visemes animated over time.\nA few details of word timing need to be addressed in any solution. For multi-\nsentence audio, the text should contain periods to indicate the end of sentences. Each\nperiod can then be assigned a timeslot so the last viseme at the end of a sentence does\nnot bleed over into the first viseme at the start of the next sentence. The amount of\n\n\ntime for this end-of-sentence delay will likely need to be discovered by trial and error.\nThe code on the companion CD-ROM uses 500 milliseconds. If using a text-to-\nspeech engine, one trick is to save a few text-to-speech audio files that contain multi-\nple sentences and then review them in a WAV file editor. Looking at the WAV data, it\nis easy to see the duration of the end-of-sentence delay.\nThere are obvious drawbacks to the proposed solution. Perhaps the biggest prob-\nlem is with actual human voice files. Unlike text-to-speech engines, which typically\nspeak at a constant rate, humans often speak at varying rates even within the same\nsentence. This can be problematic with the simple lipsync algorithm described here,\nbecause it relies on a constant rate of speech. Still, the advantage of this method is the\nlipsync data is generated on the fly, which can be very useful in a rapid prototyping\nenvironment where you want to get lipsync up and running quickly. \nConclusion\nCreating a dynamic real-time system for lipsync animation using the method pre-\nsented is likely a few days work, at most, for an experienced programmer. For better\nresults, the method could be enhanced. One idea is to take into account the coarticu-\nlation effect, which refers to changes in audio for a particular sound as a function of\nwhat sounds have come before and what sounds will follow. Implementation ideas are\ngiven in [Watt03]. \nReferences\n[CMU07] The Carnegie Mellon Pronouncing Dictionary, available online at\nhttp://www.speech.cs.cmu.edu/cgi-bin/cmudict, July 2007. \n[Lever02] Lever, Nik. Real-Time 3D Character Animation with Visual C++, Focal\nPress, 2002.\n[Watt03] Watt, Alan, and Policarpo, Fabio. 3D Games: Animation and Advanced Real-\nTime Rendering, Vol. 2, Addison-Wesley, 2003.\n5.10\nCheap Talk: Dynamic Real-Time Lipsync\n461\nON THE CD\n",
      "page_number": 487,
      "chapter_number": 50,
      "summary": "This chapter covers segment 50 (pages 487-494). Key topics include lipsync, phonemes, and word.",
      "keywords": [
        "visemes",
        "phonemes",
        "Lipsync",
        "Dynamic Real-Time Lipsync",
        "audio",
        "word",
        "speech",
        "Inking the Cube",
        "Edge Detection",
        "Animation",
        "Rendering",
        "Dictionary",
        "aah",
        "Dynamic Real-Time",
        "lipsync animation"
      ],
      "concepts": [
        "lipsync",
        "phonemes",
        "word",
        "audio",
        "time",
        "timing",
        "speech",
        "contains",
        "animation",
        "animate"
      ],
      "similar_chapters": [
        {
          "book": "Game_Engine_Architecture",
          "chapter": 27,
          "title": "Segment 27 (pages 526-548)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "makinggames",
          "chapter": 6,
          "title": "Segment 6 (pages 43-50)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 28,
          "title": "Segment 28 (pages 549-568)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 55,
          "title": "Segment 55 (pages 536-543)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 4,
          "title": "Segment 4 (pages 64-82)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 495-502)",
      "start_page": 495,
      "end_page": 502,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank \n\n\n463\nS E C T I O N\n6\nNETWORKING AND\nMULTIPLAYER\n\n\nThis page intentionally left blank \n\n\n465\nIntroduction\nDiana Stelmack\nT\nhe number of game genres that are using multiplayer gameplay is growing by\nleaps and bounds. The accessibility of the Internet is reaching more platforms\nthan ever before. The consoles are getting in the act. Console makers are providing\nInternet services to entice their customers to play online with their friends. All this\nnetworking means there is a growing need for network programming, and all these\ngenres and services mean there are more game programmers who need to interface\nwith networking. What does this mean? This means that the complex systems that\ncome together to form games need to have more clearly defined interfaces for non-\nnetworking programmers to use, tools to help find those bugs during crunch time,\nmethodologies in place to deal with security issues as they arise, and so much more.\nThis section contains some gems that just might help you address one or more of\nthese issues.\nThe first gem, by Hyun-jik Baeb, describes a technique called High Level\nAbstraction, or HLA. This technique describes a tool that could be developed to make\nit easier for the non-networking programmer to interact with the networking engine.\nWhether you are a network programmer trying to make it easier, or a non-networking\nprogrammer who wants it easy, take a look at this gem.\nAs we all know, if there is a program running on a machine, there is someone that\nwill try to hack it. Keeping up with network security is a never-ending job. This\nmeans that network programmers need to consider a strategy for keeping the player’s\ninformation safe. It is busy on that “Information Superhighway” and consumers don’t\nknow how many stops there really are between their PCs and the hosts they are con-\nnecting to. The second gem, by Jon Watte, explores the myriad of security approaches\nand presents a well-rounded solution to address most security needs of today.\nTake a game with a lot of simulation. Slow down the frame rate to do lots of cool\ngraphics. Now, for fun, add network latency to data that impacts the simulation, and\nhence the rendering of the scene. By the way, now the multiple human players who exist\nin the networked session are shooting at each other and someone wants credit for that\nkill. All of this involves a lot of network traffic, all of which needs to get from Point A to\nPoint B in a reasonable amount of time with reasonable accuracy. Put in a breakpoint,\nand that can be the end of that testing session, unless you happen to have a smart packet\nsniffer. The third gem, by David Koenig, explores the mechanism to create a game-\nspecific packet sniffer to make finding those network issues easier. Understanding the\ndata is half the battle when you are debugging a gameplay issue on the network.\n\n\nThis page intentionally left blank \n\n\n467\n6.1\nHigh-Level Abstraction of\nGame World Synchronization\nHyun-jik Baeb\nO\nne of the important roles of networked gaming hosts is communicating with\nother hosts to maintain game world synchronization, which involves keeping the\ngame worlds in the same states on all hosts around the world. Synchronization of the\ngame world across multiple hosts requires that game programmers write code that:\n• Collects changes occurring on the local host\n• Packs the changes into one or more messages\n• Transmits the messages to remote hosts\n• Applies the messages to the game world states of the remote hosts\nWriting code for these tasks can be simplified by techniques such as the Remote\nProcedure Call (RPC) system [HyunJik04]. RPC sends or receives messages for the\ncost of writing only one line of code for each message type. However, you still have to\nmanually write routines that manage the game world state, gather information to syn-\nchronize, and send and process it. This work grows quickly if your game designer has\ndeveloped hundreds of diverse battle units that cannot be easily generalized within\nyour program architecture. \nThe power of meta-programming [Wikipedia07] increases productivity over\nwriting code manually. RPC is, of course, a kind of meta-programming technique.\nThis gem introduces another meta-programming technique that synchronizes the\ngame worlds using High Level Abstraction (HLA). RPC abstracts source code lines\nthat exchange messages among hosts in a few lines in the lower code layer; however,\nHLA abstracts them in a higher layer, where the messages are exchanged for synchro-\nnizing the game world state, which is why it’s called high-level abstraction.\nRaw memory synchronization techniques also allow game world synchronization.\nHowever, they are lacking in some aspects:\n• Actual working multiplayer gaming requires latency hiding techniques such as dead\nreckoning [Aronson97]. Synchronizing raw memory has no way of doing this.\n• Raw memory synchronization requires game world data to be stored in a block. It\nis difficult in a situation where automatic memory managers or garbage collectors\nare used.\n\n\n468\nSection 6\nNetworking and Multiplayer \n• Not every last byte of data has to be synchronized precisely in actual multiplayer\ngaming worlds. For example, a unit located far from the viewport might not\nrequire full precision synchronization.\nIn an HLA world, game world synchronization can be done by declaring object\ntypes and synchronization behavior for each of them, instead of writing code that\nsends or receives messages. The actual code is automatically generated by the source\ncode generator provided in this gem.\nThis gem discusses an HLA usage case and explains the overall system of the\ngame world synchronization, and then constructs an HLA system.\nHLA Usage\nThe goal of HLA is to offer a feasible method for abstracting game world synchro-\nnization. It is composed of object type definitions, their synchronization behaviors,\nand a facility that determines the visibility of each object.\nThe definitions for synchronized objects are stored in a source file in a grammar\nyou define. You can name it the SWD (Synchronized World Definition) file. It will be\ncompiled to several source files and then built within your project files.\nThe facility that determines synchronization range will actually be a function.\nYou will be able to extend it differently, as you wish.\nAnatomy of Game World Synchronization\nBecause this example involves writing your own HLA infrastructure, there’s no limita-\ntion when you adopt the HLA technique to your game project. This gem assumes\nclient/server topology, which can be explained like this:\n• The game hosts are composed of one server and the other clients. The server owns\nall game world objects and takes control of them.\n• One or more messages are sent from the clients to the server when a change of\ngame world occurs in a client. Then they are applied to server’s game world and\nbroadcast to other clients for updating.\n• Messages are sent from a server to the clients when a change of game world occurs\nin a server. The clients receive them and update based on the changes.\nFigure 6.1.1 illustrates this collaboration.\nYou can categorize the changes in the game world state. These are the conditions\nfor sending messages:\n• Value modification of an object\n• Creation of an object\n• Destruction of an object\n• Appearance or disappearance of an object, discussed later\n• Every time interval\n\n\nThe condition every time interval is needed when data changes are frequent, but\nevery change is not necessarily propagated. A good example of this is a character’s\nposition. These kinds of changes can be announced by way of an unreliable messaging\nprotocol such as User Datagram Protocol (UDP).\nIn many actual game products, not every object is synchronized for every remote\nhost due to suffocation of network traffic bandwidth. This is critical to a massive mul-\ntiplayer game, where every client holds only a very small area of the game world state,\nwhile their server holds all of it. (The server-side game world is even incomplete on\nany single server if the server system consists of distributed processes.) The synchro-\nnization range every host occupies is determined by rules that are unique to every\ngame project.\nFigure 6.1.2 shows an example that culls the synchronization by a circle defined\nby a radius from the center of each observer. One circle reflects a viewport of a host\nand each star represents an object to synchronize. After an object outside two view-\nports goes into a viewport or a viewport approaches it and envelopes it, the host of the\nviewport gets the message “a new object has appeared” and the host creates an object\nin its game world state. In contrast, when the object leaves a viewport by moving the\nviewport or the object, the “disappear” message arrives to the appropriate host.\nChanges that cause corruption of the game world must be prohibited. For exam-\nple, no one wants his or her loving avatar to be unwillingly moved by opposing forces.\nYou can classify kinds of permissions, as shown in Table 6.1.1.\nTable 6.1.1\nPermissions of World State Modification\nChange by Server \nChange by Local \nChange by Remote \nIs Permitted\nHost Is Permitted\nHost Is Permitted\nServer-only\nYes\nNo\nNo\nServer-and-local-only\nYes\nYes\nNo\nEveryone\nYes\nYes\nNo\n6.1\nHigh-Level Abstraction of Game World Synchronization\n469\nFIGURE 6.1.1\nHLA collaboration diagram.\n",
      "page_number": 495,
      "chapter_number": 51,
      "summary": "The second gem, by Jon Watte, explores the myriad of security approaches\nand presents a well-rounded solution to address most security needs of today Key topics include game, gaming, and networking.",
      "keywords": [
        "Game World",
        "Game World Synchronization",
        "game world state",
        "game",
        "World",
        "World Synchronization",
        "world state",
        "HLA",
        "Synchronization",
        "game world objects",
        "object",
        "game world occurs",
        "messages",
        "server",
        "hosts"
      ],
      "concepts": [
        "game",
        "gaming",
        "networking",
        "synchronization",
        "synchronizes",
        "synchronized",
        "hosts",
        "messages",
        "object",
        "world"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 8",
          "chapter": 46,
          "title": "Segment 46 (pages 434-447)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 47,
          "title": "Segment 47 (pages 448-460)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 42,
          "title": "Segment 42 (pages 343-353)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 54,
          "title": "Segment 54 (pages 513-523)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Building Microservices",
          "chapter": 9,
          "title": "Security",
          "relevance_score": 0.48,
          "method": "api"
        }
      ]
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 503-511)",
      "start_page": 503,
      "end_page": 511,
      "detection_method": "topic_boundary",
      "content": "When the change arrives at the receiver, the game world is not updated with the\nexact data in the change, but rather updated in an interpolated manner. One of the\nfavorite techniques for doing this is dead reckoning [Aronson97].\nNow that you’ve put this world synchronization logic in order, you can imple-\nment the HLA infrastructure that follows it. This is an example of synchronizing\nworld state, so you might want to design your own HLA infrastructure by determin-\ning what synchronization system your game project requires.\nHLA Components\nThe HLA system consists of an SWD compiler and an HLA runtime, as well as the\nSWD files. The grammar of the SWD file depends on which factors are defined as\nimportant for the synchronized objects. The SWD file discussed in this gem has these\nfactors:\n• Object types, AKA classes\n• The classes have, of course, member variables\n• The variables have synchronization behaviors\nNow you can define the major portion of SWD grammar in a simplified BNF\nform, as in Listing 6.1.1. (Note that the symbols and keywords are omitted.)\nListing 6.1.1\nPseudo-Grammar of an SWD File\ncompilation_unit  :=  (first_id,class*)\nclass := (name,member*)\nmember := (behavior,type,name)\nbehavior := (behavior_selection,additional_attribute)\n470\nSection 6\nNetworking and Multiplayer \nFIGURE 6.1.2\nViewports and objects.\n\n\nThe grammar definition compilation_unit is the entry point of parsing.\nListing 6.1.2 is an example of the SWD file that follows the grammar in Listing\n6.1.1. The keywords conditional, periodic, and so on are explained later. \nListing 6.1.2\nAn Example of an SWD File\nworld MedivalWorld\n{\nsynch_class Knight\n{\nconditional float Life;\nperiodic(interval=0.2,duration=1) int MotionState;\nperiodic(interval=0.2,duration=1) float rotationY;\ndead_reckon Vector3 Position,Velocity;\nconditional int Type;\nstatic ItemList Inventory;\n}\nsynch_class Mountain\n{\nconditional int Type;\n// No mountain moves, of course. \nconditional Vector3 Position;\n}\n}\nThe code generated by the SWD compiler does the following:\n• Manages the synchronized objects and collects any changes to them (creation and\ndestruction of objects or member variable changes)\n• Converts the changes to messages and sends them to the networking layer\n• Receives messages from the networking layer and processes them\nThe code generated by the SWD compiler should do everything for world synchro-\nnization in an ideal situation. However, this is inefficient in the practical programming\nworld, when a small change to the HLA source code is needed. So, let’s drive much of\nHLA infrastructure into a common library.\nNow you might be able to imagine how the HLA system fits into program’s archi-\ntecture. This is shown in Figure 6.1.3.\nThe recommended way of compiling an SWD file is putting it into the custom\nbuild configuration, which was introduced in [HyunJik04].\nThe Synchronized Object\nLet’s call the synchronized object SynchEntity for avoiding ambiguity with the term\nobject. A SynchEntity is one of the classes defined in an SWD file.\nA SynchEntity is an ordinary class in practice; however, it has more attributes and\nbehaviors, which are explained next.\n6.1\nHigh-Level Abstraction of Game World Synchronization\n471\n\n\nA SynchEntity exists as the original or the replica, depending on which host has\nthe ownership (and full permission to modify any values) of it. The host that has own-\nership is the subject of the SynchEntity. So SynchEntity has an attribute subject.\nEach object identified across multiple network hosts must be unique. So every\nSynchEntity instance will have a unique identifier value, which is issued by the server.\nEvery member variable in a SynchEntity is actually a property member. The prop-\nerty member consists of a set/get function pair and an alias declaration that binds the\ntwo functions into a virtual member variable. Many contemporary compilers support\nproperty features such as the __declspec(property) keyword in Visual C++. You can\nalso work around this feature’s absence by using a casting operator and an assign oper-\nator even if your compiler doesn’t support the property feature. Listings 6.1.3 and\n6.1.4 show these two cases.\nListing 6.1.3\nUsing the __property Keyword\nclass MyClass\n{\npublic:\n472\nSection 6\nNetworking and Multiplayer \nFIGURE 6.1.3\nHLA activities within a program’s architecture.\n\n\n_ _declspec(property(get=getX,put=setX)) int X; \nvoid setX(int);\nint getX();\n};\nListing 6.1.4\nUsing a Casting Operator and an Assign Operator\nclass XType\n{\npublic:\nXType();\nXType(int value); // takes a value into self\nXType& operator=(int value); // takes a value into self\noperator int(); // outputs the internal value\n};\nclass MyClass\n{\npublic:\nXType X;\n};\nThe synchronization behavior for each member variable can be defined in an\nSWD file. The SWD compiler then generates appropriate source code depending on\nwhich behavior is defined for each member variable. Some of the code may monitor\nto see if any changes are made to the variable. You can get better performance by sub-\nstituting it with code similar to Listing 6.1.5, which can help to quickly skip compar-\nisons when there are no changes.\nListing 6.1.5\nFlagging a Variable as Changed While Assigning a Value\nvoid SetXXX(int newVal)\n{\nm_maybeChanged=true;\nm_value=newVal;\n}\nThe synchronization behavior to be bound to a SynchEntity member variable is\ntypically one of static, conditional, periodic, or dead reckoning. \nStatic behavior means it is never synchronized. If there were no the static behavior,\nyou should define a class derived from the SynchEntity just for adding member vari-\nables that don’t have to be synchronized. \nConditional behavior means that the value is synchronized when its value changes.\nThis is the most commonly used behavior; however, it can flood network traffic if the\nvalue changes are too frequent. \nPeriodic behavior resolves the potential problems with conditional behavior by\nsending the value at specified intervals. This behavior needs send interval value and\n6.1\nHigh-Level Abstraction of Game World Synchronization\n473\n\n\nsend duration. If the value of a periodic behavior member variable changes, it will be\nsent to remote hosts in the interval of send interval value until the send duration time\nelapses. Assuming, for example, that you set the send interval to 0.2 second and the\nduration to 1 second for a periodic behavioral variable, the value will be sent to\nremote hosts five times every 0.2 seconds. Periodic behavior is typically used together\nwith unreliable messaging protocols such as UDP.\nListing 6.1.6 is an example of a conditional behavioral member variable that is\nused in the SWD file, whereas Listing 6.1.7 shows its compiled code. \nListing 6.1.6\nAn Example Conditional Behavioral Member Variable Used in an SWD File\nsynch_class Knight\n{\nconditional int life;\n<...and more...>\n}\nListing 6.1.7\nGenerated Code for the Conditional Behavioral Member Variable\nclass Knight\n{\nprivate:\nint m_private_life;\nbool m_private_life_changed;\ninline void set_life(int value)\n{\nif(value!=m_private_life)\n{\n// A variable whose *_changed \n// is true will be broadcasted soon.\nm_private_life_changed=true;\nm_private_life=value;\n}\n}\ninline int get_life(int value)\n{\nreturn m_private_life;\n}\npublic:\n__declspec(property(get=get_life,put=set_life)) int life;\n<...and more...>\n};\nDead reckoning behavior allows you to hide the jittering values that occur due to\nnetwork latency. A simple dead reckoning model involves three variables to reference:\nthe actual value of the sender, the predicted value of the receiver side, and the interpo-\nlated value. So the SWD compiler should generate these three variables for each dead\nreckoning behavioral variable.\n474\nSection 6\nNetworking and Multiplayer \n\n\nThe flag that indicates whether the value is changed (m_private_life_changed in\nListing 6.1.7) is then used for collecting change information from the game world.\nOne simple model is to iterate over each SynchEntity and gather the changed ones by\nreading the flag. Because the HLA runtime itself cannot know what the flag values\nare, the iteration routine should be generated by the SWD compiler. Listing 6.1.8\nshows an example for the variable in Listing 6.1.6.\nListing 6.1.8\nGenerated Code That Identifies the Change and Collects It to the Output\nMessage Object\nclass MedivalWorld_Runtime\n{\npublic:\nvoid GatherTheChangeToMessage(SynchEntity* entity,\nCMessage &outputMessage)\n{\n// the identifier SynchEntity_Knight is\n// generated enumeration value from the SWD compiler.\nif(entity->GetType()==SynchEntity_Knight)\n{\nKnight* typedEntity=(Knight*)entity;\nif(typedEntity->m_private_life_changed)\n{\noutputMessage.Write(typedEntity->m_private_life);\ntypedEntity->m_private_life_changed=false;\n}\n<...and more...>\n}\n}\n};\nOne more part to investigate is the routine that receives messages from other\nHLA runtimes and applies them to the local game world. This task, which is called\ndeserialization, is mentioned in [HyunJik04].\nCommunication Between HLA Runtimes\nThe major cases during world synchronization that were classified here are SynchEn-\ntity creation, destruction, appearance, disappearance, and value change. Each of these\ncases corresponds to a messaging sequence.\nAlmost all SynchEntities are created only after the server decides that a creation is\nnecessary (that is, creating the object in the server side at first) and its event is broad-\ncast to the clients. Then the received client creates the replica of the new SynchEntity\nafter receiving the message. The required parameters for creating the SynchEntity are\nits ID and its initial member variable values. These values are serialized to a message\nand then sent to the clients that need to know about the newly created SynchEntity.\nSynchEntities that are trivial in presence but sensitive in performance (machine\ngun projectiles, for example) can be created by the client side even if the server does\n6.1\nHigh-Level Abstraction of Game World Synchronization\n475\n\n\nnot permit it yet. In this case, the client first creates it and notifies the server, and then\nthe rest is the same as before. The identifier value of a SynchEntity that’s created client\nside always exists in a value range that has been issued by the server when the client\njoined the game world. [Yongha06] shows more details about doing this.\nThe destruction of a SynchEntity is similar to the creation case, except for the fact\nthat the message type has only the ID of the destructed SynchEntity. A SynchEntity is\ndestroyed at the server, the server sends the event to the clients that view the object,\nand the clients also destroy the replica. The additional sequence needed for a trivial\nSynchEntity is a client-side decision to destroy the entity, at which point the server is\nnotified.\nAll changes in the SynchEntity variables are collected and sent to the clients that\npossess replicas. Messages containing these changes have the SynchEntity ID and a list\nof changed values with their variable ID numbers. Then, each of the clients receives\nthese messages and applies the changes to its replicas.\nConsider one more case: the client first decides to change and announces it to the\nserver, but only if it is trivial enough that a client has permissions to call for the mod-\nifications or the subject of the SynchEntity is the client.\nThe visibility of every SynchEntity can be changed as time goes on because its\nposition or the position of each viewer changes. If one SynchEntity enters a viewport,\nthe client that owns the viewport creates the replica of the SynchEntity after the server\nsends the appearance message with the SynchEntity ID and its serialized values. In\ncontrast, the disappearance message with the SynchEntity ID is received at the client\nand then it removes the corresponding replica. \nViewports in HLA Runtime\nThe viewport in HLA runtime maintains the current state (position and such) as well\nas a network host identifier for sending or receiving messages for synchronization.\nTypically a viewport has a camera position (or more, depending on what radar the\nplayer has) and a host identifier value. SynchEntity and the base class of viewport\nSynchViewport are both abstracted classes.\nA simple implementation of the entity-viewport visibility check is calling a func-\ntion that takes two parameters: a SynchEntity and a SynchViewport. This function is\nnormally called N \u0002 M times, where N is the number of all SynchEntity instances\nand M is the number of all SynchViewport instances. You may want to implement the\nfunction to meet your own needs. For example, your method could be based upon\ngeographical range, parent-child relationship of each scene graph node, or portal par-\ntition of BSP/PVS. The prototype of this function is shown in Listing 6.1.9.\nListing 6.1.9\nA Function for Entity-Viewport Visibility Determination\nbool IsOneEntityVisibleToOneViewport(SynchViewport *viewport,\nSynchEntity* SynchEntity);\n476\nSection 6\nNetworking and Multiplayer \n\n\nThe client/server topology discussed here allows this functionality to be on the\nserver. So this function exists only on the server side. \nHLA Event Handlers\nYou may need to handle something at the exact time when the world state changes.\nExamples of this are the appear and disappear events of a SynchEntity. These cases are\nuseful for loading just-in-time (JIT) resource files for a character type, for example.\nYou can add these event handler interfaces without any limitation because you are\nusing your own HLA system. You just inject these event handler prototypes and the\ninvoker code into the HLA compiler or HLA runtime source lines.\nConstruction of HLA Runtime\nThe HLA runtime fits in with the structure of what you’ve investigated so far. Keep-\ning that in mind, the HLA runtime’s design is shown in Figure 6.1.4.\n6.1\nHigh-Level Abstraction of Game World Synchronization\n477\nFIGURE 6.1.4\nUML class diagram of major classes of HLA system.\n\n\nHlaServer has these features:\n•\nHlaServer has every instance of SynchEntity_S-derived objects and SynchView-\nport-derived objects as well as an entity-viewport visibility decision maker. (Note\nthat _C and _S postfixes stand for server and client.) \n•\nHlaServer monitors the state of every SynchEntity_S and SynchViewport\ninstance. If a change is detected, HlaServer serializes the changes into several mes-\nsages and sends them to the remote hosts.\n•\nHlaServer interfaces with a networking engine to send or receive messages related\nto world synchronization. \nHlaClient keeps instances of SynchEntity_C replicated from the server. Like\nHlaServer, it also interfaces with the networking engine and has routines for keeping\nthe state of every SynchEntity_C synchronized with the server.\nKnight_S and Knight_C are generated classes from an example class Knight in the\nSWD file.\nClass Knight_C and class Knight_S have members, each of whose type is one of\nthe classes DeadReckonBehavior, ConditionalBehavior, and PeriodicBehavior. These\nclasses help HlaClient and HlaServer determine whether these member variables\nshould be broadcasted. The code in Listing 6.1.7 can become more concise if it uses\nConditionalBehavior class.\nFurther Issues\nThe implementation of the HLA system in this article is just a simple networking\nmodel focused on ease of reading and discussion. These features are worth extending\nbased on the HLA system in this gem:\n• Besides the conditional, periodic, static, and dead reckoning behaviors, there are\nmore models for synchronization. For example, synchronization based on time-\nstamp value. \n• The SynchEntity types discussed so far have no member functions. They could be\nadded to the HLA system by sending event messages to the remote host. There\nare two invocation behaviors—running the member functions only on a host that\nhas the original (this can be in an object-oriented remote procedure call manner),\nor on every host that has the original or replica. This may be specified where the\ninvocation begins or pre-specified in the SWD file.\n• Duplicated definitions in similar classes could be refactored into common\nobjects. This also applies to the SWD files. \n• Optimization of comparison bottlenecks may be helpful for better performance.\nThe HLA in this gem checks visibility for every SynchEntity and every\nSynchViewport, which then results in O(n2) time complexity. You could cull\nsome of them by adding a Boolean variable called “this object is changed” to the\nSynchEntity and SynchViewport classes and use it before the actual comparison.\n478\nSection 6\nNetworking and Multiplayer \n",
      "page_number": 503,
      "chapter_number": 52,
      "summary": "This chapter covers segment 52 (pages 503-511). Key topics include classes, values, and listing. The grammar of the SWD file depends on which factors are defined as\nimportant for the synchronized objects.",
      "keywords": [
        "SWD file",
        "HLA",
        "SynchEntity",
        "SWD",
        "HLA runtime",
        "member variable",
        "SWD compiler",
        "Listing",
        "HLA system",
        "Game World Synchronization",
        "variable",
        "SWD File world",
        "member",
        "game world",
        "world synchronization"
      ],
      "concepts": [
        "classes",
        "values",
        "listing",
        "behaviors",
        "behavioral",
        "change",
        "changed",
        "messages",
        "messaging",
        "members"
      ],
      "similar_chapters": [
        {
          "book": "Game_Engine_Architecture",
          "chapter": 36,
          "title": "Segment 36 (pages 719-740)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 47,
          "title": "Segment 47 (pages 448-460)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 38,
          "title": "Segment 38 (pages 764-785)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "operating_systems_three_easy_pieces",
          "chapter": 5,
          "title": "Segment 5 (pages 33-40)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 512-521)",
      "start_page": 512,
      "end_page": 521,
      "detection_method": "topic_boundary",
      "content": "Conclusion\nIf you find yourself writing a lot of similar code to keep your game world synchro-\nnized, implementing your own High Level Abstraction (HLA) system based on this\ndesign can greatly ease your subsequent efforts at game world synchronization. The\nHLA system introduced in this gem can be a guide for the first step as you write your\nown HLA system.\nReferences\n[Aronson97] Aronson. “Dead Reckoning: Latency Hiding for Networked Games,”\navailable online at http://www.gamasutra.com/features/19970919/aronson_01.htm.\n[HyunJik04] Bae, Hyun-jik. “Fast and Efficient Implementation of a Remote Proce-\ndure Call System,” Game Programming Gems 5, edited by Kim Pallister, Charles\nRiver Media, 2005, pp. 627-642.\n[Wikipedia07] “Meta-Programming,” available online at http://en.wikipedia.org/\nwiki/Meta_programming.\n[Yongha06] Kim, Yongha. “Generating Globally Unique Identifiers for Game Objects,”\nGame Programming Gems 6, edited by Michael Dickheiser, Charles River Media,\n2006, pp. 623-628.\n6.1\nHigh-Level Abstraction of Game World Synchronization\n479\n\n\nThis page intentionally left blank \n\n\n481\n6.2\nAuthentication for \nOnline Games\nJon Watte\ngpg-7@mindcontrol.org\nA\nuthentication for games, where un-trusted clients connect to one or more trusted\nservers, is an interesting special case of general authentication. This article pre-\nsents some alternatives that you should consider when designing authentication for an\nonline game system, and proposes one particular set of internally cohesive design\nchoices.\nIntroduction\nAuthentication is the process of making sure that someone is who they say they are, and\nby extension, that a given communication comes from a given party. For computer\ngames, this comes in two flavors: \n• Game login—Given credentials (a user name and password) match the informa-\ntion against a database of allowed players.\n• Game session—A network packet was sent by the logged in player it says it came\nfrom.\nNote that authentication, consisting of the ability to determine who sent a spe-\ncific message, does not have much to do with encryption, which is the ability to hide\na message from unintended recipients. The one exception is that one kind of cryp-\ntosystem (public/private key systems) has the ability to provide both functions at\nonce. Unfortunately, these cryptosystems are usually computationally expensive, and\nthus are not a great match for real-time, online services like computer games.\nSecuring Game Logins\nTo secure game logins, you need to worry about a few kinds of problems: \n• Insecure passwords—Players may have a password that is a common word (like\n“secret”), the player’s name, or even a blank. Your password setting mechanism\nshould detect weak passwords and require better ones.\n\n\n• Insecure password storage—Are your servers secure? Someone might break into\nthem. If they can read the password in clear text at that point, that’s a problem.\nAlso, are the operators of your system trustworthy? What if you have to lay them\noff or fire one?\n• Sniffed passwords—If you don’t use Secure Sockets Layer or some similar heavy-\nweight encrypted protocol, it’s possible that someone can use a packet sniffer to\nread a password sent in clear text, and then impersonate the user in question.\nAlthough this kind of attack is rare, it has actually happened, typically as part of a\npartial data center compromise.\n• Keyboard sniffers—Some kinds of malware or Trojan programs will install them-\nselves on users’ computers, and then log all the keystrokes that the users make.\nSomeone familiar with the game in question can quickly deduce the login name\nand password used from reading such a log.\n• Uneducated users—In many online games, there are users who will try to get the\naccount name and password directly from communication with other players.\nOnce these “keys” are obtained, the account is typically plundered of any valuable\nvirtual goods, and the password is changed to something random, so the original\nuser can no longer play.\n• Multiple logins—The system should not allow the same user to log in more than\nonce at the same time. Otherwise, a single player will pay for the game and share\nthe login with all his or her friends. Although this is a small bit of lost revenue,\nthe bigger problem comes when you have to ban the account because some of\nthose “friends” didn’t play by the rules. That kind of situation is a customer ser-\nvice nightmare.\nThe main point of this gem is to examine authentication in your client/server\ndesign a little closer.\nOne tradeoff you have to make is whether you want the passwords to be recover-\nable from the database or not. Depending on this choice, you have the following\noptions:\n• Recoverable passwords—If the password is recoverable, you can use the Challenge\nHash Authentication scheme, as described later. However, the passwords are more\nvulnerable when they are recoverable in the database, because an untrustworthy\noperator, or system intruder, might get hold of the list of passwords.\nDon’t store the passwords in clear text in the database. At least scramble them\nusing some key that you build into the code, to make it harder for the casual\ninspector to “accidentally” see the password. There’s still a danger that the pass-\nwords can be compromised, so be vigilant against human factors that can com-\npromise your data.\n• One-way hash passwords—When setting a password, you calculate a one-way hash\nof the password (such as an SHA256 checksum), and store the checksum. When\nusers log in the next time, they give you a password, and you calculate an SHA256\n482\nSection 6\nNetworking and Multiplayer \n\n\nchecksum of that, and compare to the stored checksum. If they match, you assume\nthe password provided is correct.\nThe main benefit here is security on the system side; reading an SHA256\nchecksum will not let anyone know what the actual password is. Finding another\npassword that generates the same checksum is computationally very hard. How-\never, when doing this, you must use Secret Exchange Authentication (described\nlater), which is more vulnerable than Challenge Hash Authentication.\n• Public key infrastructure—If you have a private/public key cryptosystem, such as\nRSA or SSL, you can publish the public key of the game servers, and even hard-\ncode it into the game client executable. The user’s password is then transmitted\nover this link, safe from eavesdropping. The added benefit is that nobody but the\nauthentic server can decrypt the message, so the client has a reasonable assurance\nthat it is talking to the real server, not an impostor. The drawback is significant\ncomplexity in implementation (the best choice here is probably to go with an\nopen cryptosystem library such as OpenSLL).\nChallenge Hash Authentication\nIn Challenge Hash Authentication, the server issues some random number, called a\nchallenge or nonce, to the client. The client computes a hash of this random number\nand the client-side entered password, and sends the hash value back to the server. The\nserver then computes a hash of the remembered challenge value and the stored (plain-\ntext) password, and compares it to what the client submitted. If the hashes match, the\nright password was supplied.\nThere are three main properties of this system: \n• Passwords are not transmitted—Thus someone sniffing the regular login traffic\ncannot determine what the password is.\n• The challenge is specific to each login attempt—Thus, if you sniff the connection,\nyou cannot remember what the hash is and then just re-supply the same hash\nvalue later to log in, because the random challenge generated by the server for a\nspecific login attempt is different each time.\n• The server has the clear-text password—This is a security problem if the server side\nbecomes compromised, but the clear-text password, which is a secret shared by\nboth sides of the communication, can be used to encrypt any data coming\nto/from that particular client. Care has to be taken to use an encryption algo-\nrithm from which the key cannot be too easily recovered—XOR or ROT-13\nwould not be appropriate!\nA common-sense precaution is to use a hash of the clear-text password as a key\nfor the communication, but not the same hash as used for authentication, or the\nbenefit of an “unsniffable” shared secret is lost.\n6.2\nAuthentication for Online Games\n483\n\n\nSecret Exchange Authentication\nIn Secret Exchange Authentication, the server stores a hash of the password. The client\nsubmits a plain-text password, and the server hashes this plain-text password, and com-\npares the hash to the stored hash. If they match, the right password was supplied.\nThere is one strength and two weaknesses in this system: \n• The server doesn’t store the plain-text password—If someone breaks in and steals the\npassword file, it doesn’t matter, because you can’t guess what a password is just by\nknowing its (cryptographic strength) hash. On old UNIX machines, the strength of\nthe cryptography is not that high, so you should still keep your /etc/shadow file\nsecure, but with a 256-bit SHA hash, you should be pretty safe. If you can’t trust\nyour backup operators, or if you get hacked, this is a major benefit!\n• The password is transmitted on each login attempt—If someone can sniff the connec-\ntion, they could recover the password. Thus, you have to secure the login attempt\nusing some kind of encryption—but it’s not clear what you should use as a key to\nachieve good security. The most secure way involves a Diffie-Hellman key exchange,\nwhich is fairly tricky code to implement correctly, but will provide for a secure,\nencrypted channel between two endpoints, without prior exchange of keys. If you\nwanted to protect against a sophisticated attacker inserting himself in the middle of\nthe network, you would additionally have to introduce a public key–based crypto-\ngraphic authentication system, which is a significant additional burden.\n• The server has the clear-text password—Because the client sends the clear-text pass-\nword, the server has at least temporary access to the clear-text password, and can\nuse this as a key for future communication encryption, after the initial login.\nUnfortunately, this means that if someone can impersonate your server, or read\nthe memory of your server process, they can still recover plain-text passwords,\neven if the password storage file itself is secure.\nPublic Key Infrastructure\nIf you have a private/public key cryptosystem, such as RSA or SSL, you can publish the\npublic key of the game servers, or even hard-code it into the game client executable.\nThe user’s credentials are then transmitted over this link, safe from eavesdropping on\nthe wire. An additional option is to generate a private key for the user when setting up\nthe account, storing the matching public key on the server side, and encrypting the pri-\nvate key locally with the user’s password (known as a pass phrase).\nSuch a system has the following properties:\n• The server never sees the pass phrase—Thus, disgruntled employees or server system\nintruders cannot easily steal the credentials through packet sniffing or log skim-\nming. A determined attacker who disassembles the server binary can still get the\ncredentials, but at that point, your entire game is compromised, and you proba-\nbly have bigger problems to worry about.\n484\nSection 6\nNetworking and Multiplayer \n\n\n• The user has a good assurance against server impersonation—As long as the server\nprivate key is not compromised, nobody else can pretend to be your server and\nextract user credentials.\n• The user credentials are not portable—If the user accesses your game from more\nthan one location, he or she needs to make a copy of the private key used for his\nor her game account, so that the client can authenticate itself on logon. This is\nnot something users generally expect, and will likely lead to a customer support\nheadache.\nSecuring Game Sessions\nOnce the player has logged in, your troubles are not over. You often need to transfer a\nplayer from one machine to another, or to allow the player to disconnect from the\nserver (perhaps through crashing) and then re-connect, resuming where the player left\noff. You clearly can’t just let the client claim any identity, and have the server blindly\ntrust that, because it would be trivial for one player to suddenly impersonate another\nplayer. Instead, you have to use one of three techniques: identity by IP address, iden-\ntity by authentication token, or identity by cryptography.\nIdentity by IP Address\nIn this method, the server looks at the source IP address and port number of the arriv-\ning packet, and internally has a table that tells which player is connected on which\naddress/port pair. This is secure, as long as you know that the player will keep sending\nfrom the same port, and as long as you trust that the Internet will not accept spoofed\naddresses in packets—or, if a packet is spoofed, that some round-trip confirmation\nwith the real client can take place.\nSuch round-trip confirmation can come in the form of explicit acknowledgement\nof particularly suspect commands (“surrender game,” for example), or implicitly by\nusing a rotating sequence number starting from a random initial starting point.\nSadly, if you use TCP for your connections, or if you need to hand connections\noff between servers, the port part of the client’s address will not necessarily stay the\nsame. TCP allocates a new port for each connection for each machine it connects to,\nand even UDP can suffer port renumbering when you switch destination machines, if\nit’s behind a non-friendly NAT gateway (although most home NAT routers don’t\nimpose this limitation). \nIdentity by Authentication Token\nWhen the player logs in, the server determines the duration for which the connection\nis good—for example, one hour. The server then calculates a hash of a few pieces of\ndata: the client ID, the expiration time of the login session, and a secret number that\nonly the game server knows. The server then sends a token to the client, which con-\ntains the client ID, the expiration time, and the hash of the three pieces of data.\n6.2\nAuthentication for Online Games\n485\n\n\nWhen the client sends data, it precedes the data with this token. The server picks\napart the identity, time, and hash parts, and recomputes the hash with its internal\nsecret number. If the hash matches the hash in the supplied token, the server knows\nthat the packet comes from the player who initially authenticated with the server, or\nat least that the claimed identity and session duration is one that the server has signed\noff on.\nIf the client crashes and then reconnects, it could read the cookie from disk and\nre-supply it, and as long as the session is still valid, no new authentication would be\nnecessary. If game sessions last more than an hour, the server that the player is cur-\nrently talking to would extend the cookie by half an hour each time the cookie is at\nleast half an hour old by re-generating a new token based on client ID, new expiration\ntime, and a hash of those entities and the server secret number. That way, a client can\ncrash and then keep playing as long as it reconnects within half an hour, without hav-\ning to log in again.\nIdentity by Cryptography\nIf you use a shared secret between the server and the client, such as a plain-text pass-\nword, you can use that secret as a key, or perhaps better, a hash of that password and\nsome known salt or nonce different from that used to authenticate the connection ini-\ntially. Each packet sent by the client contains the client ID in plain text, followed by\nthe packet data, encrypted by the shared secret, followed by a checksum of the (unen-\ncrypted) data.\nWhen the server receives a packet, it looks up the client password in an internal\ntable, decrypts the message, and verifies the checksum. If the checksum doesn’t match,\nthe data was not encrypted with the right password, and thus the packet did not come\nfrom the right client.\nBest practice says that part of the encrypted data should be a sequence number, so\nthat successive identical packets will still encrypt differently, and so that capturing and\nreplaying a packet will have a low likelihood of being accepted for real.\nOther Considerations with Game Sessions\nThe other problems mentioned in part two of this gem also bear mentioning, although\nthe solutions aren’t spelled out in as much detail as with the main topic of the article:\n• Insecure passwords—When the player generates or changes a password, you should\nverify that the password contains at least six characters (and allow up to 24). Addi-\ntionally, verify that the password contains at least one character from each of the\nthree groups—letters, digits, and non-alpha-numeric characters.\n• Insecure password storage—To protect server secrets against malicious internal opera-\ntors, follow best IT practices. Don’t let anyone in the company have access to all the\nservers. Store any plain-text password data in a scrambled format, using some key\nthat’s hard-coded into the executable. Store extra sensitive data, such as credit card\n486\nSection 6\nNetworking and Multiplayer \n\n\ninformation or user home addresses, in a server separate from the main game\nservers, with an additional firewall between game and billing information.\n• Keyboard sniffers—If you worry about keyboard sniffers, make the users enter\ntheir passwords using an on-screen keyboard (point-and-click) instead of using\nthe keyboard. Also beware that a malicious piece of software could read out all\ndata in standard text edit controls, so you might want to use a custom GUI con-\ntrol for reading passwords.\n• Uneducated users—Create a comprehensive set of rules for user conduct and safety,\nand require acceptance when users sign up. However, make sure you boil down the\nmost important bits into quick sound bites like “never give out your password, even\nif someone says they are from our company.” Add one of those sound bites to each\nloading screen, perhaps on a rotating basis, to reinforce the message.\n• Multiple logins—When one session ticket or cookie is generated, invalidate all\nprevious such tickets/cookies. This means that a second login on the same\naccount will kick out the first logged-in user. However, if a user disconnects and\nlogs in again, that user will not be affected, because the old session ticket is no\nlonger used.\nConclusion\nIf you are reading this, it’s a good sign—you care about security and want to do it right!\nA good encryption algorithm to use when both sides know the key (such as when using\nsecret exchange authentication and identity by cryptography identification) is the Tiny\nEncryption Algorithm, which is easy to implement, yet cryptographically strong. True\nsticklers for security recommend only using standardized protocols, such as AES, because\nthey undergo more study and publication, and any weaknesses will thus be known\nsooner and wider, giving you early warning when it is time to change cryptosystems.\nSHA256 is a commonly used and standardized hashing (digest) function, and has\nnot yet shown the weaknesses of the older MD5 hashing algorithm. Other alterna-\ntives are available, such as Tiger (see http://www.cs.technion.ac.il/~biham/Reports/\nTiger/tiger/tiger.html).\nA sufficient implementation of authentication and identity for a cluster of collab-\norating trusted servers (such as for an MMORPG or Virtual World) would look\nsomething like this:\n• At setup, all servers in the cluster share a large random number, known as the\ncluster secret.\n• Client connects to login server using unencrypted TCP or UDP.\n• The server issues a challenge to the client, consisting of a 256-bit random num-\nber (nonce).\n• Client calculates a hash of this number concatenated with the password the user\nenters, and supplies the hash to the server.\n6.2\nAuthentication for Online Games\n487\n\n\n• Server verifies that the hash of the challenge and stored password matches what\nthe client supplied, and issues an authentication ticket consisting of a user ID,\nticket expiration time, and hash of a cluster secret combined with these two\nitems, and supplies the ticket to the client.\n• Login server also generates a random key for use by this client during this session,\nand supplies it to the client. It also records the key for the user, and the expiry time\nof the ticket. The key is encrypted by a key generated by hashing the user password\nand a known salt (say, the string “abcd”) before sending it to the client.\n• Client connects to any server that is part of the game server cluster.\n• Client starts the connection to a new server within the cluster by sending the\nauthentication ticket previously issued.\n• The new server verifies that the ticket has not expired, and that the hash is cor-\nrect. Using the user ID in the ticket, the new server retrieves the encryption key\nfor the client from the login server.\n• The new server and the client also negotiate sequence numbers for future com-\nmunications at this point.\n• Once authenticated, the new server and client exchange data encrypted with the\nsession key, where the encrypted data includes a hash of the data proper (as\nchecksum) and a sequence number. Each of these packets needs to have only the\nclient ID and ticket identifier (a small integer) as a header, not the full authenti-\ncation ticket.\n• Periodically, the server that the client is currently connected to checks whether\nthe session authentication ticket is about to expire; if this is the case, it contacts\nthe login server to get a new ticket and forwards it to the client.\nThis scheme will protect against the dangers of someone sniffing your passwords\non the open Internet, and against the dangers of someone trying to use sniffed packets\nin a playback attack. For a man-in-the-middle attack, the session being compromised\nwould be insecure, but the man in the middle would not gain the authentication cre-\ndentials to re-authenticate at a later time. To make sure there is no tampering in the\nmiddle, you would have to add public/private key encryption and authentication.\nIt is also worth noting that no technique protects against a user looking at all the\ndata sent to his or her client machine—the user controls the machine running the\nclient, so he or she could always inspect the data in memory. This means your game\ndesign has to be cheat-proof, or you must provide incentives for users not to cheat, to\nget around that problem. Authentication and encryption save you only from third par-\nties getting hold of secret information, not the two first parties (the client and server).\n488\nSection 6\nNetworking and Multiplayer \n",
      "page_number": 512,
      "chapter_number": 53,
      "summary": "This chapter covers segment 53 (pages 512-521). Key topics include servers, client, and password. “Fast and Efficient Implementation of a Remote Proce-\ndure Call System,” Game Programming Gems 5, edited by Kim Pallister, Charles\nRiver Media, 2005, pp.",
      "keywords": [
        "server",
        "password",
        "client",
        "game",
        "Hash",
        "Authentication",
        "key",
        "Challenge Hash Authentication",
        "game servers",
        "Online Games",
        "user",
        "Secret Exchange Authentication",
        "Hash Authentication",
        "login server",
        "game client"
      ],
      "concepts": [
        "servers",
        "client",
        "password",
        "authentication",
        "authenticate",
        "authenticated",
        "user",
        "game",
        "hash",
        "hashes"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 56,
          "title": "Segment 56 (pages 532-540)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 46,
          "title": "Segment 46 (pages 434-447)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 47,
          "title": "Segment 47 (pages 448-460)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 9,
          "title": "Segment 9 (pages 67-74)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 54,
          "title": "Segment 54 (pages 513-523)",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    },
    {
      "number": 54,
      "title": "Segment 54 (pages 522-529)",
      "start_page": 522,
      "end_page": 529,
      "detection_method": "topic_boundary",
      "content": "References\nAES, the Advanced Encryption Standard algorithm. The successor to the Data Encryp-\ntion Standard, and the current U.S. Federal Information Processing Standard\nencryption algorithm, available online at http://csrc.nist.gov/CryptoToolkit/aes/\naesfact.html.\nOpenSSL. A high-quality Secure Sockets Layer library, using an Apache-style Open\nSource license, available online at http://www.openssl.org/.\n[Schneier95] Schneier, Bruce. Applied Cryptography: Protocols, Algorithms, and Source\nCode in C, 2nd edition, Wiley, 1995.\nSHA, Secure Hash Algorithm. The successor to MD5, and the current U.S. Federal\nInformation Processing Standard hash/digest function, available online at\nhttp://csrc.nist.gov/publications/fips/fips180-2/fips180-2withchangenotice.pdf.\nTiger. A fast hashing (digest) function, with no known patent encumbrance, available\nonline at http://www.cs.technion.ac.il/~biham/Reports/Tiger/tiger/tiger.html and\nhttp://en.wikipedia.org/wiki/Tiger_(hash).\nX-Tea and Corrected Block TEA. A fast, high-strength, simple-to-implement sym-\nmetric encryption algorithm, with no known patent encumbrance, available\nonline at http://en.wikipedia.org/wiki/XXTEA.\n6.2\nAuthentication for Online Games\n489\n\n\nThis page intentionally left blank \n\n\n491\n6.3\nGame Network Debugging\nwith Smart Packet Sniffers\nDavid L. Koenig, The Whole Experience, Inc.\nyarnhammer@hotmail.com\nI\nn general, most network traffic in games is very sensitive to long delays in between\ngame packets. This can be problematic when attempting to debug network code in\nreal-time. The standard practice is to use a packet sniffer application that collects net-\nwork traffic on the network during gameplay so that it can be later analyzed. Packet\nsniffers give easy access to information about packet source and destination, and other\nnetwork protocol stack information. What these do not provide are the specifics of\nyour game protocol when that data goes out over the network. \nThe Smart Packet Sniffer Concept\nThe concept of a smart packet sniffer or, perhaps better-named, game message sniffer,\nis the idea that you are not just looking at the raw binary data sent across the wire, or\nTCP/IP protocol data. You are looking at more detailed information in a human\nreadable form. In its most basic description, this is a packet sniffer that has specific\nknowledge of the internals of your game protocol.\nThis smart packet sniffer application was developed while working on Greg Hast-\nings’ Tournament Paintball Max’d for PlayStation 2 (GHTP), which was released in\nlate 2006. An engineer who was no longer with the company wrote the baseline game\nmessage system and network code. With no documentation provided for the network\ncode and message system, it was certainly overwhelming to start working with it. The\nfirst place to start is to look over the code. Doing so will give you a good idea of the\narchitecture of the underlying system. Exactly what game messages are sent across the\nwire and when can be very difficult to grasp initially. This is where a smart packet\nsniffer can come in handy.\nAn Example\nOn the GHTP project, the sniffer showed us that our server was sending a large num-\nber of 100- and sub-100-byte player position packets to the clients. With 42 bytes of\nthat consisting of Ethernet frame, IP, and UDP header information, our packet\n\n\nheader overhead was around 40 percent. We were able to improve efficiency by coa-\nlescing our packet data and greatly reducing our overall overhead. By using a standard\npacket sniffer, we probably could have examined the binary data of a number of pack-\nets and come to the same conclusion. It only took a quick glance, with our sniffer, to\nsee exactly what network messages were being sent, and how much bandwidth they\nwere consuming. Needless to say, this saved us a great deal of time.\nGotchas with Traditional Debugging Techniques\nYou don’t want to completely abandon your standard debugging functions when work-\ning with network code. However, you should be aware of the artifacts they can intro-\nduce. What you want to avoid is causing bugs that don’t really exist for end users. This\nis usually the result of changing the code path or changing the code timing. The fol-\nlowing are examples that can result in either of these two issues.\nBreakpoints\nThese are generally the developer’s first line of defense when testing a piece of code for\nvalidity. They allow you to see if an operation is following the expected path. They\nallow you to see the values of important variables and register values. This informa-\ntion is invaluable to a developer. The problem that is introduced when it comes to\nnetwork code is that you are only stopping one side of the simulation. The other side,\nwhich is the other host connected to the game, keeps running. At some point this sec-\nondary host will assume that the connection has dropped and will timeout. Now you\nmay not care about this depending on what type of issues you are trying to debug. If\nyou are just trying to find out if a given piece of code is ever executed, this is a quick\nway to obtain that information. However, if you’re trying to figure out how many\nheartbeat packets are being sent to the server, or which messages are coming out of\norder most frequently, breakpoints quickly lose their potency.\nTracepoints\nThe concept of tracepoints was introduced into Visual Studio with version 8.0, also\nknown as Visual Studio .NET 2005. These allow you to place points in the code that,\nwhen hit, do not necessarily cause the game to halt. You can do all sorts of things. You\ncan choose to halt progress. You can also run scripts, print to the debug window, or\nprint a callstack. Although these are great advances in debugging options, they can\nchange the timing of your code.\nDebug Output\nThis is usually in the form of a call to printf, OutputDebugString, or an equivalent\nfunction. These can be useful for obtaining information such as bandwidth usage per\nsecond, or percentage of packets dropped. The problem is that anytime you add\nadditional code to a given operation, the timing is going to change. With additional\n492\nSection 6\nNetworking and Multiplayer \n\n\ncode comes additional processor instructions. This can cause issues seen in the non-\ninstrumented code to go away, or may introduce other artifacts. Be sure to use caution\nwhen using the debug output pipeline to debug time sensitive code.\nImplementation\nThe base implementation for a smart packet sniffer is simple. The following sections\noutline the basic steps we took when creating our sniffer.\nExpose Network Structures\nThe basics of a smart packet sniffer require that you expose internal game protocol\ninformation. This can usually be done by simply including the same header files for\nboth the game and the sniffer. One suggestion is to set up a shared directory for any\ncode that is common between the game and sniffer. This will help you keep the pro-\ntocol version synchronized between the two applications. \nPacket Acquisition\nYou are going to need a way to pull data off of the network. There are several options\navailable. You can write the code for capturing packets yourself. An alternative, and\nrecommended solution, is to use a third-party library like pcap [Pcap]. This is the\nroute we took with our sniffer. The benefit of using pcap is that the code has been\naround and tested for many years, as well as being Open Source and easy to use.\nPacket Decoding\nOnce you have obtained a group of packets, you are going to want to translate them\ninto game messages. That is where the parsing code comes in. This is basically your\nprotocol codec and what differentiates the smart sniffer from a standard packet sniffing\napplication. In the sample, included on the CD-ROM, we took a plug-in approach.\nThe decoding for our simple example protocol is handled by a DLL, loaded at run-\ntime. This allows you to support as many protocols as you want. It also allows you to\nkeep the specifics of your protocol out of the packet sniffer core code.\nDisplay\nThere are a number of ways you can represent the data. Utilities such as tcpdump\n[Tcpd] use a command-line interface. On the GHTP sniffer, we went with an MFC\nuser interface [Mfc]. The List Control is pretty basic, but lends itself very well to the\ndata we wanted to display. It allows you to set up a simple multi-row, multi-column\nview. There are a number of options out there for building interactive user interfaces.\nDo your homework and find what works best for your project. See Figure 6.3.1.\n6.3\nGame Network Debugging with Smart Packet Sniffers\n493\n\n\nUsing the WinPcap Library\nThe pcap library is used in the Wireshark Open Source packet sniffer among many\nother network tools [Wireshark]. WinPcap is the Windows version of this library. It\nallows developers to easily capture packets being sent across the network. Developers\nonly need to use a small subset of the full library in order to get started. Make sure to\nlook at the sample code on the CD-ROM for a working example of the functions cov-\nered in this section. To save space, and your sanity, I only list function prototypes here.\nRead over the pcap documentation for more in-depth information on these functions.\nEnumerating Devices\nIn order to start capturing packets, you need to define which local network device you\nwant to listen to. First, you need to know what devices are available on your system.\nPcap provides the following two functions for obtaining device information and for\nflushing the memory used for the query.\nint    pcap_findalldevs(pcap_if_t **, char *);\nvoid   pcap_freealldevs(pcap_if_t *);\n494\nSection 6\nNetworking and Multiplayer \nFIGURE 6.3.1\nA view of the user interface of our smart packet sniffer.\n\n\nInitializing Pcap\nBefore you can start capturing packets, you have to initialize pcap. This sets up which\nnetwork device you would like to use for capturing packets. You can set filters for cap-\nture as well. You might, for example, want to filter out everything except UDP pack-\nets. Our packet sniffer sample assumes this to be true.\n//Obtain a handle to the pcap device.\npcap_t\n*pcap_open_live(const char *, int, int, int, char *);\n//Determine the medium for this device. (such as Ethernet)\nint    pcap_datalink(pcap_t *);\n//Compile the packet capture filter from text.\n// (pcap documentation covers the specifics of the filter grammar.)\nint    pcap_compile(pcap_t *, \nstruct bpf_program *, \nchar *, \nint,           bpf_u_int32);\n//Set the packet filter.\nint    pcap_setfilter(pcap_t *, struct bpf_program *);\n//Release the pcap device\nvoid  pcap_close(pcap_t *);\nAcquiring Packets\nThe next step is to set up the pipeline for handling the packets. To do that, we use the\npcap_dispatch function. In the sample code included on the CD-ROM, after initial-\nizing pcap, we set a timer via the SetTimer Windows API function. In the timer han-\ndler, we call the pcap_dispatch function to access the captured packet data.\n//Callback prototype\ntypedef void (*pcap_handler)(u_char *, \nconst struct pcap_pkthdr *, \nconst u_char *);\nint  pcap_dispatch(pcap_t *, int, pcap_handler, u_char *);\nThe packet handler callback is where your code gains access to the packet data.\nThis is where your protocol codec will handle the raw network data and turn it into\nsomething useful.\nSecurity Risk Reduction\nA tool that can decode and display all of the internals of your network code is a great\naid for the engineers working on debugging your protocol. At the same time, it’s also\n6.3\nGame Network Debugging with Smart Packet Sniffers\n495\nON THE CD\n\n\na great tool for those who might want to exploit your protocol. This makes it a poten-\ntially dangerous tool. You should therefore put some thought into how you can limit\nthe potential misuse.\nLimit Deployment\nMake sure that only those who need direct access to the tool can get it. As a network\nengineer, the last thing you want to see is your protocol hacked, on the first day of\nrelease, by a tool you created to make development life easier.\nEncryption\nIf your protocol includes some level of encryption, you may have an inherent\nuntapped line of defense. It is a good idea to provide the ability to disable encryption\nfor ease of debugging. You can do this a number of ways. You can link against an\nunencrypted version for your networking library. Another option is to have a separate\nbuild target that includes a preprocessor define to disable encryption.\nPerhaps your packet sniffer is only able to evaluate packets that are sent out across\nthe wire with encryption disabled. This should help mitigate the risks with developing\na tool like this. Even if it were to make its way to the public channels, users would not\nbe able to use it directly to analyze your network traffic. This solution is not perfect.\nSomeone could analyze the sniffer assembly code to reverse-engineer your internal\nnetwork code structures. This will certainly make it easier for a hacker to find the cor-\nresponding structures in your game binary.\nAn Alternative\nThere are options available if you would rather not write an application from the\nground up. The Wireshark packet sniffer has a plug-in architecture that allows you to\ndefine your protocol specifics. You could, for example, write a packet dissector for your\nprotocol. What this does is expose the internals of your protocol to Wireshark. This\nadds a great deal of extensibility to an already powerful tool. There are a number of\nthird-party packet dissectors that are packaged with Wireshark. For example, there is a\ndissector for the Quake 3 protocol included with the main distribution. As a network\nprogrammer, you should make sure to have a full-featured packet sniffer available.\nSample Code\nThe example on the CD-ROM includes a simple command-line client and server\nsimulation application set, as well as a simple smart packet sniffer application. Full\nsource code is included to all applications. The project files included require Visual\nStudio 2005 and the WinPcap development library.\n496\nSection 6\nNetworking and Multiplayer \n",
      "page_number": 522,
      "chapter_number": 54,
      "summary": "This chapter covers segment 54 (pages 522-529). Key topics include packet, code, and network. The successor to the Data Encryp-\ntion Standard, and the current U.S.",
      "keywords": [
        "Smart Packet Sniffer",
        "packet sniffer",
        "Packet",
        "Smart Packet",
        "pcap",
        "sniffer",
        "Code",
        "Network",
        "network code",
        "packet sniffer application",
        "Game Network Debugging",
        "protocol",
        "Game",
        "standard packet sniffer",
        "Wireshark packet sniffer"
      ],
      "concepts": [
        "packet",
        "code",
        "network",
        "sniffers",
        "protocols",
        "games",
        "data",
        "debugging",
        "debug",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 3",
          "chapter": 56,
          "title": "Segment 56 (pages 532-540)",
          "relevance_score": 0.78,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 57,
          "title": "Segment 57 (pages 541-549)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 10,
          "title": "Segment 10 (pages 77-87)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 15,
          "title": "Segment 15 (pages 135-143)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 23,
          "title": "Segment 23 (pages 457-476)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 55,
      "title": "Segment 55 (pages 530-540)",
      "start_page": 530,
      "end_page": 540,
      "detection_method": "topic_boundary",
      "content": "Conclusion\nSometimes small efforts end up with huge wins when you take your time. The core\nsniffer took us about a half a day to write. The end result was a big help in reducing\ndeveloper time when debugging our protocol. Try to make the time early on in your\nproject to think about the information you need to collect from your game in order to\nbest debug it. Put the hooks in as early as possible. It will pay off later in the project.\nReferences\n[Mfc] Microsoft Foundation Classes documentation. Available online at http://\nmsdn2.microsoft.com/en-us/library/d06h2x6e(VS.80).aspx.\n[Pcap] WinPcap Website. Available online at http://www.winpcap.org.\n[Tcpd] tcpdump Website. Avaliable online at http://www.tcpdump.org.\n[Wireshark] Wireshark Website. Available online at http://www.wireshark.org.\n6.3\nGame Network Debugging with Smart Packet Sniffers\n497\n\n\nThis page intentionally left blank \n\n\n499\nS E C T I O N\n7\nSCRIPTING AND\nDATA-DRIVEN SYSTEMS\n\n\nThis page intentionally left blank \n\n\n501\nIntroduction\nScott Jacobs\nTom Forsyth\nM\naximizing performance is a perpetual endeavor when developing games. Tradi-\ntionally, the performance focus has been on the game software’s runtime charac-\nteristics. Therefore, compiled languages have been and currently remain the bedrock\nof game programming. But often developers find they need to increase performance\nin other areas: implementing speed and rate of iterative development come immedi-\nately to mind. This is where scripting and data-driven solutions are most frequently\nput to effective use. This section introduces five scripting and data-driven gems, each\none with accompanying code on the CD-ROM.\nFirst, Julien Hamaide provides a method for automatically binding C++ classes to\nthe popular game scripting language Lua. His implementation is particularly focused\non performance, efficient memory usage, and thread safety. Next to interface with\nC++ classes is Joris Mans, who wrote a gem about serializing class instances to and\nfrom relational databases such as PostgreSQL. Storing class instance data in this way\nopens up whole new avenues for data manipulation, sharing, calculating metrics, and\nbalancing.\nMartin Linklater shares a design he calls dataports, which provide a common\ncommunication API for code to manipulate data in other pieces of code. This generic\ninterface can reduce coupling between modules and allow for more flexible interfaces.\nA data-driven approach for managing shaders is presented by Curtiss Murphy.\nThe architecture he introduces is configured by XML files and can conceivably allow\nfor shader iteration and development with little to no graphics programmer involve-\nment after the initial implementation investment.\nFinally, Zou Guangxian explores the idea of directly manipulating Python’s AST\nto create string tables. This gem makes use of powerful functionality inherent in\nPython to hook into Python’s parsing and compiling stages in order to either extract\nuseful information about the code’s structure or to dynamically affect and customize\nthe compilation results, which you will hopefully find interesting and inspiring.\n\n\nThis page intentionally left blank \n\n\n503\n7.1\nAutomatic Lua Binding\nSystem\nJulien Hamaide\njulien.hamaide@gmail.com\nW\nith game content growing faster than ever, programmers cannot hand-code all\nthe behavior anymore. They need the help of game and content creators.\nScripting languages have already been used in games for decades, but today’s console\ncan take advantage of them to increase the player’s experience further. This gem\nfocuses on an implementation of a Lua binding. This technique allows programmers\nto expose their C++ classes to Lua without any knowledge about the system. The tools\npresented here can apply to other languages as well. The design has been driven by\nusability, performance, memory footprint, and multithreading.\nIntroduction\nThe binding explained in this gem allows creation, access, and use of C++ objects\ninside a Lua script. As an example, if a list of ENTITY instances is stored inside a single-\nton class WORLD, the following script can be used to set the health of the player:\nlocal entity = WORLD:GetEntity( \"player\" )\nentity:SetHealth( 50 )\nThe binding used in this example is defined by the following declarations:\n// in .h and class definition\nSCRIPTABLE_DefineClass( WORLD )\n// in .cpp\nSCRIPTABLE_Class( WORLD )\n{\nSCRIPTABLE_ResultMethod1( GetEntity, ENTITY, std::string )\n}\nSCRIPTABLE_End()\nBinding a class is as simple as that. No other step is required, allowing the pro-\ngrammers to expose a C++ class and its methods to the Lua binding very simply. \n\n\n504\nSection 7\nScripting and Data-Driven Systems \nFeatures\nThe system has been designed with several objectives in mind: \n• Low memory footprint\n• High-performance binding\n• Support of C++ inheritance\n• Ease of use\n• Thread safety between scripts\nBinding of C Functions\nTo bind a function, Lua requires a specific interface. The function must have the type\ndefined in the following code. Lua binding is stack-based, the lua_State contains all\narguments passed to the function. Those arguments must be retrieved with lua_to*\nmethods using stack indexes. In this example, the function accepts a string as the first\nargument, a number as the second argument, and returns another number. More infor-\nmation on binding of C functions can be found in the Lua manual [Ierusalimschy06].\nThe C function binding is the only way you can bind Lua to C/C++, and will be the\nbase of the system.\nint binding_method( lua_State * state )\n{\nconst char * some_string;\ndouble some_number, another_number;\nsome_string = lua_tostring( state, 1 );\nsome_number = lua_tonumber( state, 2 );\n// Do some stuff here, including setting the return value \n// another_number\nlua_pushnumber( state, another_number );\nreturn 1; //Just say we returned one argument.\n}\nObject-Oriented in Lua\nLua is a versatile language that can be used to implement a lot of programming para-\ndigms. This gem explains how Lua can become object-oriented. To help, the Lua\nauthors have defined a set of tools providing “syntactic sugar.” The one we use in this\nsystem is shown in the following code. In it, the_object is an initialized variable, and\nthis code simulates a this_call on the method returned.\nthe_object:Test( 5 ) == the_object[\"Test\"]( the_object, 5 )\nObject-oriented methods can be implemented using this syntactic sugar. The\nobject is accessed as an array with the name of the method and returns the function to\nbe called. Lua has a mechanism that allows any type of variable to react to an array\n\n\naccess, using a metatable. (This is a feature of Lua 5.1. Lua 5.0 restricted metatables\nto table and userdata objects.) Metatables are Lua tables that are assigned to an object\nthat contains special fields: __index, __newindex, and so on [Ierusalimschy06b]. The\nfunctions set in those special fields are called depending on the situation. When an\nobject is accessed as an array, __index is called. The following code shows how to set\nup a metatable on an object:\nmetatable = {}\nmetatable.__index = function( table, key ) return key end\nsetmetatable( object, metatable )\ntest_return = object[ \"Test\" ] -- call the __index function in\nmetatable\nLua native types are number (double or float), string, table, nil, function (Lua or\nC), thread, and (light) user data. We use the latter to store the object in Lua. Light\nuser data and user data are slightly different. The first is used as a raw pointer, has no\nmetatable, and is not garbage collected. The second is a complete Lua object that can\nhave a metatable.\nBinding C++ Objects in Lua\nThe binding requires several mechanisms: the representation of the C++ object in\nLua, the storage of bound functions, and finally the registering of each C++ object in\nthe binding data. In this gem, the overall technique is given, and special cases are\nexplained later.\nThe Binding Data Structure\nIn existing implementations [Celes05], the binding is directly stored in Lua. Binding\ndata is then stored in each script. But if the number of scripts the system must support\nis high, binding data is duplicated unnecessarily. To avoid this, we decided to store the\nbinding data in C++ in a class called SCRIPTABLE_BINDING_DATA. Each class to be bound\nis assigned an index. SCRIPTABLE_BINDING_DATA contains a map between the class name\nand its index, and this map is stored in ClassIndexTable. Each class then has a map\nbetween a function name and the corresponding binding function. MethodTable is an\narray of these maps indexed by the value in ClassIndexTable. Because the delete oper-\nator has no name, its binding is put in a separate array called DeleteTable. Finally, the\nParentTable contains the index of the parent of each class. When a class has no parent,\nthe ParentTable entry is set to –1.\nA series of helper functions to access these maps can be found on the CD-ROM\nin scriptable_binding_data.h.\nclass SCRIPTABLE_BINDING_DATA\n{\ntypedef int (* BINDING_FUNCTION) (lua_State *);\n7.1\nAutomatic Lua Binding System\n505\n\n\nstd::map<std:string, int>\nClassIndexTable;\nstd::vector<std::map<std::string, BINDING_FUNCTION>*>\nMethodTable;\nstd::vector<BINDING_FUNCTION>\nDeleteTable;\nstd::vector<int>\nParentTable;\n};\nA pointer to this binding data and the index of the class is stored inside lua_State.\nThe space for this data is allocated by setting the LUAI_EXTRASPACE constant in \nluaconf.h, and the extra memory is allocated before lua_State.\nC++ Objects as Lua Objects\nA bound C++ object needs to store its class index—the result of looking up its class\nname in ClassIndexTable. This class index is used to search for bound functions in\nthe binding data. As previously said, we represent C++ objects as user data in Lua.\nThis user data contains the pointer to the bound object and its class index. The\nSCRIPTABLE_BINDING_HELPER structure helps the readability of the code. \nInside each bound class, an inner class called CLASS_SCRIPT_TYPE is declared. This\nis used in several parts of the binding process, explained individually. The interest\nright now is that it stores the index of the class, making the storage of C++ objects in\na Lua object simpler. _VALUE_::CLASS_SCRIPT_TYPE::GetClassIndex() recovers the\nclass index. \nThe following functions are used by C++ code when reading the arguments of a\ncall made from Lua, and returning the results to Lua.\ntemplate< typename _VALUE_>\nvoid SCRIPTABLE_PushValue( \nlua_State * state, _VALUE_ & object, _VALUE_ * dummy )\n{\nSCRIPTABLE_BINDING_HELPER\n*helper;\nhelper\n= lua_createuserdata( state, sizeof(SCRIPTABLE_BINDING_\nHELPER ) );\nhelper->Object = & object;\nhelper->ClassIndex = _VALUE_::CLASS_SCRIPT_TYPE::GetClassIndex();\n}\ntemplate<typename _VALUE_>\n_VALUE_& SCRIPTABLE_GetValue( lua_State * state, int index, _VALUE_\n*dummy )\n{\n506\nSection 7\nScripting and Data-Driven Systems \n\n\nSCRIPTABLE_BINDING_HELPER\n*helper;\nhelper = lua_touserdata( state, index );\nreturn *(helper->Object);\n}\nBy default, SCRIPTABLE_GetValue returns a reference to the object. But these func-\ntions can be specialized to support specific types, such as string, by value. A version is\ndefined for each C++ type that can convert to a Lua primitive: string, integer, and float. \nstd::string SCRIPTABLE_GetValue( \nlua_State * state, int index, std::string * dummy )\n{\nreturn lua_tostring( state, index );\n}\nThe function signature contains a trick. A dummy pointer is passed in as the\nthird argument. This argument selects the correct overloaded function. If template\nspecialization was used, the return value would always be a reference to the object. By\nusing the dummy pointer trick, the return value can be changed depending on the\ntype—objects can be returned by reference, whereas primitives such as strings and\nfloats can be returned by value.\nThe code is still not complete. A metatable must be assigned to the user data.\nThis metatable defines a method for __index (array access) and __gc (garbage collec-\ntion). As all binding data is stored in SCRIPTABLE_BINDING_DATA, only one instance per\nscript of this metatable is needed and it can be assigned to all C++ objects.\nBinding Function Creation\nThe binding function recovers the arguments from the Lua stack and performs the\nactual call. With the help of SCRIPTABLE_GetValue and SCRIPTABLE_SetValue, the\nbinding function creation is simple. The this pointer is always passed as argument\none. The function arguments are indexed from two.\nint ENTITY_AddHealth( lua_state * state )\n{\nENTITY &entity = SCRIPTABLE_GetValue( state, 1, ( ENTITY*) 0 );\nfloat health_add = SCRIPTABLE_GetValue( state, 2, (float*) 0 );\nfloat new_health = entity.AddHealth ( health_add );\nSCRIPTABLE_PushValue( state, new_health, (float*) 0 );\nreturn 1; // One return value\n}\nAlthough binding code like this is easy to create, the task is repetitive and error-\nprone. A macro-based system is used to generate this function instead. An example of\nsuch a macro is as follows:\n7.1\nAutomatic Lua Binding System\n507\n",
      "page_number": 530,
      "chapter_number": 55,
      "summary": "This section introduces five scripting and data-driven gems, each\none with accompanying code on the CD-ROM Key topics include functionality, functions, and function.",
      "keywords": [
        "Lua",
        "binding",
        "Lua Binding",
        "Lua Binding System",
        "SCRIPTABLE",
        "binding data",
        "object",
        "State",
        "Automatic Lua Binding",
        "data",
        "function",
        "binding function",
        "index",
        "Lua object",
        "helper"
      ],
      "concepts": [
        "functionality",
        "functions",
        "function",
        "classes",
        "binding",
        "objects",
        "objectives",
        "game",
        "state",
        "string"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 13,
          "title": "Segment 13 (pages 109-116)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 8,
          "title": "Segment 8 (pages 85-92)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 1,
          "title": "Segment 1 (pages 1-10)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 40,
          "title": "Segment 40 (pages 809-829)",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    },
    {
      "number": 56,
      "title": "Segment 56 (pages 541-548)",
      "start_page": 541,
      "end_page": 548,
      "detection_method": "topic_boundary",
      "content": "#define SCRIPTABLE_ResultMethod1( _RETURN_, _METHOD_, _\nPARAMETER_0_ ) \\\nint _RETURN_##_METHOD_##_PARAMETER_0_( lua_State * state ) \\\n{ \\\nCLASS \\\n&self = SCRIPTABLE_GetValue( state, 1, (CLASS *) 0 ); \\\nSCRIPTABLE_PushValue( \\\nstate, \\\nself._METHOD_( SCRIPTABLE_GetValue( state, \n2,(_PARAMETER_0_*)0 )), \\\n(_RETURN_*) 0 \\\n); \\\nreturn 1; \\\n}\nThis macro only creates the binding function. It also needs to be registered into\nthe system. With a little C++ trick, you can do both at the same time. By using a func-\ntion inner class with a static method, you can create and register the method at the\nsame time, as shown in the following code:\nvoid register_ENTITY( BINDING_DATA & binding_data )\n{\nclass float_AddHealth\n{\npublic:\nstatic int Call( lua_State * state )\n{\n// ... ENTITY_AddHealth() code as above...\nreturn 1;\n}\n}\nbinding_data.Register(\n\"ENTITY\", \"AddHealth\", &float_AddHealth::Call );\n}\nThis pattern can be made into a set of general macros, as follows:\n#define SCRIPTABLE_Class( _CLASS_ ) \\\nvoid register_##_CLASS_ ( BINDING_DATA & binding_data )\\\n{\n#define SCRIPTABLE_End( _CLASS_ ) \\\n}\n#define SCRIPTABLE_ResultMethod1( _RETURN_, _METHOD_, \n_PARAMETER_0_ ) \\\nclass _RETURN_##_METHOD_##_PARAMETER_0_ \\\n{ \\\npublic: \\\nstatic int Call( lua_State * state ) \\\n508\nSection 7\nScripting and Data-Driven Systems \n\n\n{ \\\n... SCRIPTABLE_ResultMethod1 () code as above...\nreturn 1; \\\n} \\\n} \\\nbinding_data.Register( class_name, #_METHOD_, \\\n&#_RETURN__#_METHOD__#_PARAMETER_0_::Call );\nThese macros are used in the following way:\nSCIPTABLE_Class(ENTITY)\n{\nSCRIPTABLE_ResultMethod1(float,AddHealth,float)\n}\nSCRIPTABLE_End(ENTITY)\n//...and then at start of day, register the class...\nregister_ENTITY(binding_data);\nAttributes (such as data members of a class) are also handled by the system. Typi-\ncal binding allows the access of attributes by doing object.attribute. To handle such\nan access, the __index and __newindex metamethods must be adapted. To avoid this,\nwhen an attribute is bound with SCRIPTABLE_Attribute, Set and Get functions are\ncreated in Lua. The demo on the CD-ROM contains a definition of this macro and\nan example use of it in the vector_3 class.\nAs shown, the creation of the binding and registration function for each class is\nnow handled by some macros. But you still need a simple way to call these functions. \nAutomatic Type Registering\nTo try to keep the system as transparent to the user as possible, we decided to encap-\nsulate the registration function into a class. Each object that is bound declares an\ninner class derived from SCRIPTABLE_TYPE. This class has a static member whose con-\nstructor adds it to a global table. This table is then walked at program invocation, call-\ning each registration function.\nclass SCRIPTABLE_TYPE\n{\npublic:\nSCRIPTABLE_TYPE()\n{\nSCRIPTABLE_TYPE_TABLE::Add( this );\n}\nvirtual void Register( BINDING_DATA & binding ) = 0;\n}\n7.1\nAutomatic Lua Binding System\n509\n\n\nThen, in the bound class, the following code is inserted: \nclass CLASS_SCRIPT_TYPE;\nfriend class CLASS_SCRIPT_TYPE;\nclass CLASS_SCRIPT_TYPE :\npublic SCRIPTABLE_TYPE\n{\npublic :\ntypedef GAME_ENTITY CLASS;\nCLASS_SCRIPT_TYPE( void );\nstatic const char * GetClassName() { return \"GAME_ENTITY\"; }\nstatic int & GetClassIndex()\n{\nstatic int index = -1;\nreturn index;\n}\nstatic int Delete( lua_State * lua_state ); \nvirtual const char * GetName() const { return GetClassName(); } \nvirtual int & GetIndex(){ return GetClassIndex(); } \nvirtual void Register( SCRIPTABLE_BINDING_DATA & binding ); \n};\nThe inner class contains the Register function, the Delete function, the bound\nclass name, and its index. Its definition is hidden inside the SCRIPTABLE_DefineClass\nmacro. The details on this macro can be found in the demo on the CD-ROM.\nclass GAME_ENTITY\n{\npublic:\nSCRIPTABLE_DefineClass( GAME_ENTITY )\n};\nWith the whole system in place, you can call all registration functions by walking\nthe table, as shown in the following code:\nvoid SCRIPTABLE_TYPE_TABLE::Register( BINDING_DATA & binding_data )\n{\nint type_index;\nfor( type_index = 0, type_index < TypeTable.size(); ++type_index )\n{\nTypeTable[ type_index ]->Register( binding_data );\n}\n}\nExtending the Binding System\nThe following sections explain some ways to extend this binding system.\n510\nSection 7\nScripting and Data-Driven Systems \n\n\nReference Counting and Raw Objects\nThe Lua instance of an object contains its pointer. If an object is destroyed while Lua\nstill has a variable containing the object, bugs can occur. To prevent such situations,\nwe handle the object in two ways: \n• If the object is reference counted, Lua increases the reference count. Even if the\nC++ object is not referenced on the C++ side of the program, the object will not\nbe deleted as long as Lua does not release its reference. The Delete function asso-\nciated on the __gc metamethod will be called when a Lua variable is being col-\nlected, and this function will decrease the reference count.\n• If the object is an uncounted object such as a vector, create a copy of it. In this\ncase, the Delete function just deletes the object.\nThe choice between the two methods is done for each class. Two defines are\navailable—SCRIPTABLE_Class for counted objects and SCRIPTABLE_UncountedClass\nfor uncounted objects. These macros must be placed in the class definition. The demo\nshows both techniques at work.\nInheritance\nInheritance is implemented by storing the class index of the parent in the ParentTable.\nIf a method cannot be found in the current class binding, it searches in its parent bind-\ning. The function BINDING_DATA::GetFunction does this search, and can be found on\nthe CD-ROM. The registration function created by the macros contains a call that sets\nthe parent for the current class. SCRIPTABLE_Class is used for baseless classes, otherwise\nSCRIPTABLE_InheritedClass can be used.\nSupporting inheritance also means that inherited classes can be pushed as argu-\nments to a method. The SCRIPTABLE_PushValue macro has the class index hard-coded.\nTo bypass this issue, the binding function is put in a virtual function of the class. The\ntemplate version of SCRIPTABLE_PushValue is shown next, calling the derived version\nof LuaPushValue. This function is hidden inside SCRIPTABLE_DefineClass (virtual)\nand SCRIPTABLE_DefineRawClass (non-virtual).\ntemplate< typename _VALUE_>\nvoid SCRIPTABLE_PushValue( \nlua_State * state, _VALUE_ & object, _VALUE_ * dummy )\n{\nobject.LuaPushValue( state );\n}\nSingletons, Static Functions, and Attributes\nSingletons, static functions, and attributes share a property: no object is associated\nwith function calls. To follow the C++ syntax as closely as possible, the call to such\nfunctions is done by prefixing the class name with the method name, as if the class\n7.1\nAutomatic Lua Binding System\n511\n\n\nwas the object. For example, WORLD:GetEntity(). This call triggers a lookup for the\nvariable WORLD in the global table (in Lua, all global variables are stored by name in a\ntable called _G). An easy way to allow this is to create a Lua variable for every C++\nclass. But this solution breaks the memory consumption target. Even if your script is\nnot using a class function, or if a class does not have any static functions, a variable\nwould have been created and inserted into the global table. \nThe chosen solution is to use the __index mechanism on the global table. \nWhen a script accesses a global variable that does not exist in the global table, the\nSCRIPTABLE_LUA_REGISTERER::GlobalIndexEventHandler is called. If the access vari-\nable name matches a class name, a new object with a null pointer is created and\ninserted into the global table with the class name. If it does not match, the handler\nreturns nil as Lua does if no table entry is found. This way, only class variables that are\nused are created, and once the event handler has been called, the variable is available\nin the global scope. This mechanism is transparent to all other variable access.\nTemplate Classes\nThe binding is also able to handle template classes. The template does not need to be \na bound class, but in this case, it should define its name in Lua by using\nSCRIPTABLE_DeclareScriptableTypeName. All the native types are already declared.\nThe name of the class in Lua is the name of the template class suffixed by the name of\nthe parameter. The codebase requires all template class to be of the form CLASS_TO_ or\nCLASS_OF_, therefore RANGE_OF_<VECTOR_3> becomes RANGE_OF_VECTOR_3 in Lua, which\nis quite consistent. If this convention does not suit your needs, the system is easy to\nchange.\nThe macros used are the same as the non-template ones with the word “Template”\nadded. For exampe, SCRIPTABLE_Class becomes SCRIPTABLE_TemplateClass. The\nbinding requires a cpp file to contain all definitions. The binding must also be explic-\nitly instantiated with the help of the macro SCRIPTABLE_InstantiateTemplateClass.\nThe demo shows a dummy template class to show how the binding works.\nThe binding has only been implemented for a single-parameter template class,\nbut it can be easily extended to any number of parameters if needed.\nSupport of Enums\nEnums are supported in our system. SCRIPTABLE_PushValue and GetValue are rede-\nfined to treat them as integers by using SCRIPTABLE_CastValue( ENUM_TYPE, int ).\nTo allow the user to use the name of the enum in the code, a preprocessor pass is done\non the code, replacing all matching enum entries by their values. A macro system is\nalso used to create and register the text as a #define in the preprocessor, but the details\nof this are beyond the scope of this gem.\n512\nSection 7\nScripting and Data-Driven Systems \n\n\nBinding Overloaded Functions\nC++ allows the overloading of a function. In a variant, SetValue can exist for bool,\nint, real, and so on. Lua does not support overloading. Two solutions exist. The \nfirst is that BINDING_DATA::GetFunction can implement an argument type matcher,\nbut this is expensive. The other solution is to rename the function in Lua. The\nSCRIPTABLE_Renamed* macros allow methods to be renamed in Lua so that SetValue\ncan be renamed to SetBoolValue, SetRealValue, and so on.\nDebug Helper\nBecause the binding functions are being created by macro instances, adding debug-\nging functions and asserts to all bound functions is simple. The debugging system is\nenabled by the preprocessor define _LUA_DEBUG_. The debug helper checks the argu-\nment count and the class type. If errors are detected, a Lua error is triggered. This\ndebugging is designed to be usable in a C++ release build, allowing the scripts to be\ndebugged at full speed. The advantage of using Lua errors is that the game does not\ncrash; it just exits the call. The behavior is also compatible with any Lua debugger you\nuse. The binding error can be treated the same way as a Lua error. The debug helpers\ncan be completely deactivated for a retail build, increasing the binding overall speed.\nSummary\nTo bind a class, this macro SCRIPTABLE_DefineClass( MY_CLASS ) must be put in the\nclass definition. Four options are available: \nSCRIPTABLE_DefineClass : LuaPushValue is virtual\nSCRIPTABLE_DefineRawClass : LuaPushValue not virtual\nSCRIPTABLE_DefineTemplateClass : template class, LuaPushValue is \nvirtual\nSCRIPTABLE_DefineRawTemplateClass : template class, LuaPushValue \nnot virtual\nThe template parameter is passed as the second argument of the macro. In the\ncpp file, the class binding implementation must be set up as follows:\nSCRIPTABLE_Class( MY_CLASS )\n{\nSCRIPTABLE_VoidMethod( SetValue, float )\n}\nSCRIPTABLE_End()\nThe options are SCRIPTABLE_(Uncounted)(Inherited)(Template)Class. If the\nobject is not reference counted, the Uncounted version of the macro should be used. If\nthe class inherits from another, use the Inherited version. The demo code covers the\ndefinition of almost all types of object.\n7.1\nAutomatic Lua Binding System\n513\n\n\nFuture Work\nThe following sections address some possible extensions to this system.\nOverloading of C++ Methods in Lua\nC++ objects exist in Lua and can be used as Lua objects. Overloading the C++ object\nmay be an interesting extension. For example, this would allow hooking of function\ncalls. To intercept each call to GetHealth and print the current health, the GetHealth\nmethod can be overloaded, as shown in the following code:\nObject.GetHealth = \nfunction( self ) \nlocal health = ENTITY.GetHealth( self ) -- Do the call to C/C++\nprint( \"Health of object \" .. self .. \" is \" .. health )\nreturn health;\nend\nWhen a value is set by array access, the metatable’s __newindex entry is called. It\nstores the Lua function into a table as a replacement of the C function. The __index\nfunction is also changed to reflect a new behavior: on array access, it searches the Lua\nfunction table first, and then searches the C++ binding data. Although this solution\nshould work, it causes another problem—the object must be kept by Lua in some\nway. If it is garbage collected, its overloading will be lost. The implementation in the\ndemo does not support overloading, but does support the persistence of objects.\nEvery time a call to SCRIPTABLE_PushValue is made, the Lua version of the object is\nsought in a table called _object. If it exists, it is reused; otherwise, the Lua version of\nthe object is created. This code can be found in the demo.\nSand-Boxing and Type Filtering\nThe presented system allows the binding of objects to Lua. But you may want to\nsand-box some scripts, limiting their access. Low-level scripts can be used as configu-\nration files, accessing features such as file I/O or graphics configuration, whereas user-\nlevel scripts can only access a selected set of classes. The definition of the access level\ncan be set up in the macros and stored in SCRIPT_TYPE. The Register method can be\ncalled with the access level wanted. In the following code, the class WORLD is declared\nas being at user level. The SCRIPT_MANAGER contains the BINDING_DATA. Several man-\nagers can be created with different access levels. When a manager is initialized, it calls\nSCRIPT_TYPE_TABLE::Register with its binding data and its access level. The binding\ndata contains only classes that are available for its access level. Scripts are created by\nand associated with a manager, and use its binding data.\n514\nSection 7\nScripting and Data-Driven Systems \n\n\nSCRIPTABLE_Class( WORLD, ACCESS_LEVEL_User )\nvoid SCRIPT_TYPE_TABLE::Register( \nBINDING_DATA & binding_data, const ACCESS_LEVEL access_level )\n{\nint type_index;\nfor( type_index = 0, type_index < TypeTable.size(); ++type_index )\n{\nif( TypeTable[ type_index ]->GetAccessLevel() >= access_level )\nTypeTable[ type_index ]->Register( binding_data );\n}\n}\nOptimization of Generated Code Size\nThe technique presented here creates a binding function for all bound methods. It\ngenerates lots of code that does the same duty again and again. A solution is to create\nfunction descriptions instead of binding functions. The binding data can be a class\nthat stores this FUNCTION_DESCRIPTION:\nstruct FUNCTION_DESCRIPTION\n{\nconst char * FunctionName;\nconst void * FunctionPointer\nint ArgumentCount;\nARGUMENT_DESCRIPTION * ArgumentDescription;\nRETURN_DESCRIPTION * ReturnValueDescription;\n}\nThe macro system fills an array of these descriptions:\n#define SCRIPTABLE_VoidMethod1( _METHOD_, _PARAMETER_0_ ) \\\n{ #_METHOD_, &CLASS::_METHOD_, 1,\n{ GetArgumentDescription<_PARAMETER_0_>() }, 0 }, \nNow only a single binding function is necessary. Its pseudocode is as follows: \nfor each argument_index  < description.ArgumentCount \ndescription.ArgumentDescription[\nargument_index ].PushArgumentOnCStack();\ncall( decription.FunctionPointer );\nif( description.ReturnValueDescription )\ndescription.ReturnValueDescription->StoreResultValueInLua();\n7.1\nAutomatic Lua Binding System\n515\n",
      "page_number": 541,
      "chapter_number": 56,
      "summary": "This chapter covers segment 56 (pages 541-548). Key topics include classes, binding, and function. Covers function. SCRIPTABLE_ResultMethod1 () code as above.",
      "keywords": [
        "SCRIPTABLE",
        "Lua Binding System",
        "lua",
        "binding",
        "Type",
        "Automatic Lua Binding",
        "Lua Binding",
        "function",
        "Binding System",
        "DATA",
        "define SCRIPTABLE",
        "binding data",
        "object",
        "State",
        "METHOD"
      ],
      "concepts": [
        "classes",
        "binding",
        "function",
        "functions",
        "object",
        "template",
        "access",
        "accesses",
        "accessing",
        "returns"
      ],
      "similar_chapters": [
        {
          "book": "Effective_Modern_C++",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 17,
          "title": "Segment 17 (pages 331-351)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 18,
          "title": "Segment 18 (pages 175-187)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 40,
          "title": "Segment 40 (pages 809-829)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 14,
          "title": "Segment 14 (pages 116-123)",
          "relevance_score": 0.54,
          "method": "api"
        }
      ]
    },
    {
      "number": 57,
      "title": "Segment 57 (pages 549-558)",
      "start_page": 549,
      "end_page": 558,
      "detection_method": "topic_boundary",
      "content": "This code must be partially written in assembly, because it accesses the C stack.\nThis system, although a little slower, saves some memory, which is useful if your sys-\ntem is memory-bound.\nDemo\nThe code on the CD-ROM provides the binding in full functionality, and it should\nbe simple to plug it into a codebase without any problem. The demo tries to cover all\nthe features explained here. If you launch the demo, you won’t see much except some\ntext. Single-stepping the code is the best way to understand how the system works.\nOnce the broad details are clear, expand the macros manually and step into the\nexpanded code to see the detailed workings.\nConclusion\nThis gem presents an automatic binding system. The user only has to set up some\ndeclarations about a bound class, recompile, and the class is accessible from scripts.\nNo knowledge of either the system or the Lua binding is needed. The system has been\ndesigned to be CPU and memory friendly. The system provides debug helpers such as\nargument checking that can be disabled for retail builds. This gem also presents sev-\neral C++ tricks, such as automatic type registering and dummy pointer function selec-\ntion. With these tools in hand, anybody should be able to write or adapt this binding\nto their engine and script language. \nReferences\n[Celes05] Celes, W., de Figueiredo, L.H., and Ierusalimschy, R. “Binding C/C++\nObjects to Lua,” Game Programming Gems 6, edited by Michael Dickheiser,\nCharles River Media, 2005, pp. 341-356.\n[Ierusalimschy06] Ierusalimschy, R., de Figueiredo, L.H., and Celes, W. “Lua 5.1\nReference Manual,” available online at Lua.org, 2006.\n[Ierusalimschy06b] Ierusalimschy, R. “Programming in Lua (second edition),” avail-\nable online at Lua.org, 2006.\n516\nSection 7\nScripting and Data-Driven Systems \n\n\n517\n7.2\nSerializing C++ Objects \ninto a Database Using\nIntrospection\nJoris Mans\njoris.mans@10tacle.be\nW\nith the ever-increasing amount of assets that need to be managed by content-\ncreation tools, managing those assets becomes more difficult, especially when\nconfronted with quantities that cannot simply all be loaded in memory at the same\ntime. Users of these tools want to be able to navigate through all those assets in order\nto quickly find the one they need. Keyword searches, categories, and hierarchical\nviews are ways of exposing this to the end user. Another issue is that there are many\npeople working on content creation who want to use the same shared assets, and the\nasset creators want to have them exposed to all the users as soon as possible.\nOne of the possible ways to implement this is by using a database backend. This\ngem presents a system that allows storage of C++ objects into an SQL database, and\ntheir retrieval using filters. The implementation and examples were created using\nPostgreSQL 8.2 and Microsoft Visual Studio 2005.\nMetadata\nBefore you can start serializing in the database, you need some introspection tools in the\ncodebase. The implementation of a robust and complete metadata system (hereafter\nreferred to as meta-system) is beyond the scope of this gem, so I will restrict it to a basic\nimplementation that has support for everything necessary for database serialization.\nThe metadata of a class is saved in an instance of a class called MetaType. For the\nsystem to work, you need to be able to retrieve the following information from this\nclass:\n• Classname \n• Parent classname\n• AttributeTable containing an instance of MetaAttribute for each serializable\nattribute\n• Size of an instance of the object in bytes\n\n\nThis class allows you to manipulate objects of arbitrary types without having to\nresort to RTTI or use polymorphism.\nAttributes\nFor every attribute in the class you want to serialize, you add its information to the\nmetadata. For this, you create a class called MetaAttribute containing the following\ninformation.\n• AttributeName\n• Attribute metatype\n• Memory offset in bytes from the start of the object\n• Whether the attribute is a pointer\n• Whether the attribute is an array\nEach MetaAttribute instance will be added to the AttributeTable list of the corre-\nsponding MetaType instance. Code for these classes can be found on the demo on the\nCD-ROM.\nArrays\nThis gem uses a special array class. This is a template class inheriting from a class called\nArrayBase. This base class contains an interface that allows you to retrieve the number\nof items in an array, to set the number of items, and to retrieve the metatype of the\nitem class used in the array. It also allows you to get the pointer to the data in the array.\nThis way you can manipulate arrays without having to know what type of object is\nstored inside them, an ability that you’ll need when manipulating objects in the data-\nbase system.\nSerializing in Text\nThere is one more thing you need before you can start implementing the database sys-\ntem. You need to be able to serialize simple attributes into text format. This demo uses\nsome overloaded C functions to do this.\nvoid WriteObjectToText( \nconst void * object, \nconst MetaType & meta_type, \nstd::string & output_text \n);\nvoid ReadObjectFromText( \nvoid * object, \nconst MetaType & meta_type, \nconst std::string & input_text \n);\n518\nSection 7\nScripting and Data-Driven Systems \n\n\nThese functions have support for reading and writing objects of scalar types and\nstrings. For example:\nint a = 5;\nstd::string output_text;\nWriteObjectToText( &a, META_TYPE_GetStaticMetaType( a ), \noutput_text);\nThis will result in output_text containing the string \"5\".\nThe Database System\nBefore you start serializing into a database, consider that you’ll want to serialize C++\nobjects. What do those objects contain?\nA C++ class consists of a combination of the following:\n• Scalar members (for example, int, float, char, and so on)\n• One or more parent classes, if present\n• Pointers\n• Instances of other C++ classes\nIn this case I will slightly change this list. For the purposes here, a C++ class will\nconsist of the following:\n• Scalar members\n• Strings\n• One parent class, if present\n• Pointers\n• Instances of other C++ classes\n• Arrays of pointers, instances of other C++ classes or scalars, using our own array\ntype\nThe system described in this gem can be extended to support more features of\nC++ classes (multiple inheritance, other collection types, and so on), but it would\nextend the scope too far so I restrict the explanation to classes fitting the previous\ndescription.\nThe Tables\nBecause you’ll store the objects in a relational SQL database, you need to define the\ntables used to store those objects.\nEach table corresponds to one class. The primary key for a table is a field called\n_Identifier and contains an auto-incrementing integer. (You can give this key any\nother name you want as long as it does not conflict with the name of an attribute of\nan object you want to serialize.)\n7.2\nSerializing C++ Objects into a Database Using Introspection\n519\n\n\nUsing the list of attribute types previously defined, you’ll see how to store each\ntype in a field in the database. For each attribute, the field name corresponds to its\nname.\nScalar Members\nEach scalar member is stored as a scalar field of type integer or real in the database. \nStrings\nEach string is stored in a varchar field.\nThe Parent Class\nYou can use any table in a database as the type of a field of another table. This exam-\nple creates a field called _Parent that will be of the type of the table created to store\nthe parent class. (You could also name this field any other name you want as long as it\ndoesn’t conflict with the name of an attribute in your class.) An example will clarify\nthis class:\nclass Base\n{\nint a;\n};\nclass Subclass : public Base\n{\nint b;\n};\nNow create a table called Base:\nCREATE TABLE \"Base\" \n(\n\"_Identifier\" serial,\n\"a\" integer\n) ;\nand a table called Subclass:\nCREATE TABLE \"Subclass\"\n(\n\"_Identifier\" serial,\n\"_Parent\" \"Base\",\n\"b\" integer\n);\nPointers\nThere are several ways to store a pointer to an object in the database. At first you\nmight think that a pointer could be considered a foreign key, corresponding to the\nprimary key of the table containing the object pointed to. For example:\n520\nSection 7\nScripting and Data-Driven Systems \n\n\nclass Base\n{\nint a;\n};\nclass StoreInDatabase\n{\nBase * basePointer;\n};\nYou could create a table called Base like this:\nCREATE TABLE \"Base\" \n(\n\"_Identifier\" serial,\n\"a\" integer\n);\nand a table called StoreInDatabase like this:\nCREATE TABLE \"StoreInDatabase\" \n(\n\"_Identifier\" serial,\n\"basePointer\" integer,\n);\nNow imagine creating some instances:\nBase * base_object = new Base;\nStoreInDatabase * store_object = new StoreInDatabase;\nbase_object->a = 10;\nstore_object->basePointer = base_object;\nYou could store them in the database like so:\nTable Base:\n_Identifier     a\n1                  10\nTable StoreInDatabase:\n_Identifier     basePointer\n1               1\nRetrieving the object from the database seems straightforward. You read out the\ncontents of table StoreInDatabase, use the value found in the field basePointer, and\nget the corresponding row from Base to instantiate the object pointed to. A simple\nsolution...or maybe not? \n7.2\nSerializing C++ Objects into a Database Using Introspection\n521\n\n\nCheck the following example:\nclass Base\n{\nint a;\n};\nclass Subclass : public Base\n{\nint b;\n};\nclass StoreInDatabase\n{\nBase * basePointer;\n};\nSubclass * subclass_object = new Subclass;\nStoreInDatabase * store_object = new StoreInDatabase;\nsubclass_object->a = 10;\nsubclass_object->b = 20;\nstore_object->basePointer = subclass_object;\nIf you were to apply the same system to these objects, you get into trouble. Saving\nthe objects would result in this:\nTable Subclass:\n_Identifier   b       _Parent\n1             20      {10}\nTable StoreInDatabase:\n_Identifier     basePointer\n1               1\nWhen you’re trying to get the object from the database, you have an issue. When\nyou’re retrieving the value stored in basePointer, there is no way of knowing that you\nare not storing a pointer to an object of type Base but an object of type Subclass. You\ndo not know which table corresponds to the key stored in basePointer.\nThere are several solutions to this problem. This gem sticks to one solution, but\nfeel free to experiment with others. Instead of storing an integer that refers to the pri-\nmary key of the object pointed to, let’s try storing a string with the following layout:\n\"( primaryKeyValue, tableName )\"\n522\nSection 7\nScripting and Data-Driven Systems \n\n\nApplying this to the previous example gives this result:\nCREATE TABLE \"Base\" \n(\n\"_Identifier\" serial,\n\"a\" integer\n);\nCREATE TABLE \"Subclass\" \n(\n\"_Identifier\" serial,\n\"_Parent\" \"Base\",\n\"a\" integer\n);\nCREATE TABLE \"StoreInDatabase\" \n(\n\"_Identifier\" serial,\n\"basePointer\" varchar,\n);\nStoring the same objects will result in these table values:\nTable Subclass:\n_Identifier     b       _Parent\n1              20         {10}\nTable StoreInDatabase\n_Identifier      basePointer\n1                \"(1,Subclass)\"\nWhen reading the field values, you can use string manipulation to get the pri-\nmary key part and the tablename part of the field value stored in basePointer, and\nuse the corresponding table to receive the contents of the pointed to object.\nAnother solution is to store a table containing all names of the classes stored in\nthe database, and instead of using a pair containing ( primaryKeyValue, tableName )\nto store a pointer, the field will contain ( primaryKeyValue, classNamePrimary\nKeyValue ), where classNamePrimaryKeyValue contains the primary key value of the\ncorresponding classname stored in the table. Especially with big databases, this would\nbe a more efficient solution, because the amount of data to retrieve from the database\nis smaller, as would be the size of the database, because classnames aren’t replicated in\ndifferent tables.\n7.2\nSerializing C++ Objects into a Database Using Introspection\n523\n\n\nInstances of Other C++ Classes\nYou could store instances of classes in the same way that you store the parent class, by\nadding a field with an attribute type corresponding to the table of the associated class.\nFor example:\nclass Base\n{\nint a;\n};\nclass StoreInDatabase\n{\nBase baseInstance;\n};\nCREATE TABLE \"Base\" \n(\n\"_Identifier\" serial,\n\"a\" integer\n);\nCREATE TABLE \"StoreInDatabase\" \n(\n\"_Identifier\" serial,\n\"baseInstance\" \"Base\"\n);\nThis will work, unless you have objects containing pointers to members of other\nobjects. For example:\nclass Base\n{\nint a;\n};\nclass StoreInDatabase\n{\nBase baseInstance;\n};\nclass AnotherToStoreInDatabase\n{\nBase * basePointer;\nint b;\n};\nStoreInDatabase * store_1 = new StoreInDatabase;\nAnotherToStoreInDatabase * store_2 = new AnotherToStoreInDatabase;\nstore_1->baseInstance.a = 10;\nstore_2->basePointer = &store_1->baseInstance;\nstore_2->b = 20;\n524\nSection 7\nScripting and Data-Driven Systems \n\n\nThere is no way you can store the pointer value in store_2->basePointer,\nbecause it references part of another object and not an entire row in a database table.\nA solution to this issue is to store instances of C++ objects the same way you store\npointers. You store the instance in the table corresponding to its class, make a varchar\nfield in the table corresponding to the object containing the instance, and write the\nstring containing the primary key and the tablename in the field. Here’s an example\nusing the same objects presented previously:\nCREATE TABLE \"Base\" \n(\n\"_Identifier\" serial,\n\"a\" integer\n);\nCREATE TABLE \"StoreInDatabase\" \n(\n\"_Identifier\" serial,\n\"baseInstance\" varchar\n);\nCREATE TABLE \"AnotherToStoreInDatabase\" \n(\n\"_Identifier\" serial,\n\"basePointer\" varchar,\n\"b\" integer\n);\nAnd the result of storing the objects will look like this:\nTable Base:\n_Identifier      a\n1                10\nTable StoreInDatabase:\n_Identifier      baseInstance\n1                \"(1,Base)\"\nTable AnotherToStoreInDatabase:\n_Identifier      basePointer      b\n1               \"(1,Base)\"       20\nArrays\nBecause an array is a data type supported by the database, you can apply the same\nrules used for the previous types, but store them in an array in the field. For a scalar it\nwill simply be an array of scalars; for a pointer, it’s an array of varchar, and so on.\n7.2\nSerializing C++ Objects into a Database Using Introspection\n525\n",
      "page_number": 549,
      "chapter_number": 57,
      "summary": "Demo\nThe code on the CD-ROM provides the binding in full functionality, and it should\nbe simple to plug it into a codebase without any problem Key topics include classes, base, and type.",
      "keywords": [
        "CREATE TABLE",
        "base",
        "Identifier",
        "object",
        "Database",
        "class Base",
        "store",
        "create",
        "table called Base",
        "Subclass",
        "StoreInDatabase",
        "system",
        "Identifier basePointer",
        "Table StoreInDatabase",
        "Table Base"
      ],
      "concepts": [
        "classes",
        "base",
        "type",
        "stored",
        "store",
        "tables",
        "containing",
        "database",
        "created",
        "create"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 38,
          "title": "Segment 38 (pages 325-336)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 7,
          "title": "Segment 7 (pages 52-61)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 26,
          "title": "Segment 26 (pages 230-237)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 20,
          "title": "Segment 20 (pages 180-188)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 13,
          "title": "Segment 13 (pages 119-126)",
          "relevance_score": 0.44,
          "method": "api"
        }
      ]
    },
    {
      "number": 58,
      "title": "Segment 58 (pages 559-566)",
      "start_page": 559,
      "end_page": 566,
      "detection_method": "topic_boundary",
      "content": "Creating the Tables\nBy using the MetaType instance of each class, you can generate the tables in the data-\nbase. You generate a string containing the SQL statement to create the table using the\nfollowing pseudocode:\nprocedure AddTypeToDatabase( meta_type )\nbegin\nif meta_type.HasParent()\nif NOT(TypeExistsInDatabase( meta_type.parentClassName )\nAddTypeToDatabase( GetMetaType( meta_type.parentClassName ) )\nendif\nendif\nsql_statement =  \"CREATE TABLE\" + meta_type.className\nsql_statement +=  \"_Identifier serial,\"\nforeach attribute in meta_type.attributeTable \nsql_statement += GenerateCreateAttributeStatement( attribute )\nendfor\nif meta_type.HasParent()\nsql_statement += \"_Parent \" + meta_type.parentClassName \nendif\nExecuteSqlStatement( sql_statement )\nend\nIn this pseudocode, GenerateCreateAttributeStatement generates the part of the\nSQL statement needed to create the field corresponding to the kind of attribute. For\na string attribute called myText, it will generate something along the lines of \"myText\nvarchar\". The actual code will need to take some more things into account, such as\ngenerating the commas between the field declarations and generating the right quotes\nin the SQL statement so there are no case issues.\nOne thing to take into consideration when executing the generated SQL state-\nment is that you need to make sure to create the tables for the base classes before those\nof the subclasses. Otherwise the database will give an error stating that the _Parent\nfield has been declared with an unknown type, hence the TypeExistsInDatabase test\nin the beginning of the procedure.\nThe corresponding code on the CD-ROM demo can be found in the method\ncalled:\nbool DatabaseManager::CreateTable( const MetaType & meta_type )\n526\nSection 7\nScripting and Data-Driven Systems \n\n\nStoring an Object\nStoring an object happens in several phases. First, you get a new primary key value for\nthe corresponding table. Next, you insert the object in the table, which is completely\nempty except for its primary key value. Finally, you update the object in the table, fill-\ning in its attributes. Why all this fuss? Why not just insert the object in the table, have\nthe database autogenerate the primary key value, and be done with it? The reason will\nbecome clear when you go to retrieve objects. \nRemember that each instance of an object in a database table is uniquely identi-\nfied by its primary key value. On the C++ side, each instance of an object of a certain\ntype is uniquely identified by its memory address. Imagine running your application\nand having one object in a table in your database. You ask the database system to give\nyou that object. The program will execute a query, will receive the contents of the\nfields, construct a new instance in memory of the C++ object, and return you its\npointer. So far, so good.\nNow somewhere down the line, you execute exactly the same query. If this sce-\nnario repeats itself, you are in trouble, because the system will actually create a second\ninstance in memory of an object that exists only once in the database. Instead, the\ndatabase manager should return the pointer to the same instance created before.\nA way to solve this issue is by having an instance table inside the database man-\nager. This cache stores the relationship between a primary key value, a tablename, and\nan instance of an object. What will happen the first time you retrieve the object from\nthe database? The system will instantiate the C++ object, fill in its values, and store\nthe pointer to the object, its tablename, and its primary key value in the cache. The\nnext time you ask for a certain object, the database system will take the primary keys\nit receives from the database and match those with the tablename in the cache. If the\nobject already exists in the cache, it will return the stored pointer instead of creating a\nnew instance.\nBut what has this got to do with storing the object? Well, imagine this scenario.\nThe program inserts an object in the database, and somewhere later on tries to\nretrieve it. The original object that was inserted still exists in memory. Here, the data-\nbase system should return the pointer to the original object, and not create a new\ninstance, so at insertion time the object should be added to the cache too. Because you\nneed the primary key value to store the object, you need to retrieve this up front. If\nyou were to insert the object there is no way of retrieving its primary key value by any\nrobust method. Even a SQL SELECT with a WHERE clause of all the attribute values of\nthe object will not be robust, because there could be multiple identical objects in the\ndatabase.\n7.2\nSerializing C++ Objects into a Database Using Introspection\n527\n\n\nThis is why you must retrieve the primary key value explicitly. But why insert an\nempty object first, and update it afterward? This is purely for code simplicity. Because\nthe SQL syntax for insert and update commands is different, it requires less code if\nyou can use the same codepath for insertion of new objects and updates of existing\nobjects.\nTo store an object in the database, you execute the following pseudocode:\nprocedure StoreObjectInDatabase(object, meta_type )\nbegin\nBeginTransaction();\nInsertObjectAndAttributePrimaryKeys( object, meta_type );\nUpdateObject( object, meta_type );\nEndTransaction();\nend\nSomething important to note are the calls to BeginTransaction and EndTransac-\ntion. Because you execute multiple consecutive SQL statements, it is very important\nto keep the database in a consistent state at all times. Using transactions allows you to\nroll back the database if between a BeginTransaction and EndTransaction something\nwas to happen that crashed the application. Imagine having your application crash\nright after inserting the empty object only containing a primary key value. Next time\nyou use the application there will be corrupt data in the database. Guarding these\nblocks of statements with a transaction block will make sure none of the statements\nexecuted will be permanently stored in the database until EndTransaction is called.\nLet’s take a closer look at the important parts of the two functions used to store\nthe object. The first function is InsertObjectAndAttributePrimaryKeys. Inside this\nfunction, you execute the following steps:\n• If the object already exists in the instance table, or if it is a native database type\n(for example, integer, string, and so on), then return.\n• Iterate over all attributes of the object and its ancestors and call InsertObject\nAndAttributePrimaryKeys on each of them.\n• Check if the table corresponding to the class of the object actually exists in the\ndatabase. If not, create it.\n• Finally, at this point in the function, all the attributes of the object and its ances-\ntors have been processed (recursively), so now you generate a primary key value\nfor the object itself and insert the object in the database with this value.\nThe second function is UpdateObject. Here, you have the following steps:\n• If the object is a native database type, return.\n• Iterate over all attributes of the object and its ancestors and call UpdateObject on\neach of them.\n• Create and execute the SQL statement to update the object’s contents.\n528\nSection 7\nScripting and Data-Driven Systems \n\n\nUpdating the Object’s Contents\nGenerating an SQL UPDATE statement consists of four parts. First is the name of the\ntable you want to update, next is the list of the field names you want to update, after\nthat are the field values you want to store, and finally the condition that decides what\nrows are getting updated.\nSelecting the name of the table you are going to update is quite easy; the table has\nthe same name as the classname stored in the metatype of the object. The condition\npart of the statement is also straightforward. You use the primary key value corre-\nsponding to the object pointer. This value can be retrieved from the instance table.\nThe interesting part is generating the field names and values. Let’s start by look-\ning at how to generate those field names.\nThe data in a C++ class consists of a group of attributes found in the class and its\nancestors. When creating the list of field names, start with the attributes in the class\nitself. This is quite simple as the field name is the same as the attribute name. For the\nparent class, you store the complete contents in a field called _Parent. Attributes in\nthat field can be referenced just like accessing a member of a class in C++, by writing\nit in the form _Parent.attributeName. If the parent class has a parent class containing\nattributes, you can access those fields in a similar way, by doing _Parent._Parent.\nattributeName. So, if you want to generate the list of field names for the SQL state-\nment for the ancestors, you iterate over each ancestor’s attributes, write out the names,\nand prefix each name with one or more _Parent. strings. For each level you go up in\nthe hierarchy, you add one extra _Parent. string in front of the attribute name.\nFor example:\nclass Base\n{\nint a;\n};\nclass Subclass : public Base\n{\nint b;\n};\nClass SubSubclass : public Subclass\n{\nint c;\n};\nGenerating the names of the attributes of class SubSubclass results in the following:\nc\n_Parent.b\n_Parent._Parent.a\n7.2\nSerializing C++ Objects into a Database Using Introspection\n529\n\n\nGenerating the values for each field should of course happen in the same order as\ngenerating the names; otherwise the data will get mangled up. You iterate over each\nattribute and depending on what type of attribute it is, generate the field value in a\ndifferent way. There are three cases, described in the next three sections.\nNative Database Type\nThis is a scalar or a string. Use the WriteObjectToText function to convert the\nattribute to a string, which you can use in the SQL statement.\nObject or Pointer to an Object\nIn this case, you take the memory address of the object (or of the object pointed to in\nthe case of a pointer), get its metatype, and get the primary key value from the\ninstance table. Remember that the instance table is guaranteed to contain those values\nbecause you generated them before generating the UPDATE statement. With this pri-\nmary key, you construct the string containing the primary key-classname pair, as\ndescribed in the “Pointers” section.\nArray\nWhen encountering an array, you construct a string corresponding to an array repre-\nsentation in SQL. The format is \"{ item1, item2, …,itemN}\". Something you need\nto know when retrieving the object later on is the number of items in the array. You\nsimply store the item count as the first element of the array, for easy retrieval later. To\ngenerate the rest of the string, you take the metatype of the object stored in the array,\niterate over each of the items, take a pointer to the item in the array, and execute the\ngeneration of the attribute value for that item.\nIf the array contains pointers to objects, you can iterate over those using the fol-\nlowing code:\nitem_address = *( array_data + sizeof( void * ) * item_index )\nWhere array_data is the start of the array buffer and item_index is the index of\nthe item you want to use in the array.\nIf the array contains instances of objects, the code looks like this:\nitem_address = array_data + item_meta_type.GetByteCount() * item_index\nIn this case item_meta_type is the metatype of the items in the array (in a typical\ntemplated array class it will be the metatype of the template argument type).\nUsing these strings, you can assemble the SQL UPDATE statement and change the\ncontents of the objects in the database.\n530\nSection 7\nScripting and Data-Driven Systems \n\n\nRetrieving an Object\nAs with insertion, object retrieval happens in two steps. First, you execute a SELECT\nstatement, which returns a list of primary key values. Next, for each primary key\nvalue, you check whether the object already exists in the instance table. If it does, you\nreturn its pointer. If it does not exist, you build a SELECT statement to fetch the field\nvalues. The result of this query is used to fill in the attributes of the object you want\nto retrieve. In pseudocode, it looks like this:\nprocedure GetObjectsFromDatabase( object_table, meta_type, predicate )\nbegin\nkey_table = GetAllPrimaryKeysCorrespondingToPredicate( predicate )\nforeach key in key_table do\nif HasObjectInInstanceTable( key, meta_type )\nobject_table.Add( GetObjectFromInstanceTable( key, \nmeta_type ) )\nelse\nobject_table.Add( CreateNewObject( key, meta_type ) )\nendif\nendfor\nend\npredicate contains the filter applied to the query, meta_type is the metatype of\nthe class you want to retrieve instances from, and object_table is the table that will\ncontain the result of the query, a list of pointers to the instances.\nIn the case of creating a new object, you use its metatype to construct a new\ninstance. The MetaType class has a method called CreateNewInstance to accomplish\nthis.\nNext, you generate the SELECT statement. As is the case for the UPDATE statement,\nyou generate a list of the field names of the attributes of the class and its ancestors,\nwhich will be used to specify the field values you want to recover and the order in\nwhich they will be recovered. When iterating through the result of this query, you\nneed to consider the different types of attributes you’ll encounter.\nNative Database Type\nReadObjectFromText is used to convert the string version of the value into the scalar or\nstring value of the attribute.\nPointer to an Object\nYou retrieve a primary key + classname pair in a string. First, check whether an object\ncorresponding to this pair already exists in the instance table. If it does, get its address\nand store it in the pointer attribute. If it does not, create a new instance, execute the\ncode to retrieve that object’s contents from the database, and store the new instance’s\naddress in the pointer.\n7.2\nSerializing C++ Objects into a Database Using Introspection\n531\n\n\nObject\nAs in the previous case, you retrieve a primary key + classname pair in a string.\nBecause this is an attribute, you do not have its address in the instance table. You add\nthe address in the instance table, together with its primary key value and metatype,\nexecute a SELECT statement to retrieve its attribute’s values, and fill in its contents.\nArray\nAn array is represented by a string of the format \"{ item_count, item1, item2,\n…,itemN}\". Using some string manipulation, you get the item_count of the array. You\ntake the pointer to the attribute, cast it into a pointer to an object of type BaseArray,\nand set its item_count. Next, you get the pointer to the data and iterate over each of\nits elements. Using the item’s metatype you get from the BaseArray class, you can cat-\negorize the elements in the array (native database type, pointer to an object, or\nobject). For each of those elements, you get its address and its string representation\nfound in the array text received from the database, and use this to fill in the element\nvalue.\nThe Demo\nThe demo is not supposed to represent an industrial-strength full-fledged database\nserialization implementation, but more of a proof of concept, where all the code that’s\nnot directly related to this gem has been kept to a minimum for the sake of clarity. In\norder to use this demo, you first need to do some things, because it requires a running\nPostgreSQL server. There is a version of PostgreSQL included with the demo, so if\nyou do not have one on your machine you can use this. Install it with the default\noptions, and when asked to create a database superuser with the name “postgres,” use\ngem as your password. Once you do this, the database will be ready for use and you\ncan run the test application.\nJust running the application will not show much, but if it runs without errors you\nknow the application is functioning correctly. To understand what is going on in the\ndatabase system, you are encouraged to trace through the code. Just by following the\nsteps executed in the main function of the application, you can see which kinds of\nobjects are created, inserted in, and retrieved from the database. There are some tests\nafter object retrieval that verify that the object returned is correct.\nUsing pgAdmin, you can look at the tables created and the way the objects are\nstored in the database.\nIssues and Future Improvements\nAs with most things in life, the system presented here is far from perfect and there are\na number of improvements that can be made to it. \n532\nSection 7\nScripting and Data-Driven Systems \n\n\nThere are things you can do in a C++ object that are not supported by this sys-\ntem. For example, storing a pointer to a string, a pointer to an integer, or any other\nnative database type. These issues can all be solved, but are beyond the scope of this\ngem.\nCurrently, the system supports pointers to objects stored as attributes in a class\n(the demo has an example of this); however, this will not work if the attribute pointed\nto will be processed by the database after encountering the pointer that points to the\nattribute. This issue can be solved by implementing a two-pass approach, where first\nall attribute addresses are stored in a table before beginning the serialization process.\nMemory management has not been touched upon. Imagine having a pointer to\nan object stored in the database instance table, but the application has already deleted\nthis object, creating a dangling pointer. Using smart pointers and reference counting\nis one of the ways to address this issue.\nOne of the strengths of using a database is that you can execute queries with pred-\nicates. In this demo, the database manager supports only simple filters that work\ndirectly on an attribute of the class you are trying to retrieve (for example, \"a =  5\").\nThere is still a lot of room for improvement. You could add functionality that allows\nfor more complicated filters or supports filters in a more human-readable format,\nwhich would then be converted to an SQL statement internally.\nWhen working on a centralized database with many users, one of the challenges\nis to keep every user’s local view synchronized with the database contents. One of the\nimprovements to this system is to implement some kind of notification mechanism\nthat notifies the database manager of user X when user Y modifies some content that’s\nrelevant to user X.\nConclusion\nThis gem presents a way of storing C++ objects in a database using a metadata system.\nThe mechanism is non-intrusive, meaning that there are no changes needed to classes\nwhose objects need to be stored. There are several advantages to this kind of approach.\nYou can store many small objects in a database and quickly retrieve the ones you’re\ninterested in, without having to read a lot of files on disk. Because the database is cen-\ntralized, there can be multiple users accessing the same data, adding new objects, read-\ning them and modifying them, and a database system has all the mechanisms needed\nto work with concurrent access, something that’s much more difficult to accomplish\nusing regular files on a shared network drive, especially when dealing with large\namounts of relatively small data. It also allows any C++ programmer to store objects\nin a database and retrieve them, without needing any knowledge of SQL, because all\nthe implementation details are hidden inside the database manager.\n7.2\nSerializing C++ Objects into a Database Using Introspection\n533\n",
      "page_number": 559,
      "chapter_number": 58,
      "summary": "This chapter covers segment 58 (pages 559-566). Key topics include object, database, and attribute. The actual code will need to take some more things into account, such as\ngenerating the commas between the field declarations and generating the right quotes\nin the SQL statement so there are no case issues.",
      "keywords": [
        "Object",
        "database",
        "primary key",
        "SQL",
        "instance table",
        "key",
        "database type",
        "native database type",
        "SQL statement",
        "pointer",
        "attribute",
        "type",
        "instance",
        "primary",
        "statement"
      ],
      "concepts": [
        "object",
        "database",
        "attribute",
        "parent",
        "classes",
        "pointer",
        "array",
        "strings",
        "values",
        "type"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 7,
          "title": "Segment 7 (pages 52-61)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 13,
          "title": "Segment 13 (pages 119-126)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 15,
          "title": "Segment 15 (pages 285-307)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 20,
          "title": "Segment 20 (pages 181-194)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 16,
          "title": "Segment 16 (pages 144-152)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 59,
      "title": "Segment 59 (pages 567-580)",
      "start_page": 567,
      "end_page": 580,
      "detection_method": "topic_boundary",
      "content": "References\n[Abrahams06] Abrahams, D., and Gurtovoy, A. C++ Template Metaprogramming:\nConcepts, Tools and Techniques from Boost and Beyond, Addison-Wesley Profes-\nsional, 2004.\n[Postgresql06] The PostgreSQL Global Development Group. “PostgreSQL 8.2.4\nDocumentation,” available online at http://www.postgresql.org/docs/8.2/static/\nindex.html, 2006.\n534\nSection 7\nScripting and Data-Driven Systems \n\n\n535\n7.3\nDataports\nMartin Linklater\nmslinklater@mac.com\nO\nne of the apparent laws of game programming is that as game projects grow in size\nthey become more complex and more difficult to manage. Because games are\nobviously getting bigger, game programmers have to deal with increasing amounts of\ncomplexity. There are two ways to manage this complexity—you either work harder\nand longer or you create better systems to manage the complexity. I for one would\nrather go for the second option. One aspect of this complexity is managing how data is\nrouted through the various systems present in the game code. Code modules commu-\nnicate by passing data around between themselves and exposing certain parts of their\ninternal data to their modules. This data needs to be stored in a format that each of the\nmodules involved can understand. The need for common knowledge shared between\nmodules creates both runtime and compile-time dependencies and more dependencies\nmeans more complex code structure and longer compilation times. Dataports are a\nway of helping to manage this complexity by reducing compile-time dependencies and\nmaking the runtime behavior more flexible and data-driven.\nThere are two basic ways of controlling communication between code modules—\nyou either code it up, binding pointers to data explicitly in the source code, or you\ncreate a data-driven system and define the data linkage by loading and parsing exter-\nnal linkage definition files. Coding behavior explicitly suffers from two main disad-\nvantages—first you have to rebuild your code whenever you change data connections,\nand second, because the behavior is explicitly encoded in the executable, it can be\nproblematic to extend or alter the behavior at runtime or post release. Dataports are a\ntool to help you create a more dynamic and data-driven flow in your programs.\nConceptual Overview\nConceptually, dataports are very simple. A dataport is a piece of data that has a unique\nglobal identity. This data can be a structure, a class, or a simple C++ data type. Once\nthey are created, dataports register their identity with a manager class. Code elsewhere\nin the program can then get access to the dataport by creating a dataport pointer and\nasking the manager class to bind it to the desired dataport. There are various nuances\nto the implementation, but the basic pattern that you need to visualize is that of data\nstructures and pointers.\n\n\nThe Dataport\nThe dataport itself is just a template wrapper for a programmer-defined piece of data.\nThere are only a couple of basic methods of a dataport, as follows:\nvoid Register( std::string ID );\nOnce created, the dataport needs to register its identity. Once registered, the data\nis public and available for other pieces of code to interrogate.\nvoid DeRegister( void );\nCalling DeRegister removes the dataport from public view.\nThe Dataport Pointer\nThe dataport pointer is in most respects a traditional pointer. The difference is that\nthe actual binding of pointer to data is done by the Dataport Manager, rather than\nbeing statically defined by source code and bound by the linker. The two methods of\ndataport pointers are as follows:\nDataport<T>* Attach( std::string );\nThis call asks the Dataport Manager to attach your dataport pointer to your\nrequested dataport. This call returns the pointer to the dataport object, so the actual\nline of C++ is as follows:\npDataport = pDataportMgr->Attach( \"Dataport ID\" );\nTo detach a dataport pointer from the data it points to, you need to call the\nfollowing:\nDetach();\nThe return value tells you whether this was successful or whether an error occurred.\nAfter you have attached a dataport pointer to a dataport, you access the data con-\ntained by using the data member.\npDataport->data.<member variable>\nThe Dataport Manager\nThe Dataport Manager is the hidden backbone to the dataport system. At its heart,\nthe Dataport Manager is a storage and retrieval system containing a list of dataports\nthat have been registered. The Dataport Manager deals with pointer binding and\nmanages reference counting. It is worth thinking a little about the implementation of\nthe Dataport Manager because you need to optimize the internal algorithms to suit\nyour application’s usage patterns.\n536\nSection 7\nScripting and Data-Driven Systems \n\n\nIf you will be creating and deleting dataports rapidly and binding infrequently,\nyou need a representation that has good create and delete performance, but that does\nnot necessarily have good search performance. On the other hand, if you create and\ndelete dataports infrequently and bind pointers often, you might need a representa-\ntion that has a fast searching performance compared to create and delete performance.\nBecause this example uses C++ for this particular implementation, it capitalizes\non the STL library and uses an STL <list> as the internal storage mechanism. I rec-\nommend this as a general and easy solution unless your profiling later shows that you\nneed a customized solution. The STL library was written with runtime performance\nas a primary aim, and it’s a shame not to use tested and robust code that’s already been\nwritten.\nThe Dataport Manager is a singleton class, meaning that there is always only one\ninstance in memory at runtime. This is because dataports have a universal unique\nidentity and, although it is possible, there is little benefit to be gained by running\nmultiple Dataport Managers in parallel.\nType Safety\nThe first time I implemented a dataport system, I didn’t have any form of type check-\ning in place. It didn’t take long before I refactored the code to include type checking,\nbecause it was entirely possible to bind one type of data to a pointer of a different\ntype. This is certain to introduce some difficult-to-track-down bugs in your code.\nImplementing type safety is definitely a good thing. The code contains a handy C++\ntemplate function (GetID<T>) which, when given a class, returns a unique 32-bit\nnumber identifying that class. It is essentially a pointer to an instance of a static class\nfunction. This ID is used by the Dataport Manager to prevent name collisions\nbetween different types.\nReference Counting\nWhenever you work with data and pointers, there is a danger of a pointer that was\nonce valid becoming invalid for some reason. These hanging pointers can be very diffi-\ncult to track down because the ensuing crash might happen a long time after the data\nhas become invalid. Ideally, you should not be able to make data invalid if there are\nstill pointers pointing to it.\nDataports use reference counting to help debug problems like these. Although\nnot strictly required for functionality, reference counting is a huge help in debugging\npotential problems.\nThe dataport template defines an m_refCount member that all dataports inherit:\n• When the dataport is registered, this reference count is set to zero.\n• When dataport pointers attach to a dataport, its reference count is incremented\nby one.\n7.3\nDataports \n537\n\n\n• When dataport pointers detach from a dataport, its reference count is decremented\nby one.\n• When a dataport is de-registered, its reference count is checked. If it does not\nequal zero, there has been a mismatch somewhere and an error is returned \n(kErrorNonZeroRefCount).\nDataport reference counting is a great help with debugging, but it does incur a\nsmall runtime performance penalty. You should consider removing reference counting\nfrom your final release builds to remove this performance penalty. As long as you keep\nreference counting in your debug builds, you will gain the extra debugging informa-\ntion that reference counting gives.\nPractical Examples\nI have been using dataports since 2000 and they have proven to be a very useful addi-\ntion to my programming toolkit. The following sections explain a few examples of\nhow I have used dataports in the past.\nCamera Systems\nIt is very useful to encode a level of abstraction into the camera system in a game. I\nabstract the camera system into tripods and cameras. Tripods are classed as camera\n“attach points” and can be placed in the scene at will. The player controller object has\nmultiple tripods, and things like the debug fly cam have a tripod. These tripods are\ncreated as dataports. The actual cameras that drive the renderer have a tripod dataport\npointer as a member variable. To move a camera to a new location, the camera’s tripod\ndataport pointer was simply detached and reattached to a different tripod.\nOnce this system is in place it is very easy for people on the team to create new\ntripod attach points and attach the cameras to them at runtime. Because all of this can\nbe driven by human readable text identifiers, a great deal of flexibility is added to the\ncode. Once the tripod and camera classes are set up, there is little or no maintenance\nneeded to introduce new camera viewpoints.\nShip Handling Debug Values\nThe handling stats for the ships in both Quantum Redshift and Wipeout Pure were\nheld in human readable XML file format. The filenames for these files included the\nteam names that were associated with the statistics. On program boot these files were\nloaded and given dataport names derived from their filenames. Then, when ships\nwere created in-game, they could bind with their corresponding handling statistics\nvery easily. New teams could be added to the code without having to explicitly add\nextra bindings to their handling statistics. This simple data-driven model simplified\nthe task for both programmer and designer.\n538\nSection 7\nScripting and Data-Driven Systems \n\n\nBroadcasting Positional Information\nOne of the most infuriating things when coding game logic is getting hold of data\nburied inside different classes. Drilling into game class data while maintaining object-\norientated encapsulation and data access rights can become a tricky engineering job in\nitself. Commonly accessed game object data can be wrapped in a dataport and exposed\nto the rest of the game engine in a very simple manner. Rather than trying to memo-\nrize how you navigate through your class hierarchy to get at data, you just need to\nknow its type and its ID, and then let the Dataport Manager find it for you. Things\nlike game object positional information can be wrapped in dataports for easy access by\nother systems.\nIn the past, I have also set up HUD dataports so the in-game HUD can be\ndynamically driven by different game objects very easily. Once you have your dataport\nstructures locked down and you decide on a sensible ID scheme, you can get hold of\ndata very easily.\nProblems\nDataports are certainly not the silver bullet that will make your code easy to use and\nbring about world peace. Sadly, they also have some drawbacks.\nIf you use any form of hashing in your Dataport Manager, it is entirely possible\nthat you will get hash collisions. You can mitigate this problem a little by including\nthe dataport type into your storage, but you will have to deal with hash clashes in a\nsensible manner. In the example code, I don’t use hashing at all, but for performance\nreasons you might want to introduce hashing into your release builds.\nDataports are harder to debug. By their very nature, dataports introduce a level of\ndynamism and freedom into your data binding that you may find makes debugging\nharder. You can mitigate this difficulty by introducing more debug and logging code\ninto the dataport system, but you are going to have to live with the idea that you are\nmaking the code harder to debug.\nHeavy use of a dataport system can introduce some fairly substantial performance\npenalties. Dataports are not meant to be a direct replacement for all your pointer\nusage, and you need to balance the need for flexibility against the added CPU over-\nhead needed to create, delete, and bind dataports with dataport pointers.\nMy personal choice is to use dataports for commonly accessed game elements that\nneed to have global scope and need to be dynamically accessed by multiple code mod-\nules. As long as you are sensible you shouldn’t see dataports hit your frame rate at all.\nIn fact, I have yet to see dataport operations show up during performance measure-\nment with final game code.\n7.3\nDataports \n539\n\n\nConclusion\nThe current implementation does not encode any sort of access behavior into the\ndataport system—all dataports are read/write and have global access. People used to\nconst pointers might feel decidedly uneasy about this freedom, and might want to\nadd “const-ness” into the dataport API. So far, I have not yet felt the need to add this\nto my implementation, but I can appreciate the desire for that extra layer of support\nthat const pointers can give the API.\n540\nSection 7\nScripting and Data-Driven Systems \n\n\n541\n7.4\nSupport Your Local Artist:\nAdding Shaders to Your\nEngine\nCurtiss Murphy; Alion Science \nand Technology\ncmmurphy@alionscience.com\nR\necent advances in hardware have made shaders an essential part of visually com-\npelling games. There are now dozens of books, including this one, filled with\nthousands of shader techniques and best practice examples. Your team is raring to go.\nSo what now? How are you going to integrate shaders into your engine? As a devel-\noper, it is tempting to just code the shaders directly into your actors. Unfortunately\nthat approach leads to a dangerous coupling of code, art assets, and shader parameters\nas well as a hard-coded, inflexible solution that is averse to scaling. So, what are you\ngoing to do? This gem is here to help.\nFollowing is a data-driven design to help you incorporate shaders into your\nengine. This design presents good techniques for isolating most shader parameters\nfrom your actor logic. It provides support for simple parameters such as floats and\nintegers as well as more complex parameters such as textures and automatic oscillating\nvalues. The resulting implementation allows artists and level designers to define\nshaders and parameters in XML with little programmer involvement. An example use\ncase shows a blimp that hovers in three dimensions and applies an animated pink\nhighlight; the example is integrated into the engine with zero lines of code. This gem\nincludes a fully working implementation that can be used as the basis of a more com-\nplete solution. \nShader Terminology\nThere are several terms that often confuse newcomers to shader development. The\nfirst is the basic definition of shader itself. Originally, shaders got their name because\nof the way pixels were shaded. Now, there are two kinds of shaders—the vertex shader\ncan manipulate vertices and the geometry shader can manipulate an entire model. \n\n\nA better term is processor. After all, shaders can process all sorts of data and are used\nfor much more than just shading. However, the term shader has become the standard\nand will be used by this gem. \nAnother source of confusion is use of the terms fragment shader and pixel shader.\nThey sound like they should be different, but in actuality they are the same. The term\nfragment means that the shader is only computing a “potential” pixel. That is, a pixel\nthat is computed as part of the graphics pipeline but that might not become part of the\nfinal frame buffer. So, for example, “per-pixel-lighting” is really “per-fragment-lighting.”\nIn an ideal world all fragments would become real pixels—there would be no over-\ndraw—so you would only need one term. However, in practice, it is more efficient for\nthe GPU to compute extra fragments than it is to perfectly isolate each pixel. \nThis gem presents general concepts that can be used in both OpenGL and\nDirectX. Because they are both state-driven, the mechanics and concepts are easily\ntransferable. State refers to the entire rendering pipeline, including bound shaders,\nrender states, bound textures, and so on. Mesh refers to any object, geometry, node, or\nprimitive that can be drawn with a single state. For simplicity, this gem generally uses\nOpenGL terms and the OpenGL Shading Language (GLSL). The example applica-\ntion is built using OpenSceneGraph (see http://www.openscenegraph.org) within the\nDelta3D Open Source Game Engine (see http://www.delta3d.org). \nPrograms and Parameters and Managers, Oh My!\nThe primary classes of this solution are the ShaderProgram, the ShaderParameter, and\nthe ShaderManager. This section describes the general purpose of these three classes.\nThe ShaderProgram Class\nThe ShaderProgram class is the heart of this solution. It corresponds directly to the\nconcept of a program (in OpenGL) or effect (in DirectX). The program is what most\npeople mean when they refer to a shader. It is the compiled executable associated with\nthe vertex and fragment shaders. The ShaderProgram class holds onto the actual\nOpenGL program. A program can have a vertex shader, a fragment shader, or both.\nHowever, the most important job of the ShaderProgram class is to hold a list of the\nshader parameters. \nThe ShaderParameter Class\nThe ShaderParameter class holds onto a single uniform variable. Each parameter\nvalue is bound directly to a mesh via its state and is the primary way that an applica-\ntion communicates with shaders. A parameter can be a base color highlight, a gloss\ntexture, a blur weight, an offset point, particle density, alpha strength, or almost any\ntype of value that you want to pass into the shader. The parameters are uniform\nbecause they stay the same for every vertex and pixel drawn by that node during a\nsingle frame. Most parameters are simple data types such as float, int, vec3, and\n542\nSection 7\nScripting and Data-Driven Systems \n\n\ntexture2D that are used to affect shader output. This architecture also supports com-\nplex parameters that have their own behavior, as seen in the time-based oscillating\nparameter (explained in a later section).\nThe ShaderManager Class\nShaderManager is the class that holds this architecture together. It is responsible for\nloading shader prototypes from the XML definition file, assigning and unassigning\nthe parameters to a state, tracking the assigned shaders, and managing a cache of com-\npiled programs. The manager is what you use to find a shader program and assign it\nto the meshes in your game engine. This class is based on the Singleton Design Pat-\ntern [Gamma95].\nTo put it all together, the manager holds onto the programs and the programs\nhold onto the parameters. Together, the three classes appear as shown in Figure 7.4.1.\n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine\n543\nFIGURE 7.4.1\nThe ShaderManager class diagram.\nFlexibility Is Key\nWhy go through all this trouble? After all, if you are already coding the behavior of\nyour actor, why not just add the shader code directly? The answer is flexibility. Games\nare now vast, complicated software behemoths [Blow04]. They require huge teams of\nartists, designers, and programmers working in concert. In fact, it is becoming\nincreasingly common to have more artists than programmers. In such an environ-\nment, it is critical to ensure the pipeline is as smooth as possible. Artists need to be\nable to test models and shader effects without having to involve a programmer. Shader\ndevelopers need to be able to edit the parameters of a shader, or even add an entirely\nnew shader without having to edit or recompile code. This design provides a flexible\nsystem that meets those needs and helps remove the dependency between program-\nmers and artists. \n\n\nThis architecture is especially helpful in allowing artists to visualize their changes\nlive within the real engine. In most studios, the art pipeline involves a suite of tools\nthat is outside of the actual engine. Often, models are created in one tool, textures in\nanother, and shader code in yet a third. This means that what the artist sees while cre-\nating an asset is disjoint from what a player will see in the actual engine. Sometimes,\nthere are additional tools to preview the asset combined with the shader to make it as\nclose to real as possible. However, regardless of how good the tool, it’s never going to\nbe more than just an approximation unless it is viewed live, in the actual game, with\nactual actors, lights, weapons, shadows, and cameras. There is simply no replacement\nfor seeing the final, combined result. Getting this level of realism was a primary moti-\nvation for the data-driven nature of this architecture. Because the shaders are easy to\ndefine and integrate, artists can test their assets in the real engine almost immediately,\nand they can do it with little programmer involvement.\nTest Mode\nAnother aspect of this solution is the ability to dynamically reload shaders at runtime.\nShader development is an art. As such, it can take hundreds of iterations to get one\n“just right.” Maybe the lighting is too bright, the fog decays too quickly, or the gloss\nhighlights are too sharp. Fortunately, the data-driven nature of this architecture makes\nit easy to reload all the shaders in the system at any time. The ShaderManager knows\nwhenever a new shader is created, keeps a list of all active programs, and has access to\nall parameters. It has everything it needs to reload the shader definition XML and\nsystematically replace existing programs and parameters with the updated values. \nThis behavior is provided by the ReloadAndReassignShaderDefinitions() method\non ShaderManager. With a single key press, the new shader is loaded and the artist can\nimmediately visualize their tweaks in the real engine. Whether using this or some\nother shader system, engine programmers should do everything in their power to\nintroduce an in-game test system that will allow artists and designers to reload shaders\nat runtime without restarting. \nPrototypes\nThis architecture is based on the Prototype Design Pattern [Gamma95]. As a refresher,\na prototype is a prototypical instance that is copied to create a new object. In this case,\nthe ShaderProgram that is loaded from the XML file is really just metadata that is not\napplied to an actual mesh. When the manager reads the definition file, it creates new\ninstances of ShaderProgram and adds them to its list of prototypes. The manager then\nloads the shader source code and parameter variables for each instance. Once complete,\nit precompiles the shaders into a program and adds it to a cache.\nThe prototypes should be loaded and compiled when loading a map or at startup\ntime. That way there is no spike in CPU work when applying a shader to a new object\n544\nSection 7\nScripting and Data-Driven Systems \n\n\nin the middle of the game. This design is further optimized so that any unique shader\ncombination (vertex plus fragment) should be compiled only once. To do this, the\nmanager looks for programs that can be shared across prototypes and stores them in\nmCachedPrograms. Essentially, if two prototypes use the same vertex and fragment\nsource code and only differ by the values of the assigned parameters, they both use the\nsame cached program. For instance, this is used to specify unique prototypes that only\ndiffer by a gloss map texture based on the vehicle type. \nNotice that most of the methods on ShaderManager have the word “prototype” in\nthem. That is because there are only two times when the manager is working with an\nactual instance instead of a prototype—first, when assigning a prototype shader to a\nmesh and second, when unassigning (that is, destroying) a shader instance from a\nmesh. In the first case (for example, AssignShaderFromPrototype()), the manager\nclones the prototype to create a unique shader instance. To support this, both Shader-\nProgram and ShaderParameter provide a Clone() method. The program is a fairly\nlight class, so when it is cloned, it simply grabs a few references and sets a few strings.\nThen it makes clones of all the parameters and adds them to its parameter map. For\neach new parameter, it calls AttachToState(). This method binds the parameter to\nthe actual mesh. Finally, the manager adds the new program instance to a list called\nmAssignedNodes.\nThe second case is much simpler. The UnassignShaderFromNode() method\nensures that each parameter is unbound from the state by calling DetachFromState()\nand then removes the shader instance from the active list. Because the implementa-\ntion uses smart pointers, all objects are cleaned up correctly. The whole process of\nassigning and unassigning results in a unique program instance whose parameters are\npart of a specific mesh’s state. The program is precompiled and shared as a prototype,\nbut applied as an instance. \nState Sets and Scene Graphs\nThis architecture has some added benefits if the engine happens to support scene\ngraphs and state sets. A scene graph is just a hierarchical way of storing the meshes in a\nscene. A state set is a mechanism that allows each mesh to have its own unique state\nvalues and provides the ability to switch between them at draw time. When these two\nideas are combined, you get a hierarchy of meshes that can manage their own state.\nThis type of hierarchy typically allows the values of the state to pass down from par-\nent to child. In other words, each child mesh can set some state values of its own and\ninherit the rest from its parent. The total collection of individual and inherited values\nbecomes the mesh’s active state. \nDuring the draw phase, a shader program is composed from both the compiled\nshader code and the associated parameters. This distinction is exactly analogous to the\ncode block and data block used by the operating system. Typically, in a state-based\nscene graph, the shader program and the parameters are tracked as independent state\n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine\n545\n\n\nvariables. In other words, you can assign the executable shader program to a mesh\nwith or without setting the parameter variables, and vice versa. \nUsing this gem, you can leverage this type of hierarchy to allow a generic, high-\nlevel shader program to cascade down from the top of the scene to any child that\ndoesn’t have its own shader. This could be used to define various default settings such\nas a lighting model. You could also specify “global” shader uniforms without knowing\nwhich program will eventually use them.\nFor example, you could set up global values for an HDR light modifier, custom\nfog variables, beginning and end values for tunnel vision, or the parameters for fish\neye or water blur effect. Because the values cascade down through the scene graph,\nthey become part of each mesh’s state. This should make it easier to tweak global val-\nues for common render effects and result in less overall management of your shader\nparameters. Note that it is possible to achieve a similar effect in the current design by\nusing the mIsShared flag on ShaderParameter (discussed in a later section).\nShader Parameters\nAfter this general overview of the architecture, you should be ready to visit shader\nparameters in more detail. Before you can do that, you need to take a closer look at\nthe way a shader receives data. Generally speaking, there are three types of variables\nthat are sent to the vertex or fragment shader. In OpenGL, they are referred to as uni-\nform, attribute, and varying parameters:\n• Uniform parameters are values that remain the same across an entire piece of\ngeometry. These values do not change during a frame and often they may not\nchange at all.\n• Attributes are like uniforms in that they don’t change very often, but different\nbecause they are unique to each vertex. That is to say, each vertex can have a dif-\nferent value for each attribute used by a shader. Vertex attributes are typically used\nto pass data such as a vertex normal, vertex color, and tangent-space vector. \n• Varying parameters are computed in the vertex shader and sent down to the frag-\nment shader. The GPU interpolates the values across the surface of a triangle\nusing the outputs sent down from each of the three vertices.\nUniforms ’R Us\nWhich of these types of data should be supported by the ShaderParameter class?\nBecause varying parameters are defined entirely in the vertex and fragment shader and\nare generated by the GPU, they are obviously out. That leaves only two types: uni-\nforms and attributes. Consider attributes first. By definition, an attribute parameter\nhas to be set for every vertex. Because each vertex requires its own value for each\nattribute, it is likely that the artist is going to use a 3D modeling tool to set values\n546\nSection 7\nScripting and Data-Driven Systems \n\n\nsuch as a vertex normal or a vertex color. Alternately, some attributes may be com-\nputed by the engine, such as the tangent and bi-tangent vectors. In either case, the\nattribute parameters are essential elements of the art pipeline that need to be agreed\nupon by the whole team and integrated into both the art tools and game engine. Con-\nsequently, there’s no reason for ShaderParameter to manage them. So, with varying\nand attributes both out of the picture, you just need to support uniforms. \nFortunately, uniforms are what you want to manage anyway. Uniforms are typi-\ncally used to pass down lights, fog conditions, clipping regions, and so on. Addition-\nally, uniforms are also perfect for sending general customizations over to the shader.\nThey are the best way to pass down textures such as a gloss map, detail map, or bump\nmap. They are the obvious choice for control values such as depth for tunnel vision or\nany value that is time-based. So the goal is to design parameters in such a way that the\nartist can easily define their own uniforms. \nAre You My Type?\nThere are many types of parameters supported by shader languages, including inte-\ngers, floats, textures, and vectors of all different sizes. This means you will need to be\nable to support multiple types. Further, you should provide a mechanism to allow for\nmore complex types. After all, the goal is to eliminate the need for programmer\ninvolvement, so it would be nice if the design supported parameters with built-in\nbehavior. Clearly, ShaderParameter cannot be all of these types at once, so it needs to\nbe a base class. Each specific parameter type then becomes a subclass. The class dia-\ngram for this is shown in Figure 7.4.2.\nThe base class provides base data members such as the actual uniform variable,\nparent program, and whether the shader is currently dirty. It exposes behaviors\nrequired by each parameter type such as the ability to clone itself and the ability to\nattach and detach itself from a mesh’s state. Each subclass then has control over how it\nbinds itself to the state and what type of value it manages. \nFigure 7.4.2 shows simple types such as ShaderParamFloat and ShaderParamInt.\nIt shows data-heavy types such as ShaderParamTexture2D, which has to load an image\nfrom file or cache and bind it to the texture unit. It also shows ShaderParamOscilla-\ntor as an example of a complex data type. This oscillating parameter cycles its value\nbetween a minimum and maximum value over some time. The default behavior\ncycles the uniform from 0 up to 1 and back down to 0 every two seconds. The artist\ncan customize the min/max range values, time interval, offset value, and how the\nvalue oscillates. You can easily expand this system by adding your own custom types. \nClone\nIn order to support the prototype design pattern, each parameter type needs to be able\nto clone itself. Simple types such as int and float merely create a new instance and\nassign the value. \n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine\n547\n",
      "page_number": 567,
      "chapter_number": 59,
      "summary": "This chapter covers segment 59 (pages 567-580). Key topics include data, code, and coding. C++ Template Metaprogramming:\nConcepts, Tools and Techniques from Boost and Beyond, Addison-Wesley Profes-\nsional, 2004.",
      "keywords": [
        "dataport",
        "Dataport Manager",
        "shader",
        "dataport pointer",
        "dataport system",
        "data",
        "parameters",
        "shader parameters",
        "manager",
        "code",
        "shader program",
        "tripod dataport pointer",
        "system",
        "program",
        "type"
      ],
      "concepts": [
        "data",
        "code",
        "coding",
        "classed",
        "classes",
        "value",
        "parameters",
        "type",
        "games",
        "programs"
      ],
      "similar_chapters": [
        {
          "book": "More Effective C++",
          "chapter": 32,
          "title": "Segment 32 (pages 320-329)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 30,
          "title": "Segment 30 (pages 950-980)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 21,
          "title": "Segment 21 (pages 651-684)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 63,
          "title": "Segment 63 (pages 2016-2049)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.67,
          "method": "api"
        }
      ]
    },
    {
      "number": 60,
      "title": "Segment 60 (pages 581-592)",
      "start_page": 581,
      "end_page": 592,
      "detection_method": "topic_boundary",
      "content": "Data-heavy types should take extreme care to correctly manage a shared reference\nor other cache mechanism. An early version of the ShaderParamTexture2D failed to\ncorrectly manage instance referencing and brought the system to its knees when it\nallocated over 500MB of duplicate texture data. \nThe cloning process enables a very interesting bit of behavior for sharing parame-\nters. Right before the new parameter instance is created, Clone() checks mIsShared to\nsee if the parameter should be shared between cloned instances. A shared parameter\nessentially acts like a global value for all instances of a shader prototype. So, if mIs-\nShared is false, the method performs as expected by creating a new parameter instance\nand copying the values over appropriately. However, if mIsShared is true, the proto-\n548\nSection 7\nScripting and Data-Driven Systems \nFIGURE 7.4.2\nThe ShaderParameter class diagram.\n\n\ntype’s parameter instance is returned instead. The return value is then added to the\nshader program. The result is that two programs will have exactly the same parameter.\nAlthough this can possibly result in weird values, it also sets up the ability to have\nmultiple objects respond to a single parameter. For example, this would allow multi-\nple objects to oscillate in exactly the same way and would allow all instances of a vehi-\ncle to use the same detail texture map. \nUse Case—The Blimp Target\nTo see how the architecture works, let’s examine a simple use case involving a blimp.\nIn this case, the artist wants the blimp to appear to hover in the air with a slight\nbounce in all three dimensions. The artist also wants to apply an animated swirl effect\nto make it look highlighted. To achieve this effect, the artist creates the blimp model\nand defines the following shaders. \nBlimp Vertex Shader\nThe vertex shader is extremely simple. To create the hover, it needs three instances of\nthe ShaderParamOscillator. It processes the X, Y, and Z dilation values and moves\neach vertex in a sinusoidal pattern, as follows:\nuniform float MoveXDilation;\nuniform float MoveYDilation;\nuniform float MoveZDilation;\n// Vertex - Simple blimp shader for 'Hover' and 'Highlight' \n// Lighting was removed for simplicity\nvoid main()\n{\ngl_TexCoord[0] = gl_TextureMatrix[0] * gl_MultiTexCoord0;\ngl_Vertex.x += 1.5 * sin(3.14159 * MoveXDilation);\ngl_Vertex.y += 1.5 * sin(3.14159 * MoveYDilation);\ngl_Vertex.z += 2.0 * sin(3.14159 * MoveZDilation);\ngl_Position = ftransform();\n}\nNow, let’s take a look at the fragment shader. \nBlimp Fragment Shader\nThe fragment shader is a bit more complex. Take a look at the code first: \nuniform sampler2D DiffuseTexture;\nuniform sampler2D HighlightTexture;\nuniform float TimeDilation;\n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine\n549\n\n\n// FRAGMENT - Provides highlight by blending from a detail texture\nvoid main()\n{\nfloat whackyOffset = sqrt(abs(TimeDilation - 0.5) + 1.0);\nfloat x = gl_TexCoord[0].x;\nfloat y = gl_TexCoord[0].y;\n// look up the three oscillating colors \nvec2 lookup1 = vec2(x + TimeDilation, y + TimeDilation+.25);\nvec2 lookup2 = vec2(x - whackyOffset, y + whackyOffset);\nvec2 lookup3 = vec2(x - (TimeDilation*2.0), y + TimeDilation);\nvec4 color1 = texture2D(HighlightTexture, lookup1);\nvec4 color2 = texture2D(HighlightTexture, lookup2);\nvec4 color3 = texture2D(HighlightTexture, lookup3);\n// Now blend the three colors together to make the highlight\nvec4 highlightColor;\nhighlightColor.a = 1.0;\nhighlightColor.r = color1.r*0.6 + color2.r*0.3 + color3.r*0.3;\nhighlightColor.g = color1.g*0.2 + color2.g*0.7 + color3.g*0.2;\nhighlightColor.b = color1.b*0.2 + color2.b*0.2 + color3.b*0.8;\n// Finally, blend the original color and highlight color\nvec4 diffuseColor = texture2D(diffuseTexture, gl_TexCoord[0].st);\ngl_FragColor = (0.2 * diffuseColor) + (0.8 * highlightColor);\n}\nThis processor takes in one float uniform and two texture uniforms (Shader-\nParamOscillator and ShaderParamTexture2D, respectively). To create the swirling\nhighlight, it does three separate look ups into the detail texture. Each lookup is a per-\nmutation of the TimeDilation uniform variable. Then, it uses the lookup to compute\na final highlight color and blends that color in with the original diffuse texture. \nTo integrate the new shaders into the engine, the artist adds the following snippet\nto the shader definition XML file:\n<shader name=\"Green\">\n<source type=\"Vertex\">Shaders/green_vert.glsl</source>\n<source type=\"Fragment\">Shaders/green_frag.glsl</source>\n<parameter name=\"diffuseTexture\">\n<texture2D textureUnit=\"0\">\n<source type=\"Auto\"/>\n</texture2D>\n</parameter>\n<parameter name=\"TimeDilation\">\n<oscillator cycletimemin=\"2.0\" cycletimemax=\"4.0\"/>\n</parameter>\n<parameter name=\"MoveXDilation\">\n<oscillator cycletimemin=\"5.0\" cycletimemax=\"8.0\"/>\n</parameter>\n550\nSection 7\nScripting and Data-Driven Systems \n\n\n<parameter name=\"MoveYDilation\">\n<oscillator cycletimemin=\"5.0\" cycletimemax=\"8.0\"/>\n</parameter>\n<parameter name=\"MoveZDilation\">\n<oscillator cycletimemin=\"5.0\" cycletimemax=\"8.0\"/>\n</parameter>\n<parameter name=\"HighlightTexture\">\n<texture2D textureUnit=\"1\">\n<source type=\"Image\">Textures/green_detail.png</source>\n<wrap axis=\"S\" mode=\"Repeat\"/>\n<wrap axis=\"T\" mode=\"Repeat\"/>\n</texture2D>\n</parameter>\n</shader>\nThis entry defines a shader program called Green. For that, it specifies the vertex\nand fragment shader files. It also specifies two texture parameters and four oscillating\nfloat values that cycle between 0 and 1. Note that the oscillator uses reasonable\ndefaults, so the artist only had to set the oscillation time. \nThe entire effect is realized with zero lines of code. The artist created the vertex\nand fragment shaders and then added an entry to the definition XML file. The artist\nwas able to see the effect in game and was able to repeatedly tweak the magic numbers\nat runtime without having to repeatedly restart. The significant code snippets are pro-\nvided on the CD-ROM and the complete working example with source can be found\nas part of Delta3D (see the section called “Conclusion”). Color Plate 14 in the color\ninsert shows a few examples of dramatically different results that were generated with-\nout a restart.\nAdvanced Techniques\nThe previous sections define a basic architecture that can be added directly to an\nengine. In addition, there are several advanced techniques that are used in the com-\nplete implementation of this system that might be useful in your environment. \nShader Groups\nShader groups allows several related shaders to be lumped together into one group.\nThis allows the definition of separate shaders for each type of actor, such as damaged\nmode, destroyed mode, and normal mode. Alternatively, you could define a shader\ngroup with a targeted and non-targeted shader, or daytime and nighttime shaders for\nall the actor categories in the system. \nTo implement this, add a new class called ShaderGroup that sits between Shader-\nManager and ShaderProgram. This changes the original design in two ways. First, the\nmanager now holds onto group prototypes instead of program prototypes. Second,\nyou have to look up the group by name before you can find the shader within the\n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine\n551\n\n\ngroup. Note that each group tags one of its shaders as the default; there is always one\nto use. In the full example, the blimp has a group with two shaders—one for the green\nhighlight and one for the normal, untargeted look. The following snippet shows an\nexample of an XML definition with groups:\n<shaderlist>\n<shadergroup name=\"Target Shaders\">\n<shader name=\"Normal\" default=\"yes\">\n...\n</shader>\n<shader name=\"Green\" default=\"no\">\n...\n</shader>\n</shadergroup>\n<shadergroup name=\"Tank Shader\">\n<shader name=\"Normal\" default=\"yes\">\n...\n</shader>\n</shadergroup>\n</shaderlist>\nCombining Shaders with Actors and Properties\nAnother advanced feature leverages actors and actor properties. This feature allows an\nartist or level designer to specify which shader to use for an object by setting the shader\ngroup actor property. Just as the XML definition allows the artists to easily define their\nshaders, the actor property system allows the artists to easily define which shader\nshould be assigned to an actor. The result is an API that is friendly to both the pro-\ngrammer and the artist. For fun, the artist used this feature to add a new shader to the\nterrain. For a complete explanation of actors and actor properties, see [Campbell06].\nThe following snippet shows all the code necessary to change the shader applied\nto an actor. This method is automatically called whenever the string property for the\nshader gets set. The shader property is just a string that can easily be defined in a map\nor received across a network in an actor update message. To apply the shader to the\nmesh, the programmer calls the three important methods on the manager: Find-\nShaderGroupPrototype(), GetDefaultShader(), and AssignShaderFromPrototype().\nNote that all error checking is omitted for brevity and that this method reduces down\nto only two calls if shader groups are not supported. \n// Set the Actor Property for Shader Group\nvoid GameActor::SetShaderGroup(const std::string &groupName)\n{\nShaderManager &sm = ShaderManager::GetInstance();\n552\nSection 7\nScripting and Data-Driven Systems \n\n\n// Make sure any old shaders are cleaned up. Better safe than sorry. \nsm.UnassignShaderFromNode(*GetOSGNode());\n// Get the shader group & the default shader \nconst ShaderGroup *group = sm.FindShaderGroupPrototype(groupName);\nconst Shader *defaultShader = shaderGroup->GetDefaultShader();\n// Make a new cloned instance of the shader from the prototype\n// and assign it to the state set for the mesh\nsm.AssignShaderFromPrototype(*defaultShader, *GetOSGNode());\n}\nFuture Work\nAlthough this version of the design is completely functional, there are many possible\nenhancements. The following list is presented as features to consider for your own\nengine and that may eventually be added to the host game engine, Delta3D (see the\n“Conclusion” section). \n• Geometry shaders—This gem does not support geometry shaders. However, based\non the 4.0 specification, it should be a relatively straightforward addition. \n• XML editor tool—Add a tool that helps the artist create the XML shader defini-\ntion file. This is similar to the level editor described in Game Programming Gems\n6 [Campbell06].\n• Generated shader source—Some engines support the ability to generate shader\nsource code at runtime. This design could be augmented to support such a tech-\nnique by inserting the generated shader code into the program prototype instead\nof loading it from disk. The design would benefit from the optimized runtime\ncode while still allowing the artist to test new uniforms. \n• Actor property parameter—Add a new data type to automatically update a mesh’s\nstate whenever an actor property (such as health or velocity) changes.\n• Enhanced cache—Maintain separate caches for vertex and fragment shaders.\nConclusion\nThis gem presents a ready-to-use shader architecture that can be integrated directly\nwith your engine. It makes a case for building a data-driven shader system that can be\nmanipulated outside of engine code, which gives artists the ability to visualize their\nassets inside the real game. It discusses the three primary classes of ShaderManager,\nShaderProgram, and ShaderParameter. It describes how to use the prototype design\npattern to provide a flexible system with good performance. It explains why uniform\nvariables are critical and how to support many different data types. Finally, this gem\ndemonstrates a real use case allowing an artist to hover and animate a blimp without\ninvolving a developer.\n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine\n553\n\n\nFor more examples of this concept, see the source snippets available on the CD-\nROM. In addition, a complete and fully working implementation of this design is\navailable in the Tank Target Example provided by the Open Source Delta3D project\n(see www.delta3d.org). \nReferences\n[Blow04] Blow, Jonathan. “Game Development: Harder Than You Think,” available\nonline at http://www.acmqueue.org/modules.php?name=Content&pa=show-\npage&pid=114, 2004.\n[Campbell06] Campbell, Matt and Murphy, Curtiss. “Exposing Actor Properties\nUsing Nonintrusive Proxies,” Game Programming Gems 6, edited by Michael\nDickheiser, Charles River Media, 2006, pp. 383–392. \n[Gamma95] Gamma, Erich, Helm, Richard, Johnson, Ralph, and Vlissides, John.\nDesign Patterns—Elements of Reusable Object Oriented Software, Addison-Wesley,\n1995, pp. 117–126.\n554\nSection 7\nScripting and Data-Driven Systems \n\n\n555\n7.5\nDance with Python’s AST\nZou Guangxian\nI\nn any MMORPG, there are plenty of conversations between NPC and player. It\ntakes processor time to encrypt them and costs a lot of bandwidth to transfer them.\nPython is a dynamic object-oriented programming language, and is widely adopted in\nMMORPG development. It is also powerful, and with the Python compiler package,\na developer can manipulate the Abstract Syntax Tree (AST) and the process of analyz-\ning and generating Python bytecode can be controlled at runtime. \nThis gem describes how to replace strings with numbers (ID) by manipulating\nthe AST. This way, bandwidth and runtime costs can be saved. A tool based on this\nidea is also given here. \nIntroduction\nIn computer science, AST means Abstract Syntax Tree. It is generated by the parsing\nphase, and it is used as the source of the bytecode generator. Its internal nodes are\nlabeled with operators such as addition or concatenation, and the leaf nodes represent\nthe operands of the operators. Thus, the grammar rules of the language can be illus-\ntrated with an AST. By visiting the nodes in the AST in order, code generation can be\nperformed and the bytecode will be emitted. \nEach node in the AST has special meaning and information, including whether it\nis a variable or a constant, and if it is a constant, what is its value? By making use of\nthis information or changing it, the programmer can control the bytecode generated,\nthat is, to change the meaning of source code. \nBackground\nThe standard way to handle text in games is to use a translation table. Each string in\nthe game is assigned an ID, and the ID is looked up in a table of strings each time it\nis displayed. The advantages of using IDs instead of raw strings are that they save\nbandwidth and memory, they can refer to any audio speech that goes with them, and\nthe language can be changed easily when translating to different territories without\nchanging the code.\n\n\nHowever, tracking all these IDs takes a lot of time and management during the\ndevelopment of the project. It is much easier and quicker to simply use the strings\ndirectly when trying out concepts. However, this means they then need to be tracked\ndown later and replaced by string ID lookups, and the code needs to be changed to\nperform the string lookup. This is a time-consuming task, and it is easy to make mis-\ntakes or miss instances.\nIt is useful to have a way to examine existing code to find all the strings, enter\nthem in a database, and then track them. As a bonus, once you can inspect code for\nstrings, it is just as easy to edit code and change strings. Instead of changing the code\nto do an indirection through a table each time, it is far simpler to edit the code to\npoint directly at the string. In the rare event that the player changes language or the\nserver updates some text, the edits can be performed again, but otherwise there are no\nextra indirections in the places where the strings are used, reducing code complexity.\nSolution\nPython strings are enclosed in single quotes (' and ') or double quotes (\" and \"). For\nexample:\ncompanyName = 'NetEase.Co'\nprojectName = \"Tang Dynasty\"\naddress = \"\"\"\nGuangZhou,\nChina\n\"\"\"\nFor general programs, it is not easy to extract these strings. Writing a parser to do\nit is tricky and not something to be attempted lightly.\nFurthermore, there can be cases where replacing the text of the string with the\nlookup function in the source text will produce problems, such as being in the wrong\nscope, or when it’s part of a compile-time macro.\nFortunately, Python already has a good mechanism to simplify this job. In the\ncompiler package, there are five functions: \ncompile( source, filename, mode, flags=None, dont_inherit=None ) \ncompileFile( source )\nparse( buf )\nparseFile( path ) \nwalk( ast, visitor[, verbose] )\ncompile and compileFile both compile the source code; however, compileFile\ngenerates a .pyc file and compile returns a code object.\nparse/parseFile returns an AST for the Python source code in the buffer or in\nthe file specified by path. \nThe walk function does an ordered walk over the AST and calls the appropriate\nmethod on the visitor instance for each node encountered. For example, when a Const\n556\nSection 7\nScripting and Data-Driven Systems \n\n\nis encountered, a visitConst function will be called. In general, for a node Type, if the\nvisitType exists, visitType will be called; otherwise ASTVisitor.default will be\ncalled. So, if you can provide a visitor to the walk function with appropriate method,\nthe node information in AST can be extracted. \nTo solve this problem, a visitor that implements visitCallFunc should be pro-\nvided. There are two phases to the solution. First, find all the constant strings in the\ncode, assign an ID to each string const found, and save the relationship between the\nstring and the ID to a file called stringres.txt. In the second phase, the walk is per-\nformed again, the string is replaced with the ID, and the new .pyc file is generated. \nTo help understand the AST, astshow.py is provided on the CD-ROM, and it will\nproduce the formatted output of the AST. Here, I explain what you will get when a\nfunction was called in source code. \nFor example, if a file called sample.py contains this:\nimport game\ngame.msg2player( \"hello\" )\nUse the walk function to walk through the AST of the previous source code, and\nthe node passed to visitCallFunc will be:\nCallFunc(Getattr(Name('game'), 'msg2player'),\n[Const('hello')], None, None)\nIts child node is \"Getattr(Name('game'), 'msg2player')\" and its argments are\n\"[Const('hello')]\".\nThe full name of a function can be extracted with the following function:\ndef getFunctionName( node ):\nif isinstance(node, compiler.ast.Name):\nreturn node.name\nelif isinstance(node, compiler.ast.Getattr):\nreturn getFunctionName(node.getChildNodes()[0])\n+ '.' + node.attrname\nelse:\nreturn ''\nIn the first phase, visitCallFunc can be implemented as:\nimport os\nfrom compiler import ast, pycodegen\nimport utils\nclass Visitor:\ndef visitCallFunc(self, node):\nfunc_name = utils.getFullName( node.node )\nif func_name in utils.helper.functions :\nfor arg in node.args :\nif isinstance( arg, ast.Const ) :\nif isinstance( arg.value, basestring ):\nutils.helper.append( arg.value )\n7.5\nDance with Python’s AST\n557\n\n\n# the rest is copied from pycodegen\n# and simply continues to walk the AST.\npos = 0\nkw = 0\nself.visit(node.node)\nfor arg in node.args:\nself.visit(arg)\nif isinstance(arg, ast.Keyword):\nkw = kw + 1\nelse:\npos = pos + 1\nif node.star_args is not None:\nself.visit(node.star_args)\nif node.dstar_args is not None:\nself.visit(node.dstar_args)\nIn this function, the argument will be checked and any const string argument\nwill be added to the ID-string map by the utils.helper.append function.\nIn the second phase, the bytecode will be generated by calling compile/compileFile.\ncompile/compileFile have a strong coupling with CodeGenerator so that you cannot\ndefine a class inherited from CodeGenerator to affect the result. Instead, you can\nassign a custom function as the visitCallFunc to the CodeGenerator, taking control\nof the generation process. The visitCallFunc is given here, and when arg.value is a\nconst string, it is replaced by the ID. \nimport os\nfrom compiler import ast, pycodegen\nimport utils\ndef visitCallFunc(self, node):\nfunc_name = utils.getFullName( node.node )\nif func_name in utils.helper.functions :\nfor arg in node.args :\nif isinstance( arg, ast.Const ) :\nif isinstance( arg.value, basestring ):\narg.value = utils.helper.get( arg.value )\n# the rest is copied from pycodegen\n# and simply continues to walk the AST.\npos = 0\nkw = 0\nself.set_lineno(node)\nself.visit(node.node)\nfor arg in node.args:\nself.visit(arg)\nif isinstance(arg, ast.Keyword):\nkw = kw + 1\nelse:\npos = pos + 1\n558\nSection 7\nScripting and Data-Driven Systems \n\n\nif node.star_args is not None:\nself.visit(node.star_args)\nif node.dstar_args is not None:\nself.visit(node.dstar_args)\nhave_star = node.star_args is not None\nhave_dstar = node.dstar_args is not None\nopcode = pycodegen.callfunc_opcode_info[have_star, have_dstar]\nself.emit(opcode, kw << 8 | pos)\nPlease refer to the full source code on the CD-ROM for more details. \nConclusion\nIn this article, with Python’s compiler package, you have the ability to access and mod-\nify the AST. Based on this idea, you get an elegant solution to constructing a string\ntable without programmer help. In addition, the script writer can worry less about later\ntranslations. This process is transparent and can be integrated seamlessly.\nReferences\n[Python] Python language Website, available online at http://www.python.org.\n7.5\nDance with Python’s AST\n559\n",
      "page_number": 581,
      "chapter_number": 60,
      "summary": "This chapter covers segment 60 (pages 581-592). Key topics include nodes, code, and string. An early version of the ShaderParamTexture2D failed to\ncorrectly manage instance referencing and brought the system to its knees when it\nallocated over 500MB of duplicate texture data.",
      "keywords": [
        "shader",
        "AST",
        "code",
        "parameter",
        "shader group",
        "source",
        "fragment shader",
        "source code",
        "artist",
        "shader group actor",
        "shader source code",
        "Python",
        "Adding Shaders",
        "actor",
        "string"
      ],
      "concepts": [
        "nodes",
        "code",
        "string",
        "strings",
        "source",
        "texture",
        "parameters",
        "called",
        "section",
        "sections"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 43,
          "title": "Segment 43 (pages 424-431)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 59,
          "title": "Segment 59 (pages 570-580)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 57,
          "title": "Segment 57 (pages 550-561)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 50,
          "title": "Segment 50 (pages 1600-1629)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 48,
          "title": "Segment 48 (pages 469-476)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 61,
      "title": "Segment 61 (pages 593-600)",
      "start_page": 593,
      "end_page": 600,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank \n\n\n561\nAbout the CD-ROM\nAbout the Game Programming Gems 7 CD-ROM\nThe CD-ROM included with this book contains source code, executable demos,\nlibraries, images, and text. All are meant to demonstrate or supplement the gems in\nthis book. Full appreciation of the book requires perusal of the CD-ROM materials.\nEvery effort has been made to ensure the enclosed source code is bug-free and able to\nbe compiled, the executables run trouble-free, and the images and text are freely view-\nable. Please refer to the book’s Website, http://www.gameprogramminggems.com/,\nfor the most recent details regarding the contents of the CD-ROM.\nContents\nFor ease of location, the materials on the CD-ROM are organized into folders that cor-\nrespond to the sections and gems of the book. For your convenience, an auto-run Win-\ndows executable is provided that helps you locate each folder, but the executable is not\nrequired to browse the CD-ROM’s contents. Source code in each folder has been veri-\nfied to compile with Microsoft Visual Studio C++ 7.0 and Visual Studio 2005 solution\nand project files are usually provided. In many cases, precompiled binaries are also\nincluded. When possible, supplemental libraries have also been included but in a few\ninstances additional libraries must be obtained by the user. Examples of these include\nthe Windows version of OpenAL, available from http://developer.creative.com/ and\nthe DirectX SDK, available from http://msdn.microsoft.com/directx/sdk/.\nSystem Requirements for Windows\nWindows 2000, XP, or Vista is required. A document reader capable of displaying\nMicrosoft Word or PDF documents is needed for article supplements. Examples\nusing or demonstrating graphical techniques require a 3D card supporting DirectX 9.\n\n\nThis page intentionally left blank \n\n\n563\nNumbers\n2D Gaussian distribution, construction of, 202\n3 \u0002 3 matrix, example of, 180\n10Hz, capturing logs at, 267\n16-bit PCM audio engine file format, pros and cons of,\n308–309\n\" (double quotes), using with Python strings, 556\n' (single quotes), using with Python strings, 556\nSymbols\n< and >, use in formulas, 163\nA\nA&C (age and cost), function in cache replacement, \n12–13\nA* search algorithm\nuse of, 290–292\nweaknesses of, 291–292\nAABB (axis-aligned bounding box), use in scenes, 184\nAABB-trees, considering in collision tests, 187\nabsolute values, impact on behavior cloning, 211\nAbstract Syntax Tree (Abstract), using to replace strings\nwith numbers, 555–559\nAC decision-making algorithms, blocks for, 236–238\nacoustics, raytracing for, 302\naction searching, improving, 286\nActionInfo flow, use in Artificial Contender, 241–242\nActionInfo objects\nretrieving from input blocks, 243\nuse in Artificial Contender, 235, 245\nactions, merging, 284–285\nactors, combining with shaders, 552–553\nAdaptive Replacement Cache (ARC) algorithm, \nuse of, 6\nAddRegion() function, use in optical flow, 31\nAddressingScheme class, using with hexagonal grids, 54\nADPCM audio engine file format, pros and cons of,\n308, 337\naffect, relationship to attitudes, 251\naffine mappings, extracting semantics from, 181\naffine matrices, inverting, 181–182\naffine transforms, use of, 180\nAge algorithm\nbenefits of, 9\nexpansion of, 10–11\nusing in cache replacement, 8–13\nage and cost (A&C), function in cache replacement, \n12–13\nAge Percentage Cost (APC)\ncalculating, 9\nrelationship to RC (relative cost), 12–13\nagents\nattitudes held by, 252–253\ncreating with behavior cloning, 210–216\ntraining, 211–212\ntraining for acceleration, 211\nvision-modeling considerations for, 224\nagent-sensing model\nhearing model for, 219\nvision model for, 217–219\nAI (artificial intelligence). See agents\nAI script, building from trees in behavior cloning, \n215–216\nNumbers with “GPG” proceeding refer to previous editions of the Game Programming Gems Series. \nNumbers without this notation refer to the current volume.\nINDEX\n\n\nAIShooter demo, running, 212\nalgorithms\nA* search algorithm, 290–292\nARC (Adaptive Replacement Cache), 6\nblocks for AC decision-making algorithms, 236–238\ncache replacement, 6–8\ncentral limit theorem, 201\nchorus and compression audio processing effects, 301\ncollision detection using MPR, 171–176\ncomponents of, 153\nCSG (constructive solid geometry), 159\nDijkstra’s algorithm and A*, 291\nfarthest feature map, 150\nFringe Search, 293–294\nIDA* (Iterative Deepening A*), 292\nK-medoids clustering, 276–277\nLoop subdivision, 383–390\nLRU (Least Recently Used), 6\nLucas and Kanade, 31–33\nLucas and Kanade algorithm in optical flow, 29–30\npage-replacement, 6\nparticle deposition, 353–354\nplan-merging algorithm, 284–286\npolar-rejection, 200\npolygon cutting, 162–163\nraytracing, 127\nrecursive node learning, 213\nridge structures, 359–360\nRP2 operations, 161–164\nskeletal animation, 367, 370\nsum-of-uniforms, 201\nvictim page determination, 6\nWELL algorithm, 120–121\nwhitening algorithms used with RNGs, 116\nworkflow for Artificial Contender, 230–232\nallocated blocks, resizing, 23\nallocation hooks\nimplementing, 101–102\nusing with memory leaks, 100–101\namplitude envelope, example of, 312\nanimating\nrelief imposters, 407–409\nrelief maps, 407\nanimation data, storing in textures, 406\nanimation systems, overview of, 365–366\nSee also relief imposters\nAPC (Age Percentage Cost)\ncalculating, 9\nrelationship to RC (relative cost), 12–13\nAPC variables, deriving, 10\nAPIs, using in audio processing, 332\napplication crashes, exception handling, 97–98\nApplication Recovery and Restart API, availability in\nWindows Vista, 103\nApplyToModified functor, code for, 243\nARC (Adaptive Replacement Cache) algorithm, \nuse of, 6\narray of pointers, using with heap allocators, 21\narray of vectors, optical flow as, 26\narrays, use with subdivision data structures, 390–391\nArtificial Contender\ndecision-making algorithms for, 232\ndevelopment of, 229\nexecution flow in, 235–236\npartial results in, 232\nusing “Pipes and Filters” design pattern with,\n230–232\nworkflow algorithms for, 230–232\nworkflow diagram for, 234–235\nArtificial Contender implementation\nActionInfo flow, 241–242\nActionInfo type, 245\nalternative block implementations, 245–246\nconstraints, 246–247\nconstructing workflows, 246\nfunction pointers versus functors in, 244–245\ngeneric programming and C++, 238\npartial results in, 244\npolymorphic workflow blocks, 238–241\nartificial intelligence (AI). See agents\nARToolkit\nobtaining marker position with, 74\nretrieving transformation matrix used by, 74\nsample programs in, 72–73\nusing with foot-based navigation, 71–72\nAST (Abstract Syntax Tree), using to replace strings\nwith numbers, 555–559\nasynchronous events, notifications as, 81–83\nasynchronous versus synchronous exceptions, 97\nAtlas terrain system, availability of, 421–422\nattitude components\nduration, 255\npotency, 254–255\nvalence, 253–254\nattitude objects, example of, 252, 256–258\nattitude systems\nexample of, 261–262\nfeatures of, 250–252\n564\nIndex\n\n\nmodel for, 255–256\npersuasion and influence in, 259–260\nsetting half-lives in, 255\nupdating values in, 252\nuse of, 249\nattitudes\naccumulation of, 251\nemotional charges of, 252\nand social exchanges, 260–261\ntoward behavior, 258–259\nuse in Fable, 253\nattributes, serializing into text format, 518–519\naudio\ncalculating room acoustics, 302–303\nkeeping in sync with graphic updates, 314\non PS3, 306\nstreaming with loop markers, 310\nsurround sound, 315–318\nSee also mixing system; next-gen audio engine; \nsounds\naudio channels\nmixing to busses, 318\nprocessing relative to playback frequency, 311\nrequirements for, 307\nsetting volume levels of, 311\nsplitting, 315\naudio compression formats\nADPCM, 337\nMP3, 337\nOGG Vorbis, 337–338\naudio data streaming, prioritizing, 310\naudio effects, using, 301\naudio engines\nconsidering, 306\nfile formats for, 308\nloop markers used with, 309\naudio files, playback of, 307–309\naudio optimization, implementing with GPUs,\n300–301\naudio processing\nAPIs available for, 332\ncompression and streaming, 337–338\neffects and filters in, 336–337\nrank buffers in, 334–336\nsound buffers in, 333–334\naudio samples, clearing out, 334\naudio tools, using, 326–328\nauthentication\nChallenge Hash Authentication, 483\nimplementation of, 487–488\nand password recovery, 482–483\nprocess of, 481\npublic key infrastructure, 484–485\nSecret Exchange Authentication, 484\nAVG usage, finding for pages, 10\naxis-aligned bounding box (AABB), use in scenes, 184\nB\nB-A Minkowski difference, considering as convex shape,\n170\nbackscattering effect, applying for diffuse-light shading,\n377–379\nbalance theory, relationship to attitude systems, 260\nbattlefield, navigating in RTS (real-time strategy) games,\n63, 65\nbehavior, attitudes toward, 258–259\nbehavior cloning\nbuilding AI script from trees for, 215–216\ndemo game for, 210–216\nexplanation of, 209\nbehavior-capture AI technology. See Artificial Contender\nbehaviors, finding in player traces, 272\nBelady’s Min (OPT) algorithm, use of, 6–13\nbest-fitting spheres, relationship to farthest feature map,\n143, 148\nBI (behavioral intention), relationship to attitudes, 259\nbin, selecting based on heap-allocation size, 16–17\nbinding\nC functions, 504\nclasses, 503, 513\nbinding function, creating for Lua, 507–509\nBiquantic subdivision scheme, features of, 382\nbit array, using with heap allocators, 21\nblimp, invoking in shader architecture, 549–551\nblimp fragment shader code sample, 549–550\nblimp vertex shader, creating hover for, 549\nblocks\nfor AC decision-making algorithms, 236–238\nActionInfo objects processed by, 235\nalternative implementations of, 245–246\nconstraints on, 238\nfunction in execution flow, 235\nimplementation examples, 242–243\npolymorphic workflow blocks, 238–241\nIndex\n565\n\n\n566\nIndex\nblocks (continued)\nproperties of, 239\nSee also workflow blocks\nBloom, Charles, 184–185\nBlum Blum Shub RNG method, description of, 121\nbones, cumulative error associated with, 366\nbook, use in debugging heap allocation, 22\nBoolean operations, performing on convex polygons,\n161, 163\nbounding boxes, use in scenes, 184\nbounding volume hierarchies, use in narrow phase,\n185–188\nbox, support mapping for, 168\nbox-box overlap test, use in narrow phase, 186–187\nbreakpoints, using with network code, 492\nbroad phase of collision detection, speeding up,\n184–185\nBrownian trees, use in particle deposition, 359–360\nBSP tree, kD-tree as, 129\nbucket organization, parameters for WER, 102\nbump maps, storing for advanced decals method,\n427–428\nbump vectors, encoding, 428\nbusses, mixing audio channels to, 318\nButterfly subdivision scheme, features of, 382\nC\nC functions, binding, 504\nC++, use with Artificial Contender, 238\nC++ classes, components of, 519\nC++ compile-time checking, use with Artificial \nContender, 246\nC++ methods, overloading in Lua, 514\nC++ objects\nand arrays, 530, 532\nbinding in Lua, 505–510\nimproving, 533\nnative database type for, 530–531\nobjects or pointers to objects, 530–531\nretrieving, 531\nstoring, 527–530\nstoring instances of, 525\nupdating contents of, 529–530\nSee also objects\nC++ STL, using with hexagonal grids, 51–52\ncache, direct-mapping main memory to, 43\ncache, function of, 5–6\ncache coherency, use in multithread job and \ndependency system, 90\ncache misses\noccurrence of, 5\ntypes of, 43\ncache replacement\nA&C (age and cost) considerations, 12–13\nAge and Cost metrics, 8–13\ncost of, 11–12\ncache systems, difficulty associated with, 5\ncached memory, reading from, 6\ncache-replacement algorithms\nBelady’s Min (OPT), 6–7\nLRU (Least Recently Used), 7\nMRU (Most Recently Used), 7\nNFU (Not Frequently Used), 7–8\nuse of, 5\ncapsule, support mapping for, 169\ncar race games, using surround sound in, 315\nCatmull-Clark subdivision scheme\nfeatures of, 382, 388–389\nfor GPU rendering, 397\nCD contents\nAddressingScheme for hexagonal grid, 52\nAIShooter demo, 212\nastshow.py file, 557\nC++ object serialization, 532\nclipmap demo, 422\nclipmap effect, 417\ndebugging framework, 103–104\ndecal system, 430, 433\ndeferred function caller, 84\nfoot-based navigation, 70\nheap allocator, 23\nhexagonal tile (grid) example, 47\nhiroPatt.pdf file for foot-based navigation, 72\nhorse animations, 410\nlipsyncing example, 457, 460\nLua binding, 516\nLua binding data structure, 505\nmixing system, 347\nmultithread job and dependency system, 87\noptical flow example, 33\nPlayerViz tool, 268\nprojective space example, 164\nraytracing demo, 139–140\nrelief imposters, 410\nshaders integrated into engine, 551\nsmart packet sniffer, 493, 496\nsound effects, 324\ntables for database backend, 526\n\n\nthreading system, 36\ncellular automata\nRNG method, 119\nusing hexagonal grids with, 56–57\nusing in RTS (real-time strategy) games, 64\ncentral limit theorem, use with GRNGs, 201–202\ncentroids, connecting for farthest feature map, 149\nChallenge Hash Authentication, properties of, 483\nchorus effects, using, 301\ncircular buffer, role in audio processing, 334\ncities, depicting with square tiles, 50\nclasses, binding, 503, 513\nclient/server topology, considering in game world \nsynchronization, 468\nclipmaps\nadvantages of, 415–416\nbackground paging, 420\nbudgeting updates, 420–421\nand clipstack size, 416\nCPU synthesis and upload, 419\ndrawbacks of, 416\nimplementing, 417–419\nmanaging large textures with, 436\nmethods for updating, 417\noptimizing fillrate/low-end support, 421\npurpose of, 414\nselecting focus points, 416\ntoroidal updates and rectangle clipper, 418–419\nuse of, 413\nclosed lists, eliminating with IDA*, 292–294\nCMU phonemes, 458–459\ncode samples\nApplyToModified function, 243\nattitude system, 255–256\naudio effects, 301\nbackscattering, 378–379\nbinding function for Lua, 507–509\nblimp fragment shader, 549–550\nblimp vertex shader, 549\nBrownian tree, 360\nC++ methods overloaded in Lua, 514\nC++ object update, 529\nC++ objects as Lua objects, 506–507\nC++ objects stored, 528\nclipmap effect, 417\ndataports, 536\ndependency group and links, 93\ndunes created with particle deposition, 362\nfarthest feature map, 148, 150\nfoot-based navigation, 73–75\nforEach function implementation for modifier, 243\nGaussian distribution, 201\nGLRCachePad macro, 43\ngraftal imposters, 451–453\nhalf-edge pair indices in Loop subdivision, 395\njob class in multithread job and dependency system,\n88–89\njob selection in multithread job and dependency\nsystem, 91\nkD-tree EventBoxSide, 135\nkD-tree traversal, 136–137\nKdTreeNode structure, 130\nLFSR113, 119–120\nLua binding, 503\nLua binding and automatic type registering, 509–510\nLua binding data structure, 505–506\nLua binding function, 509\nLua binding metatable for object-oriented method,\n505\nLua binding of C function, 504\nLua binding optimization of generated code size, 515\nLua binding sand-boxing and type filtering, 515\nLucas and Kanade algorithm in optical flow, 29\nmanager class in multithread job and dependency\nsystem, 89\nModifier block for Artificial Contender, 241\nmotion history in OpenCV, 28–29\nmWebCam.AddRegion(), 32\nnon-collinear surface points in XenoCollide, 172\nOpenCV, 26\noverhanging terrain, 363\npacket sniffer, 495\nparticle dynamics, 357\nparticle placement for volcanoes, 358\npathfinding with hexagonal grids, 56\npointers for database backend, 521–523\npolymorphic workflow blocks, 239\nPython’s AST, 556–557\nraytracing, 127\nroom acoustics real-time rendering, 302–303\nscheduler class in multithread job and dependency\nsystem, 90\nserializing attributes into text format, 518–519\nshader groups, 552\nshader in GLSL, 376\nshaders combined with actors and properties,\n552–553\nshaders integrated into engine, 550–551\nIndex\n567\n",
      "page_number": 593,
      "chapter_number": 61,
      "summary": "This chapter covers segment 61 (pages 593-600). Key topics include audio, algorithm, and functions. Every effort has been made to ensure the enclosed source code is bug-free and able to\nbe compiled, the executables run trouble-free, and the images and text are freely view-\nable.",
      "keywords": [
        "Artificial Contender",
        "Lua binding",
        "Lua",
        "audio",
        "Adaptive Replacement Cache",
        "cache replacement",
        "binding",
        "cache",
        "Artificial",
        "System",
        "Contender",
        "algorithm",
        "function",
        "Lua binding data",
        "dependency system"
      ],
      "concepts": [
        "audio",
        "algorithm",
        "functions",
        "examples",
        "mappings",
        "map",
        "maps",
        "block",
        "effects",
        "attitudes"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 55,
          "title": "Segment 55 (pages 536-543)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 53,
          "title": "Segment 53 (pages 511-519)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 56,
          "title": "Segment 56 (pages 544-551)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 52,
          "title": "Segment 52 (pages 506-515)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 62,
      "title": "Segment 62 (pages 601-608)",
      "start_page": 601,
      "end_page": 608,
      "detection_method": "topic_boundary",
      "content": "code samples (continued)\nsource blocks, 242\nspatial search with hexagonal grids, 55\nstatic polymorphism, 241\ntables for database backend, 526\ntexture-coordinate calculation, 443\ntrigonometric functions, 194–195\nvariables allocated memory for OpenCV, 26–27\nwebcamInput class cvAbsDiff function, 27\nwebcamInput class public interface, 31\nWELL algorithm, 120–121\nworker threads in multithread job and dependency\nsystem, 90\nXenoCollide pseudocode, 171\nSee also Listings\ncodecs, requirements of, 309\nCodeGenerator, using with Python’s AST, 558–559\ncognitions, relationship to attitudes, 251\ncollinear cases, detecting, 153\ncollision algorithm. See XenoCollide\ncollision culling, explanation of, 184\ncollision detection\nbroad phase task of, 184–185\nand Loop subdivision, 389–390\nmodeling, 144\nbetween models in scenes, 180\nnarrow phase of, 185–188\nsimplifying using Minkowski differences, 170–171\nusing Minkowsi Portal Refinement (MPR), 171–176\ncollision detection tasks, using semantics for, 184–188\ncollision systems, creating, 165\ncollision tests, considering AABB-trees in, 187\ncollision-detection steps\nchoose_new_candidate(), 173–174\nchoose_new_portal() step of, 175\nfind_candidate_portal(), 172\nfind_origin_ray() step of, 171\nfind_support_in_direction_of_portal(), 174\nif () return hit; step of, 174\nif (origin outside support plane) return miss, 174\nif (support plane close to portal) return miss, 175\nwhile (origin ray does not intersect candidate), 172\ncommand lifetime, use with RTS games, 65–66\ncompression, considering in audio processing, 337–338\ncompression algorithms, using with skeletal animation,\n367\ncomputer vision games, optical flow in, 26\ncomputer vision, using with foot-based navigation,\n71–72\ncones\ncreating, 169\nsupport mapping for, 170\ntesting of model human vision, 219\nconstructive solid geometry (CSG) algorithms, use of,\n159\ncontact information, acquiring with MPR, 176–178\ncontinuous collision detection, approach toward, 144\ncontrol points\ninterpolating for relief imposters, 404–405\nfor walking dog animation, 409–410\nconvex polygon, describing, 161\nconvex shapes, manipulating, 170\ncoplanar cases, detecting, 153\ncosine law, use of, 373\ncrashes, reasons for, 97\nCSG (constructive solid geometry) algorithms, use of,\n159\nCSPRNGs (cryptographically Secure PRNGs)\nBlum Blum Shub, 121\n/dev/random, 121–122\nFortuna, 122\nISAAC, ISAAC+, 121\nMicrosoft’s CryptGenRandom, 122\nYarrow, 122\ncube-map, relationship to farthest feature map,\n144–145\ncustom texture cache, design of, 8–13\ncylinder, support mapping for, 169\nD\nDA (direct-argument) functions, adding deferred calls\nto, 84\ndata loading, considering in streaming audio, 309\ndata structures, designing for subdivision surfaces,\n390–392\ndatabase backend for C++ objects\narray class used in, 518\nmetadata for, 517–518\ndatabase backend tables for C++ objects\narrays, 525\ncreating, 526\ninstances of classes, 523–525\nparent class, 520\npointers, 520–523\n568\nIndex\n\n\nscalar members, 520\nstrings, 520\ndataport examples\nbroadcasting positional information, 539\ncamera systems, 538\nproblems with, 539\nship handling debug values, 538\nDataport Manager\nusing, 536–537\nusing hashing in, 539\ndataport pointers, use of, 536\ndataports\nand reference counting, 537–538\nand type safety, 537\nuse of, 535–536\ndebug output, using with network code, 492–493\ndebugging framework\nexception handling, 104\nmemory leak detector, 104\ndebugging support, adding for heap allocation, 22\ndebugging techniques, maintaining for network code,\n492–493\ndecal system, requirements for, 423\ndecals method (advanced)\nadvantages of, 428–430\nDecodeBump function used in, 427–428\nperformance and experimental results, 430–433\nusing, 424–428\ndecision tree implementation, using with agents,\n211–215\ndecision-making algorithms, for goal-oriented planning\nsystems, 281–286\ndecomposition\neffects of, 230\nof worlds into regions, 271\ndeferred functions, use of, 82–85\ndeferred_proch system\nheader file for, 84\nparameters used with, 84\ndemos. See CD contents\nDependency Inversion Principle, applying to workflow\nblocks, 239\ndependency manager system\ndependency graph in, 92\ndependency storage in, 93–94\nentries in, 91\ngroup entry in, 94\njob entry in, 94\ndesign patterns\niterator used with hexagonal grids, 53\nPrototype Design Pattern used with shaders, 544\n/dev/random RNG method, description of, 121–122\ndfpProcessAndClear deferred function caller, use of, \n84\nDIEHARD randomness-testing suite, features of, 116\ndiffuse light\ncomputation of, 373\nmodel for, 374\nproducing, 375\ndiffuse-light shading\nbackscattering, 377–379\nflattening effect of, 375–377\ndiffusion limited aggregation (DLA), creating ridge\nstructures with, 359–360\nDijkstra, modification of A* search algorithm by,\n290–292\ndirect-argument (DA) functions, adding deferred calls\nto, 84\ndirected lines\ncomputing in R2 projective space, 159\ncutting polygons with, 161\noperations on, 157–158\nDirectSound API, features of, 332\ndisc, support mapping for, 168\ndiscrete collision detection, approach toward, 144\ndispositional liking, demonstration by attitudes, 251\nDLA (diffusion limited aggregation), creating ridge\nstructures with, 359–360\nDoo-Sabin subdivision scheme, features of, 382\ndouble quotes (\"), using with Python strings, 556\ndramatic beat, function in attitude systems, 252\nDSP effects, considering in surround sound, 317–318\ndump file, controlling information in, 98\ndunes, improving particle placement of, 361–362\nduration, purpose in attitude systems, 255\nDXT5 compressed surfaces, storing bump values in,\n427\ndynamic geometry, computing texture coordinates in,\n443\nE\nechoes, calculating, 302–303\nedge vertices, computing with Loop subdivision, 393\nedges, manipulating in Loop subdivision, 384–386\neffects versus filters, considering in audio processing,\n336–337\nIndex\n569\n\n\nellipses\naugmenting vision model toolbox with, 219–222\nEquation for, 195\nimplementing for vision model, 221–222\nsupport mapping for, 168\nellipsoid, support mapping for, 168\nentities, roles in game worlds, 55\nEnumerator class, using with hexagonal grids, 53\nepsilon values\nusing in foot-based navigation, 75\nusing with collinear and coplanar cases, 153\nEquations\naffine transform, 180\nattitude objects, 256\nbackscattering, 377–378\nbump-vector encoding, 428\nclustering IPGs, 277\ncost of split, 133\ndecals method (advanced), 426\ndecision tree implementation for agents, 213\nellipse, 195\nellipse implementation for vision model, 221\nflattening effect of diffuse-light shading, 375\ngraftal-imposter texture coordinates, 452–453\nHermite spline, 192–194\ninverse of affine matrix, 181–183\nMA and MB matrices, 186\nmatrix multiplication over vertex, 181\nMinkowski differences, 171\norigin shift, 181\nPhong-Blinn model applied to backscattering,\n378–379\npoints and directed lines in RP2, 155\npolygon cutting, 163\nray intersection with axis-aligned plane, 130\nRBFs (radial basis functions) and relief imposters, 404\nskeletal animation and cumulative error, 366\nskeletal animation rotation computation, 369\nSLERP (spherical linear interpolation), 194\nsphere with support mapping, 167\nsupport mapping for rotated and translated object,\n168\ntexture memory usage for large terrain areas, 439\ntexture stack update for large terrains, 440\nvertex and crease normals in Loop subdivision, 388\nvertices in Loop subdivision, 386\nvertices in skeletal animation, 365\nerosion, simulating effects of, 355–356, 429\nerrors\napplication crashes, 97–100\nmemory leaks, 100–102\nWER (Windows Error Reporting), 102–103\nEventBox, use with kD-tree, 134–135\nexception handling, 97–98, 104\nexceptions, types of, 97\nexplosions\nconsidering as “pops,” 322\nqualities of, 323\nEye Toy: Play, optical flow experiment with, 30–33\neyes. See vision model\nF\nFA (faces array), use with subdivision data structures,\n390–391\nFable\nopinion system in, 260\nuse of attitude in, 252–253\nFaçade, opinion events in, 252\nfade sample amount, specifying for rank buffers, \n336\nfading, use with FFT, 312–313\nfarthest feature map\n2D case of, 145–146\n3D case of, 147\nalgorithm for, 150\nand best-fitting spheres, 143\nand mean curvatures, 143\noversampling, 148\nand preprocessing, 144–148\nand principle curvatures, 143\nand runtime queries, 148–150\nstorage of vertices for, 148\nuse of, 143\nvisualizing, 145\nFast Fourier Transforms (FFT)\nrelationship to audio engines, 312–313\nrelationship to effects and filters, 336\nfeature space output, setting up for behavior cloning,\n210–211\nFFT (Fast Fourier Transforms)\nrelationship to audio engines, 312–313\nrelationship to effects and filters, 336\nwindowing required for, 314\nfield of view\ndetermining entities in, 222\nusing ellipse for, 220\n570\nIndex\n\n\nFigures\n2D case of farthest feature map, 145–146\n3D case of farthest feature map, 147\nAABB fitting model, 185\namplitude envelope, 312\nArtificial Contender decision-making workflow, 234\naudio-channel location relative to player, 316\nbackscattering, 378–379\nbest fitting circle in 2D, 149\nbin selection based on heap-allocation size, 16\nbin with single page for small allocator, 17\nBoolean operations on polygons, 163\nbounding box start and end events for kD-tree, 134\nBrownian tree created with DLA, 360\ncellular automata and hexagonal grids, 57\ncellular automaton grid in RTS game, 64\ncircular update for large terrain, 442\nclipmap update, 415\nclipped mipmap stack, 414\nclipstack-texture size, 418\ndecal techniques, 426–427\ndecal-methods tests, 431\ndecals method (advanced), 425\ndecision tree for agents, 214\ndependency graph, 91, 93\ndependency graph after propagation, 92\ndog imposter, 408\nDSP effects in buss, 318\ndunes formed as particles, 362\nDXT 1/5 compressed textures, 433\nedge mask in Loop subdivision, 385\nellipse components for vision model, 220\nEnumerator class used with hexagonal grid, 53\nerosion using decals, 429\nface splitting in Loop subdivision, 394\nfeature space, 210–211\nFFT (Fast Fourier Transforms), 313\nflattening effect of diffuse-light shading, 376–377\nFlock of Birds motion capture device, 70\nFMOD Designer interface, 327\nfoot-based navigation, 71\nFSM (Finite State Machine) for warrior, 257–258\ngame circuit for foot-based navigation, 76\ngame start indicator for foot-based navigation, 76\nGaussian distribution, 200, 202\ngeometry for Loop subdivision, 389\nGLR thread library, 37\ngoal-oriented planning systems, 282\nGPGPU graphics pipeline, 300\nGPU versus CPU computational power, 299\ngraftal imposters, 449–450\ngraftal-imposter vertices, 451\ngrid partitioned into rectangular cells, 54\nhead model for lipsyncing, 456–457\nhexagonal tiles, 48\nhexagonal tiles with axes of symmetry, 50–51\nHIVVE (Highly Interactive Information Value \nVisualization and Evaluation), 278\nHLA activities, 472\nHLA collaboration diagram, 469\nHLA viewports and objects, 470\nHLA-runtime entity-viewport visibility, 476\nHLA-system UML class diagram, 477\nhorse animation, 410\ninteraction feature points, 269\nIPGs (interactive player graphs), 275–276\nkD-tree split plane position, 132\nkD-tree splitting process, 129\nkD-tree traversal cases, 138\nkD-tree with node reduction, 130\nLambert shaded teapot, 374\nlarge allocator memory use, 18\nlava streams, 359\nLCG bias, 123\nMicrosoft XNA XACT audio tool, 327\nmixing layers in mixing system, 345\nmixing system, 342\nmixing system with central mix, 343\nmixing through MIDI control surface, 346\nmountain created with particle deposition, \n361\nmovements of soldier troops in RTS game, 60\nnil node’s place in tree, 20\noptical-flow game, 30, 32\nOren-Nayar shaded teapot, 375\norigin ray, 172\noverhanging terrain, 363\nparticle deposition, 354\nparticles and slope of terrain, 356\nPlay Audio commands relative to surround sound,\n317\nPlayerViz tool, 270\npolygon cut with line, 162\npolygon intersection, 162\nportal for XenoCollide and MPR, 173–174, \n176\npull workflow for Artificial Contender, 237\npull workflow with callback functions, 244\nray components, 128\nraytracing demo application, 140\nIndex\n571\n\n\nFigures (continued)\nrectangular domain for hexagonal grid, 52\nred-black-tree nodes for large allocator, 19\nrelief imposters with control points, 403\nrelief-imposter warping, 402\nrotational error removed, 369\nRP2 projective space, 154\nRP2 projective space points and lines, 156\nRTS (real-time strategy) games, 60\nRTS (real-time strategy) sketches, 65\nRTS focus-context interface combination, 61\nRTS games with integrated interfaces, 66\nRTS-game implementation, 63\nsearch grid with search tree, 290\nsensing-model unification, 227\nShaderManager class diagram, 543\nShaderParameter class diagram, 548\nskeletal animation cumulative error, 368\nskeletal animation reduction in cumulative transla-\ntional error, 370\nskeletal animation translation error reduction, 369\nsmart packet sniffer, 494\nsound falloff with zone approach, 225\nsound-layout comparison, 325\nsphere with support mapping, 167\nsquare grid relative to neighborhoods and marching,\n48\nstack overflow, 100\nsticky particles used with overhanging terrain, 363\nsubdividing patch, 398\nsupport mapping as moving plane, 166\nsupport mappings combined, 169\nsupport point, 167\nsupport point in direction of portal, 175\nsurround sound, 315\nsurround sound with channels synced, 316\nterrain composed of angles, 355\nterrain created with search radius, 357\nterrain generated with particle deposition algorithm,\n354\nterrain navigation system results, 444\ntexture atlas for graftal imposters, 448\ntexture stack for large terrain, 438\ntexture stack with mipmap levels, 439\ntextures (non-compressed) for decals method, 432\nthreaded and multithreaded models, 38\ntiles, 50\ntoroidal update and mapping for virtual texture, 443\ntotally-ordered plans, 284\ntransformation semantics, 186\ntrigonometric curve for circle, 196\ntrigonometric curve with constraints, 193\nUCT player trace, 267\nvertex mask in Loop subdivision, 387\nvertex neighbors in Loop subdivision, 394\nview distance check in agent-sensing model, 218\nvirtual texture, 437\nvisemes for “hello,” 460\nvision certainty, 223\nvision model with gradient zones of certainty, 224\nvision model with view angles and circle, 220\nvisual data mining of player traces, 272\nvolcano created with particle deposition, 359\nWalker class used with hexagonal grid, 53\nwalking-dog control points, 410\nworkflow for Artificial Contender, 235\nfilter, use in Artificial Contender, 231\nFilters block, use in AC decision-making algorithms,\n236\nfilters versus effects, considering in audio processing,\n336–337\nFinite State Machine (FSM), use with attitude systems,\n256–257\nFIR (finite impulse response) filters, using in audio\nprocessing, 336–337\nfirst-person shooting (FPS) games, interaction control\nin, 69\nflattening effect, applying for diffuse-light shading,\n375–377\nFloat32 PCM audio engine file format, pros and cons\nof, 308–309\nFlow Regions() function, using in optical flow, 31\nfluster, displaying for player trace, 272\nFMOD Designer interface\nfeatures of, 326–328\ngoals of, 328–329\nSee also sounds\nfolklore algorithm mistakes, occurrence with RNGs,\n123–124\nfoot-based navigation\ncapabilities of, 69–71\nimplementation of, 69–70, 72–75\nrequirements for use with computer vision, 71–72\nsample game, 75–77\ntests with users, 77–78\nfootsteps, randomizing, 328–329\n572\nIndex\n\n\nformulas. See Equations\nFortuna RNG method, description of, 122\nFourier series, constructing trigonometric spline from,\n192\nFPS (first-person shooting) games, interaction control\nin, 69\nfragment shaders, use with room acoustics, 302–303\nfragment versus pixel shaders, 542\nframes, storing information per, 10\nfree nodes, managing with large allocator, 18\nfree-list\naccessing with small allocator, 17\nfunction in heap allocation, 16\nfrequency data, using windowing techniques used with,\n312, 314\nFringe Search algorithm, using, 293–294\nfrustum, support mapping for, 170\nFSM (Finite State Machine), use with attitude systems,\n256–257\nfunction calls, categorizing for asynchronous events,\n83–84\nfunction pointers versus functors, use in Artificial \nContender, 244–245\nG\ng() cost, purpose in A* search algorithm, 291, 293\ngain, finding in behavior cloning example, 214\ngame animation systems, overview of, 365–366\ngame architecture, planning for multithreaded pro-\ngrams, 36\ngame logins, securing, 481–485\ngame sessions, securing, 485–487\ngame world state\ncategorizing changes in, 468–469\nsending messages in, 468–469\ngame world synchronization. See synchronizing game\nworlds\ngame worlds, entities in, 55\ngateways, identifying between spatial regions, 271\nGaussian distributions\nexample of, 201\nin nature, 203\nuse of, 199\nusing in RNGs, 115\nGaussian random number generators (GRNGs)\napplication for, 202\nand central limit theorem, 201\npolar-rejection, 200\nuse of, 200–203\nziggurat method, 201\nGaussian randomness, use of, 203\nGeneral Purpose computation on a Graphics Processing\nUnit (GPGPU), overview of, 300\ngeneral purpose registers (GPRs), use with asynchronous\nevents, 84\ngeneric programming\nalternative block implementations in, 245–246\nuse with Artificial Contender, 238\ngeometry creation in Loop subdivision\nedges, 384–386\nlimit positions, 387–388\nvertex and crease normals, 388\nvertices, 386–387\ngeometry models, mapping virtual textures to, \n442–443\nGHTP project, smart packet sniffer used in, 491–492\nGJK (Gilbert, Johnson, Keerthy) algorithm\nversus MPR (Minkowski Portal Refinement),\n177–178\nuse of, 171\nGLRCachePad macro, using in threaded systems, 43–44\nGLRThread interface\ncapabilities of, 39\nsize of, 40\nGLRThreadFoundation singleton, usage of, 38\nGLRThreading library\ncomponents of, 36–38\ncreating cache-aligned data structures with, 43–44\nfeatures of, 36\nthreading capabilities in, 42\nusing, 44–45\nGLRThreading system, submitting objects to, 42\nGLRThreadProperties mechanism, use in threading\nsystems, 39–40\nGNU C library\nhooking functions related to, 102\nreplacing memory functions with, 101\ngoal-oriented planning systems\noverview of, 281–283\npartially-ordered plans in, 282\nplan merging for, 283–286\ntotally-ordered plans in, 282\nGPGPU (General Purpose computation on a Graphics\nProcessing Unit), overview of, 300\nGPRs (general purpose registers), use with asynchronous\nevents, 84\nIndex\n573\n\n\nGPU subdivision, considering in Loop subdivision,\n397–398\nGPUs, relationship to audio optimization, 300–301\ngraftal imposters\nassigning texture coordinates to, 452–453\nsampling texture atlas for, 453\nusing assets during runtime, 450–453\ngraftal-imposter assets\ncolor texture and mesh, 450\ncontrol textures, 448, 451\ntexture atlas, 447–448\nvector fields, 448–450\ngraftals, use of, 447\ngranularity, role in next-gen audio engine, 313–314\ngraph edit distance, using, 275–277\ngraph searches, techniques for, 289\ngraph-based data, discovering knowledge in, 278\ngrids\nhexagonal versus square types of, 47\nimpact on visual appearance of games, 49\nrepresenting playing fields with, 49\nuse in games, 47\nGRNGs (Gaussian random number generators)\napplication for, 202\nand central limit theorem, 201\npolar-rejection, 200\nuse of, 200–203\nziggurat method, 201\nH\nh() cost, purpose in A* search algorithm, 291, 293\nHA (half-edge array), use with subdivision data struc-\ntures, 391\nhalf-edge array (HA), use with subdivision data struc-\ntures, 391, 395\nhalf-life, setting in attitude systems, 255\nhandheld gaming systems, relationship to vertical blank-\ning period, 82\nhanging pointers, tracking down, 537\nhanning and hamming window types, use of, 314\nhead model, using for lipsyncing, 455\nheader file, use with asynchronous events, 84\nheap allocation\nadding debugging support for, 22\ncombining allocators, 21\nexample on CD, 23\nextensions of, 23\nhybrid approach toward, 15–23\nwith large allocator, 18–21\nand multithreading, 22\nperception of, 15\nwith per-size template pool-style allocator, 16\nwith small allocator, 16–18\nhearing model\nwith certainty, 224–226\nconsidering in agent-sensing model, 219\nincluding other senses in, 226\nheight fields, traversing with random walkers, 353–354\nHermite spline, Equation for, 192–193\nhexagonal grids\naccess layer of, 53\naddress layer in, 51–53\nimplementing, 54–55\nhexagonal tiles\nadvantage of, 48\naxes of symmetry, 50–51\nequidistant neighbors on, 48\nforming organic shapes with, 50\nisotropy and packing density considerations, 49\nhexagonal-grid applications\ncellular automata, 56–57\npathfinding, 55–56\nspatial search, 55\nhit-test computations, using with hexagonal grids, 54\nHIVVE (Highly Interactive Information Value \nVisualization and Evaluation) tool, features of, \n278\nHLA (High Level Abstraction)\ncollaboration diagram for, 469\ncomponents of, 470–476\nusage of, 461\nHLA event handlers, use of, 477\nHLA runtimes\ncommunication between, 475–476\nconstruction of, 477–478\nviewports in, 476–478\nHLA system, extending, 478\nhook function, use with memory leaks, 101–102\nhorse-animation example, 410\nhuman cognitions, attitudes as basis for, 251\nhuman hearing. See hearing model\nhuman vision. See vision model\nI\nIA (indirect-argument) functions, adding deferred calls\nto, 84\nIDA* (Iterative Deepening A*), eliminating open and\nclosed lists with, 292–293\n574\nIndex\n\n\nidentity\nby authentication method, 485–486\nby cryptography method, 486\nby IP address method, 485\nIDs, replacing strings with, 555–559\nIIR (infinite impulse response) filters, using in audio\nprocessing, 336–337\nimage warping, using with relief imposters, 403\nindex dispenser, use in dependency storage, 93\nindirect argument data blocks, storage of, 83–84\nindirect-argument (IA) functions, adding deferred calls\nto, 84\nInfiniteReality2 hardware platform, access of miplevels\nin, 414\ninfluence and persuasion, considering in attitude \nsystems, 259–260\ninformation theory, applying to behavior cloning, 214\ninstance-based machine learning\nArtificial Contender example of, 229–230\nfeature space in, 210–211\ninteger representation, finding length in prospective\nspace, 160\ninteractions, capturing, 268\nInversive Congruential Generator RNG method, \ndescription of, 118\nIP address, identity by, 485\nIPGs (interactive player graphs)\nbuilding, 274–278\nclustering, 275–277\nclustering players by, 275\nISAAC, ISAAC+ RNG method, description of, 121\niterator design pattern, using with hexagonal grids, 53\nJ\njittering values, hiding, 474\njob selection, use in multithread job and dependency\nsystem, 91\njobs versus threads, 87\njob-system objects\ncache coherency, 90\njob, 88–89\njob selection, 91\nmanager class, 89\nscheduler, 89–90\nworker threads, 90\njumper, displaying for player trace, 272\nK\nkD-tree\naxis-aligning split planes in, 130\nconstruction of, 132–135\ncost of splits in, 133\ndetermining split plane position, 132\nEventBox used with, 134–135\ntraversal of, 135–138\nuse in raytracing, 128\nuse of, 129\nSee also raytracing\nkD-tree nodes, contents of, 130\nKdTreeNode structure, 130\nkey vertex cell decomposition, applying to worlds, 271\nkeyboard sniffers, concerns about, 482, 487\nK-medoids clustering, using on IPGs, 276–277\nKnuth mistake, occurrence with RNGs, 122–123\nKobbelt subdivision scheme, features of, 382\nL\nLagged Fibonacci Generator (LFG) RNG method,\ndescription of, 118\nLambert’s model\neffects of, 374\nversus Oren-Nayar model, 375\nuse of, 373\nlarge allocator\ncombining with small allocator, 21\nfunction in heap allocation, 18–21\nlatency, relationship to audio engines, 313–314\nlater and now lists, use in Fringe Search algorithm,\n293–294\nLCG (Linear Congruential Generator) RNG method,\ndescription of, 116–117\nleak detector, allocation registry managed by, 101–102\nLeast Recently Used (LRU) algorithm\nefficiency of, 6\nuse of, 7\nleast used pages, identifying, 10\nLFG (Lagged Fibonacci Generator) RNG method,\ndescription of, 118\nLFSR (Linear Feedback Shift Register) RNG method,\ndescription of, 118\nLFSR113, LFSR258, use of, 119–120\nliking/disliking, evaluating in attitude systems, 253–254\nLinear Congruential Generator (LCG) RNG method,\ndescription of, 116–117\nLinear Recurrence Generators (LRGs)\nLFSR113, LFSR258, 119–120\nMersenne Twister, 119\nWELL algorithm, 120–121\nline-of-sight test, doing in agent-sensing model,\n218–219\nIndex\n575\n",
      "page_number": 601,
      "chapter_number": 62,
      "summary": "This chapter covers segment 62 (pages 601-608). Key topics include games, gaming, and texture. Covers method.",
      "keywords": [
        "Loop subdivision",
        "RNG method",
        "support mapping",
        "model",
        "vision model",
        "subdivision",
        "Artificial Contender",
        "support",
        "Loop",
        "method",
        "systems",
        "attitude systems",
        "game",
        "hexagonal grids",
        "texture"
      ],
      "concepts": [
        "games",
        "gaming",
        "texture",
        "modeling",
        "functions",
        "function",
        "algorithm",
        "systems",
        "allocated",
        "allocation"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 56,
          "title": "Segment 56 (pages 544-551)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 26,
          "title": "Segment 26 (pages 243-252)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 53,
          "title": "Segment 53 (pages 511-519)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 42,
          "title": "Segment 42 (pages 401-412)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 63,
      "title": "Segment 63 (pages 609-616)",
      "start_page": 609,
      "end_page": 616,
      "detection_method": "topic_boundary",
      "content": "lines\ncutting polygons with, 162\ndefining relative to projective space, 156\nfinding intersection points of, 157–158\nleading through pair of points, 157\nlinks, creating for dependency manager system, 94\nlipsyncing\nhead model used for, 455\nin real-time, 460–461\nrequirements for, 455–457\nuse of, 455\nword to phoneme mapping for, 457–459\nlistener, role in sound systems, 332\nListings\nGLRThreading library executing test objects, 45\nGLRThreading library test game object, 44\nGLRThreading library threadable function for game\nfunction, 44\nRBF-based warping function, 406–407\nrecursive node learning algorithm, 213\nsFront and sBack for relief imposters, 409\nSWD file, 471\nSWD file for synchronized object, 474\nSWD file pseudo-grammar, 470–471\nsynchronized object _property keyword, 472–473\nsynchronized object casting and assign operators, 473\nwalking motion, 408–409\nSee also code samples\nLOD levels, smooth transitions between, 415\nlogging\nexcluding and oversimplifying, 267\nimplementation of, 268\nusefulness of, 266\nlogins, securing, 481–485\nlogs, capturing at 10Hz, 267\nloop markers\nstreaming audio with, 310\nuse with audio engines, 309\nLoop subdivision algorithm\ncollision detection, 389–390\ncomputing new edge vertices with, 393\ncreating new half-edge information, 395\ndata structure for, 390–392\nedge weights in, 385\nextensions to, 381\nfeature implementation, 388–389\nfeatures of, 382\ngeometry creation, 384–388\nGPU subdivision and rendering, 397–398\nlevels of subdivision in, 392\nperformance enhancements, 396–397\nrelief warping requirements, 407\nsplitting faces with, 393–394\ntoolset for, 383–384\nupdating features, 395–396\nupdating original vertices, 393\nSee also subdivision surfaces\nlossy compression algorithms, using with skeletal \nanimation, 367\nlozenge, support mapping for, 169\nLRGs (Linear Recurrence Generators)\nLFSR113, LFSR258, 119–120\nMersenne Twister, 119\nWELL algorithm, 120–121\nLRU (Least Recently Used) algorithm\nefficiency of, 6\nuse of, 7\nL-systems, use with particle deposition, 361\nLua binding\nattributes, 511–512\nand automatic type registering, 509–510\nof C functions, 504\nof C++ objects, 505–510\ncreating binding function for, 507–509\ndebug helper, 513\nenum support, 512\ninheritance, 511\nmaking object-oriented, 504–505\nnative types for, 505\noptimization of generated code size, 515–516\noverloaded functions, 513\noverloading C++ methods in, 514\nreference counting and raw objects, 511\nsand-boxing and type filtering, 514–515\nsingletons, 511–512\nstatic functions, 511–512\ntemplate classes, 512\nuse of, 503–504\nLua script, turning tree as, 215\nLucas and Kanade algorithm\nturning tree as Lua script, 215\nuse in optical flow, 29–30\nuse with optical flow, 31–33\nM\nMA and MB matrices, use in narrow phase, 185–186\nmain memory, direct-mapping to cache, 43\nSee also memory\n576\nIndex\n\n\nmalloc/free replacement, heap allocator for, 22\nmanager class, use in multithread job and dependency\nsystem, 89\nmarker position, obtaining with ARToolkit, 74\nmatrices, extracting semantics from, 181–184\nmatrix m, values for, 75\nMAX analysis, using with pages, 10\nmean curvatures, relationship to farthest feature map,\n143\nmemory\nadding to unified sensing model, 227\nassociation with threads, 40\nmanagement by small allocator, 17\nuse in large allocator, 18\nSee also main memory\nmemory leak detector, using, 100–102, 104\nMergers block, use in AC decision-making algorithms,\n237\nmerging plans for agents, 284–286\nMersenne Twister Linear Recurrence Generator, use of,\n119\nmesh, storing for subdivision data structures, 390–391\nMetaAttribute class, use with database backend, 518\nmetals, models for, 374\nMetaType class\nclass metadata saved in, 517–518\nusing with database backend, 526\nMicrosoft CRT, using allocation hooks in, 101\nMicrosoft’s CryptGenRandom RNG method, \ndescription of, 122\nMicrosoft’s XACT audio tool, features of, 326\nMiddle Square RNG method, description of, 116\nMidedge subdivision scheme, features of, 382\nMIDI interface, implementation for Scarface:\nThe World Is Yours, 346\nmini-dump, creating for running process, 98\nMiniDumpCallback function, use in debugging, 104\nMinkowski differences\nfinding points on interiors of, 171\nsimplifying collision detection with, 170–171\nmipmapping, generalizing with clipmaps, 413–414\nmixing system\nfeatures of, 341–342\nimplementation of, 342–345\nperformance of, 346–347\ntuning application for, 345–346\nSee also audio\nModifier block\nimplementing, 241\nuse in AC decision-making algorithms, 236\nmomentary versus dispositional liking, 251\nMost Recently Used (MRU) algorithm, use of, 7\nmountains, improving particle placement of, \n358–361\nMP3 audio compression format, explanation of, 337\nMP3 audio engine file format\nplaying back audio in, 307–308\npros and cons of, 308\nrequirements of, 309\nMPR (Minkowski Portal Refinement)\nversus GJK (Gilbert, Johnson, Keerthy) algorithm,\n177–178\nrelationship to XenoCollide algorithm, 166, \n171–176\nusing for contact information, 176–178\nMRU (Most Recently Used) algorithm, use of, 7\nMultiStream\nbusses for audio channels in, 318\nlatency considerations, 313\nprocessing capabilities of, 306, 309\nrole in SCEE audio engine, 305\nsurround-sound management by, 315\nand volume parameters, 311\nmultithread job and dependency system. See job-system\nobjects\nmultithreaded programs, designing, 36\nmultithreading\nhiding complexity of, 87\nrelationship to heap allocation, 22\nsynchronization problems associated with, 87\nmultivariate normal distribution, example of, 202\nmutex per bin, using in heap allocation, 22\nmWebCam.AddRegion() function, using in optical\nflow, 32\nN\nNA (normals array), use with subdivision data \nstructures, 391\nnarrow phase of collision detection, implementing,\n185–188\nnavigation. See foot-based navigation\nN-by-N attitudes, use in attitude systems, 260\nNeighborhood instance, using with hexagonal grids, 54\nneighborhoods, role in square tiling, 48\nneighbors, coalescing with large allocator, 20\nNetscape mistake, occurrence with RNGs, 123\nnetwork code, maintaining debugging techniques for,\n492–493\nNewton-Raphson method, use in optical flow, 29\nnext-gen, separating from last-gen titles, 317\nIndex\n577\n\n\nnext-gen audio engine\nchannels for, 307\nand FFT (Fast Fourier Transforms), 312–313\nand frequency domain processing, 312\nand latency, 313–314\npacket smoothing, 314–315\nplayback frequency of, 311\nrouting, 318\nsample formats for, 307–309\nstreaming, 309–310\nvolume parameters for, 311\nSee also audio\nNFU (Not Frequently Used) algorithm, use of, 7–8\nnil node, use with red-black trees, 20\nnodes, managing with large allocators, 19–20\nnoise functions, using with particles, 356–357\nnormal distribution, use of, 199\nnormals array (NA), use with subdivision data\nstructures, 391\nNot Frequently Used (NFU) algorithm, use of, 7–8\nnotifications, considering as asynchronous events,\n81–83\nnow and later lists, use in Fringe Search algorithm,\n293–294\nNPC behavior. See attitude systems\nNPCs, use of totally-ordered plans with, 283\nnumbers, replacing strings with, 555–559\nO\nOBB (oriented bounding box), use in scenes, 184\nobject oriented programming, applying to hexagonal\ngrids, 51–53\nobject threading, implementing, 42\nobjects\ninstantiating in optical flow, 31–33\nin R2 projective space, 155\nuse in R2 projective space, 159\nSee also C++ objects\nOGG Vorbis audio compression format, explanation of,\n337–338\nO(log(N)) search, guaranteeing with large allocator, 18\nopen lists, eliminating with IDA*, 292–294\nOpenAL API, features of, 332\nOpenCV library\nfunctions of webcamInput class in, 26–27\nuse with optical flow, 25–26\nOpenCV methods\ncvAbsDiff function, 27\nimage differences, 27–28\nLucas and Kanade algorithm, 29–30\nmotion history, 28–29\nOpenGL, shader parameters in, 546\nopinion events, function in attitude systems, 252\nopinion system\nexample of, 257\nin Fable, 260\nOPT (Belady’s Min) algorithm, use of, 6–13\noptical flow\nas array of vectors, 26\nin computer vision games, 26\ndefinition of, 25\ngame sample, 30–33\nand image differences, 27–28\ninstantiating objects in, 31–33\nLucas and Kanade algorithm used in, 29–33\nand motion history, 28–29\nand OpenCV library, 25–26\npartitioning queries for, 32\nOren-Nayar model, versus Lambert’s model, 375\noriented bounding box (OBB), use in scenes, 184\norigin shift, example of, 181\noutdoor terrain rendering. See terrain areas\nOutputData type, defining for polymorphic workflow\nblocks, 240\noverhanging terrain, creating with particle deposition,\n362–364\nP\nPA_HARD and PA_SOFT labels, using with threads,\n40\npackets, capturing with WinPcap library, 494–496\npage-replacement algorithms, use of, 6\npages\naccessing relative to Age algorithm, 8–13\nAPC/RC ratios for, 12\ncosts associated with, 11–12\ndetermining for eviction from cache, 10\nplacement with small allocator, 18\nrelationship to cache, 5\nrequesting from OS in heap allocation, 17\nparent side index, use with large allocators, 19–20\npartially-ordered plans, use in plan merging, \n282–283\nparticle deposition\nexplanation of, 353\nimproving, 354–355\nlimitations of, 355\nparticle dynamics, improving, 355–357\n578\nIndex\n\n\nparticle-placement improvements\ndunes, 361–362\nmountains, 358–361\noverhanging terrain, 362–364\nvolcanoes, 357–358\nparticles\nbehavior of, 355\nsearch radius and elevation threshold of, 356–357\nusing noise functions with, 356–357\npassword recovery, considering in authentication,\n482–483\npasswords\ninsecurity of, 486–487\nprotection in Challenge Hash Authentication, 483\ntransmitting in Secret Exchange Authentication, 484\npathfinding approaches\nA* search algorithm, 290–292\nuse of, 289\nusing hexagonal grids in, 55–56\npcap, initializing, 495\nPCS (potentially colliding set) of triangles, discovering\nat runtime, 143\nper bin marker, using with large allocator, 21\nperspective projection, applying to planes, 156\npersuasion and influence, considering in attitude \nsystems, 259–260\nphase-causing functions, managing for surround sound,\n317\nphonemes\nmapping to visemes, 459\nmapping words to, 457–459\nversus visemes, 457–458\nPhong-Blinn model, applying to backscattering,\n378–379\n“Pipes and Filters” design pattern\nliabilities of, 232–233\nusing with Artificial Contender, 230–232\npixel movement. See optical flow\npixel versus fragment shaders, 542\nplan merging, use with goal-oriented planning systems,\n283–286\nplanes, applying perspective projection to, 156\nplan-merging algorithm, implementing, 284–286\nplants, expressing shape and formation of, 447\nPlay or Pitch functions, managing in surround sound,\n317\nplayback frequency\nconsidering in audio engines, 311\nreducing for audio streams, 310\nplayer traces\nexamining, 272\nfinding emergent behaviors in, 272\nproviding contexts for, 271\nusing visual data mining with, 272\nvisualizing, 270\nplayers, clustering by IPGs, 275\nPlayerViz tool\ncapturing captured information with, 279\ndesign of, 270–271\ngenerating thumbnails of player traces with, 272\ninformation contained in, 268\nworld data in, 271\nplaying fields, representing with grids, 49\nPlaystation 3, next-gen audio engine for, 305\nPN triangles, relationship to subdivision surfaces, 382\npoint, support mapping for, 168\npointers, using with large allocators, 19\npoint-line test, using in projective space, 157\npoints\nconsecutive operations on, 158–159\nfinding in interior of Minkowski difference, 171\noperations on, 157–158\nrepresenting in projective space, 155\npolar coordinates, use with Gaussian distribution, \n202\npolar-rejection, function in GRNGs, 200\npolygon, support mapping for, 170\npolygon meshes, generation of T-intersections in,\n163–164\npolygons\nconvex quality of, 161\ncutting with lines, 162\npolyhedron, support mapping for, 170\npolymorphic workflow blocks, use in Artificial \nContender, 238–241\n“pops”\ndeconstructing, 323–324\nexplosions as, 322\npotency, purpose in attitude systems, 254–255\npotentially colliding set (PCS) of triangles, discovering\nat runtime, 143\npreempting, purpose in threading architecture, 36\nprimary buffer, role in sound systems, 332\nprinciple curvatures, relationship to farthest feature\nmap, 143\nPRNGs (pseudo-random number generators), use of,\n114–115\nprocedural modeling, potential of, 110\nIndex\n579\n\n\nprogramming errors\napplication crashes, 97–100\nmemory leaks, 100–102\nWER (Windows Error Reporting), 102–103\nprojectile paths, adding random variation to, 202\nprojective space\nobjects in RP2, 155\nuse of, 153–154\nPrototype Design Pattern, use with shaders, 544,\n547–548\nPS3, audio on, 306\npseudocode. See code samples\npseudo-random number generators (PRNGs), use of,\n114–115\npublic key infrastructure, using, 484–485\npull model, use in workflow, 236, 239\npush model, use in workflow, 236\nPython’s AST, using to replace strings with numbers,\n555–559\nQ\nQuake, animation of characters in, 365\nQuake 3, conversion mod of, 267\nQueryFlow() function\ncalling in optical flow, 31\npreventing calling in optical flow, 33\nquotes (' and \"), using with Python strings, 556\nR\nR2 projective space\nconverting vectors from, 155\nnumber range limits in, 158–161\nobjects in, 155\noperations in, 157–158, 161–164\npoints and directed lines in, 155–156\nusing integer coordinates in, 158\nradial basis functions (RBFs)\nanimating relief imposters with, 402\nand relief imposters, 404\nSee also relief imposters\nrandom number generators (RNGs)\ndistributions of, 115\nhardware RNGs, 114\nPRNGs (pseudo-random number generators),\n114–115\nand software whitening, 116\nuses of, 113–114\nrandom variation, adding to projectile paths, 202\nrandom walker, use in particle deposition, 353–354\nrandomness testing, conducting, 116\nRANDU mistake, occurrence with RNGs, 123\nrank buffers, role in audio processing, 334–336\nray, components of, 128\nray casting, explanation of, 127\nray queries, support for, 128\nraytracing\nfor acoustics, 302\ndemo application, 139–140\ndynamic scenes, 139\nuse of, 127\nand visibility queries, 128\nSee also kD-tree\nraydir array, use of, 137\nrays, finding intersections of, 130\nRBF coefficients, using with warping function and\nshaders, 405–406\nRBFs (radial basis functions)\nanimating relief imposters with, 402\nand relief imposters, 404\nSee also relief imposters\nRC (relative cost), function in cache replacement,\n12–13\nreal-time strategy (RTS) games\nfocus-context control level in, 61–63\nintegrating sketch- and unit-based interfaces in, 66\nmoving soldiers in, 63–64\nmoving troops through battlefields in, 65\npopularity of, 59\nusing sketch-based approach with, 66\nrectangle, support mapping for, 168\nrecursion, adding to raytracing, 127\nred-black tree\ncombining with book container, 22\nusing with large allocators, 18\nred-black tree node\nsearching for appropriate size, 20\nstoring, 19\nuse in non-default alignment, 21\nReif-Peters subdivision scheme, features of, 382\nrelative cost (RC), function in cache replacement,\n12–13\nrelief imposters\nanimating, 407–409\nand image warping, 403\ninterpolating warping functions for, 404–405\nobtaining, 402\n580\nIndex\n\n\nproducing, 402\nand RBFs (radial basis functions), 404\nrendering, 407\ntexels for textures used with, 409\nSee also animation systems; RBFs (radial basis \nfunctions); warping functions\nrelief maps, animating, 407\nrelief rendering, explanation of, 401\nrendering\nconsidering in Loop subdivision, 397–398\nof large terrain areas, 442–444\nmethods for, 381\nrelief imposters, 407\nsubdivision surfaces, 398\nRepeaters block, use in AC decision-making algorithms,\n237\nridge structures, creating, 359–360\nRNG methods (cryptographic)\nBlum Blum Shub, 121\n/dev/random, 121–122\nFortuna, 122\nISAAC, ISAAC+, 121\nMicrosoft’s CryptGenRandom, 122\nYarrow, 122\nRNG methods (non-cryptographic)\ncellular automata, 119\nInversive Congruential Generator, 118\nLCG (Linear Congruential Generator), 116–117\nLFG (Lagged Fibonacci Generator), 118\nLFSR (Linear Feedback Shift Register), 118\nLRGs (Linear Recurrence Generators), 119–121\nMiddle Square, 116\nTLCG (Truncated Linear Congruential Generator),\n117–118\nRNGs (random number generators)\ndistributions of, 115\nhardware RNGs, 114\nmistakes associated with, 122–124\nPRNGs (pseudo-random number generators),\n114–115\nand software whitening, 116\nuses of, 113–114\nroom acoustics, calculating, 302–303\nrotated objects, finding support mappings for, 168\nrotation error\neliminating, 367–370\noccurrence in skeletal animation, 367\nrough surfaces, model for, 374–376\nrounded box, support mapping for, 169\nRP2 projective space, use of, 154\nRTS (real-time strategy) games\nfocus-context control level in, 61–62\nintegrating sketch- and unit-based interfaces in, 66\nmoving soldiers in, 63–64\nmoving troops through battlefields in, 65\npath sketching in, 62–63\npopularity of, 59\nusing sketch-based approach with, 66\nruntime queries, using with farthest feature map,\n148–150\nS\nSAH (surface area heuristic), relationship to kD-tree,\n133\nSAT (Separating Axis Theorem), use of box-box text\nwith, 187\nScarface: The World Is Yours, MIDI interface imple-\nmented for, 346\nSCEE audio engine project, goals of, 305\nscheduler class, use in multithread job and dependency\nsystem, 89–90\nSCRIPTABLE_DefineClass( MY_CLASS ) macro,\nbinding classes with, 513\nsearch algorithms, A*, 290–292\nSecret Exchange Authentication, using, 484\nsecuring\ngame logins, 481–485\ngame sessions, 485–487\nsegment, support mapping for, 168\nSelectors block, use in AC decision-making algorithms,\n237\nsensing model\nadding memory to, 227\ncomponents of, 226–227\nSGI’s InfiniteReality2 hardware platform, access of\nmiplevels in, 414\nshader architecture, invoking blimp in, 549–551\nshader groups, using, 551–552\nshader in GLSL code sample, 376\nshader languages, parameters supported by, 547\nshader parameters, use in OpenGL, 546\nshader programs, using with room acoustics, \n302–303\nShaderManager class\nmethods on, 545\nusing, 543\nIndex\n581\n\n\nShaderParameter class\ntypes of data supported by, 546–547\nusing, 542–543\nShaderProgram class, using, 542\nshaders\ncloning of parameter types for, 547–549\ncombining with actors and properties, 552–553\nevaluating warping functions with, 405–407\nfragment versus pixel shaders, 542\nprototypes for, 544–546\nreloading at runtime, 544\nstate sets and scene graphs, 545–546\nterminology for, 541–542\nshadings, producing with Lambert model, 374\nshapes\nfinding support points for, 169\nrepresenting with support mappings, 166–170\nshrink-wrapping, 169\nsupport mappings for, 167–168\nSIMD (Single Instruction Multiple Data), role in audio,\n299\nsingle quotes ('), using with Python strings, 556\nsingletons, use of GLRThreadFoundation in threading\narchitectures, 37\nskeletal animation\nbone and rotation errors in, 366–367\nand cumulative error, 366–370\nfunctionality of, 365–366\nreconstruction errors in, 368\nsketches, creation by users in RTS games, 62\nSLERP (spherical linear interpolation), use with\ntrigonometric splines, 194\nslope of terrain, defining, 355–356\nSM2.0 clipmap path, implementing clipmaps with,\n417–418\nsmall allocator\ncombining with large allocator, 21\nfunction in heap allocation, 16–18\nusing reserved virtual address range for, 21\nsmart packet sniffer\nalternative for, 496\nexample of, 491–492, 496\nfeatures of, 491\nimplementation of, 493\nreducing security risks for, 495–496\nsmooth edges and vertices, determining in Loop subdi-\nvision, 385–386\nsmooth surfaces, representing, 381\nsnapshots, mixing in mixing system, 344\nsniffed passwords, concerns about, 482\nsocial exchanges, relationship to attitudes, 260–261\nsoldiers, moving in RTS (real-time strategy) games,\n63–64\nSorters block, use in AC decision-making algorithms,\n237\nsound buffers, role in audio processing, 333–334\nsound designers, interaction with mixing system, 343\nsound effects\ncreating, 324\nand rank buffers, 335\ntools used in creation of, 326\nsound environment, making changes to, 329\nsound falloff, demonstration of, 225–226\nsound files, comparing layouts of, 324–326\nsound instances, specifying playing of, 335\nsounds\ncomponents of, 328\ncomposition of, 322\nconceptualizing, 322\nconstructing and deconstructing, 323–324\ncreating with FMOD Designer interface, 329\nin-game rendering of, 328\nperception of, 331\nplaying limitations of, 334–335\nproperties of, 224\nSee also audio; FMOD Designer interface\nsound-system overview\nlistener, 332\nprimary buffer, 332\nsound effects, 333\nsound sources, 333\nSources block, use in AC decision-making algorithms,\n236\nSpace War game, use in behavior-cloning example, 210\nspatial movement, tracking, 274\nspatial search, using hexagonal grids in, 55\nsphere, support mapping for, 167–168\nsplines, use of, 192–194\nSplitters block, use in AC decision-making algorithms,\n237\nsqrt(d) (Kobbelt) subdivision scheme, features of, 382\nsquare tiling, neighborhoods in, 48\nstack overflows, handling, 99–100\nstatic polymorphism, implementing, 241\nsticky particles, use with overhanging terrain, 363\nstreaming audio, 309–310, 337–338\n582\nIndex\n\n\nstrings, replacing with numbers, 555–559\nsubdivision, methods for, 381\nsubdivision data structure, file format for, 391–392\nsubdivision schemes\nproperties of, 381–382\nusage of, 382–383\nsubdivision surfaces\nexplanation of, 381\nfast rendering of, 398\ntoolsets for, 382\nuses of, 382\nSee also Loop subdivision algorithm\nsubdivision type, choosing, 383\nSUBDUE tool, features of, 278\nsum-of-of uniforms algorithm, use with GRNGs,\n201–202\nsupport mappings\ncombining, 169–170\ntranslating and rotating, 168\nusing with shapes, 167–168\nusing with XenoCollide algorithm, 166–170\nsurface area heuristic (SAH), relationship to \nkD-tree, 133\nsurround sound\napproaches toward, 315–316\nDSP effects, 317–318\nsyncing channels, 316–317\nSWD compiler, code generated by, 471\nSWD files\ndefining synchronization behavior in, 473\nuse in HLA systems, 470–471\nSynchEntity class\ncreation of, 475–476\ndestruction and visibility of, 476\nsynchronization bound to, 473–474\nuse in HLA (High Level Abstraction), 471–472\nsynchronization behavior, defining in SWD files, 473\nsynchronizing game worlds\noverview of, 468–470\nsynchronized objects in, 471–475\ntechniques for, 467–468\nsynchronous versus asynchronous exceptions, 97\nT\nTables\naudio engine file formats, 308\nCMU phonemes, 458–459\nedge mask selection in Loop subdivision, 386\nfeature-space calculations, 211\ngame state data for behavior cloning, 212\nLCGs (Linear Congruential Generators) in use, 117\nmatrix inversion methods, 183\nmixing system mixing snapshots, 344\nmotion detection algorithm times for optical flow, 30\nphoneme to viseme mapping, 459\nphonemes, 458–459\nprojective space numerical ranges of results, 160\nshapes with support mappings, 168\nsubdivision data structure file-format entries, 392\nsubdivision schemes, 382\nsupport mappings for compound shapes, 169\nterrain navigation system configuration for testing,\n444\nterrain navigation system statistics, 445\nworld state modification permissions, 469\ntarget victim page, determining via age, 9\ntasks\nin multithread job and dependency system, 95\nsynchronization with dependency manager, 91–94\nterrain\ndefining slope of, 355–356\noverhanging terrain, 362–364\nterrain areas\napplying textures to, 435–437\nconcentric rings update for, 441\nmanaging virtual textures for, 437–440\nrendering issues associated with, 442–444\ntexture cache for, 437–438\ntexture memory usage for, 439–440\ntexture upload time for, 441\nupdating virtual textures for, 442\nusing trilinear filtering with, 438–439\nterrain deformation, simulating with particles, 353\nterrain formed by particles, slope of, 354–355\nterrain navigation system, results of, 444–445\nterrain texturing. See clipmaps\nTestSystem game object, use of GLRThreading library\nwith, 44–45\ntext, handling with translation tables, 555\ntexture atlas, using with graftal imposters, 447–448\ntexture cache\ndesign of, 8–13\nupdating contents of, 440–442\nusing with large terrains, 437–438\ntexture coordinates, computing in dynamic geometry,\n443\ntexture memory usage, considering for large terrain\nareas, 439–440\nIndex\n583\n",
      "page_number": 609,
      "chapter_number": 63,
      "summary": "This chapter covers segment 63 (pages 609-616). Key topics include usefulness, uses, and algorithm. Covers function.",
      "keywords": [
        "algorithm",
        "audio",
        "subdivision",
        "support mapping",
        "functions",
        "function",
        "Linear Recurrence Generators",
        "Loop subdivision algorithm",
        "Linear Congruential Generator",
        "optical flow",
        "terrain",
        "support",
        "number generators",
        "decision-making algorithms",
        "system"
      ],
      "concepts": [
        "usefulness",
        "uses",
        "algorithm",
        "function",
        "functions",
        "functionality",
        "objects",
        "game",
        "classes",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 56,
          "title": "Segment 56 (pages 544-551)",
          "relevance_score": 0.77,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 53,
          "title": "Segment 53 (pages 511-519)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 55,
          "title": "Segment 55 (pages 536-543)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 12,
          "title": "Segment 12 (pages 101-108)",
          "relevance_score": 0.7,
          "method": "api"
        }
      ]
    },
    {
      "number": 64,
      "title": "Segment 64 (pages 617-624)",
      "start_page": 617,
      "end_page": 624,
      "detection_method": "topic_boundary",
      "content": "texture pages, using in cache replacement examples,\n10–11\ntexture stack, updating for large terrains, 440–441\ntexture upload time, considering for terrain areas, 441\ntexture-based representations, animating, 403\ntextures\napplying to large terrain areas, 435\nmanaging with clipmaps, 436\nrepresenting objects with, 402\nstorage of, 435\nstoring animation data in, 406\ntexels for relief imposters, 409\nSee also virtual textures\ntexturing terrains. See clipmaps\nthrash, occurrence of, 5\nthread allocation strategies\nnaive allocation, 41\nthread pools, 41–42\nthread context switching, speed of, 40\nthread handles, storage of, 39–40\nthread local storage, using in heap allocation, 22\nthread pools, use of, 41–42\nthread stack size, altering default for, 39–40\nthreading architecture, designing, 36\nthreading engine, development of, 35\nthreading systems\nengineering, 39\nexecution of, 37\nGLRThreading library sample, 44–45\nthreads\nand cache coherency, 43–44\ndesignating task priority of, 41\nexecution properties of, 40\nversus jobs, 87\nobject threading, 42\npreemption and simultaneous execution of, 39\nand processor affinity, 40\nproperties of, 39–40\nsafety, reentrancy, object synchronicity, and data\naccess, 43\nuse of, 38–39\ntiling\ncreation of, 47\nuse with textures for large terrains, 438\nT-intersections, generation in polygon meshes, 163–164\nTLCG (Truncated Linear Congruential Generator),\ndescription of, 117–118\ntotalistic cellular automaton, use in RTS games, 64\ntotally-ordered plans, use in plan merging, 282–283\ntracepoints, using with network code, 492\ntransformation matrices\nexample of, 182–184\ninverting, 179\nretrieving for ARToolkit, 74\ntransformation semantics\ncoordinate systems used in, 187\nexplanation of, 180\nextracting from matrices, 181–184\nflexibility of, 186\nrequirements for, 183\nuse with box-box test and SAT, 187\nusing for collision detection tasks, 184–188\ntriangles\nfinding intersections of, 144\ntesting for graftal imposters, 451\ntesting for XenoCollide and MPR, 172\ntrigonometric functions, fast evaluation of, 194–195\ntrigonometric splines, use of, 192–194\ntrilinear filtering, using with large terrain areas,\n438–439\ntroops, moving through battlefields in RTS games, 65\nTruncated Linear Congruential Generator (TLCG),\ndescription of, 117–118\ntruncation, effect of, 153\ntuning application, using with mixing system, 345–346\nU\nUCT (Urban Combat Testbed), use of, 267\nunhandled exceptions, reporting, 98–99\nunified sensing model\nadding memory to, 227\ncomponents of, 226–227\nuniform distribution, using in RNGs, 115\nUNIX platforms, creating core dump on, 99\nUpdate() function, using in optical flow, 33\nUrban Combat Testbed (UCT), use of, 267\nuser input, capturing in RTS (real-time strategy) games,\n62–63\nV\nVA (vertices array), use with subdivision data structures,\n390\nvalence, purpose in attitude systems, 253–254\nvalues\nrelationship to attitudes, 250\nupdating in attitude systems, 252\n584\nIndex\n\n\nvariables, allocation in OpenCV, 26–27\nvector of C++ STL, use with hexagonal grids, 51–52\nvector spaces, mapping between, 180\nvectors\nuse in projective space, 157–158\nuse with GPUs, 300\nvertical blanking period\nwith limited time, 82–83\nrelationship to handheld gaming systems, 82–83\nvertices\nin Loop subdivision, 384, 386–387\nupdating with Loop subdivision algorithm, 393\nvertices array (VA), use with subdivision data structures,\n390\nvictim pages, finding, 10\nview cone check, doing in agent-sensing model, 218\nview distance, computing in agent-sensing model, 218\nvirtual textures\nmanaging for large terrains, 437–440\nmapping to geometry models, 442–443\nupdating for large terrain, 442\nSee also textures\nvisemes\nmapping phonemes to, 459\nversus phonemes, 457–458\nuse with head model for lipsyncing, 455–457\nvision, modeling with certainty, 222–224\nvision model\naugmenting with ellipses, 219–222\nconsidering in agent-sensing model, 217–219\ncreating, 222–224\nvisitCallFunc, using with AST, 557–558\nVista, API for WER, 103\nvisual appearance of games, impact of grids on, 49\nvisual data mining, using with player traces, 272\nvolcanoes, improving particle placement of, 357–358\nvolume levels, setting for audio channels, 311\nvoxels, using with overhanging terrain, 364\nW\nwalk function, using with AST, 557–558\nWalker class, using with hexagonal grids, 53–54\nwalking, controlling in FPS games, 71\nwalking dog animation, control points for, 409–410\nwalking motions, producing, 408–409\nwarping functions\nevaluating using shaders, 405–407\ninterpolating for relief imposters, 404–405\nSee also relief imposters\nwarrior attitudes, FSM for, 257–258\nwaveforms, oscillating values in, 331\nWebcam resolution, considering in optical flow, 33\nwebcamInput class\ncvAbsDiff function in, 27–28\nencapsulation of functionality in, 31\nfunctions in OpenCV, 26–27\nWebsites\nARToolkit, 71\nL’Ecuyer’s papers on RNG algorithms, 124\nrandom noise, 114\nsubdivision surface toolsets, 382\nSUBDUE tool, 278\nUCT (Urban Combat Testbed), 267\nWELL algorithm, use of, 120–121\nWER (Windows Error Reporting), 102–103\nwheel, support mapping for, 170\nwhitening algorithms, using with RNGs, 116\nWin32 model for threading architecture, standard for, 36\nwindowing techniques\nusing with FFT, 312–314\nusing with frequency data, 312, 314\nWindows Error Reporting (WER), 102–103\nWindows Vista, API for WER, 103\nWinPcap library, capturing packets with, 494–496\nWinQual online portal, features of, 102\nwords, mapping to phonemes for lipsyncing, 457–459\nworkflow algorithms, using with Artificial Contender,\n230–232\nworkflow blocks, requirements for, 238\nSee also blocks\nworkflow diagram\nusing with Artificial Contender, 234–235\nvisualizing constraints on, 246\nworkflows\nconstructing, 246\ninterrupting, 244\nprocess of, 235–236\nworld coordinates, transforming model to, 182\nworld geometry\nfinding information values for surfaces in, 278\nvisualizing for player traces, 271\nworlds, decomposing into regions, 271\nWriteCoreDump function, using on UNIX platforms,\n99\nIndex\n585\n\n\nX\nX, Y, Z approach, using with surround sound, 315\nXACT audio tool, features of, 326\nXbox 360, implementation in threading systems, 37\nXenoCollide algorithm\nand MPR (Minkowski Portal Refinement), 166\noptimizing, 177\nsupport mappings used with, 166–170\nuse of, 171–176\nY\nYarrow RNG method, description of, 122\nZ\nziggurat method, use with GRNGs, 201\nzone approach, applying to hearing model, 225–226\n586\nIndex\n\n\nCREATE AMAZING GRAPHICS AND\nCOMPELLING STORYLINES FOR YOUR GAMES!\nAdV4Mfrd VftaJftl Elhftl\n[•HhWr*cl3D\nISBN! 1-9MOO-961-1 •!».«•\n•MlC DttUllrtQ IM Carnal\nISBN! 1.IW2DO--J51.*\" 12? .W\nBe^lrtrtlrto Qarft* Art\nin MttMaxa\nQ.BI4. 1.W2HJ-WS-5 • 129.99\nB«ti)ftnli>Q Oarn* Oraphlei.\nISBN! 1-5920CM3O.)(« S29.99\nHiOv?ri \"or\nQMVM FreGrammtrt arifl Artl*U\nBEN. l.SSZM.[»2-*«S39.99\nCharacter Davelatwnent\nAnd ItoixittllrKi hf Gem**\nCOM. 1.W2DOJ.K1J • 139.99\nColrtt At I F« Tptfll,\nJ*t4nd Edlllsn\nISBN! \\lvVXWf.' 134,99\nO(pfn? Cha*artsf Animallsrt\nAll lrt<M«\nBEN! VS9BJ3.K4-4-I49.99>\nT1W D*rkSMi?\n#1 Gort)* Trlitlirdrt\nEBM. l-i»2DO-i9:i-fl • 139.99\nTo order,\nvisit www.courseptr.com or call 1.800.648.7450\n\n\nCall 1.800.648.7450 to order\nOrder online at www.courseptr.com\nBeginning Math Concepts\nfor Game Developers\n1-59363-290-6 • S29.99\nGame Design, Second Edition\n1-592 WW93-3 • S39.99\nBeginning\nJava Game Programming,\nSecond Edition\n1-598G3n476-3 • S 2 9.99\nGame Programming All in One,\nThird Edition\n1-59863-289-2 • S49.99\nGOT GAME?\nCOURSE TECHNOLOGY\nCENGAGE Learning-\nProfessional * Technical • Reference\n\n\nCOURSE TtCHNOLQ&Y\nCtNGA-GE LuaminB.-\nProfettiHial • TtDTiLjl - ndoEficz\nOne Force. One Solution.\nFor Your Game Development and Animation Needs)\nfturln Rim tAnix httt |iiiiiLMl tan:-, wilh Thulium <'-:ii[>:f alini Hi pfiniii1 you whh rim ntiic nf Ihr i|i.-ilily ynkln jiiu Imc ccnr- In [HIM.\ntaint; Puj^jmrfilrttl Ghlrtifi\nrt?tt.E3<5flJVLHKfLH\nAl GdrtfttPrt^rartlnitfJ WfadOrtl 3\nI;BM uiK».i=T^.!M«\nEdiiL GMur ftrilgii and CKdlldtt\nfor Fun t L^ninng\n0hl-U4«(M4^>USL5i\nihadBf ^:\nAfhanced Hendem^TKhniquK\nSEh. • ESJM'lJ^-Ha*\nPractical Pinar J\ni!BH 1-5W«HW*M«\nArinuling Facial PnaluKK\n& Expressions\niiw • »iKi:-i!)-y:-w\nMotile ID Game DeuHOfSincin:\nfrom ilnrl tfl M«rk?t\nwtti-swSd'Su-fHas\nIncuduc'tleii ta JD Grdfjliki\nK Anim^tim U^iitg Maqr'\nIbm 14OT>4i>4'U9tf\nBuiintss anJ Legal Frlmer\nfor -Game Oeudopmenl\n,^; ••'^j'/--^:-.'-'^-'^\nCheck out the entire list of Charles River Media guides at www.courseptr.com\nCall 1.800.648.7450 to order\nOrder online at www.courseptr.com\n\n\nV COURSE TECHNOLOGY\n- Tettrtcal - Reference\nJournal of Game Development\nTh* jQUimtofdme Qewtffm«v-'J<Xi\\>'.'i? B journal todictfBdlQ lha diwamrijcon «f tadmg-riae. anjnal r&warch on g»n**wlmm«nt\ntDpiH jnj ttit noa i«ctry findings in \"alblad i«*min dixiplinpa. Nnit^hw. jotaw0. and twircogv Th« nH^hr^n in lha joiir^'cflinss\n<nem batf^a«WiB qnd dia inJuflnr. ?nd co>w3gini4-rBi]odlQpic&fiMni lha arpescH physitSi mnlhamBlina artificialinwiigsnce. a'jp^cs.\nnewioriung. »ndo. amni3tiof\\ nebe(c3.^is«gle?t«ri nnd nteiigcln'BirittrtainmBnl tiattiegui nMha Jewnsilo unrternea cutting-edge ideas\n1r-:m rhe indiisirv 4th ucadBmc r«sarc.h in orderln advincB ihs luid ni game dauBbpnant snri hi pnamnla rhe Bccaplanca ed lha study of\ngjma JavBiopnant by lha acadamir: communilv\nEKA jimuji :(jh?r.npnon it far DHB lui vtMna. wtnch con??!? oJ 1 quBnertv ^sua?. jni innludBi hnlh- an elBtfranc und nnlra ww> tf wtfi\ntat ACM. ISBA DKU, lid IEEE HTI b\nFar mere Inlermtlti «rf tt wder, plsis* Midi, www.laad.tom\nPar guesbana BDOUI Ilia Jaufiatu vaa ordar. plNia cuniaci tffll Stnlili, ail.Bnttft&mmi.ait\nThtJniir.fm.lE/[7tiii~fl flflL-tf.^~flfif is tm^ accdprinq pnpar siinmisMnns Allpipsnt will bb K:attff&d .according la ththiqhasl stindirch nl Ih*\nFdlnrial Hnnrrl and K^ mlarRRS Airihnrs wil rnrRNn h tr-svoll prinlsjllhnirpiihliihfld pnpnr andwIMrjrisFiar rnn>rignt1n lha pnhlishar. Thnre\njrn no n»nF: rhnrgRS 1nrpi±4ic.ilmii Fill mslnirlinns 1nr marii'irrft prnpanhan and utmin-iinn cnn b* Iniuiri niilinF: nl ww.JDgdjcn The\njnnr.'UL'is pnhlislidd an a qnnrinrly hnns JG nhslrirls am ncrnplad nn nil nugninrj buix\nP^gesubrrii yajr sWractandiuhmissinnimn onlineji^^wM[ofld*«n ar>iseryiihe Mipnpgrto ulpKtafltJj**Rnd*djfnhfc^Mnyntf cam\nFur quDriians Diillm pifliiasiun proc«^ pluasc-cortaLl [mi^inkli iE<H^nirtl9oaBf*fajnHur Michaul Vouici aEi*tDr3JDfld.cnm\nCall far Papers\nwww.jogd.com\n\n\nLicense Agreement/Notice of Limited Warranty\nBy opening the sealed disc container in this book, you agree to the following terms and conditions. If, upon reading the\nfollowing license agreement and notice of limited warranty, you cannot agree to the terms and conditions set forth, return\nthe unused book with unopened disc to the place where you purchased it for a refund.\nLicense:\nThe enclosed software is copyrighted by the copyright holder(s) indicated on the software disc. You are licensed to copy\nthe software onto a single computer for use by a single user and to a backup disc. You may not reproduce, make copies,\nor distribute copies or rent or lease the software in whole or in part, except with written permission of the copyright\nholder(s). You may transfer the enclosed disc only together with this license, and only if you destroy all other copies of the\nsoftware and the transferee agrees to the terms of the license. You may not decompile, reverse assemble, or reverse engi-\nneer the software.\nNotice of Limited Warranty:\nThe enclosed disc is warranted by Course Technology to be free of physical defects in materials and workmanship for \na period of sixty (60) days from end user’s purchase of the book/disc combination. During the sixty-day term of the\nlimited warranty, Course Technology will provide a replacement disc upon the return of a defective disc.\nLimited Liability:\nTHE SOLE REMEDY FOR BREACH OF THIS LIMITED WARRANTY SHALL CONSIST ENTIRELY OF\nREPLACEMENT OF THE DEFECTIVE DISC. IN NO EVENT SHALL COURSE TECHNOLOGY OR THE\nAUTHOR BE LIABLE FOR ANY OTHER DAMAGES, INCLUDING LOSS OR CORRUPTION OF DATA,\nCHANGES IN THE FUNCTIONAL CHARACTERISTICS OF THE HARDWARE OR OPERATING SYSTEM,\nDELETERIOUS INTERACTION WITH OTHER SOFTWARE, OR ANY OTHER SPECIAL, INCIDENTAL,\nOR CONSEQUENTIAL DAMAGES THAT MAY ARISE, EVEN IF COURSE TECHNOLOGY AND/OR THE\nAUTHOR HAS PREVIOUSLY BEEN NOTIFIED THAT THE POSSIBILITY OF SUCH DAMAGES EXISTS.\nDisclaimer of Warranties:\nCOURSE TECHNOLOGY AND THE AUTHOR SPECIFICALLY DISCLAIM ANY AND ALL OTHER\nWARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OF MERCHANTABILITY,\nSUITABILITY TO A PARTICULAR TASK OR PURPOSE, OR FREEDOM FROM ERRORS. SOME STATES\nDO NOT ALLOW FOR EXCLUSION OF IMPLIED WARRANTIES OR LIMITATION OF INCIDENTAL OR\nCONSEQUENTIAL DAMAGES, SO THESE LIMITATIONS MIGHT NOT APPLY TO YOU.\nOther:\nThis Agreement is governed by the laws of the State of Massachusetts without regard to choice of law principles. The\nUnited Convention of Contracts for the International Sale of Goods is specifically disclaimed. This Agreement consti-\ntutes the entire agreement between you and Course Technology regarding use of the software.\n",
      "page_number": 617,
      "chapter_number": 64,
      "summary": "This chapter covers segment 64 (pages 617-624). Key topics include games, gaming, and model.",
      "keywords": [
        "large terrain areas",
        "Urban Combat Testbed",
        "Linear Congruential Generator",
        "large terrains",
        "Truncated Linear Congruential",
        "terrain areas",
        "textures texturing terrains",
        "thread",
        "Game",
        "Combat Testbed",
        "texture upload time",
        "thread local storage",
        "Urban Combat",
        "Limited Warranty",
        "model"
      ],
      "concepts": [
        "games",
        "gaming",
        "model",
        "disc",
        "thread",
        "texture",
        "functions",
        "function",
        "functionality",
        "animating"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 8",
          "chapter": 48,
          "title": "Segment 48 (pages 461-471)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 49,
          "title": "Segment 49 (pages 475-482)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 56,
          "title": "Segment 56 (pages 544-551)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 3,
          "title": "Segment 3 (pages 42-63)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 51,
          "title": "Segment 51 (pages 491-499)",
          "relevance_score": 0.65,
          "method": "api"
        }
      ]
    },
    {
      "number": 65,
      "title": "Segment 65 (pages 625-632)",
      "start_page": 625,
      "end_page": 632,
      "detection_method": "topic_boundary",
      "content": "COLOR PLATE 1\nScreenshot of the “path” command from Gem 1.6, where the user’s sketch is highlighted\nin green and the force vectors are illustrated on the battlefield.\nCOLOR PLATE 2\nScreenshot of the “target” command from Gem 1.6, where the command glyph (spiral) is\nhighlighted in red and the force vectors are illustrated on the battlefield.\n\n\nCOLOR PLATE 3\nScreenshot of the “erase” command from Gem 1.6, where the user sketch is highlighted in\nblue and the force vectors are illustrated on the battlefield.\nCOLOR PLATE 4\nA pile of shapes with collisions resolved by the methods in Gem 2.5.\n\n\nCOLOR PLATE 5\nA volcano created using advanced particle deposition, as described in Gem 5.1.\nCOLOR PLATE 6\nMountains created using advanced particle deposition, as described in Gem 5.1.\n\n\nCOLOR PLATE 7\nDunes created using advanced particle deposition as described in Gem 5.1.\nCOLOR PLATE 9\nImage of a clipped mipmap stack from Gem 5.6.\nCOLOR PLATE 8\nA dog impostor from Gem 5.5 mod-\neled as a quad-layer relief texture.\nThe depth values of the progressing\nlayers are stored in the R, G, B and A\nchannels, respectively (left). A view of\nthe rendered dog impostor is shown\non the right. \n\n\nCOLOR PLATE 10\nLeft to right, top to bottom, from Gem 5.7: the example diffuse and bump render targets, traditional decals,\ndecals using the technique in Gem 5.7, erosion over time with opacities of 0, 20, 40, 60, 80, and 100 per-\ncent, and decals applied on non-planar geometry.\n\n\nCOLOR PLATE 11\nRings of detail and an example of a virtual texture applied to terrain as in Gem 5.8, showing levels of detail\nusing color codes.\nCOLOR PLATE 12\nAn example of rendering with graftal imposters from Gem 5.9.\n\n\nCOLOR PLATE 13\nSixteen visemes, each shown at its extreme (1.0) morph, as described in Gem 5.10.\n\n\nCOLOR PLATE 14\nSeveral examples of shaders created with the data-driven shader manager, as described in Gem 7.4.\n",
      "page_number": 625,
      "chapter_number": 65,
      "summary": "This chapter covers segment 65 (pages 625-632). Key topics include color, plate. COLOR PLATE 4\nA pile of shapes with collisions resolved by the methods in Gem 2.5.",
      "keywords": [
        "COLOR PLATE",
        "Gem",
        "PLATE",
        "COLOR",
        "command from Gem",
        "force vectors",
        "vectors are illustrated",
        "command",
        "Screenshot",
        "battlefield",
        "created",
        "highlighted",
        "force",
        "vectors",
        "illustrated"
      ],
      "concepts": [
        "gem",
        "color",
        "plate",
        "geometry",
        "manager",
        "dunes",
        "mountains",
        "erosion",
        "non",
        "left"
      ],
      "similar_chapters": [
        {
          "book": "makinggames",
          "chapter": 39,
          "title": "Segment 39 (pages 340-347)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 32,
          "title": "Segment 32 (pages 632-651)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "makinggames",
          "chapter": 40,
          "title": "Segment 40 (pages 348-355)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 43,
          "title": "Segment 43 (pages 424-431)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "makinggames",
          "chapter": 21,
          "title": "Segment 21 (pages 179-188)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "GAME\n\nPROGRAMMING\n\nEdited by Scott Jacobs",
      "content_length": 41,
      "extraction_method": "OCR"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "Game\nProgramming\nGems 7\nEdited by \nScott Jacobs\nAustralia • Brazil • Japan • Korea • Mexico • Singapore • Spain • United Kingdom • United States\nCharles River Media\nA part of Course Technology, Cengage Learning\n",
      "content_length": 211,
      "extraction_method": "Direct"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "© 2008 Course Technology, a part of Cengage Learning. \nALL RIGHTS RESERVED. No part of this work covered by the copyright\nherein may be reproduced, transmitted, stored, or used in any form or by\nany means graphic, electronic, or mechanical, including but not limited to\nphotocopying, recording, scanning, digitizing, taping, Web distribution,\ninformation networks, or information storage and retrieval systems, except\nas permitted under Section 107 or 108 of the 1976 United States Copyright\nAct, without the prior written permission of the publisher.\nPublisher and General Manager, \nCourse Technology PTR: Stacy L. Hiquet\nAssociate Director of Marketing:\nSarah Panella\nManager of Editorial Services: Heather\nTalbot\nMarketing Manager: Jordan Casey\nSenior Acquisitions Editor: Emi Smith\nProject/Copy Editor: Kezia Endsley\nCRM Editorial Services Coordinator:\nJen Blaney\nInterior Layout Tech: Judith Littlefield\nCover Designer: Tyler Creative Services\nCD-ROM Producer: Brandon Penticuff\nIndexer: Valerie Haynes Perry\nProofreader: Sue Boshers\nPrinted in the United States of America\n1 2 3 4 5 6 7 11 10 09 08\nFor product information and technology assistance, contact us at\nCengage Learning Customer & Sales Support, 1-800-354-9706\nFor permission to use material from this text or product,\nsubmit all requests online at cengage.com/permissions\nFurther permissions questions can be emailed to\npermissionrequest@cengage.com\nLibrary of Congress Control Number: 2007939358\nISBN-13: 978-1-58450-527-3\nISBN-10: 1-58450-527-3\nCourse Technology\n25 Thomson Place\nBoston, MA  02210\nUSA\nCengage Learning is a leading provider of customized learning solutions\nwith office locations around the globe, including Singapore, the United\nKingdom, Australia, Mexico, Brazil, and Japan. Locate your local office at:\ninternational.cengage.com/region\nCengage Learning products are represented in Canada by \nNelson Education, Ltd.\nFor your lifelong learning solutions, visit courseptr.com\nVisit our corporate website at cengage.com\neISBN-10: 1-30527-676-0\n",
      "content_length": 2029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "iii\nContents\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\nAbout the Cover Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\nAcknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv\nContributor Bios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii\nSECTION 1 GENERAL PROGRAMMING . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nAdam Lake, Graphics Software Architect\n1.1\nEfficient Cache Replacement Using the Age and Cost Metrics. . . . . . . 5\nColt “MainRoach” McAnlis, Microsoft Ensemble Studios\n1.2\nHigh Performance Heap Allocator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nDimitar Lazarov, Luxoflux\n1.3\nOptical Flow for Video Games Played with Webcams . . . . . . . . . . . . . 25\nArnau Ramisa, Institut d’Investigació, en Intelligència Artificial\nEnric Vergara, GeoVirtual\nEnric Martí, Universitat Autónoma de Barcelona\n1.4\nDesign and Implementation of a Multi-Platform Threading Engine. . . 35\nMichael Ramsey\n1.5\nFor Bees and Gamers: How to Handle Hexagonal Tiles. . . . . . . . . . . . 47\nThomas Jahn, King Art\nJörn Loviscach, Hochschule Bremen\n1.6\nA Sketch-Based Interface to Real-Time Strategy Games Based on a\nCellular Automaton . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\nCarlos A. Dietrich, Luciana P. Nedel, João L. D. Comba\n1.7\nFoot Navigation Technique for First-Person Shooting Games. . . . . . . 69\nMarcus Aurelius C. Farias, Daniela G. Trevisan, Luciana P. Nedel\n1.8\nDeferred Function Call Invocation System . . . . . . . . . . . . . . . . . . . . . 81\nMark Jawad, Nintendo of America Inc.\n1.9\nMultithread Job and Dependency System . . . . . . . . . . . . . . . . . . . . . . 87\nJulien Hamaide\n",
      "content_length": 1954,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "1.10\nAdvanced Debugging Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\nMartin Fleisz\nSECTION 2 MATH AND PHYSICS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\nGraham Rhodes, Applied Research Associates, Inc.\n2.1\nRandom Number Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\nChris Lomont\n2.2\nFast Generic Ray Queries for Games. . . . . . . . . . . . . . . . . . . . . . . . . 127\nJacco Bikker, IGAD/NHTV University of Applied Sciences—Breda, The Netherlands\n2.3\nFast Rigid-Body Collision Detection Using Farthest Feature Maps . . 143\nRahul Sathe, Advanced Visual Computing, SSG, Intel Corp.\nDillon Sharlet, University of Colorado at Boulder\n2.4\nUsing Projective Space to Improve Precision of Geometric\nComputations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\nKrzysztof Kluczek, Gda´nsk University of Technology\n2.5\nXenoCollide: Complex Collision Made Simple . . . . . . . . . . . . . . . . . . 165\nGary Snethen, Crystal Dynamics\n2.6\nEfficient Collision Detection Using Transformation Semantics. . . . . 179\nJosé Gilvan Rodrigues Maia, UFC\nCreto Augusto Vidal, UFC\nJoaquim Bento Cavalcante-Neto, UFC\n2.7\nTrigonometric Splines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\nTony Barrera, Barrera Kristiansen AB\nAnders Hast, Creative Media Lab, University of Gävle\nEwert Bengtsson, Centre For Image Analysis, Uppsala University\n2.8\nUsing Gaussian Randomness to Realistically Vary Projectile Paths . 199\nSteve Rabin, Nintendo of America Inc.\nSECTION 3 AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\nBrian Schwab\n3.1\nCreating Interesting Agents with Behavior Cloning. . . . . . . . . . . . . . 209\nJohn Harger\nNathan Fabian\niv\nContents\n",
      "content_length": 2030,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "3.2\nDesigning a Realistic and Unified Agent-Sensing Model. . . . . . . . . . 217\nSteve Rabin, Nintendo of America Inc.\nMichael Delp, WXP Inc.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming \nApproach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\nIskander Umarov\nAnatoli Beliaev\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, \nand NPC Personalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\nMichael F. Lynch, Ph.D., Rensselaer Polytechnic Institute, Troy, NY\n3.5\nUnderstanding Intelligence in Games Using Player Traces and\nInteractive Player Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265\nG. Michael Youngblood, UNC Charlotte\nPriyesh N. Dixit, UNC Charlotte\n3.6\nGoal-Oriented Plan Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\nMichael Dawe\n3.7\nBeyond A*: IDA* and Fringe Search . . . . . . . . . . . . . . . . . . . . . . . . . . 289\nRobert Kirk DeLisle\nSECTION 4 AUDIO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297\nAlexander Brandon\n4.1\nAudio Signal Processing Using Programmable Graphics Hardware . 299\nMark France\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine. . . . . . . . 305\nJason Page, Sony Computer Entertainment, Europe\n4.3\nListen Carefully, You Probably Won’t Hear This Again . . . . . . . . . . . 321\nStephan Schütze\n4.4\nReal-Time Audio Effects Applied . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\nKen Noland\n4.5\nContext-Driven, Layered Mixing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\nRobert Sparks\nContents\nv\n",
      "content_length": 1785,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "SECTION 5 GRAPHICS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\nTimothy E. Roden, Angelo State University\n5.1\nAdvanced Particle Deposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\nJeremy Hayes, Intel Corporation\n5.2\nReducing Cumulative Errors in Skeletal Animations . . . . . . . . . . . . . 365\nBill Budge, Sony Entertainment of America\n5.3\nAn Alternative Model for Shading of Diffuse Light for \nRough Materials. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\nTony Barrera, Barrera Kristiansen AB\nAnders Hast, Creative Media Lab, University of Gävle\nEwert Bengtsson, Centre For Image Analysis, Uppsala University\n5.4\nHigh-Performance Subdivision Surfaces . . . . . . . . . . . . . . . . . . . . . . 381\nChris Lomont\n5.5\nAnimating Relief Impostors Using Radial Basis Functions Textures . 401\nVitor Fernando Pamplona, Instituto de Informática: UFRGS\nManuel M. Oliveira, Instituto de Informática: UFRGS\nLuciana Porcher Nedel, Instituto de Informática: UFRGS\n5.6\nClipmapping on SM1.1 and Higher . . . . . . . . . . . . . . . . . . . . . . . . . . 413\nBen Garney\n5.7\nAn Advanced Decal System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423\nJoris Mans\nDmitry Andreev\n5.8\nMapping Large Textures for Outdoor Terrain Rendering. . . . . . . . . . 435\nAntonio Seoane, Javier Taibo, Luis Hernández, and \nAlberto Jaspe, VideaLAB, University of La Coruña\n5.9\nArt-Based Rendering with Graftal Imposters. . . . . . . . . . . . . . . . . . . 447\nJoshua A. Doss, Advanced Visual Computing, Intel Corporation\n5.10\nCheap Talk: Dynamic Real-Time Lipsync. . . . . . . . . . . . . . . . . . . . . . 455\nTimothy E. Roden, Angelo State University\nSECTION 6 NETWORKING AND MULTIPLAYER . . . . . . . . . . . . . . . . . . . 463\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465\nDiana Stelmack\nvi\nContents\n",
      "content_length": 2053,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "6.1\nHigh-Level Abstraction of Game World Synchronization . . . . . . . . . . 467\nHyun-jik Baeb\n6.2\nAuthentication for Online Games. . . . . . . . . . . . . . . . . . . . . . . . . . . . 481\nJon Watte\n6.3\nGame Network Debugging with Smart Packet Sniffers . . . . . . . . . . . 491\nDavid L. Koenig, The Whole Experience, Inc.\nSECTION 7 SCRIPTING AND DATA-DRIVEN SYSTEMS . . . . . . . . . . . . . 499\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501\nScott Jacobs\n7.1\nAutomatic Lua Binding System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503\nJulien Hamaide\n7.2\nSerializing C++ Objects Into a Database Using Introspection . . . . . . 517\nJoris Mans\n7.3\nDataports. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535\nMartin Linklater\n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine. . . . . . . . 541\nCurtiss Murphy, Alion Science and Technology\n7.5\nDance with Python’s AST. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555\nZou Guangxian\nAbout the CD-ROM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563\nContents\nvii\n",
      "content_length": 1299,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "ix\nPreface\nS\nix volumes of Game Programming Gems have preceded the edition now in your\nhands. Useful, practical ideas and techniques have spilled out of each and every\none of them. Referred to in online discussions, contemplated by inquisitive readers,\nand consulted by both amateur and professional game developers, I believe the previ-\nous editions have contributed toward making the games we all play more innovative,\nentertaining, and satisfying. Significant efforts have been made so that this 7th volume\ncontinues this tradition.\nPassion for Game Development\nGame development is a fantastic endeavor of which to be a part. It can be a true mer-\nitocracy allowing passion and talent to shine. Degrees and experience can help get you\nin the door, but it often comes down to results. Is your code maintainable? Does its\nperformance meet or exceed targets? Are the visuals and audio compelling? Is the\ngameplay fun? The challenge of excelling in these areas certainly contributes to the\nexcitement of game development and I imagine is one of the motivations that inspired\nthe authors of the following gems to share their ideas and experiences. I hope the same\ndesires to excel have brought you to this book, as the intention of this volume and\nindeed the entire series is to provide tools and inspiration to do so.\nThere are not many industries where passion for the work runs so high that work-\ning professionals gather together with interested amateurs for weekend “jams” to do\nalmost exactly what they just spent the previous five days doing. Maybe some of the\nlumberjack competitions I’ve seen on TV come close. But how many lumberjacks\nhave a logging project going on at home just to try out some new ideas for fun and\nexperience?\nThe necessity of domain expertise requirements means that often game develop-\ners become relegated to a particular role: graphics programmer, AI programmer, and\nso on. The sections of this book certainly reflect some of the common dividing lines\nbetween disciplines, although I must respect those who wish to quarrel with a few of\nthe classifications within those categories as they sometimes don’t always easily fall\ninto just a single area. While I hope those with a more narrow focus find gems to suit\ntheir interests, I’m very excited about the diverse range and ability to appeal to those\nwith a passion for all areas of game development. I want graphics programmers to\nread audio gems and vice versa!\n",
      "content_length": 2445,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "Wanting to Make Games\nEnthusiasm for game development from industry insiders may help explain why so\nmany seem so eager to join up as game developers. Although self-taught independent\nrenegades can still get their foot in the door (sometimes even making their own fan-\ntastic doors!), it is becoming increasingly easy to find quality educational help for\nthose trying to enter game development as a first career choice. Besides the traditional\nmath and computer science educational routes and a wealth of quality introductory to\nadvanced publications, specialized game development degrees and courses are avail-\nable at secondary schools and universities around the world, sometimes working in\nclose collaboration with professional development studios. A wide variety of game\ngenres are represented by published titles able to be modded, offering unprecedented\naccess to cutting-edge multi-million-dollar game engines and a great way to enhance\nyour experience or demo portfolio. Additionally, for most genres of games you can\neasily locate quality Open Source titles or engines available for inspection, experimen-\ntation, and contribution.\nThe opportunity to contribute to gaming also looks good for those passionate\namateurs with significant non-game-related software development experience. We \ncan use them. As game designs, target hardware, and development teams themselves\nbecome increasingly large and complex, the industry finds itself continuing its vora-\ncious appetite for good ideas from the rest of the software development industry. Does\nyour development team include a DBA (pipe down, MMO developers!)? Inside you’ll\nfind a gem that suggests ways to integrate your object system with a relational data-\nbase. We have a networking gem that applies tools to multiplayer development that\nare common to many network administrators, but may not yet have widespread use in\nour industry. Recognizing trends and successes in the wider software development\ncommunity, development teams are increasingly adopting formalized project manage-\nment and production methodologies like Agile and Scrum, where we can benefit\nfrom the general experience of our colleagues outside of game development. Making\ngames isn’t like making word processors, but good solutions for managing ever\nincreasing team sizes, facilitating efficient intra-team communication, and managing\ncustomer (publisher!) relationships can’t help but be similar to good solutions to the\nsame problems experienced by those outside our industry. The shift to multi-core\nmachines, whether on a PC or current-day consoles, has developers looking beyond\nthe traditional C/C++ programming languages to solve problems of concurrency and\nsynchronization and we are actively seeking out the experiences of those versed in lan-\nguages like Haskell and Erlang to see of what we may make use.\nPassion for Fun\nGames are appealing because of their ability to challenge, amuse, and entertain. Many\nof our gems deal with the messy behind-the-curtain bits that don’t directly contribute\nto making a game fun. A genre re-definer played over and over again and a clunker\nx\nPreface\n",
      "content_length": 3130,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "abandoned prior to the first boss fight can be using the same collision detection sys-\ntem or C++ to scripting language interface. It is the experience created by playing the\ngame that produces the fun. So, in addition to gems addressing core bits, there are\ngems that contribute directly to a player’s experience of the game, including audio\nproduction gems and human-game interactions. People are hungry for and eager to\ntry new ways to interact with their games. The recent successes of Rock Band, the Gui-\ntar Hero franchise, Dance Dance Revolution, and of course, Nintendo’s Wii, have\ndemonstrated this without a doubt. New interfaces have given long-time gamers new\nexperiences as well as tempted those not normally enticed by electronic games to give\nthem a try, often opening a whole new avenue of fun for them, and new markets for\nus. I’m proud that this volume introduces three gems related to under-explored ideas\nin human-game interaction and greatly look forward to what will come in the future\nas these ideas and others are tried and refined.\nInto this world of passionate developers, eager newcomers, voracious production\nrequirements, and demands for innovating and entertaining gameplay and design\ncomes this volume. Asking one book to meet the needs of all these interests is a tall\norder, but I feel confident that what follows will deliver, and I hope you agree. Let me\nknow when your game is released. I want to check it out!\nPreface\nxi\n",
      "content_length": 1458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "xiii\nAbout the Cover Image\nC\nhristopher Scot Roby created the Game Programming Gems 7 cover. The cover\nrepresents a few of the early steps in producing content for a game. Starting with\nthe very left edge of the image, there are remnants of the original sketch, which was\nlater painted on in Photoshop, resulting in the concept art seen on the left. The right\nportion reflects one of the very latest procedures for turning concepts into playable\nassets. Google Sketchup’s Photo Match feature is used to block in geometry conform-\ning to the concept image. Depending upon the pipeline, this geometry can then be\ndirectly exported into a form usable by a game, greatly speeding up the process of\ngoing from concept to something playable!\n",
      "content_length": 736,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "xv\nAcknowledgments\nI\nmust start by thanking Jenifer Niles and the Game Programming Gems 6 editor\nMichael Dickheiser for giving me the opportunity and enjoyable burden of editing\nthis volume. I also need to thank my section editors, both returning veterans and\neager newcomers. Without these people and the years of experience they bring, I\nwould have been overwhelmed by the fantastic articles supplied by our gems authors,\nmany well outside my areas of expertise. I cannot overstate how much of their hard\nwork is directly reflected in these pages.\nAdditionally, I would like to thank for their patience and efforts working with me\nto bring this book and accompanying CD-ROM together Emi Smith from Cengage,\nKezia Endsley, Brandon Penticuff, and the unmet people undoubtedly supporting\nthem.\nMy wife Veronica Noechel needs to be acknowledged for her understanding each\ntime another night went by with cage cleanings postponed, TVs asked to be turned\ndown, and cooking duties unshared. My parents should be thanked for so much as\nwell, but I’ll specially call out all the times my father brought home for my use his\nwork PC. Honestly, they really used to be quite expensive, heavy, and require multiple\ntrips across the parking lot to the car!\n",
      "content_length": 1244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "xvii\nContributor Bios\nContains bios for those contributors who submitted one.\nDmitry Andreev\nDmitry is a software engineer specializing in 3D graphics and tools at 10Tacle Studios,\nBelgium. Previously, he was a lead programmer at Burut CT/World Forge, the\nfounder of the X-Tend gaming technology used in numerous releases, including the\nrecent UberSoldier and Sparta: Ancient Wars. Dmitry started programming on ZX-\nSpectrum about 17 years ago. He is a well known demomaker and a two-time winner\nin 64K intro competition at the Assembly demo party. He has a B.S. in Applied\nMathematics and Mechanics.\nHyun-jik Bae\nAfter Hyun-jik Bae developed the Speed Game (see his Bio in Game Programming\nGems 5), he developed Boom Boom Car (similar to Rally-X) by using Turbo Pascal\nwhen he was 11. Boom Boom Car was also mentioned by the hero actor in the movie\nWho Are You?” (from a love story of a gamer and a game developer). He is now a direc-\ntor with Dyson Interactive Inc. and developing another online game title. His major\ninterests include designing and implementing high-performance game servers, scal-\nable database applications, realistic renderers, and physics simulators, as well as piano,\ngolf, and touring with his wife and son.\nTony Barrera\nTony Barrera is an autodidact mathematician and computer graphics researcher. He\nspecializes in algorithms for performing efficient mathematical calculations, especially\nin connection with computer graphics. He published his first paper “An Integer Based\nSquare-Root Algorithm” in BIT 1993 and has since published more than 20 papers.\nHe has worked as a consultant for several companies in computer graphics and related\nfields. Currently, he is developing computationally efficient basic graphics algorithms\ntogether with Ewert Bengtsson and Anders Hast.\nAnatoli Beliaev\nAnatoli Beliaev (beliaev@trusoft.com) is a software engineer with more than 15 years\nof diverse development experience. Since 2001, he has been working for TruSoft as\nLead Engineer responsible for the architecture of behavior-capture AI technologies.\nHe is especially focused on adaptive and generic programming approaches, and their\n",
      "content_length": 2152,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "application to constructing highly efficient and flexible software in performance-\ndemanding areas. Mr. Beliaev graduated from Bauman Moscow State Technical\nUniversity with an M.S. in Computer Science.\nEwert Bengtsson \nEwert Bengtsson has been professor of Computerized Image Analysis at Uppsala Uni-\nversity since 1988 and is currently head of the Centre for Image Analysis in Uppsala.\nHis main research interests are to develop methods and tools for biomedical applica-\ntions of image analysis and computer assisted visualization of 3D biomedical images.\nHe is also interested it computationally efficient algorithms in graphics and visualiza-\ntion. He has published about 130 international research papers and supervised about\n30 Ph.D. students. He is a senior member of IEEE and member of the Royal Swedish\nAcademy of Engineering Sciences. \nJacco Bikker\nBikker is a lecturer for the International Architecture and Design course of the Univer-\nsity of Applied Sciences, Breda, the Netherlands. Before that, he worked in the Dutch\ngame industry for 10 years, for companies such as Lost Boys Interactive, Davilex,\nOverloaded PocketMedia, and W!Games. Besides his job, he has written articles on\ntopics such as ray tracing, rasterization, visibility determination, artificial intelligence,\nand game development for developer Websites such as Flipcode.com and Gamasutra.\nBill Budge\nEver since he got his first set of blocks at age 2, Bill Budge has loved building things.\nAt age 15, he discovered computer programming, the greatest set of blocks ever\ninvented. Since then, his life’s work has been to use these “blocks” to build even bet-\nter sets of blocks. Among these have been Bill Budge’s 3D Game Toolkit and Pinball\nConstruction Set. He is currently building game editors in the Tools and Technology\nGroup at Sony Computer Entertainment, America.\nJoaquim Bento Cavalcante-Neto\nJoaquim Bento Cavalcante-Neto is a professor of Computer Graphics in the Depart-\nment of Computing at the Federal University of Ceará (UFC) in Brazil. He received\nhis Ph.D. in Civil Engineering from the Pontifical Catholic University of Rio de\nJaneiro (PUC-Rio), Brazil, in 1998. While pursuing his Ph.D., he spent a year work-\ning with Computer Graphics/Civil Engineering at Cornell University. He was also a\npost-doctoral research associate at Cornell University from 2002 to 2003. During\nboth his Ph.D. and post-doc, he worked with applied computer graphics. His current\nresearch interests are computer graphics, virtual reality, and computer animation. He\nxviii\nContributor Bios\n",
      "content_length": 2565,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "also works with numerical methods, computational geometry, and computational\nmechanics. He has been the coordinator of several government-sponsored projects\nand the coordinator of the graduate program (master’s and Ph.D. programs) in com-\nputer science at the Federal University of Ceará (UFC).\nMichael Dawe\nMichael’s route to joining the games industry after college included three years in the\nconsulting industry, two cross-country moves, and one summer devoted entirely to \nfinishing his thesis. After earning a B.S. in Computer Science and a B.S. in Philosophy\nfrom Rensselaer Polytechnic Institute, Michael went on to earn an M.S. in Computer\nScience from DigiPen Institute of Technology while cutting his teeth in the industry at\nAmaze Entertainment. Michael is currently employed as an artificial intelligence and\ngameplay programmer at Big Huge Games.\nRobert (Kirk) DeLisle\nRobert (Kirk) DeLisle started programming in the early 1980s and has always had an\ninterest in artificial intelligence, numerical analysis, and algorithms. During graduate\nschool, he developed applications for his laboratory that were used for analysis of\nmolecular biological data and were distributed internationally. Currently, he works as\na Computational Chemist developing and applying artificial intelligence methods to\ncomputer-aided drug design and cheminformatics. He is author of a number publica-\ntions and is named as co-inventor of various patents in the fields of computational\nchemistry and drug development.\nMichael Delp\nMichael is the Lead Artificial Intelligence Engineer at WXP Inc. in Seattle where he\nbuilt an FPS AI from scratch in four months, which won critical acclaim. He’s been\nan AI, physics, and gameplay software engineer throughout his career, including work\non FPS, sports, and vehicle AI at small companies, like his current one, as well as large\nones like EA and Sega. He has also lectured at the Game Developers Conference and\ntaught an AI course for the University of Washington Extension. He earned his Com-\nputer Science degree at UC Berkeley.\nCarlos Dietrich\nCarlos Augusto Dietrich received a B.S. in Computer Science from the Federal Univer-\nsity of Santa Maria, Brazil, and an M.S. in Computer Science from the Federal Univer-\nsity of Rio Grande do Sul, Brazil. His research interests include graphics, visualization,\nand the use of GPUs as general purpose processors. He is currently a third-year Ph.D.\nstudent working in the Computer Graphics Group at the Federal University of Rio\nGrande do Sul, Brazil.\nContributor Bios\nxix\n",
      "content_length": 2553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "João Dihl\nJoão Luiz Dihl Comba received a B.S. in Computer Science from the Federal Univer-\nsity of Rio Grande do Sul, Brazil, and an M.S. in Computer Science from the Federal\nUniversity of Rio de Janeiro, Brazil. After that, he received a Ph.D. in Computer Sci-\nence from Stanford University. He is an associate professor of computer science at the\nFederal University of Rio Grande do Sul, Brazil. His main research interests are in\ngraphics, visualization, spatial data structures, and applied computational geometry.\nHis current projects include the development of algorithms for large-scale scientific\nvisualization, data structures for point-based modeling and rendering, and general-\npurpose computing using graphics hardware. He is a member of the ACM\nSIGGRAPH. \nPriyesh N. Dixit\nPriyesh N. Dixit is a games researcher in the Department of Computer Science at The\nUniversity of North Carolina at Charlotte and part of the Game Intelligence Group\n(playground.uncc.edu). His primary focus these days is in contributing to the devel-\nopment of the Common Games Understanding and Learning (CGUL) toolkit. He is\nthe designer and developer of the CGUL PlayerViz and HIIVVE tools. He received\nhis Bachelor of Science in Computer Science from UNC Charlotte and will complete\nhis master’s degree in Spring 2008 with the Game Design and Development certifi-\ncate. His areas of interest are in learning and understanding from playtesting, building\ntools to support interactive artificial intelligence, and working on all types of com-\nputer games.\nJoshua A. Doss\nJoshua Doss started his career at 3Dlabs in the Developer Relations division creating\nseveral tools and samples for the professional graphics developer community. His soft-\nware contributions include ShaderGen, an Open Source application that dynamically\ncreates programmable shaders to emulate most fixed-function OpenGL as well as\nseveral of the first high-level GLSL shaders. Joshua is currently at Intel Corporation\nworking with the Advanced Visual Computing team to create world-class graphics\ntools and samples for game developers.\nNathan Fabian\nNathan is a veteran hobbyist game developer, with 12 years’ experience working\nsatellite projects for Sandia National Labs. He’s often thought about joining the game\nindustry proper, but never taken the plunge. He has tinkered with various game\ntechnologies for over 20 years and cannot decide whether he prefers graphics special\neffects, physics simulation, or artificial intelligence. In the end, he’d really like to make\na game involving 3D sound localization as a key element. Until then, he’s finishing his\nmaster’s degree in Computer Science at the University of New Mexico.\nxx\nContributor Bios\n",
      "content_length": 2712,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "Marcus Aurelius Cordenunsi Farias\nMarcus Aurelius Cordenunsi Farias graduated with a degree in Computer Science from\nUniversidade Federal de Santa Maria (UFSM), Brazil in 2004. He obtained his mas-\nter’s degree in Computer Science (Computer Graphics) from Universidade Federal do\nRio Grande do Sul, Brazil (UFRGS), in 2006. Last year, he worked in developing new\ninteraction techniques for casual games for Zupple Games, a startup enterprise in\nBrazil. He has experience in the development of new interaction modalities for games,\nincluding computer vision and noise-detection techniques. Currently, he is working at\nCWI Software company in Porto Alegre, Rio Grande do Sul, Brazil.\nMark France\nMark recently graduated with a B.S. in Computer Games Technology. He is also a co-\nfounder of the independent game development studio “Raccoon Games.”\nBen Garney\nBen Garney has been working at GarageGames since it was eight guys in a tiny two-\nroom office. He sat in the hallway and documented the Torque Game Engine. Since\nthen, he’s done a lot with the Torque family of engines, working on graphics, net-\nworking, scripting, and physics. He’s also helped out on nearly every game from\nGG—most notably Marble Blast Ultra and Zap. More recently, he’s re-learning Flash\nand PHP for an avatar-creation Website. In his spare time, Ben plays piano, climbs\nbuttes, and finds ways to survive living with his fuzzy kitty, Tiffany.\nJulien Hamaide\nJulien started programming a text game on his Commodore 64 at the age of eight.\nHis first assembly programs followed soon after. He has always been self-taught, read-\ning all of the books his parents were able to buy. He graduated four years ago at the\nage of 21 as a Multimedia Electrical Engineer at the Faculté Polytechnique de Mons\nin Belgium. After two years working on speech and image processing at TCTS/Multi-\ntel, he is now working as lead programmer on next-generation consoles at 10Tacle\nStudios Belgium/Elsewhere Entertainment. Julien has contributed several articles to\nthe Game Programming Gems and AI Game Programming Wisdom series.\nAnders Hast\nAnders Hast has been a lecturer in computer science at the University of Gävle since\n1996. In 2004, he received his Ph.D. from Uppsala University based on a thesis about\ncomputationally efficient fundamental algorithms for computer graphics. He has\npublished more than 20 papers in that field. He is currently working part time as\nVisualization Expert at Uppsala Multidisciplinary Center for Advanced Computa-\ntional Science.\nContributor Bios\nxxi\n",
      "content_length": 2540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "Jeremy Hayes\nJeremy Hayes is a software engineer in the Advanced Visual Computing group at\nIntel. Before he joined Intel, Jeremy was part of the Developer Relations group at\n3Dlabs. His research interests include procedural content generation (especially ter-\nrain), independent game design, and Dodge Vipers.\nScott Jacobs\nScott Jacobs has been working in the games industry since 1995. Currently, he is a\nSenior Software Engineer at Destineer. Prior to this he worked as a software engineer at\nthe serious games company Virtual Heroes, two Ubisoft studios including Redstorm\nEntertainment, and began in the game development industry at Interactive Magic. He\nalso served as the Network & Multiplayer section editor for Game Programming Gems\n6. He lives in North Carolina with his wife and a house full of creatures. \nThomas Jahn\nThomas Jahn has been studying multimedia engineering at the Hochschule Bremen\nsince 2002. In addition to his studies, he began working for KING Art Entertainment\nin 2005, where he contributed to the development of a turn-based strategy game.\nIntrigued by the tiling characteristics of hexagons, he dedicated his final thesis to\nhexagonal grids. After receiving his degree in Computer Science in the summer of\n2007, Thomas was offered a full-time position at KING Art, Bremen, where he is cur-\nrently working on the adventure game Black Mirror 2.\nAlberto Jaspe\nAlberto Jaspe was born in 1981 in La Coruña, Spain. His interest in computer graph-\nics started with the demoscene movement when he was 15. During his studies of\ncomputer science at the University of La Coruña, he was developing 3D medical\nimage visualization systems at the RNASA-Lab. Since 2003, he has been working as\nsoftware engineer and researcher at the computer graphics group VideaLAB, taking\npart in different projects and publications, from terrain visualization to virtual reality\nand rendering.\nMark Jawad\nMark began programming when he was in the second grade, and never bothered to\nstop. He began his career in the videogame industry when he was 21, and soon spe-\ncialized in Nintendo’s hardware and software platforms. He spent eight years in Los\nAngeles writing many games, tools, and engines for systems like the N64, Game Boy\nAdvance, GAMECUBE, and Nintendo DS. Mark moved to Redmond in 2005 to\njoin Nintendo of America’s developer support group, where he currently serves as the\nteam’s Technical Leader. In his spare time, he enjoys studying compilers and runtime\nsystems, and spending time with his family.\nxxii\nContributor Bios\n",
      "content_length": 2539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "Krzysztof Kluczek\nKrzysztof was interested in game programming since he was 10. As 3D technology in\ngames began evolving, he became more and more interested in the graphical aspect of\n3D games. He received his master’s degree in Computer Science at Gdansk University\nof Technology. Currently, he is pursuing his Ph.D. degree there, enjoying learning\nnew technologies, making games, and being a part of the gamedev.pl community in\nhis free time.\nDavid L. Koenig\nDavid L. Koenig is a Senior Software Engineer with The Whole Experience, Inc. in\nSeattle. His main focus is on network and resource loading code. He serves as an\ninstructor for the networking and multiplayer programming course at the University\nof Washington. David is a returning author to the Game Programming Gems series.\nHis work has contributed to many titles, including SceneIt? Lights, Camera, Action\n(Xbox 360), Greg Hastings’ Tournament Paintball Max’d (PS2), Tron 2.0 (PC), Chicago\nEnforcer (Xbox), and many more titles. He holds a bachelor’s degree in Computer\nEngineering Technology from the University of Houston. His personal Website is\nhttp://www.rancidmeat.com.\nAdam Lake\nAdam Lake is a Graphics Software Architect in the Software Solutions Group leading\ndevelopment of a Graphics SDK at Intel. Adam has held a number of positions dur-\ning his nine years at Intel, including research in non-photorealistic rendering and\ndelivering the shockwave3D engine. He has designed a stream-programming archi-\ntecture that included the design and implementation of simulators, assemblers, com-\npilers, and programming models. Previous to working at Intel, he obtained an M.S. in\ncomputer graphics at UNC-Chapel Hill and worked in the Computational Science\nmethods group at Los Alamos National Laboratory. More information is available at\nwww.cs.unc.edu/~lake/vitae.html. He has several publications in computer graphics,\nand has reviewed papers for SIGGRAPH, IEEE, and several book chapters on com-\nputer graphics, and has over 35 patents or pending patents in computer graphics and\ncomputer architecture. In his spare time he is a mountain biker, road cyclist, hiker,\ncamper, avid reader, snowboarder, and Sunday driver.\nDimitar Lazarov\nDimitar Lazarov is a senior software engineer at Luxoflux (an Activision owned stu-\ndio). He has more than 10 years of experience in the game industry and has worked\non a variety of games, ranging from children-oriented titles such as Tyco RC, Casper,\nand Kung Fu Panda, to more mature titles such as Medal of Honor and True Crime. He\nconsiders himself a generalist programmer with passion for graphics, special effects,\nContributor Bios\nxxiii\n",
      "content_length": 2648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "animations, system libraries, low-level programming, and optimizations. He also has\na recurring dream of writing his own programming language one day. In his spare\ntime, Dimitar likes to read books, travel, play sports, and relax on the beach with his\nwife.\nMartin Linklater\nMartin Linklater is currently a lead programmer at SCEE Liverpool (UK). He has 14\nyears of experience in the games industry and has worked on many titles, including\nWipeout HD, Wipeout Pure, and Quantum Redshift.\nChris Lomont\nChris Lomont, http://www.lomont.org, is a research scientist working on government\nsponsored projects at Cybernet Systems Corporation in Ann Arbor. He is currently\nresearching image processing for NASA and designing hardware/software to prevent\nmalware from infecting PCs for Homeland Security. At Cybernet, he has worked in\nquantum computing, has taught advanced C++ programming, and has unleashed\nchaos. Chris specializes in algorithms and mathematics, rendering, computer security,\nand high-performance scientific computing. After obtaining a triple B.S. in math,\nphysics, and computer science, he spent several years programming in Chicago. He\neventually left the video game company in Chicago for graduate school at Purdue,\nWest Lafayette, resulting in a Ph.D. in mathematics, with additional graduate course-\nwork in computer science. His hobbies include building gadgets (http://www.hyp-\nnocube.com), playing sports, hiking, biking, kayaking, reading, learning math and\nphysics, traveling, watching movies with his wife Melissa, and dropping superballs on\nunsuspecting coworkers. He has two previous gems.\nJörn Loviscach\nJörn Loviscach has been a professor of computer graphics, animation, and simulation\nat Hochschule Bremen (University of Applied Sciences) since 2000. Before that, he\nwas deputy editor-in-chief at c’t computer magazine in Hanover, Germany. Jörn also\npossesses a Ph.D. in Physics. Since his return to academia he has contributed to GPU\nGems, Shader X3, Shader X5, and Game Programming Gems 6. Additionally, he has\nauthored and co-authored a number of works on computer graphics and interactive\ntechniques presented at conferences such as Eurographics and SIGGRAPH. \nMichael F. Lynch, Ph.D.\nDr. Lynch’s background is in Electrical Engineering where he worked in various tech\njobs before returning to grad school at an advanced age, where he crossed the great\nchasm over to the social sciences. Today he is a member of the faculty for the new\nGames and Simulations Arts and Sciences (GSAS) major at Rensselaer Polytechnic\nxxiv\nContributor Bios\n",
      "content_length": 2569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "Institute in Troy, New York, where he teaches “A History and Culture of Games” and\na forthcoming course in AI for games. In his spare time, he does a little gourmet cook-\ning, plays strange electronic music, and occasionally samples the Interactive Story-\ntelling kool-aid (and plays games, too).\nJosé Gilvan Rodrigues Maia\nJosé Gilvan Rodrigues Maia is a doctoral student in the Department of Computing at\nthe Federal University of Ceará (UFC) in Brazil. He received a B.S. and an M.S. in\nComputer Science from the Department of Computing at the Federal University of\nCeará (UFC) in Brazil. During his M.S., he spent two years working with computer\ngame technologies, specially rendering and collision detection. His current research\ninterests are computer graphics, computer games, and computer vision. He has been\nworking on research projects at the Federal University of Ceará (UFC).\nJoris Mans\nJoris Mans has a master’s in Computer Science from the Free University, Brussels.\nCombining the knowledge gathered during his education and the hands-on experience\nas a developer in the demo-scene allowed him to start working in the games industry\ndirectly after graduating. He currently works as a lead programmer at 10Tacle Studios\nBelgium, poking around with stuff ranging from console data cache optimization to\ndatabase backends.\nEnric Martí\nEnric Martí joined the Computer Science department at the Universitat Autònoma\nde Barcelona (UAB) in 1986. In 1991, he received a Ph.D. at the UAB with a thesis\nabout the analysis of handmade line drawings as 3D objects. That same year he also\nbecame an associate professor. Currently, he is a researcher at the Computer Vision\nCenter (CVC) and is interested in document analysis, and more specifically, Web\ndocument analysis, graphics recognition, computer graphics, mixed reality and\nhuman computer interaction. He is working on text segmentation in low-resolution\nWeb images (for example, CIF and JPEG) using wavelets to extract text information\nfrom Web pages. Additionally, he is working on the development of a mixed-reality\nenvironment with 3D interfaces (data glove and optical lenses). He is also the\nreviewer of the Computer & Graphics and Electronic Letters on Computer Vision and\nImage Analysis (ELCVIA) journals.\nColt McAnlis\nColt “MainRoach” McAnlis is a graphics programmer at Microsoft Ensemble Studios\nspecializing in rendering techniques and systems programming. He is also an adjunct\nprofessor at SMU’s Guildhall school of game development where he teaches advanced\nContributor Bios\nxxv\n",
      "content_length": 2552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "rendering and mathematics courses. After receiving an advanced degree from the\nAdvanced Technologies Academy in Las Vegas Nevada, Colt earned his B.S. in Com-\nputer Science from Texas Christian University.\nCurtiss Murphy\nCurtiss Murphy has been developing and managing software projects for 15 years. As a\nproject engineer, he leads the design and development of several Serious Game efforts\nfor a variety of Marine, Navy, and Joint government and military organizations. Cur-\ntiss routinely speaks at conferences with the intent to help the government leverage\nlow-cost, game-based technologies to enhance training. Recent game efforts include a\ndozen projects based on the Open Source gaming engine, Delta3D (www.delta3d.org)\nand a public affairs game for the Office of Naval Research based on America’s Army.\nCurtiss holds a B.S. in Computer Science from Virginia Polytechnic University and\ncurrently works for Alion Science and Technology in Norfolk, Virginia.\nLuciana Nedel\nLuciana Porcher Nedel received a Ph.D. in Computer Science from the Swiss Federal\nInstitute of Technology in Lausanne, Switzerland, under the supervision of Prof.\nDaniel Thalmann in 1998. She received an M.S. in Computer Science from the Fed-\neral University of Rio Grande do Sul, Brazil, and a B.S. in Computer Science from the\nPontifical Catholic University, Brazil. During her sabbatical in 2005, she spent two\nmonths at the Université Paul Sabatier in Toulouse, France, and two months at the\nUniversité Catholique de Louvain in Louvain-la-Neuve, Belgium, doing research on\ninteraction. She is an assistant professor at the Federal University of Rio Grande do\nSul, Brazil. Since 1991 she has been involved in computer animation research and\nsince 1996 she has been doing research in virtual reality. Her current projects include\ndeformation methods, virtual humans simulation, interactive animation, and 3D\ninteraction using virtual reality devices. \nKen Noland\nKen Noland is a programmer at Whatif Productions and has worked on several inter-\nnal projects that extend the technology and the infrastructure of the proprietary game\nengine. His focus tends to be on audio, networking, and gameplay, and he also works\non the code behind the elusive content-driven technology that drives the Whatif\nengine, also known as the WorLd Processor. He’s been involved in the games industry\nin one form or another for over six years. When not actively working, you can gener-\nally find him walking around town in a daze wondering what to do with this thing\ncalled “free time.”\nxxvi\nContributor Bios\n",
      "content_length": 2569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "Jason Page\nJason Page has worked in the computer games industry since 1988 and has held job\npositions of games programmer, audio programmer, “musician” (audio engineer/\ncontent creator), and currently is the Audio Manager at Sony Computer Entertain-\nment Europe’s R&D division. This role includes managing, designing, and program-\nming various parts of the PlayStation 3 audio SDK libraries (the PS3 audio library\n“MultiStream” is used by many developers worldwide), as well as supporting develop-\ners on all PlayStation platforms with any audio issues. Of course, none of this would\nbe possible without the hard work of his staff—so thanks also to them. Jason’s past\nwork as a audio content creator includes music and sound effects for titles such as\nRainbow Islands, The Chaos Engine, Sensible World of Soccer, Cool Boarders 2, and Gran\nTurismo. Although he no longer creates music for games, he still writes various pieces\nfor events such as the SCEE DevStation conference. Finally, he would like to say\nthank you to his wife, Emma, for putting up with his constant “I’ve just got to answer\na developer’s email” when at home. Jason personally still blames the invention of\nlaptops and WiFi.\nVitor Fernando Pamplona\nVitor Fernando Pamplona received his B.S. in Computer Science from Fundação\nUniversidade Regional de Blumenau. Since 2006 he has been a Ph.D. student at Uni-\nversidade Federal do Rio Grande do Sul, Brazil. His research interests are in computer\ngraphics, agile development, and free software. He also manages the java virtual com-\nmunity called JavaFree.org and leads seven free software projects.\nSteve Rabin\nSteve is a Principal Software Engineer at Nintendo of America, where he researches\nnew techniques for Nintendo’s next generation systems, develops tools, and supports\nNintendo developers. Before Nintendo, Steve worked primarily as an AI engineer at\nseveral Seattle start-ups including Gas Powered Games, WizBang Software Produc-\ntions, and Surreal Software. He managed and edited the AI Game Programming Wis-\ndom series of books, the book Introduction to Game Development, and has over a dozen\narticles published in the Game Programming Gems series. He’s spoken at the Game\nDevelopers Conference and currently moderates the AI roundtables. Steve is an\ninstructor for the Game Development Certificate Program through the University of\nWashington Extension and is also an instructor at the DigiPen Institute of Technol-\nogy. Steve earned a B.S. in Computer Engineering and an M.S. in Computer Science,\nboth from the University of Washington.\nContributor Bios\nxxvii\n",
      "content_length": 2591,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "Arnau Ramisa\nArnau Ramisa graduated with a degree in Computer Science from the Universitat\nAutònoma de Barcelona and is now working on a Ph.D. in Computer Vision and\nArtificial Intelligence at the Institut d’Investigació in Intelligència Artificial. He is\ninterested in computer vision, augmented reality, human-machine interfaces and, of\ncourse, video games.\nMike Ramsey\nMike Ramsey is the principle programmer and scientist on the GLR-Cognition\nEngine. After earning a B.S. in Computer Science from MSCD, Mike has gone on to\ndevelop core technologies for the Xbox 360 and PC. He has shipped a variety of\ngames, including Men of Valor (Xbox and PC), Master of the Empire, and several Zoo\nTycoon 2 products, among others. He has also contributed multiple gems to both the\nGame Programming Gems and AI Wisdom series. Mike’s publications can be found at\nhttp://www.masterempire.com. He also has an upcoming book entitled, A Practical\nCognitive Engine for AI. In his spare time, Mike enjoys strawberry and blueberry pick-\ning and playing badminton with his awesome daughter Gwynn!\nGraham Rhodes\nGraham Rhodes is a principal scientist at the Southeast Division of Applied Research\nAssociates, Inc., in Raleigh, North Carolina. Graham was the lead software developer\nfor a variety of Serious Games projects, including a series of sponsored educational\nmini-games for the World Book Multimedia Encyclopedia; and more recently,\nfirst/third-person action/role-playing games for industrial safety and humanitarian\ndemining training. He is currently involved in developing software that provides pro-\ncedural modeling and physics-based solutions for simulation and training. Graham\ncontributed articles to several books in the Game Programming Gems series, and\nauthored the section on real-time physics for Introduction to Game Development. He \nis the moderator and frequent contributor to the math and physics section at\ngamedev.net, has presented at the annual Game Developer’s Conference (GDC) and\nother industry events, and regularly attends GDC and the annual ACM/SIGGRAPH\nconference. He is a member of ACM/SIGGRAPH, the International Game Devel-\noper’s Association (IGDA), and the North Carolina Advanced Learning Technologies\nAssociation (NC ALTA).\nTimothy E. Roden\nTimothy Roden is an associate professor and head of the Department of Computer\nScience at Angelo State University in San Angelo, Texas. He teaches courses in game\ndevelopment, computer graphics, and programming. His research interests include\nxxviii\nContributor Bios\n",
      "content_length": 2530,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "entertainment computing with a focus on procedural content creation. His published\npapers in entertainment computing appear in ACM Computers in Entertainment,\nInternational Conference on Advances in Entertainment Technology, the International\nConference on Entertainment Computing and Microsoft Academic Days on Game\nDevelopment Conference. He also contributed to Game Programming Gems 5. Before\njoining academia, Roden spent 10 years as a graphics software developer in the simu-\nlation industry.\nRahul Sathe\nRahul P. Sathe is working as a software engineer in the Advanced Visual Computing\nGroup at Intel Corp where he is developing an SDK for next generation high-end\ngraphics hardware. He’s currently involved in designing advanced graphics algorithms\nfor game developers. Earlier in his career, he worked on various aspects of CPU archi-\ntecture and design. He holds a bachelor’s degree from Mumbai University (1997) in\nElectronics Engineering and his master’s in Computer Engineering from Clemson\nUniversity (1999). He has prior publications in the computer architecture area. His\ncurrent interests include graphics, mathematics, and computer architecture.\nStephan Schütze\nStephan Schütze currently resides in Tokyo where he is working in both the anime\nand game industries while trying to learn both the Japanese language and its culture.\nSee Stephan@stephanschutze.com and www.stephanschutze.com.\nAntonio Seoane\nAntonio Seoane is a researcher who works in “VideaLAB” (http://videalab.udc.es), the\nVisualization For Engineering, Architecture, and Urban Design Group of the Univer-\nsity of A Coruña, in Spain. He has been working for 10 years in the research and devel-\nopment of real-time graphics applications focused on topics like simulation, civil\nengineering, urban design, cultural heritage, terrain visualization, and virtual reality.\nMain research interests and experiences are terrain visualization (http://videalab.udc.\nes/santi/), and in recent years working on GPGPU and Artificial Intelligence.\nDillon Sharlet\nDillon Sharlet is an undergraduate student in Mathematics and Electrical Engineer-\ning at the University of Colorado at Boulder. He has been interested in computer\ngraphics for years. He likes exploring new algorithms in graphics. In his free time, he\nlikes to go climbing and skiing.\nContributor Bios\nxxix\n",
      "content_length": 2337,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "Gary Snethen\nGary’s passion for computing began before he had access to a computer. He taught\nhimself to program from examples in computer magazines, and wrote and “ran” his\nfirst programs using pencil and paper. When his parents balked at buying a computer,\nGary set out to create his own. When his parents discovered hand-drawn schematics\nfor functional digital adders and multipliers, they decided it was more than a passing\ninterest and purchased his first computer. Gary’s early interests were focused heavily\non games and 3D graphics. Gary wrote his first wireframe renderer at age 12, and\nspent many late nights throughout his teens writing one-on-one modem games to\nshare with his friends.\nGary is currently a principal programmer at Crystal Dynamics, where he shipped\nLegacy of Kain: Defiance and Tomb Raider Legend. Gary’s current professional interests\ninclude constrained dynamics, advanced collision detection, physics-based anima-\ntion, and techniques for improving character fidelity in games.\nRobert Sparks\nThis is Robert’s seventh year programming audio in the games industry. Most recently\nhe was a technical lead on Scarface: The World Is Yours (Radical Entertainment, Van-\ncouver, Canada). His past game credits include The Simpsons: Hit & Run, The Simp-\nsons: Road Rage, The Hulk, Tetris Worlds, Dark Angel, and Monsters, Inc.\nDiana Stelmack\nDiana Stelmack works at Red Storm Entertainment, where she has worked on the\nGhost Recon series since Spring 2001. Since the PC version of this title, Xbox Live\nOnline services and multiplayer game system support have become her development\nfocus. Prior to games, her network background included telecommunications, IP\nSecurity, and DoD Network communications. Just remember “If you stumble, make\nit look like part of the dance”!\nJavier Taibo\nJavier Taibo graduated in computer science at the University of Coruña in 1998. He\nhas worked on computer graphics R&D projects in the “Visualization for Engineer-\ning, Architecture and Urban Design Group” (a.k.a. VideaLAB). The fields he has\nbeen working on include real-time 3D terrain rendering and GIS visualization, virtual\nreality, and panoramic image and video. At present he is also teaching computer ani-\nmation and 3D interaction at the University of A Coruña.\nxxx\nContributor Bios\n",
      "content_length": 2298,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "Daniela Gorski Trevisan\nDaniela Gorski Trevisan graduated with a degree in Informatics from Universidade\nFederal de Santa Maria (UFSM), Brazil, in 1997. She obtained her master’s degree in\nComputer Science (Computer Graphics) from Universidade Federal do Rio Grande do\nSul (UFRGS), Porto Alegre, Brazil, in 2000. She earned her Ph.D. degree in applied\nsciences from Université catholique de Louvain (UcL), Belgium, in 2006. Nowadays\nshe is a CNPq researcher and member of the Computer Graphics group of Informatics\nInstitute at UFRGS. Her research interest topics are focused on the development and\nevaluation of alternative interaction systems, including multimodal, augmented, and\nmixed reality technologies.\nIskander Umarov\nIskander Umarov (umarov@trusoft.com) is the technical director of TruSoft (www.\ntrusoft.com). TruSoft develops behavior-capture AI technologies and provides AI\nmiddleware and consulting services to other companies to implement innovative AI\nsolutions for computer and video games and simulation applications on PC, PlaySta-\ntion 2, PlayStation 3, and Xbox 360 platforms. Mr. Umarov authored the original\nideas behind TruSoft’s Artificial Contender AI technology. Artificial Contender pro-\nvides behavior-capture AI agents for games and simulation applications. These AI\nagents capture human behavior, learn, and adapt. Mr. Umarov is currently responsible\nfor managing development of TruSoft’s AI solutions and leads TruSoft’s R&D efforts,\nincluding joint projects with Sony, Electronic Arts, and Lockheed Martin. Mr. Umarov\nholds a B.S. in Applied Mathematics and an M.S. in Computer Science, both from\nMoscow Technological University, where he specialized in instance-based learning\nmethods and graph theory.\nEnric Vergara\nEnric Vergara is a computer engineer at the Universitat Autònoma de Barcelona and\nrecently received a master’s degree in Video Games Creation at Pompeu Fabra Univer-\nsity. He now works as a C++ programmer for GeoVirtual.\nCreto Augusto Vidal\nCreto Augusto Vidal is an associate professor of computer graphics in the Department\nof Computing at the Federal University of Ceará in Brazil. He received his Ph.D. in\ncivil engineering from the University of Illinois at Urbana-Champaign in 1992 and\nwas a post-doctoral research associate at the Department of Mechanical and Industrial\nEngineering at the University of Illinois at Urbana-Champaign from 1992 to 1994.\nHe is currently a visiting researcher at VRLab at EPFL (École Polytechnique Fédérale\nContributor Bios\nxxxi\n",
      "content_length": 2517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "de Lausanne) in Switzerland. His current research interests are computer graphics, vir-\ntual reality applications, and computer animation. He has been the coordinator of\nseveral government-sponsored projects investigating the use of networked virtual\nenvironments for training and education.\nJon Watte\nIn addition to his job as CTO of serious virtual world platform company Forterra\nSystems, Jon is a regular contributor to the independent game development commu-\nnity. As a Microsoft DirectX/XNA MVP and moderator of the Multiplayer and Net-\nworking forum on the independent games site GameDev.Net, he enjoys sharing\nexperience from his time in the business, with his earliest shipping title a cassette-\nbased game for the Commodore VIC 20. Prior to heading up Forterra Systems, Jon\nworked on products such as There.com, BeOS, and Metrowerks CodeWarrior.\nG. Michael Youngblood\nG. Michael Youngblood, Ph.D., is an Assistant Professor in the Department of Com-\nputer Science at The University of North Carolina at Charlotte, co-director of the Games\n+ Learning Lab, and head of the Games Intelligence Group (playground.uncc.edu). His\nwork studies how artificial agents and real people interact in virtual environments,\nincluding computer games and high-fidelity simulations in order to understand the ele-\nments and patterns of learning for the development of better artificial agents. He has\nworked with real-time computer games, intelligent environments, and robotics since\n1997. His research interests are in interactive artificial intelligence, entertainment com-\nputing, and intelligent systems.\nxxxii\nContributor Bios\n",
      "content_length": 1623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "1\nS E C T I O N\n1\nGENERAL\nPROGRAMMING\n",
      "content_length": 38,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "3\nIntroduction\nAdam Lake, Graphics Software Architect,\nAdvanced Visual Computing Group (AVC), Intel\nadam.t.lake@intel.com\nG\name development continues to undergo significant changes in every aspect. Extract-\ning performance in previous generations of hardware meant a focus on scheduling\nassembly instructions, exploiting small vector instructions (SSE, for example), and\nensuring data remains in registers or cache while processing. Although these issues are\nstill relevant, they are now secondary relative to exploiting the multithreaded hardware\nin current generation consoles and PCs. To this end, we have included articles on tools\nand techniques for game programmers to take advantage of this new hardware: Design\nand Implementation of a Multi-Platform Threading Engine by Michael Ramsey, an imple-\nmentation of a Multithread Job and Dependency System by Julien Hamaide, and a\nDeferred Function Call Invocation System by Mark Jawad. These issues have become\nmore prevalent in the time since the last Game Programming Gems publication.\nIn the category of systems we have three articles. Two systems software articles\ninclude High Performance Heap Allocator by Dimitar Lazarov and Efficient Cache\nReplacement Using the Age and Cost Metrics by Colt McAnlis. Martin Fleisz also brings\nus an article on Advanced Debugging Techniques. Martin covers issues related to excep-\ntion handling, stack overflows, and memory leaks—common issues we all encounter\nduring application development. \nWe also have an exciting set of articles in this edition related to user interfaces for\ngames. Carlos Dietrich et al. have an article on sketch-based interfaces for real-time\nstrategy games. Arnau Ramisa et al. have written an article on optical flow for video\ngames played with a Webcam, and Marcus C. Farias et al, have described a new first-\nperson shooter interface in Foot Navigation Technique for First-Person Shooting Games.\nFinally, a wonderful paper on using hexagonal tiling instead of a traditional square grid\nis presented by Thomas Jahn and Jörn Loviscach entitled For Bees and Gamers: How to\nHandle Hexagonal Tiles.\nEach of these authors brings to the table his or her own unique perspective, per-\nsonality, and vast technical experience. My hope is that you benefit from these articles\nand that these authors inspire you to give back when you too have a gem to share.\nEnjoy.\n",
      "content_length": 2376,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "5\n1.1\nEfficient Cache Replacement\nUsing the Age and Cost\nMetrics\nColt “MainRoach” McAnlis\nMicrosoft Ensemble Studios\ncmcanlis@ensemblestudios.com\nI\nn memory-constrained game environments, custom media caches are used to amplify\nthe amount of data in a scene, while leaving a smaller memory footprint than contain-\ning the entire media in memory at once. The most difficult aspect of using a cache sys-\ntem is identifying the proper victim page to vacate when the cache fills to its upper\nbounds. As cache misses occur, the choice of page-replacement algorithm is essential—\nthis choice is directly linked to the performance and efficiency of hardware memory\nusage for your game. A bad algorithm will often destroy the performance of your title,\nwhereas a well implemented algorithm will enhance the quality of your game by a sig-\nnificant factor, without affecting performance. Popular cache-replacement algorithms,\nsuch as LRU, work well for their intended environment, but often struggle in situations\nthat require more data to make accurate victim page identifications. This gem presents\nthe Age and Cost metrics to be used as values in constructing the cache-replacement\nalgorithm that best fits your game’s needs.\nOverview\nWhen data is requested from main memory, operating systems will pull the data into\na temporary area of memory (called a cache), which can be accessed at a faster speed\nthan main memory. The cache itself is a predefined size, segmented into smaller sets\nof memory called pages. As memory accesses occur, the cache itself can get filled, at\nwhich time the operating system must choose a page from the cache in which to\nreplace with the incoming page data. This occurrence is called a cache miss. When the\namount of needed pages exceeds the size of the cache by a significant amount (usually\n2x or more) a thrash occurs, that is, the entire cache will be dumped in order to make\nroom for the entirety of the incoming data set. Thrashing is considered the worst-case\nscenario for any caching algorithm, and is the focal point of any performance testing\nof cache replacements.\n",
      "content_length": 2100,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "6\nSection 1\nGeneral Programming \nEach time the memory handler has to fetch information from main memory,\nthere is an associated cost involved with it. Reading from the cached memory has a\nsmaller cost, usually due to introduction of an extra memory chip closer to the proces-\nsor itself. So, in determining which page to dump from memory when a cache miss\noccurs, one of the key goals is to pick a page that does not need an active spot in the\ncache. For instance, if you randomly chose a page to evict during the fault, and that\npage happens to be needed immediately after its eviction, you would incur another\nperformance overhead to re-fetch that data from the main memory. The goal is to\ncreate an algorithm that best describes which page is ideal to remove from the cache\nto reduce the amount of cache misses and performance burden. \nThese types of algorithms, called victim page determination or page-replacement\nalgorithms were a hot topic in the 1960s and 1970s, and reached a plateau with the\nintroduction of the Least Recently Used (LRU) algorithm (as well as other working-\nset systems). Since that time, derivations of these algorithms have been generated \nin order to address some of the issues that LRU presents when working in specific\nsubject areas. For example, [O’Neil93] described an offshoot of LRU, called LRU-K,\nwhich was described to work more efficiently in software database systems. Adaptive\nReplacement Cache (ARC) is an algorithm developed by IBM used both in hardware\ncontrollers as well as other popular database systems [Megiddo03]. (See the “Refer-\nences” section at the end of this gem for more information.)\nIn game development, programmers have to deal with caches both on the hard-\nware and on the software level, especially in the game console arena where program-\nmers constantly struggle to increase the amount of content in the game while still\nfitting within memory constraints. Like many other replacement algorithms tailor-\nmade to solve a specific problem, there are common game and graphics systems that\nrequire a replacement system that better resembles the memory patterns and usage\nmodels. This gem describes two cache page metrics that can be interweaved together\nin a way that better fits into the video game development environment.\nCache-Replacement Algorithms\nSince their creation, cache-replacement systems have been an active area of research,\nresulting in a large number of various algorithms custom tuned to solve various\ninstances of the problem space. In order to have a frame of reference, I’ll cover a few\nof the most common algorithms here.\nBelady’s Min (OPT)\nThe most efficient replacement algorithm would always replace the page that would\nnot be needed for the longest period of time since its eviction from the cache. Imple-\nmenting this type of algorithm in a working system would require the foreknowledge\nof system usage, which would be impossible to define. The results of implementing\nOPT in test situations where the inputs over time are known can be used as a bench-\nmark to test other algorithms.\n",
      "content_length": 3071,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "In a multithreaded environment, where there is a producer-consumer architec-\nture between threads, it is possible to get close to OPT using information from the\nproducer thread if the producer thread is multiple frames ahead of the consumer.\nLeast Recently Used (LRU)\nThe LRU algorithm replaces the page that hasn’t been used for the longest amount of\ntime. When a new page is loaded into the cache, data is kept per page that represents\nhow long since the given page has been used. Upon a cache miss, the victim page is\nthen the one that hasn’t been used in the longest time span. LRU does some cool\nthings, but is prone to excessive thrashing. That is, you’ll always have an oldest page in\nthe cache, which means that unless you’re careful, you can actually clear the entire\ncache when workloads are large.\nMost Recently Used (MRU)\nMRU replaces the page that was just replaced. That is, the youngest page in the cache.\nMRU will not replace the entire cache; rather, during heavy thrashing it will always\nchoose to replace the same page. Although not as popular and robust as LRU, MRU\nhas its uses. \nIn [Carmack00], John Carmack lists a nice hybrid system between LRU and\nMRU for texture cache page replacement. In short, it works in the form that you use\nLRU most of the time, until you reach the point that you need to evict an entry every\nframe, at which time you switch to an MRU replacement policy. As mentioned previ-\nously, LRU has the problem that it will potentially thrash your entire cache if not\nenough space is available. Swapping to MRU at the point you would begin to thrash\nthe cache creates a scratch pad in memory for one page, leaving most of the cache\nunharmed. This can be useful if the amount of textures being used between frames is\nrelatively low. When you’re dealing with a situation in which the extra pages needed\nare double the available cache size this method degenerates; this might stall the ren-\ndering process.\nNot Frequently Used (NFU)\nThe not frequently used (NFU) page-replacement algorithm changes the access heuristic\nto keep a running counter of accesses for every page in the cache. Starting at zero, any\npage accessed during the current time interval will increase their counter by one. The\nresult is a numeric qualifier referencing how often a page has been used. To replace a page,\nyou then must look for the page with the lowest counter during the current interval.\nThe most serious problem with this system is that the counter metric does not\nkeep track of access patterns. That is, a page that was used heavily upon load and hasn’t\nbeen used since then can have the same count as a page used every other frame for the\nsame time interval. The information needed to differentiate these two usage patterns\nis not available from a single variable. The Age metric, presented in the next section,\n1.1\nEfficient Cache Replacement Using the Age and Cost Metrics\n7\n",
      "content_length": 2903,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "is a modification of NFU that changes how the counter is represented, so that you can\nderive extra information from it during thrashing. \nFor a more expansive list of algorithms, hit up your favorite search engine with\n“Page Replacement Algorithms,” or dust off your operating systems textbook from\ncollege.\nAge and Cost Metrics\nFor the purpose of illustration, the rest of the gem references a problem facing most\ngames in the current generation of hardware: the design of a custom texture cache. To\ndo this, you will reserve a static one-dimensional array of cache pages into which data\ncan be loaded and unloaded. Assume that you are working with a multi-dimensional\ntexture cache; that is, the data that you’re placing in the cache is of texture origin. The\ncache is multi-dimensional in the sense that multiple textures of various sizes could fit\ninto a single cache page. For example, if the cache page size fits a 256 \u0002 256 texture,\nyou can also support four 64 \u0002 64 textures, 16 32 \u0002 32 textures, and so on, includ-\ning multiples of each size existing in the same page in harmony. \nEven in this simple example, you have already laid the groundwork for standard\nreplacement functions to under-perform. Consider the case of the multi-dimensional\ncache where you need to insert a new 256 \u0002 256 page into the cache when it is\nentirely filled with only 32 \u0002 32 textures. Simple LRU/MRU schemes do not have\nthe required data available to properly calculate which full cache page is the optimal\none to replace and which group of 32 \u0002 32 textures needs to be dumped as the access\npatterns depend greatly on more than the time at which the page was last replaced. To\nthis purpose, a new set of replacement metrics are presented in order to better analyze\nthe best pages to replace when in such a situation. \nThe Age Algorithm\nThe OPT algorithm knows the amount of usage for a page in the cache and replaces\nthe one that will be used the farthest away in time. Most replacement algorithms\nattempt their best to emulate this algorithm with various data access patterns. The Age\nalgorithm emulates this process by keeping a concept of usage over the previous frames\nin order to best predict future usage; that is, you need to keep track of how many times\na page has been accessed in a window of time. To accomplish this task, every page in\nthe cache keeps a 32-bit integer variable that is initialized to 1 (0x00000001) when the\npage first enters the cache.\nEvery frame, all active pages in the cache are bit-shifted left by one bit, signifying\ntheir deprecation over time. If an active page is used in this frame, the least significant\nbit of the Age variable is set to one (1); otherwise, it is set to zero (0). This shifting and\nsetting pattern allows you to keep a usage evaluation for the past 32 frames.\nFor example, a page that is accessed every other frame would have an Age variable\n0xAAAAAAAA (010101010….01), whereas a page that was accessed heavily when it\n8\nSection 1\nGeneral Programming \n",
      "content_length": 2998,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "was first loaded, and has not been used since, would have an Age variable\n0xFFFF0000 (1111…1100000…00).\nTo show how the Age variable evolves over time, consider a page that is used in the\nfirst, third, fourth, and eighth frames over an eight-frame window. The Age variable\nwould change like such:\nframe1 - 00000001 (used)\nframe2 - 00000010 (not used)\nframe3 - 00000101 (used)\nframe4 - 00001011 (used)\nframe5 - 00010110 (not used)\nframe6 - 00101100 (not used)\nframe7 - 01011000 (not used)\nframe8 - 10110001 (used) \nWith this information layout, you can calculate the Age Percentage Cost (APC)\nof the given window. You average the number of frames a page has been used versus\nthe number of pages not used by dividing the number of frames used (ones) by the\ntotal number of frames in the Age variable. This data can be extracted using assembly\nand processor heuristics rather than higher-level code. Although you can represent\nthis data your own way, the APC presented exists as a normalized single value between\n[0, 1], and as described in the “Age and Cost” section, can be used as a scalar against\nother metrics.\nWhen using age to determine the target victim page, you seek to choose the pages\nthat have not been used in a certain time, as well as pages that are not frequently used\nin general. For example, a page that has been accessed every frame in the window will\nhave a 100% APC and will be almost impossible to replace, whereas a page with an\nAPC of 25% will have a higher chance of being replaced. \nWhat I like about Age is that it gives a number of implicit benefits: \n• It biases old textures, forcing textures to prove that they are needed for the scene.\nOnce they prove this fact, they are kept around. Once an APC gets above 50%, it\ngets difficult to release it from the cache. \n• It creates a scratch pad. New textures that haven’t proved they are valuable yet are\nturned into scratch pads. This is a good thing, as new textures can often be tem-\nporary and have a high probability to vanish the next frame \n• It is a modified NRU (not recently used) scheme. Textures that might have been\nvisible a high percentage for a short time could easily shoot up to 50% APC, but\nthen drop out, and slowly work their APC back down over a number of frames.\nAge offers a modified representation of the access variable, and allows extra analy-\nsis, so if the APC > 60%, but the texture hasn’t been used in the most recent X\nframes, you can check for this and eliminate it early. \nThe APC variable as presented so far is powerful, but not without fault—multiple\npages in the cache can have radically different access patterns but have the same APC\nvalue. That is, 0xAAAAAAAA and 0xFFFF0000 have the same usage percentage but\n1.1\nEfficient Cache Replacement Using the Age and Cost Metrics\n9\n",
      "content_length": 2790,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "it’s easy to see that the usage patterns for these two Age variables are dramatically differ-\nent. Subsequent analysis patterns on the binary data in the basic Age variable could\nhelp separate those pages with similar APC values (such as analyzing sub-windows for\nsecondary APC values) but fall into similar problems.\nExpanded Age Algorithm\nThe previously presented Age algorithm makes the assumption that you’d like to keep\nthe memory required to deduce page information fairly low; hence storing a used/\nunused flag per frame over a window of 32 frames. It should be noted, however, that\nin situations where the required page amount is larger by a significant factor, the Age\nalgorithm degenerates just as any others would. If, for example, you had 50 textures\nthat were used every frame, and only 12 cache pages in which to put them, there\nwould not be enough space in the cache for your entire memory footprint at once.\nEvery frame would thrash the entire cache, replacing each page.\nIn this situation, the constant loading/reloading of pages and textures would cause\nevery page to have an Age counter set to 1, and thus would lack any additional infor-\nmation that would be helpful in the identification of specific victim pages. To help\nsolve this problem, the Age variable can store more information per frame than just a\nused/unused bit, and, in fact, store the usage count of a texture instead. So rather than\nstoring a 0/1 in a single 32-bit integer variable, you could store a list of numbers, each\nstoring how often that page was used in the frame. This would resemble a list of [1, 18,\n25, 6, 0, 0,...1] rather than 01001010011..1. This extra information is particularly\nhelpful in the degenerate case, as you now have additional data to assist in the identifi-\ncation of a victim page.\nFor example, consider two pages (TextureA and TextureB) loaded into the cache \nat the same time with TextureA being used on 50% of the objects in the scene and\nTextureB being used on 10%. At this point, both pages would have the same APC\nvalue, although clearly, you can identify that these two textures have dramatically dif-\nferent usage amounts. When a victim page must be found, you must take into account\nthat TextureA, being used a larger amount in the current frame, increases the probabil-\nity that it will be used in the subsequent frame as well. Thus a lower usage texture,\nTextureB, should be replaced instead.\nBy storing this extra data per frame, you make available other statistical analysis\noperations to help identify the best page to evict from the cache:\n• The APC variable can still be derived from the expanded Age algorithm by divid-\ning the number of non-zero frames in the window by the total frames.\n• Finding the page with the MIN usages in a given frame window will identify the\nleast used page in general, which is helpful to identify victims. \n• Using the MAX analysis, you could identify the pages with the most accesses in\nyour window, in order to help avoid dumping them from the cache.\n• Finding the AVG usage in the window is just as easy and derives a second simplis-\ntic variable similar to APC.\n10\nSection 1\nGeneral Programming \n",
      "content_length": 3165,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "Depending on your implementation needs and data formats, either the simple\nAge algorithm or the Expanded Age algorithm are viable. The best idea is to sit down\nand analyze your data to decide which is the most efficient and useful algorithm for\nyour title.\nThe Cost of Doing Replacements\nMost victim page identification algorithms use only a single heuristic. That is, their\nalgorithms are tailor-made to specialize to the access patterns that result in the least\namount of cache page misses. For example, LRU only keeps a pointer to the oldest\npage. However, for a custom software cache there often is a second heuristic that is\ninvolved with a cache miss—the cost of filling the page of the cache with the new data.\nFor most hardware caches this is a constant cost associated with the time it takes the\nmemory handler to access main memory and retrieve the desired data.\nFor your software needs, however, this cost can often fluctuate between pages\nthemselves. Therefore, it would make sense that the victim page determination takes\ninto account the amount of performance hit involved with actually filling a page with\na given chunk of memory. This performance cost (or just cost) can come from a num-\nber of sources. It can be hand-defined by an external data set (for example, an XML\nfile that defines which textures are really used) or it can be defined by the actual cost\nof filling the page.\nConsider that in the previous example, an incoming texture page is generated by\nstreaming it off an optical drive. So, larger textures have a larger performance time\ninvolved with getting them into the cache, because they have more information to be\nstreamed from media, whereas smaller or simpler textures have fractions of that cost.\nIn this situation, it would make a great deal of sense to consider the cost involved with\npotentially replacing a page in memory during victim page determination. If you vic-\ntimize a page that has a high cost associated with it, you can incur unneeded overhead\nin the next few frames if that page is required. If, instead, you eliminate a page that\nhas a lower cost, the performance hit from incorrectly removing it from the cache is\nmuch lower. In a nutshell, factoring in cost as a page-replacement variable allows you\nto answer the question “Is it cheaper to dump five smaller textures to make space for\n1 larger texture?”\nTo review, cost allows you to be concerned with how much a given page will hurt\nperformance to reload it into the cache. If required, an expansion of this system allows\na victim page identification function to be more concerned about performance cost of\na miss, as opposed to coherence between frames. \nBy itself, cost contains the same problems that other cache replacement algorithms\nhave. When a thrash occurs, you find the cheapest texture in the cache and get rid of \nit. Because there’s always a cheaper page available, the entire cache could potentially\nthrash if the load is big enough. This algorithm also has the problem that it can leave\nhighly expensive pages in the cache indefinitely. If something like a skybox texture is\nloaded into the cache, this is a good trait as the skybox will be active every frame and\n1.1\nEfficient Cache Replacement Using the Age and Cost Metrics\n11\n",
      "content_length": 3254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "most likely not want to be removed from the cache due to its large size. Most of the\ntime, this is a bad trait and one that needs constant attention.\nCost is a powerful ally when combined with other heuristics. By biasing the\nreplacement identification of access pattern algorithms with replacement metrics, you\nallow your cache to find a nice medium between page-replacement requirements and\nthrashing. Additionally, the access pattern identification helps remove the problems\ninvolved with pure cost metrics, allowing high-cost items to eventually be freed from\nthe cache when they reach the state that they are no longer needed. \nAge and Cost (A&C) \nThe previous example assumed that each page in the texture cache had an associative\nAPC as well as a relative cost (RC), and was updated every frame. This example\nassumes that the RC is a more important metric and allows that value to be an integer\nvariable that has no upper limit. For example, if you are moving textures into your\ncache by streaming them from disk, the RC may be a result of the texture size divided\nby the time it takes to stream a fraction of that texture from the media. Consider the\nAPC a normalized floating-point variable between (0,1). \nIn a simple implementation, you can combine these two values into a single result\nwhere the APC acts as a scalar of the RC, thus giving ThrashCost = RC*APC. In general,\nthis turns out to be a very nice heuristic for identifying the proper victim page. To prove\nthis, I’ve provided a few examples of APC/RC ratio, and a description of the replacement\npattern. For the following data, assume that the highest RC value can be 10. \nAPC \nRC \nTC \nPattern\n1.0 \n10 \n10 \nThis page is highly expensive to replace, thus will be a hard candidate\nto move. With the APC of 1.0, this identifies that the page has been\nused every frame over the Age window. At this point, the only way this\npage can be replaced is if it’s forced by a full cache dump, which might\nnot be allowed depending on your implementation.\n0.2 \n8 \n4 \nThis page has a relatively high RC, but its APC shows that it’s rarely\nused, and thus could be considered as a valid replacement. However,\nbecause the RC is so high, it’s worth doing a second APC test on sub-\nwindows of the data in order to determine if this texture should really\nbe replaced. \n1.0 \n5 \n5 \nThis page is at the halfway point. That is, it’s relatively cheap to\nreplace, however its APC says that it will be needed next frame.\nReplacing this page would be no problem, but due to the high APC, \nit might be worth the extra search to find another page with a higher\nRC but lower APC. \n0.01 \n10 \n0.1 \nThis page has an extremely low APC, which points to the fact that it\nhas either just been introduced to the cache, or it is infrequently used.\nIn either case, the TC is so low that other pages with lower RCs could\nbump it if their APC is higher. Because the RC is so high, however,\nit’s worth doing a second APC test on sub-windows of the data in\norder to determine if this texture should really be replaced. \n12\nSection 1\nGeneral Programming \n",
      "content_length": 3078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "As seen in this table, using the simplistic RC*APC value can result in pages with\ndiffering APC/RC values having the same ThrashCost. This will mean that texture A\nwith an APC/RC relation of 0.5/100 can have the same cost as texture B with an\nAPC/RC of 1.0/50. The question here is how do you determine which page to\nreplace? In theory, either page is a valid target, both containing the same numerical\nweight for potential replacement. Texture A has a higher cost, and thus would be\nmore expensive to replace if it were needed in the next frame. Texture B has a lower\ncost, but has an APC value of 100%, so there’s a high probability that this page will be\nneeded immediately. \nIn practice, I’ve found that biasing this decision to replace a lower APC when\nmultiple pages return the same value works much better. In fact, that’s why the Age\nmetric is used. By analyzing the usage pattern, along with cost, you can see that\nalthough a page is more expensive, it is used less often, and thus has a lower probabil-\nity of being incorrectly thrashed. In these situations, it’s a good idea to re-scan the\ncache to find a page with a higher ThrashCost, but with a lower APC value. If one\nisn’t found, it’s safe to assume that this page may need to be replaced for the sake of\nthe cache. However, depending on your system, the better page to replace may vary. \nThis table mentions one other instance that needs discussing. As mentioned, the\nA&C system has the ability to introduce a page that can become static. This can be a\ngood thing if your cache includes textures that are visible from most every camera angle,\nsuch as a skybox texture, or an avatar skin. This can also be bad, however, if this is not\ndesired. If too many of these pages are introduced into the cache, the available working\nsize shrinks considerably, causing more cache misses and overall less cache efficiency. If\nthis is the case, it might be wise to generate a separate cache for these static textures.\nConclusion\nCustom media cache systems are critical for any high-performance environment. As\nthe usage of out-of-core media increases, the need for an accurate control model also\nincreases for game environments. Because older replacement algorithms were designed\nwith operating system memory management and hardware memory access patterns in\nmind, they lack some key properties that allow them to evaluate the more complex\nreplacement situations that can exist. Using a combination of the Age and Cost metrics\nintroduces a great deal of additional information, for a very low overhead, that fits and\nworks well in gaming environments. The Cost metric introduces a concept of overall\nperformance in page loading, which for an on-demand out-of-core system can become\na major bottleneck. The Age metric allows a more per-frame based view of usage pat-\nterns that ties easier into the concept of the game simulation than traditional metrics,\nand also contains enough usable information to create valid replacement cases in any\nenvironment-specific edge case. Because cache replacement needs change over the\ncourse of simulation, this allows a great deal of customization and second order analy-\nsis to evaluate the best victim page for best results. \n1.1\nEfficient Cache Replacement Using the Age and Cost Metrics\n13\n",
      "content_length": 3284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "The advantage of using this powerful duo of metrics is an overall increase in\ncache-page replacement performance, resulting in a lower overhead of thrashes and\ngeneral cost required to fill the cache. At the end of the day, that’s really what you’re\nlooking for. \nAcknowledgments\nBig thanks to John Brooks at Blue Shift for the metric in the “Expanded Age Algo-\nrithm” section, as well as help getting all this off the ground!\nReferences\n[Carmack00] Carmack, J. “Virtualized Video Card Local Memory Is the Right\nThing,” Carmack .Plan file, March 07, 2000.\n[Megiddo03] Megiddo, Nimrod and Modha, Dharmendra S. “ARC: A Self-Tuning,\nLow Overhead Replacement Cache,” USENIX File and Storage Technologies\n(FAST), March 31, 2003, San Francisco, CA.\n[O’Neil93] O’Neil, Elizabeth J. and others. “The LRU-K Page-Replacement Algo-\nrithm for Database Disk Buffering,” ACM SIGMOD Conf., pp. 297–306, 1993. \n14\nSection 1\nGeneral Programming \n",
      "content_length": 929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "15\n1.2\nHigh Performance Heap\nAllocator\nDimitar Lazarov, Luxoflux\ndimitar.lazarov@usa.net\nT\nhis gem shows you a novel technique for building a high performance heap allo-\ncator, with specific focus on portability, full alignment support, low fragmenta-\ntion, and low bookkeeping overhead. Additionally, it shows you how to extend the\nallocator with debugging features and additional interface functions to achieve better\nperformance and usability.\nIntroduction\nThere is a common perception that heap allocation is slow and inefficient, causing all\nkinds of problems from fragmentation to unpredictable calls to the OS and other unde-\nsirable effects for embedded systems such as game consoles. To a large extent this has\nbeen true, mostly because historically console manufacturers didn’t spend a lot of time\nor effort implementing high-performance standard C libraries, which include all the\nheap allocation support. As a result many game developers recommend not using heap\nallocation, or go as far as outright banning its use in runtime components of the game\nengine. In its place, many teams use hand-tuned memory pools, which unfortunately is\na continuous and laborious process that is debatably inflexible and error-prone. All this\ngoes against a lot of the modern C++ usage patterns, and more specifically the use of\nSTL containers, strings, smart pointers, and so on. Arguably, all these provide a lot of\npowerful features that could dramatically improve code development, so it’s not so diffi-\ncult to see why you would want to have the best of both worlds. With this in mind, we\nset out to create a heap allocator that can give the programmer the performance charac-\nteristics of a memory pool but without the need to manually tinker with thousands of\nlines of allocation-related code.\nRelated Works\nA popular open source allocator by [Lea87], regarded as the benchmark in heap alloca-\ntion, uses a hybrid approach, where allocations are handled by two separate methods,\nbased on the requested allocation size. Small allocations are handled by bins of linked\nlists of similarly sized chunks, whereas large allocations are handled by bins of “trie”\n",
      "content_length": 2157,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "binary tree structures. In both cases, a “header” structure is allocated together with\nevery requested block. This structure is crucial in determining the size of the block\nduring a free operation, and also for coalescing with other neighboring blocks. Alloca-\ntions with non-default alignment are handled by over-allocating and shifting the start\naddress to the correct alignment. \nBoth of these factors, header structure and over-allocating, contribute unfavorably\nto alignment heavy usage patterns as is common in game programming. A slightly diff-\nferent approach by [Alexandrescu01] suggests using a per-size template pool-style allo-\ncator. A pool-style allocator uses a large chunk of memory, divided into smaller chunks\nof equally sized blocks that are managed by a single linked list of free blocks, com-\nmonly known as the free-list. This has the nice property of not needing a header struc-\nture and hence has zero memory overhead and naturally gets perfect alignment for its\nelements. Unfortunately, during deallocation, you have to either perform a search to\ndetermine where the block came from or the original size of the block needs to be sup-\nplied as an additional parameter. Combining ideas from the previously described work\nand adding our little gem, we arrive at our solution.\nOur Solution\nOur solution uses a hybrid approach, whereby we split our allocator in two parts—\none handling small allocations and the other handling the rest.\nSmall Allocator\nThe minimum and maximum small allocations are configurable and are set by default\nto 8 and 256 bytes, respectively. All sizes in this range are then rounded up to the\nnearest multiple of the smallest allocation, which by default would create 32 bins that\nhandle sizes 8, 16, 24, 32, and so on, all the way to 256, as shown in Figure 1.2.1.\n16\nSection 1\nGeneral Programming \nFIGURE 1.2.1\nSelection of the appropriate bin based upon allocation size.\n",
      "content_length": 1921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "Notice that since the small allocations are arranged into bins of specific sizes, you\ncan keep any size-related information just once for the whole bin, instead of with\nevery allocation. Although this saves memory overhead, you might rightly wonder\nhow you find this information when, during a free operation, you are supplied only\nwith the address of the payload block.\nTo explain this, we need to establish how the small allocator would deal with\nmanaging memory. You might want to allocate large blocks of memory from the OS,\nas is traditionally done with pool-style allocators, but let’s do it on demand so as to\nminimize fragmentation and wasted memory. Additionally, you want to return those\nsame large blocks (let’s call them pages) back to the OS once you know when they are\nnot used anymore and you need the memory somewhere else. \nWhat is important to note here, for the correctness of this method, is that it asks\nfor naturally aligned pages from the OS. In other words, if the selected page size is\n64KB, this method expects it to be 64KB aligned.\nOnce a naturally aligned page is acquired, the method places the bookkeeping\ninformation at the back of the page. There, it stores a free-list that manages all\nelements inside the page. A use count determines when the page is completely empty,\nand a bin index determines which bin this page belongs to. Most importantly, all\npages that belong to the same bin are linked in a doubly-linked list, so that you can\neasily add, remove, or arrange pages. \nThe last piece of the puzzle is almost straightforward—during a free operation,\nthe provided payload address is aligned with the page boundary. Then you can access\nthe free list and the rest of the bookkeeping information, as shown in Figure 1.2.2.\n1.2\nHigh Performance Heap Allocator\n17\nFIGURE 1.2.2\nThe layout of a single page in a bin.\n",
      "content_length": 1849,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "This method places the bookkeeping information at the back of the page, as\nopposed to the front, because there is often a small piece of remaining memory at the\nback, due to the page size not being exactly divisible by the element size. Therefore,\nyou get the bookkeeping cost for free in those situations.\nThis method also ensures that whenever a page becomes completely full it is\nplaced at the back of the list of pages for the corresponding bin. This way, you just\nneed to check the very first page’s free-list to see whether you have a free element avail-\nable. If no elements are available, the method requests an additional page from the\nOS, initializes its free-list and other bookkeeping information, and inserts it at the\nfront of the bin’s page list.\nWith this setup, small allocations and deallocations become trivially simple. For\nexample, when the allocation size is known in advance, as is the case with “new”-ing\nobjects, the compiler can completely inline and remove any bin index calculations\nand the final code becomes very comparable in speed and size to an object specific\npool-style allocator.\nLarge Allocator\nThe small allocator is simple and fast; however, as allocation sizes grow, the benefits of\nbinning and pool allocations quickly disappear. To combat this, this solution switches\nto a different allocator that uses a header structure and an embedded red-black tree to\nmanage the free nodes. A red-black tree has several nice properties that are helpful in\nthis scenario. First, it self-balances and thus provides a guaranteed O(log(N)) search,\nwhere N is the number of free nodes. Second, it also provides a sorted traversal which\nis very important when dealing with alignment constraints. Finally, it is very handy to\nhave an embedded red-black tree implementation around.\n18\nSection 1\nGeneral Programming \nFIGURE 1.2.3\nLayout of memory use in the large allocator.\nAs shown in Figure 1.2.3, the header structures are organized in a linked list of\nmemory blocks arranged in order of their respective addresses. There is no explicit\n“next” pointer, because the location of the next header structure can be computed\nimplicitly from the current header structure plus the size of the payload memory\nblock. In addition to this, you need to store information about whether a block is\ncurrently free. This information can be stored in the least significant bit of the “size”\nfield, because the requested sizes for large allocations are rounded up to the size of the\n",
      "content_length": 2489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "header structure (8 bytes). This rounding is necessary to allow header structures to\nnaturally align between payload blocks.\nWhen a block is freed, you use the empty space to store the previously mentioned\nred-black tree node. The red-black tree is a straightforward implementation, as in\n[Cormen90], with a few notable modifications.\n1.2\nHigh Performance Heap Allocator\n19\nFIGURE 1.2.4\nThe layout of the red-black tree nodes.\nAs shown in Figure 1.2.4, you have the classic left, right and parent pointers.\nThere are also two additional pointers, previous and next, that form a linked list of\nnodes that have the same key value as the current node. This helps tremendously with\nperformance, because it’s quite common to have lots of free blocks with identical\nsizes. In contrast, a traditional red-black tree would store same key value nodes as\neither left or right children, depending on convention. This would predictably reduce\nperformance, because when searching through these nodes, the search space is not\nhalved as usual to achieve O(log(N)) speed, but is merely walked in a linear fashion of\nO(N).\nBoth left/right and previous/next pointers are organized as arrays of two entries.\nThis is done mostly to simplify operations such as “rotate left” and “predecessor,” which\nnormally have mirrored counterparts such as “rotate right” and “successor.” Using an\nindex to signify left or right, you can then have a generic version that can become either.\nFurthermore, in each node, you keep information on which side of its parent that\nnode is attached—left or right—as well as its “color”—red or black. This allocator uses\na similar packing trick as with the header structure, and places the parent side index\n",
      "content_length": 1712,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "and the node color in the two least significant bits of the parent pointer. The parent\nside index is quite important for performance, especially when combined with a red-\nblack tree that uses the so-called “nil” node, because the essential “rotate” operation can\nthen become completely branch-free.\n20\nSection 1\nGeneral Programming \nFIGURE 1.2.5\nThe nil node’s place in the tree.\nAs shown in Figure 1.2.5, the “nil” node is a special node that all terminal nodes\npoint to, and is also the node to which the root of the tree is attached. The fact that\nthe root is to the left side of the “nil” node might appear random, but in fact this is\nvery important for traversal operations. It is easy to notice that running a predecessor\noperation on the “nil” node would give the maximum element in the tree, which is\nexactly what you want to happen when you iterate the tree backward starting from the\n“nil” node—in STL terminology that is the “end” of the container. This way, the pre-\ndecessor operation doesn’t need to handle any special cases.\nWith all this setup done, during an allocation you search the red-black tree for the\nappropriate size. If the acquired block is too big, it is chopped up and the remainder is\nreturned to the tree. If there are no available blocks, more are requested from the OS,\nusually in multiples of large pages.\nDuring a free operation, you look up the header structure and determine whether\nany of the neighbors are free so they can be coalesced. Because the coalescing is done\nfor every freed block, there could be no more than one adjacent free block on each side,\nand thus this operation needs to check only two neighbors. The resulting free block is\nthen added to the red-black tree.\n",
      "content_length": 1717,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "The more interesting use of the red-black tree happens when you need to allocate\nwith non-default alignment. This allocator uses the fact that you can iterate a binary\ntree in sorted order, and notice that you need to check nodes with sizes equal or larger\nthan the requested size, but smaller than the requested size plus the requested align-\nment. You then can use the binary tree operations “lower bound” with the requested\nsize and “upper bound” with the requested size plus alignment. You can then iterate\nthrough this range until you find a block that satisfies the alignment constraint. Iter-\nating through a red-black tree is an O(log(N)) operation, so obviously larger align-\nments would take longer to find. The important thing to notice is that this will\nguarantee the smallest-fit block criteria, which is considered to be one of the major\nfactors in reducing fragmentation, something that the traditional approach of over-\nallocating and shifting doesn’t satisfy very well.\nCombining the Allocators\nNow that you have two allocators, there is another problem. The small allocator relies\non the page alignment to find its bookkeeping information. With two allocators\nthough, you need a way to distinguish which allocator a given address comes from.\nThere are several solutions, none of them is perfect, but one is the simplest. You go\nahead and access the page info structure, and try to recognize it. You need to make\nsure the large allocator always uses pages that are at least as large as the small allocator’s\npages, which makes accessing the page info safe. You place a per bin marker that gets\nhashed with the page info address and store it inside the page info. Then, during a free\noperation, you access the page info and compute the hash again. If it matches, you\naccept the address as originating from the small allocator; otherwise, you forward it to\nthe large allocator. This solution is very simple and fast, but it has the nagging prob-\nlem that it might, with a very small probability, match the incorrect page. There is a\nway to detect this mistake and in the reference implementation this verification is per-\nformed for debug builds. If a misdetection ever happens, one can increase the security\nby using bigger hashes or extra checks. \nThere are at least a few other ways to solve this problem. One is to use a bit array, \none bit for every page in the address space. On 32-bit machines with 64KB pages, this \nbit array is merely 8KB, but unfortunately this doesn’t scale very well to 64-bit machines. \nA second solution is to keep an array of pointers in every bin, each entry pointing\nback at their respective page, while the page itself has a bin and array offset indices.\nThis guarantees that the page truly belongs to that bin, but unfortunately makes the\nmemory management of that array quite complicated.\nA third solution is to use a reserved virtual address range for all small allocations,\nand with a simple check you can immediately determine whether a pointer belongs to\nit or not. Of course, this requires the use of virtual memory, which is either not present\nor severely limited on many game consoles, and most importantly requires specifying\nsome upper bound on small allocations that might not be possible in certain situations.\n1.2\nHigh Performance Heap Allocator\n21\n",
      "content_length": 3313,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "Multithreading\nIt is quite important these days to have robust multithreading, and the allocator is\noften the very first thing that needs to be properly implemented. Our solution is not\nground-breaking but provides good efficiency in mild levels of contention. Notice that\nthe small allocator can have a mutex per bin because once the bin is selected there is no\nsharing of data between bins. This means that allocations that go to different bins can\nproceed in full parallel. On the other hand, the large allocator is more complicated and\nyou need to lock it as soon as an operation needs access to the red-black tree.\nThe only faster alternative to having a mutex per bin is to use thread local storage.\nThat might be feasible especially on systems that can afford some additional memory\nslack per thread. Unfortunately, thread local store is still non-portable and has all\nkinds of quirks. \nDebugging Features\nSo far, we have a high-performance allocator that can easily be used as a malloc/free\nreplacement. Now, you can easily add features that can help you with debugging or\nextracting additional performance or functionality.\nThere are many ways to add debugging support and the reference implementation\ntakes a classic approach of keeping debug records for every allocation made. These\nrecords are kept in separately managed memory so that they interfere with the payload\nas little as possible. We reuse several of the methods developed for the main allocator,\nspecifically we use an embedded red-black tree to quickly search debug records and also\nwe use a novel container that we call a book. The idea is that we have a hybrid data\nstructure similar to a list which we manage in “pages” and we can only add elements to\nthe end, as one would write words in a book, hence the name. The need for such a\nspecific structure arises from the fact that you need to use large memory chunks\ndirectly from the OS and you want the data structure to take care of that memory,\nsimilar to how dynamic arrays over-allocate and then fill in elements one at a time.\nWhen you combine the embedded red-black tree with the “book” container, you\ncan manage the debug records quite efficiently. Besides the size and the address of the\npayload block, the debug record stores a partial copy of the call stack at the time of the\nallocation request, which can be quite useful for tracking memory leaks. Additionally,\nyou can store the so-called “debug guard,” which is a series of bytes of memory that is\nover-allocated with the payload and is filled with a sequence of numbers.\nThen, during a free operation, the “debug guard” is examined to verify the\nintegrity of the block. If the expected sequence of numbers is not found, it’s highly\nlikely there is some memory stomping going on. It is a bit more difficult to determine\nwho exactly did the stomping, but often it is code related to whoever allocated that\nblock, so that’s a good starting point. If the stomp is reproducible, a well placed hard-\nware breakpoint can quickly reveal the offender.\n22\nSection 1\nGeneral Programming \n",
      "content_length": 3068,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "Extensions\nSo far, this allocator follows quite closely the malloc/free/realloc interface. We have\nfound that exposing a bit more functionality can be very beneficial for performance.\nThe first addition is what we call “resize”—the ability to change the size of an\nalready allocated block, but without changing the address of that block. Obviously, this\nmeans the block can only grow from the back, and even though that seems limited, it\nturns out it can be very useful for implementing dynamic arrays or other structures\nthat need to periodically expand.\nAnother extension is the provision of free operations that take as additional param-\neters the original requested size and alignment. This is beneficial to this allocator\nbecause it can use that size to determine whether the small or the large allocator\nshould free that block. These additional functions cannot be used on blocks that have\nbeen reallocated or resized, and the only significant use for these additional functions\nwould be to implement high-performance “new” and “delete” replacements. \nOn the CD\nThere is a reference implementation of the allocator on the included CD. It has been\nused in practice in a next generation game engine together with a custom STL imple-\nmentation that takes advantage of our allocator’s features and extensions. We also\nprovide a simple synthetic benchmark to verify that the allocator has any performance\nadvantages. \nFurthermore, we show several ways to integrate this allocator with existing or\nfuture C++ code. The easiest way, of course, is to just override the global new and\ndelete operators. This has several disadvantages, but most notably it is not an appro-\npriate solution for a middleware library. Per class new and delete operators, as tempt-\ning as they sound, in practice prove to be quite difficult to work with and tend to have\nfrustrating limitations. We show one possible way to have correct custom per object\nnew and delete functionality with template functions. \nReferences\n[Alexandrescu01] Alexandrescu, Andrei. Modern C++ Design, Addison-Wesley\nLongman, 2001.\n[Berger01] Berger, Emery D. “Composing High-Performance Memory Allocators,”\navailable online at ftp://ftp.cs.utexas.edu/pub/emery/papers/pldi2001.pdf.\n[Cormen90] Cormen, Thomas. Introduction to Algorithms, The MIT Press, 1990.\n[Hoard00] Berger, Emery. “The Hoard Memory Allocator,” available online at\nhttp://www.hoard.org/.\n[Lea87] Lea, Doug. “A Memory Allocator,” available online at http://g.oswego.edu/\ndl/html/malloc.html.\n1.2\nHigh Performance Heap Allocator\n23\n",
      "content_length": 2551,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "25\n1.3\nOptical Flow for Video Games\nPlayed with Webcams\nArnau Ramisa, Institut d’Investigació \nen Intelligència Artificial\naramisa@iiia.csic.es\nEnric Vergara, GeoVirtual\nevergara@geovirtual.com\nEnric Martí, Universitat Autónoma \nde Barcelona\nEnric.Marti@uab.cat\nO\nne of the most widely used techniques in computer vision commercial games is\nthe concept of optical flow. Optical flow is the movement between the pixels of\nsuccessive frames of a video stream provided, for example, by a modern Webcam.\nMultiple techniques, with different properties, exist in the computer vision liter-\nature to determine the optical flow field between a pair of images [Beauchemin95].\nThis gem explains three of these techniques—the direct subtraction of successive\nframes, motion history, and a more advanced technique called the Lucas and Kanade\nalgorithm. These three techniques have a different degree of robustness and also a dif-\nferent computational cost, therefore the choice depends on the requirements of each\napplication.\nIntroduction\nThe proposed algorithms are prepared and ready to use, with many other computer\nvision techniques, in the OpenCV library. This library is an open source project that\nimplements in C the most important algorithms and data structures used in computer\nvision applications. In 2006, the first stable version of OpenCV (1.0) was released after\n",
      "content_length": 1367,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "five years of development. Game Programming Gems 6 contains an article where this\nlibrary is used to detect in which position the face of the player is located [Ramisa06].\nThis information was used to map the action of leaning around a corner in a third- or\nfirst-person shooter.\nThe optical flow is an array of vectors that describes the movement that has\noccurred by pixels between successive frames of a video sequence. Usually it is taken as\nan approximation to the real movement of the objects in the world. The optical flow\ninformation is used not only by many computer vision applications, but also by bio-\nlogical systems such as flying insects or even humans. An example of this application\nis the MPEG4 video compression standard, which uses the optical flow of blocks of\npixels to eliminate redundant information in video files.\nIn computer vision games, the optical flow is used to determine if a particular\npixel of the screen is “activated” or not. If enough pixels of a certain area are activated,\none can consider that a “button” has been pressed or, in general, that an action has\nbeen performed.\nFor this, it is usually not necessary to estimate the full optical flow field and com-\nputationally cheaper methods that compute only if a given pixel has changed with\nrespect to the previous frame can be used.\nOpenCV Code\nThis section explains the most important functions of the webcamInput class. The usage\nof the class is structured around a main loop where new frames are acquired from the\nWebcam.\n#define ESC_KEY 27\nwebcamInput webcam(true,1);\n// method = (1: Lucas-Kanade,\n2: Differences, 3: Motion History)\nwhile(1)\n{\nwebcam.queryFlow();\nif(cvWaitKey(10)==ESC_KEY)\nbreak;\n}\nIn this code, a new webcamInput object called webcam is created. This object en-\ncapsulated all the logic to acquire images through the camera and process them. The\nconstructor requires two parameters: a Boolean that indicates if the camera connection\nshould be initialized during the object construction or not, and an integer from 1 to 3\nthat indicates which method of the optical flow will be used. Later, inside the loop, \nthe queryFlow() function is used to acquire a new frame and process it. Finally, cvWait-\nKey() is used to pause for 10ms waiting for the Escape key to possibly stop the process.\nDuring initialization all required variables, depending on the method, are allocated\nin memory:\n26\nSection 1\nGeneral Programming \n",
      "content_length": 2431,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "void webcamInput::queryFlow()\n{\ngetImage();\nswitch(method)\n{\ncase 1:\nlucaskanade();\nbreak;\ncase 2:\ndifferences();\nbreak;\ncase 3:\nflowHistory();\nbreak;\n}\nflowRegions();\ncvCopy( image, imageOld );\n//to save the previous image to do optical flow\n}\nThe function getImage() acquires a new image from the Webcam and converts it\nto gray-level, then, depending on the method chosen, the appropriate function is\ncalled. Finally, flowRegions() is used to count the activated pixels in the defined\nregions of interest, and the current image is copied to imageOld for using it in the next\noptical flow computation.\nFirst Method: Image Differences\nIn this section, the three methods used are explained. The first method is the simplest\none: image differences. This method is really simple, and consists of subtracting the\ncurrent camera image from the previous one:\nvoid webcamInput::differences()\n{\ncvSmooth( image, image, CV_GAUSSIAN, 5, 5);\ncvAbsDiff( imageOld, image, mask );\ncvThreshold( mask, mask, 5, 255, CV_THRESH_BINARY ); // and\nthreshold it\n/* OPTIONAL */ cvMorphologyEx(mask, mask, NULL,kernel,\nCV_MOP_CLOSE ,1);\ncvShowImage(\"differences\", mask);\n}\nThe cvAbsDiff function subtracts image from imageOld, which are the current\nand previous images of the Webcam. The result of the operation is stored in mask, and\nin the next line its content is binarized: all pixels with a value lower than 5 (which\nindicates similar pixels in image and imageOld) are discarded, whereas the ones with a\nhigher value are marked as active. This threshold depends on the sensitivity of the\n1.3\nOptical Flow for Video Games Played with Webcams\n27\n",
      "content_length": 1625,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "used Webcam. If a lower value is used, more valid active pixels would be correctly\ndetected, but at the same time more false active pixels would appear, generated by\nnoise in the images. To reduce the effect of noise in the estimation, the first step con-\nsists in smoothing the image using a Gaussian kernel of 5 \u0002 5 pixels. This is done\nusing the function cvSmooth().\nIndividual or small groups of activated pixels are not likely to correspond to real\nmoving objects, therefore the function cvMorphologyEx() applies a closure to eliminate\nthese spurious activated pixels while not changing the correct ones [Morphology]. The\nsize and shape of the morphologic operator is defined in the structure kernel. Finally,\nthe last parameter specifies the number of times in a row that the erosion and dilation\nare performed. The closure is a powerful yet computationally expensive operation, and\ncan be avoided if the noise level of the images is low, or by raising the value of the\nthreshold.\nThis method is faster than the Lucas and Kanade method, but it is not as robust\nat handling bad camera quality.\nSecond Method: Motion History\nThe second method is called motion history. It uses the same principle of the image\ndifferences but uses more than just the previous image, and “remembers” recently\nactive pixels.\nvoid webcamInput::flowHistory()\n{\n//FLOW HISTORY\ncvSmooth( image, image, CV_GAUSSIAN, 5, 5);\ncvCopy( image, buf[last]); \nIplImage* silh;\nint idx2;\nint idx1=last;\nidx2 = (last + 1) % N; // index of (last - (N-1))th frame\nlast=idx2;\nint diff_threshold=30;\nsilh = buf[idx2];\ndouble timestamp = (double)clock()/CLOCKS_PER_SEC;\ncvAbsDiff( buf[idx1], buf[idx2], silh );\ncvThreshold( silh, silh, diff_threshold, 1, CV_THRESH_BINARY );\ncvUpdateMotionHistory( silh, mhi, timestamp, MHI_DURATION );\ncvCvtScale( mhi, mask, 255./MHI_DURATION,\n(MHI_DURATION - timestamp)*255./MHI_DURATION );\ncvShowImage(\"M_history\",mask);\n}\n28\nSection 1\nGeneral Programming \n",
      "content_length": 1955,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "In this function, buff is a cycling buffer where the last N images from the Web-\ncam are stored. When a new image enters the buffer, the oldest and the new image are\nsubtracted, and the result is thresholded to find pixels where reliable change has\noccurred. Then the motion history image mhi is updated with this new information\nand the timestamp for the latest frame. Finally, the new motion history image is\nscaled to an 8 bits per pixel mask image. Moreover, this motion history image can be\nused to determine the motion gradient and to segment different moving objects. A\ncomplete example of this method can be found in the motempl.c file of the OpenCV\nsamples.\nThird Method: Lucas and Kanade Algorithm\nThe first two methods don’t give as output the real optical flow, they just detect pixels\nwhere motion has occurred. However, in this case, you can trade the destination posi-\ntion of the moved pixels for less computational load. The last method presented is the\noptical flow estimation method by Lucas and Kanade [Lucas81]. This method esti-\nmates the position where a given pixel has moved using a procedure similar to how\nthe Newton-Raphson method finds the zeroes of a function.\nvoid webcamInput::lucaskanade()\n{\ncvCalcOpticalFlowLK(imageOld,image,cvSize\n(SIZE_OF,SIZE_OF), flowX, flowY);\ncvPow( flowX, flowXX, 2 );\ncvPow( flowY, flowYY, 2 );\ncvAdd( flowXX, flowYY, flowMOD );\ncvPow( flowMOD, flowMOD, 0.5 );\ncvThreshold( flowMOD, flowAUX, 10, 255, CV_THRESH_BINARY );\n/* OPTIONAL */ cvMorphologyEx\n(flowAUX, flowAUX, NULL,kernel, CV_MOP_CLOSE ,1);\ncvShowImage(\"LUKAS_KANADE\",flowAUX);\n}\nThe function cvCalcOpticalFlowLK() computes the optical flow between the\ngray-level images imageOld and image using the Lucas and Kanade method. The\nremaining parameters of the function are cvSize(SIZE_OF,SIZE_OF), which specifies\nthe size of the window that will be used to locate the corresponding pixel in the other\nimage, and flowX and flowY, where the components of the optical flow vectors will be\nstored. You are interested in the module of the vectors, which indicates the strength of\nthe movement. Once you have the module, you threshold it to eliminate all active\npixels caused by noise. If the camera is really noisy, it is also possible to use a morpho-\nlogical closure to remove isolated pixels.\n1.3\nOptical Flow for Video Games Played with Webcams\n29\n",
      "content_length": 2365,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "The time values shown in Table 1.3.1 were obtained on a Pentium IV 3GHz\ncomputer running the openSUSE 10.2 operating system where the execution time\nwas measured over 100 iterations. The image size was 640 \u0002 480.\nTable 1.3.1\nTime Required for Each Iteration of the Motion Detection Algorithms\nMean\nMedian\nStd\nImage differences\n0.020867s\n0.025000s\n0.0063869s\nMotion history\n0.027794s\n0.025000s\n0.0056899s\nLucas and Kanade\n0.21114s\n0.19500s\n0.043091s\nOptical Flow Game\nIn order to see the results of the optical flow in a real application, we developed a simple\ngame resembling Eye Toy: Play (a game developed by SCEE London, using a digital cam-\nera similar to a Web camera, for PlayStation 2). The idea behind the game is very simple:\nclean the stains that appear on the screen through bodily movements (see Figure 1.3.1).\nBecause players could move their arms about frantically, thereby cleaning all stains\nas they appear without any difficulty, we added some toxic stains that players have to\navoid in order to keep on playing. This forces players to be careful with their move-\nments, introducing an element of skill which makes the game more fun.\n30\nSection 1\nGeneral Programming \nFIGURE 1.3.1\nA screenshot of the game.\n",
      "content_length": 1224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "The point is to use the optical flow in the game, so we have encapsulated the\nfunctionality described previously in a C++ class called webcamInput, which makes it\neasier to apply to the game. The public interface of this class is as follows:\nclass webcamInput\n{\npublic:\nwebcamInput( int method=1 );\n// method = (1: Lukas-Kanade,\n2: Differences, 3: M History)\n~webcamInput( );\nvoid     GetSizeImage        ( int &w, int &h );\nuchar*   GetImageForRender   ( void );\nvoid     QueryFlow           ( void );\nvoid     AddRegion           ( int x, int y, int w, int h );\nstd::vector<float> FlowRegions ( void );\nprivate:\n...\n}\nIn the class’s constructor, you can decide which method, from the three introduced\nin the previous section, you want to use in order to calculate the optical flow. As the\ndefault, we use the Lucas and Kanade method, which seems to yield the best results. \nIn order to show on the screen the color image coming from the Webcam (the player’s\nbody), this class provides two important functions. The first one is GetSizeImage(),\nwhich informs you of the size of the image coming from the Webcam. The second func-\ntion is GetImageForRender(), which returns an array of unsigned char values that repre-\nsent the RGB (red, green, blue) components of the pixels, ordered in rows. By means of\nthe QueryFlow() function, you can calculate the optical flow of the current frame.\nYet, what you need for the game is to be able to query the amount of movement\nthat has taken place in a particular area of the image (the one occupied by a stain to be\ncleaned). To achieve this, at the beginning of the game you can define as many regions\nas you want by making use of the function AddRegion(int x, int y, int w, int h).\nThis function creates a region with its origin at the coordinates (x, y) of the image\ngenerated by the Webcam and a width and height given by (w, h).\nOnce the regions have been defined, every time QueryFlow() is called, you can get\nthe extent of the movement in each region by using the function FlowRegions(). This\nfunction returns a list of floating-point values (as a std::vector<float>) comprising\nvalues within the [0,1] range that represent the percentage of pixels where movement\nhas been detected.\nIn the game, you have to instantiate an object (mWebCam) of this class, subsequently\ncalling its functions from several parts of the code, as described here:\n1.3\nOptical Flow for Video Games Played with Webcams\n31\n",
      "content_length": 2443,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "1. Upon game initialization, as we have already pointed out, you need to partition\nthe query for optical flow in several distinct regions. In particular you have \nto divide the initial image into 16 regions as a 4 \u0002 4 grid (see Figure 1.3.2). \nIn order to achieve this, you need to use the function mWebCam.AddRegion() as\nfollows:\nint width, height;\nmWebCam.GetSizeImage( width, height );\nfor(int col = 0; col < 4; col++)\n{\nfor(int row = 0; row < 4; row++)\n{\nmWebCam.AddRegion(col* (width/4), row*(height/4),\nwidth/4, height/4);\n}\n}\n2. During rendering, you can call the function mWebCam.GetImageForRender()\nin order to paint the colour image coming from the Webcam. On top of the\nimage, you paint the stains with a certain alpha component, depending on\nhow clean they are. The image of the stains will occupy all the space deter-\nmined by every one of the 16 regions.\n3. During the game updates, the function mWebCam.QueryFlow() will be called\nfirst, followed by a call to mWebCam.FlowRegions(). This way, you know the\nextent of the movement that has taken place in each one of the 16 regions,\nmaking more and more transparent (based on the movement detected) the\nimages of the stains that are active at that particular moment. Stains vanish\nas they become completely transparent.\n32\nSection 1\nGeneral Programming \nFIGURE 1.3.2\nScreenshot of the game, partitioning the screen in several regions.\n",
      "content_length": 1397,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "In order to keep an acceptable frame rate, you need to consider two points. The\nfirst one is that the Update() function of the game should not call QueryFlow() every\ntime, as the latter method is computationally very costly. To circumvent this problem,\nyou can restrict the call to QueryFlow() to a certain frequency, x times per second,\nlower than that of the update calls. This will make the game run smoothly on most\nmodern computers without the game’s responsiveness being affected.\nA further consideration that needs to be taken into account affects the resolution\nof the image coming from the Webcam. That is, the higher the resolution, the more\ncalculations the CPU will have to carry out before obtaining the optical flow. This\nmeans that you have to set that parameter to, for example, 320 \u0002 240 pixels. \nAs a final remark, the CD that accompanies this book contains not only an exe-\ncutable version of the project for Microsoft Windows, but also the source code for it,\nas well as a .pdf file with the UML class diagram. The game has been programmed in\nC++, with Microsoft Visual Studio 2005, and using the DirectX and OpenCV\nlibraries.\nReferences\n[Beauchemin95] Beauchemin, S.S and Barron, J.L. “The Computation of Optical\nFlow,” ACM Computing Surveys (CSUR), Vol. 27, No. 3, pp. 433–466, ACM\nPress New York, NY, USA, 1995.\n[Lucas81] Lucas, B.D. and Kanade, T. “An Iterative Image Registration Technique\nwith an Application to Stereo Vision.” \n[Morphology] \n“Morphological \nImage \nProcessing,” \navailable \nonline \nat \nhttp://en.wikipedia.org/wiki/Morphological_image_processing.\n[Ramisa06] Ramisa, A., Vergara, E., and Marti, E. “Computer Vision in Games\nUsing the OpenCV Library,” Game Programming Gems 6, edited by M. Dick-\nheiser, Charles River Media, 2006, pp. 25–37.\n1.3\nOptical Flow for Video Games Played with Webcams\n33\n",
      "content_length": 1839,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "35\n1.4\nDesign and Implementation\nof a Multi-Platform\nThreading Engine\nMichael Ramsey\nmiker@masterempire.com\nE\nngine development is changing. As the paint of an ancient masterpiece fades over\ntime, so are some of the tried and true techniques of the past era of single-core\nengine development. Developers must now embrace architectures that execute their\ngames on multi-core systems. In some cases, performance differs on a per-core basis!\nThe development of an architecture in a multi-core environment must be acknowl-\nedged, designed, planned, and finally implemented. The implementation aspect is where\nthis gem assists, by providing a theoretical foundation and a practical framework for a\nmulti-platform threading system.\nA fundamental precept for a game engine architecture is the requirement that it \nbe able to exploit a multi-core environment. The exploitation of the environment is a\nsystem that by definition allows for multiple tasks to be executed in parallel. Addition-\nally, you want the performance of the system in question to be real-time; this real-time\nperformance requirement demands that the threading system also be lightweight. Data\nstructures, object construction, cache concerns, and data access patterns are just a few\nof the issues that you must be continuously aware of during development of objects\nthat will use the threading system.\nThis gem focuses on the development of a threading engine that has been engi-\nneered for both an Xbox 360 and a standard multi-core PC game engine. With that\nbeing said, I’ve provided details on the core systems that are applicable to all architec-\ntures, not just a multi-core desktop (Windows/Linux) or Xbox 360. So while you will\nread about cache lines, they will focus on the principles that make them important\nacross multi-platforms and operating systems.\n",
      "content_length": 1826,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "System Design for a Practical Threading Architecture\nOne of the most important aspects of designing a multithreaded program is spending\nthe time upfront to design and plan your game architecture. Some of the high-level\nissues that need to be addressed include the following:\n• Task dependencies\n• Data sharing\n• Data synchronization\n• Acknowledgment and flow of data access patterns\n• Decoupling of communication points to allow for reading, but not necessarily\nwriting, data\n• Minimizing event synchronization\nOne of the most basic, yet the most efficient, principles of threading a game system\nis to identify large systems that have relatively few dependencies (or even focused points\nof intersystem communication) and thread them. If the points of communication\nbetween the systems are focused enough, a few simple synchronization primitives (such\nas a spinlock or mutex) are usually sufficient. If a stall is ever detected, it is straight-\nforward to identify and reduce the granularity of that particular event through routing\nto a different interobject manager. It is important when designing a multithreaded game\nengine to not only be stable but to also strive for extensibility.\nA Pragmatic Threading Architecture\nOn the book’s CD, you’ll find the source to a complete multi-platform threading sys-\ntem. The GLRThreading library has been engineered in a platform-agnostic manner\nwith a provided implementation for the Windows operating system. The interfaces\nare conducive to expansion onto the Xbox 360. The GLRThreading library supports\nall general threading features from the Win32 API. Some functionality has been\nencapsulated for ease of use (for example, GLRThreadExecutionProperties). The stan-\ndard Win32 model of threading is preemptive in nature. What preemptive means in\nthis context is that any thread can be suspended by the operating system to allow for\nanother thread to execute. This allows the OS to simulate multiple processes while\nonly having a single processor. Preempting can be directly influenced by an attributed\nGLRThreadTask property, but generally you should be aware that once a task has been\nexecuted or resumed inside the GLRThreading library (that is, made available to the\nWindows OS), it could and most likely will be preempted or have its execution time\nreduced/increased by the operating system.\nBasic Components of the GLRThreading Library\nAs you learn about the structure of the GLRThreading library, use Figure 1.4.1 to better\nunderstand the system’s components and dependencies. The foundational interface to\nthe GLRThreading system is aptly named GLRThreadFoundation. GLRThreadFoundation\n36\nSection 1\nGeneral Programming \n",
      "content_length": 2671,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "is a singleton that should be available to all game systems that need access to thread-\ning capabilities. Usually, GLRThreadFoundation is placed inside a precompiled header,\nwhich is then subsequently included in all files inside an engine. Through GLRThread-\nFoundation you control the submission of tasks. But before you can look at that, you\nhave to determine and define some basic properties for the execution environment;\nthis is where the system descriptions come in.\n1.4\nDesign and Implementation of a Multi-Platform Threading Engine\n37\nFIGURE 1.4.1\nThe GLR thread library.\nIn order to execute properly, the threading system needs the ability to query infor-\nmation about its environment. This includes determining the number of processors,\nthe memory load, and whether or not hyper-threading is supported. To accommodate\nthis in a platform-agnostic manner, there is the GLRISystemDescription. The platform-\nspecific implementations are derived from this basic interface. The system description\nfor MS Windows is the GLRWindowSysDesc, and the Xbox 360 is implemented by\nGLRXBox360SysDesc.\n",
      "content_length": 1096,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "GLRThreadFoundation Usage\nThe GLRThreadFoundation is the focal point for all threading interactions. The types of\nthreading interactions that you can execute include the ability to execute tasks from a\ngame’s objects, as well as accessing threads from the pool. Inside the codebase there will\nbe a single instance of the thread foundation. For example:\nGLRThreadFoundation glrThreadingSystem;\nTo access functionality inside the threading system, you use the following method\nsyntax:\nglrThreadingSystem.FunctionName();\nwhere FunctionName is any of the platform-agnostic functions that can be executed by\nthe game level components.\nThreads\nThreads are segments of code that can be scheduled for execution by the operating\nsystem or by an internal scheduling system. See Figure 1.4.2 for a comparison between\na typical single-threaded environment and its multithreaded counterpart. These \ncode snippets can be single functions, objects, or they can be entire systems. The\nGLRThreading libraries interface is designed to be platform-agnostic, so all manipu-\nlation is done on the GLRThread level.\n38\nSection 1\nGeneral Programming \nFIGURE 1.4.2\nComparison between a single threaded and multithreaded programming\nmodel.\n",
      "content_length": 1214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "GLRThread is the platform-agnostic implementation for a thread within the\nGLRThreading library. The GLRThread interface allows the following operations on a\nplatform-implemented thread:\n• Creating a thread\n• Executing a thread\n• Altering a thread’s properties\n• Resuming a thread\n• Terminating a thread\n• Temporarily suspending a thread\n• Querying the status of a thread \nThere are several variations of property management and thread execution. Also\nassociated with a GLRThread are the following control mechanisms: GLRThread\nProperties and GLRThreadTask. These mechanisms control (among other aspects)\nwhere and generally how a thread will execute a task.\nPreemptible and Simultaneously Executed Threads\nIt is vital that every engine developer be aware of the performance characteristics of the\ncores that their engine not only targets but also is developed on. This is because a sig-\nnificant amount of development is initially implemented on a desktop PC, which usu-\nally has very different execution characteristics from a typical multi-core console. One\nof the most important aspects of engineering a threading system is the consideration of\nthreads that can be preempted and those that are agnostically called non-preemptible.\nOne of the standard paradigms that a thread follows is that an OS can suspend execu-\ntion of a thread to allow another thread to execute. When the OS decides to suspend\nexecution of a current thread, it will save the context of the currently executing thread\nand restore the context state of the next thread. Context switching is not free; there is\nsome overhead but generally the cost of idling a thread to not incur the overhead of \na context switch is not worth the added code complexity. The switching of threads\ncreates the illusion of a multitasking system.\nThere is also support on consoles for the ability to create threads that are essentially\nnon-preemptible. A non-preemptible thread is a thread that cannot be interrupted by\nthe OS. This power is not the pinnacle of blissfulness. The independent execution of\nthreads (on the same core) usually share the same L1 cache, which generally means you\nstill want like tasks to execute on the same core in order to utilize any cache coherency\ninherent in the data structures. This is to minimize the cache thrashing that could\noccur when two disparate systems execute tasks on the same core.\nThread Properties\nGLRThreadProperties is the general mechanism that stores a particular thread handle\nand its associated ID. There is also the ability to alter the thread’s default stack size.\nThe thread’s stack is the location for its variables as well as its call stack. The OS can\n1.4\nDesign and Implementation of a Multi-Platform Threading Engine\n39\n",
      "content_length": 2735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "automatically grow the stack for you, but this is a performance hit on consoles. To\navoid changing thread stacks on the fly, you should anticipate and set the stack size\nbeforehand. \nThread context switching is in general pretty fast—but as with anything there are\nassociated costs. One of these costs is memory associated with the thread to store its\ncontext information. The size of the default GLRThread context is 64KB. This 64KB\ncan and should be manually adjusted depending on the platform that you are target-\ning. If you need to increase the size of thread stacks on a Windows-based OS (such as\na PC or Xbox 360), there is a property that can be set from the Visual Studio develop-\nment environment.\nOne of the gotchas to be on the lookout for is when your stack needs to be 128KB\nor 256KB. This type of situation usually requires a scaling down of either the size of\nthe task that is being executed (that is, reducing the granularity) or identifying objects\nthat can be further decomposed into smaller implementations, as in [Ramsey05].\nThread Execution Properties\nA thread execution property is a platform-agnostic interface that allows for more granu-\nlar control over a task’s execution. GLRThreadExecutionProps provides for a number of\nproperties such as defining a task’s preferred processing element, a task’s priority, and a\ntask’s affinity mask. A thread’s execution property also has the ability to define its ideal\nprocessor element. This allows for a management system to group like tasks on a partic-\nular processor for execution. This is defined inside GLRThreadExecutionProps.h.\nOn a single physical processor with hyper-thread capabilities, the GetProcess\nAffinityMask() will return bits 1 and 2 with bit 3 set as well, to indicate that you have\nat least one physical CPU with two logical CPUs. On a dual CPU (physical) machine\nwith hyper-threading capabilities, GetProcessAffinityMask() would show 1 + 2 + 4 + 8\n= 15. This indicates two physical CPUs with two logical CPUs apiece. CPUs begin their\nidentification at 1. It should be noted that the implementation of GLRThreadExecution\nProps is inside the GLRThreadExecutionProps.h header.\nProcessor Affinity\nAffinity requests threads to execute on a specific processor. This allows the system\ndeveloper to target specific processors for repeated operations. By way of repeated\noperations, you can also group associated operations together, to further increase the\ncache coherency of similar objects. Some systems may regard thread affinity as a hint\nand not a requirement, so check the documentation before assuming any problems in\nyour threading libraries. \nSpecifying a task’s affinity is done through a simple label of either PA_HARD or\nPA_SOFT. These flags tell the OS to either use a particular processor or just the \nspecified processing element as a hint, respectively. This is defined in GLRThread\nExecutionProps.h.\n40\nSection 1\nGeneral Programming \n",
      "content_length": 2932,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "Task Priority\nChanging the priority on a thread is suggested only for certain systems. The\nGLRThreading library allows you to designate a thread’s priority according to the fol-\nlowing scale:\n• TP_Low\n• TP_Normal\n• TP_High\n• TP_Critical\n• TP_TimeCritical\nThe default setting for newly created thread execution properties priority is nor-\nmal. This can be changed depending upon a particular tasks requirements. This is\ndefined inside GLRThreadExecutionProps.h. \nThread Allocation Strategies\nThere are numerous ways to build a threading system, from the naive allocation of\nthreads on the fly to the more sophisticated implementation of preallocated work sys-\ntems. One of the underlying paradigms for the GLRThreading system is that you\nwant to do as much allocation up front as possible. This is important in developing a\nconsole title, because you not only want to be aware of the memory consumption at\nstart-up, but also memory usage of system-level components at runtime.\nNaive Allocation\nThe simplest and most straightforward way to create a threading system is to have a\nthread manager object, implemented as a singleton, that processes requests in a create-\nfor-use paradigm. Whereas this is probably the easiest way to get started, it is not an\nadvantageous decision when factoring in the complete length of a product cycle as\nwell as the runtime performance of constantly allocating and deallocating a thread.\nThread Pools\nWith the underlying principle of front-loading system level allocations in mind, the\ninnards of the GLRThreadingPool rely upon as much preallocation as possible. The\nthread pool is a system that front-loads the creation of threads. This obviates the need\nfor runtime creation of resources that can be dealt with once and for all upon start-up.\nWhereas the creation of a single thread is not that expensive, the constant allocation\nand deallocation during runtime is an unnecessary burden. For instance, the system\nexperiences memory fragmentation if threads are constantly allocated and deallo-\ncated. The actual number of threads created for the pool is dependent on the system\nand can be modified based upon the game’s needs and performance criteria.\n1.4\nDesign and Implementation of a Multi-Platform Threading Engine\n41\n",
      "content_length": 2255,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "The thread pool is a subsystem that works off of the time-tested paradigm of task\nsubmission. At regular intervals, worker threads look for a task to execute. If there is\nwork available, the execution attributes are set up and the thread is resumed. Once the\ntask has been executed, the thread is suspended and made available for subsequent\ntasks.\nThread Pool Properties\nThe thread pool has properties that allow for the defining of several characteristics\nthat will prove useful when the system is used in various games. The thread pool needs\nthe ability to change the number of created threads, the number of tasks that the\nthreads can work on, and the ability to lock the task pool at any particular time.\nMultiple Pools\nSo you might be asking yourself if one pool is good, then possibly creating multiple\nthread pools for different subsystems in a game engine might be an even better idea.\nThe issues with introducing multiple pools are manifold—the primary issue is that if\nyou have multiple pools, with differing performance characteristics (through the use\nof thread properties, task scheduling, and so on), you have to introduce another layer\nof complexity into the system—the need for interpool communication. This com-\nplexity is simply not wanted in such a performance-critical system; the more straight-\nforward the underlying thread system is, the more likely it is you’ll avoid difficulties\nintroduced through the complexity of the system.\nObject Threading\nThe GLRThreading library provides its threading capabilities through a process of\ncreating an object and then submitting that newly created object to the threading\nlibrary. To submit an object to the GLRThreading system, you just need to make the\ncall:\nGLRThreadFoundation.submitTask(&newGameSystemFunction);\nTo actually have the worker threads grab tasks from the task list inside the thread-\ning pool, you need to make a call to distribute() using this call:\nGLRThreadFoundation.distribute();\nYour object is now off and running on the same processor as the invoking process.\nTo make the execution of the objects easier, the process of allowing threads to execute\nshould be inserted in your main game update. In this manner, game systems can just\ncreate tasks, submit them, and let the threading system deal with allocation, execution,\nand all the details. \n42\nSection 1\nGeneral Programming \n",
      "content_length": 2364,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "Thread Safety, Reentrancy, Object Synchronicity, \nand Data Access\nDealing with issues of designing game systems that are thread-safe and reentrant is a\nthorny business and is beyond the scope of this gem. A lot of the issues are highly\ndependent on the architecture and data-access patterns of the game. There are a few\nprinciples and practices to keep in mind when creating threadable objects and systems.\nA rule of thumb about reentrancy is that you should make a system reentrant\nonly if it needs to be. It costs a lot of time and involves a lot of effort to make your\nunderlying game libraries 100% reentrant and the truth of the matter is that the\nmajority of them don’t have to be reentrant. Sure, libraries such as your memory man-\nager and task submission system need to be reentrant and thread-safe, but a lot of the\nmanagerial systems can serve as a gatekeeper through the use of a cheap synchroniza-\ntion construct. By using something as simple as a hardware-based spinlock, you can\npush the burden of thread safety up to the system managers. This makes even more\nsense, because they should control their own data flow. So, once you’ve identified the\ngeneral data flow inside your engine, the process of determining what actually has to\nbe reentrant is usually clear.\nDancing the Line (or Cache Coherency)\nObject alignment along cache boundaries is important for a standard single-core\nengine but it becomes paramount when you develop for a multi-core environment.\nNormally, a cache is broken into cache lines of 32 or 64 bytes. When main memory is\ndirect-mapped to the cache, the general strategy is to not be concerned with the amount\nof memory being mapped, but with the number of cache lines that are being accessed.\nThere are three basic types of cache misses:\n• Compulsory miss. This occurs the first time a block of memory is read into the cache. \n• Capacity miss. This occurs when a memory block is too large for the cache to hold.\n• Conflict miss. This occurs when you have memory blocks that map to the same\ncache line. In a multi-core environment, conflict misses should be attacked with\na vengeance. Conflict misses are usually systemic of an engine’s architecture that\ncontains poorly designed data structures. It’s the coupling of these data structures\nwith the general non-deterministic pattern of the threads’ executions that causes\nconflict misses to negatively affect performance.\nThe GLRThreading library includes a basic utility that will aid you in creating\ncache-aligned data structures. The principle utility is the GLRCachePad macro.\n#define GLRCachePad(Name,BytesSoFar) \\\nGLRByte Name[CACHE_ALIGNMENT – (BytesSoFar) %\nCACHE_ALIGNMENT)]\n1.4\nDesign and Implementation of a Multi-Platform Threading Engine\n43\n",
      "content_length": 2742,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "The GLRCachePad macro groups the data together in cache line(size) chunks and\non cache line boundaries. You want the access pattern of different CPUs to be sepa-\nrated by at least one cache line boundary. The cache alignment value is different from\nplatform to platform, so you might need to implement a different cache-padding\nscheme depending on your target system. The final caveat is that you want the GLR-\nCachePad call to occur at the end of a data structure; this will force the following data\nstructure to a new cache line [Culler99].\nHow to Use the GLRThreading Library\nThis section illustrates a simple example of how to use the threading system. A typical\ngame object will be defined and is called TestSystem (see Listing 1.4.1). \nListing 1.4.1\nThe Test Game Object\nclass TestSystem\n{\npublic:\nTestSystem();\n~TestSystem();\nvoid theIncredibleGameObject( void );\nvoid objectThreadPump( void );\nprivate:\nGLRThreadedTask<TestSystem> mThreadedTask;\nGLRThreadExecutionProps    *mThreadedProps;\n};\nThe TestSystem game object has two private data structures: a GLRThreadedTask\nand a GLRThreadExecutionProps. The GLRThreadedTask member is used to reference\nthis particular object and a function within the object that will be executed by the\nthreading system. Listing 1.4.2 contains an example of how to register an instantiated\nobject, as well as the function that will be distributed for execution.\nListing 1.4.2\nThe Implementation of the Game Object’s Threadable Function\nvoid TestSystem::objectThreadPump( void )\n{\nmThreadedTask.createThreadedTask(this,\n&TestSystem::theIncredibleGameObject,mThreadedProps );\nglrThreadingSystem.submitTask( &mThreadedTask );\n}\nThe sample code in Listing 1.4.3 shows how you use the TestSystem object\nfrom Listing 1.4.1. Listing 1.4.3 shows how to instantiate the two threadable \nobjects, myTestObject and myTestObject2. As noted previously, when you call\nobjectThreadPump, you create a task, which in turn obtains a thread from the thread\n44\nSection 1\nGeneral Programming \n",
      "content_length": 2011,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "pool and then submits a new task (myTestObject and myTestObject2) for execution to\nthe GLRThreadingSystem. The tasks are not instantly executed; they’ve only been\nadded to the task queue for execution. This allows a scheduler to rearrange submitted\ntasks based upon the game’s load and the tasks’ similarities. An eventual call to \ndistribute() is required to start the execution of these objects.\nListing 1.4.3\nCode That Creates and Executes the Test Objects\n//Create a couple test objects to thread\nTestSystem myTestObject;\nmyTestObject.objectThreadPump();\nTestSystem myTestObject2;\nmyTestObject2.objectThreadPump();\n//This call should be placed in your main game loop.\nglrThreadingSystem.distribute();\nOn the CD enclosed with this book, you will find a solution that allows you to\ncompile and execute this example code.\nConclusion\nThis gem has covered a lot of ground, including the architecture of a practical thread-\ning engine that is functional and efficient on multiple platforms. You’ve also looked at\na couple different methods to allocate threads, read about task execution, and finally\nlooked briefly at a method for more efficient data structure usage in a multi-core envi-\nronment. So as you begin developing or retrofitting your engine for the multi-core\nmarket, keep in mind some of the paint strokes that this article has covered and use\nthem to start painting your own masterpiece.\nReferences\n[Bevridge96] Bevridge, Jim. Multithreading Applications in Win32: The Complete\nGuide to Threads, Addison-Wesley, 1996.\n[Culler99] Culler, David E. Parallel Computer Architecture, Morgan Kaufmann, 1999.\n[Geist94] Geist, A. PVM. MIT Press, 1996.\n[Gerber04] Gerber, Richard. Programming with Hyper-Threading Technology, Intel\nPress, 2004.\n[Hughes04] Hughes, Cameron. Parallel and Distributed Programming Using C++,\nAddison-Wesley, 2004.\n[Nichols96] Nichols, Bradford. Pthreads Programming: A POSIX Standard for Better\nMultiprocessing, O’Reilly, 1996.\n1.4\nDesign and Implementation of a Multi-Platform Threading Engine\n45\n",
      "content_length": 2029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "[Ramsey05] Ramsey, Michael. “Parallel AI Development with PVM,” In Game Pro-\ngramming Gems 5, Charles River Media, 2005.\n[Ramsey08] Ramsey, Michael. A Practical Cognitive Engine for AI, To Be Published,\n2008.\n[Richter99] Richter, Jeffrey. Programming Applications for Microsoft Windows,\nMicrosoft Press, 1999.\n46\nSection 1\nGeneral Programming \n",
      "content_length": 344,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "47\n1.5\nFor Bees and Gamers: How\nto Handle Hexagonal Tiles\nThomas Jahn, King Art\ntjahn@kingart.de\nJörn Loviscach, Hochschule Bremen\njlovisca@informatik.hs-bremen.de\nG\nrids are the one of the most prominent tools to simplify complex structures and\nrelationships in order to simulate or visualize them. Their use in games ranges\nfrom the graphical tiles of 8 \u0002 8 pixels used in handheld games to the space represen-\ntation for AI agents. The typical choice, a square grid, is biased by the square’s simple\ncomputational rules; they do not show a surpassing behavior in simulation. Hexago-\nnal tiles, in contrast, offer highly attractive features in both logic and look. However,\nhexagonal grids are awkward in software development. This gem introduces concepts\nand techniques to deal with this issue.\nIntroduction\nA tiling is created when a shape or a fixed set of shapes is repeated endlessly to cover\nthe infinite plane without any gaps or overlaps. Tilings come in a two variations—\nwhereas non-periodic tilings are used to create textures, most applications rely on\nperiodic tilings, which are much easier to handle computationally. To increase sym-\nmetry, regular tilings are used, where the tiling is formed from a regular polygon such\nas a square, an equilateral hexagon, or an equilateral triangle.\nFor their space-filling efficiency, biology prefers hexagonal grids to square grids:\nThey appear in the layout of honeycombs and the placement of the light receptors in\nthe retina. Even though hexagonal grids have a number of other benefits, they require\ncomplex and thus error-prone code. Object-oriented abstraction comes to the rescue.\nThis gem describes a software design to hide the intricacies in a framework. \n",
      "content_length": 1721,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "The Pros and Cons of Hexagonal Tilings\nTo be able to judge whether a square or a hexagonal tiling fits the task best, you have\nto consider a number of aspects ranging from adjacency to the choice of a coordinate\nsystem.\nNeighborhoods and Stride Distances\nIn a square tiling, there are two common definitions of neighborhoods. Neighbors\neither have to share an edge (4-neighborhood) or it suffices that they share a vertex \n(8-neighborhood). This ambiguity has its consequences. Take a strategy game that is\nbased on square tiles as an example. Any movement is broken into a series of steps,\nwhere a step means the transition from one tile to one of its neighbor tiles. Now you\nas the developer have to make a choice. You can allow transitions in only four direc-\ntions, which means it will take 41% more steps to move the same distance along \na diagonal axis than along a vertical or horizontal axis, as shown in Figure 1.5.1.\nHowever, allowing transitions in eight directions isn’t much better—now the distance\ncovered by moving a number of steps along a diagonal axis will be larger by 41% than\nalong a vertical/horizontal axis. To avoid this distortion, diagonal transitions have to\nbe handled differently, adding to the complexity of both the code and the game’s\nrules.\n48\nSection 1\nGeneral Programming \nFIGURE 1.5.1\nMarching on a square grid\nshows fast and slow directions, no matter which\ndefinition of neighborhood is employed.\nHexagonal tilings offer an advantage here. Each tile has six equidistant neighbors,\neach of which connects to a different edge. No two tiles share only one vertex or more\nthan one edge. Thus, the notion of a neighbor isn’t ambiguous for hexagonal grids. In\naddition, the distance covered by moving a number of steps in an arbitrary direction\nwill vary by only 15%. See Figure 1.5.2.\n",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "Isotropy and Packing Density\nOf all shapes that can tile the plane, regular hexagons have the smallest perimeter for\na given area, meaning there’s no “rounder” type of tile. Thus, whenever a grid has to\nrepresent a continuous structure with no inherently preferred directions, a hexagonal\ntiling will work best. This is a major reason to pursue image processing with hexago-\nnal pixels [Middleton05].\nThanks to their compact shape, regular hexagons form the tiling with the highest\npacking density. A circular disk inscribed in a square occupies 79% of its area, as\nopposed to 91% percent for a circular disk inscribed in a hexagon. Thus, with a\nhexagonal grid you can reach the accuracy of a square grid with about 10% fewer\ncells. This is a chance to reduce the memory consumption and improve the speed of\ngrid-based algorithms by roughly the same number.\nVisual Appearance\nIn games, grids are often used to represent playing fields. This has a major impact on\nvisual appearance. A square grid is suited well to create cities and indoor scenes. The\nedges of hexagons, however, connect more smoothly, forming angles of 120 degrees.\nAssemblies of hexagonal tiles possess slightly jagged-looking outlines because no par-\nallel line segments are connected directly. This and the absence of sharp edges make\nthis type of grid more suited for the representation of natural scenes, as you can see in\nFigure 1.5.3.\n1.5\nFor Bees and Gamers: How to Handle Hexagonal Tiles\n49\nFIGURE 1.5.2\nThe length of the shortest connection on a\nhexagonal grid is close to that of a straight line.\n",
      "content_length": 1575,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "Axes of Symmetry\nA square grid can be mapped easily to an ordinary Cartesian system of integer coordi-\nnates. These can also be employed as indices of a two-dimensional array. Apart from\nthe two directions of symmetry parallel to the square’s sides, there are two diagonal\ndirections of symmetry. Although rarely seen in practice, these could serve as coordi-\nnate axes, too.\nHexagonal grids, however, possess 12 directions of symmetry that one could use\nas coordinate axes. There are two basic layouts, as shown in Figure 1.5.4—a hexago-\nnal grid with horizontally aligned tiles, where every second row is indented by half the\nwidth of a tile, and a vertically aligned grid.\n50\nSection 1\nGeneral Programming \nFIGURE 1.5.3\nWhereas square tiles are ideal for cities (left), hexagonal tiles lend themselves\nto organic shapes.\nFIGURE 1.5.4\nDepending on which direction of symmetry is employed as the x axis, the\ntiles of a hexagonal appear horizontally or vertically aligned.\n",
      "content_length": 973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "Any pair of axes that you can choose suffers from one of two defects—if the axes\nare perpendicular to each other, they will not be geometrically equivalent. In particu-\nlar, one of the axes runs parallel to some edges of the lattice whereas the other does\nnot. On top of that, you have to deal with fractional coordinates; see Figure 1.5.5. If\nthe axes indeed are chosen to be geometrically equivalent, they will enclose an angle of\n60 degrees, meaning for instance that distances can’t be computed naively through the\nPythagorean Theorem. To better display the symmetry, you might even work with\nthree barycentric coordinates, one of which is redundant.\n1.5\nFor Bees and Gamers: How to Handle Hexagonal Tiles\n51\nFIGURE 1.5.5\nA hexagonal grid allows perpendicular coordinate axes with half-integer val-\nues or skewed coordinate axes.\nMastering the Hexagonal Grid\nThanks to object-oriented programming, the issues of a hexagonal grid can be hidden\nbehind an elegant façade. Actually, this gem proposes two software layers at an increas-\ning level of abstraction: addressing and accessing.\nAddress Layer\nEvery scheme to translate an address into a spatial location on a hexagonal grid and\nvice versa has its benefits and limitations. Thus, the first step is to hide the addressing\nbehind a layer of abstraction. Data container classes supporting random access and\nclasses representing addressing schemes that map grid addresses to container elements.\nThe first option for the container is an indexed random-access container such as\nthe vector of the C++ Standard Template Library (STL). Its index can be computed\nfrom the tile’s address given in perpendicular or skewed coordinates. Because the index\nrange of the container is limited, so has to be the range of addresses. If perpendicular\n",
      "content_length": 1788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "coordinate axes are used, a rectangular section of cells can be defined through an upper\nand lower boundary for each coefficient. In this case, the index can be computed like\nindex = y * width + x.\nIf the coordinate system has skewed axes, this approach would result in a trape-\nzoidal set of cells. This can be avoided by altering the computation of the index so\nthat the indices again point to a rectangular patch of cells. In this case, the index\ncalculation could look similar to this: index = Math.Floor(y * (width + 0.5) + x).\nSee Figure 1.5.6.\n52\nSection 1\nGeneral Programming \nFIGURE 1.5.6\nThrough a shift in the index\ncomputation, skewed axes, too, can be used\nto define a rectangular domain.\nThe second option for the container is a key-based random-access container such\nas C++ STL map. Whereas these containers access data slower than indexed containers,\nyou can use the cell addresses directly as keys. The biggest benefits are that there is no\nbuilt-in limit to the address range and that empty cells don’t consume memory. Thus,\na map is a good choice for sparse data points.\nTo support a highly specific setting, you can use a standard container wrapped by\na class that implements the addressing scheme of your choice. The next step toward\nflexibility would be to make this class generic through type parameters so that it isn’t\nlimited in which kind of data it can store.\nTo achieve ultimate flexibility, you can split the functionality into two classes, keep-\ning the actual storage and the addressing scheme apart. That way it’s possible to pick the\noptimal combination of an addressing scheme and a container for the task at hand.\nHere, the class that deals with the mapping between addresses and data is called\nAddressingScheme.\n",
      "content_length": 1749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "Access Layer\nThe second layer is built on top of the addressing scheme. This layer employs the iterator\ndesign pattern, which is also used in the STL. An iterator serves as a pointer to an element\nstored in a container. All of the STL containers provide the same basic interface regarding\niterators, hiding the details of the container’s implementation. Thus, code that is based\non iterators can be used with any container. In addition to being flexible, iterators are also\nsimple to use. You can ask any container for an iterator to its first element; you can just\ncall the iterator’s next() method until you reach the last element in the list.\nA similar strategy can be employed to avoid much hassle with the addressing\nschemes of hexagonal grids. We suggest two different approaches:\n• The first one can be called a Walker class. Its instance can be set to represent any cell\nin the grid and provides an interface to read and write the target cell’s data. After \nthe initialization, the referenced cell can be changed by calling a method similar \nto an iterator’s next(). Instead of iterating through the cells in a pre-determined\nsequence, the Walker class offers a move(dir) method taking an argument that spec-\nifies one of the six natural directions on the grid. Calling this method will cause the\nWalker object to point to the neighbor of the old target that is specified by the\npassed direction; see Figure 1.5.7. This class provides free movement on the grid,\nhence its name.\n• The second approach, the Enumerator class, works exactly like an iterator, but\nonly steps through a sequence of cells that represent a specific subset of the grid—\nfor example, only the next neighbors of a given cell. The framework provides \nEnumerator classes to iterate over different neighborhoods, even customized ones\nlike all cells within a certain radius of a given center cell or all cells currently visi-\nble on the screen. In a strategy game, an Enumerator could provide access to all\ncells within the attack or viewing range of a certain unit or all cells that are occu-\npied by enemies. Decoupling the logic of the grid from your actual game logic\nmakes the code a lot cleaner and better to maintain.\n1.5\nFor Bees and Gamers: How to Handle Hexagonal Tiles\n53\nFIGURE 1.5.7\nWhereas a Walker can access any neighbor of a tile, the\nEnumerator iterates over a predefined pattern of tiles in the vicinity.\n",
      "content_length": 2400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "Implementation Tips\nMany algorithms never need to know actual addresses. They can use Walker objects to\nwrite and read the data. Thus, we suggest basing as much code as possible on an abstract\nWalker base class that defines a common interface but doesn’t rely on a specific address-\ning scheme. After you have decided on which kind of addressing you want and have\nimplemented a matching AddressingScheme class, you can write a compatible Walker\ninheriting the abstract base.\nAnother suggestion is to implement these classes as generics, such as C++’s class\ntemplates. This will allow specifying the data access independent from the data’s type.\nC# and Java (but not C++) provide specific interfaces to implement, resulting in\nneater code on the client side. For instance, by implementing C#’s IEnumeration inter-\nface, it’s possible to step through all the cells in the specific selection using a foreach\nloop with the same syntax as the regular for loop (unlike the for_each of the C++ STL).\nAssume that a cell in the grid is of type CellData and you have a generic Enumerator\ncalled Neighborhood and a Walker class CellPointer. The instance of Neighborhood is\ncreated and initialized by passing a Walker instance center defining which cells neigh-\nborhood you want neighbors to list. As Neighborhood implements the IEnumeration\ninterface, iterating through all the neighbors of the center cell becomes as easy as this:\nforeach(CellPointer<DataType> cell\nin new Neighborhood<DataType>(center))\n{\n// do something with cell\n}\nThe core functionality of the AddressingScheme is to provide data access on a grid\naddress layer. However, there is additional functionality this class could provide. Many\nuse cases require you to find a cell based on screen or world coordinates. This is trivial\nwith a rectangular grid, so we suggest the following approach. Partition a hexagonal\ngrid into rectangular sections as shown in Figure 1.5.8. To resolve a coordinate pair xy,\n54\nSection 1\nGeneral Programming \nFIGURE 1.5.8\nPartitioning the grid into rectangular\ncells allows simple hit-test computations.\n",
      "content_length": 2092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "the first step is to decide in which section the point lies. A section has one of two possi-\nble layouts; in both cases it is divided into three subsections, each associated with a dif-\nferent tile. Once you have resolved your coordinate to a location within a sector, there\nare only three choices left and it becomes trivial to compute the correct cell address.\nApplications\nTo show some practical benefits, consider three scenarios where hexagonal grids may\nbe used.\nSpatial Search\nA game world is populated by numerous entities with the ability to interact if they are\nwithin a certain range of each other. When a specific entity has to decide whether an\ninteraction is possible, it would be computationally expensive to consider all other\nactive entities. Instead, a grid can be used to preselect objects within a certain radius.\nThe grid divides the game world into tiles that behave as cells; based on its location,\neach entity is registered at one of these cells. To find all entities within a certain radius\nof a game object it suffices to consider objects registered with cells that contain points\nwithin the search radius.\nIf the region to be searched is circular, hexagonal grids would be the optimal\nchoice. Furthermore, with an adequate abstraction the code to perform the search can\nbe very simple.\nforeach(CellPointer<List<GameObject>> cell\nin new SearchZone<List<GameObject>>(center))\n{\nforeach(GameObject obj in cell.GetData())\n{\n// do something with obj\n}\n}\nEach cell consists of a list of instances of GameObject. A child SearchZone of\nthe Enumerator class is defined that allows iteration through all cells in the search\nzone. It yields a Walker object pointing to a specific cell. As the cell’s data is a list of\nGameObjects, another foreach loop can be used to iterate through the game objects\nassociated with this cell.\nPathfinding\nA number of games use tiles as building blocks for their game worlds. If agents are\nrequired to move within the world, path finding has to take place on the grid level. A\ntransition is possible only between adjacent tiles, where some neighbors may even be\nblocked, for instance, because they contain walls. This calls for a standard algorithm\nsuch as Dijkstra’s or A* [Mesdaghi04] to be implemented in the object-oriented\nframework.\n1.5\nFor Bees and Gamers: How to Handle Hexagonal Tiles\n55\n",
      "content_length": 2346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "The basic idea is to expand the start node until the goal is reached. On the address\nlevel, this is cumbersome. Six different offsets have to be applied to the current cells\naddress to access adjacent cells. For an orthogonal addressing scheme, these offsets will\nvary depending on whether the current cell is in an even or odd row (or column, if the\ngrid is aligned vertically).\nThe listing below performs a simple breadth-first search on a grid of cells to find\nthe shortest sequence of moveable cells connecting startCell with goalCell. The\nneighbors of a cell are accessed using a matching neighborhood enumerator. Thanks\nto object-oriented abstraction, the algorithm becomes independent of a specific grid\nlayout or addressing scheme.\nQueue<CellPointer<PathCell>> openCells\n= new Queue<CellPointer<PathCell>>();\nopenCells.Enqueue(startCell);\n// expand\nwhile(openCells.Count > 0)\n{\nCellPointer<PathCell> current\n= openCells.Dequeue();\nforeach(CellPointer<PathCell> cell\nin new Neighborhood(current))\n{\nif(cell.GetData().Moveable &&\ncell.GetData().ExpandedFrom == null)\n{\ncell.GetData().ExpandedFrom\n= current.GetData();\nopenCells.Enqueue(cell);\n}\n}\n}\n// resolve\nStack<PathCell> path= new Stack<PathCell>();\nPathCell pc = goalCell.GetData().ExpandedFrom;\nwhile(pc != null && pc != m_Start.GetData())\n{\npath.Push(pc);\npc = pc.ExpandedFrom;\n}\nCellular Automata\nCellular automata [Wolfram02] can be used to model and simulate complex dynamic\nsystems by letting a virtually infinite number of simple components interact locally.\nIn the two-dimensional setting, the interacting components are usually cells on a grid\nwhere the next state of a cell is computed based on the current state of itself and the\nadjacent cells.\n56\nSection 1\nGeneral Programming \n",
      "content_length": 1753,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "The direction insensitivity of hexagonal grids makes them attractive for cellular\nautomata as well, for instance for the simulation of liquids. The obvious choice for \nthe set of cells that determine a cell’s next state is itself and its six direct neighbors (see\nFigure 1.5.9), even though sometimes other neighborhoods are used. A smaller set of\nonly three neighbors may be sufficient and allow for faster simulation. In other cases,\nsix neighbors might not provide enough data, so the selection is expanded to the\nclosest 12 or even 18 cells.\n1.5\nFor Bees and Gamers: How to Handle Hexagonal Tiles\n57\nFIGURE 1.5.9\nTo start with a simple set of rules, Conway’s Game of Life,\nthe classic 2D cellular automaton, can be carried over to hexagonal tiles.\nIf the simulation code is directly operating on cell addresses, it is hard to experiment\nwith different selections of influencing cells. If, however, an Enumerator provides the\ncollection of influencing cells, the choice of cells can be changed with ease, even at\nruntime.\nConclusion\nBy introducing another layer of abstraction on top of the address layer, it is possible to\nwrite code that’s highly decoupled from the addressing scheme and the storage of the\ncell data. This not only increases maintainability and flexibility, but also greatly sim-\nplifies working on a hexagonal grid. You can keep your game logic clean of all the\nnasty details that make hexagonal grids so cumbersome to work with.\n",
      "content_length": 1453,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "References\n[Mesdaghi04] Mesdaghi, Syrus. “Path Planning Tutorial,” AI Game Programming\nWisdom 2, CD-ROM, Charles River Media Inc., 2004.\n[Middleton05] Middleton, Lee, and Sivaswamy, Jayanthi. Hexagonal Image Process-\ning—A Practical Approach, Springer New York Inc., 2005.\n[Wolfram02] Wolfram, Stephen. A New Kind of Science, Wolfram Media Inc., 2002.\n58\nSection 1\nGeneral Programming \n",
      "content_length": 386,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "59\n1.6\nA Sketch-Based Interface to\nReal-Time Strategy Games\nBased on a Cellular\nAutomaton\nCarlos A. Dietrich\nLuciana P. Nedel\nJoão L. D. Comba\nR\neal-time strategy games (RTS) are one of the most popular game genres in the\nworld. The combination of action and strategy is simply addictive, with lots of\ndevoted players spending days on game campaigns or instant battles over the Internet.\nWe have not been seeing, however, significant improvements in the RTS game-\nplay in recent years. By comparing a recent title to the very early ones, you can say\nthat now there are more units on the screen (maybe hundreds of them), new beautiful\ngraphical engines, and wider battlefields than ever before, but the essence of the\ngameplay is still the same—selecting units and defining their tasks by clicking with\nthe mouse. This process entails a very simple and efficient interface, on which players\nare usually well trained. But what should you do when the game demands more? How\ncan you control hundreds of units in a realistic and efficient way? And because the\ninterface is designed for hand-to-hand combat, what should you do when the army\nbecomes huge and there is no clear way to direct it? We are facing such situations in\ncurrent game titles, and, despite recent improvements, common unit-based interfaces\nsometimes leave players frustrated.\n",
      "content_length": 1341,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "This gem describes an alternative that may improve gameplay in such situations.\nWe propose a one-click higher-level interface that controls the movement of entire\narmies or groups of soldiers. The idea behind this approach is very simple, and can be\nillustrated with any battlefield map from the old history books, such as the one shown\nin Figure 1.6.1.\n60\nSection 1\nGeneral Programming \nFIGURE 1.6.1\nMovements of soldier troops in Italy (left) and Sicily (right) invasions during\nWWII, 1943.\nIn Figure 1.6.1, the troop movements are illustrated by arrows that indicate the\ndirection of movement of some soldiers or entire battalions. Note that there is no spe-\ncific information on the individual tasks each soldier performs or which formation\nthey kept during the movement. And why should there be? In a battlefield, it is nearly\nimpossible to call soldiers by name or give them specific tasks, such as to attack this or\nthat enemy. The commander shouts an instruction to the battalion commanders,\nwhich propagates the instruction to company commanders, and so on along the line\nof command, until some soldiers hear and follow their instructions.\nWe designed a tool that tries to simulate this behavior by allowing the users to\nsketch a path or a target on the screen that units must follow or stay. The user inter-\nface is very simple: using the mouse, the user draws a sketch line (or point) on the\nscreen, which is then converted into a path (or target) on the battlefield (see Figure\n1.6.2). This sketch creates an “influence zone” in the battlefield, and every unit inside\nthis zone must follow the sketched path or target.\nSimilar attempts to create such a tool have surfaced in recent RTS titles [Bosch06],\nbut we believe there is still lots of room for improvements, mainly in the implementa-\ntion of the interface infrastructure. The goal with this work is to augment such\napproaches with a more dynamic control, designed to be used inside the battle, which\nallows improved gameplay and strategy planning while playing RTS games. The next\nsections summarize our approach and illustrate it with some examples.\n",
      "content_length": 2120,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "Focus-Context Control Level\nIn the hierarchical structure of an army, generals do not deal directly with soldiers, but\ninstead their orders follow the chain of command until reaching lower ranked units. In\nmodern RTS interfaces, however, the general (represented by the player) deals directly\nwith soldiers (the units). This interface keeps the player focus on the hand-to-hand\ncombat instead of the context (army placement). In such interfaces, no matter how\ngood the players are, the one who clicks faster wins [Philip07]. In some circumstances,\nhowever, the player must deal with soldiers. For instance, when the combat starts, a\ndetailed control to instruct the units on how to pursue desired targets is necessary. \nThe proposed interface presents a focus context combination of both approaches—\nthe sketch-based interface that allows macro-management of the context, and a unit-\nbased interface to control the micro-management of the unit’s movement (see Figure\n1.6.3).\n1.6\nA Sketch-Based Interface to Real-Time Strategy Games\n61\nFIGURE 1.6.2\nBy using the mouse, the user draws a sketch on the screen, which pushes the\nunits inside the sketch influence zone to the desired target.\nFIGURE 1.6.3\nFocus-context interface: combination of a sketch-based interface\nthat controls the context (army disposal, at left) and unit-based interface that\ncontrols the micro-management (hand-to-hand fight, at right).\n",
      "content_length": 1407,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "It is easy to see a situation where you could arrange the army disposal with the help\nof sketches, from a far view, and turn to the unit-based interface when soldiers engage in\nhand-to-hand combat.\nImplementation Details\nThere are many ways to implement a sketch-based interface and the most important\naspect is to choose an efficient way to communicate user intentions to units on the\nbattlefield. We propose here an implementation based on a totalistic cellular automa-\nton [Weisstein07b], which is simple and fast enough to be added to any game engine.\nThe implementation has two major stages:\n• User input capture\n• Command propagation in the battlefield\nCapturing the user input is very simple, and is discussed in the “Patch Sketching”\nsection. Processing of user commands uses a totalistic cellular automaton, which is\nresponsible for iteratively spreading the command on the battlefield. This cellular\nautomaton is discussed in the section entitled “Moving the Soldiers.” Finally, in the sec-\ntion entitled “Putting It All Together,” we explain how this is used in a game interface.\nPath Sketching\nAs explained previously, we propose an interface where the user controls an army by\nsketching curves or points directly on the battlefield. This implementation is straight-\nforward, and must accomplish two tasks:\n• Capture of screen coordinates from user input\n• Projection of these coordinates onto the battlefield\nIn the first task, let’s assume that the player creates the sketch by simply clicking and\ndragging the mouse on the screen (creating a path), or simply clicking on the screen\n(creating a target). The outcome from this operation is an array of one or more points\ngiven in 2D screen coordinates (see Figure 1.6.4). These points are stored internally,\nwithout worrying if they form a continuous line, which will be taken into account in\nthe second stage. \nIn the second task, we unproject each 2D point onto a 3D position in the battle-\nfield. Using standard graphics API features such as the gluUnProject function from\nOpenGL, we map window coordinates to object coordinates using the transform and\nviewport matrices. Care must be taken with battlefield obstacles and screen positions,\nwhich have no correspondent positions on the battlefield. This can be avoided by ren-\ndering the battlefield at a coarser resolution first, and using the generated depth buffer\nas input to gluUnProject. This simplifies the test for invalid projections, and eliminates\nthe interference of battlefield obstacles. As a result we obtain an array of 3D points on\nthe battlefield, which will be the entry for the second stage of the implementation.\n62\nSection 1\nGeneral Programming \n",
      "content_length": 2683,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "Moving the Soldiers\nThe second stage of the implementation is responsible for handling the reaction of\nunits to the sketches. In this approach, each sketch is converted into forces that act\ndirectly over units by pushing them to desired positions on the battlefield. A grid\ndiscretization of the battlefield is used here, storing at each cell a vector representing \na force that indicates the direction in which you want to move the units. Forces are\nupdated throughout time and vary according with the user sketch. As Figure 1.6.5\nillustrates, forces are stronger in cells closer to the sketch, and are linearly attenuated\nas they move away from the sketch. This brings the notion of range of the sketch into\nplay, which resembles the behavior of a command in a real battlefield.\n1.6\nA Sketch-Based Interface to Real-Time Strategy Games\n63\nFIGURE 1.6.4\nIn the first stage of the implementation, 2D coordinates of the sketch (left)\nare captured and projected on the battlefield (right). The result is an array of 3D points that\nserves as input to the cellular automaton.\nFIGURE 1.6.5\nA sketch (left) and its underlying discretization as a grid of forces on the\nbattlefield (right). Forces affect the unit’s movement, pushing them to desired positions on\nthe battlefield.\n",
      "content_length": 1271,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "The proposed representation and sketch update can be efficiently done with a cel-\nlular automaton, which is simply a grid of cells encoding information that evolves\nthrough time according to a set of rules [Weisstein07]. Each rule is locally evaluated\nfor each cell based on information stored in neighboring cells. \nFigure 1.6.5 shows a rectangular grid of square cells, which is the configuration of\nthe cellular automaton. The update of the grid information is made by a very simple\ntotalistic rule. As previously stated, each cell stores a force (a vector quantity), which\nis given by the sketch position and direction on the grid (see Figure 1.6.6). Forces are\nspread out on the battlefield, attenuated by the distance to the sketch. We implement\nthis with a rule that propagates the state of each cell to its neighbors iteratively until\nthe system reaches the equilibrium. The update of each cell corresponds to averaging\nthe quantities of the neighboring cells through time.\n64\nSection 1\nGeneral Programming \nFIGURE 1.6.6\nThe 3D coordinates of the sketch (see Figure 1.6.4) are converted to 2D\ncoordinates in the cellular automaton grid. Each point on the grid corresponding to a sketch\npoint is marked with a force vector, which is iteratively smoothed through time.\nAs previously mentioned, such an automaton is formally called a totalistic cellular\nautomaton. You have a continuous range of states (because forces can be given in any\nmagnitude at each cell), a simple neighborhood (you are only looking to adjacent cells’\nstates to update each cell), and the rule depends only on the average of the values of the\nneighboring cells. This last defines a totalistic cellular automaton [Weisstein07b], a\ncomplex name for a simple technique.\nPutting It All Together\nIn order to use the proposed approach in real RTS environments, you need a way to\ndefine commands by sketches and to integrate the sketch-based interface with the cur-\nrent unit-based interfaces. We propose some useful commands that can be translated to\nthe force grid approach, and also suggest a simple way to integrate sketch-based unit\nmanagement in an existing implementation.\n",
      "content_length": 2153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "In RTS gameplay, you frequently need to move troops through the battlefield,\nguiding them around and in between natural obstacles, until you find some interest-\ning target. These two commands (guide and point targets) have a natural translation\nwith this approach. Guiding units can be made by line sketches, which are directly\nconverted to force vectors on the grid (see Figure 1.6.7). It can be necessary, however,\nto ensure that the sketch is represented by a sufficient number of points on the grid.\nThis can be accomplished by rasterizing the lines defined between the sketch points\ndirectly over the automaton grid. \nPointing out a target on the battlefield can be made by means of a small circular\nsketch (or even a point), which is then converted to a set of vectors around the sketch,\npointing to the sketch center, as shown in Figure 1.6.7. The update of vectors around\nthe sketch creates a vector field pointing to the sketch center, which pushes units\ncloser to the desired target.\n1.6\nA Sketch-Based Interface to Real-Time Strategy Games\n65\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nFIGURE 1.6.7\nDifferent types of sketches (first row), their representation in the automaton\ngrid (middle row), and the resulting vector field (last row). The sketch footprint is stored in\nthe grid, which automatically updates the vector field through time. The “erase” command\n(illustrated in the third column) is a special case, where the sketch cancels the forces of the grid.\nIt is easy to see, however, that the insertion of some vectors in the automaton grid\nis not enough to create a stable vector field. The update mechanism smooths the cell\ncontents every timestep, and this information quickly fades away. In order to prevent\nthis from happening, we suggest the use of a command lifetime. The command life-\ntime is the number of iterations in which cells containing vectors originated by the\n",
      "content_length": 1875,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "command are not updated, thus allowing more time for the command to spread on\nthe grid. The lifetime can be adjusted for each command type, being naturally high \nin long sketches (giving more time to units traveling along the battlefield) and small\nin sketches indicating target or smaller movements.\nThe integration of a sketch-based interface with an existing unit-based interface is\nsimple because both implementations are independent (one does not affect the\nother). The sketch-based approach just adds a new item in the unit movement equa-\ntion, which is a vector quantity indicating a direction to follow. All vectors are stored\nin a grid, which has a homeomorphic (one-to-one) mapping to the battlefield, which\nmeans that any unit on the battlefield can query the grid for the direction in which it\nshould go. The grid update can be made in parallel, because it is independent of any\nother process. This suggests that you can encapsulate the sketch control in a black box\nthat receives arrays of 2D points from the application interface. The sketch control\ncan then be queried for forces in any battlefield position (see Figure 1.6.8). \n66\nSection 1\nGeneral Programming \nFIGURE 1.6.8\nA possible integration of sketch- and unit-based interfaces. The\nsketch control can be seen as a black box, which receives arrays of 2D points from\nthe application interface and can be queried for forces in any battlefield position.\nConclusion\nThis gem discussed a simple and efficient approach for implementing sketch-based\ninterfaces. The efficiency comes primarily from the simplicity of the algorithms involved\n(such as projection of points and grid update through a cellular automaton), thus allow-\ning an easy port for any RTS game engine. It is important to observe, however, that the\nchosen grid resolution plays a fundamental role on the performance and memory\nrequirements of the application. In our experiments, we observed that a good compro-\nmise between reasonable sketch drawings and performance (or memory consumption)\ncan be obtained with very small grids (30 \u0002 30 or 50 \u0002 50 cells). On the other hand,\nlarge grids are in general too expensive and tend to create smaller influence zones, which\nresults in lines of units crossing the battlefield.\n",
      "content_length": 2254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "Our proposal can be easily extended to other types of grid patterns, which might\nbe necessary on other applications. For instance, you can use a hexagonal grid to avoid\nthe repetitive patterns of movement directions that we experience with rectangular\ngrids, or even more general irregular grids can be employed to represent battlefields\nwith many obstacles, such as mountains or rivers. The only modification to accom-\nmodate other grid types relies on smaller modifications of the automaton-updating\nrule, which can be easily adjusted to each grid type by accessing their neighborhood\ninformation.\nReferences\n[Bosch06] Bosch, Marc ten. “InkBattle,” available online at http://inkbattle.\nmarctenbosch.com, May 6, 2006.\n[Philip07] Philip G. “Too Many Clicks! Unit-Based Interfaces Considered Harmful,”\navailable online at http://gamasutra.com/features/20060823/goetz_01.shtml,\nJune 23, 2007.\n[Weisstein07] Weisstein, Eric W. “Cellular Automaton,” available online at\nhttp://mathworld.wolfram.com/CellularAutomaton.html, June 23, 2007.\n[Weisstein07b] Weisstein, Eric W. “Totalistic Cellular Automaton,” available online\nat http://mathworld.wolfram.com/TotalisticCellularAutomaton.html, June 23,\n2007.\n1.6\nA Sketch-Based Interface to Real-Time Strategy Games\n67\n",
      "content_length": 1260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "69\n1.7\nFoot Navigation Technique for\nFirst-Person Shooting Games\nMarcus Aurelius C. Farias\nDaniela G. Trevisan\nLuciana P. Nedel\nI\nnteraction control in first-person shooting (FPS) games is a complex task that nor-\nmally involves the use of the mouse and the keyboard simultaneously, and the mem-\norization of many shortcuts. Because FPS games are based on the character movement\nin the virtual world, a combination of left and right hands (keyboard and mouse) is\nused to control navigation and action. This gem proposes a technique where the player\ncontrols navigation with the foot, keeping both hands free for other types of interac-\ntion, such as shooting, weapon selection, or object manipulation. \nThis foot-based navigation technique allows walking forward and backward,\nturning left and right, and controlling acceleration. The tracking of the foot can be\ndone by using any motion capture device with at least two degrees of freedom, one\ntranslation and one rotation, although one or two additional degrees of freedom can\nbe useful too. \nIntroduction\nWe implemented the navigation technique in two ways. In the first implementation, a\nvery precise magnetic tracker (Flock of Birds from Ascension Technology Corporation)\nwas used to capture foot translation and rotation (see Figure 1.7.1 for an example of\nthis setup). Despite the very good results produced, this device is too expensive for\ndomestic users.\nThen, we tested a low cost and wireless solution for the same problem. In the sec-\nond implementation, we used ARToolKit—an open source library—and a regular\nWebcam to capture and identify the translation and orientation of a printed marker\n",
      "content_length": 1656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "attached to the player’s foot (see Figure 1.7.1 for an overview of the setup). Because\nthis second implementation also presented good results and can be easily reproduced\nby everyone with average programming skills, we present it in detail in this gem,\navoiding explanation of the first implementation. However, the code for the first\nimplementation is available in the CD.\nThe following sections present the fundamentals of the foot-based navigation\ntechnique, as well as how we implemented this using computer vision—in other words,\nby exploring ARToolKit features. This gem also provides a sample game developed\nwith the specific purpose of evaluating the technique’s usability and precision. User\ntests reveal that because most players are used to playing with keyboard and mouse,\nthey were not as fast and precise with the use of the foot as a video game controller as\nexpected. However, all of them completed the experience in reasonable time and were\nable to avoid all obstacles easily. These results encourage us to believe that with some\ntraining, users can rapidly increase their performance and become familiar with this\nnew interaction technique, in the same way they are becoming experts with the new\nNintendo Wii controller, for example.\n70\nSection 1\nGeneral Programming \nFIGURE 1.7.1\nEnvironment setup using Flock of Birds motion capture\ndevice (left), and a square marker pattern attached to the player’s foot and a\nWebcam (right).\nNavigating with the Foot\nThe navigation technique proposed allows the players to control their movement speed\nand direction in an FPS game by using only one of their feet. First, users can choose if\nthey want to sit down or stand up to play. Then, to start walking at a constant speed,\nthey must move their foot forward (see Figure 1.7.2(c)). The farther forward the users\nplace their feet, the faster they will move in the virtual environment. To stop, they\n",
      "content_length": 1907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "simply move back to the starting position (see Figure 1.7.2(b)). To walk backward, the\nplayers slightly move their foot a few centimeters back (see Figure 1.7.2(a)). If the\nplayers want to turn left or right, they just turn their foot left or right, as can be seen in\nFigure 1.7.2(d–f). \nThe following sections explain how to implement this navigation technique using\ncomputer vision.\n1.7\nFoot Navigation Technique for First-Person Shooting Games\n71\nFIGURE 1.7.2\nGame navigation control using the right foot: backward (a); rest position (b);\nforward (c); turn left (d); rest position (e); and turn right (f).\nRequirements for an Implementation Based on Computer Vision\nBecause the computer can interpret the users’ movements, gestures, and glances, com-\nputer vision is a potentially powerful tool to facilitate human-computer interaction.\nSome basic vision-based algorithms include tracking, shape recognition, and motion\nanalysis. In this gem, we propose the use of a marker-based approach provided by\nARToolKit (http://sourceforge.net/projects/artoolkit), an open source library for build-\ning augmented reality applications we used to capture the movement of the player’s foot.\nMore details about the ARToolKit marker-recognition principle can be found at\nhttp://www.hitl.washington.edu/artoolkit/documentation/index.html.\n",
      "content_length": 1327,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "We used a 3GHz Pentium 4 CPU, with 1GB of RAM, and an NVIDIA GeForce\n5200 graphics card. Taking into account the scenario shown in Figure 1.7.1 and consid-\nering that the captured image has 320 \u0002 240 pixels, we achieve approximately 1 milli-\nsecond for the marker recognition process. Such processing does not introduce any kind\nof delay in the interactive game response. More details involving performance studies\nand the minimum CPU requirements can be found at the ARToolKit Website.\nARToolKit can track the position and orientation of special markers—black\nsquares with a pattern in the middle—that can be easily printed and used to provide\ninteractive response to a player’s hand or body positions. In this case, the marker\nshould be printed and attached on the player’s foot in such a way that it remains\nalways visible to the Webcam, as shown in Figure 1.7.1.\nThe technique detailed in the next section requires that you print out the fiducial\nmarker defined in the file hiroPatt.pdf, available on the CD-ROM. Best performance\nis achieved if this is glued to a piece of cardboard, keeping it flat.\nInteraction implementation is almost trivial once you know how to extract the\nright information from ARToolKit. The first step consists of checking the foot rota-\ntion and detecting whether it is rotated to the left, to the right, or if it is pointing for-\nward or backward. You might need to use a different multiplier for each direction,\nbecause most people will find it easier to turn to one direction than the other depend-\ning on whether the right or the left foot is used to control the program.\nFirst, define a minimum value that will make the character start moving. When\nyou detect that the player’s foot has moved farther than this threshold, the character\nwill start moving accordingly, forward or backward. The farther the players move\ntheir foot, the faster they will go. The “Implementation” section discusses more details\nabout this.\nImplementation\nThe initialization of ARToolKit requires a few steps, but it is not hard to follow. We\nset up the camera and load the file that describes the pattern to be detected. There is\nalso an XML file (not shown here) with a few configurations, such as pixel format. It\ncan also be set up to show the settings at start-up so the users will be able to select the\npreferred camera settings. The following code is based on sample programs that come\nwith ARToolKit. \n#include <AR/config.h>\n#include <AR/video.h>\n#include <AR/param.h>\n#include <AR/ar.h>\n#include <AR/gsub_lite.h>\nARGL_CONTEXT_SETTINGS_REF argl_settings = NULL;\nint patt_id;\n72\nSection 1\nGeneral Programming \n",
      "content_length": 2630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "void setup()\n{\nconst char *cparam_name = \"Data/camera_para.dat\"; \nconst char *patt_name = \"Data/patt.hiro\";\nchar vconf[] = \"Data/WDM_camera.xml\";\nsetup_camera(cparam_name, vconf, &artcparam);\n// Set up argl library for current context.\n// Don't forget to catch these exceptions :-)\nif((argl_settings = arglSetupForCurrentContext()) == NULL){\nthrow runtime_error(\n\"Error in arglSetupForCurrentContext().\\n\");\n}\nRead the pattern definition with the default pattern file Data/patt.hiro:\nif((patt_id = arLoadPatt(patt_name)) < 0){\nthrow runtime_error(\"Pattern load error!!\");\n}\natexit(quit);\n}\npatt_id is a pattern identification previously identified.\nvoid setup_camera(\nconst char *cparam_name, char *vconf, ARParam *cparam)\n{\nARParam wparam;\nint xsize, ysize;\n// Open the video path\nif(arVideoOpen(vconf) < 0){\nthrow runtime_error(\"Unable to open connection to camera.\\n\");\n}\n// Find the size of the window\nif(arVideoInqSize(&xsize, &ysize) < 0)\nthrow runtime_error(\"Unable to set up AR camera.\");\nfprintf(\nstdout, \"Camera image size (x,y) = (%d,%d)\\n\", xsize, ysize);\n// Load the camera parameters, resize for the window and init\nif (arParamLoad(cparam_name, 1, &wparam) < 0) {\nthrow runtime_error((boost::format(\n\"Error loading parameter file %s for camera.\\n\") %\ncparam_name).str());\n}\nNext, parameters are transformed for the current image size, because camera\nparameters change depending on the image size, even if the same camera is used.\narParamChangeSize(&wparam, xsize, ysize, cparam);\n1.7\nFoot Navigation Technique for First-Person Shooting Games\n73\n",
      "content_length": 1559,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "The camera parameters are set to those read in and printed on the screen:\nfprintf(stdout, \"*** Camera Parameter ***\\n\");\narParamDisp(cparam);\narInitCparam(cparam);\nif(arVideoCapStart() != 0){\nthrow runtime_error(\n\"Unable to begin camera data capture.\\n\");\n}\n}\nThe quit function, referenced by setup, releases the resources previously allocated\nby ARToolKit.\nvoid quit()\n{\narglCleanup(argl_settings);\narVideoCapStop();\narVideoClose();\n}\nLet’s now show some sample code illustrating how to get the position of the marker\nusing ARToolKit. You first detect the marker in the frame using arDetectMarker in this\nway:\nARMarkerInfo *marker_info;\nint marker_num; // Count the amount of markers detected\narDetectMarker(image, thresh, &marker_info, &marker_num);\nThe markers found in the image by the library are returned in an array. This is\nuseful if you need to detect several markers at the same time. You can tell which\nmarker was found using marker_info[i].id and comparing it with the identifier\nreturned by arLoadPatt. In the following code, you will see how to get the transfor-\nmation matrix for the marker found at marker_info[i].\ndouble patt_centre[2] = {0.0, 0.0};\ndouble patt_width = 80.0;\ndouble patt_trans[3][4];\ndouble m[16];\narGetTransMat(&marker_info[i], patt_centre, patt_width, patt_trans);\narglCameraViewRH(patt_trans, m, 1.0);\nAs you can see, you need to call two functions: arGetTransMat and arglCamera\nViewRH. The former retrieves the transformation matrix used by ARToolKit, whereas\nthe latter converts it to the same format used by OpenGL so that you can use it to\ntransform scene objects (not needed here) or simply to see the transformation in a for-\nmat with which you are more familiar. \n74\nSection 1\nGeneral Programming \n",
      "content_length": 1742,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "You then extract the translations and the rotation around the y-axis from matrix m.\n// m[12], m[13], m[14] == x, y, z\nif(m[12] > start_position_x + mov_eps){\nwalk_fwd(m[12] * mov_mult);\n}else if(m[12] < start_position_x - mov_eps){\nwalk_bck(m[12] * mov_mult);\n}\ndouble angle_y = asin(mat[8]);\nif(angle_y > rot_eps){\nturn_left(angle_y);\n}else if(angle_y < -rot_eps){\nturn_right(angle_y);\n}\nAs explained, the previous code can be used when the camera is on the player’s\nright side, so that it can see the right leg of the user. You just need to reverse the signs\nif you prefer to put the camera on the left. \nYou will also need some multipliers and epsilon values to adjust control sensitivity.\nWe suggest 10 for move_eps and 0.3 for rot_eps as start values (you can tweak the\nvalues according to your needs). We used 0.0625 for mov_mult, but this value depends\non the scale used in your virtual world. The variable start_position_x must be ini-\ntialized with the position that you want to use as neutral; that is, a position that guar-\nantees the character will not move. The simplest implementation is to assign to the\nfirst m[12] captured by ARToolKit when the program starts.\nThere are other values that can be useful in matrix m, including m[13] and m[14],\nbecause they inform the translation in the other two axes. For example, m[13] can be\nused for jumping and m[14] for strafing (sidestepping). However, you’ll likely find\nthat it’s too hard to control rotation and walk/strafe at the same time, so choose the\ncontrols for your game wisely. The other rotation axes do not make much sense in this\ncontext, so we will not discuss them.\nA Sample Game\nIn order to evaluate usability and playability of an FPS using foot navigation, we\nimplemented a sample FPS game containing a simple map that the user can explore\nusing the navigation technique proposed here. \nIn the first contact with the game, the player can interact in the training zone, the\nfirst area of the map (see Figure 1.7.3), gaining confidence and permitting input cali-\nbration. The game starts only when the user passes over the cyan tile (see Figure\n1.7.4). In the remaining regions of the environment, there are a few obstacles and\nsome red checkpoints on the floor that become green when crossed by the user (see\nFigure 1.7.5). Collisions with obstacles, including walls, are detected and visual feed-\nback (screen changes color, as shown in Figure 1.7.6) is sent to the player each time it\n1.7\nFoot Navigation Technique for First-Person Shooting Games\n75\n",
      "content_length": 2528,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "happens. The game is over when, after passing over all checkpoints, the player attains\nthe exit shown in Figure 1.7.3 as the “end point.” The player’s goal is to complete this\ntask in the shortest time with as few collisions with obstacles and walls as possible.\n76\nSection 1\nGeneral Programming \nFIGURE 1.7.3\nSketch of the game circuit.\nFIGURE 1.7.4\nGame start indicator, indicated by the lighter\ntile in the foreground.\n",
      "content_length": 422,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "All game events are logged in a text file, so you can detect when users have trouble\navoiding a wall or finding a checkpoint. \nTests with Real Users\nWe have tested the proposed navigation technique with 15 people who played the\nsample game so we could measure their performance and hear their suggestions. We\nasked how comfortable they felt playing the game, how easy it was to use and learn,\nand how efficient they think it is. Six people found the navigation with the foot com-\nfortable, whereas four others found it more or less comfortable. Only one person\nconsidered the technique hard to use. Three people found the technique hard to learn\n(as opposed to “hard to use”). Regarding the efficiency, three people thought the tech-\nnique is inefficient, seven rated it as more or less efficient, and five people found it is\nefficient.\n1.7\nFoot Navigation Technique for First-Person Shooting Games\n77\nFIGURE 1.7.5\nView of the scenario with a checkpoint to be reached (left) and the same\ncheckpoint reached (right).\nFIGURE 1.7.6\nTwo frames of a game—before (left) and after (right) a collision with an\nobstacle.\n",
      "content_length": 1112,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "This data shows us that the interaction is intuitive and reasonably easy to use after\nsome practice. In the sample game, we measured the number of collisions and the time\nthe users took to cross all checkpoints and reach the end goal. Because most users had\nexperience in the use of a keyboard and mouse combination, it is expected that they\nwere not as fast when using their foot as a video game controller. However, everyone\ncompleted the task in a reasonable time and easily avoided all obstacles. We noticed\nthat it is especially easy to move fast and suddenly stop, because the acceleration control\nis intuitive (you just move your foot forward as far as you can, as long as the camera still\nsees it) and when you want to stop, you need only to go back to the rest position).\nConclusion\nThis gem presented a new technique to allow navigation for FPS games using one of\nthe player’s feet. It also described a low-cost solution to implement it using computer\nvision, more specifically ARToolKit open source library that easily tracks foot move-\nments and then transforms them into interaction controls for the game environment. \nThere are some limitations inherent in purely computer-vision-based systems.\nNaturally, the marker position is computed only when the tracking marks are in the\ncamera’s field of view. This may limit the movement of the interaction, also meaning\nthat if the users cover up part of the pattern with other objects, navigation is stopped.\nOther factors such as range issues, pattern complexity, marker orientation relative to the camera,\nand lighting conditions influence detection of the pattern.\nOn the other hand, there are basically three advantages in this technique against\ntraditional approaches. First, by using a tracker, users have more degrees of freedom to\nwork with. The users can move their foot in the 1D, 2D, or 3D space and rotate\naround one axis (left-right rotation). Moreover, the mouse and keyboard could be used\nin other ways, because navigation is no longer a concern. For example, they can be used\nfor aiming, shooting, selecting, and manipulating on-screen objects. Lastly, using the\nwhole body to interact with the game gives a deeper immersion for the player, as games\nfor the Nintendo Wii console have shown.\nFuture Work\nThere are many ways to explore the possibilities of this interaction technique. The\nfirst one is to add new simple commands, such as jump or sidestepping. Another\nalternative is to reuse the mouse and keyboard commands that are no longer needed\nand to assign more ergonomic commands to them.\nMultiplayer games are also an untapped possibility, because the marker-based\ninteraction technique, as well as the Flock of Birds, allow a multiplayer functionality.\nIt is possible to attach and track a different marker for each player as long as the Web-\ncam can capture them. If this is not the case, it is always possible to use two or more\ncameras concurrently.\n78\nSection 1\nGeneral Programming \n",
      "content_length": 2969,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "To address portability, as well as to avoid the occlusion problem that can occur\nbetween the Webcam and the markers, some kind of interaction based on remote\ncontrolled devices could be implemented. The main idea remains the same, namely:\nthe controller should be attached to the user’s foot while movements should be per-\nformed in exactly the same way. Finally, we estimate a different game design, such as\na Super Monkey Ball style–game, could be more attractive to this type of control.\nAcknowledgments\nThe authors would like to thank Fábio Dapper and Alexandre Azevedo for their work\nin the conception and first implementation of this technique, as well as all people who\nplayed the game, giving us valuable feedback. Finally, we thank the Brazilian Council\nfor Research and Development (CNPq) for financial support.\n1.7\nFoot Navigation Technique for First-Person Shooting Games\n79\n",
      "content_length": 887,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "81\n1.8\nDeferred Function Call\nInvocation System\nMark Jawad, Nintendo of America Inc.\nmark.jawad@gmail.com\nI\nt seems that lately it has become common for most computing systems to be designed\naround multiple processors; in fact that appears to be a core design axiom within the\nengineering community right now. Modern video game machines are no exception to\nthe trend, with multi-core CPU designs being prevalent in many of the game machines\non sale today. Additionally, most of them rely on auxiliary processing acceleration chips\nsuch as programmable IO controllers, DMA engines, math co-processors, and so on.\nThese run in parallel with the CPU(s), and are there as part of the system so that we as\nartists can push our craft ever farther. Each of these components may complete their\ntasks independently of the others, but all send along a notification to the game when the\ntask is done (usually in the form of an interrupt), so that the game can schedule other\nwork. \nThe way in which a game handles these notifications can either lead to a fairly\npainless and bug-free experience, or to a frustrating world of head-scratching, bug-\nchasing hurt. This article discusses the implications of asynchronous events and other\ntiming-related issues, and provides a system to handle them gracefully.\nA Matter of Time\nFrom the game’s point of view, these notifications may happen at any time, and thus\nshould be treated as true asynchronous events. These event notifications are great because\nthe game can run on the main processor at the pace it desires, while other processors do\nauxiliary work at whatever rate they can. However, there are some problems. If handled\nimproperly, these notifications can lead to timing-related bugs or system instability. \nTiming-related bugs are notoriously difficult to track down, and can happen if you try to\nuse memory before another system client is done with it, or if you change the game state\nwithout proper synchronization primitives. System instability bugs are even worse, and\ncan occur if a callback/interrupt handler runs for too long during an interrupt period,\nthus causing other interrupt signals (or subsequent interrupt signals of the same type) to\nbe missed. Such a situation can lead to all sorts of odd program behavior.\n",
      "content_length": 2271,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "Developers working on handheld gaming systems are faced not only with these\nissues but additional ones as well. A major focus that these systems have is the concept\nof a vertical blanking period, which is the point in time where the graphics engine(s)\ngo idle while the display device prepares for the next frame of output. It is during this\nsmall window of time that developers are allowed to access the memory and registers\nof the graphics system. During this time, you need to quickly determine what new\ndata is to be uploaded, as well as what settings need to change on the graphics chip. A\ndeveloper must make the changes and data uploads as quickly as possible. If they fail\nto do the work within the window, the graphics may show corruption or other notice-\nable artifacts.\nThese various issues all deal with time in some form or another—time is always\nan enemy of game developers and we never seem to get enough of it. Because we\nprobably can’t get more time in which to do work, we’ll have to settle for making\nsmarter use of the time that we’ve got. One approach is that instead of doing all the\nwork at once, you do some of it now and the rest of it later. In essence, you’re making\na decision now but deferring the actual work to some point in the future. Because\nmost work is handled by making function calls, this gem uses a system that queues up\nfunction calls and their parameters, and has the ability to invoke them sometime\nlater—hence, a deferred function call system.\nCase Studies\nLet’s examine the vertical blank period where you have very limited time. The ideal\nsituation is to not do any time-consuming logic at all during this window; rather you\nshould be purely focused on uploading lots of new data as quickly as possible. So why\nnot do all of the logic relating to what data to upload (and where to put it) earlier on\nin the frame when you are not under such extreme time pressure? You queue up\nsource and destination addresses, transfer size, and perhaps take note of the “type” of\ndata (textures, palettes, and so on). Then when the blanking window opens, you run\nthrough the queue and do all of the transfers.\nBut what happens if the type of data determines the choice of function used to\nupload the data? Well, then you have a switch statement, jump table, or cascading\nseries of if statements in order to determine which function to call. Such an approach\nisn’t worth a second thought on a PC or home console. On a portable machine with a\nlow clock speed it is definitely worth a second thought, especially given the small\nblanking window coupled with how precious each cycle is. Therefore, the ideal\napproach is to also pre-determine the function to call when you’re setting up the\naddresses and transfer sizes. Then, during the vertical blanking window, all you have\nto do is load the function’s address along with the necessary parameters and jump off\nto it. In essence, you’re setting up the function call earlier in the game loop, but defer-\nring the actual invocation until the vertical blank period arrives.\n82\nSection 1\nGeneral Programming \n",
      "content_length": 3082,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "The same tactic can be used on home consoles to deal with asynchronous notifi-\ncations. Maybe you get a callback that informs you that a certain file read has com-\npleted, or a memory card was inserted, or that the user has inserted or removed a\ncontroller peripheral. Some of these situations require more work than others, but\nmost of them are such that you don’t need to do the bulk of the work right there and\nthen. Yes, the game logic needs to know about the file read completion, or the new\ncontroller, but why not schedule the real processing of the event sometime later near\nthe end of the current processing frame?\nQueue up the incoming notification parameters (taking care to save any important\ntemporal data that might get lost) and deal with them later. This way you exit the call-\nback/interrupt as fast as possible, which is always a good thing. By hoisting the notifi-\ncation processing out of the interrupt handler and into a dedicated place in your game\nloop, you help keep the game behavior deterministic. This in turn nearly eliminates\ntiming-related bugs. An additional benefit of the approach is that you can guarantee\nthat no one on the team will mistakenly run a process that takes “too long” while inter-\nrupts are disabled, because the handling process is now at a known point in the simu-\nlation loop and not in an interrupt handler. So it can take all the time it needs.\nCategorizing a Function Call\nMost functions are set up to take their arguments directly in the function parameters,\nsuch as this one from the C standard library\nvoid *memccpy(void *dest, const void *src, int c, size_t count);\nwhich takes four parameters in its argument list. This is the most common function\ncall type in C-derived languages today, and is categorized as taking “direct” parame-\nters. Other functions, such as this one from the Windows SDK\nATOM RegisterClassEx(CONST WNDCLASSEX *lpwcx);\ntake a single argument, but really that argument just points to a control structure\nwhere the 12 “actual” parameters reside. This type of call can be categorized as a func-\ntion that takes an “indirect” parameter. Often, the parameter for the indirect argu-\nment sits on the stack of the callee (as opposed to being stored in the heap), and is\ntherefore lost once the callee exits. \nOn the surface, it would appear that the second type is just a subset of the first.\nHowever, there is an important distinction that you need to make note of. As men-\ntioned earlier, sometimes you need to store data temporally. In the case of a deferred\nfunction, where do you store indirect argument data blocks? The callee function that\nwould normally have created the argument structure will be long gone by the time the\ndeferred function is called (and with it, the data that was in its stack frame). The solu-\ntion is to hand out a pool of memory large enough to store the argument. This pool\n1.8\nDeferred Function Call Invocation System\n83\n",
      "content_length": 2929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "will come from the deferred system itself, and the argument will be constructed in-\nplace there instead of on the stack.\nA Look at the System\nThe header file for the system, deferred_proc.h, is quite small, and should be trivial to\nintegrate into your game. The header contains one function for initializing an instance\nof the system, a couple of functions and macros for adding deferred calls to lists of\ndirect-argument (DA) or indirect-argument (IA) functions, and one for executing the\ndeferred functions (and then resetting the lists when that is done).\nThe majority of the code is presented in C, although there is one function that\nmust be written in assembly code. That function is the deferred function caller. This\none function is platform-dependent, and must take into account the ABI (application\nbinary interface) of the platform that it’s being run on. So to port the system, you\nneed only to rewrite the deferred function caller. \nPlease note that the system presented herein is limited to making function calls\nthat only use arguments kept in general purpose registers (GPRs). Floating point val-\nues, native floating point vectors, or other data types that aren’t intended for a general\npurpose register are not allowed as parameters to the deferred functions. This is done\nto keep the system simple, although support for such things could be added if needed.\nAlso in keeping things simple the system allows a maximum of four parameters.\nI’ve found four parameters to be sufficient for most cases. One consideration was\nthat if more than four are used, you might have to transfer some of the parameters in\nregisters and others on the stack, depending on the platform being targeted, which\nagain could increase complexity—and complexity is something that this system\nactively tries to avoid.\nBecause this system is limited to four direct parameters, you must use the indirect\ncall for anything where five or more parameters are needed. (Don’t forget that the hid-\nden this parameter of C++ instance functions counts as one of the four parameters\nthat you can accept for direct-argument function calls!)\nThe deferred function caller, dfpProcessAndClear, may be a little difficult to\nunderstand because it’s in assembly, but the idea behind it is very simple. All it has to\ndo is loop through the contents of the list and dispatch (call) each entry. At each loop\niteration, it needs to load at minimum a control word that describes the following:\n• The function type (DA or IA)\n• The number of GPR parameters it should load\n• Any additional bytes of data that the call took from the list\nOf course, it will also need to load the target function’s address. Then it needs to\nload any parameters stored for use with the function. Once this information is loaded\nit can branch off to the target function. When that function returns, you go on to the\nnext iteration of the loop. Once done, you reset the list and exit. Please note that the\nsample code is not thread-safe. Please see the CD for the complete source code.\n84\nSection 1\nGeneral Programming \n",
      "content_length": 3063,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "Conclusion\nDeferred functions are an extremely useful tool for game developers on all platforms,\nespecially those working on home consoles or handheld systems. Their existence can\nhelp make the most of time critical sections of code, such as the vertical blank period\nand interrupt processing sequences, and the system that tracks them can handle many\ndifferent types of function signatures. The idea is flexible, easily extendable, efficient,\nand very portable—traits that all of us can surely appreciate.\nReferences\n[Earnshaw07] Earnshaw, Richard. “Procedure Call Standard for the ARM Architec-\nture,” available online at http://www.arm.com/pdfs/aapcs.pdf, January 19, 2007.\n1.8\nDeferred Function Call Invocation System\n85\n",
      "content_length": 725,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "87\n1.9\nMultithread Job and\nDependency System\nJulien Hamaide\nJulien.hamaide@gmail.com\nT\nhis gem puts next generation multi-core capabilities into the hands of all program-\nmers without the need to comprehend more complex multithreading concepts to\ncreate tasks. By providing a simple system that automatically manages dependencies\nbetween tasks, there is almost no need of other more specific synchronization mecha-\nnisms. The system is adapted for small- to medium-sized tasks such as animation\nblending or particle system updates.\nIntroduction\nWhen thinking about multithreading, synchronization problems come quickly to mind.\nDeadlock is a fear for every programmer. Even for small projects, a multithreading\nsolution is typically rewritten every time due to the interdependencies unique to the\nproject. Therefore, programmers require knowledge of synchronization primitives and\ntheir intricacies. For example, if the system is time-critical, a lock-free algorithm must be\nused. Additionally, the more complex the system, the higher the chance of creating bugs. \nThe primary goal of the system is to provide a cross-platform framework that hides\nthe complexity of multithreading issues. Any programmer, even those not familiar with\ncommon multithreading concepts, should be able to use the system. There is no con-\ncept of threads in the system; it is replaced by the concept of jobs, which are defined as\nunits of work.\nThe process of creating a job is shown in the following code. Cross-platform\ncompatibility is achieved by encapsulating primitives such as critical sections and\nthreads in classes. The framework code is then created with those classes. The demo\non the CD contains the job manager and the wrapper classes.\nvoid MultithreadEntryFunction( void * context )\n{\n//Do your stuff here\n}\nPARALLEL_JOB_HANDLE handle;\n",
      "content_length": 1829,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "handle = PARALLEL_JOB_MANAGER_CreateJob( \n&MultithreadEntryFunction, context);\nPARALLEL_JOB_MANAGER_ScheduleJob( handle );\nThe performance of the system is also optimized by preventing thread creation\nand deletion for small-sized tasks. The framework creates a thread pool that executes\nthe tasks given to the system. The threads are created only once at the launch of the\napplication. The number of threads depends on the platform and is dynamically eval-\nuated if necessary. (On a PC, the number of cores on the target platform may not be\nknown at compile time.) This approach is similar to OpenMP’s approach to distribut-\ning work over multiple threads [OpenMP].\nThe system also supports prioritization of tasks. If some tasks take more time or\nhave many dependent tasks, their priority can be increased to ensure early execution.\nSupport for idle tasks is reviewed in the future work section, but has not yet been\nimplemented.\nThe synchronization of jobs is handled by a dependency system. It allows the\ncreation of synchronization points and job dependencies. The dependency system is\ncovered in detail in following sections.\nThe Job System\nThe job system is mainly composed of four types of objects:\n• The job, which is unit of work delimited in a callback function. It must be\ndesigned to be thread-safe.\n• The manager, which maintains the job list by priority and type.\n• The scheduler, which chooses the task that suits best depending on priority and\ntype.\n• The workers, which execute the jobs assigned to them.\nJob\nJobs are represented by the class shown in the following code. It simply encapsulates a\nclosure (a function with a predefined argument). The structure also contains the pri-\nority and the handle of the job. PARALLEL_JOB_PRIORITY is an enum containing classic\npriority values (that is, High, Low, and so on). The handle identifies the job in the\nsystem. It is the object that is used outside the system to reference a job. The Type field\nis explained later.\nclass PARALLEL_JOB\n{\npublic :\nvoid Execute()\n{\nFunction( Context );\n}\n88\nSection 1\nGeneral Programming \n",
      "content_length": 2087,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "private :\nPARALLEL_JOB_HANDLE\nJobHandle;\nPARALLEL_JOB_PRIORITY\nPriority;\nPARALLEL_JOB_TYPE\nType;\nPARALLEL_JOB_FUNCTION\nFunction;\nvoid\n* Context;\n};\nManager\nThe manager class is a singleton providing the public interface of the system. As the\nsystem’s central point, it maintains the list of jobs in a multithread-safe manner. The\nmanager is responsible for worker and scheduler thread creation and initialization.\nThe interface of the manager is shown in the following code. The creation and sched-\nuling of jobs is separated to allow the user to add dependencies to and on the created\njob. In the demo code, every call is protected with critical sections [MSDN1]. Lock-\nfree algorithms are better choices but increase the complexity of the system. To ease\nthe understanding of concepts, only the critical section version is given. Implementa-\ntion of lock-free algorithms is left as an exercise for the developers.\ntypedef void (*PARALLEL_JOB_FUNCTION )( void* );\nPARALLEL_JOB_HANDLE CreateJob(\nPARALLEL_JOB_FUNCTION function,\nvoid * context,\nPARALLEL_JOB_PRIORITY priority = PARALLEL_JOB_PRIORITY_Default\n);\nPARALLEL_JOB_HANDLE CreateAndScheduleJob(\nPARALLEL_JOB_FUNCTION function,\nvoid * context,\nPARALLEL_JOB_PRIORITY priority = PARALLEL_JOB_PRIORITY_Default\n);\nvoid ScheduleJob(\nPARALLEL_JOB_HANDLE job_handle\n);\nThe PARALLEL_JOB_HANDLE is a structure that contains a unique identifier and a\ndependency index. The dependency index is covered in the following sections.\nScheduler\nThe scheduler is a thread object that waits for the worker threads to finish. As soon as\none is free, it selects the next job, assigns it to the worker thread, and puts itself back\nto sleep. The following code shows the main loop of the scheduler in pseudocode. \n1.9\nMultithread Job and Dependency System\n89\n",
      "content_length": 1792,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "PARALLEL_JOB_MANAGER_GetNextJob returns the best job to be executed next. The\ndecision rules are presented later. The WaitForJobEvent allows the scheduler to sleep\nif there is no new job available and some worker threads are free. When new jobs are\navailable, the WaitForJobEvent is signaled; otherwise, it is reset [MSDN2].\nwhile ( !ThreadMustStop )\n{\nthread_index\n= PARALLEL_WaitMultipleObjects( worker_thread_table );\nif( PARALLEL_JOB_MANAGER_GetNextJob( next_job, thread_index ) )\n{\nworker_thread_table[ thread_index ]->SetAssignedJob(next_job);\nworker_thread_table[ thread_index ]->WakeUp();\n}\nelse\n{\nWaitForJobEvent.Wait();\n}\n}\nWorker Threads\nThe worker does the dirty work. When the operating system permits, it is assigned to\na processor. The pseudocode is shown in the following code. The worker waits for\nDataIsReadyEvent to be signaled, meaning it has been assigned to a new job. It then\nexecutes it. When finished, it informs the dependency manager that its job is finished.\nFinally, it signals the scheduler that it is waiting for a new job. \nIn the implementation, the number of worker threads is set to the number of\navailable processors. If the main thread is always busy, it can be useful to set the num-\nber of worker threads to the number of available processors, minus one. \nwhile ( !ThreadMustStop )\n{\nPARALLEL_WaitObject( DataIsReadyEvent, INFINITE );\nAssignedJob.Execute();\nPARALLEL_DEPENDENCY_MANAGER_SetJobIsFinished( AssignedJob );\nWaitingForDataEvent.Signal();\n}\nCache Coherency\nTo ensure code and data cache coherency, both jobs and worker threads have been\nassigned types. The type of the job can be anything you want, depending on your sys-\ntem. In this implementation, we choose types such as particle systems, animation,\npathfinding, and so on. The type will be used in job selection, so choose them care-\nfully. The cache coherency is really specific to each platform. Some tuning can be nec-\nessary to achieve the maximum speed.\n90\nSection 1\nGeneral Programming \n",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "Job Selection \nWhen the scheduler asks for the next job to execute, the manager uses simple rules to\nchoose it. The pseudocode is shown here after. The selection tries to balance between\nthread type changes and high priority tasks. This algorithm is not adaptive, but the\nhighest priority thread can be boosted every time a lower priority thread is chosen due\nto a type difference. In this way, you can allow scheduling of only two or three lower\npriority threads before the highest priority thread is scheduled.\ncurrent_type = type of worker thread\nif( job of type current_type exists )\n{\nnew_job = job of type current_type with highest priority\nif( priority of new_job – highest priority available > 2 )\n{\nnew_job = job with highest priority\nworker thread type = new job type\n}\n}\nelse\n{\nnew_job = job with highest priority\nworker thread type = new job type\n}\nThe Dependency Manager\nThe dependency manager is an object-based system that ensures synchronization of\ntasks. The current implementation has two types of entry: jobs and groups. The system\nconstructs a graph of dependencies between entries. Groups allow the users to create\nsynchronization points such as the start of rendering. Figure 1.9.1 shows a typical\ndependency graph created by the system.\n1.9\nMultithread Job and Dependency System\n91\nFIGURE 1.9.1\nExample dependency graph.\n",
      "content_length": 1344,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "The Dependency Graph\nThe dependency graph is stored inside dependency entries. Each stores a list of depen-\ndent objects. A dependency can be in two states: met or blocked. If a dependency entry\nis met, it means that every other entry that depends on it can be executed. If only one\nof its dependencies is blocked, an entry is also blocked.\nTo prevent polling of dependencies, an entry contains a dependency count. For\neach blocked dependency an entry depends upon, the counter is increased. When the\ncounter is zero, the dependency is met; otherwise, it is blocked. When a dependency\nenters the met state, it iterates over all its dependent objects to decrease their counts.\nSimilarly, when a dependency becomes blocking, it iterates all its dependent objects\nto increase their counts. The met and blocked information is propagated along the\ngraph. Figures 1.9.2 and 1.9.3 show a step of propagation. When job 1 becomes met,\nthe Animation group also becomes met. The dependency count of job 3 decreases by\n1, whereas the dependency count of PreRender group decreases by 2.\n92\nSection 1\nGeneral Programming \nFIGURE 1.9.2\nInitial graph with dependency count.\nFIGURE 1.9.3\nGraph after propagation.\n",
      "content_length": 1196,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "Dependency Storage\nThe dependency table is a sparse vector of pointers to entries. An index dispenser is\nused to allocate a free slot to the current dependencies. The table grows if necessary,\nbut never shrinks. When an entry is created, an index is requested from the index dis-\npenser. On deletion its index is recycled and will be used by the next entry created.\nThis recycling means that the dependency cannot be identified by its index alone.\nThe PARALLEL_DEPENDENCY_IDENTIFIER has been introduced to solve this problem.\nThis structure contains an index to the table of dependency entries and a unique\nindex. This identifier serves as a handle for the users. \nThe following code shows the creation of a dependency group and links. In this\nexample, the PreRender group will wait until the Animation group is met. This can be\nused to ensure that all animations are computed before the rendering occurs. All ani-\nmation jobs will set up a dependency over the Animation group. Figure 1.9.4 shows\nthe graph created by such a construction. If a dependency no longer exists (that is, the\njob is finished), it is considered as met. \nPARALLEL_DEPENDENCY_IDENTIFIER\nanimation_identifier, prerender_identifier;\nprerender_identifier\n= PARALLEL_DEPENDENCY_MANAGER_CreateEntry( \"PreRender\nSynchronization\");\nanimation_identifier\n= PARALLEL_DEPENDENCY_MANAGER_CreateEntry( \"Animation \nSynchronization\");\nPARALLEL_DEPENDENCY_MANAGER_AddDependency(\nanimation_identifier, prerender_identifier );\n// during the game\nblend_job_handle\n= PARALLEL_JOB_MANAGER_CreateJob( &MultithreadBlendAnim, context);\nPARALLEL_DEPENDENCY_MANAGER_AddDependency(\nblend_job_handle, animation_identifier );\nPARALLEL_JOB_MANAGER_ScheduleJob( handle );\n1.9\nMultithread Job and Dependency System\n93\nFIGURE 1.9.4\nDependency graph created by previous example.\n",
      "content_length": 1819,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "This example raises a problem—the job is a volatile object and when finished, it\nis destructed. Groups are stable entries. To address these needs, two types of link have\nbeen created: \n• Dynamic link: As soon as the dependency is met, the link is removed.\n• Static link: The link is established at launch and always remains valid.\nThe entry contains two tables, one for static links and one for dynamic links.\nWhen the entry is met, the dynamic link table is emptied. In the previous example,\nthe link between the Animation Synchronization entry and the PreRender Synchro-\nnization entry is transformed into a static link. This example is used in the demo.\nGroup Entry\nA group entry creates a synchronization point. The manager provides a way to wait until\nthe dependency is met. A call to PARALLEL_DEPENDENCY_MANAGER_WaitForDependency\nwill stall until the dependency is met. It is useful, for example, to wait for all computa-\ntions to finish before starting the rendering. This behavior is implemented with events.\nAn event is created for each instance of a group. This means that an event is created \nfor each group, even if you don’t want to be able to wait for it in your code. If you want\nto avoid this waste of events, you can create two types of groups—waitable and non-\nwaitable groups. \nJob Entry\nA job entry is used for two purposes: \n• To synchronize other entries with the job\n• To synchronize the launch of the job\nThe entry is created with a dependency count set to one. This represents the\ndependency that the execution of the job sets on the entry. To prevent the need for\ntwo entries, one for the launch and one for the end of the job, the entry count is\npolled by the job manager. If the entry’s dependency count is one, there is no depen-\ndency left other than the job execution itself, so the job can be executed. As shown in\nthe worker thread pseudocode, it reports that the job is finished to the dependency\nmanager. The dependency count is then set to zero, and the entry becomes met and\npropagates the information to all entries that depend on it.\nFuture Work\nThe following sections discuss the future work—areas of this system that might be\ngood candidates for enhancements and extensions.\n94\nSection 1\nGeneral Programming \n",
      "content_length": 2250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "Idle Tasks and Preemption\nIn the presented system, every started task occupies its worker thread until it is fin-\nished. This constraint prevents users from scheduling low priority tasks that can be\ncomputed over several frames. Three solutions are possible to fix this problem: \n• Preemption: Low-priority tasks could be preempted if a new job is pushed into the\nsystem. Not all operating systems allow the users to code their own version of\nthread context switching. This solution is also platform-specific.\n• Cooperative multithreading: Low-priority tasks can be implemented using coopera-\ntive multithreading, but each task should use a special function to allow the system\nthe opportunity to execute higher-priority jobs. Such a system can be implemented\nusing, for example, Windows fibers [MSDN3].\n• Low-priority thread: The system can create new lower-priority threads, letting the\noperating system scheduler preempt them when other worker threads are work-\ning. This solution is cross-platform and easy to implement as long as the OS\nscheduler is good enough.\nThe first solution is the best from a control point of view. No special work is needed\nin the job code, you control exactly when the job is preempted but it is platform-specific\nand can be tricky to implement. The second solution is easier to implement, but it is still\nplatform-specific. Another drawback is that the job code must be adapted to allow pre-\nemption. The third solution is by far the easiest to implement, but you don’t have much\ncontrol about when and how your thread will be preempted. Ideally, the third solution\nshould be implemented first, with a view to switching to other solutions if necessary.\nIntegration of Synchronization Primitives \ninto the Dependency System\nThe dependency system supports only two types of entry: job and group. It means that\nyou can’t interface the system with an already existing synchronization mechanism. For\nexample, a loading thread can use semaphores to communicate with other threads. If you\nwant your job to wait for some resource to be loaded, the system does not allow it. A solu-\ntion is to extend the entry to other types: semaphores, events, and so on. The dependency\nsystem has been designed to be extensible. The PARALLEL_DEPENDENCY_ENTRY can be\nderived to support primitives easily. Virtual functions that inform the dependency state\nof the new synchronization mechanism must be written. The job is complete when the\nimplementation of those virtual functions has been written. The new system is then\nready to interact with the dependency system.\nConclusion\nThe presented system provides the power of multithreading to all programmers. It\nabstracts the complexity and the danger of using synchronization primitives. The critical\ncode is consolidated in a single place and synchronization issues are solved only once. By\n1.9\nMultithread Job and Dependency System\n95\n",
      "content_length": 2896,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "providing a dependency graph system, the jobs can be chained without any other action\nthan creating a dependency link between them. Already rather complete, the system has\nbeen designed to be extensible. The implementation is cross-platform; the only things\nyou need to port are the wrapper classes provided in the demo package (mainly thread,\ncritical section, and event). Performance is also targeted by limiting the overhead of\nthread creation. The system can also interface with existing synchronization techniques\nwith the help of the PARALLEL_DEPENDENCY_ENTRY interface. All your programmers should\nnow be able to create and schedule a job enjoying considerably less threading-related\ncomplexity than without this gem. Enjoy!\nReferences\n[MSDN1] “Critical Section Objects (Windows),” available online at http://msdn2.\nmicrosoft.com/en-us/library/ms682530.aspx, June 1, 2007.\n[MSDN2] “Event Objects (Windows),” available online at http://msdn2.microsoft.\ncom/en-us/library/ms682655.aspx, June 1, 2007.\n[MSDN3] “Fibers (Windows),” available online at http://msdn2.microsoft.com/en-\nus/library/ms682661.aspx, June 1, 2007.\n[OpenMP] OpenMP Architecture Review Board. “OpenMP: Simple, Portable, Scal-\nable SMP Programming,” available online at http://www.openmp.org.\n96\nSection 1\nGeneral Programming \n",
      "content_length": 1301,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "97\n1.10\nAdvanced Debugging\nTechniques\nMartin Fleisz\nMartin.fleisz@kabsi.at\nD\nue to the high expectations that players have in games nowadays, development\ngets more and more complex. Adding new features or supporting new technolo-\ngies often requires more code to be written, which automatically leads to more bugs.\nGame projects are usually tightly scheduled and hard-to-find bugs are often the cause\nfor delays. Although you cannot avoid that erroneous code is written, you can try to\nimprove the process of finding and fixing these errors. If an application crashes or\ndoes something wrong, information about the crash becomes the most important\ndata in order to find the cause of a malfunction. Therefore, this gem presents a small\nframework that detects and reports a vast number of programming errors and can be\neasily integrated into any existing project.\nApplication Crashes\nThe reasons that an application crashes are manifold. Stack overflows, dividing by\nzero, or accessing an invalid memory address are just a few of them. All of these errors\nare signaled to the application through exceptions, which, if you fail to handle them\nproperly, will terminate the game. You can distinguish between two different types of\nexceptions. Asynchronous exceptions are unexpected and often caused by the hardware\n(for example, when your application is accessing an invalid memory address). \nSynchronous exceptions are expected and usually handled in an organized manner.\nThese exceptions are explicitly thrown by the application—that is, using the throw\nkeyword in C++. \nException Handling\nThe C++ language offers built-in support for handling synchronous exceptions\n(through the try/throw/catch keywords). However, this is not the case for handling\nasynchronous exceptions. Their handling depends on how the underlying platform\nimplements them.\n",
      "content_length": 1843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "Microsoft unified the handling of both exception types on their Windows plat-\nforms and refers to it under the term Structured Exception Handling (SEH). If you\nwant to know how SEH and the new Vectored Exception Handling technologies\nwork in detail, refer to [Pietrek97] and [Pietrek01].\nWhat you have to know is what happens if the operating system does not find a\nhandler for an exception. In this case, the UnhandledExceptionFilter function is called\nby the kernel. [Pietrek97] shows a pseudocode sample of what this function does \nin detail. The most interesting part is that the function will execute a user-defined\ncallback that you can register using the SetUnhandledExceptionFilter API. Using this\ncallback, you will be notified when your application crashes so that you can do the\nerror reporting.\nUNIX-based operating systems use a different approach. In the case of an asyn-\nchronous exception, the system sends a signal to the application, at which time its\nnormal execution flow is stopped and the application’s signal handler is called. Using\nthe sigaction function, you can install custom signal handlers where you are going to\ndo your error reporting later on.\nBesides doing some error reporting, you might want to consider saving the cur-\nrent game in your exception handler. Nothing bothers a gamer more than a crashing\ngame after having played for hours and not having saved. Of course such tasks should\nall be done after you have finished your reporting.\nReporting Unhandled Exceptions\nIf an unhandled exception occurs and the process is being debugged, the debugger\nwill break at the code location that caused the error. Of course it would be nice to\nhave similar information about crashes even if there is no debugger attached to the\napplication. For this purpose, you can utilize crash dump files that contain a snapshot\nof the process at the time of the crash.\nOn Windows platforms, you can use the Microsoft Debugging Tools API that is\nprovided by the dbghelp.dll library. With help of the MiniDumpWriteDump function,\nyou can create a mini-dump file of the running process. Unlike a usual crash dump,\nlike the ones the old Dr. Watson utility created, a mini-dump does not have to con-\ntain the whole process space. Some sections are simply not required for most debug-\nging, like the ones that contain loaded modules. You just need to know the version\ninformation of these files so that you can provide them to the debugger later on. This\nmeans that the dump files created with this API are usually much smaller in size than\na full dump.\nThe information stored in a dump file can be controlled with the DumpType para-\nmeter. Whereas MiniDumpNormal includes only basic information like stack traces and\nthe thread listing, MiniDumpWithFullMemory writes all accessible process memory to\nthe file. To find out what dump type is the right one for you, I recommend reading\n[Starodumov05]. For further information about the MiniDumpWriteDump API and its\nparameters, refer to [MSDNDump07]. You should also consider distributing the \n98\nSection 1\nGeneral Programming \n",
      "content_length": 3085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "latest version of dbghelp.dll when releasing your game because older versions of this\nlibrary (like the one that comes with Windows 2000) do not have the right exports.\nAlso keep in mind that the API exposed by dbghelp.dll is not thread safe! This means\nit is the caller’s responsibility to synchronize calls to any of these functions.\nIn order to explicitly create a core dump on UNIX platforms, you have to use a\nthird-party tool. The Google coredumper project [Google05] is an open source\nlibrary that provides a simple API for creating core dumps of a running process. The\nWriteCoreDump function writes a dump of the current process into a specified file for\nyou. Alternatively, you can also use WriteCompressedCoreDump which writes the dump\nin either bzip2 or gzip compressed format. Finally, there is GetCoreDump, which creates\na copy-on-write snapshot of the process and returns a file handle from which the\ndump can be read. Currently the library supports x86, x64, and ARM systems and is\npretty easy to include in a project. \nHandling Stack Overflows\nSo far, this implementation can handle almost all cases that can cause an application\nto crash. The only condition where the error handling fails is in case of a stack over-\nflow. Before being able to handle this special error situation, you need to know the\nstatus of the thread after a stack overflow. When the application starts, the stack is ini-\ntially set to a small size (see Figure 1.10.1a). After the last page of the stack, a so-called\nguard page is placed. If the game consumes all of the reserved stack memory, the\nfollowing things happen (see Figure 1.10.1b):\n1. When accessing the stack while the stack pointer is pointing to the guard\npage, a STATUS_GUARD_PAGE_VIOLATION exception is raised.\n2. The guard protection is removed from the page, which now becomes part\nof the stack.\n3. A new guard page is allocated, one page below the last one.\n4. Execution is continued from the instruction that caused the exception.\nIf the stack reaches its maximum size, the allocation of a new guard page in step 3\nfails. In this case, a stack overflow exception is raised, which can be handled by the\nthread’s exception block. As you can see in Figure 1.10.1c, the stack now has just one free\npage left to work with. If you take up all of this space in the exception handler, you will\ncause an access violation and the application will be terminated by the operating system.\nThis means you do not have a large scope left to work with. For instance, if \nyou are calling MiniDumpWriteDump or even MessageBox, the game will cause an access\nviolation because the functions have too large stack overhead. However, you can over-\ncome this problem with a rather simple trick. Because you have enough space left to\ncall the CreateThread function, you can move all the exception handling into a new\nthread. Then you can work with a clean stack and do whatever kind of reporting you\nwant. In the exception handler, you wait until the worker thread has finished its work\nand returns.\n1.10\nAdvanced Debugging Techniques\n99\n",
      "content_length": 3072,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "Unfortunately, this approach does not work on UNIX platforms. When the sig-\nnal handler is called, the stack is so exhausted that you can’t properly report the error.\nThe only way to overcome this problem is by using the sigaltstack function to\ninstall an alternative stack for the signal handler. All signal handlers that are installed\nwith the SA_ONSTACK flag will be delivered on that stack, whereas all other handlers\nwill still be executed on the current stack.\nIn order to successfully create a core dump, you have to allocate at least 40KB of\nmemory for the alternative stack. In case you want to do even more reporting in the\nhandler, the size should be adjusted appropriately. When using sigaltstack, you\nshould check its documentation for your target platform because some implementa-\ntions can cause problems when used in multithreaded applications.\nMemory Leaks\nIf you are not using a global memory manager to satisfy the memory needs of your\ngame, you should certainly think about using a memory leak detector. There are a vast\namount of tools available that can help you find memory leaks. However, using them\nis not always straightforward and can often be a real pain. One of the techniques used\nmost often is to overload the new/delete operators. However, this approach has quite\na few flaws:\n• System headers must be included before the operator overloading while project\nheaders must be included afterward. Violating this rule can result in erroneous\nleak reports or even application crashes. \n• Leaks caused by allocations using malloc won’t be detected.\n• Conflicts with other libraries that overload these operators (MFC, for instance)\nare possible.\nOf course there are also various external leak detection tools available; however,\nthey are usually quite expensive. The leak detector presented in this article uses \nso-called allocation hooks for tracking the application’s memory requests. Allocation\n100\nSection 1\nGeneral Programming \nFIGURE 1.10.1\nInitial stack, stack growth, and stack overflow.\n",
      "content_length": 2022,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "hooks are provided by the C runtime library and allow you to override memory man-\nagement functions like malloc or free. The advantage of using hooks is that no mat-\nter where or how memory is allocated or freed, you will be notified by the runtime\nlibrary through the hook functions. The only thing you have to do is install the hooks\nbefore doing any allocations. \nInstalling Allocation Hooks\nThe Microsoft CRT allows you to install an allocation hook through its _CrtSetAlloc\nHook function. When or where do you install an allocation hook? The answer is as\nsoon as possible so that you are not missing any allocation requests. However, this is\nnot as easy as it sounds. Inside the main method can be too late if there are global\nobjects that dynamically allocate memory on construction. Even if you define the leak\ndetector as a global instance, there is no guarantee it will be initialized before any\nother global object. One way to overcome this problem is to use the Microsoft specific\n#pragma init_seg(lib) directive. By using this preprocessor command, you can tell\nthe compiler to initialize the objects defined in the current source file before any other\nobjects (this does not include CRT library initializations).\nThe GNU C library even allows you to completely replace the memory functions\nusing global hook variables defined in malloc.h. By assigning your own handlers to\n__malloc_hook, __realloc_hook, and __free_hook, you gain full control over all\nmemory-management requests. Initialization of the hooks can be easily achieved using\nthe __malloc_initialize_hook handler. This is a simple function without parameters\nor return value that is called after the library finishes installing its default allocation\nhooks. It is important to back up the default hooks for later use in your own hook\nfunctions. Otherwise, you would have to provide a complete implementation for the\noverloaded memory functions. \nImplementing Allocation Hooks\nThe leak detector manages an allocation registry that contains information about the\nallocated memory blocks. Each of these blocks contains a unique identifier, the\nrequested block size, and the call stack during the allocation request. The call stack\ndata will be used during the reporting for symbol resolving to get meaningful infor-\nmation. After a memory block is freed its block information is removed from the reg-\nistry. When the application ends, you have to enumerate the entries left in the registry\nand report them as memory leaks. The task for the allocation hooks is to update that\nregistry on each memory request with the required information.\nThe hook function passed to _CrtSetAllocHook must have the following signature:\nint AllocHook(int allocType, void* data, size_t size, int blockType,\nlong request, const unsigned char* filename, int line);\nThe most interesting parameters are allocType, blockType, and request. The\nallocType parameter specifies what operation was triggered (_HOOK_ALLOC, _HOOK_\n1.10\nAdvanced Debugging Techniques\n101\n",
      "content_length": 3008,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "REALLOC, or _HOOK_FREE). blockType specifies the memory block type that can be used to\nfilter memory allocations (in this implementation, I exclude all CRT allocations from\nleak detection). Finally, request specifies the order of the allocation, which you can use\nas a unique ID for bookkeeping. When the function has finished its work, it returns an\ninteger that specifies whether the allocation succeeds (returns TRUE) or fails (returns\nFALSE). For more information on the allocation hook function, refer to [MSDNHook07].\nThe signature of the GNU C library hooking functions is similar to the runtime\nfunctions they overload. Each function receives one additional parameter that con-\ntains the return address found on the stack when malloc, realloc, or free was called.\nThe hook functions all work in the following way:\n1. The original hook functions are restored.\n2. Call malloc, realloc or free.\n3. Update information for the memory leak detection.\n4. Back up the current hook functions.\n5. Install the custom hook functions.\nThis is the recommended workflow for allocation hooks, as described in the GNU\nC library documentation [GNUC06]. Instead of the request ID, use the address\nreturned by malloc or realloc as the unique memory block ID. \nWindows Error Reporting (WER)\nWith Windows Vista Microsoft introduced the Windows Feedback Platform that\nallows vendors and developers to access crash dumps sent through WER to Microsoft.\nThis is done through the Windows Quality Online Services (WinQual), an online\nportal offered by Microsoft. The service is free but requires a VeriSign Code Signing\nID to verify the identity of a company that is submitting software or accessing the\nWER database. WinQual organizes crash dumps into so-called buckets where each\nbucket contains crash reports caused by the same bug. The following parameters are\nused for bucket organization:\n• Application name—For example, game.exe\n• Application version—For example, 1.0.1234.0\n• Module name—For example, input.dll\n• Module version—For example, 1.0.123.1\n• Offset into module—For example, 00003cbb\nBy default, WER calls dwwin.exe, which collects the bucket data and creates a\nmini-dump of the crashed process. WinQual additionally offers the possibility to\nspecify feedback or request further information on a bucket. If users experience an\nalready known bug, they will be notified by WER. The users may be pointed to the\nvendor’s support site to download a hotfix, or can be informed of the current state of\nany bug fixes. Vendors can also request further information by executing WMI\nqueries, listing registry keys, or asking users to fill out a questionnaire.\n102\nSection 1\nGeneral Programming \n",
      "content_length": 2682,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "The Client API\nOn Windows XP, the WER implementation is rather simple. Just two APIs (ReportFault\nand AddERExcludedApplication) are available to the developer. The ReportFault API\ninvokes WER and must be called inside the application’s exception handler. It will exe-\ncute the operating system utility dwwin.exe, which creates the crash report and prompts\nthe users to send it to Microsoft. AddERExcludedApplication can be used to exclude an\napplication from error reporting (this function requires write access to HKEY_LOCAL_\nMACHINE in the Windows Registry).\nWindows Vista offers a completely new API to work with WER. The library\nincludes several functions to create and submit error reports. Another difference from\nthe old WER API is that reports can be generated at any time during execution. The\nfollowing table gives a short overview of the most important functions offered by\nWER on Windows Vista. For a complete listing of available WER functions, refer to\n[MSDNWER07].\nFunction\nDescription\nWerAddExcludedApplication\nExcludes the specified application from error reporting.\nWerRegisterFile\nRegisters a file to be added to reports generated for the\ncurrent process.\nWerRemoveExcludedApplication\nReverts a previous call to WerAddExcludedApplication.\nWerReportAddDump\nAdds a dump to the report.\nWerReportAddFile\nAdds a file to the report.\nWerReportCloseHandle\nCloses the report.\nWerReportCreate\nCreates a new report.\nWerReportSetUIOption\nSets the user interface options.\nWerReportSubmit\nSubmits the report.\nWerUnregisterFile\nRemoves a file from the reports generated for the current\nprocess.\nAnother feature introduced with Windows Vista is the new Application Recovery\nand Restart API. It enables an application to register itself to get restarted or recovered\nafter a crash. Especially interesting is the RegisterApplicationRecoveryCallback\nfunction, which enables you to register a recovery callback. This function will be\ncalled by WER when an application becomes unresponsive or encounters an unhan-\ndled exception. This would be the ideal place to try saving game and player data,\nenabling the player to continue the game later. For detailed information about this\nnew Vista API, refer to [MSDNARR07].\nThe Framework\nThis section provides a short overview of the debugging framework provided on the\nCD. The CDebugHelp class contains helper functions for creating dumps and call\nstacks. Inside of DebugHelp.cpp, you will also find CdbgHelpDll, which is a wrapper\n1.10\nAdvanced Debugging Techniques\n103\n",
      "content_length": 2513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "class for Microsoft’s Debugging Tools API. It will automatically try to load the newest\ndbghelp.dll instead of taking the default one in the system directory. Maybe you also\nnoticed the MiniDumpCallback function? MiniDumpWriteDump allows you to specify a\ncallback to control what information is added to the dump. In my implementation, I\nexclude the exception handler thread that you start when handling a stack overflow\nexception. The UNIX implementation of these functions is simple and doesn’t require\nany further explanation (documentation for backtrace and backtrace_symbols can\nbe found at [GNUC06]).\nException Handling\nThe exception handlers are part of the CDebugFx class declared in DebugFx.h. On\nWindows, you install an UnhandledExceptionFilter that creates a mini-dump and\ncalls a user-defined callback to do any more work when an exception occurs. In the\nUNIX implementation, the framework registers signal handlers for SIGSEGV and\nSIGABRT. I included the source of the Google coredumper into the library to elimi-\nnate the dependency on an external library. \nMemory Leak Detector\nThe memory leak detector is globally instanced and automatically active as soon as\nyou link the debugging framework to your application. Reporting is done via a\nreporting API so that the users can implement their own reporting. The default\nreporter will write the leak information into the debug output window when run on\na Windows platform and to the error output on UNIX. Another feature of the default\nreporter is the filtering of useless information from the call stack. Listing function\ncalls for operator new or the CRT allocation functions only bloats the output and\ndoesn’t help in finding bugs.\nSometimes there are problems with the symbol resolving, in particular when a\nmodule that is referenced in the call stack was already unloaded. In this case, the sym-\nbol resolving APIs cannot resolve the address to a symbolic name. On Windows, you\ncan reload the symbol table of a module using the SymLoadModule64 function. To use\nthis function, you just have to store the module base address (which is equal to the\nmodule handle) and the module name, along with the allocation information. Unfor-\ntunately, the GNU C library provides only a single function (backtrace_symbols) for\nsymbol resolving. The only solution on this platform is to store the resolved symbols\ninstead of the call stack for all allocations. Because this method would result in a huge\nmemory overhead and decreased performance, the leak detector currently doesn’t\nsupport this feature on UNIX platforms.\nConclusion\nWith the proper handling of application crashes, you can gain invaluable information\nthat can help you during debugging. Depending on the platform, you have different\n104\nSection 1\nGeneral Programming \n",
      "content_length": 2787,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "methods to handle unexpected exceptions, like Windows Unhandled Exception Filters\nor UNIX signal handlers. Crash dumps are a great tool for post-mortem debugging of\nyour application because they provide a snapshot of the process when it crashed. By\nusing a threaded exception handler on Windows or an alternative signal handler stack\non UNIX, you can even handle stack overflows. You can also get rid of nasty memory\nleaks with a memory leak detector. Using allocation hooks, provided by the CRTs, you\ncan report all leaks with a complete stack trace when your game exits. Finally, you get a\ncomplete debugging framework providing unhandled exception handling with crash\ndump creation using the CDebugFx class. Memory leak detection is implemented in the\nCMemLeakDetector class and is automatically enabled when you link the library to your\ngame. With this set of tools, bugs won’t have any chance in your future projects.\nReferences\n[GNUC06] GNU C library documentation. “The GNU C library,” available online\nat http://www.gnu.org/software/libc/manual/, December 6, 2006.\n[Google05] Google coredumper library available online at http://code.google.com/\np/google-coredumper/, 2005.\n[Pietrek97] Pietrek, Matt. “A Crash Course on the Depths of Win32 Structured\nException Handling,” available online at http://www.microsoft.com/msj/0197/\nexception/exception.aspx, Microsoft Systems Journal, January, 1997.\n[Pietrek01] Pietrek, Matt. “New Vectored Exception Handling in Windows XP,”\navailable online at http://msdn.microsoft.com/msdnmag/issues/01/09/hood/\ndefault.aspx, MSDN Magazine, September, 2001.\n[MSDNDump07] MSDN Library. “MiniDumpWriteDump,” available online at\nhttp://msdn2.microsoft.com/en-us/library/ms680360.aspx, June 1, 2007.\n[MSDNHook07] \nMSDN \nLibrary. \n“Run-Time \nLibrary \nReference—\n_CrtSetAllocHook,” available online at http://msdn2.microsoft.com/en-us/\nlibrary/820k4tb8(VS.80).aspx, June 1, 2007.\n[MSDNWER07] MSDN Library. “WER Functions,” available online at\nhttp://msdn2.microsoft.com/en-us/library/bb513635.aspx, June 1, 2007.\n[MSDNARR07] MSDN Library. “Application Recover and Restart Reference,” avail-\nable online at http://msdn2.microsoft.com/en-us/library/aa373342.aspx, June 1,\n2007.\n[Starodumov05] Starodumov O. “Effective Mini-Dumps,” available online at\nhttp://www.debuginfo.com/articles/effminidumps.html, July 2, 2005.\n[Wikipedia] Wikipedia. “Signal (Computing),” available online at http://\nen.wikipedia.org/wiki/Signal_(computing)#List_of_signals.\n1.10\nAdvanced Debugging Techniques\n105\n",
      "content_length": 2520,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "107\nS E C T I O N\n2\nMATH AND PHYSICS\n",
      "content_length": 37,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "109\nIntroduction\nGraham Rhodes, Applied Research\nAssociates, Inc.\ngrhodes@nc.rr.com\nM\nathematics makes the world go around! At least within the realm of electronic\ngame development, there is real truth in that statement. We use math, in one\nway or another, for just about everything. The heaviest uses of math by game develop-\ners these days lie in the rendering of game worlds, both two- and three-dimensional,\nartificial intelligence algorithms of all sorts, and physics-based simulation, which is\nevolving at a frantic pace. I’m awed by the current level of sophistication being\napplied in all of these areas! Model surfaces rendered in real-time 3D no longer resem-\nble shiny plastic, thanks to the advent of programmable graphics hardware and the\nability to apply advanced physically-based lighting and material models on a per-pixel\nbasis.\nCharacter AI sits on the cusp of Mori’s uncanny valley, and we are starting to see\nflash glimpses that one day it might be crossed. Crossing that valley visually, within\nthe realm of real-time game simulation, will rely not only on AI, but also on physically-\nbased animation techniques that enable game characters to interact with a dynamic\ngame world where collectible items are not necessarily located at scripted locations,\nand where the game world itself is subject to play-driven geometric change. Some of\nthese physically-based character animation technologies are already appearing in\nmiddleware products that are emerging into the industry. The dynamically-changing \ngame worlds themselves, with which sophisticated characters must interact, are also\nbecoming more physically based each year. Several current games, released or immi-\nnent, feature fully dynamic environments that exploit the many robust physics\nengines that are available today. Mankind’s and nature’s mathematical models enable\nall of these great entertainment technologies, of course.\nI would like to make a few brief comments about one important emerging area of\ngame development that is heavily math-driven. And that is the area of procedural\nmodeling, otherwise known as procedural generation, generative modeling, and prob-\nably a dozen other terms. One fact of commercial game development has been that\nthe cost of content development using traditional digital content-creation tools has\nincreased significantly as computer hardware and game engines have become able to\nsupport all of the features mentioned previously. The tools also have evolved, and\nartists can work more efficiently than ever before. But it is often the sheer volume of\ncontent that becomes a problem.\n",
      "content_length": 2602,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "So large studios are now beginning to pay homage, in a way, to the game develop-\nment ways of old, and are mimicking developers from the demo scene who have been\ninventing content-generating algorithms for years. Procedural modeling, in all its\nmany forms, can greatly reduce the cost of content development, by effectively\nremoving the need for an art team to model and place every tree/bush/clump of grass\nthat is to appear in a scene. Procedural modeling of flora for game levels is currently\nquite heavily used for commercial game development. Tools exist, but are not yet\nubiquitous, for the procedural modeling of other game level elements, such as build-\nings and structures with interiors and exteriors modeled at multiple levels of detail,\nauto-populated with furniture and clutter, with everything looking good from a first-\nperson camera. There is even a highly anticipated game in development that claims to\napply procedural modeling to create cellular organisms, planets, stars, nebulae, and\nmany things in between.\nIt is clear to me that there is going to be a strong future for procedural modeling\nin games, although artists should not worry that they may be out of a job as a result!\n(That won’t be the case.) For this to work, of course, tools and game developers must\napply the proper mathematical or physically-based techniques.\nIn this section, you will find a variety of useful gems that provide insight into\nclassical techniques, as well as new techniques that you can apply to core problems. A\nduo of gems provides you with a deeper understanding of random number genera-\ntion, for application toward artificial intelligence, physics techniques, and procedural\ngeneration. Chris Lomont provides a comprehensive overview of random number\ngeneration techniques, their strengths, weakness, and occasionally dramatic failures,\nwhereas Steve Rabin’s gem on Gaussian randomness provides a highly efficient\nimplementation.\nTony Barrera, Anders Hast, and Ewert Bengtsson summarize an efficient tech-\nnique for evaluating trigonometric splines, which can be used to generate curves made\nfrom straight line segments and perfect elliptical arcs. Combining several of these\ntrigonometric splines can generate curves that are visually more elegant and contain\nfewer curvature artifacts than other piecewise spline techniques such as traditional\ncubic polynomial splines—just perfect for digital content-creation tools as well as in-\nengine procedural generation of model geometry. Krzysztof Kluczek continues the\npresentation of techniques that are quite useful for procedural model generation in his\nchapter, describing the use of a projective space to enable highly robust geometric\noperations while reducing storage requirements and computational expense com-\npared with other techniques for achieving similar results.\nThe last four gems focus on a variety of techniques for collision detection and\nother geometric queries, which continue to be areas of active research within the indus-\ntry and academia alike. Jacco Bikker provides a fantastic summary of the kD-tree\nspatial partitioning technique, with a strong practical focus on minimizing storage\nrequirements and on building trees that are optimized according to query type. He also\n110\nSection 2\nMath and Physics\n",
      "content_length": 3284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "describes approaches for dealing with dynamic scenes. José Gilvan Rodrigues Maia,\nCreto Augusto Vidal, and Joaquim Bento Cavalcante-Neto describe transformation\nsemantics, giving a sort of geometric intuition to the interpretation of transformation\nmatrices. Their discussion shows how this intuition can be applied in practice to the\nvarious phases that are common to modern collision detection algorithms.\nRahul Sathe and Dillon Sharlet describe a new technique for collision detection\nthat can be applied from broad phase through narrow phase. Finally, Gary Snethen\ndescribes a collision detection technique inspired by the GJK algorithm that is elegant\nin its simplicity and intuitiveness, while being also quite flexible. Take these ideas,\nyoung men and women and go forth and develop!\nIntroduction\n111\n",
      "content_length": 808,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "113\n2.1\nRandom Number Generation\nChris Lomont\nwww.lomont.org\nT\nhis article is an introduction to random number generators (RNGs). The main\ngoal is to present a starting point for programmers needing to make decisions\nabout RNG choice and implementation. A second goal is to present better alterna-\ntives for the ubiquitous Mersenne Twister (MT). A final goal is to cover various classes\nof random number generators, providing strengths and weaknesses of each. \nBackground: Random Number Generation\nRandom number generators (RNGs) are essential to many computing applications.\nFor some problems, algorithms employing random choices perform better than any\nknown algorithm not using random choices. It is often easier to find an algorithm to\nsolve a given problem if randomness is allowed. (The class of problems efficiently\nsolvable on a [Turing] machine equipped with a random number generator is BPP,\nand it is an open problem if BPP=P, P being the class of problems efficiently solvable\non a computer without random choice.)\nMost random numbers used in computing are not considered truly random, but\nare created using pseudo-random number generators (PRNGs). PRNGs are deter-\nministic algorithms, and are the only type of random number that can be algorithmi-\ncally generated without an external source of entropy, such as thermal noise or user\nmovements. \nDesigning good RNGs is hard and best left to professionals. (Robert R. Coveyou\nof Oak Ridge National Laboratory humorously once titled an article, “The Genera-\ntion of Random Numbers Is Too Important to Be Left to Chance.” Like cryptogra-\nphy, the history of RNGs is littered with bad algorithms and the consequences of\nusing them. A few historical mistakes are covered near the end of this article.\nUses\nRandom numbers are used in many applications, including the following:\n• AI algorithms, such as genetic algorithms and automated opponents.\n• Random game content and level generation. \n• Simulation of complex phenomena such as weather and fire.\n",
      "content_length": 2009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "114\nSection 2\nMath and Physics\n• Numerical methods such as Monte-Carlo integration.\n• Until recently, primality proving used randomized algorithms. \n• Cryptography algorithms such as RSA use random numbers for key generation. \n• Weather simulation and other statistical physics testing.\n• Optimization algorithms use random numbers significantly—simulated anneal-\ning, large space searching, and combinatorial searching.\nHardware RNGs\nBecause an algorithm cannot create “true” random numbers, many hardware-based\nRNGs have been devised. Quantum mechanical events cannot be predicted, and are\nconsidered a very good source of randomness. Such quantum phenomena include:\n• Nuclear decay detection, similar to a smoke detector.\n• Quantum mechanical noise source in electronic circuits called “shot noise.”\n• Photon streams through a partially silvered mirror.\n• Particle spins created from high energy x-rays.\nOther sources of physical randomness are as follows:\n• Atmospheric noise (see www.freewebs.com/pmutaf/iwrandom.html for a way to\nget random numbers from WiFi noise).\n• Thermal noise in electronics.\nOther physical phenomena are often used on computers, like clock drift, mouse\nand keyboard input, network traffic, add-on hardware devices, or images gathered\nfrom moving scenery. Each source must be analyzed to determine how much entropy\nthe source has, and then how many high-quality random bits can be extracted.\nHere are a few Websites offering random bits of noise and the method used to\nobtain them:\n• http://random.org/—Atmospheric noise.\n• http://www.fourmilab.ch/hotbits/—Radioactive decay of Cesium-137.\n• http://www.lavarnd.org/—Noise in CCD images.\nPseudo-Random Number Generators (PRNGs)\nPRNGs generate a sequence of “random” numbers using an algorithm, operating on\nan internal state. The initial state is called the seed, and selecting a good seed for a given\nalgorithm is often difficult. Often the internal state is also the returned value. Due to\nthe state being finite, the PRNG will repeat at some point, and the period of an RNG\nis how many numbers it can return before repeating. A PRNG using n bits for its state\nhas a period of at most 2n. Starting a PRNG with the same seed allows repeatable ran-\ndom sequences, which is very useful for debugging among other things. When a\nPRNG needs a “random” seed, often sources of entropy from the system or external\nhardware are used to seed the PRNG.\n",
      "content_length": 2421,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "Due to computational needs, memory requirements, security needs, and desired\nrandom number “quality,” there are many different RNG algorithms. No one algo-\nrithm is suitable for all cases, in the same way that no sorting algorithm is best in all\nsituations. Many people default to C/C++ rand() or the Mersenne Twister, both of\nwhich have their uses. Both are covered in this gem.\nCommon Distributions\nMost RNGs return an integer selected uniformly from the range [0,m] for some max-\nimum value m. C/C++ implementations provide the rand() function, with m being\n#defined as RAND_MAX, quite often the 15-bit value, 32767. srand(seed) sets the initial\nseed, often using the current time as an entropy source like srand(time(NULL)). Most\nC/C++ rand() functions are Linear Congruential Generators, which are poor choices\nfor cryptography. Most C/C++ implementations (as well as other languages) generate\npoor quality random numbers that exhibit various kinds of bias.\nThe most common distribution used in games is a uniform distribution, where\nequally likely random integers are needed in a range [a,b]. A common mistake is to use\nC code like (rand()%(b-a+1)) + a. The mistake is that not all values are equally likely\nto occur due to modulus wrapping around. This only works if b – a + 1 divides\nRAND_MAX+1. For example, if RAND_MAX is 32767, then trying to generate numbers in the\nrange [0,32766] using this method causes 0 to be twice as likely as any other single\nvalue. A valid (although slower) solution is to scale the rand output to [0,1] and back\nto [a,b], using:\ndouble v = (static_cast<double>( rand()) ) / RAND_MAX;\nreturn static_cast<long>(v*(b-a+1)+a);\nThe second most commonly used distribution is a Gaussian Distribution, which\ncan be generated from a uniform distribution. Let randf() return uniformly distrib-\nuted real numbers in [0,1]. Then the polar form of the Box-Muller transformation\ngives two Gaussian values, y1 and y2, per call.\nfloat x1, x2, w, y1, y2;\ndo {\nx1 = 2.0 * randf() - 1.0;\nx2 = 2.0 * randf() - 1.0;\nw  = x1 * x1 + x2 * x2;\n} while ( w >= 1.0 );\nw = sqrt( (-2.0 * log( w ) ) / w );\ny1 = x1 * w;\ny2 = x2 * w;\nBoost [Boost07] documents techniques for generating other distributions start-\ning with a uniform distribution.\n2.1\nRandom Number Generation\n115\n",
      "content_length": 2287,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "Randomness Testing\nTo test if a sequence is “random,” a definition of “random” is needed. However “random-\nness” is very difficult to make precise. In practice (because many PRNGs are useful) tests\nhave been designed to test the quality of RNGs by detecting sequence behavior that does\nnot behave like a random sequence should.\nThe most famous randomness-testing suite is DIEHARD [Marsaglia95], made of\n12 tests (for more information, see http://en.wikipedia.org/wiki/Diehard_tests).\nDIEHARD has been expanded into the open source (GPL) set of tests DieHarder\n[Brown06], which includes the DIEHARD tests as well as many new ones. Also\nincluded are many RNGs and a harness to add new ones easily. A third testing frame-\nwork is TestU01 [L’Ecuyer06]. Each framework provides some assurance a tested\nRNG is not clearly bad.\nSoftware Whitening\nMany sources of random bits have some bias or bit correlation, and methods to\nremove the bias and correlation are known as whitening algorithms. Some choices:\n• John von Neumann. Take bits two at a time, discard 00 and 11 cases, and output\n1 for 01 and 0 for 10, removing uniform bias, at the cost of needing more bits.\n• Flip every other bit, removing uniform bias.\n• XOR with another known good source of bits, as in Blum Blum Shub.\n• Apply cryptographic hashes like Whirlpool or RIPEMD-160. Note MD5 is no\nlonger considered secure.\nThese whitened streams should still not be considered a secure source of random\nbits without further processing.\nNon-Cryptographic RNG Methods\nNon-cryptographically secure methods are usually faster than cryptographic methods,\nbut should not be used when security is needed, hence the classification. Each of the\nfollowing methods is a PRNG with output sequence Xn. Some have a hidden internal\nstate Sn from which Xn is derived. Either X0 or S0 is the seed, as appropriate.\nMiddle Square Method\nThis was suggested by John von Neumann in 1946—take a 10-digit number as a\nseed, square it, and return the middle 10 digits as the next number and seed. It was\nused in ENIAC, is a poor method with statistical weaknesses, and is no longer used.\nLinear Congruential Generator (LCG)\nThese are the most common methods in widespread use, but are slowly being replaced\nby newer methods. They are computed with Xn+1 = (aXn + b)modm, for constants a\n116\nSection 2\nMath and Physics\n",
      "content_length": 2343,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "and b. The modulus m is often chosen as a power of 2, making it efficiently imple-\nmented as a bitmask. Careful choice of a and b is required to guarantee maximal period\nand avoid other problem cases. LCGs have various pathologies, one of which is that\nchoosing points in three-tuples and plotting them in space shows the points fall onto\nplanes, as exhibited later in the section on RANDU, and is a result of linear relations\nbetween successive points. LCGs with power-of-two modulus m = 2e are known to be\nbadly behaved, especially in their least significant bits [L’Ecuyer90]. For example\nNumerical Recipes in C [Press06] recommends a = 1664525, b = 1013904223, m =\n2^32, and the lowest order bit then merely alternates.\nLCGs’ strengths are they are relatively fast and use a small state, making them\nuseful in many places including embedded applications. If the modulus is not a power\nof two then the modulus operation is often expensive.\nRepresenting an LCG as LCG(m, a, b), Table 2.1.1 shows some LCGs in use.\nTable 2.1.1\nSome LCGs in Use\nLCG\nUse\nLCG(231, 65539, 0)\nThe infamous RANDU covered later in this gem.\nLCG(224, 16598013, 12820163)\nMicrosoft VisualBasic 6.0.\nLCG(248, 25214903917, 11)\ndrand48 from the UNIX standard library; was used in\njava.util.Random.\nLCG(1012 \u0003 11, 427419669081, 0) \nUsed in Maple 9.5 and in MuPAD 3. Replaced by MT19937\n(below) in Maple 10.\nTruncated Linear Congruential Generator (TLCG)\nThese store an internal state Si updated using an LCG, which in turn is used to generate\nthe output Xi. Symbolically, Sn+1 = (aSn + b)modm,\n. This allows\nusing the fast m as a power of two but avoids the poor low order bits in the LCGs. If\nK is a power of 2, the division is also fast. This algorithm is used extensively through-\nout Microsoft products (likely as a result of being compiled with VC++), including\nVC++ rand(), with the implementation\n/* MS algorithm for rand() */\nstatic unsigned long seed;\nseed = 214013L * seed + 2531011L;\nreturn (seed>>16)&0x7FFF; // return bits 16-30\nThis is not secure. In fact, for a cryptographic analysis project, this author has\ndetermined only three successive outputs from this algorithm are enough to deter-\nmine the internal state (up to an unneeded most significant bit), and thereby know all\nfuture output. A simple way to compute the state is to notice the top bit of the state\nX\nFloor\nS\nK\nn\nn\n+\n+\n=\n⎡\n⎣\n⎢\n⎢\n⎤\n⎦\n⎥⎥⎥⎥⎥⎥⎥⎥\n1\n1⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥\n2.1\nRandom Number Generation\n117\n",
      "content_length": 2448,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "has no bearing on future output; so only 31 bits are unknown. The first output gives\n15 bits of the state, leaving 17 bits unknown. Now, given two more outputs, take the\nfirst known 15 bits and test each of the possible 217 unknown bit states to see which\ngives the other two known outputs. This probably determines the internal state. Two\noutputs are not enough because they do not uniquely determine the state.\nBorland C++ and TurboC also used TLCGs with a = 22695477 and b = 1.\nAlthough the C specification does not force a rand implementation, the example one\nin the C Programming Language [Kernighan91] is a TLCG with a = 113515245 and\nb = 12345, with a RAND_MAX of the minimum allowable 32767.\nLinear Feedback Shift Register (LFSR)\nA Linear Feedback Shift Register (LFSR, see Figure 2.1.1) generates bits from an\ninternal state by shifting them out, one at a time. New bits are shifted into the state,\nand are a linear function of bits already in the state. LFSRs are popular because they\nare fast, easy to do in hardware, and can generate a wide range of sequences. Tap\nsequences can be chosen to make an n bit LFSR have period 2n – 1. Given 2n bits of\noutput the structure and feedback connections can be deduced, so they are definitely\nnot secure.\n118\nSection 2\nMath and Physics\nFIGURE 2.1.1\nLinear Feedback Shift Register (LFSR).\nInversive Congruential Generator\nThese are similar to LCGs but are nonlinear, using Xn+1 = (aXn\n–1 + b)modm, where\nXn\n–1 is the multiplicative inverse modm, that is, XnXn\n–1 ≡1modm. These are expensive\nto compute due to the inverse operation, and are not often used.\nLagged Fibonacci Generator (LFG)\nUse k words of state Xn = (Xn–j\nXn–k)modm, O < j < k where \nis some binary oper-\nation (plus, times, xor, others). These are very hard to get to work well and hard to ini-\ntialize. The period depends on a starting seed and the space of reached values breaks\ninto hard to predict cycles. They are now disfavored due to the Mersenne Twister and\nlater generators. Boost [Boost07] includes variants of LFGs.\n⊗\n⊗\n",
      "content_length": 2048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "Cellular Automata\nMathematica prior to Version 6.0 uses the cellular automata Wolfram rule 30 to gener-\nate large integers (see http://mathworld.wolfram.com/Rule30.html). Version 6.0 uses a\nvariety of methods.\nLinear Recurrence Generators\nThese are a generalization of the LFSRs, and most fast modern PRNGs are derived\nfrom these over binary finite fields. Note that none of these pass linear recurrence test-\ning due to being linear functions. The next few are special examples of this type of\nPRNG, and are considered the best general-purpose RNGs.\nMersenne Twister\nIn 1997, Makoto Matsumoto and Takuji Nishimura published the Mersenne Twister\nalgorithm [Matsumoto98], which avoided many of the problems with earlier genera-\ntors. They presented two versions, MT11213 and MT19937, with periods of 211213-1\nand 219937-1 (approximately 106001), which represents far more computation than is\nlikely possible in the lifetime of the entire universe. MT19937 uses an internal state of\n624 longs, or 19968 bits, which is about expected for the huge period. It is (perhaps\nsurprisingly) faster than the LCGs, is equidistributed in up to 623 dimensions, and\nhas become the main RNG used in statistical simulations.\nThe speed comes from only updating a small part of the state for each random\nnumber generated, and moving through the state over multiple calls. Mersenne\nTwister is a Twisted Generalized Feedback Shift register (TGFSR). It is not crypto-\ngraphically secure: observing 624 sequential outputs allows you to determine the\ninternal state, and then predict the remaining sequence. Mersenne Twister has some\nflaws, covered in the “WELL Algorithm” section that follows.\nLFSR113, LFSR258\n[L’Ecuyer99] introduces combined LFSR Tausworthe generators LFSR113 and\nLFSR258 designed specially for 32-bit and 64-bit computers, respectively, with peri-\nods of approximately 2113 and 2258, respectively. They are fast, simple, and have a\nsmall memory footprint. For example, here is C/C++ code for LFSR113 that returns\na 32-bit value:\nunsigned long z1, z2, z3, z4; /* the state  */\n/* NOTE: the seed MUST satisfy \nz1 > 1, z2 > 7, z3 > 15, and z4 > 127 */\nunsigned long lfsr113(void) \n{ /* Generates random 32 bit numbers.    */ \nunsigned long b; \nb  = (((z1 << 6) ^ z1)   >> 13); \nz1 = (((z1 & 4294967294) << 18) ^ b); \nb  = (((z2 << 2) ^ z2)   >> 27); \nz2 = (((z2 & 4294967288) <<  2) ^ b); \nb  = (((z3 << 13) ^ z3)  >> 21); \nz3 = (((z3 & 4294967280) <<  7) ^ b); \n2.1\nRandom Number Generation\n119\n",
      "content_length": 2490,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "b  = (((z4 << 3) ^ z4)   >> 12); \nz4 = (((z4 & 4294967168) << 13) ^ b); \nreturn (z1 ^ z2 ^ z3 ^ z4); \n}\nBecause 2113 is approximately 1034, this already represents a huge number of val-\nues, and has a much smaller footprint than MT19937. The LFSR generators also are\nwell equidistributed, and avoid LCGs problems.\nWELL Algorithm\nMatsumoto (co-creator of the Mersenne Twister), L’Ecuyer (a major RNG researcher),\nand Panneton introduced another class of TGFSR PRNGs in 2006 [Panneton06].\nThese algorithms produce numbers with better equidistribution than MT19937 and\nimprove upon “bit-mixing” properties. WELL stands for Well Equidistributed Long-\nPeriod Linear, and they seem to be better choices for anywhere MT19937 is currently\nused. They are fast, come in many sizes, and most importantly produce higher quality\nrandom numbers.\nWELL period sizes are presented for period 2n for n = 512, 521, 607, 800, 1024,\n19937, 21701, 23209, and 44497, with corresponding state sizes. This allows users to\ntrade period length for state size. All run at similar speed. 2512 is about 10154, and it is\nunlikely any video game will ever need that many random numbers, because it is far\nlarger than the number of particles in the universe. The larger period ones aren’t really\nneeded except for computations like weather modeling or earth simulations. A stan-\ndard PC needs over a googol of years to count to 2512. (A googol is 10100. Google it.)\nA significant place the WELL PRNGs perform better than MT19937 is in escap-\ning states with a large number of zeros. If MT19937 is seeded with many zeros, or\nsomehow falls into such a state, the generated numbers have heavy bias toward zeros\nfor many iterations. The WELL algorithms behave much better, escaping zero bias\nstates quickly.\nThe only downside is that they are slightly slower than MT19937, but not much.\nThe upside is the numbers are considered to be higher quality, and the code is signif-\nicantly simpler. Here is WELL512 C/C++ code written by the author and placed in\nthe public domain (if you use it, I’d appreciate a reference or at least an email with\nthanks). It is about 40% faster than the code presented on L’Ecuyer’s site, and is about\n40% faster than MT19937 presented on Matsumoto’s site. \n/* initialize state to random bits  */\nstatic unsigned long state[16];\n/* init should also reset this to 0 */\nstatic unsigned int index = 0;\n/* return 32 bit random number      */\nunsigned long WELLRNG512(void)\n{\nunsigned long a, b, c, d;\na  = state[index];\nc  = state[(index+13)&15];\nb  = a^c^(a<<16)^(c<<15);\n120\nSection 2\nMath and Physics\n",
      "content_length": 2591,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "c  = state[(index+9)&15];\nc ^= (c>>11);\na  = state[index] = b^c; \nd  = a^((a<<5)&0xDA442D20UL);\nindex = (index + 15)&15;\na  = state[index];\nstate[index] = a^b^d^(a<<2)^(b<<18)^(c<<28);\nreturn state[index];\n}\nCryptographic RNG Methods\nCryptographically Secure PRNGs (CSPRNGs) make it hard for an attacker to deduce\nthe internal state of the generator or to predict future output given large amounts of\noutput. Several CSPRNGs have been standardized and can be found online (check\nout http://en.wikipedia.org/wiki/CSPRNG). Two RFCs dealing with randomness\nrequirements for security are RFC1750 and RFC4086 (see www.ietf.org/rfc). Any\nimplementation of these methods has to be done very carefully to avoid many pitfalls.\nWhenever possible, use an implementation from a trusted and competent source. \nBlum Blum Shub\nPublished in 1986 by Lenore Blum, Manuel Blum, and Michael Shub, Blum Blum\nShub [Blum86] is considered a secure PRNG. It is computed via Sn+1 = (Sn\n2)modm\nwhere m = pq for two properly chosen large primes p,q. Then the output Xn+1 is some\nfunction on Sn+1, which often is taken as bit parity or some particular bits of Sn+1. Its\nstrength relies on the hardness of integer factoring, which is the same problem RSA\npublic key encryption relies on for security. Blum Blum Shub is only useful for cryp-\ntography, because it is much slower than the non-cryptographic PRNGs. (Note Shor’s\nquantum factoring algorithm factors integers efficiently, so once quantum computers\nare in use Blum Blum Shub will become insecure.)\nISAAC, ISAAC+\n[Jenkins96] introduced ISAAC, a CSPRNG based on a variant of the RC4 cipher. It\nis relatively fast for a CSPRNG, requiring an amortized 18.75 instructions to produce\na 32-bit value. There are no cycles in ISAAC shorter than 240 values, and the expected\ncycle length is 28295 values. ISAAC-64, a version for 64-bit machines, requires 19\ninstructions to produce a 64-bit result.\n/dev/random\nAlthough not a specific algorithm, Linux and many UNIX flavors implement a source\nof randomness in /dev/random, which returns random numbers based on system\nentropy, so it is considered a true random number generator. /dev/random blocks,\nthat is, does not return until enough entropy has been gathered to satisfy the request.\n2.1\nRandom Number Generation\n121\n",
      "content_length": 2289,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "As a result, many programs use the non-blocking /dev/urandom. However, these\nnumbers are not as secure, and use of /dev/urandom depletes system entropy, allow-\ning some attacks on bad implementations. The underlying algorithm is not specified;\nsome systems use Yarrow as mentioned in a following section.\n[Gutterman06] revealed exploitable weaknesses in the Linux implementation at\nthe time, which should have been fixed by now. Overall /dev/random is the preferred\nplace on Linux to get CSPRNGs.\nMicrosoft’s CryptGenRandom \nMicrosoft’s CryptoAPI function CryptGenRandom function fills a buffer with cryp-\ntographically secure random bytes. Like /dev/random, it is considered a true random\nnumber generator. Although closed source, it is FIPS validated, and is considered\nsecure. This author is unaware of any weaknesses with recent implementations. On\nWindows, it is the preferred source of CSPRNGs.\nYarrow\n[Kelsey99] introduces Yarrow, which uses system entropy to generate random numbers.\nIt is explicitly unpatented and royalty-free, and no license is required to use it. Yarrow is\nused in Mac OS X and FreeBSD to implement /dev/random. Yarrow is no longer sup-\nported by the designers, who have released an improved design titled Fortuna.\nFortuna\nFortuna is another CSPRNG from the book Practical Cryptography [Ferguson03].\nThe generator is based on any good block cipher, and encrypts in counter mode,\nencrypting successive values of a counter. The key is changed periodically to prevent\nsome statistical weaknesses. It uses entropy pools that gather information from ran-\ndom sources available to the system, and is considered a true RNG because it uses\nexternal entropy.\nCommon Mistakes in Creating Random Number Generators\nCreation of good RNGs is not trivial and the history of RNGs is scattered with exam-\nples of bad design. You can always learn something from the failures of others, so let’s\ntake a look at some common mistakes in this area.\nKnuth Example\nEven algorithm master Donald Knuth tells a story in [Knuth98] about trying his\nhand at making a random number generator by creating a “super-random” generator.\nHis first run settled onto a 10-digit number that then repeated forever. His second\nrun began to repeat itself after 7401 values with a cycle of 3178.\n122\nSection 2\nMath and Physics\n",
      "content_length": 2312,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "Here are a few more examples that hopefully will prevent people from using\nhomemade RNGs in critical applications. \nRANDU\nRANDU is an infamous LCG used since the 1960s; it is LGC(231,65539,0), and requires\nan odd initial seed. The constants were chosen for easy and fast implementation. As all\nLCGs, it suffers from linear relations between successive numbers. Figure 2.1.2 shows\nthe output of 10,000 triplets (x,y,z) plotted in 3D, which happen to fall into planes.\n2.1\nRandom Number Generation\n123\nFIGURE 2.1.2\nLCG bias.\nNetscape\nAn early version of Netscape needed a CSPRNG, but seeded it with three values that\nweren’t very well spread out (time of day, process ID, and parent process ID) and used\nthe result for cryptography. [Goldberg96] published a successful attack on Netscape’s\nSSL protocol, with the exploitable flaw being a poor choice of seed. \nFolklore Algorithms\nThe author encountered a folklore algorithm from a game programmer around 1992,\nwho explained that he had a fast and simple PRNG for his NES code. The basic idea\nwas to shift bits out of a seed, and whenever the seed had 1 bit about to shift off,\n",
      "content_length": 1125,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "exclusive-or in a constant. This was fast and looked nice in assembly, but being a skep-\ntic this author thought about the claim of randomness, and said this should produce\nnumbers that tend to decrease, and every so often jump back up, making saw tooth\noutputs. This led the original programmer to discover a bug in his AI that was using\nthe PRNG which he hadn’t suspected (he had used the PRNG for years). Although\nanecdotal, it is wise to test a new RNG and see that it behaves as expected before com-\nmitting it to your toolbox.\nOne last particularly funny example is the xkcd Webcomic version of a random\nnumber generator at http://xkcd.com/c221.html, reproduced for your viewing pleasure:\nint getRandomNumber()\n{\nreturn 4; // chosen by fair dice roll.\n// guaranteed to be random.\n}\nCode\nThere are many online places to obtain source code for the algorithms covered in this\narticle. Boost [Boost07] contains high-quality implementations for many of them,\nand Wikipedia contains more information and links to most of the presented topics.\nL’Ecuyer’s Web page (www.iro.umontreal.ca/~lecuyer/papers.html) is a good source of\npapers and many implementations. In addition, Technical Review 1 (TR1) for the\nC++ language includes many distributions and generators (including MT19337), so\nit is likely C++ will someday have some of these features built-in.\nConclusion\nThis gem has provided basics of RNGs, including many common algorithms. LFSR113,\nLFSR258, and the WELL generators offer better choices than the Mersenne Twister for\nmany applications, and this presentation brings knowledge of them to a wider audience.\nStrengths and weaknesses were presented for algorithms where possible. Knowledge\nabout RNG types and when to apply them should be in the toolkit of any serious devel-\noper, just as any serious developer should know multiple sorting algorithms, or numerous\ntree structures. Hopefully, this gem provides you with a base and reference for such\nknowledge.\nReferences\n[Blum86] Blum, Lenore, Blum, Manuel, and Shub, Michael. “A Simple Unpre-\ndictable Pseudo-Random Number Generator,” SIAM Journal on Computing, Vol.\n15, pp. 364–383, May 1986.\n[Boost07] “The Boost C++ Library,” 2007, www.boost.org.\n124\nSection 2\nMath and Physics\n",
      "content_length": 2241,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "[Brown06] Brown, Robert G., and Eddelbuettel, Dirk. “DieHarder: A Random\nNumber Test Suite Version 2.24.4,” available online at www.phy.duke.edu/~rgb/\nGeneral/rand_rate.php.\n[Ferguson03] Ferguson, Niels, and Schneier, Bruce. Practical Cryptography, Wiley,\n2003. ISBN 0-471-22357-3.\n[Goldberg96] Goldberg, Ian, and Wagner, David. “Randomness and the Netscape\nBrowser,” Dr. Dobb’s Journal, January 1996, pp. 66–70. Available online at\nwww.cs.berkeley.edu/~daw/papers/ddj-netscape.html.\n[Gutterman06] Gutterman, Pinkas, and Reinman. “Open to Attack: Vulnerabilities\nof the Linux Random Number Generator,” March 2006, Black Hat 2006,\nwww.pinkas.net/PAPERS/gpr06.pdf. \n[Jenkins96] Jenkins, Bob. “ISAAC and RC4,” www.burtleburtle.net/bob/rand/\nisaac.html as of 2007. \n[Kelsey99] Kelsey, J., Schneier, B., and Ferguson, N. “Yarrow-160: Notes on the\nDesign and Analysis of the Yarrow Cryptographic Pseudorandom Number Gen-\nerator,” Sixth Annual Workshop on Selected Areas in Cryptography, Springer\nVerlag, August 1999, www.schneier.com/paper-yarrow.html.\n[Kernighan91] Kernighan, B., and Ritchie, Dennis. The C Programming Language,\nSecond Edition, Prentice-Hall, 1991.\n[Knuth98] Knuth, Donald. The Art of Computer Programming, Volume 2: Seminumer-\nical Algorithms, Third Edition, Addison-Wesley, 1998.\n[L’Ecuyer90] L’Ecuyer, P. “Random Numbers for Simulation,” Communications of\nthe ACM, 33 (1990), pp. 85–98, www.iro.umontreal.ca/~lecuyer/papers.html.\n[L’Ecuyer99] L’Ecuyer, P. “Tables of Maximally-Equidistributed Combined LFSR\nGenerators,” Mathematics of Computation, 68, 225 (1999), pp. 261–269,\nwww.iro.umontreal.ca/~lecuyer/papers.html.\n[L’Ecuyer06] L’Ecuyer, P. and Simard, R. “TestU01: A C Library for Empirical Testing\nof Random Number Generators,” May 2006, Revised November 2006, ACM\nTransactions on Mathematical Software, 33, 4, Article 1, December 2007, to\nappear. www.iro.umontreal.ca/~lecuyer/papers.html.\n[Marsaglia95] Marsaglia, George. “DIEHARD,” http://www.csis.hku.hk/~diehard/.\n[Matsumoto98] Matsumoto, M., and Nishimura, T. “Mersenne Twister: A 623-\nDimensionally Equidistributed Uniform Pseudorandom Number Generator,”\nACM Trans. Model. Comput. Simul. 8, 3 (1998), www.math.sci.hiroshima-\nu.ac.jp/~m-mat/MT/emt.html.\n[Panneton06] Panneton, F., L’Ecuyer, P., and Matsumoto, M. “Improved Long-\nPeriod Generators Based on Linear Recurrences Modulo 2,” ACM Transactions on\nMathematical Software, 32, 1 (2006), pp. 1–16, www.iro.umontreal.ca/~lecuyer/\npapers.html.\n[Press06] Press, William H. (Editor), Teukolsky, Saul A., Vetterling, William T., and\nFlannery, Brian P. Numerical Recipes in C++: The Art of Scientific Computing,\nCambridge University Press, 2nd edition, 2002.\n2.1\nRandom Number Generation\n125\n",
      "content_length": 2720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "127\n2.2\nFast Generic Ray Queries \nfor Games\nJacco Bikker, IGAD/NHTV University of\nApplied Sciences—Breda, The Netherlands\nbikker.j@nhtv.nl\nR\necently, real-time ray tracing on consumer PCs has become possible. A real-time\nray tracer traces millions of rays per second, per core, using instruction-level par-\nallelism (in particular, SIMD code [Wald04]). Thread-level parallelism allows you to\nscale this number almost linearly with the number of cores.\nRay tracing is useful for more than just rendering. This gem describes implemen-\ntation details for a generic ray tracer that can be used for various parts of a game, such\nas line-of-sight queries, physics, and sound propagation. The focus is on efficient tra-\nversal of individual rays.\nThe kD-tree, built using the surface area heuristic (SAH) [McDonald90], is an\nefficient spatial subdivision for ray queries in a static scene. This gem describes some\napproaches to extend this algorithm to support dynamic scenery.\nIntroduction to Ray Tracing\nRay tracing is a simple algorithm. When applied to rendering, the core algorithm\nlooks like this:\nfor each screen pixel\nconstruct a ray from the camera to the pixel\nintersect the ray with each primitive in the scene\nshade the pixel based on the intersection results\nHere, a ray is an infinite line with an origin, O, and a direction, D, as shown in\nFigure 2.2.1.\nTracing a single ray is referred to as ray casting. The basic algorithm was invented\nby Appel in 1963 [Appel63]. In 1979, Whitted extended this process by adding\nrecursion [Whitted79]. At the intersection point, a new ray to each light source is cre-\nated, to see whether the light affects the intersection point. If the material at the inter-\nsection point is reflective or refractive, this will also recursively spawn new rays.\n",
      "content_length": 1792,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "If you look at the process of tracing a single ray alone, you see a visibility query.\nThe camera needs to know which primitive is visible through a screen pixel; the inter-\nsection point wants to know which lights are visible from that point. Queries like this\nare very useful in a game:\n• An enemy AI wants to know if the player is visible or audible.\n• The user highlights an object in the scene to select it.\n• A sniper fires a bullet at the player. The bullet ricochets around the scenery if it\nmisses the player, or if it cannot hit the player directly. Or perhaps it simply con-\ntinues its path after it pierced through the player.\n• A hovering vehicle tries to avoid obstacles and probes the surroundings to do so.\n• A destroyed opponent emits a collectable item that should drop until it hits the\nfloor.\nSome of these obviously require a ray query (player visibility, bullets, and ray\npicking), and some are often left to the physics engine’s internal collision detection\nsystem (when dropping items), even though they could be easily implemented using\none or more rays. Most 3D engines support ray queries in one form or another, but\noften for performance reasons game developers choose not to use them; they are\nassumed to be too slow. If rays are traced using a naive search of the scene geometry,\nthis is a good assumption. However, we can do (much) better. Using an optimized\nspatial subdivision, a ray query can be reduced to a few tree traversal steps and a lim-\nited number of ray/triangle intersections.\nThe remainder of this gem describes the kD-tree, considered to be the most effi-\ncient acceleration structure for ray tracing [Havran01], and approaches to a highly\nefficient implementation. Because this structure takes some preprocessing time, it is\nbest used for static scenery. Because of the importance in games and other real-time\ninteractive applications, this gem presents alternatives for dynamic scenery as well.\nYou can combine these for mixed environments. This is demonstrated in the accom-\npanying demo application on the CD-ROM.\n128\nSection 2\nMath and Physics\nFIGURE 2.2.1\nA ray consists of an origin and a direction.\n",
      "content_length": 2153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "kD-Tree Concepts and Storage Considerations\nThe kD-tree (k-dimensional tree) is a structure that recursively splits space in two\nhalves. In this sense, it is a BSP tree. There is, however, a restriction—the splitting planes\nare axis-aligned, instead of the arbitrary planes that a generic BSP tree uses. An example\nis shown in Figure 2.2.2.\n2.2\nFast Generic Ray Queries for Games\n129\nFIGURE 2.2.2\nThree iterations of the kD-tree\nsplitting process.\n",
      "content_length": 448,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "Note that in the third iteration (c), the bottom-left cell is not split, because it does\nnot contain any geometry. Also note how geometry is quickly isolated from empty\nspace, because of the arbitrary split plane position.\nMaking the split planes axis-aligned may seem limiting. In practice, however, it\nallows you to traverse a ray more efficiently. Finding the intersection of a ray with an\naxis-aligned plane is computationally very efficient, as shown in the following equation:\nt = (splitpos[axis] – ray.origin[axis]) / ray.direction[axis]   (1)\nHere, t is the distance along the ray, starting at the origin. By pre-calculating the\nreciprocal of the ray direction, this is reduced to a subtraction and a multiplication.\nA kD-tree node thus contains a split plane position and an orientation. Because\nthis can be the x, y, or z axis, two bits suffice to store the orientation. Besides the split\nplane, a node stores a list of primitives or as a pointer to a left and right child node.\nFinally, a flag is stored to identify a node as either an internal node (which has no\ngeometry but is split into two child nodes) or a leaf node (which has geometry but no\nchild nodes). \nstruct KdTreeNode\n{\nfloat splitpos;\nint axis;\nKdTreeNode* left, *right;\nbool leaf;\nPrimitive** primitive;\nint primcount;\n};\nUsing a careful layout of the data, this can be stored in just eight bytes:\n• By allocating child nodes at each split in pairs, the KdTreeNode that the right\npointer points to is simply left + 1. This way, only a single child node pointer\nneeds to be stored.\n• By storing pointers to primitives in a separate array, you can index this array from\nthe kD-tree node using an index and a count (see Figure 2.2.3). In order to be\nable to store both the index and the count in a single unsigned integer, you must\nuse five bits for the count and the remaining 27 bits to index the array.\n• If a node is a leaf, it doesn’t require the child node pointer. If it is an internal\nnode, it doesn’t reference primitives. These can thus safely occupy the same vari-\nable in the KdTreeNode data structure.\n• An optimized KdTreeNode requires eight bytes of storage. By aligning KdTreeNodes\nto 8-byte boundaries, the address of a KdTreeNode (and thus the value of the left\npointer) is guaranteed to be a multiple of 8. The lowest bits are thus zero; these\ncan be used for the leaf flag (1 bit) and the split plane axis (2 bits). Because this\ndata is shared by the object list index and count, this data must also be shifted by\nthree bits.\n130\nSection 2\nMath and Physics\n",
      "content_length": 2552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "The KdTreeNode structure now becomes:\nstruct KdTreeNode\n{\n// member data access methods\nvoid SetAxis( int a_Axis ) { m_Data = (m_Data & -4) + a_Axis; }\nint GetAxis(){ return m_Data & 3; }\nvoid SetLeft( KdTreeNode* a_Left )\n{ m_Data = (unsigned long)a_Left + (m_Data & 7); }\nKdTreeNode* GetLeft(){ return (KdTreeNode*)(m_Data & -8); }\nKdTreeNode* GetRight(){ return GetLeft() + 1; }\nint IsLeaf() { return (m_Data & 4); }\nvoid SetLeaf( bool a_Leaf )\n{m_Data = (a_Leaf)?(m_Data|4):(m_Data & -5);}\nint GetObjOffset() { return (m_Data >> 8); }\nint GetObjCount() { return (m_Data & 248) >> 3; }\n// member data\nfloat m_Split;\nunsigned long m_Data;\n};\nIn this structure, the m_Data member contains the leaf bit, the split plane axis,\nand either a pointer to the left child node or a pointer to an entry in the primitive\n2.2\nFast Generic Ray Queries for Games\n131\nFIGURE 2.2.3\nA kD-tree showing the use of an intermediate data structure to reduce the\nsize of the nodes.\n",
      "content_length": 961,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "pointer array, plus a primitive count. The various Get and Set methods filter out the\nrelevant bits for the requested information.\nkD-Tree Construction\nThe previous section described the basic concept of a kD-tree, along with its efficient\nstorage. The biggest unanswered question is how to determine the split plane position.\nExactly what makes a good kD-tree depends on what you want to use it for. If the\ngoal is to walk a list of primitives front-to-back in as few steps as possible, you will\nprobably want a balanced tree with few primitives spanning split planes. For ray trac-\ning, you have a different objective—reduce the number of triangles that you need to\nintersect. And that means that you would rather traverse empty space. Consider a\nscene with a single tiny triangle in the top-left corner, as shown in Figure 2.2.4.\n132\nSection 2\nMath and Physics\nFIGURE 2.2.4\nFinding the optimal split plane position.\nWhat is the optimal tree for this situation? There are a few options:\n• No split at all.\n• A single vertical split through the right vertex of the triangle.\n• A single horizontal split through the bottom vertex of the triangle.\n• Multiple splits.\nNote that splits will always occur at bounding box extrema; all other positions will\neither split the triangle or will add empty space to a node. No split at all means that\nevery ray will be tested against the triangle. A single vertical split will prevent this for\nsome percentage of the rays, and so does a horizontal split. Several splits will reduce this\npercentage even further. It turns out that, for the illustrated case, the vertical split is the\n",
      "content_length": 1621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "best option. In 3D, the chance that you hit any node with the triangle is proportional to\nthe area of the bounding surface of the node. The vertical split produces a non-empty\nleaf node with a smaller area than the horizontal split, so it reduces the chance that a ray\nneeds to be intersected with the triangle more than the horizontal split.\nWhether or not a split is needed at all depends on the cost of intersecting a triangle,\nversus the cost of traversing one level of the kD-tree. Usually, intersecting a triangle is\nmore expensive and so it pays to do a split, and perhaps more than a single split. This\nis the idea of the surface area heuristic (SAH)—determining the best split plane posi-\ntion, calculating an expected cost for each candidate position, and choosing the split\nwith the lowest expected cost. You should perform a split only if that cost is lower than\nthe cost of not splitting at all.\nThe cost of a given split can be computed using the following equation:\ncost_nosplit = primitive_count * total_area * intersection_cost;  (2)\nThe cost of a split can be expressed as the following:\nleft_cost = left_count * intersection_cost;\nright_cost = right_count * intersection_cost;  (3)\nsplit_cost = traversal_cost + left_area * left_cost\n+ right_area * right_cost;\nYou now have a heuristic to find the best plane, and a termination criterion: If you\ncannot find a cost lower than the cost of not splitting at all, you do not split. Addition-\nally, it is a good idea to stop splitting at a certain depth, to limit memory usage and\nconstruction time.\nEven though the SAH is elegant and produces high-quality trees, it needs some\nintervention to produce trees that are more suitable for fast ray tracing. By itself, the\nSAH will not try to isolate empty space. To encourage empty space cutoff, scores for\nsplits that produce empty leafs are lowered.\nFast kD-Tree Construction\nConstructing the kD-tree using the surface area heuristic is a potentially time-\nconsuming process. Because you need the number of triangles to the left and right of\neach possible split plane, you must walk the list of triangles 2N times per split, per\naxis, which amounts to 6N2 iterations of the inner loop. Because you are building a\ntree, the expected runtime of the build process is thus O(N2logN). For any realistic\nscene, this will take too long. This can be improved however, as shown by Wald and\nHavran [Wald06a]. As pointed out, the main bottleneck in the SAH algorithm is the\ntriangle counting. Luckily, you can get rid of this expensive process altogether.\nIn Figure 2.2.5, a simple scene of two triangles is shown. There are four possible\nsplit plane positions along the horizontal axis. At the first position, the number of\ntriangles to the left is zero, and the number of triangles to the right is two. Now,\nwhenever a new triangle begins (left side of bounding box), the left count increases.\n2.2\nFast Generic Ray Queries for Games\n133\n",
      "content_length": 2940,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "Whenever a triangle ends, the right count decreases. So, if you create a sorted list of\nthese events (start events and end events), the left and right triangle counts can be\nupdated incrementally, while walking the list of events from left to right. \nAlthough this already improves performance considerably, there are some remain-\ning problems. First of all, the events need to be sorted per axis. Secondly, at each level\nof the tree, the events need to be resorted, because many triangles will no longer be in\nthe list, whereas others were clipped, introducing new events.\n134\nSection 2\nMath and Physics\nFIGURE 2.2.5\nBounding box start and end events.\nIt turns out that it is possible to do the sorting just once, at the top level of the\ntree. For this, you use a rather complex data structure. The EventBox (see the code\nthat follows), is a linked list with no less than six next pointers, one for each side of a\nprimitive bounding box, for each axis in 3D space. \nFor the scene shown in Figure 2.2.5, two EventBoxes are needed, storing four\nevents per axis. Consider the EventBox for the left triangle: It has two next pointers\nper axis—one of them points to the start event of the second EventBox, the other one\npoints to the end event of the second EventBox.\nstruct EventBox \n{\nEventBoxSide side[2];\nPrimitive* prim; \n};\n",
      "content_length": 1326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "The first EventBoxSide contains the start events for the x, y, and z axes. The sec-\nond element contains the end events. Using two instances of the EventBoxSide object\nallows you to point from one side to another. Each EventBoxSide instance stores a\nposition along each axis, three next pointers, and the side. The pointers and the side\nvalue are stored in a single 32-bit value for efficiency.\nstruct EventBoxSide\n{\nEventBoxSide* next( int axis ) { return (EventBoxSide*)\n(n[axis] & -3); }\nvoid next( int axis, EventBoxSide* p ) \n{\nn[axis] = (n[axis] & 3) + (unsigned long)p; \n}\nint side( int axis ) { return n[axis] & 3; }\nvoid side( int axis, int side ) { n[axis] = (n[axis] & -3) \n+ side; }\nunsigned long n[3];\nfloat pos[3];\n};\nOnce the list is constructed, it can be updated on-the-fly. This way, sorting is lim-\nited to a single sort at the top level of the tree; tree construction time is now reduced\nto O(NlogN).\nkD-Tree Traversal\nOrdered traversal (front-to-back, for ray tracing) of a kD-tree is identical to BSP tree\ntraversal. Traversal starts by finding the leaf node that contains the ray origin. For each\nnode, the side of the split plane that the origin is on is determined, and the branch for\nthat side is followed, while the other side is pushed on a stack. Once a leaf is found, a\nnode is popped from the stack, and the process is continued until the stack is empty.\nFor ray queries, there is an extra termination criterion—once an intersection is found,\nthere is no need to look beyond the current node.\nThis process visits nodes in the correct order. However, for ray queries it needs\nsome adjustments. Specifically, you need the exact entry and exit points for the ray in\nthe current node, tmin and tmax. These values are needed because some intersection\npoints might be outside the current node, illustrated in Figure 2.2.6.\nThe large triangle resides partially in the node that contains the ray origin. If \nyou allow intersections outside the current node, intersecting the ray with this triangle\nwill result in a hit, and so traversal is terminated. The smaller triangle is thus never\nconsidered.\nYou must therefore keep track of tmin and tmax. To do this, you first calculate tmin\nand tmax by clipping the ray against the scene bounding box. After that, tmin and tmax\nare incrementally updated.\n2.2\nFast Generic Ray Queries for Games\n135\n",
      "content_length": 2364,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "The three situations that can occur during traversal are shown in Figure 2.2.7:\n• (Figure 2.2.7a): The intersection of the line with the split plane (tsplit) lies between\ntmin and tmax. The left node is traversed first, with tmax = tsplit. The right node is\npushed on the stack, with tmin = tsplit.\n• (Figure 2.2.7b): tsplit lies beyond tmax. You need only to traverse the left node, and\nno changes to the interval are needed.\n• (Figure 2.2.7c): tsplit lies before tmin. You need only to traverse the right node, and\nno changes to the interval are needed.\nRays that hit the split plane from the right side are handled in the same manner;\nthe only difference is that the left and right child nodes are now swapped.\nThis translates to the following code:\n// precomputed data\nint raydir[8][3][2];\nfor ( int i = 0; i < 8; i++ )\n{\nint rdx = i & 1;\nint rdy = (i >> 1) & 1;\nint rdz = (i >> 2) & 1;\nraydir[i][0][0] = rdx, raydir[i][0][1] = rdx ^ 1;\nraydir[i][1][0] = rdy, raydir[i][1][1] = rdy ^ 1;\nraydir[i][2][0] = rdz, raydir[i][2][1] = rdz ^ 1;\n}\n136\nSection 2\nMath and Physics\nFIGURE 2.2.6\nFalse hit for the larger triangle in the first node.\n",
      "content_length": 1140,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "// prepare data for traversal\nKdTreeNode* node = Scene::GetKdTree()->GetRoot();\nvector3 O = ray.origin;\nvector3 D = ray.direction;\nvector3 R( 1 / ray.direction.x, \n1 / ray.direction.y, \n1 / ray.direction.z );\nint oct = ((D.x < 0)?1:0) + ((D.y < 0)?2:0) + ((D.z < 0)?4:0);\nint* rdir = &raydir[oct][0][0];\nint stackptr = 0;\n// actual traversal\nwhile (1)\n{\nwhile (!node->IsLeaf())\n{\nint axis = node->GetAxis();\nKdTreeNode* front = node->GetLeft() + rdir[axis * 2];\nKdTreeNode* back  = node->GetLeft() + rdir[axis * 2 + 1];\nfloat tsplit = (node->m_Split - O.cell[axis]) * R.cell[axis];\nnode = back;\nif (tsplit < tnear) continue;\nnode = front;\nif (tsplit > tfar) continue;\nstack[stackptr].tfar = tfar;\nstack[stackptr++].node = back;\ntfar = MIN( tfar, tsplit );\n}\n// leaf node found, process triangles\nint start = node->GetObjOffset();\nint count = node->GetObjCount();\nfor (int i = 0; i < count; i++ ) // intersect triangles\n// terminate, or pop node from stack\nif ((dist < tfar) || (!stackptr)) break;\nnode = stack[--stackptr].node;\ntnear = tfar;\ntfar = stack[stackptr].tfar;\n}\nThe array raydir is used to swap the left and right child nodes efficiently. The\nlayout of this array is [octant][axis][child]. Each entry in the array contains the\noffset of the near and far nodes (with respect to the ray direction) for an axis, for an\noctant.\n2.2\nFast Generic Ray Queries for Games\n137\n",
      "content_length": 1378,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "138\nSection 2\nMath and Physics\nFIGURE 2.2.7\nkD-tree traversal cases.\n",
      "content_length": 69,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "Dynamic Objects\nThe method for performing ray queries described so far relies on the availability of a\nhigh-quality kD-tree, a structure that generally requires offline precomputations. The\nscenery therefore must be static: In a game, this is generally not the case. There is no\neasy solution to this problem. Ray tracing dynamic scenes is an area of active research.\nDepending on your specific needs, there are several possible solutions.\nThe main reason for not updating the kD-tree every frame is the expensive SAH\nheuristic. You can however use two trees. The first tree contains the static geometry,\nwhereas the second one contains only the dynamic objects. Tracing rays in a mixed\nenvironment (dynamic and static triangles) is then implemented using a two-stage\ntraversal process. First, the ray traverses the tree for the static scenery; then, the ray\ntraverses the tree that contains the dynamic triangles. This may seem inefficient at\nfirst, but in practice, most rays will miss the dynamic geometry, because this geometry\ntypically covers only a small area of the screen. Most of these rays will therefore travel\na small number of empty nodes, without intersecting any triangles.\nIn the proposed scheme, the tree containing the dynamic triangles is rebuilt each\nframe. Using the SAH, determining the split plane position is by far the most expen-\nsive part. By fixing plane positions, a tree for the dynamic triangles can be built in\nvery little time. One way to do this is to choose the spatial median for alternating\naxes. This essentially creates an octree. Using a separate tree for dynamic triangles\nassumes the scenery contains more static geometry than dynamic geometry, which is\ngenerally the case.\nAlternatively, the kD-tree can be abandoned altogether. Different acceleration\nstructures yield reasonable results, but can be built in less time than a kD-tree. Exam-\nples are bounding volume hierarchies (BVHs) [Wald07], the bounding interval hier-\narchy [Wächter06], and nested grids [Wald06b].\nDemo Application\nThe demo application on the CD-ROM demonstrates the described concepts. It\nreads a scene file (in OBJ format) and displays a wireframe representation of the\nmesh. A small dynamic object is also loaded. Per frame, the dynamic object is rotated,\nand a small kD-tree is built. The beam consists of 5,000 rays, of which every 64th ray\nis drawn. These rays are traced through the static kD-tree and the dynamic kD-tree to\ndetermine visibility from the center of the scene. Figure 2.2.8 is a screenshot from the\ndemo. Note that even though the visualization is 2D, the actual data is 3D, and so are\nthe ray queries.\n2.2\nFast Generic Ray Queries for Games\n139\n",
      "content_length": 2684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "Conclusion\nGeneric ray queries in a polygonal environment offer a powerful tool for many oper-\nations in a game. This gem described how these queries are efficiently implemented\nusing a high-quality kD-tree based on the surface area heuristic. Dynamic geometry is\nstored in a less efficient kD-tree, which can however be built much quicker.\nImplemented well, the presented approach lets you trace 1 million rays per sec-\nond easily. Should you need more, it is worthwhile to explore SIMD-enhanced packet\ntraversal (traversing multiples of four rays simultaneously).\nYou might even want to explore the fascinating world of real-time ray traced\ngraphics, one of those rare algorithms that you can never throw enough processing\npower at. You will love the intuitive nature of ray tracing—whether you use it for\ngraphics or other purposes.\nReferences\n[Appel63] Appel, A. “Some Techniques for Shading Machine Renderings of Solids,”\nProceedings of the Spring Joint Computer Conference 32 (1968), pp. 37–45.\n[Havran01] Havran, V. “Heuristic Ray Shooting Algorithms,” PhD thesis, Czech\nTechnical University, Praha, Czech Republic, 2001.\n[McDonald90] MacDonald, J., and Booth, K. “Heuristics for Ray Tracing using Space\nSubdivision,” The Visual Computer, Vol. 6, No. 3 (June 1990), pp. 153–166.\n140\nSection 2\nMath and Physics\nFIGURE 2.2.8\nThe demo application displays a wireframe\nrepresentation of the mesh.\n",
      "content_length": 1400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "[Wächter06] Wächter, C., and Keller, A. “Instant Ray Tracing: The Bounding Inter-\nval Hierarchy,” In Rendering Techniques 2006, Proceedings of the 17th Eurographics\nSymposium on Rendering (2006), pp. 139–149.\n[Wald04] Wald, I. “Realtime Ray Tracing and Interactive Global Illumination,” PhD\nthesis, Saarland University, 2004.\n[Wald06a] Wald, I., and Havran, V. “On Building Fast kD-trees for Ray Tracing, and\nOn Doing That in O(NlogN),” Proceedings of the 2006 IEEE Symposium on\nInteractive Ray Tracing (2006), pp. 61–69.\n[Wald06b] I. Wald, I., Ize, T., Kensler, A., Knoll, A., and Parker, S. “Ray Tracing Ani-\nmated Scenes Using Coherent Grid Traversal,” ACM Transactions on Graphics,\nProceedings of ACM SIGGRAPH 2006, pp. 485–493.\n[Wald07] Wald, I., Boulos, S., and Shirley, P. “Ray Tracing Deformable Scenes Using\nDynamic Bounding Volume Hierarchies,” ACM Transactions on Graphics (2007),\nVol. 26, No. 1.\n[Whitted79] Whitted, T. “An Improved Illumination Model for Shaded Display,”\nCommunications of the ACM (August 1979), Vol. 23, No. 6, pp. 343–349.\n2.2\nFast Generic Ray Queries for Games\n141\n",
      "content_length": 1098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "143\n2.3\nFast Rigid-Body Collision\nDetection Using Farthest\nFeature Maps\nRahul Sathe, Advanced Visual Computing,\nSSG, Intel Corp.\nrahul.p.sathe@intel.com\nDillon Sharlet, University of Colorado \nat Boulder\ndillon.sharlet@colorado.edu\nW\nith the rapidly increasing horsepower of processors and graphics cards, physics\nhas become an important aspect of real-time interactive graphics and games.\nWithout physics, although every frame might look quite realistic, the overall realism\nwill be missing during the gameplay. Modeling of real-world physical phenomena can\nbe quite mathematical in nature and computationally intensive. Game developers\nusually try to simplify the models without compromising visual fidelity. At the same\ntime, they try to improve the computational efficiency of the models. One of the\nmost computationally intensive tasks is collision detection of objects in gameplay.\nThis involves determining whether two objects are colliding (or have collided in the\nlast timestep). If they are colliding, the physical simulation requires some more quan-\ntities like penetration depth and a separating axis.\nThis gem tries to solve some of the complexities involved in the process using\nsome acceleration data structures. It introduces a new data structure called the farthest\nfeature map that is used to accelerate the discovery of a potentially colliding set (PCS) of\ntriangles at runtime. This algorithm, like our previous algorithm based on distance\ncube-maps, works only with convex rigid bodies and has a preprocessing step and a\nruntime step. \nThe intuition behind this new proposed approach is as follows—convex rigid\nbodies when far from each other behave roughly like point masses. However, when\n",
      "content_length": 1712,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "they approach each other, this point mass approximation is not good enough. At that\npoint, you must perform more detailed analysis using the local properties of the bod-\nies near the contact manifold. \nBackground\nCollision detection can be modeled in two ways—discrete collision detection or con-\ntinuous collision detection. The former updates the positions of the game objects at\ndiscrete timesteps and tries to find out if two objects are intersecting at a moment in\ntime. One technical approach to discrete collision detection tries to find intersecting\ntriangles by using a recursively subdivided hierarchy of bounding boxes that are either\naxes aligned (AABB) [Bergen97] or oriented (OBB) [Gottschalk96]. Continuous\ncollision detection systems use a variable timestep and modify that timestep for each\npotentially colliding pair such that two objects never intersect each other. Continuous\ncollision detection is often modeled by algorithms like the one by Gilbert Johnson\nand Keerthy [GJK88]. Refer to these and other references for further details on the\nfundamentals of each approach.\nOur recent work tries to move a lot of work to the preprocessing step for rigid\nbodies [Sathe07]. At runtime, all you do is access the distance cube-maps to find the\ncollision. Distance cube-maps access can be hardware accelerated but there is some\nroom for improvement and optimization, which the new farthest feature map concept\ncan provide.\nFollowing are some terms that are used throughout this gem:\n• Principle curvatures: In differential geometry, principle curvatures at any point on a\nsurface are the curvatures corresponding to the curves with maximum and mini-\nmum curvature at that point. These curves are always at right angles to each other.\n• Mean curvature: The average of principle curvatures is called the mean curvature.\n• Best-fitting sphere: For the typical game mesh, represented by planar triangles with\ncreases at neighboring edges rather than continuous geometry with smooth deriv-\natives at every seam, the curvature definitions cannot be applied in a meaningful\nway to many algorithms that require geometric interrogation. So you have to come\nup with some approximation of the curvatures. The ring-1 neighborhood of a vertex\nis the triangle fan around that vertex. We consider the ring-1 neighborhood\naround a given vertex and try to find the sphere that has center along the normal\naxis passing through that vertex. This circle is a crude approximation of the sphere\nwith the radius equal to that corresponding to mean curvature for a triangle mesh. \nPreprocessing\nThis approach uses a data structure that is similar to a cube-map; that is, it is a directional\nlookup, although the stored data exceeds the capacity of cube-map texture formats for\ncurrent generation graphics hardware. Place the object’s centroid at the origin. Then\nimagine an axis-aligned cube centered at the origin and then shoot rays from the origin\n144\nSection 2\nMath and Physics\n",
      "content_length": 2973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "through all the pixel centers on the faces of the cube-map. For each of these rays, you pro-\nject the line segment from the centroid to the vertices on this ray. You select the maxima\nof these projections and store the corresponding vertex (vertices) for that sampled direc-\ntion. We call this data structure a farthest feature in that sampled direction.\nAn intuitive way to visualize a farthest feature map is to think of a perpendicular\nplane for a given sampled direction. Then, think of moving that plane outward from\nthe centroid keeping it perpendicular to that sampled direction. This plane will inter-\nsect with the convex object to yield some polygon in that plane. If you keep on mov-\ning this plane outward, eventually the plane will intersect with one or more of the\nvertices of the convex model. As you continue moving the plane farther away from\nthe centroid, it will eventually move far enough to not intersect with the convex\ngeometry. At this point, the distance to the perpendicular plane is the farthest dis-\ntance in that direction and any vertices from geometry that lay on the plane just prior\nto the farthest distance are the farthest features.\nFigures 2.3.1 through 2.3.3 show this in the 2D case. Here, the thick solid line is\nthe convex geometry of the game object being processed. The geometry is defined\nhere by vertices V0 through V4. The plane is shown in thick dotted lines. Arrows rep-\nresent the directions that they are being moved in order to find the farthest features\nfor the two sampled directions dir1 and dir2.\n2.3\nFast Rigid-Body Collision Detection Using Farthest Feature Maps\n145\nFIGURE 2.3.1\nPlanes perpendicular to the sampled directions\ndir1 and dir2 are shown in dotted lines. They are moving away\nfrom the centroid.\n",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "146\nSection 2\nMath and Physics\nFIGURE 2.3.2\nPlanes moving farther away from the\ncentroid.\nFIGURE 2.3.3\nPlanes perpendicular to the sampled\ndirections dir1 and dir2 in their farthest positions. V0\nand V1 form the farthest features in dir2 direction and\nV2 is the farthest feature in dir1 direction.\n",
      "content_length": 298,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "Figures 2.3.4 and 2.3.5 illustrate the construction of the farthest feature map in\n3D, showing the farthest features in two different sampled directions.\n2.3\nFast Rigid-Body Collision Detection Using Farthest Feature Maps\n147\nFIGURE 2.3.4\nThe plane that is perpendicular to dir1 moving\naway from the centroid C is in its final position. CQ1 is the\nprojection of CP1 on dir1. P1 and Q1 lie on the plane.\ni\nFIGURE 2.3.5\nThe plane that is perpendicular to dir2 moving away\nfrom the centroid C is in its final position. CQ2 is the projection of\nCP2 on dir2. P2 and Q2 lie on the plane.\n",
      "content_length": 582,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "The data that you store in a given sampled direction (equivalent to a single pixel\ncenter in the cube-map analogy) is represented by class Node. Here, you store the\nindex of the vertex representing the farthest feature along the direction. If there is\nmore than one vertex, you store the index of all of them for this direction. At runtime,\nyou can start off with any one of these features.\nThe detailed farthest feature information for the vertices is stored separately. \nFor every vertex, you store its neighborhood information as shown in the class \nVertexNeighborhood. You store the following data per vertex—the center of the best\nfitting sphere, as well as equations of planes, face IDs, and edges. You also store the\nnormal and center for the best-fitting sphere for each of the vertices that form the\nfarthest feature for a given direction. Edges are used to find edge-edge intersections.\nThe reason for splitting this into per vertex and per direction (farthest feature\nmap) is storage optimization. For low poly objects, if you oversample the farthest fea-\nture map, you want to store as little information as possible per sample; for example,\nthe indices into the VertexNeighborhood buffer.\nClass Node \n{\nint *Index;\n}\nClass VertexNeighborhood\n{\nVector < Plane > Faces;\nVector < DWORD > Triangles;\nVector < Pair < DWORD, DWORD > > Edges;\nVector3 CenterOfBestFittingSphere;\n}\nThe best fitting sphere at the farthest feature is the sphere that best fits the trian-\ngle fan around the farthest feature. In the 2D case, this will correspond to a circle that\nbest fits the ring-1 neighborhood of the vertex, which for a 2D piecewise linear curve\nis always two vertices. It’s always possible to find the exact circle that passes through\nthree points (unless they are collinear). This is shown in Figure 2.3.6. In the 3D case,\nit usually is not possible to find a sphere that exactly passes through all of the ring-1\nvertices, because ring-1 will usually contain more vertices than needed to minimally\ndefine a sphere, and the surface will rarely happen to be exactly spherical at any given\npoint.\nRuntime Queries\nAt runtime, you start off with the assumption that two convex meshes under consid-\neration behave like a point mass. This assumption is true when they are far away from\neach other. The question that you have to answer then is how far is far enough? It\nturns out that you can approximate these objects with point masses so long as their\nbounding volumes do not intersect each other. The bounding volumes that are used\n148\nSection 2\nMath and Physics\n",
      "content_length": 2566,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "to approximate the object determine how far the objects can be. Bounding spheres,\naxis-aligned bounding boxes, and oriented bounded boxes are popular bounding\nvolume approximations.\nAt runtime, you proceed as follows. First, connect the two centroids. Along this\ndirection connecting two centroids, find the farthest features of the two objects using\nfarthest feature-maps that were created during preprocessing. In some sense, you have\nused the farthest feature map to quickly generate the bounding volumes. However,\nthey differ from conventional bounding volumes because they can be different along\nany direction. In this case, you are looking at the bounds along the line joining two\ncentroids.\n2.3\nFast Rigid-Body Collision Detection Using Farthest Feature Maps\n149\nFIGURE 2.3.6\nBest fitting circle in 2D.\nIf the sum of the distances to the farthest feature maps from the respective cen-\ntroids along the line joining centroids is more than the distance between the centroids,\nyou know that the bounding volume test along this axis has failed, and the objects\nbelong to the potentially colliding set. At this point, you lose the liberty to treat convex\nobjects as point masses located at their centroids and you have to perform a more\ndetailed analysis.\nIn this case, you query the farthest feature map to find the center of the best\nfitting sphere at the farthest features for both the objects. Then, connect these centers\nof best fitting spheres at farthest features of the two objects. This step in some sense\napproximates the two interacting objects locally with spheres of radii corresponding\nto the best fitting spheres. The important thing to note here is that the approximation\n",
      "content_length": 1690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "is only a “local” approximation. Find the farthest features along the line joining these\ncenters and do the distance test that you did earlier when you joined the centroids. If\nthe distance test fails, continue finding the best fitting spheres at the farthest features\nuntil, at two successive steps, you find the same farthest feature. At this point, you\nconclude the algorithm.\nThe pseudocode for the algorithm follows:\nlastC1 = lastC2 = null;\nConnect the two centroids C1C2\nFind the distance C1C2 between two centroids\nFind the distance d1 and d2 to farthest features along C1C2\nwhile (C1C2 < d1+d2 || lastC1 != C1 || lastC2 != C2)\n{\nlastC1 = C1\nlastC2 = C2\nC1 = center of best fitting sphere at farthest feature \nOf object 1\nC2 = center of best fitting sphere at farthest feature \nOf object 2\nd1 = Distance to farthest feature along C1C2 from C1\nd2 = Distance to farthest feature along C2C1 from C2\n}\nWhen you conclude the algorithm, you are left with two farthest features from two\nobjects and their ring-1 neighborhoods. For a typical game mesh, these two triangle fans\nwill have around six triangles each. At this point, you have to determine if a triangle from\nthe triangle-fan for one object intersects with any triangle from the triangle-fan cor-\nresponding to the other object. You thus are left with having to do triangle-triangle\nintersection tests for typically 36 (triangle-triangle) pairs. All these triangle-triangle\nintersection tests can be performed in parallel, using the appropriate processor technol-\nogy and programming language.\nPerformance Analysis and Concluding Remarks\nIn most cases, the algorithm converges in O(1) time. This will be the case when the\ndistribution of the vertex density of the two convex meshes doesn’t vary too much\nwith the directions. If the contact point is in the region where concentration of\nvertices is higher, the algorithm takes longer to converge.\nIt will be interesting to see how this algorithm can be extended to work with con-\ncave objects. With the ever-increasing general programmability of the graphics hard-\nware, we want to look at how one can exploit a flexible cube-map that can store more\nthan just a fixed format texture data. Adaptively sampled cube-maps will save a lot of\nstorage but will increase the lookup costs. Some clever encoding scheme that solves\nthis problem will be nice. \n150\nSection 2\nMath and Physics\n",
      "content_length": 2389,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "Acknowledgments\nWe would like to thank the management group in Advanced Visual Computing\nGroup, SSG at Intel for allowing us to pursue this work. We would also like to thank\nour co-workers Adam Lake and Oliver Heim for their help at various stages of this\nwork. \nReferences\n[Bergen97] Van den Bergen, G. “Efficient Collision Detection of Complex\nDeformable Models Using AABB Trees,” Journal of Graphics Tools, Vol. 2, No. 4,\npp. 1–14, 1997.\n[GJK88] Gilbert, E.G., Johnson, D.W., and Keerthi, S.S. “A Fast Procedure for Com-\nputing the Distance Between Objects in 3 Dimensional Space,” IEEE Journal of\nRobotics and Automation, Vol. RA-4, pp. 193–203, 1988.\n[Gottschalk96] Gottschalk, S., Lin, M.C., and Manocha, D. “OBBTree: A Hierarchi-\ncal Structure for Rapid Interference Detection,” Proc. SIGGRAPH 1996, pp.\n171–180.\n[Sathe07] Sathe, Rahul. “Collision Detection Shader Using Cube-Maps,” ShaderX5\nAdvanced Rendering Techniques, Charles River Media, 2007.\n2.3\nFast Rigid-Body Collision Detection Using Farthest Feature Maps\n151\n",
      "content_length": 1029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "153\n2.4\nUsing Projective Space \nto Improve Precision of\nGeometric Computations\nKrzysztof Kluczek, Gda´nsk University \nof Technology\nkrzych82@poczta.onet.pl\nM\nany geometric algorithms used in modeling, rendering, physics, and AI modules\ndepend on point-line and, in case of three-dimensional space, point-plane tests.\nThe finite precision of computing introduces problems when collinear or coplanar cases\nare to be detected. A common solution to this problem is detecting such cases based on\na set minimum distance (epsilon), below which elements being processed are assumed\nto be collinear or coplanar. This set minimum distance solves the problem only partially\nbecause checking results against such a distance is prone to signaling false-positives,\nwhich reduces the overall robustness of any algorithm using such a solution.\nIn order to create robust geometric algorithms that will operate correctly in every\ncase, you can’t use any operations that will truncate intermediate results (for example,\noperations on floating-point numbers). Truncation leads to loss of information, which\nunder certain circumstances can be needed to obtain results. This leaves math based on\nintegers the only way to ensure that truncation doesn’t happen. The straightforward\nrepresentation of points using integer Cartesian coordinates isn’t the solution because\nthe intersection of a pair of lines with endpoints with integer coordinates doesn’t\nnecessarily occur at integer coordinates. Using rational numbers can solve the problem\n(see [Young02]), but this requires storing six integer values per vector (numerator and\ndenominator for each component) and implementations of efficient operations on\nsuch vectors aren’t straightforward in three-dimensional space. Using projective space\ncan reduce this number to four integers per vector by using vectors with one extra\ndimension and storing only a single integer per vector component, which is demon-\nstrated in this chapter.\n",
      "content_length": 1961,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "Projective Space\nThe concept of projective space is fundamental to understanding algorithms presented\nhere. RP2 projective space can be described as a space of all lines in R3 space passing\nthrough point [0,0,0], as shown on Figure 2.4.1. Each line can be uniquely identified\nby a point in R3/[0,0,0], which lies on the line, and so you can use an [x,y,z] coordi-\nnate vector to identify each element in this space, where [x,y,z] \u0004 [0,0,0]. Still, \nyou have to keep in mind that any pair of vectors P and Q identifies the same line if\nP = Qc for certain scalars c \u0004 0. Representation of the elements of RP2 as [x,y,z]\nvectors is known as homogeneous coordinate representation. Keeping in mind that\nelements of RP2 can be understood as lines crossing the origin of R3, let’s define a \nz = 1 plane in this R3 space. Every line crosses the z = 1 plane exactly once, unless it’s\nparallel to this plane. The intersection point can be calculated from the line equation\nand in the case of lines crossing the origin it is located at [x/z,y/z,1] where [x,y,z] is any\npoint on the line other than [0,0,0]. Therefore, you can easily relate points in R2 and\nelements of RP2. Any point [s,t] in R2 can be represented by element [s,t,1] in RP2 or\nin general, any [s,z,tz,z] where z \u0004 0. Therefore, any vector [x,y,z] with z \u0004 0 repre-\nsenting an element of RP2 space will represent point [x/z,y/z] in R2 space. This repre-\nsentation can be easily extended to higher-dimensional spaces, where [p1,...,pn,w] in\nRPn is related to [p1/w,...,pn/w] in Rn.\n154\nSection 2\nMath and Physics\nFIGURE 2.4.1\nRP2 projective space as a space of lines\nin R3 crossing at the origin. Each line can be uniquely\nidentified by a point in R3/[0,0,0] and each line crosses\nthe Z = 1 plane exactly once unless it’s parallel to it.\nThis gem focuses on the representation of R2 space by RP2 projective space and\nrepresentation of R3 space by RP3 space. The printed portion of this gem focuses on\noperations in R2 space using RP2 projective space. Because a point in RP2 space can\nbe represented by a three-component vector, computations on such data are far easier\nto imagine and understand than computations on four-component vectors used to\nrepresent elements in RP3 space. The CD-ROM contains a separate document that\nexpands the concepts described for RP2 space for application to RP3 space to allow\ncomputations on three-dimensional data.\n",
      "content_length": 2403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "Basic Objects in R2\nIn two-dimensional space R2, you will focus on two types of objects, points and\ndirected lines, which let you define more complex structures. You define a directed\nline in R2 space as a line with specified running direction, which allows us to define\nleft and right sides of the line with respect to its running direction. This directional\nfeature of the line definition is essential for efficient polygon representation, where we\ncan assume that the interior of the polygon lies on a certain side of the line. In the case\nof convex polygons, which can be represented by an ordered list of directed lines\ndefining its boundary, the above assumption can lead to very efficient algorithms for\noperating on such polygons. Just like convex polygons, many other objects in R2 such\nas segments and rays (half-lines) can be represented using points and directed lines, so\nit’s safe to focus only on these two types of objects.\nPoints and Directed Lines in RP2\nAs it was defined at the beginning of this chapter, a point [s,t] in R2 can be represented\nby element [s,t,1] in RP2 space, so conversion from R2 to RP2 space is straightforward.\nBecause scaling vectors in RP2 space doesn’t affect their meaning, point [s,t] can be rep-\nresented by any vector [sz,tz,z] where z \u0004 0. For simplicity of future computations, let’s\nassume z \u0005 0. If this is not the case, the entire vector can be scaled by –1 to fix this.\nRecall that vectors with z = 0 don’t represent valid points in R2. To obtain the formula\nfor conversion of vectors from RP2 projective space to points in R2 space, you can use\nthe same rule. Every vector [x,y,z] in RP2 space represents the point [x/z,y/z] in R2\nspace. This is similar to perspective projection onto z = 1 plane with the center of pro-\njection placed at the origin. Therefore, you can think of RP2 space almost like R3 space\nkeeping in mind that it is not the vector [x,y,z] that is important, but its perspective\nprojection onto the z = 1 plane, which results in the vector [x/z,y/z,1] representing\npoint [x/z,y/z] in R2 space. As you can see, this is very similar to the representation of\npoints in RP2 using rational coordinates with a common denominator (in this case z),\nbut defining it as a vector in RP2 space will give you efficient tools for performing com-\nputations on such points. Functions used to perform transformations of points\nbetween R2 and RP2 spaces are shown in Equations 2.4.1 and 2.4.2. Note that these\nfunctions operate on elements of R3 space as they take an argument or result in a vec-\ntor in R3 referring to one of the elements (lines passing through the origin) of RP2 pro-\njective space and not the element of RP2 projective space itself.\n(2.4.1)\n(2.4.2)\nThe other object in R2 space of particular interest is a directed line. Just as points\nare perspective projections of vectors onto the z = 1 plane, lines in RP2 are perspective\ng\nx y z\nx z y z\n, ,\n/ , /\n⎡⎣\n⎤⎦\n(\n) =⎡⎣\n⎤⎦\ng\nz\n: R\nR\n≠→\n0\n3\n2\nf\ns t\ns t\n,\n, ,\n⎡⎣\n⎤⎦\n(\n) =⎡⎣\n⎤⎦\n1\nf :\n,\nR\nR\n2\n3\n→\n2.4\nUsing Projective Space to Improve Precision of Geometric Computations\n155\n",
      "content_length": 3094,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "projections of planes onto the z = 1 plane. The only case when perspective projection\nof a plane results in a line is when the plane passes through the center of the projec-\ntion, so every plane you use to define a line has to pass through point [0,0,0]. In fact,\naccording to the definition of RP2 space you use, you can’t define a plane in this space\nthat does not pass through the origin, because RP2 space elements are lines passing\nthrough the origin and every plane you define in RP2 must contain entire lines. The \nz = 1 plane is the only exception to this rule because it is used only for projection of\nRP2 space to R2 space. Considering that every plane is passing through the origin, the\ndefinition of a line as projection of a plane in RP2 onto z = 1 plane can be simplified.\nThe defined line is simply the intersection of the given plane and the z = 1 plane.\nAgain, because all such planes pass through the origin, it’s sufficient to store only their\nnormal vectors. You don’t require a normal to be normalized and in general it won’t be\nnormalized, because you will be using only integer coordinates for the normal vectors.\nThe previous definition of a line can be extended to a definition of a directed line\nby simply taking into account the direction of the normal vector of the plane used for\nthis representation. You define the positive side of a plane as a half-space containing all\nvectors for which the dot product with the normal of this plane results in a positive\nvalue. To put it more straightforward, the positive side of the plane is the half-space the\nnormal vector is pointing at. Similarly, you can define the negative side of this plane.\nBecause a line represented with a plane is an intersection of this plane with the z = 1\nplane, the plane divides the z = 1 plane into two half-planes, one lying in the positive\nhalf-space and the other in the negative one. You can call the half-plane on the positive\nside the right side of the directed line and the other half-plane the left side. From this,\nyou can derive the direction of the directed line, which makes complete definition of\nrepresentation of a directed line with a plane in RP2 space. Figure 2.4.2a contains an\nexample representation of a directed line with a plane in RP3 projective space.\n156\nSection 2\nMath and Physics\nFIGURE 2.4.2\nPoints and lines in RP2 projective space: (a) a line passing through a\npair of points; vectors u and v represent points u’ and v’ and the line is represented\nby a plane with normal N, (b) intersection of a pair of lines; N and M are normals\nof planes representing the lines and their cross product results in a vector represent-\ning the intersection point.\n",
      "content_length": 2681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "Basic Operations in RP2\nThere are three basic operations on points and directed lines. Given an ordered pair of\npoints, you can find a directed line passing through them in the given order. Given a\npoint and a directed line, you can determine which side of the line the point lies on or\nif it lies on the line. Finally, given a pair of directed lines you can find their intersec-\ntion point. All these operations are fairly easy and straightforward in RP2 space.\nGiven a pair of vectors representing two points in RP2 space, you can look for a\ndirected line passing through the points. Because of the nature of RP2 space, both\nvectors representing the points have to lie on the plane representing the line you are\nlooking for; otherwise, the points resulting from their perspective projection wouldn’t\nlay on this plane as every vector lying on the plane is parallel to the plane normal.\nGiven a pair of such vectors, you can find the normal you are looking for using the\ncross product of these vectors. An example of a line led through a pair of points is\nshown in Figure 2.4.2a. The normal computed this way will correctly define the plane\nin RP2 space representing the directed line you are looking for. The direction of the\nline depends on the ordering of points used to compute the line, because swapping\ncomponents of cross product inverts the result. It’s desired that the line computed in\nthis operation will be directed in such a way that its running direction will be the\ndirection from first point to the second one, so you have to make sure that computed\nnormal correctly defines the right side of the directed line by correctly defining the\npositive side of the plane in RP2 space. Because this depends on the handedness of \nthe coordinate system being used, be sure to check this during the implementation of\nthis operation and reverse the order of the vectors in the cross product if needed.\nAnother operation of particular interest is a point-line test, where you can find\nwhere a given point lies with respect to a given directed line. As stated previously,\nwhen a point lies on the line, the vector in RP2 space representing the point will lie on\nthe plane representing this line and therefore it will be perpendicular to the plane’s\nnormal. This makes the dot product the perfect tool for this check. If a result of a dot\nproduct of vector representing the point and the normal of the plane is zero we can be\nsure that the point lies on the line. Because the normal of the plane defines its positive\nside and the right side of the directed line, if the point doesn’t lie on the line, you can\nuse the sign of the dot product to determine the side of the line the point lies on.\nThanks to the assumption of z \u0005 0 for coordinates of the vector representing the\npoint, the result of the dot product will always be positive when the point lies on the\nright side of the directed line and it will always be negative when it lies on the left side\nof this line.\nThe last important basic operation on points and directed lines is finding the\nintersection point of a pair of lines. Again, the vector representing the point you are\nlooking for has to be perpendicular to the normals of both planes representing the\ngiven directed lines, as shown on Figure 2.4.2b. By performing the cross product on\nthese normals, you can find the vector in RP2 space representing the point you are\nlooking for. To make sure that the z \u0005 0 condition is met, even when z \u0006 0 in the\n2.4\nUsing Projective Space to Improve Precision of Geometric Computations\n157\n",
      "content_length": 3549,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "resulting vector, the entire vector has to be scaled by –1 to obtain the proper vector rep-\nresenting the point. If the given directed lines are parallel, the resulting vector will have\nthe coordinate z = 0, which indicates that intersection point of these lines doesn’t exist.\nPrecise Geometrical Computations in RP2 Using\nInteger Coordinates\nAll of the operations described previously are based mainly on basic comparisons and\ndot and cross products of vectors. Both dot and cross products require addition, sub-\ntraction, and multiplication to implement. Note that if only vectors with integer\ncoordinates are used, none of these operations will result in a fractional number, or a\nvector with fractional components. Therefore, by using only points with integer coor-\ndinates, these operations are guaranteed to return exact results, free of numerical\nerrors that are inherent to floating point computations. Unfortunately, to make such\na system usable, you have to make sure it works correctly with floating point input\ndata as most existing systems, like modeling packages, can provide only floating point\ndata. In most cases, you can choose a certain finite precision (for example, 10–3), scale\nthe data up, round it to nearest integers and, after performing all necessary opera-\ntions, scale the result back. Note that rounding used here only affects positions of\ninput points, but doesn’t affect further operations, especially tests where it should be\ndetermined whether or not a point lies on a line.\nNumber Range Limits in Geometrical Computations in RP2\nBecause integer multiplication is a frequent operation in computations described in\nthis chapter, you have to be aware of range limits of an integer representation. Con-\nsecutive multiplications quickly make the values you operate on large. To estimate\nranges used in each stage of computations, you can use symmetrical range estimates\n[–a,a] that define the minimum and maximum values that can be achieved at a given\nstage in a worst-case scenario. Knowing the range of input values, you can easily esti-\nmate the range of results of operations such as addition, subtraction, and multiplica-\ntion and using this, the range of results of more complex operations like dot and cross\nproducts can be estimated. Having two values within ranges [–a,a] and [–b,b], you\ncan be sure that their sum will be within range [–a–b,a+b]. Because the estimated\nranges are symmetrical, the difference of a pair of given values results in values within\nexactly the same range [–a–b,a+b]. Similarly, the result of multiplication of two values\nwithin ranges [–a,a] and [–b,b] can be proven to lie within the range [–ab,ab]. Know-\ning this, you can analyze the numerical ranges required for performing computations\nat every step.\nBecause consecutive operations on points result in consecutive multiplications\nintroducing large numbers, you can limit operations to three classes of objects that are\nenough for most geometric computations. The first class is a point being a part of\ninput data. Such a point has its coordinates in R2 space given explicitly (for example,\n158\nSection 2\nMath and Physics\n",
      "content_length": 3144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "imported from a digital content creation program) and therefore you can easily pre-\ndict the range of its coordinates based on game level extents and the required preci-\nsion of its representation. The extents and precision are used for scaling model\ncoordinates during the import of the data. Although it is desired to increase allowed\nrange of coordinates of input points because it allows for larger game levels and/or\nmore precision, this range is the main factor influencing all numerical ranges used in\ncomputations. The numerical range analysis aims to find a compromise between algo-\nrithm efficiency (depending on ranges of intermediate values) and range and preci-\nsion of input data, because large integers require both more storage space as well as\nmore processing power.\nThe second class of objects is directed lines computed as lines passing through a\npair of points from input data. Input point coordinates are given explicitly, so the nor-\nmal vectors of planes representing directed lines of this class are obtained using a sin-\ngle cross product. You can easily estimate the ranges required to represent components\nof normal vectors of planes representing such lines.\nThe last class of objects being used is a class of points obtained as a result of inter-\nsection of a pair of lines of previous class. Vectors representing such points in RP2\nspace are the results of cross products on normal vectors of planes representing the\nlines, which make these vectors the results of two consecutive cross products. In\neffect, the range of their coordinates is larger than the range of coordinates of vectors\nrepresenting other classes of objects.\nBecause of a growing numerical range of coordinates used with the various classes\nof objects, you should aim to use only these three classes of objects (points from input\ndata, lines led through such points, and intersection points of these lines). Operations\nresulting in objects outside these three classes, like defining a directed line passing\nthrough an intersection point, should be avoided because they can make resulting\nnumbers grow without a control. Fortunately, many geometrical algorithms, includ-\ning constructive solid geometry (CSG) algorithms, can be implemented in a way that\ndoesn’t require operations on objects outside the three classes specified. If for some\nreason an algorithm cannot be implemented with just the three listed classes, you can\nintroduce new classes of objects, such as lines passing through a pair of intersection\npoints. But be aware that the numerical range required for operations on these new\nclasses can grow exponentially with the number of classes of objects and this range\nshould be estimated for each introduced class.\nThe analysis of numerical range being used in described operations is shown in\nTable 2.4.1. It can be seen that subsequent operations make numerical range of result-\ning values grow quickly. Table 2.4.2 shows the maximum ranges of input point coor-\ndinates and ranges of further computation results depending on number of bits used\nto perform the computations. Even with 64-bit integers, the allowed input point\ncoordinate range is only [–20936,20936]. In some applications, the [–20936,20936]\nrange can be sufficient and in this case algorithms described can be implemented\n2.4\nUsing Projective Space to Improve Precision of Geometric Computations\n159\n",
      "content_length": 3378,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "without much effort, allowing geometric computations even on devices lacking float-\ning point support (for example, cell phones). However, in many applications this\nrange won’t allow for required precision and game level extents. For these applica-\ntions, long integer math can provide a solution.\nTable 2.4.1\nNumerical Ranges of Results in Used Projective Space Operations for\nTwo-Dimensional Geometry\nObject\nEquation\nCoordinates\nRange\nInput point (P)\nP = [x,y,1]\nx,y\n[–n,n]\nz\n1\nNormal of a plane (N)\nN = P1 × P2\nx,y\n[–2n,2n]\nz\n[–2n2,2n2]\nIntersection point (Q)\nQ = N1 × N2\nx,y\n[–8n3,8n3]\nz\n[–8n2,8n2]\nInput point check versus line\nN \u0007 P\n[–6n2,6n2]\nIntersection point check versus line\nN \u0007 Q\n[–48n4,48n4]\nTable 2.4.2\nMaximum Values Allowed at Every Step with Respect to Length of Used\nInteger Representation\nRange\n16 Bits\n32 Bits\n64 Bits\n[–n,n]\n5\n81\n20 936\n[–2n,2n]\n10\n162\n41 872\n[–2n2,2n2]\n50\n13 122\n876 632 192\n[–8n2,8n2]\n200\n52 488\n3 506 528 768\n[–8n3,8n3]\n1 000\n4 251 528\n73 412 686 286 848\n[–48n4,48n4]\n30 000\n2 066 242 608\n9 221 808 000 608 698 368\nMaximum value\n32 767\n2 147 483 647\n9 223 372 036 854 775 807\nTo find out the length of the integer representation required for a given applica-\ntion, you have to decide how large the game level extents are and how much precision\nyou need. With that information, you can derive the required range of integer coordi-\nnates being used. For example, if desired workspace size is [–1000,1000] range of\npoint coordinates and required precision is 0.01, after scaling the point data during\nthe import, the required range of input coordinates of points used in further opera-\ntions is equal to [–100000,100000]. Then you can find out the ranges required to\ncarry out operations being used without risking overflows. This in turn gives you the\nnumber of bits required for geometry representation with integers (be sure to include\nthe sign bit in this number of bits).\n160\nSection 2\nMath and Physics\n",
      "content_length": 1946,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "Although the maximum range used can require a large number of bits for integer\nrepresentation (often over 64 bits), this number is required only for the most complex\ncases; most basic operations can be performed using much shorter integers. To find\nthe number of bits required, in every case an analysis similar to the one given previ-\nously should be done. The range estimates given in Table 2.4.1 can be useful during\nimplementation of used long integer math, because these estimates can be used to\nfind required range of integers according to input data point coordinate range.\nExample Application of Operations in RP2\nTo prove the usefulness of geometric operations in RP2 space, a simple algorithm per-\nforming Boolean operations on polygons is presented. For simplicity, polygons are\nassumed to be convex, as every set of input polygons can be partitioned into a set of\nconvex polygons. A convex polygon can be described with a loop of edges. Each edge\nis defined by its points and a directed line running along the edge in such a direction\nthat the interior of the polygon lies on the right side of this line. The example assumes\nthat in a polygon no three vertices are collinear.\nThe basic operation useful during Boolean operations on convex polygons is\ncutting a polygon with a directed line. This operation can be accomplished using the\nfollowing algorithm, which is illustrated in Figure 2.4.3:\n1. For each polygon vertex, determine which side of the cutting line the vertex\nlies on or whether it lies on the line.\n2. If there are no vertices on the right side of the cutting line or there are no\nvertices on its left side, stop. The line doesn’t intersect the interior of the\npolygon.\n3. Split each edge for which starting and ending points lie on opposite sides of\nthe cutting line. Two edges are created in place of the edge being split. The\nmiddle point of the split is the intersection of the line running along the\nedge and the cutting line.\n4. Create the first of resulting polygons from edges lying on the right side of\nthe cutting line. Close this polygon by adding an edge running along the\ncutting line between vertices lying on this line (if the initial polygon was\ncorrect, there are exactly two such vertices).\n5. Similarly, create the second resulting polygon from edges lying on the left\nside of the cutting line. Close this polygon by adding an edge running along\nthe cutting line, but in the opposite direction (reverse the normal of the\nplane defining the cutting line in RP2 space).\nIt’s worthwhile to note that this algorithm uses only the three classes of objects in\nRP2 space listed earlier. In step 3, new intersection points are introduced to the poly-\ngon and in steps 3, 4, and 5, existing directed lines are used to define polygon edges.\nThe algorithm doesn’t create new lines using existing points, so it isn’t important whether\nthe vertices in the initial polygon are points from input data or intersection points.\n2.4\nUsing Projective Space to Improve Precision of Geometric Computations\n161\n",
      "content_length": 3035,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "The algorithm will work flawlessly on both types of vertices. It’s also worthwhile to\nnote that in step 3, the splitting point of the edge has to be computed as an intersec-\ntion of a pair of lines. In the case of floating point–based computations this point\ncould have been computed using linear interpolation of edge endpoints based on dis-\ntances of these points to the cutting line. In the case of integer-based operations using\nRP2 space, you don’t have an operation computing point-line distance defined. How-\never, as this algorithm shows, this operation isn’t required in this case.\nAlthough cutting polygons with lines can be a useful operation by itself, you can\nuse this operation to implement Boolean set operations on polygons. The following\nalgorithm describes the initial steps required to perform such operations, which is\nillustrated in Figure 2.4.4.\n162\nSection 2\nMath and Physics\nFIGURE 2.4.3\nCutting a polygon with a line: (a) determining which side of the line each\nvertex lies on (steps 1 and 2), (b) finding intersection vertices and splitting edges (step 3), \n(c) separating edges and closing resulting polygons (steps 4 and 5).\nFIGURE 2.4.4\nFinding the intersection of a pair of polygons. Steps (a) to (c) show resulting\npolygons after each cut done during step 3 of the algorithm. Initial polygon B is filled gray,\ncurrent polygon C is outlined with thick line, and polygons in set outA have normal outline.\n",
      "content_length": 1434,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "1. Let A and B be a pair of given polygons.\n2. Let C be a copy of A that will be used for finding the intersection.\n3. For each edge of polygon B, cut polygon C using directed line associated\nwith this edge. Add the part of former C lying on the left side of the cutting\nline to set outA and replace polygon C with the part of this polygon lying\non the right side of the cutting line. If no part of C lies on the right side of\nthe line, stop, as polygons A and B don’t intersect.\n4. Repeat step 3 for each edge of B until all edges have been considered (unless\nthe algorithm was stopped).\nWhen this algorithm finishes, providing that initial polygons A and B intersected\n(indicating that the algorithm didn’t stop early), the polygon C after all operations\nwill be the intersection of initial polygons A and B and set outA will contain parts of\ninitial polygon A lying outside polygon B. Basic Boolean operations on initial A and\nB can be expressed as follows, which can be seen in Figure 2.4.5.\nA ∪B = outA ∪B\nA ∩B = C\nA\\B = outA\nIn these formulas, ∪is the union of a pair polygons, ∩is their intersection, and \\\nis their difference. When operating on sets of polygons, ∪is a union of a pair of sets.\nWhen polygons in set outA and polygon B don’t overlap, the union outA ∪B can be\ncomputed by simply adding polygon B to set outA.\n2.4\nUsing Projective Space to Improve Precision of Geometric Computations\n163\nFIGURE 2.4.5\nBoolean operations on polygons: (a) sum of polygons A and B as a sum of\noutA and B, (b) intersection of polygons A and B as resulting polygon C, (c) difference of\npolygons A and B as resulting set outA.\nBecause this algorithm is based entirely on integer operations in RP2 space, it can\nbe proven to work with all possible sets of valid input data, which is rarely the case\nwhen it comes to algorithms performing Boolean operations. However, this algorithm\nmight not be useable in all applications because it can generate T-intersections in a\nresulting polygon mesh. To address this problem, the algorithm could be extended to\nwork on mesh data with connectivity information—for example, using a half-edge\n",
      "content_length": 2128,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "data structure. But this extension is outside of the scope of this chapter and is not pre-\nsented here—this algorithm is demonstrated only as a proof-of-concept for integer\noperations using RP2 space.\nExtension into the Third Dimension\nThe previous discussion introduced the use of projective space for efficient and error-\nfree computational geometry operations in two-dimensional space. The extension to\nthe third dimension, critical for most of today’s games, is straightforward, but it\nrequires using four-dimensional vectors and operations on them. The CD-ROM con-\ntains a document providing a discussion similar to the one you’ve read here, for three-\ndimensional space and RP3 projective space.\nConclusion\nMany geometric algorithms suffer from rounding and loss of precision due to used\nnumerical representation, which may lead to wrong behavior of such algorithms in\nthe case of nearly collinear or coplanar points in input data. This is especially true for\nconstructive solid geometry (CSG) algorithms, because collinear and coplanar cases\npresent a serious problem in many implementations. The presented operations in\nprojective space allow implementation of most of these algorithms in such a way that\nthey can be proven to operate correctly for all cases of valid input data. This is done at\nthe cost of additional computational power required for operations on large integers,\nbut resulting robustness of the algorithms based on operations in projective space may\nbe worth its cost. On the other hand, as 64-bit processors and their new instruction\nsets extensions will become more and more common, the extra computational cost of\nsuch algorithms doesn’t have to be very large if long integer math is implemented in\nan efficient way.\nRobust CSG algorithms used in a digital content creation tool or in-engine level\neditor will give artists and “mod” developers more freedom and will save development\ntime when they would otherwise have to find workarounds where other CSG algo-\nrithms failed. Also, the robustness of a CSG algorithm can be critical when such an\nalgorithm is used in the game itself, allowing the player to interact with the environment\nin every imaginable way without concern that a CSG failure will result in a catastrophic\ngameplay bug.\nReferences\n[Hollash91] Hollasch, Steven R. “2.1 Vector Operations and Points in Four-\nDimensional Space,” in “Four-Space Visualization of 4D Objects,” Chapter 2,\navailable online at http://steve.hollasch.net/thesis/chapter2.html.\n[Young02] Young, Thomas. “Using Vector Fractions for Exact Geometry,” Game Pro-\ngramming Gems 3, pp. 160–169.\n164\nSection 2\nMath and Physics\n",
      "content_length": 2640,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "165\n2.5\nXenoCollide: Complex\nCollision Made Simple\nGary Snethen, Crystal Dynamics\ngary@snethen.com\nT\nhis gem introduces a new collision algorithm that works on a limitless variety of\nconvex shapes, including boxes, spheres, ellipsoids, capsules, cylinders, cones,\npyramids, frustums, prisms, pie slices, convex polytopes, and many other common\nshapes. The algorithm detects overlap and can also provide contact normals, points of\ncontact, and penetration depths for rigid body dynamics. A working implementation,\nalong with a simple rigid body simulator, is provided on the CD-ROM for immedi-\nate use and experimentation.\nThe algorithm is simple and geometric in nature, so it’s easy to visualize and\ndebug. The algorithm reduces to a series of point-plane clipping checks, so the math\ncan be understood by anyone familiar with dot products and cross products. \nIntroduction\nCreating a robust collision system can require a great deal of time and effort. The most\ncommon approach is to choose a handful of basic primitives and create O(N 2) separate\ncollision routines, one for each possible pair of primitives. Using this approach, the\namount of coding, testing, and debugging can quickly balloon out of control. Even a\nsmall set of four simple primitives, such as spheres, boxes, capsules, and triangles will\nrequire 10 separate collision routines. Each of these routines will have special cases and\nfailure modes that need to be tested and debugged. Each time an additional collision\nprimitive is added, multiple new collision routines need to be created, one for each pre-\nexisting primitive plus an additional routine to collide the new primitive with itself.\nThis gem introduces an efficient and compact collision algorithm that works on\nevery convex shape commonly found in real-time collision systems. New shapes can\nbe quickly and easily introduced without changing the algorithm’s implementation.\nAll that’s required to add a new shape is a simple mathematical description of the\nshape. If desired, collision shapes can be inexpensively modified in real-time for use\non animated objects.\n",
      "content_length": 2098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "The algorithm is named XenoCollide. It is an example of a broader class of algo-\nrithms based on a technique called Minkowski Portal Refinement (MPR).\nXenoCollide and the MPR technique presented in this gem are novel, but they\nshare important similarities to the GJK collision detection algorithm introduced by\n[GJK88]. The differences are outlined in the section entitled “Comparison of MPR\nand GJK.”\nThis gem proceeds by introducing support mappings and Minkowski differences.\nIf you are already comfortable with these concepts, you can skip ahead to the section\nentitled “Detecting Collision Using Minkowski Portal Refinement.”\nRepresenting Shapes with Support Mappings\nAlgorithms that work on a large number of shapes need a uniform way to represent\nthose shapes. XenoCollide relies on support mappings to fill this role. Support map-\npings provide a simple and elegant way to represent a limitless variety of convex shapes.\nA support mapping is a mathematical function (often a very simple one) that takes a\ndirection vector as input and returns the point on a convex shape that lies farthest in\nthat direction. (Support mappings are frequently defined as returning the point farthest\nin the direction opposite the normal. I’ve chosen the opposite convention to avoid an\nexcessive number of confusing negations in the equations.) If multiple points satisfy the\nrequirement, any one of the points can be chosen, so long as the same point is always\nreturned for any given input.\nSupport mappings are intuitive and easy to visualize. Imagine that you are given\nthe normal of a plane. If you slide this plane toward a convex shape along the plane’s\nnormal, the plane and the shape will eventually touch, as illustrated in Figure 2.5.1.\n166\nSection 2\nMath and Physics\nFIGURE 2.5.1\nVisualization of a support mapping as a moving plane.\n",
      "content_length": 1835,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "At the moment they touch, the shape is being supported by the plane and the\npoint on the shape that is touching the plane is the support point for that plane.\nIf an entire edge or face touches the plane, any one of the points on the edge or\nface can be chosen as the support point for that normal, as shown in Figure 2.5.2.\n2.5\nXenoCollide: Complex Collision Made Simple\n167\nFIGURE 2.5.2\nChoosing a support point\nwhen an entire edge supports the plane.\nBasic Shapes\nMany common shapes have simple support mappings. Consider a sphere of radius r,\ncentered at the origin. If you slide a plane from any direction, n, the first point you\nwill encounter on the sphere will be in the direction n from the origin at a distance r,\nas illustrated in Figure 2.5.3. \nFIGURE 2.5.3\nSupport mapping of a sphere.\nWritten as a function, the support mapping for the sphere is as follows:\nSsphere(n) = rn\n(2.5.1)\n",
      "content_length": 895,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "Table 2.5.1 lists the support mappings for several other common shapes.\nTable 2.5.1\nSupport Mappings for Simple Basic Shapes\nShape\nDescription\nSupport Mapping\nPoint\np\nSegment\nRectangle\nBox\nDisc\nSphere\nEllipse\nEllipsoid\nTranslating and Rotating Support Mappings\nThe support mappings in Table 2.5.1 represent shapes that are axis-aligned and cen-\ntered at the origin. To support shapes in world space, you need to rotate and translate\nsupport mappings.\nThe support mapping for a rotated and translated object can be found by first\ntransforming n into the object’s local space, and then finding the support point in local\nspace, and finally transforming the resulting support point back into world space:\n(2.5.2)\nFor the remainder of this gem, all support mappings are in world coordinates and\naccount for the rotation and translation of their respective shapes.\nS\nn\nRS\nR n\nT\nworld\nlocal\n( )\n(\n)\n=\n+\n−1\nr n\nr n\nr n\nr n\nr n\nr n\nx\nx\ny\ny\nz\nz\nx\nx\ny\ny\nz\nz\n2\n2\n2\n⎡⎣\n⎤⎦\n⎡⎣\n⎤⎦\nr n\nr n\nr n\nr n\nx\nx\ny\ny\nx\nx\ny\ny\n2\n2\n0\n0\n⎡⎣\n⎤⎦\n⎡⎣\n⎤⎦\nr n\nr\nn\nn\nn\nn\nx\ny\nx\ny\n0\n0\n⎡⎣\n⎤⎦\n⎡⎣\n⎤⎦\nr\nn\nr\nn\nr\nn\nx\nx\ny\ny\nz\nz\nsgn\nsgn\nsgn\n⎡⎣\n⎤⎦\nr\nn\nx\nx\nsgn\n0\n0\n⎡⎣\n⎤⎦\nr\nn\nx\nx\nsgn\n0\n0\n⎡⎣\n⎤⎦\n168\nSection 2\nMath and Physics\n",
      "content_length": 1173,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "Compound Support Mappings\nSupport mappings provide an efficient and compact way to represent basic shapes.\nThey can also be used to represent more complex shapes. This is done by mathemati-\ncally combining the support mappings of two or more simple primitives.\nYou can “shrink-wrap” a set of shapes by finding the support points for each\nshape and returning the point that is farthest along the direction vector. For example,\na disc centered at the origin can be shrink-wrapped with a translated point to create a\ncone, as given by Equation 2.5.3.\n(2.5.3)\nTo simplify the appearance of compound support mappings, drop the (n) from\nthe function names. Every support mapping requires a normal, so you can assume it’s\nalways present:\n(2.5.4)\nA second way to combine support mappings is to add them together. This results\nin one shape being “inflated by” or “swept about” the other. For example, if you add\nthe support mapping of a small sphere to the support mapping of a large box, you’ll\nget a larger box with rounded corners, as given by Equation 2.5.5:\n(2.5.5)\nSome useful combinations of support mappings are listed in Table 2.5.2.\nTable 2.5.2\nSupport Mappings for Compound Shapes\nShape\nDescription\nSupport Mapping\nCapsule\nmaxsupport(Ssphere,Ssphere + [length 0  0])\nor Sedge + Ssphere\nLozenge\nSrectangle + Ssphere\nRounded box\nSbox + Ssphere\nCylinder\nmaxsupport(Sdisc,Sdisc + [0  0 height])\nor Sedge + Sdisc\n→\nS\nS\nS\nsmoothbox =\n+\nbox\nsphere\nS\nmaxsupport S\nS\ncone =\n(\n)\npoint\ndisc\n,\nS\nn\nmaxsupport S\nn S\nn\ncone( )\n( ),\n( )\n=\n(\n)\npoint\ndisc\n2.5\nXenoCollide: Complex Collision Made Simple\n169\n",
      "content_length": 1592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "Table 2.5.2\n(Continued)\nShape\nDescription\nSupport Mapping\nCone\nmaxsupport(Sdisc,Spoint + [0  0 height])\nWheel\nSdisc + Ssphere\nFrustum\nmaxsupport(Srectangle1,Srectangle2 + [0  0 height])\nPolygon\nPolyhedron\nmaxsupport(Svert1,Svert2,...)\nExtremely complex shapes can be easily created by algebraically combining many\nbasic and compound shapes. These algebraic operations can be turned into intuitive\ncontrols for content creators as well. Artists and designers can use the basic shapes to\nsketch out the major external features of an object and then use interactive shrink-\nwrapping and sweeping/extruding to handle the rest.\nSimplifying Collision Detection Using Minkowski Differences\nEvery convex shape can be treated as a convex set of points in world space. Something\ninteresting happens if you subtract every point in one solid shape from every point in\na second solid shape. If the two shapes overlap, there will be at least one point within\nshape A that shares the same position in world space as a point within shape B. \nWhen one of these points is subtracted from the other, the result will be the zero\nvector (that is, the origin). Similarly, if the two shapes do not overlap, no point from\nthe first shape will be equal to any point in the second shape and the new shape will\nnot contain the origin.\nTherefore, if you can detect whether the origin is in the new shape, you have\ndetected whether or not the two original shapes are colliding.\nThe shape that’s formed by the subtraction of one convex shape from another is\ncalled the Minkowski difference of the two shapes. The Minkowski difference, B–A, is\nalso a convex shape. If B–A contains the origin, A and B must overlap. If B–A does\nnot contain the origin, A and B are disjoint.\nIt would be prohibitively expensive to actually subtract every point in one shape\nfrom every point in the other. However, you can easily determine the support map-\nping of the Minkowski difference B–A if you’re given the support mappings of A and\nB. Using Equation 2.5.6, you can reduce the problem of detecting collision between\n170\nSection 2\nMath and Physics\n",
      "content_length": 2103,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "two convex shapes, A and B to that of determining whether the origin lies within a\nsingle convex shape, B–A.\n(2.5.6)\nDetecting Collision Using Minkowski Portal Refinement\nUp to this point, everything described applies equally well to GJK and XenoCollide. For\nthe remainder of this gem, the discussion will focus on XenoCollide and the MPR tech-\nnique. If you’re interested in reading more about GJK, [van den Bergen03], [GJK88],\nand [Cameron97] are excellent references. If you want additional background and\ndetails on XenoCollide, please see [Snethen07].\nHere’s the pseudocode for XenoCollide:\n// Phase 1: Portal Discovery\nfind_origin_ray();\nfind_candidate_portal();\nwhile ( origin ray does not intersect candidate )\n{\nchoose_new_candidate();\n}\n// Phase 2: Portal Refinement\nwhile (true)\n{\nif (origin inside portal) return hit;\nfind_support_in_direction_of_portal();\nif (origin outside support plane) return miss;\nif (support plane close to portal) return miss;\nchoose_new_portal();\n}\nEach step is described in detail next.\nThe find_origin_ray(); Step\nStart by finding a point known to be in the interior of the Minkowski difference B–A.\nSuch a point can be easily obtained by taking a point known to be inside B and sub-\ntracting a point known to be inside A. The geometric center (or center of mass) is a\nconvenient point to use. However, any deep interior point will work. The point that\nresults from the subtraction is the interior point of B–A. The interior point is labeled\nV0 in Figure 2.5.4.\nThe interior point is important because it lies on the inside of B–A. If the ray\ndrawn from the interior point through the origin, called the origin ray, passes through\nthe surface of B–A before it encounters the origin, the origin lies outside of B–A.\nConversely, if the ray passes through the origin before the surface, the origin is inside\nof B–A.\nS\nn\nS\nn\nS\nn\nB-A ( ) =\n( )−\n−(\n)\nB\nA\n2.5\nXenoCollide: Complex Collision Made Simple\n171\n",
      "content_length": 1940,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "The find_candidate_portal(); Step\nThis step of the algorithm uses the support mapping of B–A to find three non-collinear\npoints on the surface of B–A that form a triangular portal through which the origin ray\nmay (or may not) pass. See Figure 2.5.5.\nThere are many ways to obtain three non-collinear surface points. XenoCollide\nuses the following support points:\n// Find support in the direction of the origin ray\nV1 = S( normalize(-V0) );\n// Find support perpendicular to plane containing\n// origin, interior point, and first support\nV2 = S( normalize(V1 x V0) );\n// Find support perpendicular to plane containing\n// interior point and first two supports\nV3 = S( normalize((V2-V0) x (V1-V0)) );\nThe while ( origin ray does not intersect candidate ) Step\nYou now test the candidate triangle to determine whether the origin ray intersects it.\nYou do this by testing whether the origin lies on the inside of each of the three planes\nformed by the triangle edges and the interior point—(v0,v1,v2); (v0, v2, v3); and\n(v0,v3,v1). If the origin lies within all three planes, you’ve found a valid portal and\ncan move on to the next step. If not, you need to choose a new portal candidate \nand try again.\n172\nSection 2\nMath and Physics\nFIGURE 2.5.4\nStep 1 involves finding the origin ray.\n",
      "content_length": 1281,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "The choose_new_candidate(); Step\nIf the origin lies outside one of the planes, use that plane’s outer-facing normal to find\na new support point, as illustrated in Figure 2.5.6.\n2.5\nXenoCollide: Complex Collision Made Simple\n173\nFIGURE 2.5.5\nStep 2 involves finding a candidate portal.\nFIGURE 2.5.6\nStep 3 involves finding a new candidate portal.\n",
      "content_length": 346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "This new support point is used to replace the triangle vertex that lies on the inside\nof the plane. The resulting support points provide you with a new portal candidate\n(see Figure 2.5.7); repeat the loop until you obtain a hit. \n174\nSection 2\nMath and Physics\nFIGURE 2.5.7\nStep 4 involves finding a valid portal.\nThe if (origin inside portal) return hit; Step\nThe points V0, V1, V2, and V3 form a tetrahedron. Due to the convexity of B–A,\nthis entire tetrahedron lies inside B–A. If the origin lies within the tetrahedron, it\nmust also lie within B–A. You know that the origin lies within three of these faces,\nbecause the origin ray starts at V0 and passes through the portal, which forms the\nopposite side of the tetrahedron. If the origin lies within the portal, it lies within the\ntetrahedron and therefore lies within B–A. In this case, you return with a hit.\nThe find_support_in_direction_of_portal(); Step\nIf you make it here, the origin lies on the far side of the portal. However, you don’t\nknow whether it lies within B–A nearby on the outside of the portal or whether it lies\ncompletely outside of B–A. You need more information about what lies on the far side\nof the portal, so you should use the exterior facing normal of the portal to obtain a\nnew support point that lies outside of the portal’s plane. See Figure 2.5.8.\nThe if (origin outside support plane) return miss; Step\nIf the origin lies outside of the new support plane formed by the support normal and\nthe new support point, the origin lies outside B–A and the algorithm reports a miss.\n",
      "content_length": 1562,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "The choose_new_portal(); Step\nThe origin lies between the portal and the support plane, so you need to refine your\nsearch by finding a new portal that is closer to the surface of B–A. Consider the tetra-\nhedron formed by the support point and the portal.\nThe origin passes into the tetrahedron through the portal and is therefore guaran-\nteed to exit the tetrahedron through one of the three outer faces formed by the sup-\nport point and the three edges of the portal. This step determines which of the three\nouter faces the ray passes through and replaces the old portal with this new portal. To\ndetermine which of the three outer faces the origin ray passes through, you test the\norigin against the three planes—(V4, V0, V1); (V4, V0, V2); and (V4, V0, V3).\nThe origin will lie on the same side of two of these planes. The face that borders\nthese two planes becomes the new portal, as illustrated in Figure 2.5.9.\nThe if (support plane close to portal) return miss; Step\nAs the algorithm iterates, the refined portals will rapidly approach the surface of B–A;\nhowever, if B–A has a curved surface, the origin may lie infinitesimally close to this\ncurved surface. In this case, the refined portals may require an arbitrary number of\niterations to pass the origin. To terminate under these conditions, you have to rely on\na tolerance. When the portal gets sufficiently close to the surface (as measured by the\ndistance between the portal and its parallel support plane), you terminate the algo-\nrithm. You can terminate with a hit or a miss, depending on whether you prefer to err\non the side of imprecise positive or negative results.\n2.5\nXenoCollide: Complex Collision Made Simple\n175\nFIGURE 2.5.8\nStep 5 involves finding a new support point\nin direction of portal.\n",
      "content_length": 1768,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "For physical simulation, it’s generally better to err on the side of a false negative\n(that is, returning a miss even though the point may be slightly below the surface).\nVery slight penetration won’t be noticed, but any visible gap at the contact point\nwould appear unnatural.\nTermination\nThe algorithm will continue running until one of the following conditions is met:\n• The origin lies on the inside of a portal (hit)\n• The origin lies on the outside of a support plane (miss)\n• The distance between the portal and its parallel support plane drops below a small\ntolerance (close call—can be treated as a hit or miss depending on the application)\nA formal proof of termination is beyond the scope of this gem. However, an infor-\nmal proof can be found in [Snethen07].\nUsing MPR for Contact Information\nIf you need only to detect overlap, MPR can be terminated as soon as the portal passes\nthe origin. However, if your application requires contact information, MPR can con-\ntinue executing to discover a contact point, contact normal, and penetration depth.\nMPR offers several possible ways of acquiring contact information. The tech-\nnique employed by XenoCollide is to simply continue projecting the origin ray out to\nthe surface of the Minkowski difference. This represents pushing the objects away\nfrom each other along the line connecting their interior points until they are just\ntouching and then using the normal of this first contact as the collision normal. This\nis efficient and simple, and it results in stable contact information.\n176\nSection 2\nMath and Physics\nFIGURE 2.5.9\nStep 6 involves establishing the new portal.\n",
      "content_length": 1635,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "A second choice is to find the relative velocity of a pair of overlapping points, one\nfrom each object, and then project a new ray from the origin to the surface of B–A\nalong the negative direction of the velocity vector. This results in a calculation of pen-\netration along the direction of motion, which may result in more realistic dynamic\ncollisions.\nAdditional Optimizations\nXenoCollide exhibits good performance. However, highly optimized routines that\ntarget specific pairs of shapes will inevitably be faster than any general-purpose rou-\ntine. Readers interested in pursuing ideal performance can begin with XenoCollide as\na foundation to handle all shapes and then add special-case routines for handling the\npairs of shapes that will benefit the most from optimization later in development.\nOne obvious candidate for optimization is a sphere-sphere check, which has minimal\ncost when implemented as a special case.\nAnother important optimization is caching the results of each collision test to\nbootstrap the same check in the next timestep. If a separating support plane is found\nthat proves that two objects do not collide, this same separating plane can be used as\nan early-out separation test in subsequent frames. Likewise, if a portal is found that\nlies outside the origin, this same portal can often be used to quickly verify that the\nobjects are still in contact.\nThe support mapping functions are called multiple times during each collision\ncheck. To maximize performance, transform, normalization, and function call over-\nhead should be kept to a minimum. In some cases, it may be better to perform the\nsupport mapping evaluation in world space. For example, it’s generally less expensive\nto check a segment using dot(n, d) in world space than it is to transform n to local\nspace, test only the x component, and then transform the result back again.\nComparison of MPR and GJK\nGJK was one of the inspirations for MPR. GJK also supports a limitless variety of\nconvex shapes, but it suffers from several limitations that MPR attempts to correct:\n• The simplex refinement algorithm in GJK is based on an algebraic formulation\nthat isn’t intuitive to most game programmers. This formulation relies on deter-\nminants and cofactors, which are notoriously sensitive to floating point precision\nproblems. The combination of precision issues and hard-to-visualize mathematics\nmakes it difficult for many game developers to implement GJK robustly. \n• As a result of the previous issue, many variations on GJK have been created that\nattempt to frame GJK as a geometric problem rather than an algebraic one. How-\never, every approach to GJK requires considering the Voronoi regions of 8 to 15\nfeatures (points, edges, faces, and interior) of a tetrahedron, to see which is clos-\nest to the origin. This can be a complex and potentially expensive task due to the\nmany different conditions and branching operations.\n2.5\nXenoCollide: Complex Collision Made Simple\n177\n",
      "content_length": 2972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "• In the general case, GJK doesn’t provide an accurate contact normal, penetration\ndepth, or point of surface contact. A separate algorithm, such as EPA [van den\nBergen03], is required to obtain this information.\nMPR addresses each of these issues:\n• MPR is simple and geometric. Each step of the technique can be easily visualized\nand verified on-screen, which makes it easier to understand, test, and debug.\n• MPR requires fewer branching tests. Instead of choosing among 8 to 15 separate\nfeatures, only 2 or 3 need to be evaluated.\n• MPR can be used both to detect collision and to identify collision details that are\nwell-behaved and consistent from frame to frame.\nConclusion\nThis gem introduced a simple algorithm for detecting collision among shapes chosen\nfrom a limitless pool of useful convex designs. Introducing new shapes is extremely\neasy and can be wrapped in a graphical user interface for artists and designers. The\nalgorithm provides a robust foundation for a general purpose collision system for\ngameplay and rigid body dynamics. The algorithm is efficient; however, if additional\nperformance is ever required, optimized pair-specific routines can be layered on the\ngeneric framework as needed.\nAcknowledgements\nThe author wishes to thank the managers, programmers, and developers of Crystal\nDynamics for their support and review of this gem. The author would also like to\nthank Erin Catto, for his friendship and encouragement to share this work.\nReferences\n[Cameron97] Cameron, S. “Enhancing GJK: Computing Minimum and Penetration\nDistances Between Convex Polyhedra,” Proceedings of IEEE International Confer-\nence on Robotics and Automation (1997), pp. 3112–3117.\n[GJK88] Gilbert, E.G., Johnson, D.W., and Keerthi, S.S. “A Fast Procedure for Com-\nputing the Distance Between Complex Objects in Three-Dimensional Space,”\nIEEE Journal of Robotics and Automation, Vol. 4, No. 2 (1988), pp. 193–203.\n[Snethen07] Snethen, Gary. “XenoCollide Website,” available online at http://\nwww.xenocollide.com, September, 2007.\n[van den Bergen03] van den Bergen, Gino. Collision Detection in Interactive 3D Envi-\nronments, Morgan Kauffman, 2003.\n178\nSection 2\nMath and Physics\n",
      "content_length": 2183,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "179\n2.6\nEfficient Collision Detection\nUsing Transformation\nSemantics\nJosé Gilvan Rodrigues Maia, UFC\ngilvanmaia@gmail.com\nCreto Augusto Vidal, UFC\ncvidal@lia.ufc.br\nJoaquim Bento Cavalcante-Neto, UFC\njoaquimb@lia.ufc.br\nH\now does a first-person shooter game determine whether a shot hit the head of an\nenemy? How can you avoid game objects passing through walls or falling to infin-\nity? How do you check whether the player touched an item or reached a checkpoint?\nHow does an engine determine intersections for a dynamics simulation? Although\nthere are many possible ways for solving these problems, collision detection (CD) is a\nvery important tool for dealing with such situations. CD allows for checking whether\na given set of geometric entities are overlapping. Because of this, it plays an essential\nrole in almost any computer game.\nCurrent rendering technology provides support for placing geometry in a scene\nby means of transformation matrices. Therefore, it is important for a game program-\nmer to know how to construct and manipulate these matrices. Moreover, collision\ndetection methods must consider not only the shape of objects but also their corre-\nsponding matrices in order to carry out intersection tests.\nThis gem shows you a method for inverting transformation matrices used for\nplacing models in games and for extracting useful semantic information from them. It\nalso explains how this information can be used for implementing efficient collision\ndetection tasks.\n",
      "content_length": 1487,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "Affine Transforms and Games\nThis section briefly discusses a few concepts of linear algebra. Affine transforms are\nused to map between two vector spaces by means of a linear transformation (that is, a\nmatrix multiplication) followed by an origin shift vector that is added to the result. An\naffine transform (also called affine mapping) is defined as follows:\n(2.6.1)\nAffine mappings are used in many computer graphics applications. Vertex trans-\nformation from world space to camera space in a transformation pipeline, for exam-\nple, is implemented using a change of basis—a particular type of affine mapping.\nBecause computer graphics systems implement linear transformations using 4 \u0002 4\nmatrices, we define the mapping in Equation 2.6.1 using block matrix form:\n(2.6.2)\nObserve that in Equation 2.6.2 the affine mapping, A, is a 3 \u0002 3 matrix and the\norigin shift, t, is a 3 \u0002 1 (column) matrix. Except for the last item, all components in \nthe fourth row are defined as zero. For computer games, it can be assumed that input\nvertices to be transformed have their homogeneous w component set to 1 before the\nmatrix multiplication is carried out. Three-dimensional geometry is specified using ordi-\nnary Cartesian coordinates, and vertices are then processed in homogeneous coordinates. \nFrom the game programming perspective, the problem is to detect collisions\nbetween the models in a scene. Recall that each model has a corresponding matrix\nthat places it in world coordinates; hence, this matrix must be considered for collision\ndetection against a given model.\nObserve that typical matrices placing models in world space are affine mappings\nmatching Equation 2.6.2, because these matrices usually arise from a combination of\nbasic transformations, each in itself an affine mapping—scaling, rotation, and trans-\nlation. These basic transformations can be used to “explain” matrices and answer some\ncommon questions about the placement of a model. What is its size? Which direction\nis the model facing? Where is the model’s origin? Because of this, scaling, rotation,\nand translation are called transformation semantics.\nSemantics can be used to simplify and speed up computations involved in the\ncollision detection process. The next section presents an efficient decomposition\nmethod for both inverting and obtaining semantics from a transformation matrix.\nThis answers the following questions—given a 4 \u0002 4 array representing an affine\nmapping, what is its inverse, and what are its corresponding scaling, rotation, and\ntranslation parts?\nM\nA\nt\n0\n=\n⎡\n⎣⎢\n⎤\n⎦⎥\n1\nq\nAp\nt\n=\n+\n180\nSection 2\nMath and Physics\n",
      "content_length": 2610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "Extracting Semantics from Matrices\nSome semantics information can be obtained in the general case, as shown briefly\nnext. However, in game development, you are not usually interested in a method that\ndecomposes an arbitrary affine mapping. Instead, most of this gem focuses on matri-\nces such as those described previously, used to place models in a typical game. This\nenables you to apply a more efficient method.\nThe General Affine Mapping Case\nCan you extract some semantics from any affine mapping? Yes, you can. As shown in\nEquation 2.6.2, t represents an origin shift. Hence, the translation part is available for\nfree—you just get it from the last column of the given matrix. Some orientation infor-\nmation can also be extracted easily. Take a closer look at matrix multiplication over a\ngiven vertex:\n(2.6.3)\nGrouping Equation 2.6.3 by each axis-aligned component from p, you obtain:\n(2.6.4)\nUsing Equation 2.6.4, it is clear that the first, second, and third columns from A\nrepresent the new x, y, and z axes, respectively, after a point gets transformed. This\nmeans you can use columns from your affine transform for solving problems consider-\ning orientation and scale—a convenient example is detailed later. Although Equation\n2.6.4 provides a clear view about how the orientation of vertices is modified by matri-\nces, only translation and orientation semantics are obtained through this analysis. \nThe inverse of an affine matrix is well-known, and can be written as:\n(2.6.5)\nReaders interested in inverting a general affine mapping Equation 2.6.2 are\ninvited to take a look at Kevin Wu’s article about this matter [Wu91].\nM\nA\nt\n0\nA\nA t\n0\n−\n−\n−\n−\n=\n⎡\n⎣⎢\n⎤\n⎦⎥\n=\n−\n⎡\n⎣\n⎢\n⎤\n⎦\n⎥\n1\n1\n1\n1\n1\n1\nAp =\n⎡\n⎣\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n+\n⎡\n⎣\n⎢\n⎢\n⎢\n⎤\n⎦\np\nA\nA\nA\np\nA\nA\nA\nx\ny\n11\n21\n31\n12\n22\n32\n⎥\n⎥\n⎥\n+\n⎡\n⎣\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\np\nA\nA\nA\nz\n13\n23\n33\nAp =\n⎡\n⎣\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\nA\nA\nA\nA\nA\nA\nA\nA\nA\np\np\nx\n11\n12\n13\n21\n22\n23\n31\n32\n33\ny\nz\nx\ny\nz\nx\ny\np\np A\np A\np A\np A\np A\n⎡\n⎣\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n=\n+\n+\n+\n11\n12\n13\n21\n22 +\n+\n+\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\np A\np A\np A\np A\nz\nx\ny\nz\n23\n31\n32\n33\n2.6\nEfficient Collision Detection Using Transformation Semantics\n181\n",
      "content_length": 2116,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "Our Specific Case: Model Matrices\nConsider a transformation matrix M that transforms a model from its local space into\nworld coordinates. You can assume M results from a composition of a non-uniform\nscaling matrix, followed by as many rotations and translations as needed, in this order.\nThis assumption is reasonable because such form is compatible with most scene rep-\nresentations, such as scene graphs. For convenience, M can be written as the product\nof just three matrices—S (scaling), R (rotation), and T (translation):\n(2.6.6)\nHere, each Mi represents rotations or translations.\nKevin Wu [Wu94] used an equivalent matrix form in his gem, and described a\nsimple and efficient method for inverting either matrices that preserve angles between\nvectors or matrices that preserve vector lengths. However, his method only considers\nuniform scaling. Our matrix representation is more general (considers non-uniform\nscales) and can be rewritten using a block matrix format:\n(2.6.7)\nSome details about this representation must be observed: t represents the transla-\ntion part; i, j, and k are column matrices that form an orthonormal basis; and sx, sy,\nand sz are non-zero (otherwise M is singular) scaling factors. As M results from a prod-\nuct of basic transforms, its inverse is known and can be written as:\n(2.6.8)\nNow, let’s see how a matrix in this form can be inverted without computing any\nsquare roots. Because i is a unit vector, the squared scale factor in the x axis can be\ncomputed by taking the dot product of the first column with itself:\n(2.6.9)\ns\ns\ns\ns\ns\nx\nx\nx\nx\nx\ni\ni\ni\ni\n(\n)⋅(\n) =\n⋅\n(\n) =\n( ) =\n2\n2\n2\n1\nM\nTRS\nS R T\ni\ni t\nj\nj t\nk\nT\nT\n−\n−\n−\n−\n−\n=(\n)\n=\n=\n−⋅\n−⋅\n1\n1\n1\n1\n1\ns\ns\ns\ns\nx\nx\ny\ny\nT\nk t\n0\n1\ns\ns\nz\nz\n−\n⋅\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\n⎥\nM\nTRS\nI\nt\n0\ni\nj\nk\n0\n=\n=\n⎡\n⎣⎢\n⎤\n⎦⎥\n⎡\n⎣⎢\n⎤\n⎦⎥\n1 0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\ns\ns\ns\nx\ny\nz\nx\ny\nz\ns\ns\ns\n0\n0\n0\n0\n1\n0\n0\n0\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\n⎥\n=\n⎡\n⎣\n⎢\n⎢\n⎤\n⎦\n⎥\ni\nj\nk\nt\n1⎥\nM\nM\nM S\nTRS\n=\n=\n1K\nk\n182\nSection 2\nMath and Physics\n",
      "content_length": 1969,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "This also holds for the second and third columns\n(2.6.10)\n(2.6.11)\nThis property becomes quite useful, because the squared scale factor can be used\nto compute the first row of the 3 \u0002 3 part of M-1 (Equation 2.6.8) by simple division:\n(2.6.12)\nKnowing that, you can compute the inverse as follows: compute the squared scale\nfactors using Equations 2.6.9, 2.6.10, and 2.6.11; transpose the 3 \u0002 3 submatrix A of\nEquation 2.6.2; and divide its resulting rows by the corresponding squared scale factors.\nObserve that, at this point, you have computed A-1. Finally, the product –A-1t corre-\nsponding to the last column is easily computed.\nThis inversion method offers the flexibility of supporting non-uniform scale, and\nit is quite efficient. Only 27 multiplications, three divisions, and 12 additions are\nrequired. Table 2.6.1 provides a comparison of this method with the brute-force\nmethods (Cofactors and Gaussian Elimination with pivoting), with Wu’s method for\naffine mappings [Wu91], and with Wu’s method for angle-preserving matrices\n[Wu94], which only supports uniform scaling. In Table 2.6.1, only a “raw” C imple-\nmentation is considered. Readers are encouraged to implement these methods using\nSIMD/MIMD instructions—SSE, SSE2, and so on.\nTable 2.6.1\nComparison of Matrix Inversion Methods in Terms of Required Operations\nInversion Method\nDivisions\nMultiplications\nAdditions\n[Wu94] (angle-preserving)\n1\n21\n8\nOur method\n3\n27\n12\n[Wu91] (general case)\n1\n48\n22\nCofactors\n1\n280\n101\nGaussian elimination with partial pivoting\n10\n51\n47\nTransformation semantics extraction requires a slight modification of the method\npresented here. As shown in Equation 2.6.2, the translation part comes for free.\nSquare root computation over Equations 2.6.9, 2.6.10, and 2.6.11 gives you the scale\nfactors. The rotation part can be obtained dividing the first three columns by the cor-\nresponding scale factors. Therefore, semantics extraction demands three square roots,\nthree divisions, 18 multiplications, and six additions.\ns\ns\ns\nx\nx\nT\nT\nx\ni\ni\n2\n⎛\n⎝\n⎜⎜\n⎞\n⎠\n⎟⎟=\ns\ns\ns\ns\ns\nz\nz\nz\nz\nz\nk\nk\nk\nk\n(\n)⋅(\n) =\n⋅\n(\n) =\n( ) =\n2\n2\n2\n1\ns\ns\ns\ns\ns\ny\ny\ny\ny\ny\nj\nj\nj\nj\n(\n)⋅(\n) =\n⋅\n(\n) =\n( ) =\n2\n2\n2\n1\n2.6\nEfficient Collision Detection Using Transformation Semantics\n183\n",
      "content_length": 2240,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "When only uniform scaling is considered, it is evident that the inversion method\nis reduced to Wu’s method for angle-preserving matrices [Wu94]. Moreover, seman-\ntics extraction in this case demands one square root, one division, 12 multiplications,\nand two additions.\nNow you can invert and extract semantics efficiently from a typical matrix that\nplaces models in a scene. The next section explains how semantics information can be\nused when performing collision detection tasks.\nUsing Semantics for Collision Detection Tasks\nCollision detection methods for real-world applications must consider a given set of\nmodels, and this work is usually split into two successive tasks. The first task, known as\nbroad phase or collision culling, is responsible for finding all object pairs that are in prox-\nimity—the potentially colliding set. More importantly, this phase aims at discarding\ndistant pairs that cannot collide, thus avoiding expensive intersection tests. The next\ntask, known as narrow phase or pair processing, consists of effective intersection tests\nover object pairs collected during the broad phase. Transformation semantics can be\nused in these collision detection tasks. \nSpeeding Up Broad Phase\nGiven N objects, potentially O(N2) pairs have to be checked for collision. Neverthe-\nless, it happens that objects in most pairs are not even close to each other, so, many\npairs can be discarded quickly—this explains why this phase is also known as collision\nculling. Spatial hashing and sweep-and-prune methods, among others, were specially\ndesigned for this purpose. Common implementations of these techniques require\nknowledge of the axis-aligned bounding box (AABB) tightly fitting each model in\nworld coordinates.\nScene representations usually maintain a local AABB per geometric model; for\nexample, an AABB in the model’s own local space. This box also gets transformed by\nthe model’s matrix, so that it becomes an oriented bounding box (OBB) in world\nspace. The problem at this point is the following: how can you efficiently compute the\nmodel’s global AABB from its local AABB and a transformation matrix M? A brute-\nforce approach transforms all eight corner vertices from the AABB first and then com-\nputes the AABB of the transformed vertices—this requires 21 branches. Avoiding not\nonly computations but also branches is very important for improved performance.\nAlthough modern CPUs can predict the behavior of code before it is executed,\nbranchless code may still run a bit faster.\nCharles Bloom proposed an elegant, efficient solution for this problem in his\ngame engine [Bloom06]. His approach is purely geometric and is based on the orien-\ntation semantics shown in Equation 2.6.4. Consider a min-max representation for\nAABBs. First, the minimum extreme vertex is transformed using the model’s matrix,\nas usual, obtaining a point p. Orientation semantics are used in order to obtain the\n184\nSection 2\nMath and Physics\n",
      "content_length": 2946,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "new axes in world coordinates after the model gets transformed by M. These axes are\nthen multiplied by the respective box dimensions, obtaining the transformed box’s\nedge vectors. Each sign of each component of these edges is then checked.\nObserve that negative components “move” p towards the global AABB’s mini-\nmum extreme vertex, so this extreme point is obtained by adding all negative edge\ncomponents to p. Conversely, all positive edge components are added to p in order to\nobtain the maximum extreme vertex—this requires only nine branches. See Figure\n2.6.1.\n2.6\nEfficient Collision Detection Using Transformation Semantics\n185\nFIGURE 2.6.1\nA local AABB fitting a given model (a) is transformed into world coordinates\nthrough a matrix (b). Observe that p lies on the boundary of the global AABB that tightly\nfits the transformed AABB. The global box is obtained by adding the components of the\nedge vectors (c) to the minima point, moving it toward the global extreme vertices.\nIn a gem from the previous edition, Chris Lomont explained how to perform many\ntricks efficiently, using floating points [Lomont07]. His approaches can be used for sign\nbit extraction in order to come up with a branchless implementation of this method,\nwhich is provided on the CD-ROM. \nNarrow Phase\nThe final step for collision detection is to process all pairs reported from broad phase.\nThis task must consider two models A and B, as well as their respective transforma-\ntion matrices MA and MB. The following question must be answered: do the models\nintersect after they are transformed? A list of intersecting primitive pairs (that is, trian-\ngles) describing the contact surface is reported in case the models intersect.\nBounding volume hierarchies are well-suited for this problem because they pro-\nvide a multi-resolution representation of the models that is useful for discarding non-\nintersecting parts. AABB-trees are particularly useful for dealing with deformable\nmodels (characters, for example) because they are much cheaper to refit as geometry\ngets deformed [Bergen97].\n",
      "content_length": 2073,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "Let’s define two useful matrices:\n(2.6.13)\n(2.6.14)\nMAB maps geometry from A’s local space into B’s local space. Conversely, MBA\nmaps geometry from B’s local space into A’s local space. These matrices can be obtained\nusing the presented inversion method followed by a matrix multiplication. Triangles\ncan be mapped from the local space of one object into that of another object, provid-\ning support for direct intersection tests.\nActually, transformation semantics provides flexibility for processing geometry\ncoming from a given space (A, B, or world spaces) in a common space where compu-\ntations can be carried out more comfortably. Using this approach, you can choose from\nseven coordinate systems for performing computations, as illustrated in Figure 2.6.2.\nM\nM M\nBA\nA\nB\n=\n−1\nM\nM M\nAB\nB\nA\n=\n−1\n186\nSection 2\nMath and Physics\nFIGURE 2.6.2\nSchematic view illustrating how transformation semantics can be used for\nprocessing geometry coming from different local spaces. \nIn Figure 2.6.2, arrows show how geometry coming from a given space (A, B, or\nworld) is transformed into a common space, which is more adequate for processing.\nFollowing this scheme, it can be seen that a mapping from B to A’ space corresponds\nto RA\n-1TA\n-1TBRBSB (transformations are ordered from right to left).\nThe box-box overlap test is also necessary in order to perform collision detection\nbetween a pair of AABB-trees. It is important to notice that occurrence of a non-\nintersecting box pair avoids many triangle-triangle tests because entire subtrees cannot\ncollide given that their bounding boxes are not overlapping. Moreover, the box-box\n",
      "content_length": 1624,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "overlap test also allows for computing collision detection between a model and an ori-\nented box placed in world coordinates. In a game, this can be used to determine whether\nthe player has touched an important item in order to trigger some AI, for example.\nThe box-box test is usually implemented using the Separating Axis Theorem\n[Gottschalk96], also known as SAT. This method allows for very early exits based on\niterative search for a separating axis, so that box projection intervals over that axis are\nnot overlapping. The given boxes are intersecting only when there is no separation\nalong any of the 15 potentially separating axes. \nBergen pointed out that about 60% of the separating axes found in SAT corre-\nspond to the normal of a box’s face [Bergen97]. Based on this, he adopted an approxi-\nmate intersection test between boxes (SAT-lite) that checks only this kind of separating\naxes. Although all separation cases cannot be handled, this test provides faster collision\nculling because only 6 of 15 axes are tested.\nUsing transformation semantics, the box-box test using SAT can be performed as\nfollows. First, scale each box using the scale semantics extracted from its respective\nmodel’s transformation matrix; after this, the intersection method can be carried out\nnormally considering only the rotation and translation semantics during the search\nfor a separating axis using either SAT or SAT-lite. Observe that this scale adjust causes\nthe test to be performed as boxes are coming from A’ and B’ spaces (see Figure 2.6.2).\nOf course, any box-box intersection method can benefit from this slight modification\nin order to provide support for scaling models. Moreover, only 12 additional multipli-\ncations are necessary, which is not excessive given that scaling is supported efficiently.\nIn this example, only AABB-trees are considered in collision tests. Actually, other\nuseful intersection tests can be performed against a model using volumes, rays, or\nlines placed in world space. Examples of volumes are sphere, cone, AABB, OBB, and\ncapsule (also called a line-swept sphere). \nBasically, support for testing a given primitive against an AABB-tree requires two\nintersection methods. The first method checks whether the primitive intersects a box\ncoming from the transformed AABB-tree, providing means for fast collision culling of\nsubtrees. Finally, the second method checks overlap between the primitive and a trian-\ngle from the model. This second method can be implemented by transforming trian-\ngles using the model’s matrix before carrying out the intersection test.\nAs shown in Figure 2.6.2, transformation semantics extraction allows for choosing\none of seven coordinate systems in order to carry out tests. This choice affects both the\ncomplexity and efficiency of intersection tests. For example, choosing to transform a\nsphere from world coordinates into the local space of a model (A, for example) may\ncause the deformation of a sphere into an ellipsoid due to non-uniform scaling, giving\nrise to a more complicated intersection tests. On the other hand, the sphere can be\ntransformed into the A’ space, allowing for simpler intersection tests: just scale triangles\nand boxes before carrying out the computations.\n2.6\nEfficient Collision Detection Using Transformation Semantics\n187\n",
      "content_length": 3314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "You might deduce from this that avoiding the local model space during intersection\ntests provides simpler and faster intersection tests. Actually, this does not hold when\nconsidering a model against a ray. Transforming a ray into the model’s local space can\ndistort its direction and length. As result, unit vectors representing the ray direction in\nworld space may not be unit after they are transformed into the model’s local space. \nHowever, observe that this process is based on a linear transformation. Because of\nthis, any parametric point reported in this local space is still mapped at the same posi-\ntion in world space by using the same parameter. Hence, a faster computation is\nobtained by using an intersection test that does not rely on unit direction vectors and\npre-computing the ray in A space. Our tests on a Pentium D processor using different\nmodels in a ray-casting algorithm pointed out that performing intersection computa-\ntions in the model’s local space (A) is about 57% faster when compared to an imple-\nmentation based on the A’ space.\nConclusion\nThis gem covered how to use semantics information in order to speed up and simplify\ncomputations arising from collision detection tasks. You have seen how to efficiently\ninvert and extract semantics information from matrices placing models in a typical\ngame. Moreover, now you can efficiently perform collision detection tasks using trans-\nformation semantics. Although only the 3D case was shown, the concepts presented in\nthis gem are straightforward and could be used in other dimensions and in other sorts\nof problems.\nIdeas presented here were used to modify OPCODE [Terdiman03], an existing\noptimized collision detection library written by Pierre Terdiman, in order to add sup-\nport for scaling models. The modified version also provides support for non-indexed\ngeometry, as well for triangle fans, strips, and point grids representing terrain. This\nversion is being used in a number of open source projects, and can be obtained at the\nfollowing link: http://www.vdl.ufc.br/gilvan/coll/opcode/.\nReferences\n[Bergen97] Bergen, G. Van Den. “Efficient Collision Detection of Complex\nDeformable Models Using AABB Trees,” Journal of Graphics Tools, Vol. 2, No. 4,\npp. 1–13, 1997.\n[Bloom06] Bloom, Charles. “Galaxy3,” available online at http://www.cbloom.com/\n3d/galaxy3/index.html, January 22, 2006.\n[Cohen95] Cohen, J.D., Lin, M.C., Manocha, D., and Ponamgi, M.K. “I-COL-\nLIDE: An Interactive and Exact Collision Detection System for Large-Scale\nEnvironments,” Symposium on Interactive 3D Graphics, pp. 189–196, 1995.\n188\nSection 2\nMath and Physics\n",
      "content_length": 2625,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "[Gottschalk96] Gottschalk, S. “Separating Axis Theorem,” Technical Report\nTR96–024, Dept. of Computer Science, UNC Chapel Hill, 1996.\n[Lomont07] Lomont, Chris. “Floating-Point Tricks,” Game Programming Gems 6,\nCharles River Media, 2007.\n[Terdiman03] Terdiman, Pierre. “OPCODE,” available online at http://www.coder-\ncorner.com/Opcode.htm, December 13, 2003.\n[Wu91] Wu, Kevin. “Fast Matrix Inversion,” Graphics Gems II, Academic Press, Inc.,\n1991.\n[Wu94] Wu, Kevin. “Fast Inversion of Length- and Angle-Preserving Matrices,”\nGraphics Gems IV, Academic Press, Inc., 1994.\n2.6\nEfficient Collision Detection Using Transformation Semantics\n189\n",
      "content_length": 639,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "191\n2.7\nTrigonometric Splines\nTony Barrera, Barrera Kristiansen AB\ntony.barrera@spray.se\nAnders Hast, Creative Media Lab, \nUniversity of Gävle\naht@hig.se\nEwert Bengtsson, Centre For Image Analysis,\nUppsala University\newert@cb.uu.se\nT\nhe user interfaces, level, and actor designs for modern games, both 2D and 3D,\noften contain geometry that an artist intended to be a perfect circle or elliptical\narc. The mathematics of gameplay in some cases requires that a game’s runtime engine\nbe capable of generating the same types of shapes as efficiently as possible and with\nminimal error. There are numerous mathematical techniques that can be used to gen-\nerate arcs, and each has its benefits and weaknesses. The robustness of digital content-\ncreation tools, level editors, and game runtime code can be maximized if developers\nuse a unified approach for generating geometric shapes, rather than having special\ncase math for different basic types of shapes.\nThis gem shows you how splines can be constructed that can create both straight\nlines and perfect circle arcs. The latter is not possible with ordinary cubic splines and\nthe trigonometric spline will make it possible to create new forms for 3D models.\nFurthermore, this gem will show you that the trigonometric functions involved can\nbe computed without the use of the sine and cosine functions in the inner loop,\nwhich enables higher performance.\n",
      "content_length": 1402,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "Background\nCubic splines cannot be used to create perfect circle arcs but trigonometric splines can\nbe used for this purpose. Trigonometric splines were introduced by Schoenberg\n[Schoenberg64] and are sometimes called trigonometric polynomials. They have been\ninvestigated extensively in the literature of math and computer-aided geometry and\nsome examples are found in [Lyche79] and [Han03]. However, they have not gained\nmuch interest in the computer graphics community, perhaps because they involve the\ncomputation of trigonometric functions which are relatively computationally expensive.\nAs hardware becomes faster they may gain more interest in the field of computer\ngraphics as a modelling tool, because it is possible to construct everything from straight\nlines to perfect circle arcs. A later section shows how you can evaluate a trigonometric\npolynomial without using sine and cosine in the loop and this enables fast evaluation,\neven if no specialized graphics hardware is available.\nTrigonometric Splines\nA trigonometric spline [Alba04] can be constructed from a truncated Fourier series\n[Schoenberg64]. The Hermite spline is defined by two points and the tangents at\nthese points, which are depicted in Figure 2.7.1.\n(2.7.1)\nTherefore, you need four terms in the Fourier series. The trigonometric curve is\ndefined over the interval [0,\b/2] as:\n(2.7.2)\nThe coefficients for the curve can be found by using the constraints in Equation\n2.7.1, producing the following system which must be solved:\n(2.7.3)\n1\n1\n0\n1\n1\n0\n1\n1\n0\n0\n1\n0\n0\n1 0\n0\n−\n−\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\na\nb\nc\nd\n⎥\n⎥\n⎥\n=\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\np\np\nT\nT\n0\n1\n0\n1\nP\na\nb\nc\nd\nθ\nθ\nθ\nθ\n( ) =\n+\n+\n+\ncos\nsin\ncos2\nP\nP\nP\nP\nP\nT\nP\nT\n0\n1\n0\n0\n1\n0\n1\n1\n( ) =\n( ) =\n′( ) =\n′( ) =\n192\nSection 2\nMath and Physics\n",
      "content_length": 1780,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "The solution for this equation is:\n(2.7.4)\nIt should also be noted that the trigonometric Hermite spline can also be written\nin the following form: \n(2.7.5)\nHere, the coefficients A, B, C, and D are different from a, b, c, and d. You can prove\nthis by starting with Equation 2.7.2 and first expanding cos2θ to obtain the following:\n(2.7.6)\nThen you put Equation 2.7.4 into Equation 2.7.6 to obtain:\n(2.7.7)\nP\n1\n2\nP\nP\nT\nT\n1\n2\nP\nP\nT\nT\nT\n0\n1\n0\n1\n0\n1\n0\n1\n1\nθ\nθ\n( ) =\n+\n−\n+\n(\n)−\n−\n+\n+\n(\n)\n−\ncos +\n+\n−\n+\n+\n(\n)\nT\n1\n2\nP\nP\nT\nT\n0\n0\n1\n0\n1\nsin\ncos\nθ\nθ\n2\n2\nP\na\nb\nc\nd\na d\nb\nc\nθ\nθ\nθ\nθ\nθ\n( ) =\n+\n+\n+\n−=\n−\n+\n+\ncos\nsin\n( cos\n)\ncos\ns\n2\n1\n2\nin\ncos\nθ\nθ\n+ 2\n2\nd\nP\nA\nB\nC\nD\nθ\nθ\nθ\nθ\nθ\n( ) =\n+\n+\n+\ncos\ncos\nsin\nsin\n2\n2\na\n1\n2\nP\nP\nT\nT\nb\nT\nc\nT\nd\n1\n2\nP\nP\nT\nT\n0\n1\n0\n1\n1\n0\n0\n1\n0\n1\n=\n+\n−\n+\n(\n)\n= −\n=\n=\n−\n+\n+\n(\n)\n2.7\nTrigonometric Splines\n193\nFIGURE 2.7.1\nA trigonometric curve and its constraints.\n",
      "content_length": 864,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "Simplify and you get:\n(2.7.8)\nYou can split the last term in two parts and rewrite one of them using the trigono-\nmetric identity:\n(2.7.9)\nFinally, simplifying you get:\n(2.7.10)\nThis shows that you can rewrite the curve in different forms. You can use Equa-\ntion 2.7.2 or Equation 2.7.10. But you can also use Equation 2.7.8. This flexibility\nwill be useful when you evaluate the function, as shown in the next section.\nFast Evaluation of the Trigonometric Functions\n[Barrera04] illustrated a technique for efficiently interpolating between vectors or\nquaternions. The main idea is to use spherical linear interpolation (SLERP), which was\nintroduced to the computer graphics society by Shoemake [Shoemake85]. SLERP is\ndifferent from linear interpolation in the way that the angle between each vector or\nquaternion will be constant; that is, the movement will have a constant speed. SLERP\ncan be set up between two orthogonal unit vectors A and B as:\n(2.7.11)\nIn this case, both the cosine and sine functions need to be evaluated per step in\nthe interpolation. In [Barrera04] it is shown how this can be done for k steps using\nC++ code in the following way:\n#include <math.h>\n#include <stdio.h>\n#define M_PI       3.14159265358979323846\nvoid main() {\nint k=10; // nr of steps\ndouble A[2]={1,0}; \ndouble B[2]={0,1};\ndouble tm1[2];\ndouble t0[2];\ndouble tp1[2];\ndouble t=M_PI/2.0;\ndouble kt=t/k; // step angle\nP\nA\nB\nθ\nθ\nθ\n( ) =\n+\ncos\nsin\nP\nT\nT\nP\nT\nP\nT\n1\n0\n0\n1\n1\n0\nθ\nθ\nθ\nθ\n( ) = −\n+\n+\n+\n+\n−\n(\n)\ncos\nsin\n(\n)cos\nsin\n2\n2θ\nP\nP\nT\nT\nT\nP\nT\nT\nP\n1\n0\n1\n0\n0\n1\n0\n1\nθ\nθ\nθ\nθ\n( ) =\n−\n−\n+\n+\n+\n+\n−\ncos\nsin\n(\n)cos2\n(\n)\n−\n(\n)\n1\n2\nsin θ\nP\nP\nT\nT\nT\nP\nP\nT\nT\n1\n0\n1\n0\n0\n1\n0\n1\nθ\nθ\nθ\nθ\n( ) =\n−\n−\n+\n+\n−\n+\n+\n(\n)\ncos\nsin\ncos2\n194\nSection 2\nMath and Physics\n",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "tm1[0]=A[0]*cos(kt)-B[0]*sin(kt);\ntm1[1]=A[1]*cos(kt)-B[1]*sin(kt);\nt0[0]=A[0];\nt0[1]=A[1];\ndouble u=2*cos(kt);\ntp1[0]=t0[0];\ntp1[1]=t0[1];\nprintf(\"%f %f\\n\", tp1[0], tp1[1]);\nfor(int n=2; n<=k+1; n++) {\ntp1[0]=u*t0[0]-tm1[0];\ntp1[1]=u*t0[1]-tm1[1];\nprintf(\"%f %f\\n\",tp1[0], tp1[1]);\n// switch\ntm1[0]=t0[0];\ntm1[1]=t0[1];\nt0[0]=tp1[0];\nt0[1]=tp1[1];\n}\n}\nThis code prints the cosine and sine between A and B and the result is in the vari-\nable tp1. If you want to compute cosine and sine between two arbitrary vectors, you\ncan compute an orthogonal vector using the Gram Schmidt orthogonalization algo-\nrithm, as shown in the referenced paper.\nDiscussion\nBy forcing d to be equal to zero in Equation 2.7.2, you get\n(2.7.12)\nThis is obviously the equation for an ellipse and this proves that it is possible to\nconstruct a perfect circle/elliptical arc with the trigonometric splines. Moreover, because\nthe curve is parametric, it is possible to construct straight lines using the trigonometric\nsplines. The coefficients are vectors and the function produces a point in space. Each\ncoordinate has its own expression and the only thing that differs is the coefficients.\nTherefore, it is no problem to construct a straight line even though trigonometric func-\ntions are involved. Figure 2.7.2. shows a perfect arc drawn using the trigonometric spline.\nNote also that when you set d = 0, T0 + T1 = P1 – P0 from Equation 2.7.4.\nP\na\nb\nc\nθ\nθ\nθ\n( ) =\n+\n+\ncos\nsin\n2.7\nTrigonometric Splines\n195\n",
      "content_length": 1482,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "Conclusion\nThe trigonometric spline is interesting because it is possible to construct perfect circle\narcs. The trigonometric nature of the spline makes it possible to construct splines and\nsurfaces that are not really possible with ordinary cubic splines, unless the surface is\napproximated with several cubic splines, in which case ripples in curvature can create\ngeometric artifacts that are visually obvious and undesirable.\nAlthough this discussion has been purely theoretical, we believe trigonometric\nsplines have great potential for solving certain problems that occur in modern game\ndevelopment. \nOne natural place for the application of trigonometric splines is within digital \ncontent-creation (DCC) tools. These splines are a perfect, simple solution to the problem\nof modelling game geometry that needs to be a pristine conic section.\nBeyond DCC, these splines can make an impact within a game’s runtime envi-\nronment as well. We envision a technique, for example, using trigonometric splines to\nimplement specialized game physics solutions, such as simulating various “machin-\nery” and Rube Goldberg machines with perfect arcs that osculate at perfect tangents,\nin a beautiful way that is visually free of aliasing that piecewise polylines or traditional\ncubic splines might introduce.\nWe also believe that there is great potential to apply this technique to various pro-\ncedural geometry, procedural texturing, and procedural animation techniques. The\ntechnique is fairly efficient, and may be suitable for implementation in the geometry\nshader stage introduced with the most recent graphics hardware. Imagine the possibil-\nities that might include a whole new generation of awesome roller coaster games!\n196\nSection 2\nMath and Physics\nFIGURE 2.7.2\nA circle arc is drawn using the trigonometric curve.\n",
      "content_length": 1817,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "References\n[Alba04] Alba-Fernandez, V., Ibanez-Perez, M.J., and Jimenez-Gamero, M. D. “A\nBootstrap Algorithm for the Two-Sample Problem Using Trigonometric Hermite\nSpline Interpolation,” Communications in Nonlinear Science and Numerical Simu-\nlation (April 2004), Vol. 9, No. 2, pp. 275–286.\n[Barrera04] Barrera, T., Hast, A., and Bengtsson, E. “Incremental Spherical Linear\nInterpolation,” Proceedings of SIGRAD (2004), Vol. 13, pp. 7–10.\n[Han03] Han, X. “Piecewise Quadratic Trigonometric Polynomial Curves,” Mathe-\nmatics of Computation (July 2003), Vol. 72, No. 243, pp. 1369–1377.\n[Lyche79] Lyche, T. “A Newton Form for Trigonometric Hermite Interpolation,”\nBIT Numerical Mathematics (June 1979), Vol. 19, No. 2, pp. 229–235.\n[Schoenberg64] Schoenberg, I. J. “On Trigonometric Spline Interpolation,” Journal\nof Mathematics and Mechanics (1964), Vol. 13, No. 5, pp. 795–825.\n[Shoemake85] Shoemake, K. “Animating Rotation with Quaternion Curves,”\nProceedings of the 12th Annual Conference on Computer Graphics and Interactive\nTechniques, ACM SIGGRAPH (1985), Vol. 19, No. 3, pp. 245–254.\n2.7\nTrigonometric Splines\n197\n",
      "content_length": 1121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "199\n2.8\nUsing Gaussian Randomness\nto Realistically Vary\nProjectile Paths\nSteve Rabin, Nintendo of America Inc.\nsteve.rabin@gmail.com\nW\nhether shooting a gun or firing off some arrows, we all have an intuitive sense\nof what the shot distribution on a bull’s eye target should look like. Shots will\ngenerally be peppered near the center with a few straying shots. This isn’t the kind of\ndistribution generated from rand(), but rather a special kind of randomness com-\nmonly represented by a bell curve. Fortunately, there are random number generators\nthat are capable of generating this type of Gaussian randomness. This gem discusses a\nvery efficient Gaussian random number generator, detailing how it should be applied\nto simulate natural variations in the paths of projectiles.\nGaussian Distribution\nPseudo-random number generators (PRNGs) like rand() produce uniform distribu-\ntions, in which there is an equal (or uniform) chance of any given number being\nselected from a given range. For example, in the range [0, 1], the odds of selecting 0.3\nor 0.5 are the same. A Gaussian distribution, which is sometimes known as a normal\ndistribution, favors positive and negative numbers centered near zero. When the stan-\ndard deviation of this distribution is 1.0, it is called a standard normal distribution, as\nshown in Figure 2.8.1. This distribution is often referred to as a bell curve because of\nits shape.\nTo interpret Figure 2.8.1, consider the 68–95–99.7 rule. According to this rule, \n68% of the values lie within one standard deviation of the mean [–1, 1], 95% of the\nvalues lie within two standard deviations [–2, 2], and 99.7 % of the values lie within\nthree standard deviations [–3, 3]. The remaining 0.3% of the values lie beyond \nthis range, with the chance of seeing numbers beyond [±]5.0 being less than one in a\nmillion.\nNow that you have a better feel for what a Gaussian distribution looks like, let’s\nlook at a few random number generators that can create such distributions.\n",
      "content_length": 1994,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "Generating Gaussian Randomness\nGaussian random number generators (GRNGs) are useful for statistical analysis and\nhave been studied in-depth for both speed and quality [Thomas07]. Speed is a concern\nbecause large simulations require billions of normally distributed random numbers.\nThese simulations, which might perform communications or financial modeling, are\nconcerned with the quality and accuracy of the distribution in the far tails (beyond six\nstandard deviations). The reason is that extremely rare events can affect the outcome of\nimportant features that these simulations wish to explore. \nFortunately, video games don’t require this level of rigor. In fact, games have the\nopposite problem in that a GRNG shouldn’t generate extreme, but rare, numbers\nbecause they would appear as an error to the player. For example, if tree heights were\ndetermined with a GRNG, it would be odd to see most trees between 10 and 15 meters\nwith just one really tall 30 meter tree. Consequently, for most purposes, any GRNG you\nuse should reject (but not clamp) values beyond three standard deviations.\nGaussian Random Number Generators\nOne popular high-quality GRNG is polar-rejection [Knop69] (also known as the polar\nform of the Box-Mueller transform). This algorithm was made popular by its inclusion\nin the book Numerical Recipes in C [Press97] and is notable because its most expensive\noperations consist of only one logarithm and one square root (avoiding sine and\ncosine, which are required in the original Box-Mueller transform [Box58]). Although\nthis is a reasonable GRNG, there are faster algorithms.\n200\nSection 2\nMath and Physics\nFIGURE 2.8.1\nGaussian distribution (normal distribution) with a mean of zero and a \nstandard deviation of 1.0. The horizontal axis represents the value of the random numbers\ngenerated and the vertical axis is the likelihood of seeing any particular value. The tails of \nthis distribution are the seldom-seen values beyond three standard deviations (less than –3.0\nand more than 3.0).\n",
      "content_length": 2018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "The ziggurat method [Marsaglia00] is a second popular GRNG and is often cited\nas the best algorithm given speed versus quality tradeoffs [Thomas07]. It’s faster than\npolar-rejection, due to 1KB of lookup tables and very rare calls to transcendental func-\ntions. However, for game development, accessing these lookup tables will result in data\ncache pollution, causing the game to run slower than with polar-rejection, due to the\nhigh cost of memory access relative to the CPU speed on most platforms. For example,\nwhile the ziggurat method is actually very fast, the lookups will evict other parts of the\ngame program from the cache, negatively affecting the overall game speed more than\npolar-rejection.\nGiven that the two previous algorithms are overkill for game applications, the best\nmethod is a simple and efficient technique called the central limit theorem, sometimes\nreferred to as the sum-of-uniforms [Thomas07]. This algorithm takes several uniform\nrandom numbers, such as those generated by a PRNG like rand(), and adds them.\nAccording to the central limit theorem, the sum of these uniform random numbers will\nresult in a single Gaussian distributed random number. This algorithm performs poorly\nin the tails, but this is acceptable because you generally aren’t interested in values beyond\nthree standard deviations.\nMore precisely, the central limit theorem states that the sum of K uniform ran-\ndom numbers in the range [–1, 1] will approach a Gaussian distribution with mean\nzero and standard deviation \n. For example, if you add three uniform random\nnumbers, they will have a mean of zero and a standard deviation of \n(which is very convenient because the mean and standard deviation are identical to a\nstandard normal distribution). The following code generates a Gaussian distribution\nby adding three 32-bit signed uniform random numbers (generated by a very fast xor-\nshift PRNG [Marsaglia03]).\ndouble gaussrand(void)\n{\nstatic unsigned long seed = 61829450;\ndouble sum = 0;\nfor(int i=0; i<3; i++)\n{\nunsigned long hold = seed;\nseed^=seed<<13; seed^=seed>>17; seed^=seed<<5;\nlong r = hold+seed;\nsum += (double)r * (1.0/0x7FFFFFFF);\n}\nreturn sum; //Returns [-3.0,3.0]\n}\nThe function gaussrand() returns a double in the range [–3.0, 3.0]. If you want\na number in the [–1.0, 1.0] range, simply divide the result by 3.0 (which will conse-\nquently shrink the standard deviation to 0.33). The distribution roughly follows the\n68–95–99.7 rule, but because the tails are missing, the distribution for this particular\nalgorithm (with this seed) is 66.7–95.8–100.\n3 3\n1 0\n= .\nK 3\n2.8\nUsing Gaussian Randomness to Realistically Vary Projectile Paths\n201\n",
      "content_length": 2661,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "The central limit theorem method can be made more accurate, especially in the\ntails beyond three standard deviations, by summing more numbers (increased K ).\nHowever, this makes the algorithm slower for not much benefit, because you gener-\nally don’t care about the tails.\nVarying Projectile Paths\nAn ideal application for a GRNG in games is adding random variation to projectile\npaths. As discussed earlier, projectiles, like bullets and arrows, are expected to have\nsome variation that follows a Gaussian distribution (probably due to many random\nvariables like wind, hand shakiness, and projectile irregularities that additively con-\ntribute to the final path). However, this Gaussian distribution needs to be expanded\ninto 2D, as shown in Figure 2.8.2.\n202\nSection 2\nMath and Physics\nFIGURE 2.8.2\nThe left target shows a Gaussian probability distribution in 2D. This can best\nbe visualized as the middle figure, which is a Gaussian distribution revolved around the center.\nThe right target is an example of 30 bullets perturbed by the revolved Gaussian distribution.\nBecause the rings are placed at one and two standard deviations, roughly 68 % of the bullets\nstrike within the smallest ring and 95% of the bullets strike within the two smallest rings.\nThe distribution in Figure 2.8.2 was created with the help of polar coordinates.\nThis requires two random numbers for each 2D point: an angle and a distance. The\nbullets in Figure 2.8.2 were computed by generating a uniform random angle in the\nrange [0, 2\b], along with the absolute value of a Gaussian random distance in the range\n[–1, 1]. By using a uniform random number for the angle, you guarantee that the bul-\nlets are evenly distributed at all angles around the center. By using a Gaussian random\nnumber for the distance, you guarantee that the bullets are concentrated near the center,\nfollowing a normal distribution and the 68–95–99.7 rule.\nThe 2D distribution in Figure 2.8.2 is not technically a 2D Gaussian distribution\n(also known as a multivariate normal distribution). A true 2D Gaussian distribution\nis constructed with two Gaussian random numbers plotted against each other in\nCartesian coordinates (not polar coordinates). This distribution is useful in statistics,\nbut is not desirable for what you’re trying to model.\n",
      "content_length": 2297,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "The flaw in this kind of a distribution, for these purposes, can be seen in the fol-\nlowing example. If x is a Gaussian random number and y is a Gaussian random num-\nber, a coordinate of (1.41, 1.41) is statistically less likely than a coordinate of (2.0,\n0.0), even though these coordinates are equidistant from the origin. Therefore, a true\n2D Gaussian distribution will favor the coordinate axes over the diagonals, which is\nundesirable for a 2D projectile distribution.\nAdditional Applications\nGaussian randomness is useful for many game applications other than projectiles. For\nexample, if there are multiple characters or vehicles that move together, there is a ten-\ndency to see lockstep movement. This can be avoided by perturbing each agent’s\nacceleration, top velocity, or animation speed by a GRNG. This will cause small vari-\nations around an average that will break up any synchronized movement or anima-\ntions. The result is subtle variations with a few outliers.\nAnother application is to use a GRNG to perturb the heights of characters, trees,\nor buildings. If you have algorithmic control over the geometry of objects in your\ngame, realistic variability can be created with a GRNG. This helps when the number\nof visible objects at any one time is large and you need natural variation. In general,\nmany physical characteristics or attributes that should be randomized around an aver-\nage will likely benefit from a Gaussian distribution. \nGaussian Distributions in Nature\nWhy do many distributions in nature follow a Gaussian distribution (or bell curve),\nsuch as human intelligence or the heights of trees? The central limit theorem alludes to\nthe answer. When there are many uniform (or even non-uniform) random variations\nthat contribute to a given property, the distribution of that property becomes more\nnormal (rather than remaining uniform). Although this is a gross simplification of\nmost systems in nature, it does shed light on why so many properties and systems\nroughly display a Gaussian distribution.\nFor example, if scores on an IQ test are influenced by genetics, diet, schooling,\nlife experiences, and environment, each of these variables combine into the single IQ\nscore. If all of these variables were uniform and weighted equally, the central limit the-\norem says that the result would be a normal distribution. Of course, each of these\nvariables is not likely to be uniform, but rather the sum of other random variables,\nwhich are in turn affected by even more random variables. Therefore, many of these\nrandom variations like diet or schooling that influence IQ probably already follow a\nnormal distribution. Ultimately, you can approximate many properties in nature by\nassuming that many small, independent effects are additively contributing to a given\nproperty.\n2.8\nUsing Gaussian Randomness to Realistically Vary Projectile Paths\n203\n",
      "content_length": 2875,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "Conclusion\nGenerating Gaussian randomness for games is embarrassingly simple and efficient\nusing the central limit theorem. However, many game programmers aren’t even aware\nof this type of random number generator. Therefore, the biggest challenge is simply\ngetting the word out and letting developers know that this extra tool exists. \nMany physical systems and characteristics tend to have a normal distribution that\ncan be modeled using Gaussian randomness. By combining uniform randomness\nwith Gaussian randomness in polar coordinates, applications like adding realistic vari-\nation to projectiles can easily be accomplished.\nReferences\n[Box58] Box, G.E.P. and Muller, Mervin E. “A Note on the Generation of Random\nNormal Deviates,” The Annals of Mathematical Statistics (1958), Vol. 29, No. 2,\npp. 610–611.\n[Knop69] Knop, R. “Remark on Algorithm 334 [g5]: Normal Random Deviates,”\nCommun. ACM, 12(5), 1969.\n[Marsaglia00] Marsaglia, George, and Tsang, Wai Wan. “The Ziggurat Method for\nGenerating Random Variables,” Journal of Statistical Software, Vol. 5, 2000, paper\nand code available online at http://www.jstatsoft.org/index.php?vol=5.\n[Marsaglia03] Marsaglia, George. “Xorshift RNGs,” Journal of Statistical Software,\nVol. 8, 2003, available online at http://www.jstatsoft.org/v08/i14/xorshift.pdf.\n[Press97] Press, W.H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. Numer-\nical Recipes in C, 2nd edition, Cambridge University Press, 1997.\n[Thomas07] Thomas, David B., Leong, Philip G.W., Luk, Wayne, and Villasenor,\nJohn D. “Gaussian Random Number Generators,” ACM Computing Surveys 39,\n2007.\n204\nSection 2\nMath and Physics\n",
      "content_length": 1645,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "205\nS E C T I O N\n3\nAI\n",
      "content_length": 23,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "207\nIntroduction\nBrian Schwab\nE\nvery year, new games come out. Game programmers rush to see what they’ll do to\nfurther our ideas of what a game can accomplish. Will they be photo-realistic?\nWill all physical interactions be modeled with true-to-life physics? Will the audio\nmake you feel like you’re six feet from the action, and your heart pumping hard with\nthe music? Finally, and most importantly to this section of Game Programming Gems\n7, will the in-game characters have half a damn brain?\nAI is one of the hardest categories to get right. Sure, art and music are pretty sub-\njective, but what seems intelligent to people is an individual distinction on a whole\nother level. Take for instance a gut level reaction. You’re walking down the street, and\na full grown, adult male lion steps out from behind the next tree in front of you.\nWhat’s the intelligent thing to do? The vast majority of people would say one of the\nfollowing things:\n• Run.\n• Wait and see what’s going to happen.\n• Walk backward slowly and don’t make any sudden moves.\nHowever, any kind of question like this would also guarantee you plenty of not-\nso-common responses:\n• Look for something to defend yourself with.\n• Flag down a cab and get out of there.\n• Throw your huge leather purse at the lion, hoping he’d stop to eat it.\n• Scream at the top of your lungs, scaring him off.\n• Pepper spray him.\nWhich one is correct? The answer might easily be all of them. In real life, roughly\n70% of all people in this situation would freeze exactly like a deer in the headlights.\nWe’re hardwired to sit perfectly still, mostly because during evolution, we learned that\nmost of our predators had motion-based eyesight (meaning, they see motion much\nbetter than other types of visual stimulus, like color or shape). Had lions and hyenas\non the plains of Africa during our evolution instead been equipped with color-based\neyesight, humans might today be able to change color like the chameleon, which\nwould be a tragedy if you’re a tattoo artist.\nObviously, we can’t model that kind of standstill in games. If you came around\nthe corner in a shooter and pretty much everybody froze, players would likely think\nthe game was broken. However, if everybody ran, the game would get monotonous\n",
      "content_length": 2254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "quick (although this still might work for some games). If everybody readied a weapon\nof some sort, you’ve got a pretty overwhelming scene. But a combination of these \ncan work. You can even tune what mix of them they use in order to get the degree of\nchallenge you want the player to overcome.\nThe point is, you can’t always base your game’s AI behaviors on realism. You also\ncan’t base your AI on what any one of us might think is the correct behavior, because\npeople’s notion of what constitutes an intelligent response can vary so greatly. Only \nby continuing the search for new techniques can you ever hope to convey a little\nperceived “intelligence” from your creations.\nThe gems in this section show just how far AI is coming. No longer is the devel-\nopment community as concerned about the trivial matters of AI implementation.\nNow we’re delving into issues like more realistic perception models, using new pro-\ngramming techniques to simplify the creation of our AI systems, giving our AI char-\nacters true personality traits, and analysis of our AI at a statistical trend level.\nJohn Harger and Nathan Fabian have written a gem in which you’ll learn how to\nuse a supervised learning technique called behavior cloning, which can be used to\ncapture human performances. Steve Rabin and Michael Delp detail a unified sensing\nmodel, showing that you can model large portions of reality quite effectively with a\nnice, orderly, systemic approach. Iskander Umarov and Anatoli Beliaev explain how to\nuse generic programming to create and manage hugely complex AI systems using\ncode that is small, fast, and robust. Michael F. Lynch brings you a detailed gem\nconcerning modeling attitudes within your AI agents. G. Michael Youngblood and\nPriyesh N. Dixit describe advanced player logging analysis, which can uncover useful\npatterns of gameplay and player interaction that can be difficult to see with just cur-\nsory observation. Michael Dawe shows you how to improve your planning systems \nby using plan merging to increase the reactivity of your systems without incurring full\nre-planning costs. Finally, because you might never tire of hearing about ways to\nimprove usage and understanding of path-finding algorithms, Robert Kirk DeLisle\nprovides insight into an A* technique called fringe search.\n208\nSection 3\nAI \n",
      "content_length": 2317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "209\n3.1\nCreating Interesting Agents\nwith Behavior Cloning\nJohn Harger\nNathan Fabian\nH\numan opponents are interesting. The popularity of gaming online may have\nsomething to do with the kinds of opponents you find there (or maybe mostly\nthe idea that you can “pwn some noobs”) but regardless of why it’s popular, it wouldn’t\nhurt to breathe some life into the offline, single player opponents (or even allies). This\ngem explains how to use a machine-learning technique called behavior cloning \n[Sammut92] to borrow from styles and strategies of humans playing the game and\nplace them into game agents.\nBehavior cloning is essentially a version of supervised learning. In supervised learn-\ning, the idea is that the human trainer provides a set of labels for particular objects and\nthe algorithm learns to recognize, or predict, labels based on the attributes of those\nobjects. When it sees similar characteristics in a new object, it should correctly label it.\nIn behavior cloning, the trainer acts in response to a stimulus. The response becomes\nthe label associated with the stimulus, which is the object. The algorithm then learns to\nrepeat the same kinds of actions in the presence of the same kinds of stimuli.\nThis extends very nicely into games where it’s easy to find the response a person\nwill make to a stimulus, that is, the game state. (In the game nearly all interaction takes\nplace within the game context and each game session is fairly similar to the last. It is\nimportant to note, however, that in some cases the game can include out-of-band infor-\nmation like clan membership and rivalries that the agent wouldn’t be able to simulate.)\nBecause there is so much similarity between supervised learning and behavior\ncloning, this technique can be done with any off-the-shelf supervised learning algo-\nrithm. This gem uses a decision tree because the output is easier to edit than, say, the\nweights on a neural net.\nIn other words, it is not necessary to become a machine learning expert to exploit\nadvances using this technique. Even better, the game playing trainers don’t even need\nto know that the learning is going on in order for it to be effective. This gem shows\nyou how well the tried-and-true decision tree learning algorithm works when borrow-\ning human characteristics to create interesting, playable game agents.\n",
      "content_length": 2338,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "210\nSection 3\nAI \nExample: The Demo Game\nThroughout this gem, the examples refer to a game design that resembles the classic\ncomputer game Space War. That is, two ships face off on an infinite 2D playing field;\nthe goal of the game is to destroy the other ship. This simple design provides an easy-\nto-understand game environment in which you can train an agent.\nHow to Set Up the Feature Space Output\nThe feature space is the most important thing in instance-based machine learning,\nwhich is what you use to do behavior cloning. It is a set of attributes or “features” used\nto record instances of the game state for learning (see Figure 3.1.1). You must carefully\nconsider the design of the features to be able to train interesting agents, and perhaps to\nbe able to train an agent at all.\nFigure 3.1.1\nExample of a feature space.\nThe features must provide enough information to make decisions. Remember, the\ngoal is to train an agent to behave like a particular human. Try thinking of how you\nwould approach the situation; you might consider the distance to your opponent’s\nship, the direction of his or her ship from your current heading and even the direction\nhe or she is facing from you. Don’t be afraid to keep adding features; some players\nmight act differently if their opponent is “off radar,” and that distinction is going to\nprovide more information than distance alone. It might take the learning algorithm 30\nminutes to train an agent with detailed features, but it will likely produce a richer one.\n",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "However, you should be careful not to use absolute values for information such as\nposition and orientation. If the agent is trained to accelerate based on an absolute\ndirection, the resulting actions might be the opposite of what was expected! For exam-\nple, if your ship is at point (2,1) and the enemy is at (0,0), you may have turned right.\nIf this is what the agent learns, it will always turn right when its opponent is located\nat (0,0), even if the trainer would have turned left. If the feature was relative, such as\n15° to the left (\t2.5°), the agent will turn right when the opponent is just to the left,\nregardless of its absolute position. Think of it this way—would you consider your\nabsolute heading, latitude and longitude when entering a brawl? Neither should the\nagent—at least if you want it to act realistic.\nIn addition to erratic behavior, absolute values can give a search space that might\nbe much too large to process. If the range of location is infinite, the learning algo-\nrithm might never be able to classify behavior correctly. Instead of a tight, well struc-\ntured tree, you end up with a noisy mess.\nTable 3.1.1\nEquations for Calculating the Features Shown in Figure 3.1.1\nName \nEquation \nDescription \nDistance \nDistance between the two ships\nDirectionTo\nAngle to Ship2 from Ship1’s facing direction\n(–180° to 180°) \nDirectionFrom\nAngle from Ship2’s facing direction to Ship1 \n(–180° to 180°)\nVNorm\nChange in distance between ships\nDDirectionTo \nChange in DirectionTo \nDDirectionFrom \nChange in DirectionFrom \nTraining an Agent\nNow comes the fun part: training the agent. This can be done with nearly any machine-\nlearning technique available. When developing your own game, feel free to experiment\nwith existing implementations or write your own. If you are interested in machine learn-\ning, take a look at [Witten99].\nWe chose to create our own simple decision tree implementation. It is certainly\nnot the best—because it performs no linear regression, it is limited to working with\ndiscreet values. Because predefined linear ranges must be mapped to integers, the trees\nproduced are probably not going to fit the data as tightly as they could otherwise. As\nθ\nθ\nτ\nτ\n2\n2\n1\n−\n−\n(\n)\nθ\nθ\nτ\nτ\n1\n1\n1\n−\n−\n(\n)\nr\nd\nd\nτ\nτ\n−\n−1\nθ 2\n1\n2\n2\n=\n⋅\n⋅\n−\n⊥\ntan\nr\nr\nr\nr\nd\nv\nd\nv\nθ1\n1\n1\n1\n=\n⋅\n⋅\n−\n⊥\ntan\nr\nr\nr\nr\nd\nv\nd\nv\nr\nd\n3.1\nCreating Interesting Agents with Behavior Cloning\n211\n",
      "content_length": 2390,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "a result, if distance 0.5 through 1.0 is a predefined interval, and one trainer tended to\nreact at 0.54 and the other at 0.98, they both would branch at distance 1.0, giving\nboth agents a similar feel.\nWhen you run our demo AIShooter, found on the CD-ROM, the game records\nthe state 100 times a second. These states fill in the feature space defined earlier: the\ndistance between the two ships, their relative directions to each other, and so on. In\naddition, the states of the player’s controls are also recorded, such as turning left or\nright and accelerating forward or backward, and you offset these controls by 150ms to\naccount for human reaction time. This is the stimulus/response information you need\nto build the decision tree.\nBuilding the Trees from the Samples\nOnce you have a game session recorded, shown in Table 3.1.2, you can use that infor-\nmation to build the decision trees. Different control groups are split between different\ntrees. Forward, Reverse, and None are the possible decisions for one tree. Left, Right,\nand None are possible decisions for another tree. The nodes of the tree test for all the\nvalues that one feature takes on. There is one child and one path down the tree for\neach value the feature can take on. Each child is passed from its parent all the data that\ncorresponds to the value of that feature for that path. Recursively then, the child node\ndetermines whether the data is pure (meaning all the same).\nTable 3.1.2\nSample Recorded Game State Data\nDistance\nDirectionFrom \nHitpoints \nTurning\n“2 to ∞”\n1\n100\nRIGHT\n“0 to 1”\n1\n100\nLEFT\n“0 to 1”\n1\n100\nLEFT\n“1 to 2”\n1\n100\nNONE\n“0 to 1”\n3\n100\nNONE\n“2 to ∞”\n2\n100\nLEFT\n“1 to 2”\n2\n100\nNONE\n“2 to ∞”\n3\n100\nRIGHT\n“0 to 1”\n3\n100\nRIGHT\n“2 to ∞”\n2\n100\nLEFT\n“0 to 1”\n3\n100 \nRIGHT\nThere is enough data left to make the determination and undergo searches for a\nfeature, if so. If not, the node is a leaf and determines which label (that is, Forward) is\nthe majority in its data and sets that as the return value. Listing 3.1.1 shows the recur-\nsive algorithm for Node learning.\n212\nSection 3\nAI \n",
      "content_length": 2074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "Listing 3.1.1\nRecursive Node Learning Algorithm\nTreeNode* DataSet::learnNode (const string& targetName,\nconst string& columnName, const col_t& column,\nconst col_set_t& workingSet, unsigned int threshold)\n{\ncol_t newTarget = getColumn (targetName, workingSet);\nint majority = for_each (newTarget.begin (), newTarget.end (),\nMajority ());\nfloat purity = (float) count (column.begin (), column.end (),\nmajority) / (float) column.size ();\nif (column.size () <= threshold || purity >= 0.99f) {\nreturn new TreeNode (majority);\n}\nint max = *max_element (column.begin (), column.end ());\nTreeNode *node = new TreeNode (columnName, 0, max);\nfor (int i = 0; i <= max; i ++) {\ncol_set_t newWorkingSet = getAllWhere (columnName, i,\nworkingSet);\nnewWorkingSet.erase (columnName);\nif (newWorkingSet.empty ()) {\ncontinue;\n}\npair<string, col_t> best = getBest (targetName, \nnewWorkingSet);\nnode->addChild (i, learnNode (targetName, best.first,\nbest.second, newWorkingSet, threshold));\n}\nnode->fillIn();\nreturn node;\n}\nTo determine which feature to use for the node, you must find the feature that\nleads to the most uniform data. Because the tree is trying to find the one label that\nmakes sense for a given state, you want to find the set of states that describe that one\nlabel. This is the crux of what the decision tree does by encoding the features of those\nstates that correspond to the label. Ideally, when the tree is at a leaf node, as described\npreviously, there is one label in every state.\nFor example, assume you are training a tree from the data in Table 3.1.2 for\ndeciding whether to turn, and your feature space defines distance on three intervals: 0\nto 1, 1 to 2, and 2 to infinity. Let’s take a look at the distance feature in terms of infor-\nmation gain. First, look at Equation 3.1.1 for computing the information in a set of\ndata.\n(3.1.1)\nInfo V\nv\nv\nn\nn\nn\nn\nv\nv V\nv V\n( )\nlog\nlog\n,\n=\n−\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟+\n=\n∈\n∈\n∑\n∑\n2\n2\n3.1\nCreating Interesting Agents with Behavior Cloning\n213\n",
      "content_length": 1974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "Without going into detail on the meaning of information theory, the purpose here\nis to track how different the data is. The less information the better because it means\nthe labels are more uniform. In the example, there are three None, four Left, and \nfour Right labels, giving an information of Info([3,4,4]) = (–3 log2 3 – 4 log2 4 – 4 log2\n4 + 11 log2 11)/11 = 1.5726.\nIf you look at only those labels where Distance is “1 to 2,” you have only two\ninstances and both are None, giving you Info([2]) = (–2 log2 2 + 2 log2 2)/2 = 0. In\nother words, there is no information in that set because they are all the same.\nThe information in “0 to 1” is Info([1,2,2]) = 1.5219, and “2 to ∞” is Info([2,2]) =\n1. Notice these are both close to the original information because the distributions of val-\nues are very similar.\nNow you find the gain as 1.5726 – 3/11 * 0 – 4/11 * 1.5219 – 4/11 * 1 = 0.6555.\nIt is very easy to see that the gain on the hitpoints would be 0, as there is no change.\nThe gain using direction is 0.4816. Getting a uniform set in one of those examples\nreally helps the gain when you use distance.\n(3.1.2)\nIn each of the three new nodes, you pass the subset of data that corresponds with\nthat value of distance and recurse using only that. There is no reason to use distance\nagain in the second pass (it would no longer improve the information), so direction\nbecomes an obvious choice as hitpoints still provides no information about the labels.\nThe tree is shown in Figure 3.1.2. Notice again for distance “1 to 2” that since the set\nis already pure you just return that label; there is no reason to continue the recursion.\nGain O F\nInfo O\nInfo O F\n( ; )\n( )\n( | )\n=\n−\n214\nSection 3\nAI \nFigure 3.1.2\nExample decision tree.\nYou pick the label, or control, by finding the majority value of that set. It is impor-\ntant to note that for a majority to make sense in statistics, you want to make sure there\nare enough samples to make the data meaningful. In Listing 3.1.1 for learnNode, you’ll\n",
      "content_length": 2004,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "notice a check against the column to ensure that there are at least a certain number of\nrows. This is the “no free lunch” rule of machine learning. There is always one free\nparameter and it often depends on the data. We use 250 as we guess that seeing about\n2.5 seconds worth of a game is enough to counteract any accidental key presses, which\nare considered noise. Additionally, we offset the controls in the game state by 150ms to\nsimulate human reaction time.\nBuilding the AI Script from the Trees\nOnce your trees are finished, you need to convert them into a form that’s usable by\nyour game. Although you could go with some tree interpreter, which is usually fast\nand efficient, this example translates them into script form, so they can be read and\nedited by hand. All you really have to do is convert your tree into the scripting lan-\nguage of your choice using conditional statements. For example, the turning tree in\nFigure 3.1.2 written as a Lua script would look like the code in Listing 3.1.2. (Please\nnote that we used strings in some places for demonstration purposes, whereas a real\nscript would use numbers.)\nListing 3.1.2\nTurning Tree as a Lua Script\nfunction turn\n--Root node\nif GameState.Distance == \"0 to 1\" then --Left branch\nif GameState.DirectionFrom == 1 then \nShip.turn (\"LEFT\")\nelseif GameState.DirectionFrom == 2 then\nShip.turn (\"NONE\")\nelseif GameState.DirectionFrom == 3 then\nShip.turn (\"RIGHT\")\nelse\nShip.turn (\"NONE\")\nend\nelseif GameState.Distance == \"1 to 2\" then --Center branch\nShip.turn (\"NONE\")\nelseif GameState.Distance == \"2 to inf\" then --Right branch\nif GameState.DirectionFrom == 1 then\nShip.turn (\"RIGHT\")\nelseif GameState.DirectionFrom == 2 then\nShip.turn (\"LEFT\")\nelseif GameState.DirectionFrom == 3 then\nShip.turn (\"RIGHT\")\nelse\nShip.turn (\"NONE\")\nend\nelse\nShip.turn (\"NONE\")\nend\nend\n3.1\nCreating Interesting Agents with Behavior Cloning\n215\n",
      "content_length": 1886,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "That’s all there is to it. When the agent is running, you execute this script func-\ntion after setting the current game state, and the decision tree does its work. The agent\nwill respond to the state in a way that approximates the human who trained it.\nConclusion\nThis gem illustrates a very concise and convenient way to make agents that learn\nbehaviors from humans in a simple game. One of the best places to use this technique\nis for creating those supporting-role characters, like guards, that normally have a very\nlimited behavior, but could benefit from the introduction of some variety, especially\nin how they respond to the players.\nSome of the details we didn’t get into here involve expanding the feature space to\naccount for more opponents or obstacles. As you can imagine, adding the distance\nand direction for each one of those can start to really grow. It gets even worse if you\nconsider needing to add the distance and direction from an ally to an opponent. It is\na fully connected graph. The trick in this case is to group the objects and treat them as\na large mass with a single distance and direction.\nThese agents have limited understanding of the passage of time as well. Instead of\nknowing time directly, they know damage to the ship, which lowers as time goes on.\nHowever, if the ship was repaired back up to a certain level they would have no mem-\nory of having been damaged in the first place. You could consider adding time as a fea-\nture itself, but adding the absolute time poses the same problems as adding absolute\nposition or orientation. However, as you’ll see if you play around with the demo on\nthe CD-ROM, it isn’t necessary to have that memory to get pretty good behaviors.\nAs with anything, “there ain’t such a thing as a free lunch.” You cannot make the\nend game super villain with this algorithm, but you can add some variety of behaviors\nto his (or her) minions. We have set up a forum at http://www.tosos.com to talk\nabout some of the issues and solutions to creating more complex behaviors and deal-\ning with more complex games. There are links there that go into more depth as to\nwhy this works and the theoretical background. We would love to have you join us\nand share your experiences!\nReferences\n[Sammut92] Sammut, C., Hurst, S., Kedizer, D., and Michie, D. “Learning to Fly,”\nProceedings of the Ninth International Conference on Machine Learning\n(ICML–1992), Aberdeen: Morgan Kaufmann, pp. 385–393.\n[Witten99] Witten, Ian H., and Frank, Eibe. Data Mining: Practical Machine Learn-\ning Tools and Techniques with Java Implementations, Morgan Kaufmann, 1999.\n216\nSection 3\nAI \n",
      "content_length": 2621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "217\n3.2\nDesigning a Realistic and\nUnified Agent-Sensing Model\nSteve Rabin, Nintendo of America Inc.\nsteve.rabin@gmail.com\nMichael Delp, WXP Inc.\nmichaeljdelp@gmail.com\nW\nith increased visual realism, players expect agents to sense the game world with\ngreater fidelity and subtlety. However, agent vision models in games have tradi-\ntionally been very simplistic, using a combination of view distance, view cone, and\nline-of-sight checks [Rabin05]. Hearing models, when implemented, have also been\nfairly simple, usually testing against some cutoff distance to verify whether a sound is\nheard [Tozour02]. Although these basic agent-sensing models are efficient and simple\nto program, they are transparent and appear shallow to game players. For example,\nwhen agents use a discrete distance check for vision, it results in an absolute blind\nzone beyond a certain distance. Players intimately know this and routinely use this\nknowledge to manipulate the enemy AI. This is commonly seen when players lure\nindividual enemies away from enemy groups by repeatedly inching toward them and\nrunning away.\nOnce the developer realizes that current agent-sensing models are rather primi-\ntive, dozens of clever ways to enhance these basic models begin to appear. This gem\ncovers many such additions, eventually combining them into a unified sensing model,\nbecause all senses should collaboratively inform an agent’s awareness of the world.\nThe final model may then be used in any game genre as a core part of the AI.\nThe Basic Vision Model\nBefore the gem begins looking at specific enhancements to vision, this section recaps\nthe core vision model used in the majority of games today. The three core vision cal-\nculations are view distance, view cone, and line-of-sight. Note that these three checks\nare usually computed in this order for efficiency reasons, because a radius test is very\ncheap and a line-of-sight test is very expensive. Figure 3.2.1 illustrates all three tests.\n",
      "content_length": 1968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "The computation for the view distance check is a simple distance test. However,\nit is more efficient to test against the distance squared instead of the actual distance,\nbecause it avoids taking a square root. For example, if the agent can see up to 10\nmeters away, is at coordinate (0,0,0), and the player is at coordinate (5,8,0), you can\ncompare the dot product of the vector between the two entities against the square of\nthe view distance. The dot product of the vector between them is 52 + 82 + 02 = 89.\nCompare this against the view distance squared (102 = 100) and you find that the\nagent can see the player, because 89 is less than 100. The distance squared optimiza-\ntion can be used because you’re only interested in the relative distance, not the actual\ndistance.\nThe second common step is to do a view cone check. This is done by taking the\ndot product of the agent’s normalized forward vector with the normalized vector that\npoints from the agent to the player (refer to the two vectors in Figure 3.2.1). If the\nresult is greater than zero, the player is within the agent’s 180° view cone. If the result\nis greater than 0.5, the player is within the agent’s 120° view cone (cos 60° = 0.5). As\nan optimization, if only the 180° view cone test is required, there is no need to nor-\nmalize the vectors (which potentially eliminates two square root calculations).\nThe final check, the line-of-sight test, is the most costly to perform. This test\nshoots a ray from the agent’s eye level to the location of the player. If it intersects any\ngeometry before it hits the player, the agent can’t see the player. This test can be opti-\n218\nSection 3\nAI \nFigure 3.2.1\nExample of view distance check, view cone check, and line-of-sight\ncheck. In this case, the enemy sees the player because the player passes all three tests. \n",
      "content_length": 1828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "mized by testing against bounding boxes that surround the level geometry, instead of\ntesting against individual polygons. For testing against objects in the world, the\nobject’s lowest LOD can be useful, as well as bounding boxes.\nThese three tests lay the groundwork for a vision model, but as you’ll see later,\nthere are many improvements that can be made.\nThe Basic Hearing Model\nGames that simulate agent hearing, such as games that emphasize stealth, typically\nimplement this feature by having objects emit single-shot sound events that travel a\nparticular distance. For example, each footstep of a player might send out a sound\nevent that gets delivered only to agents within a particular distance from the player.\nThe distance the sound event travels depends on the loudness of each footstep, which\nin this case usually corresponds to the speed of the player. A tiptoeing player spawns\nvery weak sound events that travel only a meter or so, whereas a running player gen-\nerates sound events that travel a great distance.\nFor many games, this is a sufficient hearing model, but it suffers from a similar\nproblem as the vision model, namely that there is an absolute and arbitrary distance\ncutoff. It seems odd and unnatural that a distance of one centimeter might make the\ndifference between completely hearing and recognizing a sound and not hearing it at\nall. As you might suspect, there are many improvements that can be made to this basic\nsensing model.\nAugmenting the Vision Model Toolbox with Ellipses\nThe simple vision tests discussed previously don’t model human vision well. In partic-\nular, view cones have several drawbacks.\n• The agent potentially won’t be able to see entities right next to itself.\n• Visual acuity is highest in the center of vision and degrades with distance. View\ncones overestimate the area of vision far away and underestimate the area of vision\nclose by.\n• To avoid overly large fields of vision far away, designers tend to make view dis-\ntances unrealistically short.\nOne way that designers have dealt with these issues is by testing against multiple\ncones to model human vision [Leonard03]. However, multiple cones can leave holes\nin the agent’s vision. The left portion of Figure 3.2.2 shows a vision model that uses\ntwo cones and a circle to model vision. The narrow cone models the center of focus,\nwhich extends to far distances. The wider cone provides a broader field of view at\nshort distances. The circle catches any entities adjacent to the agent or even behind\nhim (as humans tend to have a sense when someone is right behind them). Note the\nlarge gaps in the vision model outside the intersection of the two cones.\n3.2\nDesigning a Realistic and Unified Agent-Sensing Model\n219\n",
      "content_length": 2730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "A simple solution that solves all of these problems is to use an ellipse for the field\nof view. As you see in the right side of Figure 3.2.3, an ellipse gracefully deals with the\ndegradation of visual acuity with distance without leaving holes in the vision. The\nellipse “starts” a few feet behind the agent to model the sixth sense humans have about\npeople right behind them and encompasses entities adjacent to the agent.\n220\nSection 3\nAI \nFigure 3.2.2\nThe left figure illustrates a vision model using two view angles\nand a circle. Note the holes in the vision system. The right figure illustrates\nan ellipse overlaying the old model. The ellipse gracefully encompasses the\nvarious view angles to give a more accurate model of vision.\nFigure 3.2.3\nThe left figure shows the important components of an ellipse. The\nmiddle figure shows how an example point on the ellipse is calculated. The right\nfigure shows an example view from an agent where [\n] is half the view angle and a is\nhalf the maximum view distance.\n",
      "content_length": 1014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "Ellipse Implementation\nIn order to model vision with an ellipse, it is important to understand its components.\nTake a look at the left side of Figure 3.2.3. The length of the major axis is 2a, and the\nlength of the minor axis is 2b. The two focal points (f1 and f2) are at \t c from the\ncenter of the ellipse, where c2 = a2 – b2. The middle figure illustrates an important fact\nabout ellipses: the distance from the two foci to any point on the outside of the ellipse\nequals 2a. To determine whether something is within the ellipse, you must find the\npositions of the focal points.\nTo model human vision, you place one end of the ellipse at the agent’s eyes. The\ndesigner can then specify a view angle much as he or she would with view cones. The\nview angle will make a triangle with the agent’s eyes and the endpoints of the center\naxis of the ellipse as in the right side of Figure 3.2.3. The designer can also specify a\nmaximum viewing distance; half of which will be the distance from the agent to the\ncenter of the ellipse. So given that [\n] is half the view angle, and a is half the view dis-\ntance, you must first find the equation for c given \n and a:\n(3.2.1)\n(3.2.2)\n(3.2.3)\nSubstitute Equation 3.2.2 into Equation 3.2.3 to get the following:\n(3.2.4)\n(3.2.5)\n(3.2.6)\nEquation 3.2.6 can be precalculated at initialization. Now to find the equations\nfor the focal points, you can use the following equations given the agents’ eyes are sit-\nuated at vPos, and they are looking in the direction vDir.\n(3.2.7)\nYou want the ellipse to start fBehindDist behind the character (so the character\ncan sense characters right next to him). The final equations are as follows.\n(3.2.8)\nOf course fBehindDist could be subtracted from a at initialization in order to\nsave the extra subtractions. \nF\nF\nvPos\nvDir a\nfBehindDist\nc\n1\n2\n,\n=\n+\n−\n±\n(\n)\nF\nF\nvPos\nvDir a\nc\n1\n2\n,\n=\n+\n±\n(\n)\nc\na\n=\n−\n1\n2\ntan θ\nc\na\n2\n2\n2\n1\n=\n−\n(\n)\ntan θ\nc\na\na\n2\n2\n2\n=\n−(\n)\ntanθ\nc\na\nb\n2\n2\n2\n=\n−\na\nb\ntanθ =\ntanθ = b\na\n3.2\nDesigning a Realistic and Unified Agent-Sensing Model\n221\n",
      "content_length": 2038,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "To determine whether an entity is within the agent’s field of view, you simply take\nthe entity’s distance from each of the focal points, add the distances, and check that\nthey are less than the maximum view distance (2a). So two distance checks for each\nentity is all that is needed per agent. Note that you cannot use squared distances in\nthis equation because you have to add them together. These equations work for 3D or\n2D ellipses. Use 3D if height is important to your world.\nUsing an ellipse to model vision is easy to calculate and is not much more expen-\nsive than a cone solution. It is the first building block in providing a more accurate\nhuman-sensing model. \nModeling Human Vision with Certainty\nAs observed previously, the fact that objects are either seen completely or not seen at all\nis an unfortunate side effect of discrete vision tests. The flaw of the discrete vision test is\nmost obvious when you consider the subtlety of real human vision, such as peripheral\nvision in which objects are sometimes only partially recognized. In order to understand\nthis more precisely, let’s quickly review the mechanics of real human vision.\nHuman vision has been studied in-depth, from the retina to the neurons in the\nbrain, but for the purposes here, let’s extract useful measurements and properties that\ncan be modeled in this artificial vision system. Humans have two eyes, of course, and\ntogether they can see a collective range of 200° with 120° of overlap (binocular vision)\n[Wandell95]. The eye focuses light onto the back of the eye, which uses rod and cone\ncells to detect light and color. Visual acuity is greatest at the center of fixation and\ndecreases rapidly with distance from the center. Cones detect color and are densely\npacked toward the center of the retina, whereas rods are 100 times more sensitive to\nlight and are primarily responsible for night vision and peripheral vision. As a result,\nperipheral vision has very little color response but is extremely sensitive to movement. \nWith a little more science behind this model now, you can start to make several\nobservations. The first is that visual acuity and color detection is highest in the center\nof vision and falls off rapidly in the periphery. The second is that, while peripheral\nvision is poor, it is adept at detecting movement.\nUsing the discrete tests in the toolbox, a vision model can be created that scores\nobjects by which area they occupy in the range of vision. In Figure 3.2.4, the percent-\nages represent the certainty that a particular object is identified. Objects in the center\nof vision are fully identified, whereas objects in the near-peripheral, mid-peripheral,\nfar-peripheral, and the behind-the-head (sixth sense) areas have lower certainties. \nAn important feature of this model is that moving objects get a score increase of\n50%, which accounts for the special perception of movement. Depending on the game,\nwalking and running might trigger the 50% increase, whereas sneaking or crawling \ndoes not.\nIf camouflage or hiding (possibly in shadows) plays a significant role, identification\ncan be decreased depending on a combination of contrast and size of exposed profile.\nFor example, when players are crouched in a dark corner, their profiles are smaller and\n222\nSection 3\nAI \n",
      "content_length": 3291,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "they are difficult to see, which should consequently discount their identification by as\nmuch as 100%. Even if the agent is looking directly at the player, the agent might stare\nfor a few seconds and move on, because it can’t identify the object 100%. If this level of\nsubtlety is employed, it might be advisable to add a smaller ellipse in which identifica-\ntion is unconditionally 100%, regardless of profile or contrast.\n3.2\nDesigning a Realistic and Unified Agent-Sensing Model\n223\nFigure 3.2.4\nCertainty in vision as a collection of dis-\ncrete tests. In this model, objects are fully identified at\n100%, highly suspect at or above 80%, and slightly suspect\nat or above 50%. Moving objects get an extra 50% score\nincrease in order to model peripheral sensitivity to move-\nment. Camouflaged, hiding, or crouching (reduced pro-\nfile) objects decrease certainty by as much as 50%.\nThe percentages of certainty can be interpreted in whatever way makes sense within\nthe game design. One approach would be to put thresholds at which the agent would\nperform particular actions. For example, at 100% certainty the object in question is\nfully identified and the agent might shoot at the object. At 80% or higher, the agent\nmight turn their head and start approaching the object in question. At 50% or higher,\nthe agent might only turn their head. Anything below 50% might not be enough stim-\nulus to take any action.\nOne downside of the model in Figure 3.2.4 is that it still contains arbitrary dis-\ncrete zones. The model can be further refined to support gradual falloff with angle\nand distance. Figure 3.2.5 shows a vision model in which the inner circle falls off with\nthe angle and the outer circle combines a distance falloff with the angle falloff. The\nellipse remains a discrete test with 100% certainty in the forward direction and 80%\nbehind.\n",
      "content_length": 1848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "The arbitrary models depicted in Figures 3.2.4 and 3.2.5 are only examples and\nshould be modified as needed for the game design. They are very coarse approximations\nof real human vision based on the particular features identified here. Clearly, these mod-\nels take great liberties and approximate the science. For example, these models favor\n180° vision over 200° for simplicity reasons. However, these models are a big improve-\nment in terms of subtlety and sensitivity compared with typical game vision models.\nAnother important feature of this vision model is to take into account the mental\nalertness of the agent. When the agent is highly alert, the percentages should be\nincreased and the zones enlarged. If the agent is distracted or sleepy, the percentages\nshould be decreased and the zones reduced.\nModeling Human Hearing with Certainty\nAs demonstrated with the vision model, calculating sensory identification as a per-\ncentage can be an effective way to introduce subtlety into a sensing model. Similarly\nit’s worth constructing a hearing model that produces percentages of certainty. How-\never, before you dive in and create a hearing model, let’s look at some issues related to\nsound and hearing.\nThe most important property of sound is that intensity falls off exponentially\nwith distance. Although sound propagates easily through air, only lower tones travel\nwell through walls. This makes conversations and many high pitched tones hard to\nhear from adjacent rooms. Lastly, sound reflects off walls and reverberates within\nrooms, making sounds capable of getting around most obstacles.\n224\nSection 3\nAI \nFigure 3.2.5\nVision model with gradient zones of certainty. The inner\ncircle falls off with angle, whereas the outer zone falls off with angle and\ndistance. The ellipse test remains discrete. Note that black equals 100%\ncertainty and white equals 0% certainty.\n",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "When constructing a hearing model, a simple radius check for whether an agent\nhears a particular sound could be augmented in several ways. First, the volume of the\nsound should affect how far it travels, with clear recognition falling off to uncertain\nrecognition. Second, walls might cause some degree of uncertain recognition depending\non thickness and such. Third, because sound bounces off surfaces, if the line-of-sight\nbetween the source and listener is blocked, a path could be computed to see whether a\nclear route can be found. If a path is found, the distance of the path can be used to deter-\nmine the falloff. Alternatively, as a less processor intensive solution, zones could be used\nin which all sounds made within a particular zone or an adjacent zone can be heard\nregardless of walls between the source and listener. In some cases, the coarse search space\nused for hierarchical pathfinding can also be used for determining sound zones.\nFigure 3.2.6 demonstrates sound falloff coupled with the zone approach. Based on\nthe sound intensity, the sound will have a radius at which it is recognized at 100%.\nBeyond that radius the certainty drops off to zero after some distance. However, the\nsound is heard only if the listener is in the same or adjacent zone from the sound source.\n3.2\nDesigning a Realistic and Unified Agent-Sensing Model\n225\nFigure 3.2.6\nHearing model demonstrating sound inten-\nsity falloff coupled with zones. An agent can hear a sound\nonly if the sound was made in the same or an adjacent zone.\nIn this example, the sound does not propagate to Zone C\neven though the radius check would allow it.\nIf a zone approach requires too much preprocessing of the game world or isn’t\nsuitable for randomized maps, the pathfinding engine can be exploited to determine\nwhether a sound can propagate from the source to a listener in the case that the line-\nof-sight is blocked. Although this is more processor intensive, it can accurately tell if\n",
      "content_length": 1968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "the sound can travel unimpeded to the listener and an approximate distance that the\nsound traveled.\nIn the way that walls can block vision, other sounds can drown out a particular\nsound and make it hard to hear. In order to model this effect, you need to consider all\nsounds, including ambient sounds, and determine whether a louder sound might be\noverpowering and masking all other sounds. For example, if a train is rushing by\nwhen the player is running, their footsteps might not be heard. However, if the player\nshoots their gun, the gunshot sound might be reduced by the noise level of the train,\nmaking it much more difficult to hear. This kind of modeling opens up new gameplay\nopportunities because players are then encouraged to time noisy actions with other\nnoisy events. \nSimilar to how a sixth sense was added to the vision model, the hearing model\ncan also include other senses such as smell. For example, if a rotting corpse creates a\nsmell, that smell travels some distance and then falls off, just like sound. Additionally,\nstronger smells might overpower weaker smells, so consider modeling this feature as\nwell. Smell might not be interesting in every game, but it can be extra information\nthat can add to the identification of an object when vision isn’t sufficient, thus open-\ning up more gameplay opportunities.\nUnified Sensing Model\nHaving created sensing models for vision, hearing, smell, and a sixth sense, the final\ntask is to combine them into a single unified sensing model. The motivation is that all\nsenses should together inform the agents of their surroundings, combining their clues\ninto a complete picture of the current situation as best the agents can sense.\nBecause this example has been working with percentages representing certainty,\nthe natural extension is to combine them in some way. There are three options. The\nfirst is to take the maximum certainty between the vision, hearing, and smell senses,\nas shown in the left diagram in Figure 3.2.7. For example, if a vision zone has 30%\ncertainty and hearing is 50% certainty, that zone would have max(30%, 50%) = 50%\ncertainty. The second option is to add the certainty of all senses, as shown in the mid-\ndle diagram of Figure 3.2.7. For example, if a vision zone has 30% certainty and hear-\ning is 50%, that zone would have 30% + 50% = 80% certainty. The third option is to\ntake the vision model certainties and add half of the remaining headroom for hear-\ning/smell, as shown in the right diagram of Figure 3.2.7. For example, if a particular\nvision zone had 30% certainty, hearing a sound in that zone would add (100 – 30) /\n2 = 35% resulting in a total 75% certainty. This last option avoids having any zone\nwith greater than 100% certainty.\nTo understand the repercussions of this unified sensing model, consider the white\ncircle in Figure 3.2.7 to be the player. If the player was both quiet and motionless, he\nwould go completely undetected by the agent with a certainty of only 30%. If the\nplayer makes a loud noise, he is identified at 50%, 80%, or 75%, respectively, using\n226\nSection 3\nAI \n",
      "content_length": 3093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "each method. This might result in the agents turning their heads in response. If the\nobject was running and fired a loud shot, it would be identified at 80%, 100%, and\n90%, respectively, using each method. In this case, the agents might turn their heads\nand bodies quickly and shoot once they are facing the player.\nAdding Memory to the Unified Sensing Model\nTo achieve a greater degree of realism, agents must have short term memory to augment\ntheir sensing model. This is necessary in order for agents to not forget about objects that\nthey have recently identified. For example, if the player moves quickly through the mid-\nperipheral vision of the agent, the player will be identified at 100%. If the player out-\nruns the agent, moving into the far-peripheral vision area and stops, the memory of the\nplayer at 100% identification needs to be retained for some period of time (even though\nthe player should now technically be identified at only 30%). This makes sense, because\nthe agent identified the player and still has visual contact, making it reasonable to\nassume that it is the same object that is still fully identified.\nIn order to implement this type of memory, each object that enters the sensing\nmodel needs to be tracked. The object should have some unique identification num-\nber that can be associated with varying levels of identification. A timestamp and the\nlocation of the object’s last known position should also be recorded. This information\nwill be stored in the agent. The general rule is to allow only the certainty level to\nincrease, as clues only add to the knowledge of the object. Once the object is not\nsensed for several seconds, the structure can be purged from memory.\n3.2\nDesigning a Realistic and Unified Agent-Sensing Model\n227\nFigure 3.2.7\nThree examples of the unified sensing model combining vision with\nhearing/smell. Hearing has a max certainty of 50% with no falloff shown. The left diagram\ntakes the max sense (vision, hearing) from each zone. The middle diagram adds vision with\nhearing. The right diagram takes vision and adds half of the remaining overhead due to\nhearing (to avoid certainties above 100%). In this example, the white circle is the player\nmaking a loud sound, which results in 50%, 80%, and 75% certainty respectively in each\n",
      "content_length": 2290,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "Conclusion\nThe unified sensing model brings together vision, hearing, smell, and even a sixth\nsense to give game agents a coherent and detailed view of the game world. Many very\ncompelling features have been folded into the model, such as movement detection,\nhiding, and alertness, which allow for very rich and interesting gameplay to emerge.\nAs players better understand the underlying sensing model, they can devise innovative\nways to manipulate and deceive the agents, which adds greatly to the quality of the\nexperience.\nAs presented, the unified sensing model is intended to bring subtlety and added\nrealism to game agents. However, it is a very flexible model. The zones and percent-\nages given are simply suggestions and they will inevitably need to be tweaked for any\nparticular game. Consider each of the tools at your disposal and create your own sens-\ning model that matches and enhances your particular game design.\nReferences\n[Leonard03] Leonard, Tom. “GDC 2003: Building an AI Sensory System: Examining\nthe Design of Thief: The Dark Project,” available online at http://www.gamasutra.\ncom//gdc2003/features/20030307/leonard_01.htm#, 2003.\n[Orkin05] Orkin, Jeff. “Agent Architecture Consideration for Real-Time Planning in\nGames,” AIIDE Proceedings, Artificial Intelligence for Interactive Digital Enter-\ntainment Conference, 2005.\n[Rabin05] Rabin, Steve. Introduction to Game Development, Charles River Media,\n2005.\n[Tozour02] Tozour, Paul. “First-Person Shooter AI Architecture,” AI Game Program-\nming Wisdom, edited by Steve Rabin, Charles River Media, 2002, pp. 387–396.\n[Wandell95] Wandell, Brian. Foundations of Vision, Sinauer Associates, 1995.\n228\nSection 3\nAI \n",
      "content_length": 1684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "229\n3.3\nManaging AI Algorithmic\nComplexity: Generic\nProgramming Approach\nIskander Umarov\nAnatoli Beliaev\nD\nuring the past seven years, TruSoft International Inc. has been focusing on the\nresearch and development of behavior-capture AI technologies. These technolo-\ngies allow a new type of AI game agent to be created that can learn and adapt in the\nway real humans do. They do this by learning the playing styles of human players and\nadapting these strategies to achieve set goals.\nThe system allows game designers to train behavior-capture AI agents directly, by\nsitting down with a console or a PC and playing the role of the agent to be trained.\nAgents can then learn tactics and strategies straight from the human controller, with-\nout the need for coding. End users can also train game characters using the same sys-\ntem, bringing traditional bot development to a whole new level. By simply playing\nthe game, a behavior-capture enabled system allows the users to create AI-controlled\nagents that play with very distinct styles.\nIntroduction\nDuring the work on our behavior-capture AI technology—Artificial Contender—we\nencountered an interesting challenge common to many AI systems. Sometimes the\ncomplexity of AI decision-making related algorithms grows out of control. They start\nas a simple piece of code and end up as chaos in the form of handcrafted loops and\nbranches. We needed a method of managing this complexity without introducing sig-\nnificant abstraction penalties.\nThis Artificial Contender technology is an instance-based learning system. It col-\nlects instances of learned behaviors and utilizes them during the decision-making\nprocess. The data collected while learning may not be applicable directly to the cur-\nrent situation. The learned instances are reevaluated. Possible actions can be filtered\nout or modified, generalized or specialized, and the priorities can be adjusted. An\naction should be chosen from a group of actions, and, if the action is not good\n",
      "content_length": 1988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "enough, another group of actions should be analyzed. You might need to apply differ-\nent algorithms for the next group, or different filtering criteria. However, if the next\ngroup does not contain better actions, you might need to reconsider actions from the\nprevious group, compare the consistency of data from each group, and so on. Algo-\nrithms of this level of complexity are quite typical.\nHow might you go about solving this problem? Let’s consider the most straightfor-\nward implementation first. No over-design, no premature optimization. When you have\nto deal with a group of actions, you just create a container of objects describing actions.\nWhen you have to iterate through actions, you implement a loop. When you have to\niterate through a subset of actions, you implement a nested loop. When you have to fil-\nter actions, you check the necessary conditions inside the loop. Trying to implement it\nthis way, we ran into problems:\n• Each part of the algorithm increases the complexity of the implementation. The\ncode becomes difficult to follow. The approach that appeared simplest and the\nmost straightforward leads to very complex code.\n• Maintaining the code in this form becomes very expensive. It is difficult to\nunderstand and change. Debugging it is very challenging.\n• It also becomes increasingly difficult to optimize the performance of this algo-\nrithm. It is difficult to find performance bottlenecks. It is difficult to change the\ncode and make sure it is still correct. It is difficult to create customized versions of\nthe code, optimized for different environments and conditions.\n• The pieces of code are tightly coupled, which makes it impossible to reuse them.\n• The risk of introducing bugs while making changes is high and unit testing does\nnot eliminate the risk, because often it becomes difficult to determine the\nexpected results for the entire algorithm.\nHow do you manage this complexity? The most obvious answer is to use decompo-\nsition. There are different ways to decompose. We wanted to make the implementation\neasy to understand, modify, and reuse. We wanted to be able to build these algorithms\nquickly, and make fast and safe changes. On the other hand, we could not sacrifice the\ntechnology’s performance characteristics. When you decompose a system into compo-\nnents, you have to deal with inter-component communication issues—sending data\nback and forward, converting data, and so on. Artificial Contender processes a lot of\ndata, and introducing even a little overhead to processing every single data item can lead\nto unacceptable processing time and resource consumption increases.\nAction Choosing Workflow\n“Pipes and Filters” Design Pattern\nWe have found that workflow is the most appropriate metaphor for representing this\nclass of algorithms. The idea is based on the well-known “Pipes and Filters” design\n230\nSection 3\nAI \n",
      "content_length": 2877,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "pattern [Buschmann96]. Consider the reasons that make this design pattern a good\nfit, as follows. (Note that we use the term “block” instead of the term “filter” from the\noriginal “Pipes and Filters” pattern, in order to avoid ambiguity—filter in our work-\nflow metaphor is a block of a special type.)\n• We want to construct workflows out of separate blocks.\n• Each block should be responsible for exactly one aspect of the algorithm.\n• Each block should have well-defined inputs and outputs.\n• The structure of the workflow should be homogeneous, so that we can connect\nblocks in different ways, unless the nature of the implemented aspects does not\nallow the blocks to be connected.\n• We want to be able to create non-linear workflow configurations, containing\nbranches, merges, and loops.\n• We want to be able to change workflow configurations with minimum effort.\nThe advantages of the “Pipes and Filters” pattern are well known [Buschmann96]:\nflexibility of the processing line configuration, reusability of the components, potential\nparallel processing, and so on. Let’s see how this pattern can help in this case, setting\naside the implementation issues for now, but remembering the priorities here—manage\ncomplexity and do not sacrifice performance.\nThe following benefits help to manage complexity:\n• Separation of concerns. The processing algorithm is broken into a sequence of indi-\nvidual transformations. Every transformation is a distinct and independent task.\n• Modularity. Every processing task is encapsulated by a separate block, which can\nbe coded independently.\n• Reduced coupling. Blocks communicate only through well-defined channels. Nor-\nmally, blocks do not share state and are unaware of surrounding blocks’ implemen-\ntations, as long as the surrounding blocks adhere to the common requirements.\n• Testability. Every block can be tested independently. It is much easier to specify the\nrequired results for a separate simple task than for the entire algorithm. If it is easy\nto change inputs and check outputs, each block can be treated as a black box. It is\nalso possible to test the result of cooperation of any combination of the blocks.\n• Configuration flexibility. It is possible to build different configurations out of the\nsame set of blocks. It is also possible to compose simpler blocks into aggregates\nthat can be, in turn, treated as more complex blocks.\n• Specialization flexibility. Blocks can have alternative replaceable implementations,\nspecialized for different environments.\n• Reusability. Low coupling allows you to treat blocks as standalone modules that\ncan be easily reused. Avoiding extra dependencies makes blocks more adaptable.\nThe following benefits help to improve performance:\n• Parallelism. Blocks performing incremental processing do not have to wait until\nthe surrounding blocks complete their calculations; they can continue working\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n231\n",
      "content_length": 2964,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "concurrently. This makes the workflow significantly faster, because it takes advan-\ntage of multiprocessor architectures.\n• Specialization flexibility. You can create alternative implementations of blocks\nspecifically for the purposes of performance improvement.\n• Performance profiling. Breaking the processing down into separate components\nmakes it easier to profile the code and find performance bottlenecks.\n• Partial results. This aspect is very important for the Artificial Contender decision-\nmaking algorithms, and is discussed it in more detail in later sections.\nPartial Results\nArtificial Contender decision-making algorithms operate on sequences of data describing\npotential actions. The lowest blocks (sources) generate action data sequences, usually\nextracting data stored in the knowledge database, or based on statistics, or suggested \nby heuristic rules. Querying some sources is expensive in terms of processing time. If\nthe current game situation is well known, only the data from the “cheapest” sources is\nnecessary. Only if there is no exact match for the current game situation, is it necessary to\nquery more sophisticated and expensive sources.\nIn most cases the complete action data sequences are not really needed. Partially\ncalculated sequences may be enough to make the final decision, and expensive calcu-\nlation can be avoided. If an action is good enough, it can be accepted and calculations\ncan be stopped. If you calculate everything in advance, it is very probable that you’ll\nhave to throw away most of the calculated results anyway, and you cannot afford that.\nIt is possible to implement the workflow in a way that the higher blocks control the\nexecution flow. It is up to higher blocks to decide how, when, and for how long they\nwant to continue getting the results from lower blocks. They can interrupt querying\nlower blocks at any moment, or even not start querying some of the lower blocks. If\npossible, the lower blocks should not pre-calculate the results until they are asked.\nThere is one more consideration. Working in real-time environments, sometimes\nit is better to make a decision that is not perfect, but acceptable, than to spend more\ntime calculating. It is possible to arrange the blocks of the workflow in such a way that\nthey calculate and defer the “acceptable but not perfect” actions first, and then con-\ntinue calculations while it is not too late to act, and to accept one of the deferred\nactions if nothing better has been found.\n“Pipes and Filters” Liabilities\nHowever, the “Pipes and Filters” pattern is not free from liabilities. Let’s take a look at\nthem and what can be done to minimize them.\n• Sharing state information. If the blocks need to share state information, it can be\ninefficient or inflexible. It does not seem to be a serious issue for this application,\nbecause most of the blocks do not need to share any state information directly. In\nsome exceptional cases, you can relax the “Pipes and Filters” restriction and allow\n232\nSection 3\nAI \n",
      "content_length": 3018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "blocks to communicate in special efficient ways, bypassing the “official” block\ninputs and outputs. Those exceptions should be very rare, though.\n• Communication and data transformation overhead. Blocks have to exchange data,\nand this can incur some overhead. Data manipulations that do not contribute\ndirectly to the implemented algorithm may be required. If the blocks are inter-\nchangeable, they have to agree upon a common communication protocol, some-\ntimes as low as a character stream. Serializing and parsing can be expensive\nenough to make the pattern not applicable. We are going to address this issue and\nminimize or completely eliminate this overhead.\n• Parallel processing disappointments. For different reasons, the performance of par-\nallel processing can be disappointing [Buschmann96]:\n• It can happen because of the communication overhead, but we have already\npromised to minimize it.\n• It can also happen because of architecture-dependent reasons, such as context-\nswitching and synchronizing overhead, and we are not going to consider these\nissues for now, because they seem to be common for all parallel-processing\nsolutions.\n• It can happen because of the nature of the processing, or because of bad cod-\ning, like when a block consumes all input data before emitting any output data.\nThis block can become a performance bottleneck of the entire workflow. In\norder to avoid this problem, you should make processing incremental when-\never possible.\n• Complex flow control logic. The workflow processing nature is mostly sequential,\nso it becomes difficult to implement branches, loops, and other complex con-\nstructs. However, in this application, we were almost always able to rethink and\nredefine algorithms in terms of sequential processing. These new definitions are\nvery beneficial themselves. When a complex task that seems to require complex\ncontrol of the execution flow is transformed to a sequence of simpler steps, it def-\ninitely improves the internal quality of the implementation. In rare cases when\nyou are unable to do it, special constructs outside of the traditional “Pipes and\nFilters” patterns can be used.\n• Error handling. In general, error handling can be quite complicated in the “Pipes\nand Filters” pattern. However, the nature of Artificial Contender decision-making\nalgorithms does not involve any recovery after errors. So, the whole error-handling\nissue is not important for this application.\n• Complexity, increased maintainability efforts. The pattern introduces its own complex-\nity and maintainability efforts, because decomposing a monolithic implementation\ninto multiple components increases the number of components and dependencies.\nThis problem is not specific to this pattern, it is rather a consequence of any decom-\nposition. Our solution is as simple and lightweight as possible.\nLet’s take a closer look at the Artificial Contender decision-making workflows\nand their specific requirements.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n233\n",
      "content_length": 3028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "Workflow Diagram\nAny workflow can have a visual representation, for instance a block diagram. We devel-\noped a special graphical language that allows you to express different workflow configu-\nrations in a very compact and vivid form. Using different shapes and connecting lines,\nyou can illustrate sequences of processing steps, data flows, and interdependencies.\nFigure 3.3.1 shows an example of a workflow of average complexity. The descrip-\ntion of this workflow in English would be cumbersome and perplexing. However, for\ndevelopers familiar with these diagrams, it is quite easy to understand what is going on.\n234\nSection 3\nAI \nFigure 3.3.1\nArtificial Contender decision-making workflow example.\n",
      "content_length": 703,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "These diagrams are compact and readable. They also make it very easy to modify\nworkflows. You can swap the blocks around, rearrange and reconnect, add and remove.\nFor example, you may want to modify actions before or after filtering. Take a look at\nthe diagram, and you will know what the current workflow does. If you want to change\nthe order, just reconnect the blocks. Compare this to the first straightforward imple-\nmentation of the algorithm. How long would it take to change the order of action\nfiltering and modification? How can you make sure that the change is correct? The\nworkflow diagrams make the answers obvious.\nExecution Flow\nA workflow diagram shows just a static picture of the workflow. It represents the work-\nflow configuration in a declarative manner, but it does not illustrate the actual execution\nsequence. It is enough when the reader of the diagram knows the basic rules. Omitting\nthe details of data and execution flows is what makes the diagrams compact and expres-\nsive. But what is going on here?\nBlocks work with sequences of separate objects representing some knowledge\nabout a single action or a set of actions. These objects are called ActionInfo. Blocks\nconsume, process, and emit sequences of ActionInfo objects. ActionInfo objects travel\nfrom lower blocks to higher blocks. On their way, they can be transformed to other\nActionInfo objects, they can be filtered out, they can be split into sets of separate\nActionInfo objects, they can be merged with other objects, and so on.\nHow and in what order do blocks process ActionInfo objects? To answer this\nquestion, consider a very simple workflow shown in Figure 3.3.2.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n235\nFigure 3.3.2\nVery simple workflow.\nThis is how this workflow is supposed to work:\n• Source generates ActionInfo objects (based on, for example, the Artificial Con-\ntender knowledge base).\n• Modifier changes ActionInfo objects, adjusting them to the current game situation.\n",
      "content_length": 2002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "• Filter checks whether ActionInfo objects are good enough and lets them through\nor filters them out.\n• Acceptor accepts the first ActionInfo that is emitted by Filter.\nYou could make it work in exactly that sequence, implementing the “push”\nmodel—start processing from Source, pass the data generated by Source to Modifier,\nand so on. This is how many implementations of the “Pipes and Filters” pattern work.\nHowever, this is not what you need for the Artificial Contender system. Querying\nsome of the knowledge sources can be expensive in terms of performance.\nFirst of all, it might not be necessary to query all of them. Second, you might not\nneed the whole sequence of ActionInfo from each of them. The workflow in this\nexample may accept the first ActionInfo generated by Source if it makes it through\nFilter; in that case there is no need to generate more than one ActionInfo. But Source\nis not supposed to be aware of that fact, so you cannot let Source decide when to\ngenerate the whole ActionInfo sequence. Higher blocks must be able to decide which\nlower blocks to query and when to stop. Although it is theoretically possible to imple-\nment this using the “push” model, the “pull” model looks more natural:\n• Acceptor asks for one ActionInfo from Filter.\n• Filter asks for ActionInfo from Modifier and checks them one by one, looking for\nActionInfo that should be returned to Acceptor.\n• Modifier queries Source retrieving ActionInfo one by one, modifies them, and\nreturns to Filter in the same manner: one by one.\n• Source answers Modifier’s requests, emitting ActionInfo objects one by one.\n• As soon as the top block (Acceptor) stops asking for more ActionInfo, the work-\nflow stops.\nFigure 3.3.3 shows the sequence diagram of a “pull” workflow.\nTypical Blocks\nThere are a few categories of blocks that you usually need for AC decision-making\nalgorithms:\n• Sources generate ActionInfo objects based on data external relative to the decision-\nmaking workflow: from AC knowledge database, from heuristic algorithms, from\nstatistics tables, and so on.\n• Filters determine whether consumed ActionInfo satisfy specific conditions, and\noutput or ignore these ActionInfo. Usually filters make sure that the potential\nactions are applicable to the current game situation.\n• Modifiers change consumed ActionInfo objects and output the changed objects.\nFor example, they can adjust the actions retrieved from the knowledge according\nto the current game situation, or they can adjust priorities of the actions.\n236\nSection 3\nAI \n",
      "content_length": 2533,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "• Sorters consume sequences of ActionInfo objects and output the same objects in a\nnew order. For example, they can sort actions by priorities, by estimated effect, by\ncategories, and so on. Sorting criteria can be very flexible and do not have to spec-\nify a deterministic order. They can perform weighted random reordering, thus\nintroducing more variety into AC agent’s behavior.\n• Splitters divide the incoming flow of ActionInfo objects into multiple flows and\nredirect these flows to multiple outputs. Splitters are used to separate some\nActionInfo objects for special processing.\n• Mergers have multiple inputs and redirect the flows of ActionInfo objects from all\ninputs to a single output. They are often used to query a set of ActionInfo sources\nsequentially and join the results into one set.\n• Selectors have multiple inputs and redirect the flow of ActionInfo objects from\none of the inputs to a single output. Usually, selectors have a special control chan-\nnel that makes it possible to switch the active input. They are also often used to\nquery a set of ActionInfo sources sequentially. But, as distinct from mergers, they\nallow treating the ActionInfo objects from each source as a separate group.\n• Repeaters consume and output all available ActionInfo objects, and then perform\na specified action, and then consume and output all available ActionInfo objects\nagain, and so on. Cooperating with selectors, repeaters make it possible to imple-\nment loops that are querying and processing actions from different sources.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n237\nFigure 3.3.3\nPull workflow.\n",
      "content_length": 1637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "Generic implementations of the frequently used blocks are included in the Artifi-\ncial Contender SDK. However, this is not an exhaustive list of block types. It is possi-\nble to develop more customized and specialized blocks. Combining these blocks into\ndifferent configurations, you can build very versatile and flexible workflows.\nConstraints\nBlocks can be connected in many different ways. However, system freedom is not\nunlimited. Some combinations do not make sense, because the nature of the blocks\ncan be very different. For example, if you want a block to perform a weighted random\nchoice of an action, you have to make sure that the inputted ActionInfo objects have\nweights associated with them. You want to be able to express these constraints and\nassociate them with blocks. Then you can visualize the restrictions and check them\nautomatically, ensuring the correctness of the workflow.\nImplementation\nGeneric Programming and C++\nWe chose C++ as the main implementation language for Artificial Contender. This\nlanguage provides tools to deal with abstractions, and still allows control over low-\nlevel implementation details in order to achieve high performance. Using C++, we\ntake advantage of the “Pipes and Filters” pattern benefits, and overcome the potential\nperformance hits at the same time.\nGeneric programming helps us achieve both goals simultaneously. Implementing\ncode in a very general and abstract form without sacrificing efficiency is one of the key\nideas of generic programming [JLMS98].\nIn order to make the workflow flexible enough, we apply generic programming\nprinciples while designing workflow blocks. Block implementation should make min-\nimal assumptions about the surrounding environment. The less it relies on implemen-\ntation details, such as concrete data types, the more adaptable the implementation is.\nBlocks that have well-defined orthogonal responsibilities should be aware only of the\ndetails directly related to those responsibilities, and in the most generic way. The main\nrule while designing a block is no over-specification. If the essential functionality of the\nblock does not depend on a particular data type, do not even mention this data type\nin the implementation. If it depends on a data type, but still is able work with differ-\nent types, make this data type a parameter. This lack of concrete details can make the\nimplementation look a little bit vague, but in fact it is exactly the opposite—it\nbecomes succinct and precise.\nPolymorphic Workflow Blocks\nThe most common requirements to workflow blocks is that they should process and\noutput data. If necessary, blocks can also consume data generated by other blocks. At\nthe same time, it must be possible to connect blocks in different ways.\n238\nSection 3\nAI \n",
      "content_length": 2770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "The Dependency Inversion Principle [Martin02] states that blocks should not\ndepend directly on each other. Instead, they should depend on abstract requirements\nto input and output. In this case, changing one block does not require changing other\nblocks, as long as all the blocks satisfy the requirements. How abstract are the require-\nments? More abstraction gives more flexibility, but makes it more difficult to ensure\nthat the developers of the blocks have enough information to really satisfy the require-\nments in the concrete implementation. You have to balance these issues, considering\nthe requirements and the tools’ limitations.\nAlthough the nature of blocks can be absolutely different, they still have a common\nproperty: the ability to input and output data items. If you implement this property\nsimilarly in all blocks, it will give you an opportunity to build different configurations\nfrom the same blocks. You can then connect and reconnect blocks without taking care\nof different input/output interfaces.\nBecause we’re implementing the “pull” model, blocks should only provide a com-\nmon way of getting data. If the blocks know how other blocks output data, they auto-\nmatically have a way to input data. We’ll use some form of polymorphism in order to\nmake this process look unified.\nHow do you make the blocks polymorphic? Developers with object-oriented\nbackground might already have an answer—unified interfaces based on virtual func-\ntions or another form of dynamic binding.\nA block that requires input can hold a reference to an object implementing the\nBase interface. The block does not need to know the concrete type of other blocks; it\ncan just rely on the interface. See the following code:\nclass BaseBlock {\npublic:\nvirtual OutputData getData() = 0;\n};\nclass Modifier : public BaseBlock {\npublic:\nModifier(BaseBlock& input) : input_(input) { }\nvirtual OutputData getData() { return modify(input_.getData()); }\nprivate:\nBaseBlock& input_;\n};\nIt looks flexible enough, right? But it will not do for this implementation. Why\nnot? The performance of this system is not high enough.\nPolymorphism based on virtual functions can introduce significant performance\nhits. If you follow the Single Responsibility Principle [Martin02] and make the blocks\nfine-grained, you will end up having a lot of functions with simple or even trivial\nimplementation.\nSometimes you can ignore the overhead of indirect function calls, but you defi-\nnitely cannot ignore the fact that these indirect calls create optimization bottlenecks\nfor compilers. Usually, compilers can optimize a sequence of static function calls. If\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n239\n",
      "content_length": 2700,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "the function definition is visible, it is possible to inline it, eliminate the overhead of\npassing parameters and returning results, and generate very compact and efficient\nexecutable code. However, if the compiler does not know which function implemen-\ntation is going to be called, inlining is not an option anymore, and all this overhead is\nnecessary. \nIn the previous code example, how would you define the OutputData type? You\nhave a similar challenge here. The output data objects should be either fixed or poly-\nmorphic, because the blocks should process data objects that are acquired from other\nblocks. However, the problem looks even more severe in this case. Every block can\nexpect different properties from input data objects. There are almost no common\nrequirements. Most of the requirements are block specific, and do not make any sense\nin the context of other blocks. If you fix the data type, the type will have to imple-\nment all imaginable function and data member placeholders. Only some of them will\nbe really used, and (even more scary) some of them must not be used until initialized\nproperly. Instead, you could try to extract the interface, covering everything possible,\nand then build the inheritance hierarchy and override virtual functions, providing\nstub implementations of methods that are not “legal” for particular subtypes. How-\never, it provides perfect opportunities to violate the Liskov Substitution Principle and\nsuffer from the Refused Bequest code smell. And, even if you manage to implement it\nthis way, you’ll run into the performance issues described previously.\nWhy not use other forms of polymorphism? You could use the unbounded\ndynamic polymorphism, similar to the one available in dynamically typed languages,\nsuch as Smalltalk and Ruby. These approaches require additional work for C++ devel-\nopers, but are definitely possible. You could even introduce reflection and runtime\nmeta-programming capabilities, analyze block requirements dynamically, and build\nappropriate objects in runtime. \nThe main obstacle is the same: performance. Although they are incredibly flexible,\nall kinds of dynamic polymorphism, both bounded and unbounded [Czarnecki00], do\nnot seem to be applicable to this problem, mostly because of performance overhead.\nThe more fine-grained the blocks are, the more visible the performance hit becomes. \nAlso, it would make the workflows too flexible. You would not be able to rely on\nstatic type checking anymore, and you would have to execute the workflow just to\ndetect obvious constraint violations. This would make designing workflows over-\ncomplicated, which defeats the purpose of the solution. Reflection and runtime meta-\nprogramming would make the performance problems even worse.\nAll types of dynamic polymorphism provide you with the ability to substitute\nobjects of different types at runtime, but they make you pay for this ability with per-\nformance. You do not want to pay for flexibility that you are not going to use and nor-\nmally you do not need to change the configuration of the workflow at runtime. You\nneed as much flexibility as possible while the workflow is designed, but you do not\nneed to change it after the code is compiled.\n240\nSection 3\nAI \n",
      "content_length": 3240,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "Static polymorphism can help you move as much work as possible from runtime to\ncompile-time. This is why we choose the generic programming approach based on\nC++ templates. In this scenario, you still can construct workflows out of fine-grained\nblocks, but you do not have to pay for that with processing time or memory. Also,\ncompile-time type safety is intact. Let’s use this idea to implement the Modifier block\nagain, as follows:\ntemplate <typename Input>\nclass Modifier {\npublic:\nModifier(Input& input) : input_(input) { }\nOutputData getData() { return modify(input_.getData()); }\nprivate:\nInput& input_;\n};\nNo inheritance, no virtual functions, but still polymorphic. The “implement\nBaseBlock interface” requirement is replaced with a fuzzy, but much more flexible\none: “implement getData function returning an object that behaves like OutputData.”\nImplementing static polymorphism, you are not losing the opportunity to return\nto dynamic polymorphism when it is more appropriate (for example, if it is necessary\nto change the workflow configuration at runtime). You do not have to change the\nblock implementation, you just need a simple adapter that converts the statically\npolymorphic interface to a similar dynamically polymorphic interface, based on vir-\ntual functions or message dispatching. See the next example:\ntemplate <typename AdaptedBlock>\nclass Adapter : public BaseBlock {\npublic:\nAdapter(AdaptedBlock& adapted) : adapted_(adapted) { }\nvirtual OutputData getData() { return adapted_.getData(); }\nprivate:\nAdaptedBlock& adapted_;\n};\nModifier modifier;\nAdapter<Modifier> adaptedModifier(modifier);\nSimilar adapters can be implemented for unbound dynamic polymorphism, too.\nObviously, the performance issues come back when these adapters are used. But now\nyou have a choice; you can use it only when necessary.\nActionInfo Flow\nWhat does the output data look like? You need some degree of polymorphic behavior\nfrom ActionInfo objects. However, the performance considerations should steer you\ntoward avoiding the involved overhead. The ActionInfo implementation details are\ndiscussed later. For now, assume that you have already defined the ActionInfo type\nthat is generic enough to satisfy the requirements of every block in the workflow.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n241\n",
      "content_length": 2328,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "Most of the time you’ll work with sequences of ActionInfo objects. How should\nyou store the sequences? How do you pass them between blocks? Should you pre-\nallocate memory before querying a block? Should the called block allocate memory\nitself? Acquiring the whole sequence might be expensive; should you do that if you\nmight end up choosing the first action anyway?\nFortunately, most of the time you don’t really need the whole sequence. At least,\nnot at the same time. In most cases, you can process ActionInfo objects one by one,\nand make appropriate decisions regarding these objects separately. Instead of deciding\nhow to store and pass the data that you might not even need, you can pass functions\ninstead of requesting data. We applied the “tell, don’t ask” approach. Instead of asking\nfor all data, you tell the block what to do with the data and when to stop.\nReplace the block-interface requirements with the following:\n• The block should implement a function with a fixed name (called forEach).\n• The forEach function should accept a function as a parameter.\n• The forEach function should apply the received function to every ActionInfo\ninstance that should be emitted by the block.\nThis implies that the function passed to forEach should be able to accept Action-\nInfo objects as a parameter. Note that we don’t specify any types in the requirements,\nso the blocks are free to implement the forEach function and the callback function in\ndifferent ways.\nBlock Implementation Examples\nThis next listing shows what source blocks (blocks that do not require input) look like:\nclass Source {\npublic:\ntemplate <typename F>\nvoid forEach(F f) const {\n...\nf( generateNext() );\n...\n}\n};\nYou want the forEach function to be able to accept any invokable entity, includ-\ning functions and function objects (functors); this is why F is a template parameter.\nNote that the caller of the forEach function does not have to worry about allocat-\ning storage for new ActionInfo objects. The Source block manages this storage and\ncan reuse it for every next ActionInfo. Normally, blocks do not have to store previous\nActionInfo objects, unless it is required by the nature of the block (for example, sort-\ning blocks usually must collect all input ActionInfo before emitting the first output\nActionInfo). This helps minimize the data transfer overhead.\n242\nSection 3\nAI \n",
      "content_length": 2362,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "If all the blocks satisfy these requirements, the blocks that require input can\nexpect that the other blocks implement a similar forEach function. The following\ncode listing shows a typical modifier’s forEach function implementation:\ntemplate <typename F>\nvoid forEach(F f) const {\ninput_.forEach(ApplyToModified<F>(f));\n}\nThe input_.forEach call ensures that ActionInfo objects are retrieved from the\ninput block. The ApplyToModified functor’s purpose is to modify each ActionInfo\nobject received from input_ and apply the original F function to the modified Action-\nInfo, as follows:\ntemplate <typename F>\nclass ApplyToModified {\npublic:\nApplyToModified(F f) : f_(f) { }\nvoid operator()(const ActionInfo& ai) const { f_(modify(ai)); }\nprivate:\nF f_;\n};\nThe following code listing shows a typical filter implementation:\ntemplate <typename F>\nbool forEach(F f) const {\nreturn _input.each(ApplyIfAcceptable<F>(f));\n}\ntemplate <typename F>\nclass ApplyIfAcceptable {\npublic:\nexplicit ApplyIfAcceptable (F f) : f_(f) { }\nvoid operator()(const ActionInfo& ai) const {\nif (isAcceptable(ai)) f_(ai);\n}\nprivate:\nF f_;\n};\nNote that there is no physical copying of data here. The ActionInfo objects created\nby the input block are checked in place, and the original function may be applied. Of\ncourse, you do not know anything about the original function, and it might copy or\nconvert data for its own purposes. But there is no copying or conversions for the Filter\nblock purposes, which means that the performance overhead is completely eliminated.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n243\n",
      "content_length": 1612,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "Partial Results\nOne of the most important requirements is the ability to interrupt the workflow\nwhen an acceptable ActionInfo is found, or when there is not enough time to com-\nplete the entire decision-making process. It is easy to implement: just make the func-\ntion passed to forEach return a Boolean value, indicating whether the block is allowed\nto continue emitting ActionInfo objects or not. Each time the function is called, the\nforEach implementation should check the result and exit as soon as possible when\nnecessary. It will effectively stop the whole workflow.\nThe sequence diagram shown in Figure 3.3.4 illustrates the execution flow.\n244\nSection 3\nAI \nFigure 3.3.4\nPull workflow with\ncallback functions.\nFunction Pointers versus Functors\nYou could pass function pointers to forEach functions. However, unlike calls through\nfunction pointers, calls to functors (function objects) can be inlined, efficiently elim-\ninating most or all of the function call overhead [Meyers01]. Furthermore, empty or\n",
      "content_length": 1012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "trivial implementations may be optimized away altogether. This is a very important\nway to get an abstraction bonus instead of an abstraction penalty. Imagine the imple-\nmentation of the previous workflow where all the called code is implemented “in\nplace,” even for blocks that are located very far from each other in the workflow, so\nthat there is no more need to really call functions and pass parameters. When inlined\nproperly, the whole algorithm in the resulting executable can be merged into one\nhighly optimized chunk of code that implements the necessary data processing only,\nwithout moving and converting data. In the meantime, the developers still deal with\na very high-level, abstract, and decomposed representation of this algorithm.\nThis approach has a caveat, though. Depending on your compiler, the results of\ninlining may vary.\n• First of all, the compiler might not use inlining opportunities fully and might still\nmake real function calls. You may need to experiment, measure, and tweak your\ncode and compiler settings in order to achieve the expected results.\n• Secondly, uncontrolled inlining may lead to code bloat when large functions are\nduplicated. In that case, you can always return to function pointers.\nActionInfo Type\nWe keep mentioning the ActionInfo type, but we still have not shown you its definition.\nThere is a reason: the generic ActionInfo type simply does not exist. Each block has its\nown requirements to the incoming ActionInfo flow, and each block can emit ActionInfo\nobjects having specific properties. Combining all properties in one ActionInfo type is\ninefficient and unsafe. You need to be able to define minimalistic ActionInfo types in\nthe lowest blocks (sources), and add or remove properties moving up the workflow, in\ncompile-time. How can you achieve this? Here is what we did:\n• Only sources define concrete ActionInfo types. These types do not have to be the\nsame. Each Source includes only members relevant to this source.\n• All other blocks make ActionInfo a template parameter. Then, they derive their\noutput ActionInfo type from input ActionInfo types, using inheritance or aggre-\ngation to modify ActionInfo properties. As a result, each block’s output Action-\nInfo type depends on the workflow configuration.\n• All blocks use ActionInfo properties that are directly related to a block’s responsi-\nbility and do not use any other properties. This makes the blocks very adaptable:\nthey accept different input ActionInfo types, but any unknown properties are just\npropagated to the output and can be used by higher blocks.\nAlternative Block Implementations\nAnother key idea of the generic programming idea is that you can provide alternative\nimplementations of the same generic algorithm, specialized for some particular condi-\ntions, in order to make it more efficient when possible. This approach is extremely help-\nful for our application. For example, it allows us to have different versions of the same\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n245\n",
      "content_length": 3038,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "block, optimized for different platforms. These versions can be chosen either manually\nor automatically in compile-time. Also, policy-based design [Alexandrescu01] helps to\ncustomize generic block implementations partially, without re-implementing entire\nblocks and duplicating code.\nConstructing Workflows\nHow do you connect the developed blocks to each other and make the workflow run?\nIn C++, creating and connecting blocks looks as simple as the following listing:\ntemplate <typename Input>\nFilter<Input> makeFilter(const Input& input) {\nreturn Filter<Input>(input);\n}\n...\nmakeFilter( makeModifier( makeSource() ) ).forEach(AcceptFirst());\nHowever, you do not have to always do it manually. Because all blocks follow the\nsame rules, the code has a very regular structure, which makes it possible and relatively\neasy to generate it automatically from scripts or even from the visual representation.\nConstraints\nThanks to static polymorphism, you still can take advantage of C++ compile-time type\nchecking. If, for example, the Filter tries to use members of ActionInfo objects received\nfrom the Modifier, and these members do not exist or have different types, this code can-\nnot be compiled. In that case, the Filter cannot consume data directly from Modifier’s\noutput.\nSometimes it isn’t enough, though. Compiler error messages can be unreadable or\nmisleading, especially for code that makes heavy use of templates. This makes it diffi-\ncult to understand which requirement has been broken. In order to simplify the diag-\nnostics, we use C++ concept checking [Stroustrup03]. Also, we could have used the\n“Red Code, Green Code” approach suggested by [Meyers07].\nThe constraints can also be visualized on the workflow diagram:\n• A set of labels is attached to block inputs. Every label represents a requirement to\nthe incoming ActionInfo flow.\n• Another set of labels is attached to block outputs. Every label represents a prop-\nerty that is added by the block, or a property that is removed by the block.\n• When blocks are about to be connected, first of all the label should be analyzed.\nThe requirements of the higher block should be satisfied by the output of the\nlower block. If they do not contradict, the connection is established. After that, the\nlabels of the lower block’s output can be propagated to the higher block’s output.\n246\nSection 3\nAI \n",
      "content_length": 2359,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "This visual representation makes the process of building workflows quite intuitive\nand straightforward.\nConclusion\nAlthough generic programming is definitely not a new idea, it still takes some non-\ntrivial efforts to grasp and apply it properly. In addition, prepare for a struggle with\nC++ compilers and other development tools. Even when they conform to the contem-\nporary C++ standard, efficiency of C++ templates support leaves a lot to be desired.\nFortunately, compilers and tools are being improved, and the upcoming new C++\nstandard is going to facilitate generic programming and template meta-programming.\nAnd the result is worth it, especially if you need to write abstract and reusable\ncode, but have very strict performance requirements. The approach described in this\ngem makes Artificial Contender very flexible, compact, and fast, all at the same time.\nReferences\n[Alexandrescu01] Alexandrescu, A. Modern C++ Design: Generic Programming and\nDesign Patterns Applied, Addison-Wesley Professional, 2001.\n[Buschmann96] Buschmann, F., Meunier, R., Rohnert, H., Sommerlad, P., and Stal,\nM. Pattern-Oriented Software Architecture Volume 1: A System of Patterns, Wiley\nand Sons Ltd., 1996.\n[Czarnecki00] Czarnecki, K., and Eisenecker, U.W. Generative Programming: Meth-\nods, Tools, and Applications, Addison-Wesley Professional, 2000.\n[JLMS98] Jazayeri, M., Loos, R., Musser, D., and Stepanov, A. Report of the Dagstuhl\nSeminar 98171 “Generic Programming,” Schloss Dagstuhl, April 27–30, 1998.\n[Meyers01] Meyers, S. Effective STL: 50 Specific Ways to Improve Your Use of the Stan-\ndard Template Library, Addison-Wesley Professional, 2001.\n[Martin02] Martin, R.C. Agile Software Development. Principles, Patterns, and Prac-\ntices, Prentice Hall, 2002.\n[Meyers07] Meyers, S. “Red Code, Green Code: Generalizing Const,” available\nonline at http://nwcpp.org/Meetings/2007/04.html, 2007.\n[Stroustrup03] Stroustrup, B. “Concept Checking—A More Abstract Complement\nto Type Checking,” Technical Report N1510, ISO/IEC SC22/JTC1/WG21,\nSeptember 2003.\n3.3\nManaging AI Algorithmic Complexity: Generic Programming Approach\n247\n",
      "content_length": 2121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "249\n3.4\nAll About Attitude: \nBuilding Blocks for Opinion,\nReputation, and NPC\nPersonalities\nMichael F. Lynch, Ph. D., \nRensselaer Polytechnic Institute, Troy, NY.\nlynchm2@rpi.edu\nT\nhe concept of attitude, a positive or negative evaluation about some attitude\nobject, has a long history in psychology. Several games have used this concept in\nopinion and reputation systems, but the concept of attitude is more general than that.\nAttitude systems can be used to enrich NPC behavior in other ways besides opinion\nand reputation systems, for example, as inputs into decision tree or behavior trees, as\npart of modeling social networks, and to enrich NPC “personalities.” \nAttitude systems are more appropriate for games where NPCs need to exhibit\nbelievable social behaviors toward the player and/or toward one another. These can\ninclude game genres like god games, RPGs, dating games, games about political fac-\ntions or palace intrigue, espionage, and perhaps some types of RTSs. They may also\nbe appealing for use in certain forms of serious games, for example, games that need\nto simulate political processes, media or propaganda effects, or marketing campaigns.\nThis gem presents enough basic attitude theory to get started and suggests some\nlightweight implementations that can be used in conjunction with other parts of the\nAI.\nIntroduction\nAs game consoles and personal computers become more powerful, the prospect of\ndeveloping games with NPCs that behave something like real human (or maybe alien)\nbeings becomes more and more attractive—and more and more demanded by play-\ners. Emotions and attitudes are a large part of what makes us human, and if we are to\ndevelop human-like behaviors in our game characters, developers need to tackle these\nmessier aspects of human behavior.\n",
      "content_length": 1786,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "This gem presents the psychological construct called attitude that can become\npart of your toolkit for building more interesting and human-like NPCs. In present-\ning these ideas, please note that I am going to be doing considerable violence to how\nthese concepts are actually treated in these social sciences, by drastically oversimplify-\ning a number of their more subtle aspects to the point where they will (I hope)\nbecome useful in game development. After all, for these concepts to be practical, they\nultimately have to be implemented in code.\nFortunately, game characters are caricatures of real humans: simpler, more\nextreme, and more over-the-top than real people. This should simplify the challenge.\nSo what you will be doing here is not proper science. Call it cognitive engineering\nor even psychological hacking. One good term for it is “critical technical practice,”\nfirst proposed by Agre in 1997 and quoted by Michael Mateas [Mateas02].\nMy own way of describing critical technical practice is that it is a style of engi-\nneering practice that’s informed by scientific theory but that nevertheless develops\nalong its own trajectory and builds on its own successes. It is nevertheless prepared \nto reexamine its own premises and techniques in the light of new findings. Game AI\ncertainly fills the bill.\nAttitude\nWhat is this thing called attitude? Attitude, as defined by social psychologists, has a\nmeaning much like popular usage, as in “having a positive attitude about something,”\nbut not in the sense of ‘having a bad attitude” or “copping an attitude.” The study of\nattitude in the social sciences has a long history, and there is a vast body of literature\nabout it spanning several disciplines within the social sciences.\nAcademic researchers strive to be precise in their definitions, and this is no differ-\nent for attitude. Many different definitions have been offered over the years, but one\ngood one is by Alice Eagly and Shelly Chaiken [Eagly93], who state that attitude “is a\npsychological tendency that is expressed by evaluating a particular entity with some\ndegree of favor or disfavor.” For excellent and in-depth coverage of this subject, the\ninterested reader is directed to their magisterial text [Eagly93].\nCentral to the attitude construct is the evaluative dimension. Reducing it to the\nbarest essentials, it simply means that the person holding the attitude has made a judg-\nment about the degree to which the holder likes or dislikes the attitude object; that is,\nthe person has judged how appealing or unappealing the target of the attitude is. \nAttitudes can be held for just about anything that can be evaluated; the “target” of\nan attitude is usually termed the attitude object. Attitude objects can be concrete, like\npersons, physical objects, and places; or abstract, like notions of freedom, equality,\nnationalism, or justice. Attitudes about abstract entities that imply a moral dimension\n(like freedom or equality) are usually termed values.\nThe attitude object can be a singular item or an entire category or class of related\nitems. Humans use the tendency to have attitudes about classes of objects, rightly or\nwrongly, to reduce cognitive load and streamline decision-making. The dark side of\n250\nSection 3\nAI \n",
      "content_length": 3262,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "this is that it can lead to unfair prejudice and stereotyping. The bright side is that it\ncan simplify life. If one’s attitudes toward Tide detergent are that it smells nice,\nremoves stains, and cleans well, one can, by collapsing down a long chain of reasoning\nand detergents, lead to the simple behavior outcome “just buy Tide.” Humans are\n“cognitive misers;” we try to keep hard thinking down to the minimum amount\nneeded to get the job done.\nImportantly, attitudes can also be about events, attitudes about other attitudes,\neven attitudes about one’s reactions to having an attitude, and so on. When dealing\nwith human attitudes, it can get complicated very quickly.\nAttitudes are also the basis for other forms of human cognitions. One way to view\na belief, for example, is that it is an attitude about a proposition, that is, a logical state-\nment. One can believe that the “Earth Is Flat” (a proposition that can be true or false),\nor that “Bobby Cheated on Danielle” (which might also be true or false). \nAttitudes demonstrate what is sometimes called dispositional liking. Dispositional\nliking (the basis of attitude) is not the same thing as momentary liking, which is an\nimmediate emotional response to some entity. Attitudes are evaluative beliefs about\nattitude objects; they are formed though a process sometimes called ABC, for affect,\nbehavior, and cognition. Affect (accent on first syllable: AF-fect) is (briefly) the techni-\ncal term for an emotion response. Some attitudes are formed because of an emotion-\nally involving experience with an attitude object. Behavior is of course action; a series\nof pleasant dining experiences in fine restaurants can lead to a positive attitude toward\nfine dining. Cognitions are, of course, thoughts. Thinking about entities or issues can\nlead to the formation of attitudes; by thinking about what effect freedom has in\nhuman affairs may lead to positive attitudes about freedoms, and in turn, about the\npolicies and philosophies that can lead to greater freedom.\nAttitudes accumulate through a lifetime of interacting with the world. Attitudes\nform about attitude objects as you experience them, use them, or think about them.\nImmediate reactions of liking or disliking become attached to prior experiences and,\nby doing so, attitudes form and harden.\nThe distinction between dispositional and momentary liking may seem like a\npetty or unimportant point; however, you’ll see later that attitudes are more or less\nenduring, and although they can be modified by momentary experiences, they persist\nbeyond those experiences.\nStrictly speaking, this discussion is not quite correct. People do not walk around\nsporting attitude meters that we can read off directly; instead we infer that people\nhold these attitudes based on what we can observe. How exposure to an attitude\nobject (observable) leads to the activation of an internally held attitude (not observ-\nable), which can then (although not inevitably) lead to some outward expression of\nthe felt attitude (observable). The expression of an attitude can be through many\nchannels—facial expressions, posture, and other non-verbal communications, spoken\nlanguage, and actual behaviors.\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n251\n",
      "content_length": 3284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "Complex attitude objects, like people, historical events, and organizations, can be\nevaluated along as many attributes about the attitude object that are of interest. You\ncan love her dimples, hate her cooking, be mildly put off by her political views, enjoy\nher taste in movies, and detest her horselaugh, all at the same time. This idea will\ncome up later when we look at the matter of love/hate.\nAlso, because an attitude represents an evaluation held by an individual about\nsomething, it is entirely subjective, and because it is, an attitude is not actually a state-\nment of truth or falseness, even if very strongly felt.\nAttitudes often (although not always) carry an emotional “charge.” Some atti-\ntudes are purely intellectual, but others are learned as a result of an emotionally intense\nexperience, and these can become among the most enduring of attitudes. In such cases,\nthe affective (emotional) reaction (along the axis of appealing/unappealing) is non-\ncognitive—we do not think about our reactions but instead experience them immedi-\nately and spontaneously. This is because such reactions involve more ancient regions\nof the brain where much of the emotional apparatus of the brain resides. Emotion is\nalso bound with memory; highly emotional events are nearly always well remem-\nbered. It is what happens in extreme cases of Post Traumatic Stress Disorder (PTSD),\nand it is also why we usually can easily remember what we were doing during 9/11,\nthe Challenger disaster, or the Kennedy assassination.\nWhat’s in an Attitude?\nFortunately, by taking some careful liberties with the rigorous social science here, one\ncan produce a reasonably lightweight model of attitude that can be used by AI game\nprogrammers. In fact, something very much like attitude has been used in a number\nof games under various guises, perhaps most notably in the Xbox game Fable [Rus-\nsell06]. Greg Alt and Kristen King mention an earlier approach used in the Ultima\nOnline series [Alt02].\nCalculations to update values in the attitude system need not be performed every\nframe; in fact, one nice thing about an attitude system is that attitudes need to be\nrefreshed only when an event in the game that can affect attitudes occurs. Russell\n[Russell06] called these opinion events. More generally, they occur at points in the\ngame story called the dramatic beat. How often do these happen? For Façade, Michael\nMateas [Mateas02] estimated that these occur about every minute or so. An attitude\nsystem is therefore not going to stress the CPU budget very much, unless you are\ndealing with an exceptionally large number of NPCs carrying around a large number\nof attitudes. If so, the updates can be spread out over multiple frames without causing\ntoo many problems or getting them too far out of synch. Memory usage, on the other\nhand, is likely to be much more of a concern. [Alt02] discusses this point at length. \nLet’s begin by assuming that each agent capable of holding an attitude will in fact\nhold a collection of attitudes for as many attitude objects as the game requires. Most\nlikely, the chief attitude object will be the human player or player character (PC), which\n252\nSection 3\nAI \n",
      "content_length": 3185,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "then forms the basis for an opinion system—how the NPCs in the game world regard\nthe player along some number of dimensions that the designers feel is important.\nThe agents here are presumably all the significant NPCs in the game, but may\nalso be collections of agents, such as an entire village or even the entire game world.\nAgents that represent other forms of organizations can also hold attitudes. This was\nthe approach taken in Fable, where attitudinal information about the PC was stored\nglobally (called the hero stats), then at the village level for each of the 10 villages in the\nAlbion game world, then for each of the many significant individual NPCs [Rus-\nsell06]. This approach brought many important implementation benefits that are well\ncovered in that article.\nValence\nWhat data should an individual attitude contain? Obviously, the first item has to be\nthat evaluative dimension of liking/disliking, a value commonly called the valence. So\nto start, each attitude needs to carry at least a single integer or floating point value to\nrepresent this dimension. Most likely it will be a bipolar value, to express the full\nrange “of like/dislike, love/hate, satisfied/dissatisfied, agree/disagree,” and all the rest\nof the ways an evaluation might be expressed. In some cases, a unipolar value may be\nmore appropriate, as you will see later.\nThe valence can be stored as a single precision floating point value, especially if\nthe game requires the ability to model small shifts in attitude taking place over time\n(persuasion). For many uses, however, an integer may be perfectly okay. If there is an\nanticipated need for storing a large number of attitudes for a large number of NPCs,\nthe valence could be stored in as little as four bits, yielding a –7 to +7 range. (The\n16th unused value of –8 (1000b) could be reserved as a sentinel value.)\nHow valence should be scaled, however, is somewhat less clear-cut. The simplest\napproach is to use –1.0 to +1.0 (and normalizing integer values in calculations as if\nthey spanned this range), and further assume that liking/disliking is linear within that\nrange. Indeed many systems implicitly assume this. But is it true? First, it is not clear\nfrom research how many orders of magnitude a liking/disliking reaction can span. If\n“mild dislike” is –0.1, does that mean “blind seething hatred” is a –1.0, or is it more\nlike –10.0, or –100.0? Two possible alternatives are as follows.\n• One is to still place the evaluation value on a scale from –1.0 to +1.0, but have the\nresponse curve be a sigmoid. Chris Crawford suggested this approach for story-\ntelling systems [Crawford04].\n• Another possibility is to allow the evaluation value to range over a small span,\nperhaps {1.0 … 5.0} to represent (for example) five orders of magnitude in loga-\nrithmic fashion, in a manner akin to the decibel scale or the Richter scale. Then\n“mild dislike” is a –1.0, strong dislike –2.0, moderate hatred a –3.0, strong hatred\na –4.0, and blind seething hatred a –5.0. The descriptive phrases used here are\nsimply to give an approximate idea of what each level represents. They are not\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n253\n",
      "content_length": 3217,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "intended to express linearity of English descriptions (this is another can of worms\nentirely).\nThere are some occasions when a unipolar representation may be more appropri-\nate, but these are unlikely to arise in most game design situations. For example, in\nRational Emotive psychotherapy, the opposite of love is not hate, but rather indiffer-\nence [Ellis75]. In that view, hate cannot be the opposite of love because it too requires\nan intense entanglement with the attitude object, and simply is a different emotion\nmanifested relative to that attitude object, and not one that is 180° apart. The oppo-\nsite of hate is also indifference.\nUse caution here. If the nature of the game you are developing requires modeling\nthis sort of emotional subtlety, expect to spend quite a bit of time developing a model\nthat is up to the task.\nPotency\nA second value that can be stored in the Attitude class is potency. This is a measure of\nhow strongly held the attitude is. Potency is not the same as the valence. It is possible,\nfor example, to be very strongly politically moderate, having a valence close to 0.0,\nwhile feeling very strongly that way. This occurs because an attitude represents the\naccumulation of a lifetime of exposures to the attitude object, yet it is expressed as a\nsingle snapshot value. As exposures to the attitude object accumulate, the attitude\ntends to become more and more resistant to change, provided that each exposure does\nnot depart too much from where the attitude is now. A person’s first exposure to Brus-\nsels sprouts might result in a momentary liking of +0.2. Eventually, as occasions for\neating Brussels sprouts pile up, the dispositional liking (the attitude) may settle down\nto +0.16. It will tend to stay there unless the person experiences some particularly\ntranscendent or horrid (or even traumatic) experience with Brussels sprouts that forces\na dramatic reevaluation.\nIt is possible to omit this potency dimension, but if you do, you will need some\nother technique to dampen down the large (and not believable) swings in attitude on\nthe part of NPCs that would otherwise result. \nThis leads to another observation. In real life, a person who has occasion to radi-\ncally rethink his position about something highly personal generally goes though con-\nsiderable emotional upheaval along the way. Say, for example, a character learns that\nher brother has been discovered to be the perpetrator of a number of particularly\nheinous murders. All at once, her lifetime of accumulated feelings and attitudes about\nthe brother will begin to collapse. She may move through the well-known sequence of\ndenial/anger/guilt/resignation/reconciliation, sequencing (and possibly skipping)\nthrough these at a rate that is impossible to know beforehand.\nIf your game has moments of serious betrayal or treachery, any practical attitude\nsystem will probably break down under these conditions. Here, the best advice may\n254\nSection 3\nAI \n",
      "content_length": 2958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "simply be to bail out. For this, the Attitude class needs to implement a method that\ncan forcibly reset the valence and potency to any desired values. Then when the dramat-\nically heavy moment arrives, the game scripting system needs only do two things—use\nthat method to forcibly reset all the affected attitudes held by anyone who needs to be\n“adjusted” to new, more appropriate values, and play the carefully-crafted animations \nof any NPCs that need to be shown going through their emotional upheavals. Periods of\nextreme emotional distress in humans are so complex that attempting to model them in\ngame AI is probably hopeless at present.\nDuration\nOver time, people forget things, and extreme attitudes and bad memories usually soften\nover time. In many situations, we may want to allow attitudes to weaken or fade away.\nThis is more a matter of getting realistic behavior out of an NPC than of remaining\nfaithful to formal theory. As with most of these issues, the situation for real humans is a\nlot more complex than we would like it to be for these models.\nDuration can be expressed in several ways, and since the fade-out generally fol-\nlows a more or less logarithmic shape, a half-life measure is one way to represent it. If\nthe game spans only a short time frame, attitudes and memories will remain fresh,\nand duration can be omitted altogether. But if game time frame is to span many years,\nsome sort of decay function will probably be needed. If the game is supposed to span\neight game years (to pick a convenient duration) and if memories fade by 50% every\ntwo game years, by game’s end, attitudes first picked up near the start of the game will\ndecay to only 1/16th of their original strength. Other approaches for decaying atti-\ntude strengths or valences are also possible, of course.\nAnother thing to consider is personality factors of the NPC. You may want some\nof them to hold grudges, and for them the half-life should be set very long so that very\nlittle (or even no) decay in their negative attitudes occurs. [Russell06] describes how\nthis problem was tackled in Fable.\nThe Model\nFrom these factors, the Attitude construct can be implemented with a lightweight\nclass more or less like this:\nclass CAttitude\n{\nprivate:\nEntity* target;   // attitude object; \n//   points to a game entity \nValence valence;  // typedef Valence as needed\nPotency potency;  // typedef Potency as needed\nint months;         // game months as half-life\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n255\n",
      "content_length": 2544,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "public:\nCAttitude (Entity*);\nDecay;            // compute another month’s decay\nFloat Product;    // valence * potency\n// etc.\n}\nComplex Attitude Objects\nOne useful metric often required from the Attitude class is an overall or aggregate\nevaluation when dealing with complex attitude objects (like the PC) that get evalu-\nated on the multiple attributes that they possess. \nAt first, it might seem that, for linear representations, a normalized SRSS (square\nroot of sum of squares) would work perfectly well. The game engine’s math SDK may\neven already supply a method for computing this value from a vector of attribute-\nbased evaluation values.\nHowever, this is not what theory calls for. Instead a better value is computed as\nthe normalized sum of the products of each attitude’s valence times that attitude’s\nstrength, as in:\n(3.4.1)\nIf you’re using some other representation, like sigmoid or logarithmic curves, gen-\nerating this aggregate value will naturally require more computational overhead. \nA Simple Example\nConsider a game in which the player is battling a small but ferocious tribe over a\nperiod of time. The enemy NPCs are great warriors and smart enough to know when\nto retreat or melt away in order to fight another day. In other words, the individual\nNPCs last a lot longer than the typical 11 or so seconds a typical orc in a typical orc\nhoard survives in games of this type.\nFigure 3.4.1 shows a very simple FSM (Finite State Machine) [Schwab04]. Of\ncourse, if the warriors were as good as just described, this FSM would be totally inad-\nequate. A more modern and convenient algorithm would be the behavior tree (actu-\nally a DAG) [Isla05]. But to make the point, I’ll use this FSM. \nIf there is no opinion or attitude system in use, you could set up an FSM much\nlike this one, choosing and tuning various values of A (approach distance), R (retreat\ndistance), and H (health) to generate slightly different behaviors among the warriors.\nYou can still use this but now add the attitude system.\nA\n=\nA valence* A strength\nA strength\ntot\ni\ni\ni\ni\ni\n∑\n∑\n256\nSection 3\nAI \n",
      "content_length": 2087,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "In that case, each warrior hates the player (to some extent) and fears the player (to\nsome extent) because the attitude system has in some fashion supplied each warrior\nwith hate and fear values about the player to hold as attitudes. But then the player\nkills off a warrior who is the brother of one of the other warriors. The surviving war-\nrior will now strengthen his hate attitude toward the player, and possibly also change\nthe fear value (depending, perhaps, on how valiantly or cowardly the player fought\nwhile killing the brother). From there on, the surviving warrior can communicate his\nheightened hatred and fear, first of all to his buddies (affinity group), and then, like\nripples in a pond, to more remote members of the tribe with ever lessening intensity.\nHow this might be done is discussed briefly in a bit.\nYou now have a basis for dynamically changing the behavior of the FSM by mod-\nifying A, R, and H with attitude values that modify the initial settings upward or\ndownward. Fearful warriors can now retreat at higher values of H. Hate-filled warriors\nmight now require longer R distances before breaking off an attack, and also require\nlonger A distances, on the theory that these hate-filled warriors are much more likely\nto monitor the environment awaiting the player’s return.\nEven this simple use of an opinion system can enrich an otherwise simple model.\nBut you need not stop here. Figure 3.4.2 shows an extended FSM with extra nodes\nadded to model the warrior who becomes an implacable foe. Now, if the level of hate\n(D) rises high enough and the level of fear (F) drops low enough, a tipping point is\nreached, and the FSM for this warrior now transitions to a new mode, whereby the\nwarrior will pursue, fight, and only briefly retreat from combat with the player, until\none or the other is killed. By doing this, you can further add to the believability of the\nNPC, because it is certainly believable for a warrior to reach a point where the battle\nwith the player becomes personal. Note that you now begin to encroach on the design\nof the game itself. Once a warrior transitions into “implacable,” the nature of the\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n257\nFigure 3.4.1\nA simple FSM for a warrior. A is approach distance, R is retreat dis-\ntance, and H is health.\n",
      "content_length": 2346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "gameplay itself changes, and this sort of decision is probably better handled at the\nscript level and not coded into an NPC’s FSM.\nSo, you might be thinking, “You are creating a nightmare not only for the Q/A\ntesters but for the script and level designers who have to handle all this added complex-\nity!” Sorry, this is true, but cannot be helped as more human-like NPCs are sought.\nClearly, better strategies for designing, testing, and tuning these NPCs are part of the\nchallenge.\nAttitude and Behavior\nThis is another area having an extensive body of theories (and their attendant contro-\nversies). A human, of course, holds literally millions of attitudes about just about\neverything she has ever encountered, and only in some cases is a particular attitude—\nheld at a given time and place—followed by an observable behavior. But storing atti-\ntudes that don’t lead to behaviors that the player can observe in a game environment\nare wasted.\nYou want to use attitude data structures only where they are useful, and this\nmeans once again cutting through a lot of theory to get at a minimal configuration\nthat gets the job done.\n258\nSection 3\nAI \nFigure 3.4.2\nAn extended FSM where attitude influences transitions\nbetween the nodes. D is the level of hate and F is the level of fear.\n",
      "content_length": 1285,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "So first, let’s introduce two new wrinkles into the model. The first is the notion of\nan attitude toward a behavior. Yes, that is possible. Any action that an NPC is able to\ntake can have associated with it an attitude, which represents the direction and degree\nthe NPC feels that behavior is desirable. In a social game, murder may be possible,\nbut a particular character may view it sufficiently negatively to entirely rule it out, or\nto commit murder only when the provocation reaches some extreme threshold.\nThe next notion is behavioral intention (abbreviated to BI). BIs sit between attitudes\nand behaviors, and in this model a behavior has to have a BI connected to it before the\nbehavior can occur. This can turn out to be a welcome simplification, however, and one\nthat lends itself for use in behavior trees or other models of NPC behavior. Let’s see how\nthis might work in practice.\nLet’s say an NPC has developed a positive enough attitude toward the player that\nhe or she wants to help. This NPC has an elaborately scripted behavior tree that con-\ntains a number of possible “helping” behaviors—for example, Give Gold, Share\nSecret, or Arrange Lodging. However, these can only fire when conditions are right.\nGive Gold and Share Secret can only occur when the player is in proximity with the\nNPC, whereas Arrange Lodging can be done beforehand and at a distance. (The\nplayer can show up at the tavern, expecting to have to beg for a place to stay, only to\nbe told that a room and meal have already been provided for.) Give Gold or Arrange\nLodging can happen only if the NPC has sufficient gold to make either of these gifts.\nShare Secret can happen only if the NPC has a valuable secret to share. \nBIs can be used here to “prime” those helping behaviors, which are then further\nsubject to the previous test conditions. So, if the NPC has developed a positive\nenough attitude toward the PC, the NPC can form a BI to help the PC. The BI can\nthen test the IF portions of the stack of rules in the behavior tree, to determine\nwhether the other conditions are also met. If so, the behavior can actually occur.\nPersuasion and Influence\nThis is yet another complex and messy body of theory, which you need to pare down\nas well. How much paring is required depends on the kind of game you are building.\nSerious games that need to model political struggles, or advertising or propaganda\ncampaigns, may need more of the model than games that don’t.\nIn theory, how effectively a person or group A can persuade B to adopt or shift\nsome position (that is, adopt or shift some attitude) depends on a large number of\nfactors arranged in a causal chain. The persuader, A, is usually called the sender, and\nthe target of the persuasive message the receiver. At a minimum, the sender needs to be\ncredible (to the receiver), likeable (by the receiver), and similar (to the receiver), in\ncombination to some degree. The receiver, for his part, has to attend to the message,\nbe able to process the message, and be sufficiently involved with what the message is\nabout. The message itself might appeal to reason, emotion, or both. If the appeal is to\nreason, it has to be logically sound, as the receiver interprets it. Also, the message can\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n259\n",
      "content_length": 3323,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "easily fail to persuade if it advocates a position too extreme relative to the receiver’s\ncurrent attitude and falls outside of positions the receiver is willing to accept.\nIn a game, you can do away with a lot of this. First, let’s assume that all persuasive\ncommunications in a game are important, that receivers will always find them impor-\ntant, attend to them, and are able to understand them. This immediately removes four\nvariables. Categorizing senders into a much smaller number of groups can collapse\ndown credibility, liking, and similarity. It will then be necessary to maintain a matrix\nof how much Group A trusts messages from Group B.\nNote that this matrix is itself a representation of N-by-N attitudes, and could\nthus also see its values shift over time as groups interacted with each other over the\ncourse of the game. If this is too complex for the game, and such attitudes are not\ngoing to shift, you should compose the matrix with static values and leave it alone\nduring the game.\nThis matrix can also be augmented by adding entries for particular individuals\n(who are also members of groups) to the matrix. This permits members of Group A to\nmostly distrust messages from Group B, while allowing members of Group A to\ngrudgingly accept messages from Person P (who otherwise happens to be a member of\nGroup B). [Alt02] provides details of a nice implementation of this idea.\nFinally, let’s assume that you aren’t going to worry about how the message is com-\nposed; simply that it carries content and persuasive strength. To continue with the\nexample, the surviving warrior who now passionately hates the PC can be shown in an\nin-game cutscene haranguing his fellow warriors as to why it’s now important to make\nslaughtering the PC top priority. Internally, the message is simply conveyed, and the\nattitudes of the warrior’s buddies are suitably adjusted.\nSocial Exchanges of Attitudes\nAs mentioned, the warrior personally affected by the outcome of a battle can communi-\ncate his heightened hate and fear about the player to his affinity group. Moreover, if \nhe was close to his brother, and is close to his affinity group, the other members of that\ngroup presumably also have liking toward the deceased brother, and will be responsive\nto the surviving warrior’s messages. This gets into balance theory [Wikipedia07]. Balance\ntheories are a powerful part of modeling social networks, which is another aspect of\nhuman behavior that is outside the scope of this gem. Using the matrix approach in the\nprevious section, possibly with enhancements, can handle much of this.\nIn many games, all that is important is for the various NPCs to hold attitudes\ntoward the player only, and not hold attitudes about each other. If there is no reason\nin the design of the game to keep information about inter-NPC attitudes, leaving\nthem out greatly simplifies the design. In fact, [Russell06] was quite explicit about the\nfact that Fable stored opinion data only about the PC and no one else to get complex-\nity under control. The opinion system used in Fable is essentially based on the attitude\nconstruct, just not by that name.\n260\nSection 3\nAI \n",
      "content_length": 3154,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "However, some game designs benefit by having such a system if the gameplay\nrevolves around alliances, treachery, and betrayal. Doing so uncovers a hornet’s nest of\nadded complexity.\nOne such complexity is sheer algorithmic cost. Of course for N characters that\ncan hold attitudes, there are N * (N – 1) pairings, yielding an O(N2) complexity. You\ndo not even get to divide this by two, because it is unsafe to assume that A’s attitude\ntoward B will be symmetrical with B’s attitude toward A. One strategy is to reduce the\nsize of N by assigning less critical NPCs to a much smaller number of groups and\ntracking those instead.\nAnother Example\nConsider how this can be used in a hypothetical open-world action-adventure game\nabout warring crime syndicates and the sleazy yet colorful NPCs who inhabit this game\nworld. The object of the game is to compete against the NPCs by doing dirty deeds,\nenforcing the rules laid down by the mob, expanding turf, making money, switching\nallegiances, and, on occasion, even betraying or killing an NPC when it will do you the\nmost good.\nAs the PC, you engage in acts that earn the respect or disgust of the NPCs, who\nform opinions about you as you claw your way to the top. In a manner following the\napproach used in Fable (which used five dimensions of Morality, Renown, Scariness,\nAgreeableness, and Attractiveness), let’s use several dimensions of opinion scale for\nthis example—Competent, Honorable, Ruthless, Charismatic, and Loyal. \nIdeally, the selected attributes should be as orthogonal as possible, that is, each\nattribute is as close to statistically independent of the others as possible. These five\nseem to meet this requirement. It may seem strange to include Honorable, but con-\nsider it as the code of conduct that dictates that innocent “civilians” are not to be\nharmed (especially one’s mother), but that shopkeepers, gamblers, druggies, and johns\nare fair game, as is anyone who crosses you. \nAssume the game features several different competing crime organizations, each\nwith its own style of achieving dominance. One is led by a capo who favors brazen,\novert violence (he will probably go down in flames early on); another prefers corrupt-\ning police and judges, a third likes to insinuate his organization into legitimate enter-\nprises. Each capo is likely to value a different pattern of attributes of the player or any\nNPC as best suited for his style of organization. \nAs the player continues to work up the ranks, different bosses will tend to value\ndifferent combinations of these perceived attributes. A capo who prefers to keep vio-\nlence hidden away, used only as a last resort, might well bypass a player who is too\nhot-headed for that capo’s style. A player who botches too many jobs will find that his\nperceived Competence has deteriorated and so be left off the important missions that\ncould best advance his career. This could lead to the player having to do many more\nlow-level missions in order to get back into the good graces of his superiors in the\ngang hierarchy. \n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n261\n",
      "content_length": 3134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "Just a handful of attribute/attitude dimensions are needed to make this sort of\ngameplay possible. In Fable, the opinion system and hero stats operated using only\nthe five dimensions listed previously. Even if you only consider high/low on each (on\nthe premise that extreme characters are more fun), that leads to 32 possible combina-\ntions of how the player is perceived and the player’s reputation is built. \nOther combinations of these dimensions can be part of the personalities the\ndesigners build into the rest of the NPCs. After all, even a dim-witted but ruthless and\nhighly loyal NPC can be useful to a crime syndicate. \nCautions and Conclusion\nAt this point you may be thinking, “Why not just make games that need NPCs like\nthis be multiplayer games and let real human players handle all this complexity?” To\nsome extent a multiplayer approach will work and already does in many games, but\nthe problem with this approach is analogous to the problem with player-created con-\ntent—most of it (perhaps 95%) is too poor in quality to be of much use, let alone fun.\nNot that many people are excellent storytellers, role players, or improvisational actors,\nand, moreover, it is a lot of work. It seems like it will be the fate of game designers and\nother talented folk to create the rich game worlds and believable characters that play-\ners demand. \nThis gem introduced a few fundamental concepts about the psychology of attitude.\nAttitude is only one aspect of the psychology of those most complex of organisms,\nhuman beings, but is a useful start and one within reach. Fortunately, the representation\nof a single attitude can be quite lightweight, although in a game with any complexity\nthey can become quite numerous.\nAs more and more CPU and RAM budgets are allocated to game AI, game devel-\nopers will increasingly need to mine the very large body of knowledge about human\nbehavior from psychology, social psychology, and cognitive science. The good news is\nthat game developers need to build caricatures and not real humans, and the challenge\nis one of adopting what we know of real humans and re-engineering that knowledge\ninto practical implementations. \nReferences\n[Agre97] Agre, Phil. Computation and Human Experience, Cambridge University\nPress, 1997.\n[Alt02] Alt, Greg and King, Kristen. “A Dynamic Reputation System Based on Event\nKnowledge.” AI Game Programming Wisdom, Charles River Media, 2002: pp.\n425–435.\n[Crawford04] Crawford, Chris. Chris Crawford on Interactive Storytelling, New Riders\nBooks, 2004.\n[Eagly93] Eagly, Alice and Chaiken, Shelly. The Psychology of Attitudes, Harcourt\nBrace Jovanovich, 1993: pp. 1.\n262\nSection 3\nAI \n",
      "content_length": 2657,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "[Ellis75] Ellis, Albert and Harper, Robert A. A Guide to Rational Living, Wilshire\nBook Company, 1975.\n[Isla05] Isla, Damien. “Handling Complexity in the Halo 2 AI,” GDC 2005 Proceed-\nings, \navailable \nonline \nat \nhttp://www.gamasutra.com/gdc2005/features/\n20050311/isla_01.shtml.\n[Mateas02] Mateas, Michael. Interactive Drama, Art and Artificial Intelligence, (Ph. D.\nDissertation), Carnegie Mellon University, School of Computer Science, 2002,\nreport CMU-CS-02-206.\n[Russell06] Russell, Adam. “Opinion Systems,” AI Game Programming Wisdom 3,\nCharles River Media, 2006: pp. 531–554.\n[Schwab04] Schwab, Brian. AI Game Engine Programming, Charles River Media,\n2004.\n[Wikipedia07] “Balance Theory,” available online at http://en.wikipedia.org/wiki/\nBalance_theory.\n3.4\nAll About Attitude: Building Blocks for Opinion, Reputation, and NPC Personalities\n263\n",
      "content_length": 854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "265\n3.5\nUnderstanding Intelligence\nin Games Using Player\nTraces and Interactive \nPlayer Graphs\nG. Michael Youngblood, UNC Charlotte\nyoungbld@uncc.edu\nPriyesh N. Dixit, UNC Charlotte\npndixit@uncc.edu\nA\ns the field of game AI has grown, the ability to create characters and game reactions\nthat impart reasonable, challenging, and even insightful actions in their control\nhas improved. However, the basic ability to describe what makes something truly\nseem generally intelligent has lagged behind. This gem provides some insight into a\nspecific visualization and graph-based AI analyses mindset with some tools and tech-\nniques that forward a goal of better understanding both artificial and human intelli-\ngence in games. This gem shows how logging player-centric game data can be used to\nbetter understand both natural and artificial player behavior through the use of visual\ndata-mining, graph-based interaction representations, and clustering tools. \nIntroduction\nThe game AI developer is focused on the creation of an entity, multiple entities, or\npossibly just a system which the engaged human player must perceive as a challenge in\nsome form or fashion. Satisfying the cognitive needs of human players is what makes\nthe game interesting, fun, and challenging, and it is what ultimately makes the game\nsell. The interesting problem is that not every human player perceives the same, or\neven plays the same. What is intelligent to one player is dumb to another. This makes\ngame AI development very difficult, and it leads to discussions about the tradeoffs\nbetween focusing on games that pit human versus human or human versus machine\n(AI). Fortunately, there is a strong desire for a better single player experience and\n",
      "content_length": 1723,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "hence a strong need for better AI—or as the need is often stated, there is a need for\nhuman-level AI. There have been numerous discussions about the pursuit of human-\nlevel AI [Heinze02, Laird01, Laird02], but there still remain many open questions\nabout how to determine whether you have achieved it. \nIs a game AI turing test [Russell03] a valid way to determine whether an entity is\nhuman or machine controlled? Although it may prove to be an interesting endeavor, it\nwould undoubtedly be mired in much debate and conjecture as with all rather subjective\nmethods. What is needed is a method for objective evaluation of game AI to a human\nbaseline of performance on the same or similar task or scenario. This act requires data. \nThe current trend in modern computer games is to leave out detailed logging in\norder to free up system resources for other material; however, this data could be used\nto enhance the interactive game experience by providing insight into the behaviors of\nboth human and machine players. \nThe Value of Information\nLogging in games is often tied to the game’s save features because these subsystems\ncommonly track the progress of the player. However, most games do not log player\ninformation; this trend is getting worse, as you can note by the various number of\ngames that now use a checkpoint system for saving rather than being able to save the\ngame at any point. This means that the game will only save when a player reaches a\ncertain location in the level. This way, the developers do not need to track where the\nplayer is going at all times—just when they reach certain milestones. As argued by Eil-\ners [Eilers05], this is fine for the first few times that a player plays the level; however,\nit becomes a hindrance later on once they have already memorized the level. It creates\nwork for the player by them having to drudge through mastered areas to reach the\nchallenge. It also hinders the ability for real logging because there is now no in-game\nprogress monitoring which is a nice place to invoke logging. \nLogging is very useful during the play-testing phase because it can make the\nprocess more efficient. The development team could capture the tester’s session in\nvideo form but it is often too time consuming to watch all the videos, analyses may end\nup very subjective, videos consume a lot of disk space, and objective, automated video\nprocessing is a difficult process. Watching the session firsthand can introduce bias from\nthe observer’s opinion. Another problem with video or screen capture playback is that\nit is often difficult to get a complete picture of play just from the player’s perspective.\nSeeing the entire path or desired sections of the player’s interaction at once or in a spe-\ncific focused view in an interactive analysis tool would be very useful in understanding\nthe behavior of that player—whether it was a human or machine.\nA good set of logged data and some analysis tools (including visualization tools)\nalso helps to find emergent behaviors, or behaviors that are not expected by the devel-\nopers. These are not necessarily errors but interesting “accidents” that differentiate\ngames from non-interactive forms of entertainment [Consalvo06]. \n266\nSection 3\nAI \n",
      "content_length": 3230,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "The most common reason for excluding or oversimplifying logging is that it can\nslow down the game. For instance, during the development for Age of Empires II: Age\nof Kings, the developers used faster machines for play testing than those targeted for\ndeployment due to the slow down induced by logging play test data [Marselas00].\nThis does not always have to be the case because one of the most machine expensive\nsteps in logging is file IO, which can be done more opportunistically at periods of\nlessened hardware need, at cutscenes or designed lulls, or by using multiple threads\ninstead of constantly writing to a file during gameplay. There is also an issue of the\nneeded fidelity for logging. Plenty of information can be gained from 1Hz or slower\nlogging, so 10Hz or better is not always necessary.\nOver the past few years, we have been performing AI research with a number of\ngame testbeds of our own creation, some built from the ground up and others modifi-\ncations of commercial games. We often use the Urban Combat Testbed (UCT), which\nhave been made freely available at www.urban-combat.net. UCT is a total conversion\nmod of the popular Quake 3 game by Id Software. The installed base of Quake fans has\nmade it easy to find study participants (especially at Quakecon, which is always a great\ntime), and we have captured hundreds of players interacting in our game scenarios. (If\nthe reader is interested in being a study participant for our game studies, please visit\nour play testing Website at playground.uncc.edu/PlayTesting.)\nWe often capture logs at 10Hz, but as you can see in Figure 3.5.1, the information\nfrom a 1Hz capture can often be just as informative and useful. However, in Human\nComputer Interface (HCI) analysis work, 10Hz sampling is typically conducted because\nit is the fastest normal response time for a human using an interface device. \n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n267\nFigure 3.5.1\nPlayer trace from UCT (Urban Combat Testbed) player data showing player\nmovements from logging at (a) 1Hz and (b) 10Hz. The thin line represents the spatial move-\nment of the focused entity over time in the environment.\n",
      "content_length": 2198,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "Our analysis tool whose interactive output is shown in Figure 3.5.1, PlayerViz,\nrequires data files that contain the following information at each timestep:\n• Position (x, y, and z)\n• Orientation (yaw, pitch, and roll)\n• Speed\n• Elapsed time\n• Time-score\n• Health\n• Shots fired\n• Whether a flag is captured\nThe PlayerViz tool is included on the book’s CD-ROM and is freely available\nonline at playground.uncc.edu/GameIntelligenceGroup/Projects/CGUL.\nThe frequency of the timesteps is not fixed and can be adjusted depending on the\ndetail required. Ideally, a logging system could dynamically adjust logging frequency\nto minimize impact on the game by scaling to the available hardware capabilities.\nHowever, even if you log once per second it is enough to do some analysis on player\nbehavior. The implementation of the logging and recorded aspects will vary based on\nthe game, but consideration should be given as to what information is easily available\nand useful for knowledge discovery and understanding the intelligent actions of enti-\nties in the game. \nCapturing interaction is the key to understanding the intelligent actions taken in a\ngame by all rational agents participating. So, you log player interactions with the inter-\nactive feature points of the environment [Youngblood02]. Interactive feature points are\nelements in a game upon which an entity can perform an action (for example, open,\nclose, push, jump over, stand in, shoot, and lasso). These elements might occupy posi-\ntive space and represent a real world or fantasy object (for example, window, door, tree,\nstage coach, crate, and magical potion), they might be negative space areas that can\ncontain other game elements or even the player (for example, a courtyard, inside a\nroom, or in a vehicle), or they might be other agents within the game (for example, an\nopposing force, a horse you can ride, or a dragon you may fly). Anything with which a\nplayer can interact in any fashion can be considered an interactive feature point. \nIn the real world, the number of interactive feature points is infinite, but in a\ngame world they are finite and determined by the designers and the capabilities of the\ngame engine. All of the interactive feature points in a game or game scenario can be\ndescribed in a interaction possibilities graph, as shown for a simple example FPS envi-\nronment in Figure 3.5.2a. The vertices represent interactive feature points and the\nedges indicate that an interaction may occur next from the current interaction—an\nextension would be to enumerate the types of interactions possible with each interac-\ntive feature point. This graph can be used to look for invalid interactions or design\nissues when analyzing player traces from logged data. \n268\nSection 3\nAI \n",
      "content_length": 2760,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "Generating information for validation from design, capturing information from\nplay testing and agent interaction testing, and creating insightful knowledge as new\ninformation from analyses emphasizes how valuable this information is to the design\n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n269\nFigure 3.5.2\nThe interaction feature points of a game can be used to generate an interac-\ntion possibilities graph, shown in (a). This illustrates the interaction possibilities in the FPS\ngame scenario shown in (b) from the top and from a 3D perspective in (c).\n",
      "content_length": 603,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "of games and the desired interaction in games, which largely consists of the human\nand AI driven behaviors. A very important factor in understanding behaviors in\ngames is being able to visualize these behaviors. See Figure 3.5.3.\n270\nSection 3\nAI \nFigure 3.5.3\nA screen capture of the PlayerViz tool used for visual-\nizing single or multiple player traces. It shows the path of a player\nover time, where the spheres represent position and the line segments\nprotruding from the spheres represent orientation.\nA Picture Is Worth a Thousand Words \nThe information gathered from in-game logging of player actions can easily become\noverwhelming. There are several variables that are all tracked simultaneously, including\ntime, position, orientation, and interaction. This data is easier to comprehend if it is rep-\nresented visually with a player trace. A player trace shows the actions of the player from\nstart to finish in a visual manner. The PlayerViz tool is designed to allow interactive visu-\nalization of player trace data from one or more players. As seen in Figure 3.5.3, a player\ntrace is represented as a series of spheres with connecting lines. The color of the spheres\ncycles through a rainbow color-map, blue to green to red over time, in a way that the col-\nors appear to change faster when moving slower and vice versa. The position of the\n",
      "content_length": 1352,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "spheres represents the position of the player, and the player’s orientation at that time is\nportrayed as an oriented line segment coming out of the sphere. A wireframe white\nsphere represents that the player found the goal and it is usually located at the end of a\ntrace. A red wireframe sphere around the player indicates lost health, and a red solid\nsphere at the end of the line segment means the player fired a shot. A player trace provides\na great overview of physical interactions, but it may not always apply to all games—some\ngames do not have a clear spatial component to them, such as puzzle games. \nPlayerViz can also be used to examine multiple player traces at once, which can\nprovide some interesting results. For example, if the players were playing together,\ntheir group dynamics can be studied. You could also take the average of their player\ntraces to get an overview of their composite performance. If the players are adver-\nsaries, you can examine the strategies taken by each and how they responded to each\nother’s actions. For example, if two players are simultaneously pursuing the same goal,\nit would be interesting to see the paths taken by each player and where the paths\nintersect. Player traces can also be utilized to show AI agent behavior and compare it\nto human behavior. The composite trace of all humans and all agents could be com-\npared to give a general overview of how human-like the AI is for the game. \nSimplification of World Data\nIn order to provide context for the player traces, the world geometry must also be visual-\nized. However, the geometry does not need to be shown in great detail and can be greatly\nsimplified. For PlayerViz, the world is broken down into positive and negative space\nregions. The 3D modeler manually specifies positive space. It represents an approximation\nof the external world geometry for an object (for example, a box around a building). These\nregions are not exact, but you need only a rough estimate of geometry to provide spatial\ncoherence between the player trace and the world. The negative space regions can be man-\nually specified or automatically calculated using methods such as cell decomposition.\nIn order to simplify the negative space geometry, you must break the world down\ninto a series of convex regions. There are several techniques for decomposing the\ngeometry of a world into regions. The technique we used is called key vertex cell\ndecomposition [Youngblood06]. This method involves creating polyhedrons by con-\nnecting key vertices on the positive space objects (such as the corners), avoiding cross-\ning segments, and combining adjacent regions to increase size as long as the\npolyhedrons remain convex.\nThese regions have a lot of extraneous polygons because negative space generally\ncontains mostly empty regions. However, if you render only the polygons that are\nroughly parallel to the ground, you generally get useful geometry. You can also identify\ngateways between spatial regions of the geometry. These gateways are identified by\ncoplanar boundaries between regions. However, the boundaries must be completely\ncoplanar for a gateway to be easily detected. These regions and their connecting gate-\nways can also be used to assist path planning and other spatial tasks for AI agents. The\nagent can learn about the world around it by keeping track of the regions it has visited.\nLooking at the player trace for such an agent can be useful in machine learning studies.\n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n271\n",
      "content_length": 3563,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "A Pixel for Your Thoughts \nEven with the help of player trace visualization, it can still be a difficult task to examine\nall of the player traces if there are several hundred players. One way to make it easier is\nto use visual data mining [Keim02]. PlayerViz can be used to generate a set of Web\npages showing a table of thumbnails of each player trace from different angles. This\nallows the user to quickly scan through the dataset and find interesting traces, which can\nthen be loaded into PlayerViz to examine further. Once the desired or anomalous arti-\nfacts are identified visually, the user can then implement ways to automatically find\noccurrences of these artifacts, often through the creation of specific feature-finding algo-\nrithms. The visual data mining is mainly a bootstrap method to guide the creation of\nmore specific tools for a particular game, but it can be powerful in helping to define\nwhat you should be looking for, which is often difficult to determine a priori.\n272\nSection 3\nAI \nFigure 3.5.4\nA table of interesting artifacts found through visual\ndata mining of player traces, as follows—(a) jumper, (b) fluster, \n(c) positive learning, (d) emergent behavior, and (e) crazy Ivan.\n",
      "content_length": 1207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "In our own work using these player trace images, as shown in Figure 3.5.4, we can\nfind several interesting artifacts that would be near impossible to find by looking at\nthe numbers alone. For example, Figure 3.5.4a shows a jumper. This is a person who\nlikes to jump continuously even when he or she is walking on a flat surface. Another\nexample, shown in Figure 3.5.4b, is called a fluster. A fluster is an artifact in the player\ntrace where the player seemingly loses control of his or her cursor. It can be caused by\nlack of experience with mouse aiming or also by a faulty mouse. These occurrences\nwould be very hard to track without visual data mining.\nFigure 3.5.4c shows two player traces of the same player. The left image is the\nsource, or first attempt, and the right image is the target, the second attempt. It can be\nclearly seen that the player learned from his previous attempt how to get out of that\nclosed-off area. The opposite can also be true when a player finds the goal the first\ntime but forgets it in the second attempt. Thus, you can use this technique to observe\nboth positive and negative learning.\nYou can also find emergent behaviors using this technique, such as the player\ntrace shown in Figure 3.5.4d. The intent was for the player to climb the wall and find\nthe goal, but instead this player climbed the side of a house and jumped down from\nthe roof. Such behavior is also difficult to track without visual reference. The last\nexample is a crazy Ivan, or an instance where a player turns a full 360° to survey his or\nher surroundings, as seen in Figure 3.5.4e. \nUsing this visual data, you can also analyze the areas that are the most visited (or\nleast visited) by the players. This allows the game designers to determine whether their\ndesign for the level matches the player experience. For example, if the designer places\na clue in an area of the map that very few players ever go to, the designer will know\nthat most players are not likely to find it.\nThere are several future additions to the PlayerViz tool planned, such as calculating\nthe average player trace using a set of several player traces. This average trace can be used\nto quickly and easily see the general path taken by most of the players. Another feature\nmight be to generate a congestion map using a set of several player traces to highlight\nareas that were the most visited. This feature would give game designers a good idea of\nwhere future players are more likely to explore, so that they can place interactive feature\npoints accordingly.\nInteractive Player Graphs\nUnderstanding the spatial trajectory and observing simple spatiotemporal interactions\nprovides a great deal of understanding about the intelligence of the observed rational\nagent, but interactive visualization tools still require a lot of manual work for the analyst.\nA representation of interaction that could be used for comparison with other players\nwould be useful in this case. The trajectory of interaction in the context of a game is a\nrepresentative of the strategy taken by the individuals playing. The ability to capture and\ncompare strategies could be very useful. The establishment of a finite set of interactive\n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n273\n",
      "content_length": 3288,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "feature points and the introduction and enforcement of a designed interaction possibil-\nities graph along with proper in-game logging come together to allow for the generation\nof an interactive player graph for each player in a game. \nAn interactive player graph (IPG) is built either during play or in post-play pro-\ncessing of the running or completed player log file. The game environment needs to\ncapture the desired interactions to make the graph. The vertices of the graph represent\nan interaction event (for example, pushing a button, picking up a health item, stun-\nning an opposing force, picking up a key, going through a door, or standing in a new\nregion of the map). IPGs can be very detailed, but it is less important and more com-\nputationally feasible to reduce the data stream and ignore the minor noise generated\nfrom movement, orientation, and other esoteric changes in state. What you need is a\nsomewhat higher abstraction of player activity that gives you the proper resolution for\nunderstanding while reducing the extraneous information that can bog down analysis\nand obscure intention.\nTypically, you track spatial movement within the qualitative convex regions deter-\nmined from cell decomposition methods as described in the previous section and report\nposition by those numbered regions when entered—in many game types, especially\nFPS/3PS, spatial interactions by normal movement can dominate an IPG if tracked at\ntoo high a resolution. Other interactions are typically recorded as they occur. So, you\nbuild an IPG from a combination of interactions, which come from rough spatial move-\nments through environmental regions and player interactions with specific objects in\nthose regions. However, IPGs do not have to incorporate spatial interaction and are\ntherefore also very useful for analyzing games that do not have a clear spatial component\nto them. \nOne problem with the graph representation is that there is a definite issue of tim-\ning in many games, and often the real difference between performances in a scenario,\nespecially ones with few paths of choice through the environment, is the time it takes\ndifferent players to accomplish the same task. Gonzalez [Gonzalez99] and Knauf et al.\n[Knauf01] also note the importance of time in validation of human models. IPGs\ncapture time by weighting the edges between interaction feature points with the time\nit had taken the player between interactions. IPGs abstract a player’s performance in a\ngame or game scenario, removing the detailed state change noise through the environ-\nment while capturing their approach of interaction, moving from one interaction fea-\nture point to another and also capturing the time aspect of their performance. Figure\n3.5.5 shows a player trace converted to an IPG using the associated spatial decompo-\nsition map associated with the scenario—note that the only interaction represented is\nmap traversal; other interactions would merely extend the graph with additional\ninteraction vertices.\nTo be noted is that there are a number of extensions or variations to the base IPG\nformat described here. For example, it may be useful to associate the type of interac-\ntion with an interaction feature point representing each as a separate vertex. Agents\nmay tag internal state to the transitions or interactions as well. \n274\nSection 3\nAI \n",
      "content_length": 3346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "Clustering the IPGs\nThe usefulness of a graph format is that it can be used to compare to other graphs. In\norder to compare graphs, a measurement of graph similarity is needed. We suggest using\ngraph edit distance, which is defined as the minimum number of changes required to\nchange one graph into another. A change is defined as the insertion of an edge, the dele-\ntion of an edge, the insertion of a vertex, the deletion of a vertex, an increase in time on\nan edge by 0.1 seconds, or the decrease in time on an edge of 0.1 seconds. Each change\ncarries an equal weight of one—this can be changed to reduce bias. This scheme gives\npreference to time since it is the major factor of difference, but in many of the games\nyou’ll evaluate, time performance is the major differentiator between players. You can\nalso evaluate graphs without time weights using the same metric, but without the\nincrease/decrease in time weighting. \nOne goal of comparison is to be able to group or cluster players by their IPGs,\nwhich essentially represents their strategic choices and performance in the game or\ngame scenario. Figure 3.5.6 illustrates the many different interaction trajectories play-\ners may take even in the same game scenarios. If you cluster player performance, you\nshould see clusters of players grouped by their relative skill level [Youngblood02]. If\nplayers cluster into their skill levels, this technique can be used for player classifica-\ntion. More interestingly from an AI perspective is that if you evaluate machine-driven\n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n275\nFigure 3.5.5\nThe interactive player graph in (a) was created from logged data from the\nplayer whose same player trace is shown in (b) using PlayerViz. The vertices represent inter-\nactive feature points (entry into new spatial map regions in this case), and the edges are\nweighted by the time between interactions.\n(a)\n(b)\n",
      "content_length": 1948,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "agent data with human data and the agents cluster into groups with human players,\nyou can assert that the agent played consistent with that group of humans, or that the\nagent behaved in a human-consistent manner. \nWe typically use K-medoids clustering [Friedman99]. K-medoids is an iterative algo-\nrithm where based on a given set of data points and k clusters with centers approximated\nby initial representative objects (we seed ours initially with random members of our data\nset), all members are initially assigned to their nearest k cluster representative (or medoid ).\n276\nSection 3\nAI \nFigure 3.5.6\nInteractive player graphs should use different approaches (strategies), as seen in these four\nexamples from the same game scenario and constraints as the previous example in Figure 3.5.5.\n(a)\n(b)\n(c)\n(d)\n",
      "content_length": 809,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "Then, for each step the cluster medoids are recalculated based on whether one of the non-\nmedoids improves the total distance of the cluster and then members are re-clustered\nbased on their distances to the new centers. This process continues until there is no dif-\nference (no improvement) between two consecutive iterations. A clustering criterion\nfunction is applied to evaluate clustering quality for that k clustering. Due to the high\ndimensionality of IPGs and seeding with existing members, we iterate our clusters over all\npossible initial seed values. Due to the abstracted dimensionality of the data, we also\nsuggest exercising k from 2 to (n–2), where n is the number of IPGs being compared, \nto ensure the discovery of the best clustering in accordance with the clustering quality\ncriterion function. \nIn clustering IPGs, the clustering criterion functions utilize the distance measure\nd(xij,xpq), which is the distance between the jth member of cluster i and the qth mem-\nber of cluster p, where the distance d represents the graph edit distance between two\nmembers. The mean intra-cluster distance of cluster i is as follows, where ni is the\nnumber of members in cluster i:\n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n277\nAchieving a desired clustering of data is moreover a result of optimizing the clus-\ntering criterion function [Zhao02]. In our experience clustering IPGs, we have found\nthat the following clustering criterion function to work the best [Youngblood02,\nYoungblood03]. \nMinimize, as follows:\nUtilizing K-medoids on IPGs should produce clusters based on similar strategies\nand performance in the observed game. Clustering human players and agents can be\nhelpful in determining whether the agents are behaving similar to a known group of\nhuman players or even sets of other agent players. Clustering can be used to determine\nplayer skill level and distance from the next skill level. Evaluated in-game dynamically,\nclustering of the current human player could be used to determine the actions of the\ngame AI (based on perceived player skill or strategy). Although K-medoids may be\nunsuitable for real-time processing in-game, building player type profiles from play\ntesters and utilizing simple and fast methods such as the K-Nearest Neighbors (kNN)\nto match and respond appropriately can be effective.\n",
      "content_length": 2379,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "Digging in the Graphs\nIn addition to clustering techniques, there are other methods for discovering knowl-\nedge in graph-based data such as that presented by IPGs. The area of graph-based data\nmining offers tools such as SUBDUE (www.subdue.org) by Larry Holder. SUBDUE\nhas been used to discover common patterns in IPGs for UCT data [Cook07]. Utiliz-\ning compression techniques and the minimum description length principle, SUBDUE\ncan find common substructures in IPGs. These represent common strategies taken by\nplayers regardless of their actual cluster similarity. Game AI using appropriate responses\nfor the anticipated actions could exploit these common sub-strategies. \nA Deeper Understanding of Behavior\nData logging and player traces can also be used for other purposes. Players’ look direc-\ntions can be just as useful as their positions. By using the view directions of a player over\ntime, you can derive the surfaces that received maximum or minimal exposure. We have\ndeveloped a tool called HIIVVE (Highly Interactive Information Value Visualization and\nEvaluation), designed for this purpose [Dixit07]. The tool, as shown in Figure 3.5.7, uses\nplayer trace data to calculate intersections and find the information value for each surface\nin the world geometry. The information value of a surface represents the likelihood that a\nplayer will see information placed on that surface. This data can be used to make design\ndecisions about the placement of art assets or interactive feature points. \n278\nSection 3\nAI \nFigure 3.5.7\nThe Highly Interactive Information Value Visualization and Evaluation\n(HIIVVE) tool helps determine the information value of game surfaces. Another example\nof useful in-game logging and knowledge gained from analysis of play testing data.\n",
      "content_length": 1775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "PlayerViz could also be used to track nearly any type of captured information.\nFor example, something useful for better understanding AI agents and potentially\ndebugging issues within their intelligence mechanisms would be to reflect agent inter-\nnal state changes (for FSM and FuSM agents) or subsumption level firings. Agent\naction decisions could also be explicitly represented. \nOur group at UNC Charlotte continues research in better understanding both\nhuman and agent intelligence in games. We offer a full set of analysis tools and meth-\nods for improving game AI through the CGUL (pronounced “seagull”) Toolkit—the\nCommon Games Understanding and Learning Toolkit. CGUL is available online for\nfree at playground.uncc.edu/GameIntelligenceGroup/Projects/CGUL.\nConclusion\nThere are some strong and compelling reasons to include good logging capabilities in\ngames. Data collected from logging human players and AI interacting in a game envi-\nronment can be used to perform visual data mining with tools such as the provided\nPlayerViz, which can be used as a bootstrapping process to guide the development of\na repertoire of tools for game specific analysis and better understanding of intelligent\nactions in the game. Player traces tell only half of the story, though. By tracking and\nconstructing interactive player graphs generated from the observed trajectory of\nplayer/agent interactions that are analyzed with clustering and knowledge discovery\ntechniques, developers can garner new insights into player performance and classifica-\ntion. This information can then be exploited to develop better game AI. \nReferences\n[Consalvo06] Consalvo, Mia and Dutton, Nathan. “Game Analysis: Developing a\nMethodological Toolkit for the Qualitative Study Of Games,” The Interactive\nJournal of Computer Game Research, Vol. 6, No. 1, December 2006. \n[Cook07] Cook, Diane J., Holder, Lawrence B., and Youngblood, G. Michael.\n“Graph-Based Analysis of Human Transfer Learning Using a Game Testbed,”\nIEEE Transactions on Knowledge and Data Engineering, 2007.\n[Dixit07] Dixit, Priyesh and Youngblood, G. Michael. “Optimal Information Place-\nment in 3D Interactive Environments,” Sandbox Symposium, 2007.\n[Eilers05] Eilers, Michael M. “Soapbox: Difficulty and the Interstitial Gamer,” avail-\nable online at http://www.gamasutra.com/features/20050809/eilers 01.shtml,\nAugust, 2005.\n[Friedman99] Friedman, Menahem and Kandel, Abraham. Introduction to Pattern\nRecognition: Statistical, Structural, Neural, and Fuzzy Logic Approaches, Imperial\nCollege Press, London, 1999. \n[Gonzalez99] Gonzalez, Avelino. “Validation of Human Behavioral Models,” Twelfth\nInternational Florida AI Research Society Conference, Menlo Park, AAAI Press,\n1999, pp. 489–493.\n3.5\nUnderstanding Intelligence in Games Using Player Traces and Interactive Player Graphs\n279\n",
      "content_length": 2829,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "[Heinze02] Heinze, Clinton, Goss, Simon, Josefsson, Torgny, Bennett, Kerry, Waugh,\nSam, Lloyd, Ian, Murray, Graeme, and Oldfield, John. “Interchanging Agents\nand Humans in Military Simulation,” AI Magazine, Vol. 23, No. 2, 2002, pp.\n37–47.\n[Keim02] Keim, Daniel. “Information Visualization and Visual Data Mining,” IEEE\nTransactions on Visualization and Computer Graphics, Vol. 8, No. 1, (March 2002).\n[Knauf 01] Knauf, Rainer, Philippow, Ilka, Gonzalez, Avelino, and Jantke, Klaus.\n“The Character of Human Behavioral Representation and Its Impact on the Val-\nidation Issue,” Fourteenth International Florida AI Research Society Conference,\nMenlo Park, AAAI Press, 2001, pp. 635–639.\n[Laird01] Laird, John E. and van Lent, Michael. “Human-Level AI’s Killer Applica-\ntion: Interactive Computer Games,” AI Magazine, Vol. 22, No. 2 (2001), pp.\n15–25.\n[Laird02] Laird, John. “Research in Human-Level AI Using Computer Games,”\nCommunications of the ACM, Vol. 45, No. 1, 2002, pp. 32–35.\n[Marselas00] Marselas, Herb. “Profiling, Data Analysis, Scalability, and Magic \nNumbers, Part 1: Meeting the Minimum Requirements for Age of Empires II:\nThe Age of Kings,” available online at http://www.gamasutra.com/features/\n20000809/marselas 01.htm, August 2000.\n[Russell03] Russell, Stuart and Norvig, Peter. Artificial Intelligence: A Modern\nApproach, Prentice Hall, 2003. \n[Youngblood02] Youngblood, G. Michael. “Agent-Based Simulated Cognitive Intelli-\ngence in a Real-Time First-Person Entertainment-Based Artificial Environment,”\nMaster’s thesis, The University of Texas at Arlington, 2002.\n[Youngblood03] Youngblood, G. Michael and Holder, Lawrence B. “Evaluating\nHuman-Consistent Behavior in a Real-Time First-Person Entertainment-Based\nArtificial Environment,” Proceedings of the Sixteenth International FLAIRS\nConference, 2003, pp. 32–36.\n[Youngblood06] Youngblood, G. Michael, Nolen, Billy, Ross, Michael, and Holder,\nLawrence. “Building Test Beds for AI with the Q3 Mod Base,” Artificial Intelli-\ngence in Interactive Digital Entertainment (AIIDE), June 2006.\n[Zhao02] Zhao, Ying and Karypis, George. “Evaluation of Hierarchical Clustering\nAlgorithms for Document Datasets,” Proceedings of the 11th Conference of\nInformation and Knowledge Management (CIKM), 2002, pp. 515–524.\n280\nSection 3\nAI \n",
      "content_length": 2292,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "281\n3.6\nGoal-Oriented Plan Merging\nMichael Dawe\nU\nsing goal-oriented action planning systems to create and manage behavior in\nautomated agents is a powerful technique that has quickly found acceptance\namong game developers. In game development, planning systems are a relatively new\ntechnology, whereas academia has been using planning to solve problems for well over\n50 years. Thus, it isn’t surprising to find a large base of research that game developers\ncan use to improve their planning systems.\nOne way planners can be improved is through the use of plan merging, a tech-\nnique used in several ways under academic settings but not yet applied to games.\nUsing plan merging can allow a broader range of behaviors for automated agents and\neven let them attempt to pursue multiple goals at once. This gem examines one way\nof implementing a plan-merging system in the context of a real-time game and dis-\ncusses the implications of using such a system.\nReview of Goal-Oriented Planning Systems\nGoal-oriented action planning systems are decision-making algorithms designed to\ntake the burden of choosing particular agent behaviors off the programmer and put\nthem into the agent’s own sense-think-act cycle. The primary benefit of using these\nsystems is the reduced complexity of designing individual actions for artificial agents\nwhile retaining a high level of realism in the agent’s total behaviors.\nGoal-oriented planning lets particular agents decide their own actions through\nthe pursuit of particular goals. An agent’s goals might include destroying a target or\nobtaining an item. Goals are represented as desired world states in whatever system\nthe agent uses to keep track of the state of the world. In traditional planning systems,\nthe agent is restricted to picking one goal as being most important at any given point\nin time. Once this goal is picked, an agent can create a plan by stringing together a\nsequence of atomic actions, sometimes also known as operators.\nFor example, if your agent has decided on the DestroyTarget goal, an action it\ncould pick to accomplish that goal might be the Attack action. Actions have precon-\nditions, which describe conditions on the world that must be true before the action\nexecutes, and effects, which describe necessary conditions on the world after the\n",
      "content_length": 2306,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "action has completed. In the case of the Attack action, a precondition might be that\nthe agent’s weapon is loaded. An effect would be the destruction of the target.\nUsing the effects and preconditions as guides, any heuristic search can create a\nplan by listing a sequence of actions an agent can use to achieve the desired goal. Jeff\nOrkin describes how to use the A* algorithm for planning purposes in [Orkin04].\nThe completed plan is then just that sequence of actions the agent executes to accom-\nplish its goal.\nSome final terminology is needed before discussing plan merging. Totally-ordered\nplans are plans in which the order of each action is completely specified, such that one\nparticular action is first, another occurs second, and so on. Partially-ordered plans may\nspecify individual orderings of actions but leave the precise ordering of all actions as\nunspecified as possible. In other words, a partially-ordered plan doesn’t specify the order\nof actions unless an action satisfies the precondition of another. Totally-ordered plans can\nbe made from partially-ordered plans by giving a specific order to the actions in the plan.\nFigure 3.6.1 shows an example of some partially- and totally-ordered plans for\nmaking a sandwich. In the partially-ordered version, notice that independent actions\n(obtaining the meat, cheese, and bread) are unordered relative to each other. Actions\ncan have a relative ordering, though; all the ingredients must be obtained before mak-\ning the sandwich. In general, the only orderings given to actions are those required by\nthe actions’ preconditions. Totally-ordered plans, on the other hand, enforce a specific\nordering of all actions, regardless of whether the individual actions satisfy precondi-\ntions for other actions. Although you could obtain the meat, cheese, and bread for\nyour sandwich in any order, a totally-ordered plan specifies an order in which to per-\nform these actions. It stands to reason that any partially-ordered plan can be expressed\nas a totally-ordered one.\n282\nSection 3\nAI \nFigure 3.6.1\nPartially- and totally-ordered plans. Not all totally-ordered instantiations of\nthe partially-ordered plan are given.\n",
      "content_length": 2179,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "Although the vast majority of academic-based planning algorithms produce\npartially-ordered plans, these types of planners have not yet found widespread use in\ngames. There are a few reasons why totally-ordered plans are of more immediate use\nto an NPC:\n• First, given a partially-ordered plan, an agent will at some point have to define,\neither explicitly or implicitly, a totally-ordered plan in order to execute the actions\nof the plan. In other words, the agent still needs to choose one action to perform\nfirst among any number of unordered actions. Given this, there are some reasons\nwhy executing one action before another might be advantageous, but the reasons\nfor which the agent might put one action first could easily be abstracted into the\nplanner itself.\n• The second major reason that games have traditionally dealt with totally-ordered\nplans is the ease with which A* is adapted to creating plans. Because A* is such a\nwell known and versatile algorithm, it is an easy choice for use in a goal-oriented\nplanning system, and A* produces totally-ordered plans by its nature.\n[Orkin04a], [Orkin04b], and [Orkin06] cover the many practical details of\nimplementing an A* planning system for games. \nPlan Merging for Goal-Oriented Plans\nPlan merging refers to the process of taking several independently generated plans and\ncreating a single plan out of them, usually with the intention of reducing the overall\ncost of the plan. Often, a reduced-cost plan has the benefit of also producing more\nrational-looking behavior. To demonstrate the power of plan merging, let’s look at an\nexample before getting into the details of the algorithm.\nSuppose an agent has the task of collecting items from around the world and\nreturning those items to a home base. If the agent can carry only one item at a time, it\nis apparent that it has no better choice than to go to an item, collect it, and return to\nbase. However, if the agent can carry multiple items, it is also evident that many situa-\ntions exist where the agent could reduce its total distance traveled by collecting several\nitems at once.\nThere are several ways you could accomplish this behavior utilizing a planning\nsystem. Suppose that your goal of collecting items and returning them to base was the\nReturnItems goal. You could write a GatherItems action that accomplishes that goal.\nAn agent executing the GatherItems action would look for the nearest items, gather as\nmany as it could, and return them to base. Although this would be a solution, it is\nclear that the GatherItems action would be quite complicated. It would need to\ninclude code to pathfind and travel between items, pick up items, pathfind and travel\nto base, and drop off the items once arrived. The increased functionality contained\nwithin one action works to defeat the purpose of having a flexible planning system.\nIt is much easier to write smaller, reusable, atomic actions, such as GoTo for\npathfinding, GetItem to gather the item from the world, and ReturnItem to drop off\n3.6\nGoal-Oriented Plan Merging\n283\n",
      "content_length": 3047,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "the item at base. These multiple actions allow the planner to do the complicated work\nof stringing together the actions into the right order, and further allow you to reuse\nthe actions among many types of NPCs. Yet none of these actions can communicate\nto the agent that it should try to gather multiple items at a time. Instead, you can\naccomplish the desired behavior through plan merging.\nThe general idea is to take two plans with some overlapping actions and combine\nthe plans to produce a single plan with a lower cost than independently executing\neach of the original plans. In the current example, the agent could plan to gather each\nitem independently, producing two plans that were unrelated but very similar, as\nshown on the left in Figure 3.6.2. A possible result from a merge of those two plans\nwould combine as many actions as possible together, producing the single plan shown\non the right in Figure 3.6.2. When the agent executes this plan, it collects both items\nbefore making the return trip to base.\n284\nSection 3\nAI \nFigure 3.6.2\nTwo totally-ordered plans and the result of a possible merge between them.\nImplementing a Plan-Merging Algorithm\nAcademically, the interest in plan merging centers mostly on plan optimization.\n[Foulser92] points out two major components to optimizing a plan: finding actions\nthat can be merged and then computing the optimal way to merge the actions if there\nexists more than one way to put the operators together. It is easiest to deal with these\nproblems separately, so that is the approach taken in this article.\nThe first challenge in finding mergeable actions is discovering precisely what\nkinds of actions can be merged. Put simply, any number of actions can be merged if\nthere is another action that can replace the merged actions with these results:\n• If the action has the same useful effects.\n• If the replaced action costs less than the sum of the merged actions it is replacing.\n",
      "content_length": 1941,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "Effects are “useful” if they directly establish a precondition of another action in\nthe plan, or a precondition of the goal itself. For example, suppose an agent has a plan\nto destroy a target using the FireWeapon and ReloadWeapon actions. The Reload-\nWeapon action has a couple of effects. First, it makes the weapon be loaded, and sec-\nond, it reduces the agent’s ammunition store. The first effect would be a useful effect,\nbecause it accomplishes a precondition of another action in the plan. The second\neffect isn’t useful, because it has no bearing on the execution of the plan. \nSearching plans for mergeable actions would be incredibly expensive without\nknowledge of the actions themselves, so it is best to specifically look for actions that are\nknown to be mergeable. In an implemented system, this means either looking for a\nspecific action that can be merged with itself, or looking for a known combination of\nactions that could be merged. In the earlier resource-gathering example, you know that\nthe agent is likely to have multiple plans, each with an instance of the ReturnItems\naction. This is an excellent candidate action to look for, because you know it’s possible\nto merge two ReturnItems actions. In this specific case, you might even start the search\nat the end of the plan, because it is likely to be the last action in each of the plans that\nare being merged. GoTo(Base) can also be merged with itself, because it obviously\naccomplishes the same effect. \nThe second challenge is creating an optimal plan once a possible merge has been\ndiscovered. [Foulser92] deals with the difficulties of creating an optimal plan, noting\nthat creating an optimal plan quickly becomes computationally expensive, and probably\noverkill for games. For the resource-gathering NPC, you’ve already improved behavior\nby allowing the agent to collect multiple resources at once. Rather than spend much\ntime worrying about the optimality of the plan, you could just place the rest of the two\nplans together, as was shown in Figure 3.6.2.\nHowever, to make the agent appear even more intelligent, you could employ crit-\nics, special-purpose checks used to help order the remaining actions. For example, you\nknow you have two pairs of GoTo(Item) and Get Item actions to be placed before the\nmerged actions, so you could write a critic to make sure the agent goes to the closest\nitem first. Critics are then general rules written to enforce a desired behavior in plan\nmerges.\nAt its simplest, then, the plan-merging algorithm accepts two plans generated\nthrough the general-purpose A* planning system. The agent could send its two most\nimportant goals to the planner, for example, and then send those two independent\nplans to the plan merger. For every action in the first plan, the algorithm checks to see\nwhether it can be merged with an action in the second. If a merge can be performed,\nthose two actions are put together into a single plan, being careful to put preceding\nactions from both plans before the merged action, and likewise putting any actions\noccurring after the merged action afterward. If more precise control over the order of\nthe non-merged actions is needed, critics can be employed to determine the best\nordering and rearrange the actions as necessary. For a wider range of possible merges,\na complete plan-merging algorithm should examine the net effects of every possible\n3.6\nGoal-Oriented Plan Merging\n285\n",
      "content_length": 3427,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "group of actions in each plan, looking for situations where a sequence of actions could\nbe replaced by a single cheaper action. Such an algorithm produces the most impres-\nsive improvements to mergeable plans, but is also expensive to run.\nBeyond Single-Agent Merges\nAlthough merging two plans for a single agent certainly offers opportunities for\nimproved behavior, plan merging also offers remarkable benefits in the areas of squad-\nbased planning. For instance, an agent utilizing plan merging could merge an individ-\nual goal (picking up a weapon or health power-up) with a squad-issued goal (providing\ncover fire). Utilizing plan merging in these situations allows an agent to maintain its\nown goals and personality in the face of squad-issued orders and even allows for situ-\nations where the agent can accomplish many goals at once.\nStrategies for Improving Action Searching\nSearching two or more plans for actions with similar effects is expensive, especially if\nyou consider replacing groups of actions with different net effects. If the game is fast-\npaced, typical of many FPSs, the agent’s primary and secondary goals could change\nmore quickly than it could even devise a plan for its secondary goal. Clearly, plan\nmerging is of no use unless you can quickly perform the merge.\nOne possible strategy to reduce the time needed to search through actions is to\nlook for mergeable actions only when specific actions are present in the plan, some-\nthing that can be determined in the middle of the plan-making process. For extremely\nlong plans, hooks direct to possibly-mergeable actions could be included in the plan\nstructure itself, directing the algorithm not only into the correct places immediately,\nbut also informing it if a merge is worth looking for at all. In specific kinds of agents,\nit might even be worth only looking for a specific action to merge in each plan.\nSimilarly, you might attempt a merge only when the goals being planned for are\ncompatible. Conversely, it makes sense to not even bother to attempt a merge if the\ntwo intended goals are incompatible. Indeed, even making a plan for a secondary goal\nis wasted time if the goals are incompatible. This determination is probably best made\nby the programmer. It might be obvious to you that an Attack and a Retreat goal will\nnever produce mergeable plans, but a generically written algorithm would search\nthrough every action of each plan before reporting that no mergeable actions exist.\nConclusion\nPlan merging offers a way to improve the perceived intelligence of an agent acting\nindependently or within a squad. Although potentially a very expensive process, with\nsome careful consideration, it can be accomplished with little extra time spent exam-\nining the plans generated.\n286\nSection 3\nAI \n",
      "content_length": 2779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "It should be noted that this is only one way of performing plan merging.\n[Thangarajah02] and [Thangarajah03] present different systems and ways of per-\nforming plan merging that may be more appropriate for agents acting over a longer\nterm than the agent described here. For example, the plan-merging algorithm\ndescribed in [Thangarajah03] would be especially well suited to a strategy game AI\nopponent, able to accomplish goals in a variety of different ways and potentially delay\nactions to take advantage of positive merge opportunities.\nPlanning is a versatile AI system, with many opportunities for expansion and\nimprovement. Even if plan merging is not useful in a given situation, the ideas it suggests\nare applicable to other AI systems, or even other planning systems such as hierarchical\ntask networks (HTNs). Thinking about the behavioral improvements afforded by such\ntechniques lends the agents greater intelligence and the players a better experience.\nReferences\n[Foulser92] Fousler, David, Li, Ming, and Yang, Qiang. “Theory and Algorithms for\nPlan Merging.” Artificial Intelligence, 57(2–3): pp. 143–181, 1992.\n[Orkin04a] Orkin, Jeff. “Applying Goal-Oriented Action Planning to Games,” AI\nGame Programming Wisdom 2, Charles River Media, 2004.\n[Orkin04b] Orkin, Jeff. “Symbolic Representation of Game World State: Toward\nReal-Time Planning in Games.” AAAI Challenges in Game AI Workshop Technical\nReport, 2004.\n[Orkin06] Orkin, Jeff. “Three States and a Plan: The A.I. of F.E.A.R.,” Proceedings\nfrom Game Developers Conference, 2006.\n[Thangarajah02] Thangarajah, John, Winikoff, Michael, Padgham, Lin, and Fischer,\nKlaus. “Avoiding Resource Conflicts in Intelligent Agents,” Proceedings of the\n15th European Conference on Artificial Intelligence 2002 (ECAI 2002).\n[Thangarajah03] Thangarajah, John, Padgham, Lin, and Winikoff, Michael. “Detect-\ning & Exploiting Positive Goal Interaction in Intelligent Agents,” AAMAS ’03,\nJuly 14–18, 2003.\n3.6\nGoal-Oriented Plan Merging\n287\n",
      "content_length": 1990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "289\n3.7\nBeyond A*: IDA* and \nFringe Search\nRobert Kirk DeLisle\nG\nraph search techniques are ubiquitous in game programming. Regardless of \nthe game genre, methods of graph search inevitably form a basis for game AI.\nThe currently leading genre of 3D FPS games is heavily dependent on pathfinding\napproaches that enable non-player characters to move within the environment for the\npurpose of self-defense or aggressive action. This can also be extended to 2D (or\n2.5D) games in which maze or terrain traversal is an integral part of gameplay. Fur-\nthermore, games such as checkers, chess, Othello, and even tic-tac-toe involve some\nlevel of evaluation of game trees or state graphs in order to develop convincing and\ncompetitive artificial intelligence.\nWithin the realm of path-planning, problems typically take on the form of trees,\nwith the start being considered the root of the tree (see Figure 3.7.1).\nThe root node can then be expanded into a number of child nodes that represent\nall the next possible steps in the search. The typical 2D pathfinding process is most\nobvious with the children representing each of the directions of allowed movement.\nFor example, if the four cardinal directions are allowed, the root node expands into\nfour child nodes, one for each direction, north, south, east, and west. Diagonal move-\nments increase this to eight child nodes with the additional four representing north-\nwest, northeast, southwest, and southeast. Child nodes can be further expanded as\nyou extend the path in search of the goal. This type of problem formulation can also\nbe applied to problems such as searching for the shortest possible solutions for a\nscrambled Rubik’s Cube. The initial, scrambled state of the cube is the root, and each\npossible turn of a cube face corresponds to a child of that root node. By formulating\nproblems in this way, as graph traversal problems with starting states and goal states\nwithin the graph, you open the door to a number of algorithms.\n",
      "content_length": 1986,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": "A* and Dijkstra\nA* has emerged as the most common search algorithm for pathfinding within game\nAI. The history of A* begins with breadth first search, in which all the children of the\nroot node are expanded and evaluated before moving on to the next level of tree\ndepth. When the goal is not identified at the current depth, the next layer of children\nare expanded and evaluated. Dijkstra modified this algorithm by adding an “open\nlist” and “closed list” to provide two fundamental capabilities:\n• Each node keeps track of the cost of the path to that point, and the open list can\nbe sorted based upon this cost allowing a “best first” search strategy. This is par-\nticularly useful when transitions from one node to another do not have the same\ncost, such as choices in moving through swamp versus dry ground. Allowing\nexpansion of the best path thus far can bias the search away from costly paths.\n• Together, the open and closed lists act as a catalog of previously evaluated nodes,\nthus preventing the re-expansion of already visited nodes. These additions\n290\nSection 3\nAI \nFigure 3.7.1\nA search grid and its associated search\ntree are shown with corresponding grid and tree nodes\nsimilarly colored. A path is shown in both with arrows.\nPaths that result in redundantly visiting the same node\nhave been removed.\n",
      "content_length": 1318,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "improve upon Breadth First Search considerably; however, further improvements\nwere to be found by incorporating an “Informed Search” strategy through the use\nof heuristics.\nUp to this stage, the cost of any particular node is considered to be the cost from the\ngoal to this point, and is commonly referred to as g(). An uninformed search of this sort\nis significantly improved upon if you include an estimate of the cost from this point to\nthe goal. This heuristic calculation, typically referred to as h(), gives you another method\nto estimate the total path cost and once again significantly biases the search toward the\ngoal. The resulting overall cost of any particular node becomes f() = g() + h(), and it\ndeserves mentioning that h() should always be admissible, or an underestimate of the\ncost from the node to the goal. If h() ever overestimates the cost to the goal, searching\npotentially fruitful paths may be delayed or missed completely. A* follows the same gen-\neral algorithm as Dijkstra’s, but the cost associated with any particular search node now\nincludes the heuristic cost, that is, the estimated cost to the goal. Algorithm 3.7.1 shows\na comparison of Dijkstra’s algorithm and A*.\nAlgorithm 3.7.1\nDijkstra’s Algorithm and A*\nopen – priority queue of search nodes\nclosed – searchable container of search nodes (such as an associative\narray)\nroot = start node\npush root onto open\nwhile goal not found and open not empty\nsort open by f() of each search node\nremove top of open and set to current node\nif current node = goal\nstop\nelse\npush current node onto closed\nfor each child of current node\nif child present in closed\ncontinue\nelse\nset child’s f() = g() + h()(for Dijkstra’s, h() = 0)\npush child onto open\nIt comes as no surprise that A*’s most fundamental weakness is the management\nof the open and closed lists. The open list must be maintained in sorted order with the\ntop of the list being the node with the lowest cost. Perhaps more significantly, the\nopen and closed lists are continuously polled to determine whether a node has already\nbeen evaluated, and this can lead to a high computational burden. Although various\noptimizations of A* have been developed, the overall costs associated with the open\n3.7\nBeyond A*: IDA* and Fringe Search\n291\n",
      "content_length": 2274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "and closed lists can lead to loss of performance or even to the complete lack of func-\ntion if the search space is extremely large. Although simple 2D pathfinding problems\nmay not suffer significantly enough from these drawbacks to warrant different\napproaches, pathfinding within a complex 3D environment can easily present situa-\ntions that hinder A*’s capabilities.\nOther problems, such as searching for the shortest solution for a scrambled Rubik’s\nCube, present search spaces so large that A* quickly exceeds the memory capacity of\nthe computer within a few minutes. The 3 \u0002 3 \u0002 3 Rubik’s Cube, for example, has as\nmany as 18 child nodes for any particular search node. Even if you restrict the next\nmove to not include certain manipulations (for example, you should not allow the\nsame side to be turned twice), you can only reduce the number of children in the next\nlayer to approximately 13. This leads to over 1 billion possible states after only eight\nturns! Clearly in such complex search trees, other methods must be used in order to\nsimply enable the identification of a solution.\nThe Iterative Deepening A* (IDA*)\nAn extension of A* is Iterative Deepening A* (IDA*), as described in Algorithm 3.7.2.\nIn its most basic form, this algorithm eliminates the open and closed lists. It is true\nthat this imposes the risk of repeated evaluation of states, but this may be accommo-\ndated by properly structuring the way in which nodes are expanded (specific ordering,\nprevention of backtracking, and so on).\nAlgorithm 3.7.2\nIterative Deepening A* (IDA*)\nroot = start node\nthreshold\n= root’s g()\nperform a depth-first search starting at root\nif goal not found,\nset threshold = minimum g() found that is higher than current\nthreshold\nrepeat depth-first search starting at root\ndepth-first search(node):\nif node = goal\nreturn goal found\nif node’s f() > threshold\nreturn goal not found\nelse\nfor each child of node, while goal not found, depth-first\nsearch(child)\n292\nSection 3\nAI \n",
      "content_length": 1982,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "It may also be self-accommodating due to the fact that a node expanded early will\nhave a lowered value for g() than if it is expanded later, and should always have the\nsame value for h() regardless of when it was evaluated. In IDA*, a cost threshold is\nestablished for f() defining the maximum allowable cost above which a node will not\nbe evaluated. All nodes are expanded below this threshold and if the goal node is not\nfound, the threshold is increased. As you have no history maintained, you must reini-\ntiate the search from the original start node and expand all nodes allowed given the\nnew threshold. It may seem counterintuitive to repeat the evaluation of all previous,\nnon-goal nodes, but the cost of expanding and evaluating a node is typically much\nlower than the cost of maintaining the open and closed lists. In addition, the frontier\nnodes, those at the edge of the search that were not explored before, will always be\ngreater in number than the number of expanded nodes below the threshold. This fact\neffectively reduces the cost of re-investigation of previous nodes to a fraction of the\ncost to expand the new frontier. The ultimate result is minimal overhead in terms of\nmemory at the expense of time required for the search.\nThe Fringe Search Algorithm\nBetween A* and IDA* is an algorithm called Fringe Search (see Algorithm 3.7.3), in\nwhich nodes are expanded given a cost threshold as in IDA*, but in this case the fron-\ntier nodes are not lost. Rather, the frontier nodes are maintained in now and later lists.\nAlgorithm 3.7.3\nFringe Search\nnow – linked list of search nodes, list order determines order of\nevaluation\nlater – linked list of search nodes\nroot = start node\nthreshold = root’s g()\npush root into now\nwhile now not empty\nfor each node in now\nif node = goal\nstop\nif node’s f() > threshold\npush node onto end of later\nelse\ninsert children of node into now behind node\nremove node from now and discard\npush later onto now, clear later\nset threshold = minimum g() found that is higher than current \nthreshold\n3.7\nBeyond A*: IDA* and Fringe Search\n293\n",
      "content_length": 2084,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "The node at the top of the now list is evaluated and if its f () value is greater than\nthe threshold, it is moved to the later list. If the f() value is lower than the threshold,\nthe node’s children are expanded and the current node is discarded. The newly\nexpanded child nodes are added to the top of the now list and are thus next in line for\nevaluation.\nThis procedure maintains the list in a weakly sorted order and effectively expands\nthe nodes in a depth-first fashion much like IDA*. If the goal is not found after the\ncompletion of one pass through the now list (one iteration), the threshold is increased\nas it was in IDA*, the later list is transferred to the now list, and search is resumed\nfrom the top of the now list. Although the fringe-search process does require mainte-\nnance of the now and later lists, there is no sorting cost. Furthermore, this extra mem-\nory cost is lower than that of A*, because there is no need to store all previously\nevaluated nodes. Fringe search also does not suffer from speed losses seen with IDA*\ndue to repeated search from iteration to iteration.\nIn a study by Bjornsson, Enaenberger, Holte, and Schaeffer [Bjornsson05], these\nalgorithms were compared using game maps extracted from Baldur’s Gate II. It was\nfound that search times for fringe search were reduced as much as 25–40% compared\nto A* and 10 times compared to IDA*. These improvements in search times over\nIDA* were maintained even though IDA* was optimized to accommodate repeated\nvisits to nodes in the search tree. Overall, the gains in speed were attributed to the lack\nof needing to maintain an open list in sorted order. The cost for fringe search’s perfor-\nmance is obviously increased memory usage due to the requirement to maintain some\ndegree of the search’s history in the now/later lists. In this way, fringe search seems to\nrepresent a useful intermediate between A* and IDA*.\nConclusion\nThere is no paucity of algorithms for graph search and pathfinding. Although A* rep-\nresents the most widely used algorithm, the degree of specialization and optimization\nof the A* algorithm for individual cases expands the set tremendously. The driving\nforce behind algorithm selection has always been and will always be defined by mem-\nory and time constraints, and in nearly every case one must be traded for the other.\nIDA* and fringe search represent useful modifications of the A* family of algorithms\nand may ultimately provide advantages over traditional approaches to pathfinding.\nReferences\n[Bjornsson05] Bjornsson, Yngvi, Enzenberger, Markus, Holte, Robert, and Schaeffer,\nJonathan. “Fringe Search: Beating A* at Pathfinding on Game Maps,” IEEE Sym-\nposium on Computational Intelligence and Games (2005), pp. 125–132.\n294\nSection 3\nAI \n",
      "content_length": 2760,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "295\nS E C T I O N\n4\nAUDIO\n",
      "content_length": 26,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "297\nIntroduction\nAlexander Brandon\nG\name audio programming is becoming more complex than ever. As game audio is\ngetting closer and closer to film post production, a great deal of factors come\ninto play. Where will point-sourced specialized sounds be in relation to the camera?\nWill they be looped? Will there be a delay on them, or will they be in sequence? How\nare they triggered? How are they categorized and mixed? The authors in this section\nhave provided some very impressive fresh ways to tackle new issues like these so that\nyour titles can remain competitive with high audio quality.\nJason Page from Sony Computer Entertainment Europe provides some insights\ninto the revolutionary cell processing power of the Playstation 3 and the MultiStream\ntool Jason and his team has developed. Robert Sparks provides a vital yet elegant solu-\ntion for multiple layers of mixing groups. Especially with the different hardware play-\nback settings on multiple platforms, this particular gem is a godsend. You might have\nread recently that the pro studio standard for effects, Waves, are in use in Halo 3.\nCheck out Mark France’s article for more information on how to get this kind of real-\ntime effect functionality. Ken Noland also provides even deeper tips for optimizing\nthese effects. Finally, Stephan Schütze bangs his head against the wall of repetition,\nwhich is still all too present in games today.\nAll of these authors are respected pros in the field with ideas that could make your\nnext game the next award-winning title for audio. Enjoy the gems!\n",
      "content_length": 1554,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "299\n4.1\nAudio Signal Processing\nUsing Programmable\nGraphics Hardware\nMark France\nmark@raccoongames.com\nR\neal-time modern audio processing can sometimes be very compute-intensive, as\nmany algorithms often need to be performed simultaneously. Programmable digital\nsignal processors are usually available only to developers and are much too expensive for\nconsumers. Also, modern sound cards are still only fixed function implementations that\nevolve at a slower pace and can be limiting for audio programmers. This gem suggests\ntechniques that enable you to offload audio routines from the CPU and benefit from the\nGPU’s relatively huge SIMD (Single Instruction Multiple Data) parallel stream process-\ning power (see Figure 4.1.1). This increased flexibility allows creation of customizable,\nhigh-quality reverb models that can be calculated in real-time from scene geometry,\nrather than relying on the use of simple presets found in previous generation hardware. \nFIGURE 4.1.1\nComparison of computational power for GPUs and CPUs\n[Owens07].\n",
      "content_length": 1037,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "300\nSection 4\nAudio \nGPGPU Programming Overview\nThe GPU’s shift to a programmable pipeline and its increasing programmability has\nallowed it to be used as a powerful general purpose coprocessor. The pipeline shown\nin Figure 4.1.2 can be programmed for use in applications other than the specific\ngraphical ones it was designed for. This is known as GPGPU (General Purpose com-\nputation on a Graphics Processing Unit) programming and has been successfully used\nfor applications from artificial neural networks [Rolfes04] to cloth physics simula-\ntions [Zeller05].\nFIGURE 4.1.2\nThe recent graphics pipeline.\nFor GPGPU, fragment shaders are more useful because there are more fragment\npipelines than vertex pipelines and, because the fragment processor is at the end of \nthe pipeline, it allows for direct output. Shader programs can be written in assembly\nlanguage or high-level shader languages such as Cg, HLSL, and GLSL. My preferred\nlanguage for this purpose is Brook for GPUs, which is specifically designed for stream\nprocessing and runs directly on GPUs by generating Cg code with a C++ runtime.\nMore information on GPGPU programming can be found at [GPGPU07].\nGPU Audio Optimization\nGPU features such as multiple execution units or multiply-accumulate instructions\nare similar to those of professional audio DSP hardware [Gallo04], therefore it can be\nsuggested that the ubiquitous GPU can be used as an efficient DSP substitute.\nGPUs operate on vectors containing four floats, often representing the RGBA\ncomponents. Therefore, audio sample data is often stored in one of these components\nand 1D arrays of samples are mapped to 2D square textures before being processed on\nthe GPU.\nDoes using the GPU for audio calculations significantly optimize performance?\nWhalen [Whalen05] asked this question by using shading languages to process an\narray of DSP effects on both graphics hardware and CPU. The point was to discover\nwhich was fastest. It was discovered that algorithms such as chorus and compression\n",
      "content_length": 2012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "had a significant decrease in execution times when processed on the GPU; others such\nas Filter and Delay effects were slightly slower. The GPU excels at certain tasks that\nare suited to its model of stream processing—that is, many processors executing the\nsame code in parallel—therefore, not all audio programming techniques may be opti-\nmized by running them on the GPU.\nAudio Effects\nThis section concentrates on describing algorithms for chorus and compression audio\nprocessing effects. A chorus effect introduces a short delay and slight pitch change to\nan audio signal in order to add an audible “thickness” to the sound. The chorus effect\ncan be used in games to help create a surreal “dreamy” effect. The processing of this\neffect requires two texture lookups; interpolation between them is shown here:\nlookahead(coord, index)\n{\ncoord.x = coord.x + index * step;\nif(coord.x > 1.0)\n{\nrowsUp = floor(coord.x / rowSize);\ncoord.x = coord.x - rowsUp * (1 + step);\ncoord.y = coord.y + rowsUp * step;\n}\nreturn coord;\n}\nchorus(coord, texture)\n{\ns1 = lookUp(texture, coord);\ns2 = lookUp(texture, lookahead(coord, 20 * sin(coord.x)));\nreturn interpolate(s1, s2, 0.5);\n}\nAudio compression effects that are unrelated to data compression reduce the\ndynamic range of audio signals and are useful for balancing the game’s overall audio\nmix. This effect needs one texture lookup and the logarithmic compression calcula-\ntion to be performed:\ncompress(coord, texture)\n{\ns1 = lookUp(texture, coord);\nreturn pow(abs(s1), 1 - level / 10);\n}\nMany other audio effects, such as delay and normalization, can be optimized\nusing similar techniques.\n4.1\nAudio Signal Processing Using Programmable Graphics Hardware\n301\n",
      "content_length": 1700,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": "Room Acoustics\nAnother type of audio-processing technique that could be made more efficient using\nGPU is calculating real-time room acoustics, as demonstrated by [Jedrzejewski06]. \nCalculating echoes, occlusions, and obstructions in real-time from environment\ngeometry requires a lot of computation; a ray tracing method can be used to imple-\nment this, which is well suited to GPU processing. Ray tracing for acoustics is differ-\nent from graphical ray tracing because the scenes that are computed don’t need to be\nvisually accurate and smaller render targets are often used. Rays are traced from the\nsound source until they reach the listener’s position.\nPrecomputation\nThe scene geometry consists of polygons that represent walls; other game objects that\nare considered large enough to affect the audio environment can be approximated as\nboxes. During the precomputation stage of this algorithm, the geometry is partitioned\nto a BSP tree with solid convex regions for leaves. After the BSP tree is computed, it is\nused to create a portal graph that shows the paths between each leaf. If a portal and\npolygon lay on the same plane in a certain leaf, additional leaf splits need to be made;\nnew portal and paths computation might be needed if there is need for additional leaf\nsplitting. Information on portals and planes is stored in separate 1D textures that con-\ntain data such as whether it is a portal or plane, and its absorption values. The leaf data\ncontains indexes into the plane texture and how many planes it contains. This stage\ncan be performed every time the scene geometry is changed.\nReal-Time Rendering\nFragment shaders are executed that first compute intersections in the current leaf for\neach ray, and then the propagation to new leaf, and then the reflected ray and intersec-\ntion with listener. The listener’s position can be approximated as a bounding sphere;\noften the bounding volume for the player’s avatar is used if the listener object is\nintended to represent the player. Pseudocode for the shader programs is shown here:\nLeafPlaneIntersect(Ray)\n{\nGet plane index for current Ray\nfor (i=1; i<=6; i++)\n{\nIntersect with plane for current leaf\nStore data for closest intersected rays\n}\n}\nPropagateRay(Ray)\n{\nCheck if currentLeaf contains more planes\n302\nSection 4\nAudio \n",
      "content_length": 2298,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": "if(currentLeaf == listener.leaf)\nIntersect Ray with boundingsphere\nif(intersection with plane)\nReflect ray and its absorption\nIf(intersection with portal)\nSet new leaf for ray\n}\nThe environmental reverberation model is then constructed. It involves retrieval\nof the render target texture and final ray data. Three render target textures are used,\none for state information, one for the ray origin, and one for the ray direction.\nConclusion\nNot all audio algorithms can take advantage of the GPU’s parallel computation; how-\never, certain tasks such as some audio effect algorithms and acoustical ray tracing excel\nwhen executed on graphics hardware. Other than the audio techniques described in\nthis gem, GPUs have also been shown to greatly outperform CPUs for techniques\nsuch as FFT (Fast Fourier Transforms), which are ubiquitous in audio processing.\nWith PCI-Express cards becoming common, transfering large amounts of data from\nvideo memory to system is no longer a significant bottleneck. These techniques show\nthat the GPU can be utilized as a practical optimization for many audio algorithms\nand even a feasible replacement for specialized audio DSP hardware.\nReferences\n[Buck04] Buck, Ian, et al. “GPGPU: General Purpose Computation on Graphics\nHardware,” SIGGRAPH, 2004.\n[Gallo04] Gallo, Emmanuel, and Tsingos, Nicolas. “Efficient 3D Audio Processing\nwith the GPU,” Proceedings of the ACM Workshop on General Purpose Com-\nputing on Graphics Processors, ACM, 2004.\n[GPGPU07] “General Purpose Computing Using Graphics Hardware,” available\nonline at www.gpgpu.org.\n[Jedrzejewski06] Jedrzejewski, Marcin, and Krzysztof, Marasek. “Computation of\nRoom Acoustics Using Programmable Video Hardware,” Computer Vision and\nGraphics, Springer Netherlands, 2006.\n[Owens07] Owens, John D., Luebke, David, Govindaraju, Naga, Harris, Mark,\nKrüger, Jens, Lefohn, Aaron E., and Purcell, Tim. “A Survey of General-Purpose\nComputation on Graphics Hardware,” Computer Graphics Forum, 26(1), pp.\n80–113, March 2007.\n[Rolfes04] Rolfes, Thomas. “Artificial Neural Networks on Programmable Graphics\nHardware,” Game Programming Gems 4, Charles River Media, 2004.\n4.1\nAudio Signal Processing Using Programmable Graphics Hardware\n303\n",
      "content_length": 2216,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": "[Whalen05] Whalen, Sean. “Audio and the Graphics Processing Unit,” available at\nhttp://www.node99.org/projects/gpuaudio/, 2005.\n[Zeller05] Zeller, Cyril. “Cloth Simulation on the GPU,” SIGGRAPH, NVIDIA\nCorporation, 2005.\n304\nSection 4\nAudio \n",
      "content_length": 242,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": "305\n4.2\nMultiStream—The Art of\nWriting a Next-Gen Audio\nEngine\nJason Page, Sony Computer Entertainment,\nEurope\nJason_Page@scee.net\nF\nor the past three years, the SCEE R&D’s audio team has been writing a “next-\ngen” audio engine for the Playstation 3 that was to be part of the official SDK. My\naim for this gem is, having been a part of the SCEE engine project, to give you an idea\nof the work involved in designing and creating your own audio engine. In turn, this\ninformation may be useful in allowing you to create your own audio engine, or by\nknowing the magnitude of the job depending on your goals, you might decide to use\nsomething from an SDK or middleware provider instead.\nI’m not going to cover the MultiStream function calls in detail—any licensed PS3\ndeveloper can look at the docs at any time, but I would like to bring your attention to\nthe issues that my team had to overcome. At the end of this gem, I will look at new\nproblems that might also need resolving due to the expectations of next-gen audio.\nAt the very beginning of the project, we had no idea what hardware would be avail-\nable for us; no idea of how much RAM we would require or the expected performance.\nWe decided to take the approach of creating everything in software and to expect\nthere to be no help from hardware DSPs. Later, we found that this was indeed the cor-\nrect choice, as there was to be no audio hardware in the PS3. I also find that planning\naudio engines around known hardware or game requirements can mean that the final\nresult is rather mediocre. If you know that a game requires a low-pass filter for occlu-\nsion and obstruction but don’t allow the capability for such a filter to handle high-\npass, band-pass, notch—or indeed the many other filter types—how many creative\npossibilities have you lost? Thinking big means you can trim down areas that might\nnot be feasible in the long term, and also means you’ve got something different from\neveryone else. \nA “stream” in MultiStream consists of audio data to play (up to eight channels),\nplayback frequency, volume parameter/surround sound position, amplitude envelope,\nDSP effects, and output routing locations.\n",
      "content_length": 2165,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": "How It All Began\nApart from the technical aspects of creating an audio engine, we also had to decide on\nother features to include to make MultiStream “next-gen.” The following sections\nexplain a little more about the questions we had to answer. Again, the idea of creating\nyour own audio engine might be appealing, but such a project could end up taking years\nto complete, and there are many things that you need to have planned for in detail first.\nHaving a team working on an audio engine for three years does not come cheap. The\nfollowing sections describe the design process we used before writing any code.\nUnderstanding “Next-Gen” Audio\nAlthough it should seem simple, creating an audio engine that makes games sound\nbetter than ever before isn’t as straightforward as it may seem. The ability to play CD\nquality audio tracks has been available to game developers for over a decade. The abil-\nity to add high-quality reverb (although perhaps not to the standard of professional\naudio plug-ins by companies such as Yamaha or Lexicon) to hundreds of audio chan-\nnels has been with us since the late 90s. Would just upping these values create a “next-\ngen” feel? After all, it is not we who decide this—it’s Jimmy and Jenny who just spent\n$60 on a game and need to be impressed.\nIf you are thinking of writing your own audio engine, first ask yourself what it\nneeds to do. For MultiStream, one purpose was to allow game audio to sound better\nthan, and in a certain sense, different than current generations. More channels but with\nthe same audio capabilities as previous audio engines—this might make a game sound\nbetter by creating a richer environment, but it’s unlikely that it will really stand out of\nthe crown as being “next-gen.” Sure, you can do what you want with offline processing,\nbut the real power of next-gen is to do it all in real-time. There’s a whole load of great\nsounding effects like vocoders or convolution reverb that have never been done before in\nreal-time, but these all need frequency domain processing. This has previously been seen\nas impractical to run in real-time along with a full game, but we wanted it. It soon\nbecame obvious that next-gen audio means gaining expertise in a number of areas we’d\nnever had to worry about before.\nWish Lists\nFrom my experience with audio on the PS3, it seems that a good approach for anyone\nto take is to make a wish list of what kind of audio processing they require. At the\ntime of writing, it seems like just about any type of audio process is not only possible,\nbut is also possible in real-time. To give you an example, MultiStream can process over\n50 mono convolution reverb effects in real-time. However, this also means that there\nis no processing left for any audio channels! But, it does mean that even if you require\none convolution reverb, which was previously thought of as not being possible for\ngames, it is now a reality. Programming audio on the PS3 does literally allow for new\napproaches to audio, where techniques that had previously only been seen in profes-\nsional music packages can now be used.\n306\nSection 4\nAudio \n",
      "content_length": 3111,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": "How Many Audio Channels?\nIt is presumed that for any audio engine, it must be able to process enough audio chan-\nnels of data to meet expectations. Yes, more channels do help produce “better audio,” but\nas Mozart once said, “The silences between the notes are as important as the notes them-\nselves.” Even so, today’s (and tomorrow’s) game requirements mean more audio channels\nare required for creating the same game sounds as before. It would be reasonable to think\nthat a car engine sound might be created with at least 25–30 audio channels:\n• Car engine rev loops * eight (each for, say a 1000 RPM rev range)\n• Car exhaust loops * eight (recorded at the same time as the car engine)\n• Skid sounds (four looping skid sounds, one for each wheel)\n• Road rumble sounds (four sounds, one for each wheel)\n• Gear changing noises\nDue to the number of channels required for a single car in the standard race\ngame, it was previously only the player’s car that used such a detailed model. All other\nAI cars might be using a far lower channel count, due to hardware constraints, CPU\nand/or RAM constraints.\nToday, this is not such a problem. MultiStream has limited its maximum channel\ncount to 512. In a racing game, this would make it possible to handle 20 race cars, all\nwith the same audio capabilities as the player’s own car. Of course you could go beyond\n512 voices (depending on platform you’re developing for), but you have to draw the line\nsomewhere. In our case, sticking with this limit still means there’s plenty of processing\ntime to spare for DSP effects, buss routing, re-sampling, and amplitude envelopes. \nFinally, in the case of car engines, it must also be noted that by the time you read\nthis, the method of cross-fading loops for engines may well be a thing of the past.\nMethods that use granular synthesis techniques, whereby playback of small sections—\nor “grains”—of a car engine sample, create a far more realistic engine sound than loops\nalone. Again, this was not really possible until now. The RAM footprint required for\nsuch samples without the ability to use file formats such as MP3 or ATRAC3 meant\nthat these techniques, although tried and tested in theory, used too much RAM.\nSample Formats\nPlayback of an audio file must also take into consideration the format of the sample\ndata and the number of channels. Note that stereo files do not necessarily take twice\nas much processing or RAM as mono files; this depends on the file format. MP3 joint\nstereo mode for example records some of the audio in mono, where if the left and\nright channels contain the same frequencies, there’s no need to store them twice.\nActually, there are indeed many books and Websites explaining why you should store\nthem twice, but again, explaining this would take too many pages! \nFor game audio, one of the main issues is accessibility. Sample accurate playback\nis something you really need to aim for. For the MP3 format, it is relatively simple to\nplay back audio approximately +/–1000 samples from where is required. Therefore,\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n307\n",
      "content_length": 3099,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": "extra work (and RAM and CPU) is required to make MP3 fully sample accurate, or as\nI like to call it, “game-compatible.”\nThe file formats your audio engine accepts also have to be considered, as shown\nin Table 4.2.1.\nTable 4.2.1\nAudio Engine File Formats\nFormat\nPros and Cons\nNotes\nFloat32 PCM\nPros:\nNo decoding required\nBest quality audio\nEasy to loop to sample boundaries\nCons:\nLarge memory footprint\n16-bit PCM\nPros:\nGood “CD-quality” audio\nEasy to loop to sample boundaries\nSmaller memory footprint than Float32\nCons:\nStill quite a large memory footprint.\nUnlikely that a game will have enough \nRAM to store all samples in this format.\nADPCM\nPros:\nPassable quality\nSmaller memory footprint than PCM\nQuite fast to decode\nCons:\nPossible that decoders only handle \nmono input files\nHigher CPU overhead required for \ndecoding\nLooping to sample boundaries may \nnot be possible\nMP3\nPros:\nGood quality\nExcellent compression\nMany decoders can handle multiple \nchannel data\nCons:\nHigh CPU overhead\nNot easy to seek to sample accurate \npositions\n308\nSection 4\nAudio \nFaster processing means more \nCPU spare for other tasks.\nMore usable in games than float32\nformat, but in many cases, the \nlisteners aren’t going to notice \nthe difference.\nIn many cases, this is still used as \na standard game audio format. It \noffers compression and a fast decode.\nSample accurate seeking or looping\nmight not be possible; it does not\nrequire too much tweaking of the\ninput data to align loop markers to\nboundaries.\nBest for getting as many sounds in\nRAM at one time, but you must\nconsider the processing required to\ndecode such formats.\n",
      "content_length": 1616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": "It also has to be noted that codecs such as MP3 require data buffers per audio\nchannel too, where decoded data and other information needs to be stored. Consider-\ning that MultiStream can play 512 MP3s at once, even if each audio channel only\nrequired a 2KB buffer, the codec still requires 1MB. Although this may seem obvious,\nit is areas such as this that are best explained to game producers and designers early on\nin the development cycle when they request such codecs. \nAlso, as shown in Table 4.2.1, although float32 input would offer the best qual-\nity, another issue can be DMA bandwidth (a method for transferring data around a\nsystem). In which case, using 16-bit data could half the bandwidth, while still produc-\ning audio of CD quality.\nLoop markers need to be considered too. The loop markers may be stored in the\nfile header (such as .WAV), within the sample data (such as the SCE’s .VAG ADPCM\nformat), or not at all (such as .MP3s converted from .WAVs, where the loop informa-\ntion is lost). Handling looping of audio is not as simple as it may seem. If the sample\nis memory resident, you just play the sample and know where in RAM the address of\nthe loop position is. If you are streaming audio content, care needs to be taken so the\nloop point is in memory when it comes time to loop.\nTo Stream or Not to Stream\nMost audio systems need to be aware that data may be streamed. Here, your audio\nengine has to cater for some kind of buffer mechanism, where data is copied into an area\nof RAM for playback (this data is normally loaded from disk, but there could also be\nPCM data obtained from a decoded .MP3 via a codec outside of your audio engine).\nFrom experience I would not recommend handling data-loading functions\nwithin your audio engine. If the audio engine requires more data for a streaming\nbuffer, it should request this. (In MultiStream, this is handled via a callback function.)\nIf you start handling data loading in your audio engine, expect a world of pain later\non. Here’s why:\n• You need to sync other game data-loading with your audio engine loading.\n• You need to handle all cases of corrupt data loads (disk removed during loading or\na damage disk). \nEssentially, your audio engine becomes far trickier to optimize and maintain. It\nmust also be noted though, that any audio streaming needs to take priority over any\nother data loading. Why? Simply due to the fact that if audio is not streamed in time,\nyou will have to either repeat playback of the last buffer of data (which sounds like a\nbroken CD player) or play silence. Either of these may well cause your title to fail dur-\ning any QA process.\nEven if your audio system is not going to handle streaming of data from disk\ndirectly, the programmer(s) in charge of the IO systems must be aware of the follow-\ning priorities in order of importance:\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n309\n",
      "content_length": 2900,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": "• Currently playing streams must be updated first\n• Newly requested streams can be updated next\n• Data load for the game happens last\nUsing this method, if a player keeps requesting more audio to be streamed, say,\neach game frame, any currently playing music will still play correctly without skipping\nor jumping. In many cases, streaming is used for areas such as sports commentary. This\nis also usually in context with the current action on-screen, which is why the playing of\nsuch audio is, as far as I am concerned, more important than game data loading. There\nis nothing worse than a commentator saying the wrong thing at the wrong time. \nThe size of stream buffers is not an exact science. This will depend on the sample\nrate and format of the data you are streaming, along with how often you expect to be\nloading data. Streaming data from hard disk is by no means as tricky as loading from\nDVD, where the time taken for the DVD head mechanism to physically move to the\ncorrect place and the time taken for data to load is also a factor. In many cases, having\nmultiple copies of the same file on a disk is a common technique for speeding up\nDVD loading. The current head position is kept track of in software, and then the\nstreaming engine (note that the streaming engine and the audio engine are separate\nengines) will choose which file on the disk is closest to this position.\nThis method can also help with prioritizing audio data streaming. If multiple\naudio channels need more data, you need to choose which should be the first to be\nloaded. Again, this is not an exact science. If multiple streams require more data, then\nyou need to make sure that they all get that data as soon as possible. If you can’t \nload the data in time, a simple solution is to either increase the stream buffer sizes or\nreduce the sample rate of the audio. Halving the sample rate has the same effect as \ndoubling the stream buffer size. For example, playing 48000 samples at 24kHz will\ntake twice as long as playing 48000 samples at 48kHz.\nThe method of reducing the playback frequency of a stream is also very useful for\ndetermining whether pops or clicks in audio playback are caused by the system run-\nning out of data to process. This modification is normally very simple to make to any\naudio calls, compared to increasing streaming buffer sizes, which can be limited in size\ndue to the restrictions set by other non-audio game requirements. \nFor streaming audio with loop markers, depending on the sample data format,\nthe only time you might know that you need to loop the data is when you’ve reached\nthe loop marker, which is too late. For MultiStream, we decided to ignore all loop\nmarkers within the audio engine. It is therefore up to the user to either decode .WAV\nheaders, or stream correctly to the required data. Not only does this allow the user to\nfeel in control, but it also takes care of any of the issues mentioned previously.\nSo if we are required to loop to a certain offset within a file, we can first check to\nsee if that portion of the file is indeed in RAM. If it isn’t, we need to load this portion\nfirst so that playback will continue as desired. \n310\nSection 4\nAudio \n",
      "content_length": 3186,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": "Volume Parameters\nSetting volume levels of an audio channel, along with setting its frequency, are the\ntwo most basic audio DSP effects.\nFor MultiStream, you still had a number of issues to decide upon: As the PS3 can\noutput audio up to 7.1, you needed to allow any audio channel to be routed to any\nnumber of speakers. For example, a mono audio signal may want to be heard on both\nthe front-left and the rear-right speakers. This means that any single audio channel\nrequires eight volume parameters. Furthermore, as MultiStream can play data con-\ntaining up to eight audio channels, there are a total of 64 volume parameters available\nper stream. Finally, MultiStream can process up to 512 channels and these volume\nparameters can be Float32s. This means that 512 \u0002 64 \u0002 4 bytes are required just for\nvolume parameters alone.\nYou could reduce the memory footprint if you used 16-bit volume parameters, or\nmaybe even less. Imagine that MIDI volume parameters range from 0–127, giving\nyou 128 possible settings. Why do you need to use floats that give you millions of\npossible settings? First, you must ask when you set a volume parameter in MIDI, is\nthe hardware (or software plug-in) using this value directly? It could be that this value\nis then scaled to work in a floating point system where volumes are ramped toward\nthe required volume level. Secondly, for ease of use, having a system that uses floats\ncan make the rest of the audio engine quicker in general. There will be less need for\nconversion of volume levels between various formats, for a start. \nPlayback Frequency\nAs stated in the “Volume Parameters” section, volume and frequency are the two most\nbasic audio DSP effects.\nWith a purely software-based system, even setting the playback frequency of a\nsample needs consideration. Any resampling is going to take CPU time and it would\nbe foolish to waste this time on such basic functionality. Not only does the resampling\nalgorithm need to be considered, but also the fact that playing back audio at high fre-\nquencies can in turn take longer to process. Therefore, a system that can process 4000\naudio channels may only be able to do so at a maximum playback sample rate of, say,\n48kHz.\nTo explain a little more, if you need to create one second worth of audio data for\nplayback at 48kHz, you need to process 48000 samples to do so. If you need to play\nback at 96kHz, you need to process 96000 samples. “Processing” in this sense could\nmean decoding of MP3 files. So again, playing back a 48kHz MP3 at 96kHz means\nyou need to decode twice as much of the MP3 file.\nPerhaps this processing time could have been spent more wisely? If you require a\nsample to be played at an octave higher than its original pitch, it may make sense to\nresample it down to 24kHz, which in turn means that going one octave higher would\nput it at 48kHz. Simply put, by halving your audio files sample rate, you can cut the\nprocessing required to resample in half.\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n311\n",
      "content_length": 3020,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": "Frequency Domain Processing\nFor any frequency domain processing, you are looking at requiring a FFT/iFFT rou-\ntine. Understanding the fine detail of the FFT is not as important as it may seem. Yes,\nthere’s a lot of math involved here, but once written, it’s not something that you really\nhave to worry about again. Using it is a different issue.\nThe main problem with frequency domain processing is choosing the correct win-\ndow size. If the effect you’re working on needs high-frequency resolution (for example,\nsay you’re implementing some kind of parametric EQ), you need a large window. Large\nwindows give very poor time resolution, so it’s not possible to change parameters\nquickly. Large windows also result in greater latency, require more memory, and use\nmore CPU. Although shorter windows do not suffer from these problems, they lead to\npoor frequency domain resolution, which defeats the objective of trying to implement\na frequency domain effect.\nThe answer really lies in finding the right window size for your application, tuning\nit to make the best use of the available resources, and listening to hear if it sounds right.\nEven then, different effects may require different window sizes. Are you prepared to\nswitch window sizes in the signal path, or is a “one-size-fits-all” solution good enough?\nBasics of FFT\nThere are a number of issues to consider when using FFT. First, the number of input\nsamples needs to be double the number of output samples. So, for example, you need\nto feed the FFT 1024 samples for it to output 512. They have implications for other\nroutines too. For things like amplitude envelopes, you may need to actually process all\n1024 samples but then rewind the envelope parameters by 512 samples so that the\nnext time the amplitude envelope is processed, you are using the correct values.\n312\nSection 4\nAudio \nFIGURE 4.2.1\nSimple amplitude envelope\n(fade in/fade out) over 2048 samples.\nThe very big plus point of FFT (and windowing) is that you’ll find it can remove\na lot of possible pops and clicks normally heard with large volume changes or looping\nto boundaries where samples do not match up.\nAs you can see in Figure 4.2.2, on each step, the envelope needs to be re-calculated\nfor the first half of the data packet, even though it has already just calculated it for the\nsecond half of the previous packet.\nNote that some systems would also include a step before this, where, being the\ninverse of the last step, the fade-in part of the amplitude envelope would only process\nthe first 512 samples. This is illustrated in Figure 4.2.3.\n",
      "content_length": 2577,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": "This step adds a lot of latency to the final output, which we found to be far too\nnoticeable in real-time applications.\nLatency\nReal-time applications obviously require a low latency. For MultiStream, we decided\nthat it should generate 512 samples per channel each time the update routine is\nprocessed (this technique is known as granularity). When using FFT, outputting 512\nsamples requires 1024 input samples due to windowing functions (see the section\ncalled “Basics of FFT”).\nWith 512 sample granularity, this gives the FFT function enough data to meet\ntwo goals:\n• Latency is low enough for most game requirements.\n• 512 bands (where MultiStream requires 1024 samples as input data) gives enough\nscope for many FFT-based DSP effects but keeps the quality high. If you drop to\n256 bands (512 samples as input data), you would find the audio quality to be too\npoor to be of any use for just about any application.\nProcessing of 512 samples means that the update routine will need to be called\n93.75 times per second (every 10.66 milliseconds):\n48000 samples = 1 second of playback\n48000 / 512 = 93.75 Hz (Number of audio updates required per second)\n1000/93.75 = Audio engine will be called every 10.66 milliseconds\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n313\nFIGURE 4.2.2\nAt least four passes of the data are required when processing in 1024 sample\npackets if you’re using windowing techniques.\nFIGURE 4.2.3\nSome\nsystems use a fading that\nprocesses only the first\n512 samples.\n",
      "content_length": 1500,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": null,
      "content": "This is generally fast enough to keep the audio in sync with any graphic updates\nrunning at a maximum of 60 updates per second. Even though outputting 512 sam-\nples may seem like an easy task (remember that there are 48000 samples required for\none second of playback), processes such as MIDI sequencers run at a faster rate than\nthis. In many cases, they run up to 240Hz or even 384Hz (between 2–4 milliseconds!)\nTherefore, the problem may be that if a MIDI sequence requires an instrument to start\nplaying, it will not actually start until the next audio update. Now, many people will\nnot notice this, but those who have very good hearing (such as the audio engineers who\nare going to be listening to their work played through your audio engine) will notice. If\na lower latency than 512 samples is required, FFT processing may not be the one for you. \nFor MultiStream, we have both frequency and time domain processing modes. So\nif you do not require frequency domain effects, it is possible to process totally in time\ndomain. This means window size is no longer an issue and we can offer optional gran-\nularity settings of 128 or 256.\nPacket Smoothing\nAs discussed in the “Latency” section, granularity is the number of samples generated on\neach audio update. On the simplest level, each update would use the settings the user\nhas required for each audio channel, such as what frequency and volume with which to\nplay back. One issue to consider here is that if each packet just uses the required volume,\nit is possible to get aliasing artifacts due to the sudden jump of volume. Another artifact\nis clicking or popping, which is noticeable on audio such as car engines where multiple\naudio channels would be cross-faded depending on the motor rev required.\nFor time-domain processing, a filter process is required so that volume changes\nare smoothed, whereas for frequency domain processing, you will find that the win-\ndowing which is required for FFT (such as a hamming or hanning window) does all of\nthe hard work for you.\nAs first discussed in the “Frequency Domain Processing” section, windowing\ntechniques are used when processing frequency data. The reason for windowing when\nconverting to frequency domain is that when you process the data, you only focus on\na single portion of the data. Analysis therefore knows nothing about what audio sig-\nnals proceeded or follow this data and if you don’t take this into consideration, there\nwill be discontinuity between each data packet (known as pops and clicks to you and\nme). Window types such as hanning or hamming are essentially just algorithms used\nto modify each packet of data. Each packet of data is then processed and mixed with\nthe previous packet, producing an output which resembles the desired data. This is a\nreally simplified paragraph on what would normally require chapters in other books,\nbut hopefully there’s enough information here to give you something to Google with!\nWindowing may also mask a multitude of sins that normally cause pops and\nclicks to be output, such as looping samples whose start and end samples do not\nmatch. Note that care must be taken here still. Although looping to any sample might\nsound fine when using window techniques, if for any reason you need to move your\n314\nSection 4\nAudio \n",
      "content_length": 3285,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": null,
      "content": "audio engine to a pure “time domain” mode, where no such windowing or filtering\ncolors the audio output, you will hear these pops again. Source data that loops per-\nfectly by default is always preferred.\nSurround Sound\nConsideration must be taken on how to handle surround sound. There are two main\napproaches to take:\n• User supplies X, Y, and Z coordinates of both the source and listener positions\n• User supplies an angle and distance for the source compared to the listener position\nMultiStream uses the X, Y, Z approach, using OpenAL 1.1 algorithms, although\nit would also be sensible for such a routine to accept either approach considering that\nthe X, Y, Z system creates a surround sound panning position (angle) and an overall\nvolume (distance) from this position anyway.\nProcessing multi-channel audio in surround sound must also be considered.\nAgain, MultiStream will fold multi-channel audio down to a single point source\nif it requires positioning in surround sound. Another way to handle multi-channel\naudio is to play each channel as mono (for example, channel 0 = front-left and chan-\nnel 1 = front-right for a stereo channel), and set the surround sound X, Y, Z position\nfor each speaker. Figure 4.2.4 shows six channels of audio.\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n315\nFIGURE 4.2.4\nSix channels of audio.\nSplitting the .WAV into separate\nchannels, you can position each\nspeaker’s position in the game world\nto replicate the desired effect.\nThis approach can be also used for car race games, where moving the camera\nposition from behind to inside a player’s car means all the audio playback works cor-\nrectly, such as the exhaust being heard from behind the player (see Figure 4.2.5).\nFor certain types of games, a common approach for game audio is to keep non-\nplayer audio as mono (point source) and player-specific audio can be multi-channel if\ndesired. As the player is always in front of a camera, it is safe to presume that no surround\n",
      "content_length": 1984,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": null,
      "content": "sound processing of their audio is required and just playing their audio as stereo will be\nfine. Not only does this allow for higher quality samples, but it also reduces processing\noverheads because there are fewer surround sound objects in the game world.\nSyncing Channels\nOne problem that often occurs in audio programming is being able to sync multiple\nchannels. This allows the starting, stopping, and pitch changing of multiple channels to\nhappen at the same time. You might hear phasing or chorus effects if this is not taken\ninto consideration.\nThe reason for this can be seen in Figure 4.2.6.\n316\nSection 4\nAudio \nFIGURE 4.2.5\nChannel location relative to the player.\nFIGURE 4.2.6\nChannels can become out of sync if the audio engine updates between play\naudio commands.\nIn Figure 4.2.6, you can see that two audio channels have been requested to play,\nbut due to the audio engine’s update routine firing in between the initialization of\nthese two audio channels, the output of “Audio 1” is now one data packet ahead of\n“Audio 2.” In real life MultiStream terms, this means that “Audio 1” is 512 samples\nahead of “Audio 2.” This can also occur if you pause and resume channels, or set the\npitch of multiple channels, except that in both of these cases it is possible for the audio\nto drift farther and farther out of sync!\n",
      "content_length": 1330,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": null,
      "content": "For a solution to this problem, you need to make sure that any phase-causing\nfunctions (the Play or Pitch Change functions) are not split by the audio update rou-\ntine. The simplest method for this is to have two functions:\nVoid Sync_On(void)\nVoid Sync_Off(void)\nHere, any Play or Pitch functions called between these calls are “remembered”\nand are processed in the next audio update function after Sync_Off, as illustrated in\nFigure 4.2.7.\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n317\nFIGURE 4.2.7\nThe Play Audio commands are queued up to be synchronously started during\nthe same audio engine update.\nAs you can see in Figure 4.2.7, “Audio 1” has now waited until the “Sync Off”\nfunction has processed, which means both channels are now playing in sync as desired.\nDSP Effects\nDSP effects separate the “next-gen” from current or last-gen titles. The processing\npower available, again from my experience on the PS3, means that it is possible to\nprocess audio in real-time, and using a minimal amount CPU at the quality normally\nonly experienced in professional effect units.\nThe purpose of this gem is not to discuss each DSP effect. There are many books\nalready available covering filter design, FFT and so on. Therefore, I will leave it to you\nto research this area.\nRest assured, having only a low-pass filter to use as occlusion/obstruction is not\ngoing to make your title sound next-gen. You will need to go a little further to impress\npeople! Just think about the amount of DSP effects available for general music or\nsound effect creation and then think of how any of these effects could be used within\nyour game title. Think about every room in every level having its own reverb type, for\nexample. As “anything is possible,” a good start is having programmers communicate\nto audio engineers about what effects they would like to see in real-time and why.\nOf course, processing DSP effects in real-time also means that there is less pre-\nprocessing required for audio samples. Considering that a game title may contain tens\nof thousands of samples, it can make sense to process these in-game, allowing the\n",
      "content_length": 2130,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": null,
      "content": "developer to tweak and change parameters at will, rather than needing to go back to\nthe audio engineer and ask for changes or just put up with an effect that’s close\nenough to what you want. Imagine a sample of a human voice that you decide would\nsound better if it were talking through a radio headset. Having the ability to test these\neffects without the need to waste time pre-processing data not only speeds up devel-\nopment, but also allows for far more creativity when creating your audio.\nRouting\nThe number of busses an audio channel can be mixed to cannot be underestimated.\nFor MultiStream, we currently have 31 sub-busses and one master buss. It is already\nbecoming apparent that these values should be increased in the future. The grouping\nof sound sources has previously been used for volume scaling. For example, all SFX\nwould route to one bus, all music to another, and all commentary to another. The vol-\nume parameters can then be modified in, say, game “option” menus and will then just\nset the volumes for these busses, scaling all audio playing through them.\nToday, with the number of audio channels required for creating things like car\nengines, busses can be used for far more than just volume scaling. By adding DSP\neffects to busses, it is easier and less CPU intensive to set such effects for all of these\ncomponents in one go (see Figure 4.2.8). Imagine a car game where you see a car go\nbehind an object. Instead of processing low-pass filters for 30 or more audio channels,\nyou could just do it once.\n318\nSection 4\nAudio \nFIGURE 4.2.8\nPutting DSP effects into the buss can reduce the amount of processing\ndone per channel.\n",
      "content_length": 1651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": null,
      "content": "Conclusion\nCreating a good master mix is still seen as something of a black art. Indeed, it can be seen\nas something that you will never get right. It should go without saying that games, unlike\nfilm, are unpredictable. You can never be sure where the camera is pointing or which\nsituation the player is in. Trying to work out what audio should be heard is not simple.\nDucking techniques have been used previously to provide a little clarity. Most\nsports titles will automatically reduce the volume of all other audio whenever com-\nmentary is played. This has previously been handled by a simple “if I am playing\ncommentary, reduce all other volume by x percent” approach. In the real world, a\nducker (or side chain compressor) would be used. This analyzes the audio input signal\n(in this case, the voice of the commentator) and then reduces another input signal (all\nother audio) accordingly.\nThis technique can now easily be introduced into a next-gen title and gives a far\nmore realistic result. The previous method does not check for what commentary is\nplaying, it just knows it is. If there is a long silence in the commentary audio sample,\nall other audio volume will still be reduced. Using a ducker DSP effect will not cause\nthis problem.\nPriority systems can also be used to make sure that you hear audio that’s more impor-\ntant to a scene. The choice of what is important in a scene is still really up to the game\nengine. For example, imagine a game where 10 enemies who are all the same distance\nfrom the player are shooting; you may need to choose which ones are more important.\nPerhaps you need to order this by the direction the enemies are shooting or by what kind\nof weapons they are shooting (laser rifles being more powerful than pistols perhaps).\nThe number of priority levels is also a factor (where, say, a higher level will give\none sound priority over another). I have previously written systems that give the user\n256 priority levels for any SFX. Although this feels like a good idea, in practice it is\nnot common for there to be any noticeable difference between using a priority level of\n122 compared to 121. A smaller range of something like 0–7 is far more usable.\nMixing the two techniques of both the ducker and a priority system can allow\nyou to automate a master mix. Here, a number of busses are used—one buss for each\npriority level. On each buss apart from one (which has the highest priority), a ducker\nis placed and each buss also feeds into the adjacent buss. Buss 0 will duck busses 1–6.\nBuss 1 will duck busses 2–6. Buss 2 will duck busses 3–6, and so on. Simply by mak-\ning sure your audio routes to the selected buss, it should be possible that volume lev-\nels are controlled correctly. This requires minimum input from the users; they just\nselect the buss for audio to route to in the same way as you select the sound’s priority.\nUnder MultiStream, this would be a feasible routing and DSP setup, although I\nadmit that there are still other considerations. Other busses may contain reverb effects\nand you will need to know how to route from the six priority busses to these other\nbusses. Even so, I believe this is an area that may well make games feel far more “film-\nlike” with regard to post-production values, and it is only possible to do this now,\nunder the next-gen banner.\n4.2\nMultiStream—The Art of Writing a Next-Gen Audio Engine\n319\n",
      "content_length": 3387,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": null,
      "content": "321\n4.3\nListen Carefully, You Probably\nWon’t Hear This Again\nRemoving Repetition from Audio Environments \nin Games and Discussing a New Approach to\nSound Design\nStephan Schütze\nBeing REALLY Different\nD\nrop a coin on a table and listen to the sound it makes. Drop the same coin a sec-\nond, third, or hundredth time and the chance of the sound it makes being the same\nis incredibly unlikely. The creation of sound is influenced by a staggering number of\nfactors and apart from scientifically measurable sounds, such as a sine wave, is extremely\nvariable. Sounds used in most games, however, are generally static or limited in their\nvariation. In some cases this may be desirable. For the most part, though, having the\nsame sound effect repeat with little or no change not only reduces the realism of a game\nenvironment but, more importantly, it is often a source of frustration or annoyance for\nthe players.\nThe technology to create real-time variable in-game sound effects has been avail-\nable for some time. These techniques not only remove the issue of repetitive sounds,\nbut they also allow for far more complex audio assets to exist in a game than would\nhave generally been possible with the limited resources of some game consoles. With\nthe advance into the newest generation of game consoles, these methods can allow an\naudio designer to create rich audio environments featuring complex reactive and truly\ninteractive sound and music. At last developers can achieve a level of sound design\ncomparable to the incredible levels of graphics that have been achieved in interactive\nentertainment in the past few years.\nThis gem discusses the methodology behind creating these more complex and\nvariable sound environments, as well as illustrates a need to shift our thinking as cre-\nators of audio assets. I will also look at some of the tools available to asset creators.\nThe goal of this gem is to inform about the techniques available but also to generate\nthought amongst sound designers about how we practice our craft. I also hope to\ninform producers of the potential that exists for incredible audio environments.\n",
      "content_length": 2119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": null,
      "content": "How It Works; Thinking Differently\nThe first step is to move away from the traditional static linear audio used in film and\ntelevision. Games do not function in a linear fashion, but for want of a better role\nmodel the industry has often strived to achieve movie industry standards of quality\nand production. Initially as game technology was developing, this was a useful bench-\nmark, but the closer games come to meeting the standards of big budget film and tele-\nvision productions, the more we should look at exceeding them. It is apparent now\nthat in the very near future games will surpass film and television in the potential to\ndeliver entertainment. As a result, the benchmarks for production quality may also\nmove beyond those of linear media. Audio can and should be one of the leading areas\nin which interactive entertainment production methods surpass film standards. A\nselection of static pre-made sounds to be triggered as required in-game, although ade-\nquate, completely fails to utilize the creative possibilities available to designers and\ndevelopers. \nThe basic principle of this technique is to construct complex sounds from their\nindividual raw component sounds. Although this may be inefficient on a sound-by-\nsound basis, when implemented for the entire audio environment it often actually\ntakes less memory and fewer resources to create sounds that are infinitely variable and\noften far more interesting than pre-made sound effects. It also provides the sound\ndesigner with a much bigger selection of possibilities for sounds in-game. So you can\nactually have more sounds in-game with no repetition and for less memory. Initially\nthis process has a steeper learning curve for designers, and may take longer to set up.\nHowever, the resources gathered will provide ongoing material for future projects\nwithout the risk of sounding like you are simply reusing the same sound library.\nGoing Bang!\nTo begin with, it is useful to think of the sounds we record and add to the engine as\nbeing the core building blocks from which we will create all in-game sounds. This is no\ndifferent than going out and recording raw source material, preparing the source sounds\nand mixing them together to produce a finished sound effect. The difference here being\nthat creating the actual sound happens in-game each time a sound is needed. This\napproach does preclude your ability to simply drop in pre-made library sound effects,\nbut the benefits are worth the effort.\nExplosions are common sounds required in a great many games. I will refer to them\nas “pops.” I use the term “pop” because it encompasses a lot more than simply saying\nexplosion. Pops appear in most shooter-style games as sounds for grenades, missiles, or\nrockets detonating or for objects in the world exploding. Pops however also exist in\nmany platform games to represent an adversary being defeated, an item being collected,\nor a special effect such as teleporting, turning invisible, or gaining invulnerability. An\nactual explosion effect is very similar in structure to a literal “pop” sound or many of the\nother sounds I have mentioned, as they contain many or all of the same elements. \n322\nSection 4\nAudio \n",
      "content_length": 3186,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": null,
      "content": "It is important to understand that a real explosion is a release of energy, usually\nthrough some kind of chemical reaction. The actual release of energy will create a basic\nBANG, which will then echo or reverberate with a fading effect. The extraordinary\nexplosions heard in Hollywood films are a result of the initial energy affecting other\nthings in the world. So smashing glass, splintering wood, bending metal, and so on are\nnot actually apart of the initial explosion of energy; they are consequences of this\nenergy rushing out and meeting wooden, metal or glass objects and having an effect on\nthem, in some destructive way. Those items then react in a similar manner; you get an\ninitial sharp attack sound followed by a drop-off. When the item affected is a plate-\nglass window, the result is of thousands of small attacks and drop-offs combined to\nspectacular effect.\nOften, when creating sounds, the recorded material alone can sound dull or life-\nless. The recorded sound of a real gun being fired can be quite unsatisfying in its raw\nstate. Sound designers will often combine several raw sounds together to create a single\nnew sound. Sometimes the raw material used is to accentuate certain frequency ranges\nto add depth to the final assets. A low frequency impact can add considerable weight to\na sound, whereas high frequencies can make a sound seem much louder and brighter.\nEQing can add further depth to the final sound and is often helpful if you want a par-\nticular sound to stand out from the rest of the audio environment. Balancing the final\naudio environment should consider the mix of frequencies used as well as the ampli-\ntude levels of the sounds. Too much of any particular frequency range can quickly tire\nthe listener and become annoying.\nTo better understand how to construct a sound, it helps to first deconstruct it:\n• An initial sharp attack sound/surge of energy. A very short, hard attack, zero\ndrop-off sound. Think of a handclap or gunshot.\n• A drop-off and fade sound. Think of the echo of a handclap in a church or a\ngunshot. This is actually part of the initial sound, but it is useful to think of it as\na separate element when deconstructing sounds.\n• Affected elements. These are the sounds of everything that are affected by the\ninitial surge of energy.\n• The drop-off of every affected element.\n• Major subsidiary effects. Elements returning to a state of rest. Think large\nfalling debris.\n• Minor subsidiary effect. As the previous entry, but smaller debris, such as dust,\nand so on.\nThis example deconstructs a traditional explosion into its basic sound elements.\nSometimes the inclusion of extra sound material can significantly improve the final\nresult. The same thing can be done for any game pop. For example, a musical pickup\nsound in a children’s platform game.\n• An initial sharp attack sound/surge of energy. A very short, hard attack zero\ndrop-off sound, such as striking a chime or bell tree.\n4.3\nListen Carefully, You Probably Won’t Hear This Again\n323\n",
      "content_length": 3010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": null,
      "content": "• A drop-off and fade sound. The actual ring of the bell and its fade over time.\n• Affected elements often occur in a cascade of sounds. The bell chime moves\nand hits the surrounding chimes, but with less energy and in a random pattern.\n• The drop-off of every affected element. The other bells all ring.\n• Major subsidiary effects. The overtones or harmonics of the initial bell and fur-\nther minor contact between chimes.\n• Minor subsidiary effect. The fading rings of all chimes as they return to a state\nof rest.\nAfter deconstructing explosions, bubble pops, or chimes ringing, you can then\nreconstruct those sounds from their individual components. When you understand\nexactly how these components sound in their raw state, you can construct a convincing\npop using a very limited number of raw components and cleverly combining them.\nSo, let’s actually make a sound effect. Previously, I deconstructed a sound so that\nyou can understand the elements you need to construct the same type of sound effect.\nLet’s use the following elements: \n• Big_Bang01–03: A short sharp metallic impact sound\n• Stone_Fall01–02: Stone objects affected by the energy\n• Debris01–02: Small objects returning to a state of rest\nThese base sounds are included on the CD-ROM in standard PCM .wav file for-\nmat. Also included are seven in-game sounds (Ingame_Sound01a–Ingame_Sound03)\ncreated using only the seven base sounds.\nSeven wav files totalling 629KB were combined to create seven new in-game sounds\ntotalling 1.38MB. All the new sounds were created and recorded directly out of the\nMicrosoft XACT (Cross-Platform Audio Creation Tool) authoring tool using the initial\nseven base sounds. The three variations of Ingame_sound01 and Ingame-Sound02 \nare examples to show the variation, which is essentially limitless. Ingame_Sound03 was\nconstructed simply to illustrate an entirely different result from the base material.\nI allowed myself only one hour for gathering the base sounds, setting up the\nXACT project and creation, audition and recording of the new sounds. This was an\nintentional limitation to demonstrate the speed at which the tool can be used. I’m not\nsaying these new sounds are going to win any awards, but they show how a few sim-\nple definitions allow you to create infinite realistic variations quickly in real-time. I\npurposely did not descriptively name the sounds, as I did not want to influence the\nlistener’s thoughts when they were first played.\nThe Old and the New\nFigure 4.3.1 illustrates various files laid out as they might be in a traditional linear\nsound-editing program to create an explosion sound effect. The tracks allow for\nsounds to be triggered with varying degrees of overlap and the horizontal axis is used\nto position the sounds relative to each other in time. The sounds themselves can be\nany combination that produces the desired final sound effect.\n324\nSection 4\nAudio \n",
      "content_length": 2898,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": null,
      "content": "This is a traditional linear editing method as used for audio and video; once the\ndesigner is happy with the result the sounds are combined by rendering them together\nto produce a new file in the desired file format.\nFigure 4.3.2 illustrates the same layout of sounds events with the same temporal\npositioning and overlap as Figure 4.3.1. In Figure 4.3.2 however, the layout is just a\nrepresentation of how you would like the sounds to be combined in real-time by the\ngame engine; there is no rendering process. The sound events are also not limited to\nan individual sound file. The number in brackets in each sound event represents a\npool of sound files that are drawn from randomly to create the desired final output\nsound. The number of sounds available for each sound event is limited only by the\nphysical memory available on the end platform. \n4.3\nListen Carefully, You Probably Won’t Hear This Again\n325\nFIGURE 4.3.1\nStandard editing software shows linear progression.\nFIGURE 4.3.2\nSound tool layout.\n",
      "content_length": 1007,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": null,
      "content": "Another difference with this method is the ability to alter the sound’s position\nrandomly in time. The black arrows represent a time-offset value. Each time a sound\nis played, each of the tracks will count its time offset before the sound is triggered.\nThe gray arrows represent a variable time offset. In this case, the time before the\nsound is triggered is randomized up to the maximum value set. For example, sound\n01 will randomly wait a short period of time each time it is played. By comparison,\nsound 02 will wait a set time approximately twice that of sound 01 each time it is\nplayed. Sound 03 combines a set wait time with a further randomized wait time. This\nmeans it can sometimes play almost directly after sound 02 triggers, and sometimes as\nlate as halfway through sound 02 triggering.\nThe main tools used to create sound effects are amplitude, pitch, and time manip-\nulation. Combinations of these three factors can change an original source sound into a\nnew sound completely unrecognizable from the original. Sound designers in all media\nuse these tools to create the sounds they want to use and render out a new altered sound\nin the required format. This method replaces the tools that manipulate the sounds. The\nmanipulation occurs in real-time in the game. No permanent rendering occurs; a sound\nis created as it is needed according to the parameters provided using a source sound and\nthen it is discarded. Each time the required sound is called, the process is repeated, the\nvariable parameters are applied, and a unique sound is created. \nNew Tools for a New Approach\nFigure 4.3.3 shows the FMOD sound designer interface. In many ways it appears sim-\nilar to the two previous diagrams. There are sound events arranged horizontally on two\ntrack layers. FMOD’s use of sound events rather than actual wave files in the design\ntool allows for a sound event to include multiple sound files as described in Figure\n4.3.2. In Figure 4.3.3, the sound events overlap to allow for a cross-fade between them.\nA significant feature in FMOD is that the horizontal axis is not limited to repre-\nsenting time alone. This is another way in which moving away from traditional meth-\nods can be extremely effective. In Figure 4.3.3, movement along the horizontal axis\nrepresents the RPM of an engine, but it could just as easily represent altitude, speed,\nor number of hit-points. As any of these parameters are affected, the sounds change as\ndefined. The strength of these systems is that they allow the content creator to set the\ndesired parameters and how they will affect the audio environment. This frees up\ncoder time considerably, because the coder can be provided with a few simple tags to\nlink up. In the case of the car example, once the sound is added, all that is needed in\ncode is for the RPM data from the game to be linked to the RPM tag from FMOD. \nMicrosoft’s XACT audio tool in Figure 4.3.4 has a considerably different interface\nthan FMOD’s Sound Designer, but many of the same features and strengths. XACT\nuses wavebanks and soundbanks that are defined by the designer. The soundbanks are\nrepresentative of the end sound that is desired, and each sound event can consist of\nmultiple sound files in the same way as FMOD. Parameters for randomizing pitch and\nvolume are accessible at multiple levels when creating a sound. As such, it is possible to\n326\nSection 4\nAudio \n",
      "content_length": 3390,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": null,
      "content": "randomize each smaller component making up a sound event, and then pitch or alter\nthe final event as needed. XACT works in the same way as the example in Figure 4.3.2,\nit just does not use a traditional linear type of editing window.\n4.3\nListen Carefully, You Probably Won’t Hear This Again\n327\nFIGURE 4.3.3\nFMOD Designer.\nFIGURE 4.3.4\nMicrosoft XNA XACT audio tool.\n",
      "content_length": 367,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": null,
      "content": "In some ways this is a good thing, because it forces the user to approach asset creation\nin a different way. Although FMOD supports nearly all currently available platforms,\nXACT is limited to the Microsoft platforms and PC. Hardly surprisingly though, it does\ninterface extremely well with the supported platforms and is easy to use.\nMicromanagement\nAll but the simplest sounds (such as a sine wave) are made up of many smaller sounds.\nBy dividing sounds into their smaller components, you increase their usefulness to the\noverall sound environment. For example, the click/clunk sound of a car door being\nclosed is reasonably characteristic, and will provide only so much usefulness as a\nsound for another purpose, even with some pitch shifting of the sound. If, however,\nthe sound is divided into the separate elements that create the final sound (click and\nclunk), not only do you have two new source sounds that can be combined into other\ncomplex sounds, but you can also add some slight variation to the original car door\nsound by subtle pitch shifting or varying slightly the time between the click and the\nclunk.\nThis is a relatively basic example, and a non-repetitive car door sound will probably\nnot win you any awards, but it is certainly relevant when thinking about how to\napproach sound design for greater realism. Go and open and close a car door a few\ndozen times and see how different the sounds are each time. It is also worth noting that\ndividing the two sounds will not add significantly to memory. The combined wav data\nis the same length. \nThis method will however drastically increase the number of files you will be\ndealing with and as a result there will be increases in resources. If nothing else, your\nheader files or wherever you have your assets listed will be bigger. These changes are\nquite small and with third-generation consoles they should be completely ignorable.\nThe benefits of a more dynamic audio environment far outweigh the issues of having\nto wrangle more files. That is our job, after all.\nWhy Are We Doing This Again?\nThe ultimate goal with this system is to have every sound rendered in-game and to\navoid repetition and create a dynamic and effective audio environment. Implementa-\ntion time can take longer, especially initially as the designer learns to get the most out\nof the system, depending on the level of complexity of the audio environment. Obvi-\nously spending a lot of time on very minor sounds may not be cost effective, but the\nfreedom exists in the system for the designers to choose how detailed they want to be\nin creating sounds.\nThe time it would take to randomize simple footsteps by separating the foot impact\nand gravel crunch underneath, and then replacing the gravel sound as required when\ndifferent surfaces are walked on is trivial when compared to the benefits of not having\n328\nSection 4\nAudio \n",
      "content_length": 2869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": null,
      "content": "annoyingly repetitive footsteps. Add in pitch and volume randomization and it might\neven sound real. The player will probably never notice; that’s often a sign of good sound\ndesign.\nThe designer creates the sounds by choosing the raw material and setting the vari-\nables that will control how the sound is created in real-time. Because of the random\nnature of the sounds, it is important that the designer audition a considerable selection\nof each sound to ensure it doesn’t output undesirable results. Often regular tweaking\nmight be necessary as more sounds are added to the audio environment and they need\nto balance with each other. One of the best aspects of this method is that once a sound\nis in the game it can be tweaked using the parameters in the tools.\nThis means often drastic changes can be made to the sound environment with\nnothing more than the changing of a single data file. This should not require a full\nrebuild of the game engine. As a result, the sound department should be able to work\nwith considerably less support from the code team, balancing and changing the audio\nenvironment regularly and easily. This method is also incredibly useful for online con-\ntent, as new sounds could be included in game updates without the need to download\nlarge amounts of data. The designer uses the available assets that each player will\nalready have installed and creates new sound assets by making new definitions only.\nAn MMORPG could have hundreds of new sounds added to it by simply download-\ning a new definitions file and a new EXE file of only a few hundred kilobytes.\nGoing Further\nThis gem has focused on the most basic tools for sound production and manipulation:\ntime, pitch, and amplitude, and their most basic uses. The available software tools do,\nhowever, offer far more advanced tools such as filtering, effects, and implementation\ntools. More importantly, though, these tools can allow you to create incredibly com-\nplex audio environments. A series of musical motifs or even individual note events\ncould be combined in real-time to predefined parameters and played in-game to react\nand interact with a player’s actions. If you want an ascending and descending musical\npattern as Doofy Duck runs up and down the stairs, you can do it. If the player wants\nto test you by stopping halfway and jumping up and down, that’s okay too; the music\ncan respond appropriately.\nAlthough this method certainly isn’t limitless, it allows a freedom of creativity\nthat benefits greatly from thinking outside the box. An entire game could center on a\nmusical score that grows organically from the actions of the player, or where every\npossible interaction in the game world was supported by a unique audio representa-\ntion. Insert your idea here and go and make it happen!\nEven though I refer to this method as rendering or creating the sounds in real-\ntime, these ideas will not reduce or replace the work of a sound designer. In fact, it\nmakes the role even more critical and requires the sound designer to work far beyond\nsimply using library sounds. This method will very quickly expose a designer with\nweak skills or poor imagination. Conversely, a great designer could use this system to\n4.3\nListen Carefully, You Probably Won’t Hear This Again\n329\n",
      "content_length": 3267,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": null,
      "content": "create an audio experience worthy of the best titles in the industry. This method will\nhave an impact on the time required to create and implement audio at least for the\nfirst project on which it’s utilized. However, once developers overcome the initial\nlearning curve, this method can be extremely flexible. The method allows for last\nminute changes and alterations to the sound assets far more easily than traditional\nmethods of game sound design. \nConclusion\nGame production standards have increased dramatically in the last five years, and as\nstudios better understand the importance of good tools and production processes, the\nincrease in quality should continue. In the past, game audio was often overlooked or\ngiven minimal attention. The development of new middleware software and produc-\ntion tools such as XACT allows audio content producers to approach content design\nand creation in a whole new way. Once designers unlearn some of the traditional\napproaches to sound construction, these new methods can allow for incredible flexibil-\nity and variety. The ability to create audio environments never before possible is not\nonly a great opportunity for talented audio teams, but will hopefully provide entertain-\nment for players that exceed the experiences available through any other media.\n330\nSection 4\nAudio \n",
      "content_length": 1323,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": null,
      "content": "331\n4.4\nReal-Time Audio Effects\nApplied\nKen Noland\nT\nhe purpose of this gem is to outline some of the more basic fundamentals of audio\nprocessing from a high-level perspective, taking into account all the tips and tricks\nI’ve learned over the years in designing an audio engine for video games. Some of these\ntips are straightforward and others require a little more thought to work around.\nA quick search on the Web will show you how to efficiently create a graphics ren-\ndering pipeline or perhaps an AI framework. However, when it comes to creating\nyour own sound system, a large portion of articles are, in my opinion, too API specific\nor too general and don’t cover the niche cases that always tend to show up with audio\nprogramming. Lately this has been changing, and a much larger focus has been put on\naudio programming from the perspective of a digital signal analysis perspective.\nThis gem is less API specific, although I do mention a couple APIs available and\nsome of the more interesting features, but instead this gem is focused on the general\nprinciples of building an audio system. As a note of caution though—as the gem pro-\ngresses, I will go into more and more advanced topics that will likely require further\nreading.\nBefore I begin, I want to introduce a very basic concept. Sound is perceived as a\ndifference in samples. Be very mindful of this. If you’ve seen a waveform, you know\nthat it consists of mostly oscillating values that are constantly changing. Those changes\ndenote the frequency over time. If the signal is flat, there is no frequency. If a signal\nchanges very rapidly, there is a very high frequency.\nThis is a very important concept to know. Keeping in mind that the values are\nconstantly oscillating, if you drop from one high value to another, because say you\nwant to clear the buffer and fill it with all zeros during the middle of a peak oscillating\nvalue, you introduce a frequency change that can be perceived as a tick or a pop. A\nmuch more accurate way to deal with clearing a sound buffer is covered in the follow-\ning sections.\n",
      "content_length": 2076,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": null,
      "content": "A quick note about the two primary APIs available—you have DirectSound (for\nWin32 and Vista) and OpenAL (available on most platforms, including Win32,\nVista, Linux, and most consoles). Both APIs do what they do well and support a wide\nvariety of formats and effects. I have no preference when it comes to choosing one\nAPI over another and it depends on what environment you are developing for.\nWith that being said, both sound APIs have their benefits and drawbacks.\nBecause of the distinct difference in drivers for DirectSound and OpenAL, I recom-\nmend writing a sound system that is abstract enough that the end user can readily\nswitch between the two different sound APIs depending on the card and drivers they\nhave installed. I also recommend including an option for software processing for both\nAPIs; that way any driver-related problems are addressed.\nOpenAL and DirectSound have two very distinct design methodologies and are\nmuch like their graphical counterparts. If you have worked with OpenGL, OpenAL\nwill come very naturally to you. If you’ve worked primarily with DirectX, Direct-\nSound is going to be very straightforward.\nOverview of a Sound System\nThere are four concepts to understand when dealing with a high-level overview of a\nsound system—the primary buffer, the listener, the sound, and any effects applied to\nthe sound or the listener.\nThe Primary Buffer\nThe primary buffer is the final resting place for the PCM samples you send to it. Under\nmost sound systems you won’t be filling the primary buffer directly, but you will be\ndealing with it from the perspective of the listener. The only thing that you are con-\ncerned about with the primary buffer is how much it advances from frame to frame. \nThe Listener\nThe listener is a special object that exists in 3D space. It listens to the incoming\nsounds and applies any special transformations and effects such as panning and falloff,\nand advanced filters like Doppler Shifting and Head Relative Transfer Delay.\nYou should always assume that under any given API you are going to have only\none listener. Normally this is not a problem, but for those of us who write games that\nhave multiple viewports or monitors, it represents a slight challenge. The solution to\nthis problem is actually very easy. Simply transform all sounds to the listener and\nrecord things such as velocity in the sound properties so that effects, such as Doppler\nshifting, can still be correctly calculated. Things get a little more complex when listen-\ners have effects applied to them and those effects are different from listener to listener,\nbut I’ll explore effects in a little while.\n332\nSection 4\nAudio \n",
      "content_length": 2656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": null,
      "content": "The Sound Sources\nThe sound sources themselves are typically mono channel signals coming from within\nthe world. Sound sources typically have properties such as position, falloff, and veloc-\nity. Those properties are then used by the listener and the effects to process the sound.\nUnder any sound system, you should differentiate between sound sources and the\nactual sound data. Sound sources contain a reference to the sound data as well as the\nposition and orientation of the particular sound and the current play position within\nthe actual sound data. The actual sound data is merely the container for the PCM\ndata as well as any other audio designer related properties, such as falloff reference,\nmaximum number of instances, and any general effects to be applied to all instances\nof the sound itself.\nThe Sound Effects\nSound sources also contain effects. Some of those effects are inherited from the sound\ndata and other effects are applied from its position within the world. Either way, it is\na good idea to stack up the effects so that you can easily collapse them upon request.\nPutting these concepts together, you’ll see that the primary buffer requests data\nfrom the listener, the listener then goes out and determines what sounds to play and\nrequests the samples from the sound sources. Upon getting that request, the sound\nsources collapse the effect stack and fill the listener with the correct data. The listener\nthen runs a digital signal peak limiter on the sound effects and collapses its own\neffects stack; then it presents the contents to the final buffer.\nOne thing to note in this entire example of a sound system is that it uses a model-\nview-controller architecture. The data is encapsulated in the sound data (the model)\nand is requested by the sound source (the controller), which then applies the individ-\nual sound effects (more controllers), which in turn is requested by the listener and\nthen finally outputted to the primary buffer (the view).\nSound Buffers\nOn almost all machines you are limited to the amount of sounds the hardware can\nplay. Even when processed in software mode, you should still clamp the number of\navailable sound buffers to something within the range of your performance targets. As\nof writing this gem, the maximum available hardware accelerated sounds on the aver-\nage top-of-the-line consumer sound card is 128 sounds. Keep this in mind for later. \nThis does not take into account that Vista will force you to use software process-\ning under DirectSound at the time of writing this gem. The only alternative is\nOpenAL if you want to utilize the hardware processing under Windows Vista.\nIn most cases, you will want to allocate enough sound samples in your sound buffers\nso that continuous playback is possible, even in the most dramatic frame rate drops. I\n4.4\nReal-Time Audio Effects Applied\n333\n",
      "content_length": 2852,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": null,
      "content": "typically create my buffers with enough room for one second’s worth of sound data at\n44100kHz.\nOne thing I’ve picked up is that it can actually take more time and more resources\nto stop and start sound buffers than to let them play beyond the duration of their\nsound (making sure to clear the sound buffer so that they are not heard!). But do this\nwithin reason. For instance, using the previous example, where you have 128 sound\nbuffers, you should start playing eight of them. As soon as all eight sounds are occu-\npied by sound data, you start playing eight more. Once it drops below a certain\nthreshold and the sound buffers haven’t been accessed in a while, you go ahead and\nstop them. This kind of balancing is not necessary, but I found it to help in situations\nwhen I had a lot of short sounds playing one right after the other.\nOnce you get a request to play a sound, populate as much of the sound buffer as\nyou can at the current write position, remembering that you’re likely to have already\nstarted playing this buffer. You can get the playback advancement by recording where\nyour previous playback cursor was to where it is now from frame to frame. One thing\nto note here is that drivers will sometimes give you the wrong playback position. The\nposition is sometimes off by only a couple of samples, but other times it can be signif-\nicantly off. In order to compensate for that, take half the size of the buffer and fill that\non the first request. Thus, the one second buffer actually only contains half a second\nof data.\nSo let’s say you’ve got a 44100 sample size buffer and the write position is at\n44000 and your playback has advanced 150 samples in the last frame. Using this\nknowledge, you can request 22050 samples (1/2 buffer size) from the sound source on\nthe first pass. Now that you’ve got those samples from the sound source, you need to\nwrite 100 samples to fill the current write position to the end of the buffer and then\nthe remaining 21950 samples go to the start of the buffer at offset 0. This is simply\nknown as a circular buffer.\nOn the next update, all you have to do is continue to fill the buffer at the last\nwritten position with the amount of samples that playback has progressed. In the last\nexample, you’d then be writing 150 samples to buffer position 21950.\nAs a safety precaution, you also want to clear out the previously played samples.\nWhen you do this, you’ll want to stay three to four frames behind the playback cur-\nsor’s current position, remembering that the playback cursor could be off as well.\nAnother safety precaution is to set a callback at the last written position, clearing any\nprevious callbacks. When the play cursor gets to that position, it triggers the callback,\nwhich then should fade the entire buffer into silence. Because sound is generally\nprocessed on a separate thread, this should work in all cases. This way, you’ll never get\nthose repeating sounds looping in the event that the main update thread locks up.\nRank Buffers\nUsing the example of the sound card from before, I’m going to say that you will have\n128 sounds in total that you can play at any given time. The problem is that within\n334\nSection 4\nAudio \n",
      "content_length": 3188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": null,
      "content": "your 3D world you have hundreds, perhaps even thousands, of sounds coming from\nall different directions. This is where you want to implement a special kind of buffer\nknown as a rank buffer.\nA rank buffer is a very simple concept. You algorithmically generate a rank and\nthen you request a buffer. If all buffers are full and the rank exceeds another already\nplaying sound buffer then the lowest ranking sound is booted out and the new sound\nis played.\nThe rank can be calculated any number of ways. The most general way to calcu-\nlate the rank is to determine the attenuation (distance, falloff, and volume), and then\nmultiply that by a value given to you by the audio designer. This works in most cases,\nbut not all. It’s best to take into account all properties of the sound such as distance,\nfalloff, effects, and other items associated with the sound so you can get a clear idea of\nthe sound rank. It’s not acceptable to have a high priority sound just repeating its sub-\ntle echo effect, as another lower priority, but potentially more noticeable, sound is get-\nting skipped.\nAlso worth noting is that audio designers like to specify how many instances of a\nparticular sound or sound category can be played. For instance, if you’re in a room with\ntons of machine gun fire going off, it only makes sense to play 10 or so of these types\nof sounds. Be sure to take this into consideration when building your rank buffer\nalgorithm. One thing I did was to allow the sound data to figure out its rank given its\nparticular context by abstracting a simple function that took in the parameters passed\nto the sound, like its position relative to the listener and the general world data.\nThere are some catches to the rank buffer solution that you must address specific\nto audio processing. The primary catch is that you can’t just stop a sound and then\nfollow that up with another sound. Remember earlier when I stated that sound is per-\nceived as the difference in samples. If you stop playing one sound abruptly, you’ll hear\na tick or a pop. Instead, you have to transition one sound to the next, fading out the\nprevious sound.\nThings get even more complex when the previous sound has effects applied to it.\nBecause of the way audio drivers handle the effects applied to the sound buffer, you\nshould not just linearly transition the effect, but instead you have to wait until the\nprevious sound has finished fading and then you can switch the effects properties\nover. Thus, once your gain (volume) has reached zero for the previous sound source,\nyou can apply the new effects and start copying over the new sound. One thing to\nnote is that you do not want to commit the switch in effects until the playback cursor,\nnot the write cursor, reaches the desired switch point.\nRemember previously when you copied half a second of sound samples into the\nbuffer to accommodate for drops in frame rate? You want to be able to transition\nimmediately. Keep in mind that once you send the data to the sound buffer, it’s up to\nthe driver’s implementation if it wants to keep that data around, so I wouldn’t count\non it still being there. To get around this, you should keep copies of all the samples\nyou copy over to the sound buffer so that you can go back in time and fade out at the\nplayback buffer’s current write position.\n4.4\nReal-Time Audio Effects Applied\n335\n",
      "content_length": 3352,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": null,
      "content": "The fade sample amount varies, but I generally keep it at around five millisec-\nonds, or roughly 220 samples at 44100kHz playback. You can set this up to be a\nproperty of the sound data so the audio designer can adjust this value. \nEffects and Filters\nEffect objects should be created through an abstract factory method generated by\nyour individual sound system API, so that hardware processing is possible, and then\nattached to the sound source or listener so that they can be collapsed when requested.\nEffects are different from filters. An effect can contain multiple filters or simply\ngenerate sound data or perhaps contain a wrapper for a hardware accelerated feature.\nIn any case, think of the effect as the middleman between the sound source and, if the\neffect calls for it, the filter. When designing filters, keep in mind that filters should be\nas generic as possible and that any implementation details should be gathered and\nstored in the effect object itself, thus allowing you to abstract new effects quickly. To\nput it simply, effects are implementation specific and filters are not.\nThere are two types of filters to be concerned with, as follows:\n• Infinite impulse response (IIR) filters, which recursively work on the sound samples.\n• Finite impulse response (FIR) filters, which just deal with transforming the sound\nsamples in some manner without regard to prior output.\nYou’ll have to differentiate the two filters when designing the effect.\nWithin the filters, there is a concept known as wet/dry mix. Wet samples are\nsamples that have been previously transformed and dry samples are the raw incoming\nsamples without any transformation. You should have a distinguishing factor of\nwet/dry mix and allow for your effects to change that ratio.\nTo complicate matters even more, there are multiple ways of transforming the\nsamples. One of the most common methods is through the use of Fast Fourier Trans-\nform (FFT). This type of calculation, although extremely useful and applicable, is\nvery time- and processor-consuming and much research has been done to improve the\nspeed of this operation. Be sure to run this type of operation only when absolutely\nneeded, caching any data that you can from it. This means that you should be able to\ntransform the sound in the effect object to the frequency domain, run all of the filters\nin the transformed frequency domain (ensuring that the filters can use the frequency\ndomain data), and then transform back to sample space in the effect itself when all fil-\nters have been processed, if the effect calls for it.\nFIR filters are the easiest to deal with because all you have to do is feed it the data\n(dry mix) and it spits out the result. IIR filters are a little more complex because they\nrely on the previously generated result (wet mix). The easiest way to deal with this is\nto have a separate buffer set up within the effect that records the output from the fil-\nter (the wet mix buffer). The size of that buffer is specified when the effect is created,\nthus setting the delay line. In some effects, this delay line can be set up using the\ninputs for the effect, such as feedback delay, which can then be translated to buffer\n336\nSection 4\nAudio \n",
      "content_length": 3212,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": null,
      "content": "size. Otherwise, you can optionally explicitly set the buffer size, thus clamping your\nwindow.\nIIR and FIR filters can also be arranged in a directed graph, allowing the filter to\nreference other filters in a cyclical manner, running until it has reached the extents as\nspecified by delay line. This is the style of filter design outlined in Game Programming\nGems 5 in the article entitled “Fast Environmental Reverb Based on Feedback Delay\nNetworks” [Schüler05]. Using these types of filters is very handy, because you can\ndesign new effects quickly as well as extend those effects to simulate audible character-\nistics of the world around you.\nI have yet to talk about signal timing—all those cases where you have looping\nsounds combined with effects that elongate a sound beyond the original sound\nlength, such as with an echo. I’ve outlined a system where you request samples and\nthose samples are filled via collapsing a stack of effects resulting in the final data, thus\na sound is finished when no samples are returned via the listener. However, there is a\ncaveat to this. Simply waiting for the request to return no samples on a looping sound\nsource with an IIR filter will result in a sound that never loops. Therefore, you do\nhave to push a separate flag that informs the sound source that it is looping and that\nwhen an effect reaches the end of reading the sound data, it should loop back to the\nbeginning.\nCompression and Streaming\nThere are many audio compression formats available, each one focusing on a particu-\nlar need. Some formats, such as ADPCM, are focused on performance and quick\ndecoding, whereas others, such as MP3 and OGG, are focused heavily on compres-\nsion ratio, giving you small file size while maintaining quality. Here’s a quick compar-\nison between those three formats.\n• ADPCM is the simplest of the three formats. It uses a simple predictive algorithm\nto generate deltas on blocks of audio. Those deltas are stored in four-bit values,\nthus making the decompression algorithm as simple as two table lookups and\ndecoding a four-bit delta, coupled with two multiplies and an add makes this the\nleast CPU intensive algorithm with the highest payoff in compression. However,\nthe compression ratio is a measly 4:1 compared to the other formats and the sig-\nnal restoration at low sampling is not nearly as good as the other formats.\n• MP3 is a common format, widely known and used across multiple platforms.\nMP3 uses frames, similar to chunks used in ADPCM. These frames contain infor-\nmation on the acoustical makeup of the sound signal in transformed frequency\ndomain, which then is broken down into a quantized lookup table [MP307a]\n[MP307b]. MP3 allows for many encoding options such as variable bit rate and\nID3 tags.\n• OGG Vorbis uses the modified discrete cosine transform to convert from signal\nspace to frequency domain, similar to MP3, and then clamps the floor value.\n4.4\nReal-Time Audio Effects Applied\n337\n",
      "content_length": 2952,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": null,
      "content": "Afterward it quantizes the entropy coding and then stores the delta into a lookup\ntable [OGG07]. This kind of encoding allows for lossy compression at variable\nbit rates and is specially tuned for fast decompression, but still not as efficient as\nADPCM.\nYou might be wondering why I’m going into detail about these three compression\ntechniques. There are many libraries out there that will handle the conversions for you\n([MP307a], [MP307b], [OGG07]), and aside from the performance related data,\nthere’s really no need to go into detail about each format. But then again, there’s\nsomething there that you may have picked up. OGG and MP3 store their informa-\ntion in the frequency domain, which means that the really expensive FFT that I men-\ntioned earlier is already present.\nWhat this means is that using the libraries from each respective format, you can\nextract the frequency data and use that information to run your frequency domain\neffects, and then translate into the signal space for final presentation!\nAnother reason for going into detail about each respective format is that you’ll\nnotice each compression scheme has “frames” or “blocks” that they work with. Using\nthis information, you can create a separate rank buffer mechanism for caching\ndecoded PCM samples or decoded frequency data. When you’re streaming from disk,\nit means that you can cache certain frames or blocks in an already decoded fashion as\nopposed to having to store the entire decoded file. For music, this is extremely impor-\ntant. You want to read ahead as much as you can and cache the decoded data, but you\ndon’t want to dedicate 300MB or more of memory just to your sound track. By\ndecoding on a frame-by-frame basis, you can limit your memory usage to any arbi-\ntrary number and by utilizing the rank buffer (without the need for fading samples),\nyou have a mechanism for streaming files from disk efficiently.\nConclusion\nBuilding an entire audio system from scratch seems like a daunting task at first look,\nbut by utilizing the methods you know as a programmer and using the concepts out-\nlined here, you should be able to get up and running fairly quick. There are many\nother topics to learn about and a ton of resources to get you started—a few of which\nI’ve noted in the references section. I would also go so far as to suggest reading up on\nthe many dedicated forums and newsgroups. They contain some of the best informa-\ntion available.\nAudio programming is both rewarding and challenging. After you develop your\nown sound system, tailored to your game’s needs and performance requirements,\nexpanding upon that knowledge and implementation to facilitate design decisions\nand extended effects makes a difference in the overall playability of the final video\ngame. That difference is then perceived by the players, and they leave the game with a\nbetter sense of immersion, so in my opinion, it is one of the most important areas of\nvideo game programming.\n338\nSection 4\nAudio \n",
      "content_length": 2971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": null,
      "content": "References\n[dsnd07] Microsoft “DirectSound,” available online at http://msdn2.microsoft.com/\nen-us/library/bb219818.aspx, August 1, 2007.\n[MP307a] Underbit Technologies. “MAD: MPEG Audio Decoder,” available online\nat http://www.underbit.com/products/mad/, August, 2007.\n[MP307b] Mike Cheng. “The LAME Project,” available online at http://lame.\nsourceforge.net/index.php, August, 2007.\n[OGG07] Xiph.Org “Vorbis audio compression,” available online at http://xiph.org/\nvorbis/, August, 2007.\n[openal07] Creative. “OpenAL: A Free (LGPL-ed) and Open Source, Cross-Platform\nAudio Library Used for 3D and 2D Sound,” available online at http://www.\nopenal.org, August 1st, 2007.\n[Schüler05] Schüler, Christian. “Fast Environmental Reverb Based on Feedback\nDelay Networks,” Game Programming Gems 5, Charles River Media, 2005.\n4.4\nReal-Time Audio Effects Applied\n339\n",
      "content_length": 858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": null,
      "content": "341\n4.5\nContext-Driven,\nLayered Mixing\nRobert Sparks\nsparks.robert@gmail.com\nT\nhe technical quality of sound in our industry is beginning to approach that of the\nfilm industry. Next-generation consoles are here. Games support Dolby Digital\nand DTS; they use high sampling rates; they have virtually unlimited numbers of\nvoices; and they use perceptually lossless compression algorithms. That said, the film\nindustry still has great advantages over us when it comes to overall control of the final\nproduct.\nConsider the process of sound mixing. A film can be mixed with total control of\nevery sound effect. Each scene can be mixed with purpose and deliver a specific emo-\ntional experience. A game is mixed with much less control. For the most part, we can’t\nchange much from scene to scene. We rely on positional and environmental simula-\ntion to do the rest. \nThis gem presents a mixing system that brings the overall sound of a game under\ngreater human control. A similar system was used with great success in developing\nScarface: The World Is Yours and supported a three-week final mixing session of the\ngame at Skywalker Sound.\nOverview\nThis mixing system takes for granted the idea that game parameters can be tuned in\nreal-time. It concerns itself with organizing that tuning experience into an effective\nworkflow—a workflow based on the mixing of films.\nThe system presents sound parameters (for example, volume, pitch, and filter set-\ntings) as if they were the rows of faders and knobs on a mixing board. Each of the rows\ncontrols whole groups of related sounds (for example, music, dialogue, or footsteps).\nThe system also divides the action of the game into logical scenes. Unlike the\nscenes of a film, which can be defined chronologically, the scenes of a game must be\ndefined by actions of the player. \n",
      "content_length": 1816,
      "extraction_method": "Direct"
    },
    {
      "page_number": 375,
      "chapter": null,
      "content": "Associating a set of mixing parameters with each logical scene allows precise con-\ntrol of the overall sound (see Figure 4.5.1). It also allows each scene to be mixed inde-\npendently in real-time in a series of mixing sessions.\nThe scene-by-scene approach of this system makes it context-driven. Later, you’ll see\nthat scenes can overlap and modify each other, making it also a layered mixing system.\n342\nSection 4\nAudio \nFIGURE 4.5.1\nAn example of context-driven mixing in which the player enters a dark alley\nand activates “invincibility mode.”\nImplementation\nWhat follows is a high-level description of the mixing system logic and its main\nclasses. A more detailed C++ implementation is available on the CD-ROM.\nMixing System\nThe mixing system provides a central mixing interface to other systems in the game. It\nmanages component lifetimes and performs calculations.\nMixing Categories\nThe mixing system groups related sounds into mixing categories. The system works\nonly in terms of these categories rather than in terms of individual sounds.\nPossible mixing categories include music, ambience, explosion, glass, footsteps, or\nbirds. When a sound plays in the game, it is assigned a mixing category. \nThe Central Mix\nThe mixing system centralizes the mixing (or tuning) parameters for all sounds into a\nsingle logical object, the central mix. Parameters may include volume, pitch, LFE\ngain, auxiliary effect gain, or parameters related to positional simulation. The central\nmix provides a set of parameters for each mixing category. \n",
      "content_length": 1538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": null,
      "content": "Conceptually, the central mix is like a mixing board through which all sounds in\nthe game are routed. As sounds play in the game, they do so according to the parame-\nters assigned to their mixing category in the central mix (see Figure 4.5.2). \n4.5\nContext-Driven, Layered Mixing\n343\nFIGURE 4.5.2\nThe central mix acts as a mixing board for the game,\ncontrolling the playback of groups of sounds.\nMixing Snapshots\nThe sound designer works with the central mix in terms of sets of parameter values\nknown as mixing snapshots. The state of the central mix is calculated using these snap-\nshots (see Figure 4.5.3). Mixing snapshots are like mixing board presets or fader\nautomation controls for the central mix.\nFIGURE 4.5.3\nThe mixing system calculates the central mix using mixing snapshots pro-\nvided by the sound designer.\nThe sound designer defines a mixing snapshot for each logical scene of the game.\nWhen the scene begins, a mixing event triggers, adding the associated snapshot to the\ncentral mix calculations. When the scene ends, another mixing event triggers, remov-\ning the snapshot. The snapshots provide fade-in durations and fade-out durations that\nsmooth transitions as the snapshots are added and removed from the calculations. \n",
      "content_length": 1242,
      "extraction_method": "Direct"
    },
    {
      "page_number": 377,
      "chapter": null,
      "content": "Mixing snapshots give the designer complete control over each scene. The granular-\nity of this control depends on the number of scenes and the number of mixing categories. \nScenes can be very general and appear throughout the game or they can be very\nspecific. Scenes may overlap and be defined as modifications of other scenes. Table\n4.5.1 defines some example mixing snapshots and scenes.\nTable 4.5.1\nExample Mixing Snapshots\nSnapshot Name\nScene Description\nSound Highlights\non_foot_night\nActive when the player is \nFootsteps and foley.\non foot at night.\nNighttime ambient sounds.\nNighttime reverb settings and\nroll-off settings.\non_foot_day\nActive when the player is \nDaytime ambient sounds\non foot in the daytime.\nFootsteps and foley.\nDaytime reverb settings and\nroll-off settings.\nin_car\nActive when the player is \nPlayer’s vehicle sounds.\ndriving a vehicle.\nReduced ambient sounds.\nIn car reverb settings.\nTraffic levels increased.\ninterior\nActive when the player enters \nReduced outdoor sounds.\na building. This snapshot may \ninstall at the same time as \non_foot_day or on_foot_night.\ndialogue_duck\nActive when the player speaks. \nEmphasis on dialogue clarity.\nThis may install at the same \nReduction of music and other \ntime as almost any other snapshot.\ninterfering sounds.\ninvincible\nActive when the player enters \nPitch lowering of specific \na special invincibility mode. \nsound effects.\nThis may install with almost any \nIncreased volume of the \nother snapshot.\nsub-woofer.\nnis_2\nActive during the cinematic, \nAll in-game sound effects \nnon-interactive sequence (NIS) \nremoved from the mix except \nnamed nis_2.\nthose required by the NIS.\npre_mix\nActive at all times.\nAllows for global adjustments\nin all sounds.\nMixing Layers\nMixing layers organize the mixing snapshots that are active. The mixing snapshots are\nassigned to layers by the sound designer. Three mixing layers exist, each exhibiting a\nspecific behavior:\n344\nSection 4\nAudio \n",
      "content_length": 1951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": null,
      "content": "• The pre-mix layer contains one snapshot that is always present and never changes.\nThis layer allows sound properties to be changed globally in all contexts.\n• The base layer always contains one snapshot and never more than one. As new\nbase layer snapshots become active, they replace previous base layer snapshots.\n• The modifying layer contains any number of snapshots at a time, allowing scenes\nto overlap. These snapshots act as modifiers to other snapshots, typically reducing\nspecific volumes and applying special filters or pitch effects. For example, a mod-\nifying snapshot will duck music during dialogue or apply special filters during key\ngame play moments.\nFigure 4.5.4 illustrates the three mixing layers.\n4.5\nContext-Driven, Layered Mixing\n345\nFIGURE 4.5.4\nThree mixing layers organize the active mixing snapshots.\nExtending the Concept with Live Tuning\nA remote tuning application is essential for achieving a truly efficient mixing work-\nflow. Only live tuning enables the sound designer to fix problems as they are heard\nand to precisely adjust volume levels and other settings.\nThe tuning application can present parameters with simple arrays of numbers or\nwith a graphical representation of a mixing board. It is useful for the application to\ndisplay both the active mixing snapshots and the state of the central mix. This allows\nmixing snapshots for each scene to be selected and tuned individually.\n",
      "content_length": 1421,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": null,
      "content": "The resulting workflow is very sophisticated. Typically, the process involves\nteleporting to the location or mission that requires mixing, selecting the appropriate\nmixing snapshot in the remote tuning application, and then mixing the scene while\nplaying through it. \nFor Scarface: The World Is Yours, our team implemented a MIDI interface between\nour tuning application and a physical mixing board. This interface made it easy for\npeople from outside the video game industry to work on our project (see Figure 4.5.5).\n346\nSection 4\nAudio \nFIGURE 4.5.5\nLive mixing through a\nMIDI control surface. \nPerformance\nCPU requirements of the mixing system are low. Calculating the central mix consumes\nmost of its energy, which involves combining the parameters of the active mixing snap-\nshots. Typically there may be four active mixing snapshots and 20 sound categories\nwith four parameters each. The parameters are often combined using addition or\nmultiplication.\nMemory requirements grow with the number of mixing snapshots and the num-\nber of sound categories. Large games may require several hundred mixing snapshots\nand a few dozen sound categories. An un-optimized mixing snapshot may require 512\nbytes. As a result, 200 snapshots will consume 100KB of memory. Optimization\nreduces the memory footprint of the system considerably. \nThe most effective optimization reduces the number of mixing snapshots held in\nmemory at a time by loading snapshots only when needed (for example, bundling\n",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": null,
      "content": "snapshots with art for a mission, a location, a cinematic sequence, or a character).\nThis requires pipeline work and coordination with other content loading systems. \nAnother optimization stores mixing snapshot parameters as shorts instead of\nfloats, which halves the size of a mixing snapshot.\nCombining these optimizations, a 512 byte mixing snapshot becomes 256 bytes;\n200 snapshots in memory become 10 in memory. Therefore, a 100KB footprint\nreduces to 2.5KB.\nSample Program\nThe CD-ROM includes a sample program for this article. The program presents a\nvery simple game and an equally simple mixing environment. Click the buttons to\ntrigger sound effects and mixing events. Use the mixing board and related controls to\nselect and tune mixing snapshots and experience context-driven, layered mixing.\nConclusion\nThis gem discussed a powerful approach to sound mixing that has proven itself prac-\ntical and effective in the field. \nWorkflow is paramount when it comes to delivering quality sound. Well-defined,\nintuitive processes enable creative and polished work. Established, effective processes\nare available to be borrowed from the film industry. Technical decisions should focus\non establishing these processes in the gaming industry.\n4.5\nContext-Driven, Layered Mixing\n347\n",
      "content_length": 1281,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": null,
      "content": "349\nS E C T I O N\n5\nGRAPHICS\n",
      "content_length": 29,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": null,
      "content": "351\nIntroduction\nTimothy E. Roden, Angelo State University\ntroden@angelo.edu\nI\nn the early days of 3D computer games, developers were generally concerned with\nkeeping polygon counts low and reducing scene complexity. Graphics engines had\nfixed function pipelines that allowed very narrow creative freedom in terms of render-\ning and animation. It is amazing how things have changed. The graphics section of\nthis edition of Game Programming Gems presents a wide range of articles covering top-\nics as diverse as content creation, animation, and rendering.\nJeremy Hayes of Intel expands on the work Jason Shankel did to show advanced\nmethods of procedural terrain generation using a method called particle deposition.\nNew techniques are described for volcano placement, mountain ranges, dunes, and\noverhanging terrain. These new methods add more control, which enable a level\ndesigner to better define the placement of important terrain features. Because crafting\ninteresting and useful terrain is not only a function of geometry, another gem explores\nthe mapping of textures onto terrain. Antonio Seoane, Javier Taibo, Luis Hernández,\nand Alberto Jaspe present a method for mapping very large textures onto outdoor ter-\nrain and Ben Garney provides an implementation of that idea with pointers for\nenabling the technique on SM 1.0-level graphics cards.\nThe graphics section features several excellent gems that cover rendering. Joris\nMans and Dmitry Andreev of 10Tacle Studios describe an advanced decal system that\nproperly blends bump and diffuse maps under a decal, thereby removing the “on top\nof” look that decals can sometimes exhibit. A system for real-time rendering of diffuse\nlighting for rough materials is presented in the gem by Tony Barrera, Anders Hast,\nand Ewert Bengtsson. Chris Lomont presents a comprehensive overview of high-\nperformance subdivision surfaces. Joshua Doss demonstrates the use of graftal imposters\nin rendering cartoon-style plants and fur effects.\nAnimation receives a good treatment in this edition of the Gems series. Bill\nBudge of Sony Entertainment of America explains techniques for dealing with cumu-\nlative errors in skeletal animation sequences. A technique for animating relief impos-\ntors is described by Vitor Fernando Pamplona, Manuel M. Oliveria, and Luciana\nPorcher Nedel. Finally, I have contributed a gem on procedural generation of lipsync\ndata for human models using a freely available phonetic dictionary.\n",
      "content_length": 2461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": null,
      "content": "353\n5.1\nAdvanced Particle\nDeposition\nJeremy Hayes, Intel Corporation\narmyofzin@gmail.com\nP\narticle deposition is a procedural terrain generation technique that has so far been\nlimited to creating topography for volcanic mountain ranges. However, the\nbeauty of particle deposition lies within its versatility. This gem demonstrates several\nadvancements to particle deposition that allow the creation of new types of terrain\ntopography as well as improved volcanic mountain ranges. These advances to particle\ndeposition also improve artistic control by allowing a level designer to preview and\nrefine the position and size of terrain features.\nWhy Particles?\nThe surface layer of the earth is called the continental crust. The continental crust floats\non another layer of the earth, called the mantle, because it is less dense than the mantle.\nOver very long periods of time, the continental crust behaves like a ductile solid (like\nhot wax) [Grotzinger07]. The earth’s topography is created by forces above and below\nthe surface. The continental crust is fractured, rippled, and twisted by plate tectonics,\nwhich are powered by geothermal forces inside the earth. Above the surface, the earth’s\nclimate also molds the topography. Erosion by wind, water, and ice can cause dramatic\nchanges over time.\nParticles can be used to naturally simulate the deformation of terrain by plate tec-\ntonics and erosion. Particles can be used to simulate the flow of material and they can\nbe joined to form solids. In other words, particles provide a simple and versatile\nmechanism to generate the topography of virtual terrain.\nParticle Deposition\nShankel proposed the original particle deposition algorithm as a way to generate terrain\nthat looks similar to volcanic mountain ranges [Shankel00]. Particle deposition traverses\na height field with a random walker. The random walker drops at least one particle at\neach location it visits. The particle must check the height of the adjacent positions after\n",
      "content_length": 1989,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": null,
      "content": "354\nSection 5\nGraphics \nit lands on the height field. If a lower adjacent position is found, the particle moves to\nthat position. The particle repeats this process until it can no longer move to an adjacent\nposition of lower elevation. Figure 5.1.1 demonstrates a single particle descending a\none-dimensional height field. The algorithm can be stopped when a predetermined\nnumber of particles have been dropped or when the user is content with the results. Fig-\nure 5.1.2 shows an example of terrain created with particle deposition.\nFIGURE 5.1.1\nDepositing a single particle.\nImproving Particle Deposition\nAlthough particle deposition does create interesting topography for volcanic moun-\ntain ranges, it is easy to see it has some limitations. Notice that the slope of the terrain\nFIGURE 5.1.2\nA screenshot of terrain generated with the original particle deposition algorithm.\n",
      "content_length": 879,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": null,
      "content": "formed by the particles is almost always 45°, which is a consequence of the heuristic\nused to settle the particles on the terrain. Particles are not allowed to stack if there is a\nlower adjacent position, so the slope will never be greater than 45°. Sometimes parti-\ncles will briefly form slopes less than 45°. This usually occurs when particles are accu-\nmulating in a valley or near an existing peak. Unfortunately, these gentler slopes will\nnever span more than a few positions. Developers would like to be able to create more\ninteresting terrain slopes composed of various angles that span small and large dis-\ntances, as shown in Figure 5.1.3.\n5.1\nAdvanced Particle Deposition\n355\nFIGURE 5.1.3\nAn example of ideal terrain composed of various angles.\nAnother limitation of particle deposition is there is no control over the placement of\nmajor terrain features such as the volcano’s peak. It is also hard to control how many\npeaks to create. The outcome of the terrain is almost entirely random. This is a big dis-\nadvantage if a level designer wants to create a certain number of volcanoes at specific\nlocations. It would be nice to give a level designer more control over the major terrain\nfeatures (for example, size, general shape, and placement). Perhaps the biggest limitation\nto particle deposition is that it only creates topography that is suitable for a volcanic\nmountain range. What if you want to create other types of terrain? Fortunately, all of\nthese limitations can be overcome with simple modifications to particle deposition.\nNotice that particle deposition can be broken into two main steps. The first step\ndefines where to initially drop the particles. The second step defines where the parti-\ncles settle after they have been dropped. Let’s refer to the first step as particle place-\nment, and the second step as particle dynamics. In order to overcome the limitations\nof particle deposition, you need to improve both particle placement and particle\ndynamics. Let’s start by examining particle dynamics.\nImproving Particle Dynamics\nParticle dynamics are required to simulate the effects of erosion. After a particle is\ndropped on the height field, it begins randomly searching the adjacent positions to\ndetermine if the particle can move to a lower elevation. The slope of the terrain is\nimplicitly defined by how far away the particle is allowed to search. The monotony of\n",
      "content_length": 2400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": null,
      "content": "the terrain’s slope can be broken up by varying the search radius and elevation thresh-\nold of the particles placed on the slope. If the search radius is large, the slope will be\nshallow, as shown in Figure 5.1.4.a. Conversely, if the search radius is small, the slope\nwill be steep. Figure 5.1.4.b shows how particles can accumulate to form a very steep\nslope. To make this possible, the particle dynamics need to be changed so that parti-\ncles will not move to an adjacent position until the difference in elevation reaches a\ncertain threshold.\n356\nSection 5\nGraphics \nFIGURE 5.1.4\nIn (a), particles that search a large radius form a gentle slope. \nIn (b), particles with elevation thresholds larger than 1 form very steep slopes.\nThe search radius and elevation threshold of each particle can be chosen randomly,\nbut this will cause only small changes in the terrain’s slope. Better results can be\nachieved using a noise function. Noise will allow smooth transitions between gentle\nand steep slopes. There are several widely known noise functions but for simplicity the\nresults in this gem were obtained using value noise. Refer to [Ebert03] for a thorough\n(a)\n(b)\n",
      "content_length": 1168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": null,
      "content": "discussion of noise functions. Figure 5.1.5 demonstrates the difference between using a\nconstant search radius and a search radius defined by a noise function. In Figure 5.1.5,\nall of the particles were dropped at the same location to emphasize the change in slope\ncharacteristics. In a similar manner, the elevation threshold can be varied to create ter-\nrain with even more extreme slopes. The following pseudocode represents the particle\ndynamics used in this article:\nfor each dropped particle:\ndetermine the search radius using a 2D (or 3D) noise function\nwhile there is a lower position (within the search radius):\nmove to the closest position that is lower\nincrement the height field at the final position\n5.1\nAdvanced Particle Deposition\n357\nFIGURE 5.1.5\nThe terrain on the left was created using a constant search radius equal to 1,\nand terrain on the right was created using value noise to vary the search radius between 1 and 4.\nImproving Particle Placement\nThe particle placement heuristic defines where particles are initially dropped on the\nheight field. This is a very important step in particle deposition. If particle placement\nis random, the terrain features are going appear random. Different particle placement\nheuristics will generate different types of terrain. The next three sections investigate\ndifferent particle placement heuristics—each one designed to create a specific type of\nterrain.\nVolcanoes\nBefore you consider a suitable particle placement heuristic for volcanoes, it helps to\nknow how real volcanoes are formed. A volcano is formed by layers of ash and lava\nthat are ejected from its central vent. The layers of ash and lava accumulate, over\nmany years, to create a cone-like shape. The exact shape of the cone is determined by\nthe type of magma ejected from the volcano. Different magma types result in differ-\nent types of eruptions and landforms. Some volcanoes also have side vents and radiat-\ning fissures, which create more asymmetric shapes. Volcanoes can have gentle slopes\nor steep slopes, and they can have symmetric shapes or asymmetric shapes. Like all\nlandforms, the shape of a volcano is also defined by erosion of the surface.\n",
      "content_length": 2179,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": null,
      "content": "One possible particle placement heuristic for volcanoes would be to dump a lot of\nparticles at a single location until the stopping criteria are met. Although this might\nbe adequate, the resulting shape would be fairly symmetric and somewhat boring. A\nmore interesting particle placement heuristic, as demonstrated in Figure 5.1.6, is to\nloosely simulate the lava streams that wander radially from the central vent of the vol-\ncano. The pseudocode for this particle placement heuristic follows:\nchoose a position for the central vent\nchoose the number of streams\nchoose a random length and direction for each stream\nwhile the stopping criteria has not been met:\nfor each stream:\nstart at the central vent\nwhile the end of the stream has not been reached:\ndrop a particle and compute particle dynamics\nmove in the direction of the stream (+/- small random angle)\nUsing this heuristic, the shape of the volcano is defined by the number of streams,\nthe length of each stream (which doesn’t have to be the same for every stream), and\nthe total amount of particles dropped. The stopping criteria can be when a certain\nnumber of particles have been dropped or when the peak of the volcano has reached a\ncertain elevation. If you implement particle deposition in a way that allows users to\nwatch the terrain being generated in real-time, the users can stop the algorithm when\nthey are content with the results.\nIf a caldera at the peak of the volcano is desired, you can use the same inversion\nalgorithm used in [Shankel00] to invert the peak of the volcano. Begin by arbitrarily\nchoosing the elevation of the caldera plane, and invert the elevation at the central\nvent’s position across the caldera plane. Then check the neighboring positions, invert\nthem if they lie above the caldera plane, and check their neighbors. Repeat this\nprocess until there are no more neighbors to invert.\nNotice the shape of the volcano can now be more easily defined by a level designer.\nA level designer can choose the location of the volcano by deciding where to place the\ncentral vent. In addition, the paths of the lava streams can be precomputed, as shown\nin Figure 5.1.7, and overlaid on the height field. This would allow a level designer to\npreview the size and general shape without dropping a single particle.\nMountains\nParticle deposition can also create realistic mountains by using a clever particle place-\nment heuristic. The ridges between mountain peaks form a very distinct tree-like struc-\nture. For obvious reasons, this will be referred to as the mountain’s ridge structure. This\nis the result of many years of erosion, and the tree-like structure of the surrounding\nriver network is correlated to the mountain’s ridge structure. The ridge structure is\nimportant because it provides ideal locations where particles should be dropped, hence\nthe particle placement heuristic to generate mountains.\n358\nSection 5\nGraphics \n",
      "content_length": 2915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": null,
      "content": "Now you need a way to procedurally create a realistic ridge structure. Fortunately,\na suitable algorithm already exists. Diffusion limited aggregation (DLA) is a physical\nprocess that forms dendrite-like structures known as Brownian trees. Figure 5.1.8\nshows a Brownian tree that was created using DLA on a two-dimensional lattice.\n5.1\nAdvanced Particle Deposition\n359\nFIGURE 5.1.6\nA volcano created using advanced particle deposition. See the color insert\nsection for color versions of the photos from this gem.\nFIGURE 5.1.7\nAn example of the paths taken by simulated\nlava streams.\n",
      "content_length": 583,
      "extraction_method": "Direct"
    },
    {
      "page_number": 393,
      "chapter": null,
      "content": "Qualitatively this looks similar to the ridge structure of mountains. The pseudocode\nto create a Brownian tree on a two-dimensional lattice follows:\nchoose one or more seed positions\nwhile stopping criteria has not been met:\nplace a random walker at a random position\nmove the random walker until it is adjacent to a seed \n(i.e. touching)\nplace a new seed at the random walker’s position\nThe most obvious stopping criteria for a Brownian tree are when a desired num-\nber of particles have been dropped or when the Brownian tree covers a desired area or\nvolume. After the Brownian tree has been generated, it is straightforward to define the\nparticle placement heuristic to generate mountains. First, overlay the Brownian tree\non the height field. Then traverse the entire height field and drop a particle at every\nlocation covered by the Brownian tree. You’ll need to traverse the height field several\ntimes until the terrain features reach a desired size. Figure 5.1.9 shows the results\nusing this particle placement heuristic with the particle dynamics discussed earlier.\n360\nSection 5\nGraphics \nFIGURE 5.1.8\nAn example of a Brownian tree created\nwith DLA.\nNotice the Brownian tree provides a nice way to preview the shape of a mountain\nrange, like the lava streams of volcanoes, without needing to drop a single particle. A\nlevel designer can use the Brownian tree to easily decide the position and size of the\nmountains. The general shape of the Brownian tree can also be controlled by starting\nthe random walkers at positions that lie in the direction of desired growth. \n",
      "content_length": 1577,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": null,
      "content": "If you are familiar with L-systems (see [Prusinkiewicz96]), you may wonder if L-\nsystems can be used to generate a tree-like structure suitable for the particle placement\nheuristic. The answer is yes. However, L-systems require a grammar to define the tree’s\nstructure. The simplicity of Brownian trees was preferred for this article, but the\npotential of L-systems should not go unmentioned.\nDunes\nDunes are very interesting landforms that are usually associated with deserts, but can\nalso form underwater. Dunes found in the desert are formed by the wind so they are\nconstantly moving and changing shape. In fact, dunes have been recorded moving as\nmuch as 20 meters per year. There are several types of dunes, but this gem focuses on\na common type of dune called a traverse dune. Traverse dunes form a ridge that is per-\npendicular to the direction of the prevailing wind. As shown in Figure 5.1.10, a dune\nis formed as the wind rolls and tosses particles up the windward slope, and deposits\nthe particles on the leeward slope. This motion can be easily simulated using particle\ndeposition.\nOne obvious solution is to randomly pick particles off the height field and dis-\nplace them by a small random distance in the direction of the wind. However, this\ndoes not quite work. The missing key is that particles are more likely to be deposited\non the leeward slope than they are on windward slope because of wind’s “shadow” on\nthe leeward slope. To simulate this, you can assign a cost to the distance each particle\ntraverses. The cost of traveling up the windward slope should be less than the cost of\n5.1\nAdvanced Particle Deposition\n361\nFIGURE 5.1.9\nMountains created using advanced particle deposition. (Also shown in color\nin the color insert section.)\n",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": null,
      "content": "traveling down the leeward slop. The following pseudocode implements a suitable\ncost function, and Figure 5.1.11 demonstrates the results:\nwhile stopping criteria has not been met:\nchoose a random position and remove a particle\ndisplacement = small random number\nwhile displacement >= 0:\nmove the particle one position in the direction of the wind\nif the particle moves up\ndisplacement -= 1\nelse\ndisplacement -= 2\ndrop the particle and compute particle dynamics\n362\nSection 5\nGraphics \nFIGURE 5.1.10\nDunes are formed as particles are carried up the windward\nslope and deposited on the leeward slope.\nFIGURE 5.1.11\nDunes created using advanced particle deposition.\nIn this example the cost of traveling up the windward slope is only the horizontal\ndistance traveled (i.e. no vertical cost), and the cost of traveling down the leeward slope\nis the horizontal and vertical distance traveled. This is a very simple, yet effective, cost\nfunction. Different cost functions will yield different dune shapes and dynamics so\nexperimentation is encouraged.\nOverhanging Terrain\nAs shown in Figure 5.1.12, overhanging terrain is terrain that protrudes over other\nterrain. Particle deposition can create this type of terrain with some minor modifica-\n",
      "content_length": 1238,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": null,
      "content": "tions to the particle dynamics. Assign a stickiness attribute to each particle that is\ndropped on the terrain, and look at the path a particle takes as it falls toward the\nterrain. If a particle touches another particle at an adjacent position before landing on\nthe terrain, the stickiness of the falling particle will determine if the particle stops or\ncontinues to fall. As shown in Figure 5.1.13, when very sticky particles brush the face\nof a steep slope they will accumulate to form an overhang. The stickiness of a region\ncan be user-defined or it can be defined by a noise function. The following pseudocode\nprovides more details:\nchoose an arbitrary threshold, S\nfor each dropped particle:\ndetermine the particle’s stickiness, Sp (3D noise)\ncheck the particle’s path as it falls toward the height field\nif the particle touches an adjacent particle\ndetermine the adjacent particle’s stickiness, Sa (3D noise)\nif (Sp >= S) and (Sa >= S)\nleave the particle at this position\nelse\nuse the heuristic discussed in the particle dynamics section\n5.1\nAdvanced Particle Deposition\n363\nFIGURE 5.1.12\nAn example of overhanging terrain.\nFIGURE 5.1.13\nSticky particles attach\nto steep slopes as they fall to the surface.\n",
      "content_length": 1214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": null,
      "content": "Notice that traditional height fields cannot be used to define overhanging terrain\nbecause a height field is a two-dimensional lattice with elevation assigned to each\nposition (that is, a two-dimensional scalar field). Voxels can represent volumes in a\nthree-dimensional lattice (that is, a three-dimensional scalar field), and they are ideal\nfor modeling overhanging terrain because they can be polygonalized using a marching\ncubes/tetrahedrons algorithm. Voxel representations will increase the space and time\ncomplexity of particle deposition. Hybrid representations, which only use voxels\nwhere they are needed, can ameliorate some of this cost.\nConclusion\nParticle deposition is a powerful tool for creating various types of realistic terrain. The\nterrain types shown here do not represent an exhaustive list of what is possible with\nparticle deposition. Canyons, craters, caves, plateaus, terraces, and various outcroppings\nare just a few other examples of what might be possible using particle deposition.\nReferences\n[Ebert03] Ebert, David S., Musgrave, F. Kenton, Peachey, Darwyn, Perlin, Ken, and\nWorley, Steven. Texturing & Modeling: A Procedural Approach, Morgan Kauf-\nmann Publishers, 2003.\n[Grotzinger07] Grotzinger, John, et al. Understanding Earth, W. H. Freeman and\nCompany, 2007.\n[Prusinkiewicz96] Prusinkiewicz, Przemyslaw, and Lindenmayer, Aristid. The Algo-\nrithmic Beauty of Plants, Springer Verlag, 1996.\n[Shankel00] Shankel, Jason. “Fractal Terrain Generation—Particle Deposition,”\nGame Programming Gems, pp. 508–511. Charles River Media, 2000.\n364\nSection 5\nGraphics \n",
      "content_length": 1592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": null,
      "content": "365\n5.2\nReducing Cumulative Errors\nin Skeletal Animations\nBill Budge, Sony Entertainment of America\nbill_budge@playstation.sony.com\nT\nhis gem describes a simple trick to reduce the amount of cumulative error during\nplayback of skeletal animations. It is applied during the offline processing of anima-\ntion data, as part of the normal animation tool chain, and doesn’t affect the size of the\nanimation data. No changes are required in the runtime animation playback engine.\nA Quick Tour of Game Animation Systems\nIn the pioneering 3D game Quake, characters were animated by storing a separate\nmesh for each pose and rendering a different one each frame [Eldawy06]. This kind of\nanimation is simple and in theory capable of the highest quality, but it is difficult to\nmodify and blend animations, and a lot of memory is needed to store the meshes. For\nthese reasons, skeletal animation is now the standard in 3D games.\nSkeletal animation works by attaching the vertices of a single character mesh to a\ncollection of coordinate transforms—the bones of a “skeleton”—and animating the\nbones to deform the mesh. The vertices of the character mesh are positioned in a sin-\ngle coordinate space, together with the transforms that align the bones with the mesh.\nThese transforms make up what is called the “default” or “rest” pose of the skeleton.\nTo deform the mesh into a new pose, you first use the inverses of the rest transforms\nto take the vertices from their original space to “bone” space and then use the new\nbone transforms to move the vertices to their final positions. The equation for each\nvertex is as follows:\n(5.2.1)\nSkeletal animation works well because most game characters and objects are not\nshapeless blobs, but can be closely approximated by collections of rigid bodies. This\nleads to a great reduction in data because there are far fewer bones than mesh vertices\nto animate, and a great increase in flexibility, because it is much easier to modify and\nblend bone transforms than meshes.\nV\nM\nM\nV\n'=\n(\n)\n−\npose\nrest\n1\n",
      "content_length": 2031,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": null,
      "content": "A further observation leads to a trick that reduces the animation data by almost\nhalf again. Game characters and objects are not just random collections of rigid bod-\nies; they are jointed (at least until you blow them to pieces!). Each bone is connected\nto others at these joints, so you can organize the transforms into a hierarchy and make\nall but the root transform relative to its parent transform. Because jointed bones don’t\nmove relative to each other, all of the child translations reduce to constant vectors,\nwhich can be removed from the animation and stored with the skeleton. In fact,\nthey’re already there in the rest pose. Thus animations need only have a single transla-\ntion track for the root and rotation tracks for every bone.\nPlayback of the parent-relative transforms is straightforward. First, you construct\nthe root transform from the root translation and rotation tracks. Next, for each child\nof the root, you construct the child transform by concatenating the child rotation\nwith the parent transform. This process is repeated for the children’s children, and so\non, until you have reconstructed the transform for every bone in the hierarchy.\nFor a more in-depth description of skeletal animation and an introduction to\nskinning techniques to improve mesh deformation around joints, see [Lander98].\nCumulative Error\nUnfortunately, there is a price to pay for the data reduction that you achieved by mak-\ning the transforms parent-relative. It’s not so much the extra work—by doing the\nreconstruction in breadth-first order as described, you only require one additional\nmatrix concatenation per bone. The real problem is that Equation 5.2.1 has effec-\ntively become:\n(5.2.2)\nThe reconstruction of the transforms using Equation 5.2.2 is less robust than\nEquation 5.2.1. There are two reasons for this. First, any error at a transform higher\nin the hierarchy will affect every transform below it. An error at the root transform,\nfor example, will affect every other transform. Second, the error at each step will\nnaturally tend to accumulate, creating a larger error. For the purposes of this gem, we\nassume that the error at each transform behaves like a random variable (otherwise,\nyou would be able to compensate for it). Therefore, the second effect due to concate-\nnating rotations is like adding random variables\nThese two effects mean that the greatest error will be at bones that are furthest\nfrom the root. These are usually the character’s hands and feet. Such artifacts can be\nseen in many games. The classic example is a standing idle animation where the feet\nappear to slide over the ground. An even worse situation is when a character is grip-\nping a bat or sword with two hands. The gripped object is a leaf bone, parented to one\nof the hands. The other hand, being at the end of a different transform chain, will\nappear to be swimming around and through the object.\nV\nM\nM\nM\nM\nV\n'\n...\n=\n(\n)\n−\nroot\nparent\nchild\nrest\n1\n366\nSection 5\nGraphics \n",
      "content_length": 2980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": null,
      "content": "For parent-relative animations, you should be mostly concerned with rotation\nerror. Where does this error come from? A small amount is due to the use of floating\npoint arithmetic, which introduces round-off and precision error. However, by far the\nmost important sources of error are lossy compression schemes that most games use to\nfurther reduce the size of animation data.\nThere are many ways to compress rotation data. Some are lossless. For example,\njoints like the knees and elbows have only a single degree of freedom. Storing a full\nrotation such as a quaternion for each pose is wasteful. Instead, the axis of rotation\ncan be stored, and the animation data reduced to a series of angles.\nLossy compression algorithms can lead to even greater reductions in storage cost.\nOne of the simplest techniques is key frame reduction, which looks at the rotation val-\nues and tries to remove those values that can be interpolated from neighboring values\nwithout exceeding some error threshold. The problem with key frame reduction is that\nit is difficult to know which values to keep and which to reject. A better technique is to\nuse a curve fitting algorithm to convert the rotation values into a multidimensional\nspline curve that approximates the data to some tolerance [Muratori03]. Spline curves\nare a good fit to real-world rotational data, are very compact, and are easy to evaluate at\nruntime. Wavelet compression is another popular technique [Beaudoin07].\nEven if an efficient representation is found, storing lots of floating point data can\nbe inefficient if the numbers are in a known small range. If you are using quaternions,\nall numbers are in the range [–1…1], so the eight bits of exponent for each is waste-\nful. You can compress the numbers to 12- or 16-bit fixed point form. For an entire\ngem on quaternion compression, see [Zarb-Adami02].\nIt is common to push the compression algorithms to the point where artifacts due\nto cumulative errors just become visible. In this case, you will always have significant\nerrors at each rotation.\nFigure 5.2.1 shows a simple 2D hierarchy of two bones in the original pose and\nsome possible reconstructed poses, given the same random error at each transform.\nThe parent transform is shown with its error range as a gray region. Three child trans-\nforms are shown with their error ranges, one aligned with the actual child transform,\nand the other two where the parent has the greatest error. Note how the error at the\nends of the bones increases from parent to child.\nEliminating Cumulative Rotation Errors\nThe conventional algorithm for processing the animation begins by extracting all of\nthe transform data from the authored representation into a common coordinate space.\nNext, all child transforms are made parent-relative by concatenating with the parent’s\ninverse transform. Finally, the relative transforms are compressed and formatted for the\nruntime playback engine. Let’s call this the naive algorithm, because it assumes that\nthere is no error introduced by compression and reconstruction by the runtime.\n5.2\nReducing Cumulative Errors in Skeletal Animations\n367\n",
      "content_length": 3129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": null,
      "content": "The idea of this gem is to take reconstruction errors into account and use the recon-\nstruction algorithm to get the parent transform with error, and make the child trans-\nforms parent-relative to that transform rather than the original.\nThis leads to the following procedure, which we call Algorithm 1:\n1. Compress and format the root transform data.\n2. Run the decompression algorithm on the result of Step 1 and replace the\noriginal transform with the results.\n3. For each child of the root, make its transform data relative to the decompressed\nroot transform data. Compress and format the parent-relative rotation data.\n4. Run the decompression algorithm on each child from Step 3 to get its final\ntransform data and replace the original child data with the result.\n5. Continue down the hierarchy until all bones have been processed.\nThis completely eliminates the accumulation of rotational error because for each\nchild transform, Step 3 subtracts the rotational error of the parent transform. However,\nthe parent’s rotation error does more than just rotate the child. It also translates the\nchild (unless the child’s origin is at the parent’s origin). That means that the parent-\nrelative transform resulting from Step 3 will generally have a translation that is differ-\nent from the constant one you store with the skeleton. This translation error can’t be\neliminated by any child rotation. Although you could correct it by adding a new trans-\nlation, that would defeat the whole purpose of making the transforms parent relative,\nwhich was to eliminate these translations in the first place! So translation error is still\naccumulating, although total error is less than with the naive algorithm because the\nrotation error is less.\nFigure 5.2.2 shows the results of the naive algorithm and Algorithm 1. Note how\nthe child bones all have the same orientation as in the true pose (although still with\nlocal error), and how they are offset by the translation error as a result of the rotational\nerror of the parent bone.\n368\nSection 5\nGraphics \nFIGURE 5.2.1\nCumulative error increases from parent to child.\n",
      "content_length": 2110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": null,
      "content": "It is possible to reduce this translation error, but to do this you have to rotate the\nchild bone away from its true orientation. To calculate this rotation, you first select a\nfixed point on the bone where you would like to minimize the translation error. Let’s\ncall it a significant point. A significant point could be the origin of a child bone, or\nsome arbitrary point that identifies the “end” of the bone. You rotate the reconstructed\nbone from Algorithm 1 so as to move the significant point closest to its true position.\nFigure 5.2.3 shows the geometry.\n5.2\nReducing Cumulative Errors in Skeletal Animations\n369\nFIGURE 5.2.2\nRemoving cumulative rotational error.\nFIGURE 5.2.3\nReducing translation error at\nthe significant point.\nThe rotation is computed by the following equations:\n(5.2.3)\n(5.2.4)\nangle=\n⋅\n(\n)\n−\ncos 1 O'S'\nO'S\nAxis\nO'S'\nO'S\n=\n×\n",
      "content_length": 854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": null,
      "content": "Modify Step 3 of Algorithm 1 to get Algorithm 2:\n1. Compress and format the root transform data.\n2. Run the decompression algorithm on the result of Step 1 and replace the\noriginal transform with the results.\n3. For each child of the root:\na. Make its transform data relative to the decompressed root transform data.\nb. Concatenate this with the decompressed parent transform to get the\nreconstructed transform, without error.\nc. Compute the rotation that takes the significant point in this reconstructed\ntransform closest to its actual position, and add it to the transform.\nd. Compress and format the parent-relative rotation data.\n4. Run the decompression algorithm on each child from Step 3 to get its final\ntransform data and replace the original child data with the result.\n5. Continue down the hierarchy until all bones have been processed.\nFigure 5.2.4 shows the results of the naive algorithm and Algorithm 2. Note how\nthe child bones now have slight rotation errors (although they don’t accumulate, as\neach step still corrects for the parent’s error) and how the translation error has been\nreduced.\n370\nSection 5\nGraphics \nFIGURE 5.2.4\nReducing cumulative translational error.\nAlgorithm 2 does not completely eliminate translational error. One way to address\nthis is to add translation tracks at leaf bones to combat any objectionable artifacts.\nAnother way is to employ an inverse kinematics system to make sure that bones are\nwhere they should be. Even if a game uses an IK system, these error reduction tech-\nniques are useful because they improve the quality of the pose reconstruction so that it\nis closer to the artist’s original version.\n",
      "content_length": 1656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": null,
      "content": "Conclusion\nYou have seen how skeletal animation systems suffer from cumulative error, and how\nconventional processing of animation data can lead to noticeable artifacts on playback.\nWith a simple modification to the processing algorithm, however, you can eliminate\ncumulative rotational error and reduce the translation errors.\nOnly the preprocessing of animation is changed, so it has no performance or\nmemory impact on the game runtime. Finally, translation tracks can be added at\nimportant bones to eliminate any remaining artifacts.\nReferences\n[Beaudoin07] Beaudoin, Philippe. “Adapting Wavelet Compression to Human\nMotion Capture Clips,” available online at http://www.cs.ubc.ca/~van/papers/\n2007-gi-compression.pdf.\n[Eldawy06] Eldawy, Mohamed. “Trends of Character Animation in Games,” available\nonline at http://adlcommunity.net/file.php/23/GrooveFiles/Games%20Madison/\ncharAnimation.pdf.\n[Lander98] Lander, Jeff. “Skin Them Bones: Game Programming for the Web Gener-\nation,” Game Developer Magazine, May 1998, pp. 11–16, www.gamasutra.com/\nfeatures/gdcarchive/2000/lander.doc.\n[Muratori03] Muratori, Casey. “Discontinuous Curve Report,” available online at\nhttp://funkytroll.com/curves/emails.txt.\n[Zarb-Adami02] Zarb-Adami, Mark. “Quaternion Compression,” Game Program-\nming Gems 3, Charles River Media Press, 2002.\n5.2\nReducing Cumulative Errors in Skeletal Animations\n371\n",
      "content_length": 1383,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": null,
      "content": "373\n5.3\nAn Alternative Model for\nShading of Diffuse Light \nfor Rough Materials\nTony Barrera, Barrera Kristiansen AB\ntony.barrera@spray.se\nAnders Hast, Creative Media Lab, \nUniversity of Gävle\naht@hig.se\nEwert Bengtsson, Centre For Image Analysis,\nUppsala University\newert@cb.uu.se\nT\nhis gem shows how it is possible to improve the shading of rough materials with\na rather simple shading model. This gem discusses both the flattening effect,\nwhich is visible for rough materials, as well as the possible methods for creating the\nbackscattering effect.\nIntroduction\nUsually Lambert’s model (cosine law) [Foley97] is used to compute the diffuse light,\nespecially if speed is crucial. This model is used for both Gouraud [Gouraud71] and\nPhong [Phong75] shading. However, this model is known to produce plastic looking\nmaterials. The reason for this is that the model assumes that the object itself is perfect\nin the sense that the surface scatters light equally in all directions. However, in real life\nthere are no such perfect materials.\n",
      "content_length": 1036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": null,
      "content": "A number of models have been introduced in literature that can be used for met-\nals [Blinn77, Cook82]. These models assume that the surface consists of small v-\nshaped cavities. Oren and Nayar proposed a model for diffuse light suitable for rough\nsurfaces [Oren94, Oren95a, Oren95b]. This model can be used for rough surfaces,\nlike clay and sand. However, this model is quite computationally expensive, even in\nits simplified form. Nonetheless, the benefit of using their model is that it produces\nmore accurate diffuse light for rough surfaces. They showed that a cylindrical clay vase\nwill appear almost equally bright over the entire lit surface except for the edges where\nthe intensity drops quite suddenly.\nThe Lambert model will produce shadings which drop off gradually and this is\nseldom the case in real life. This effect is shown in Figures 5.3.1 and 5.3.2. Note that\nthe intensity is not scaled down for the Lambert shaded teapot in Figure 5.3.1 and\ntherefore it appears brighter than the teapot rendered with the Oren-Nayar model in\nFigure 5.3.2. Nonetheless, it is apparent that the intensity is almost equally bright\nover the surface for the Oren-Nayar model.\n374\nSection 5\nGraphics \nFIGURE 5.3.1\nA Lambert shaded teapot.\nWe proposed earlier in a poster a model that simulates the same behavior, but is\nmuch simpler and faster to compute [Barrera05]. This gem develops the idea further.\n",
      "content_length": 1401,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": null,
      "content": "The Flattening Effect\nOne of the main differences between Lambert’s model and the Oren-Nayar model is\nthat the Oren-Nayar model produces diffuse light that is almost equally bright over the\nsurface. This flattening effect can be modeled by forcing the diffuse light to be closer to\nthe maximum intensity, except on the edge where it should drop down rather quickly\nto zero. Thus, the shading curve will be horizontally flat over a large portion of the\ninterval. The following function could be used for this purpose:\n(5.3.1)\nwhere cosθ= n·l is the Lambert’s law, ρ is the surface roughness property that tells how\nflat (or close to one) the function should be, and k is a constant.\n(5.3.2)\nThe constant k makes sure that Id=1 for cosθ=1. Note that k can be precomputed\nand can also contain surface color.\nThe roughness property ρ is not derived in a way that it describes the physical\nbehavior in the way that Oren and Nayar does for the distribution of cavities. Instead\nk = +\n1 ρ\nρ\nI\nk\nd =\n−\n+\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n1\n1\n1 ρ\nθ\ncos\n5.3\nAn Alternative Model for Shading of Diffuse Light for Rough Materials\n375\nFIGURE 5.3.2\nAn Oren-Nayar shaded teapot.\n",
      "content_length": 1142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": null,
      "content": "it can be used to adjust the slope of the curve, thus simulating different degrees of\nroughness. Figure 5.3.3 shows Lambert (cos θ), the steepest curve, compared to the\nnew model with ρ = {0.75, 1.5, 3.0, and 6.0}. The larger the value for ρ that is used,\nthe closer to 1 the curve will be over the interval.\n376\nSection 5\nGraphics \nFIGURE 5.3.3\nIntensity for different angles between the normal\nand the light source vector for ρ = {0.75, 1.5, 3.0, and 6.0}.\nIn Figures 5.3.4 through 5.3.7, the effect of using the method is shown for a\nshaded teapot. Notice how the surface appears flatter when ρ increases.\nThe shader code in GLSL looks like this:\nuniform float shininess;\nvarying vec3 normal, color, pos;\nvoid main()\n{\nvec3 l = normalize(gl_LightSource[0].position.xyz - pos);\nvec3 n = normalize(normal);\nfloat nl=max(0.0, (dot(n,l)));\n// Flattening \nfloat rho=6.0;\nfloat k=(1.0+rho)/rho;\nfloat diff = k*(1.0-1.0/(1.0+rho*nl));\ngl_FragColor = vec4(color * diff, 1);\n}\n",
      "content_length": 971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": null,
      "content": "Backscattering\nThe backscattering effect is visible in many materials and it is a contributing reason to\nwhy you can see things quite well in the dark using a flashlight. However, it is a rather\nsubtle effect and it is quite hard to notice it in real life and it should therefore be used\nwith care. Because it is visible only when the light source is in the same direction as\nthe viewer, it could be modeled using l·v. The following equation was used as an\nattenuation factor that is multiplied with the diffuse light:\n5.3\nAn Alternative Model for Shading of Diffuse Light for Rough Materials\n377\nFIGURE 5.3.4\nρ is the surface roughness property\nthat tells how flat (or close to one) the function\nshould be; here ρ is 0.75.\nFIGURE 5.3.5\nHere the surface roughness prop-\nerty, ρ, is 1.5.\nFIGURE 5.3.6\nHere, ρ is 3.0. Notice how the\nsurface appears flatter when ρ increases.\nFIGURE 5.3.7\nHere, ρ is 6.0. The flattest surface\nof all.\n",
      "content_length": 931,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": null,
      "content": "(5.3.3)\nWe used the power function for f but the Schlick model [Schlick94] can also be\nused. This function determines how the effect will be distributed over the surface in a\nsimilar manner as for the specular light.\nThe constant b will determine how much impact the backscattering effect should\nhave on the diffuse light. A large b will yield a small effect and vice versa. In Figure\n5.3.8, a small b is used only to demonstrate the effect.\nIn Figure 5.3.9, it is clear that the backscattering effect vanishes as the viewer is\nlooking at the object from a different direction than the light source direction.\nF\nf\nb\nb\nbs =\n•\n(\n)+\n+\nl v\n1\n378\nSection 5\nGraphics \nFIGURE 5.3.8\nNote how the center of the teapot\nappears brighter because the light source is in the\nsame direction as the viewer.\nFIGURE 5.3.9\nThe light source is no longer in\nthe direction of the viewer and the backscattering\nis not visible.\nThe extra code needed for computing the backscattering is as follows:\nvec3 v = normalize(-pos);\nfloat lv=max(0.0, (dot(l,v)));\n// Backscattering\nfloat b=1.00000;\nfloat bs=(pow(lv,80.0)+b)/(1.0+b);\ngl_FragColor = vec4(color*diff*bs, 1);\nAnother possibility is to add the effect as a term of its own to the Phong-Blinn\nmodel. The following equation was used for Figure 5.3.10.\n(5.3.4)\nI\nK f\nbs\nbs\n=\n•\n(\n)\nl v\n",
      "content_length": 1311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": null,
      "content": "The constant Kks determines how much the effect will be visible and once again\nthe function f determines how the effect will be distributed over the surface. It should\nbe mentioned that the backscattering intensity was multiplied with the color of the\nsurface in the picture.\n5.3\nAn Alternative Model for Shading of Diffuse Light for Rough Materials\n379\nFIGURE 5.3.10\nOnce again the teapot appears brighter in\nthe center because the light source is in the same direction\nas the viewer.\nChange the code as follows:\nfloat kks=0.3;\nfloat bs=kks*pow(lv,80.0);\ngl_FragColor = vec4(color *(diff+bs), 1);\nConclusion\nThe Oren-Nayar model is rather complex while the proposed model is quite simple\nand easy to use. Still it produces a result that mimics behavior typical for rough mate-\nrials. You saw two possible ways of computing the backscattering effect and it is hard to\ntell which one is the better method. You can used large values for the power function to\nmake the difference visible in the images, but when an object is rotated interactively it\nis clear that a much lower value gives a more pleasing result.\n",
      "content_length": 1110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": null,
      "content": "References\n[Barrera05] Barrera, T., Hast, A., and Bengtsson, E. “An Alternative Model for Real-\nTime Rendering of Diffuse Light for Rough Materials,” SCCG’05 Proceedings\nII, pp. 27–28, 2005.\n[Blinn77] Blinn. J.F. “Models of Light Reflection for Computer Synthesized Pic-\ntures,” Proceedings of the 4th Annual Conference on Computer Graphics and\nInteractive Techniques, 1977, pp. 192–198.\n[Cook82] Cook, R.L., and Torrance, K.E. “A Reflectance Model for Computer\nGraphics,” ACM Transactions on Graphics (TOG), 1, 1, 1982, pp. 7–24.\n[Foley97] Foley, J.D. van Dam, A., Feiner, S.K., and Hughes, J.F. Computer Graphics:\nPrinciples and Practice, Second Edition in C, 1997, pp. 723–724.\n[Gouraud71] Gouraud H. “Continuous Shading of Curved Surfaces,” IEEE Transac-\ntions on Computers, Vol. c–20, No. 6, June, 1971.\n[Oren94] Oren, M., and Nayar, S.K. “Generalization of Lambert’s Reflectance\nModel,” Proceedings of the 21st Annual Conference on Computer Graphics and\nInteractive Techniques, 1994, pp. 239–246.\n[Oren95a] Oren, M., and Nayar, S.K. “Generalization of the Lambertian Model and\nImplications for Machine Vision,” International Journal of Computer Vision,\n1995, pp. 227–251.\n[Oren95b] Oren, M., and Nayar, S.K. “Visual Appearance of Matte Surfaces,” Science,\n267, 5201, 1995, pp. 1153–1156.\n[Phong75] Phong, B.T. “Illumination for Computer Generated Pictures,” Commu-\nnications of the ACM, Vol. 18, No. 6, June, 1975.\n[Schlick94] Schlick, C. “A Fast Alternative to Phong’s Specular Model,” Graphics\nGems 4, 1994, pp. 385–387.\n380\nSection 5\nGraphics \n",
      "content_length": 1553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": null,
      "content": "381\n5.4\nHigh-Performance\nSubdivision Surfaces\nChris Lomont\nchris@lomont.org\nS\nubdivision surfaces are a method of representing smooth surfaces using a coarser\npolygon mesh, often used for storing and generating high-detail geometry (usu-\nally dynamically) from low-detail meshes coupled with various scalar maps. They have\nbecome popular in modeling and animation tools due to their ease of use, support for\nmulti-resolution editing, ability to model arbitrary topology, and numerical robust-\nness. This gem presents extensions to Loop subdivision, blending results from numer-\nous places and adding useful implementation details. The result is a complete set of\nsubdivision rules for geometry, texture, and other attributes. The surfaces are suitable\nfor terrain, characters, and any geometry used in a game.\nThis gem also presents a general overview of methods for fast subdivision and\nrendering. After learning the material presented, you’ll be able to implement subdivi-\nsion surfaces in a production environment in either tools or in the game engine itself.\nIntroduction to Subdivision Schemes\nThere are many types of subdivision schemes, with varying properties. Some of the\nproperties are as follows:\n• Mesh type—Usually the mesh is made of triangles or quads.\n• Smoothness—This is the continuity of the limit surface, and is usually denoted\nC1, C2…, and so on, or G1, G2…., and so on.\n• Interpolating [Zorin96] or approximating—Interpolating schemes go through\nthe original data points, whereas approximating schemes may not.\n• Support size—This is the amount of neighboring geometry affecting the final\nposition of a given surface point.\n• Split—Some schemes work by replacing faces with more faces, others work by\nreplacing vertices with new sets of vertices. A few more work by replacing the\nentire previous mesh, making a “new” mesh.\nTable 5.4.1 lists common schemes and some data about them.\n",
      "content_length": 1905,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": null,
      "content": "Table 5.4.1\nSubdivision Schemes\nMethod\nMesh\nSmoothness*\nSplit\nScheme\nCatmull-Clark\nQuads\nC2\nFace\nApproximating\nDoo-Sabin\nAny\nC1\nVertex\nApproximating \nLoop\nTriangles\nC2\nFace\nApproximating\nButterfly\nTriangles\nC1\nFace\nInterpolating\nKobbelt\nQuads\nC1\nFace\nInterpolating\nReif-Peters\nAny\nC1\nNew\nApproximating\nSqrt(3) (Kobbelt)\nTriangles\nC2\nFace\nApproximating\nMidedge\nQuads\nC1\nVertex\nApproximating\nBiquartic\nQuads\nC2\nVertex\nApproximating\n*Smoothness generally has one degree less of continuity at exceptional points.\nAlthough this gem focuses mainly on generating geometry and rendering issues,\nsubdivision surfaces have many other uses, including:\n• Progressive meshes\n• Mesh compression\n• Multi-resolution mesh editing ([Zorin97])\n• Surface and curve fitting ([Lee98] and [Levin99])\n• Point set to mesh generation\nMost 3D animation and rendering packages support subdivision surfaces as a\nprimitive, although there is no standard type used throughout the industry. (See\nhttp://www.et.byu.edu/~csharp2/#A_SubD [as of 2007] for a partial list of toolsets\nsupporting subdivision.) Catmull-Clark and Loop subdivision are the most commonly\nused, because they are arguably the simplest, are well documented, and are well suited\nto real-time rendering. \nA related topic is PN triangles [Vlachos00], which provide a way to replace trian-\ngles at the rendering level with a smoother primitive. The basic idea is to quadratically\ninterpolate surface normals, similar to Phong shading, and to use this cubically to\ninterpolate new geometry. A good overview of subdivision is [Zorin00].\nSubdivision Schemes Usage\nFor a production tool chain for interactive games, one method for using subdivision\nsurfaces is to create art using high-resolution geometry and textures (and the geome-\ntry might be modeled in whatever subdivision flavors the tools support). The art is\nthen exported as high-density polygon models and associated data. Tools then reduce\nthe assets to a low poly count mesh with associated displaced subdivision maps, tex-\ntures, and animation data. A subdivision kernel in the GPU dynamically converts\nassets back to needed poly counts at runtime based on speed, distance from camera,\n382\nSection 5\nGraphics \n",
      "content_length": 2205,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": null,
      "content": "hardware support, and so on. This allows different subdivision surfaces to be used in\nthe asset creation and asset rendering stages, which has advantages.\nA tool along these lines is ZBrush, which allows you to edit meshes using sub-\ndivision surfaces in order to add geometry at multiple resolution levels, and then con-\nverts the resulting high-detail geometry to low-detail meshes and displacement maps.\nChoice of Subdivision Type\nThis gem covers implementing Loop subdivision [Loop87]. Some reasons are that it is\ntriangle based, making it (perhaps) easier to implement on GPUs, most artists and\ntools already work with triangle meshes, it is well studied, and it produces nice-looking\nsurfaces. Another common choice is Catmull-Clark subdivision [Catmull78], but\nbeing quad-based, it seems less suitable for gaming. Pixar uses Catmull-Clark subdivi-\nsion for animating characters. Many ideas presented in this article are applicable to\nquad-based subdivision as well as other schemes.\nLoop Subdivision Features and Options\nA single iteration of the original Loop Subdivision algorithm applied to a closed trian-\ngle mesh returns another closed triangle mesh with more faces. Repeated applications\nresult in a smooth limit surface. Extensions to the original method are needed to\nmodel more features; a full-featured subdivision toolset includes the following:\n• Boundaries—Allow non-closed meshes.\n• Creases—Allow sharp edges and surface ridges. Adding boundaries gives creases.\n(Creases technically should have the techniques in [Biermann06] to prevent minor\ncorner errors, but [Zorin00] claims these errors are visually minor. The corrections\nrequire more computation than what is presented and intended: a technique\nsuited for real-time rendering.)\n• Corners—Useful for making pointed items.\n• Semi-sharpness—Modifies the basic rules for boundaries, creases, and corners to\nget varying degrees of sharpness.\n• Colors and textures—Easy extensions of the subdivision process needed for ren-\ndering and gaming.\n• Exact positions—After a few subdivisions, vertices can be pushed to what would\nbe their final position if the subdivision were carried out to the limit. This com-\nputation is not very expensive.\n• Exact normals—Computing exact normals for shading is not very expensive, and\nis less costly than face normal averaging.\n• Displacement mapping—Adds geometry to the subdivided surface, and is a very\nnice feature to have, but not implemented in this gem. Instead, see [Lee00] and\n[Bunnell05].\n5.4\nHigh-Performance Subdivision Surfaces\n383\n",
      "content_length": 2552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": null,
      "content": "• Evaluation at arbitrary points—Allows for computing the limit surface at an\narbitrary position on the surface [Stam99]. This is useful for ray tracing or very\ndetailed collision detection, but for game rendering is not likely to be needed.\n• Prescribed normals—Allows for requesting specific normals on the limit surface\nat given vertices [Biermann06], and is useful for modeling. However, it is more\nexpensive to implement than what is presented in this gem, and for this and space\nreasons details, it is omitted. \n• Multi-resolution editing support—By storing all the levels of the subdivided\nmesh, users can work on any level of the subdivision, making many editing fea-\ntures easier [Zorin97].\n• Collision detection—Needed for game dynamics; one method is in [DeRose98].\n• Adaptive subdivision—Subdivides parts of the mesh into different amounts based\non some metric, patching any holes formed in the process. Adaptive subdivision is\nuseful for keeping polygon counts low while still giving nice curves, silhouettes,\nand level of detail. The decision on where to subdivide the mesh is usually based\non curvature.\nFeatures are added to the mesh by tagging vertices, faces, and edges with parame-\nters to direct the subdivision algorithm. Tag combination restrictions can be enforced\nin software to prevent degenerate cases if needed.\nGeometry Creation\nTo implement boundaries, creases, corners, and semi-smooth features, each vertex\nand edge is tagged with a floating-point weight 0 ≤w < ∞. A weight of 0 denotes stan-\ndard Loop subdivision, and ∞denotes an infinitely sharp crease or boundary. Infinity\nneed not be encoded in the data structures, because the weight is really a counter for\nthe levels of subdivision affected. Any number larger than the highest level of subdivi-\nsion performed will suffice. For example, 32767 should suffice, because it is unlikely\nthat any mesh will be subdivided this many times. \nLoop subdivision takes a mesh and creates a new mesh by splitting each old trian-\ngular face into four new faces, as shown in Figure 5.4.5. This is done in two steps. The\nfirst step inserts a new vertex on each existing edge, and the second step modifies old\nvertices (not those inserted on the edges).\nMost of these geometry rules are from [Hoppe94a] and [Hoppe94b], with some\nideas merged from [DeRose98] and [Schweitzer96].\nEdges\nThe first step inserts a new vertex on each edge using a weighted sum of nearby vertices.\nThe edge weights and the types of vertices at each endpoint of the edge serve to catego-\nrize the edges. Vertex categories are listed in the next section. \n384\nSection 5\nGraphics \n",
      "content_length": 2626,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": null,
      "content": "Each (non-boundary) edge has two adjacent triangles; the new vertex has the posi-\ntion (v0 + v1), where v0 and v1 are the vertices on the edge to split, and the other two \nvertices are the remaining vertices on the two adjacent triangles. This is illustrated in\nFigure 5.4.1, where the circle denotes the new vertex on the edge between the triangles.\nThe weights can be written \n, where position j corresponds to vertex j \n(0-indexed).\n3\n8\n3\n8\n1\n8\n1\n8\n,\n,\n,\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n3\n8(\n5.4\nHigh-Performance Subdivision Surfaces\n385\nFIGURE 5.4.1\nEdge mask.\nThe weights used to create a new edge depend on the edge weight and the vertex\ntypes of the two vertices that define the edge: v0 and v1. Given the two vertices on an\nedge, Table 5.4.2 shows which type of weights to use to create the new edge vertex.\nWeights are as follows:\n• Type 1 weights : \n.\n• Type 2 weights :\n.\n• Type 3 weights : \n, where the \nweight goes with the corner edge. \nAn edge is smooth if it has weight w = 0. An edge is sharp if its weight is w ≥0. If\nan edge has weight 0 < w < 1, the new vertex is linearly interpolated between the two\ncases w = 0 and w = 1, keeping the end vertex types fixed.\n3\n8\n3\n8\n5\n8\n0 0\n,\n, ,\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n1\n2\n1\n2\n0 0\n,\n, ,\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n3\n8\n3\n8\n1\n8\n1\n8\n,\n,\n,\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n",
      "content_length": 1259,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": null,
      "content": "Table 5.4.2\nEdge Mask Selection\nDart\nRegular Crease\nNon-Regular Crease\nCorner\nDart\n1\n1\n1\n1\nRegular crease\n1\n2\n3\n3\nNon-regular crease\n1\n3\n2\n2\nCorner\n1\n3\n2\n2\nWhen an edge is split, each new edge gets weight ˜w = max{w–1,0}. This gives finer\ncontrol over sharpness because the crease rules are applied to a few levels, and then the\nsmooth rules are applied, with possible interpolation on one step. An option for more\ncontrol is to tag each end of an edge with a weight, giving two weights per edge, to\ninterpolate the new edges, and then to make the corresponding changes throughout.\nNote in all cases the total weight sums to 1 (also true for vertex masks).\nVertices\nThe type of a vertex depends on the vertex weight and the types of incident edges. A\nsmooth vertex is one with zero incident sharp edges and weight 0. A dart vertex has one\nsharp incident edge and weight 0. A crease vertex has two sharp incident edges and\nweight 0. A corner vertex has > 2 sharp incident edges or has weight w ≥1. An interior\ncrease vertex is regular if it has six neighbors and exactly two non-sharp edges on each\nside of the crease; a boundary crease vertex is regular if it has four neighbors. Otherwise,\ncrease and boundary vertices are non-regular. If an edge has weight 0 < w < 1, it suffices\nto call it smooth for vertex classification.\nThe second step of Loop subdivision modifies all the original vertices (not the\nvertices inserted on each edge in step one) using a weighted sum of the original vertex\nand all neighboring vertices.\nThe weighting is dependent on the number n of neighboring vertices. For\nsmooth and dart vertices, this is illustrated in Figure 5.4.2. The value of b is usually\n, although other values are in the literature. (For example, \n[Warren95] proposes b(n) = 3/(8n) for n > 1 and b(3) = 3/16, but this has unbounded\ncurvature for a few valences.) The old vertex is given weight 1 – b(n)  and each old\nneighbor (not the vertices created in step one!) is given weight b(n)/n to determine\nthe new vertex position, which is then the weighted sum of all these vertices: \n.\nv\nb n\nv\nb n\nn\nv\nnew\nold\nj\nj\nn\n=\n−\n(\n)⋅\n+\n=∑\n1\n1\n( )\n( )\nb n\nn\n( )\ncos\n=\n−\n+\n⎧\n⎨\n⎩\n⎫\n⎬\n⎭\n⎛\n⎝\n⎜\n⎜\n⎞\n⎠\n⎟\n⎟\n1\n64\n40\n3 2\n2\n2\nπ\n386\nSection 5\nGraphics \n",
      "content_length": 2230,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": null,
      "content": "For corner vertices, the vertex position does not move, so vnew = vold.\nFor crease vertices, the new vertex is the sum of \nof the original vertex and\nof each of the two neighbors on the crease.\nIf a vertex has weight 0 < w < 1, the new vertex is linearly interpolated between\nthe two cases w = 0 and w = 1. A new vertex also has a new weight ˜w = max{w–1,0}.\nThe final case is when a vertex has weight 0 < wv < 1 and some neighboring edge\nhas weight 0 < we < 1, leading to many possible combinations of interpolation. In this\ncase evaluate each with weights 0 and weights 1, and interpolate on wv, instead bi-\nlinearly interpolating the four cases of the weights (0,0), (0,1), (1,0), and (1,1).\nAnother option is to require integer weights, avoiding interpolation cases entirely\nat the loss of control on semi-sharp creases.\nDisplacement-mapped surfaces are implemented by moving the vertices as needed\naccording to a displacement map. Vertices are also modified using [Biermann06] to\nimplement prescribed normals, and this also splits crease rules into convex and concave\ncases, avoiding certain degenerate cases.\nLimit Positions\nVertices can be projected to the position they would take if the surface were sub-\ndivided infinitely many times. This is often done after a few subdivision levels have\nbeen applied. This is optional and often doesn’t modify the surface much.\nLimit positions v∞are computed from a weighted sum of the current vertex v0\nand n neighbors vj. Corner vertices stay fixed, that is v∞= v0. Smooth vertices are pro-\njected using \n. A regular crease uses weights \nwith v0,\n1\n6\n2\n3\n1\n6\n,\n,\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\nv\nb n n\nv\nn\nv j\nj\nn\n∞\n=\n=\n+\n+\n+ ∑\n3\n8\n1\n1\n1\n0\n1\n( )(\n)\n1\n8\n3\n4\n5.4\nHigh-Performance Subdivision Surfaces\n387\nFIGURE 5.4.2\nVertex mask.\n",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 421,
      "chapter": null,
      "content": "getting . The two crease neighbors get weight , and the rest of the neighbors get\nweight 0. Similarly, non-regular creases use weights \n.\nVertex and Crease Normals\nTrue normals can be computed for each vertex, which should be done after comput-\ning limit positions for each final vertex. Surprisingly this is faster than computing\napproximate normals by averaging each adjacent face normal (partitioned to each side\nof a crease).\nComputing two true tangents and taking a cross product computes true normals\nat each vertex.\nFor a smooth or dart vertex, the two tangents are \nand\n.\nCrease and boundary vertices require more work. Normals are not defined per\nvertex for corners, and must be done for each face. Tangents need to be computed for\neach side of the crease. Along a crease (or boundary), one tangent is –1 times one\ncrease neighbor plus 1 times the other crease neighbor. The second tangent is more\ncomplicated to compute and is done as follows. Weights wj are computed for each ver-\ntex, with j = 0 being the vertex where a normal is desired. Then the other indices are\nnumbered j = 0,2...,n from one crease to another. The weights depend on the number\nof vertices and for each case are as follows:\nw0 = 0, w1 = wn = sin(z), wi = (2 cos(z) – 2)(sin(i–1)z),  \nfor n ≥5.\nThis creates over four subdivision levels from a tagged cube, with one face missing\nand marked as boundary, as in Figure 5.4.3. Notice that some corners have varying\nsemi-sharpness.\nFeature Implementation\nBesides geometry, a full solution needs colors, textures, and other per-vertex or per-face\ninformation.\nFace parameters like color and texture coordinates can be interpolated using the\nsame subdivision methods when new vertices are added. A simple method is to inter-\npolate by distance after the old vertices are modified, giving new values for the new\nfaces. Many features can be subdivided per vertex except at exceptional places, like\nalong an edge where texture coordinates form a seam. Some details for Catmull-Clark\nz\nn\n=\n−\nπ\n1\nw\nw w\nw w\n0\n1\n2\n3\n4\n2 1 2 2 1\n,\n,\n,\n,\n,\n, , ,\n(\n) = −−\n−\n(\n)\nw\nw w\nw\n0\n1\n2\n3\n1 0 10\n,\n,\n,\n, ,\n(\n) = −(\n)\nw\nw w\n0\n1\n2\n2 1 1\n,\n,\n, ,\n(\n) = −(\n)\nt\nv\nj\nn\nj\nj\nn\n2\n1\n2\n1\n=\n∗\n+\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n=∑\ncos\n(\n)\nπ\nt\nv\nj\nn\nj\nj\nn\n1\n1\n2\n=\n∗\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n=∑\ncos\nπ\n1\n5\n3\n5\n1\n5\n, ,\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n1\n6\n2\n3\n388\nSection 5\nGraphics \n",
      "content_length": 2318,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": null,
      "content": "surfaces (but applicable to Loop surfaces) are in [DeRose98]. Basically, per-vertex\nparameters are interpolated like vertex coordinates, so adding (u,v) texture coordi-\nnates is as simple as treating vertex points as (x,y,z,u,v) coordinates. Per-vertex textures\ndon’t allow easy handling of seams, in which case per-face texture coordinates are use-\nful. However, all internal points on a subdivided face become per-vertex parameters.\nPossible features to add but not covered in this article for lack of space are adaptive\ntessellation (where only part of the mesh is subdivided as needed for curvature, such as\nwith silhouettes, clipping, and so on, and making sure cracks aren’t introduced), and\ndisplaced subdivision surfaces (which add geometry by using a “texture” map to offset\ngenerated vertices as they are computed). Adaptive tessellation is covered in [Bunnell05]\nand displaced subdivision surfaces are covered in [Bunnell05] and [Lee00]. \nCollision Detection\nIf prescribed normals are not implemented, the surface has the convex hull property;\nthat is, sits inside the convex hull of the bounding mesh. This can be used for coarse\n5.4\nHigh-Performance Subdivision Surfaces\n389\nFIGURE 5.4.3\nExample geometry.\n",
      "content_length": 1219,
      "extraction_method": "Direct"
    },
    {
      "page_number": 423,
      "chapter": null,
      "content": "collision detection. More accurate (and more expensive) collision between subdivision\nsurfaces is covered in [Wu04] using a novel “interval triangle” that tightly bounds the\nlimit surface. [Severn06] efficiently computes the intersection of two subdivision sur-\nfaces at arbitrary resolutions. Collision detection will not be covered here further.\nIn the next section, a data structure is presented that accommodates Loop subdivi-\nsion on a triangle mesh. An algorithm follows that performs one level of subdivision,\nreturning a new mesh. This structure supports most of the features described in this\ngem, and is extensible to many of the other features.\nSubdivision Data Structure\nThere are many approaches in the literature for data structures used to store and\nmanipulate subdivision surfaces including half-edge, winged-edge, hybrid, and grids\n[Müller00]. For Loop subdivision, the data structure should allow finding neighbor\nvertices and incident edges easily, and preserve this ability on each level of subdivision. \nThere are many factors when designing the data structure. Converting a mesh to\na Loop-subdivided mesh is the main goal, leading to certain structures, but other\ntimes the end purpose is GPU rendering, in which case optimizing data structures for\nthis use makes sense. The approach presented here is somewhat of a hybrid, resulting\nin a data structure that ports easily to a GPU. A later section covers performance\nissues when moving to a GPU.\nThe following data structure is easy to read/write from files or elsewhere, fast to\nuse internally, and does not use pointers. Avoiding pointers makes it easier to move to\nGPUs or languages not as pointer friendly as C/C++, and makes the memory foot-\nprint smaller than the previously mentioned schemes (useful for large mesh tools),\nbecause instead of storing connectivity information explicitly, it is deduced from\nindex positions. This structure also makes sending meshes to a GPU easier because\nitems are arranged into vertex arrays, normal arrays, and so on, using indices to render\npolygons.\nData Structure\nSee Figures 5.4.4 and 5.4.5 for insight. Extensions allow storing all the levels of sub-\ndivision for multi-resolution editing.\nThe mesh and supported features are stored in various arrays. Each array is \n0-based. There is one array for each of the following:\n• Vertices array VA—Each vertex is a three-tuple x, y, z of floats, and a float sharp-\nness weight 0 ≤w < ∞, with 0 being smooth, and a half-edge index vh of a half-\nedge ending on this vertex (for fast lookup later). If the vertex is a boundary, vh is\nthe boundary half-edge index ending on the vertex. Optional per-vertex color,\ntexture, or index to a normal can also be stored.\n• Faces array FA—Each face represents a triangle, stored as three indices v0,v1,v2\ninto the vertex array. Also stored are three indices n0,n1,n2 into the normal array\n390\nSection 5\nGraphics \n",
      "content_length": 2913,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": null,
      "content": "NA, corresponding to the three vertex indices. Each face can optionally store\ncolor, texture, and other rendering information, per face or per vertex as desired.\nFaces are oriented clockwise or counter-clockwise as desired, but all must be ori-\nented the same way.\n• Half-edge array HA—Each face has three (half) edges in the half-edge array,\nstored in the same order. Thus, a face with index f and (ordered) vertex indices\n{v0,v1,v2} has ordered half-edges with indices 3f, 3f+1, and 3f+2, denoting half-\nedges from vertex v0 to v1, v1 to v2, and v2 to v0, respectively. Note that half-edges\nare directed edges, with the two half-edges of a pair having opposite directions. A\nhalf-edge entry is two values: an integer marking the pair half-edge index or a –1\nif it’s a boundary, and a floating-point sharpness weight 0 ≤w < ∞ denoting the\ncrease value, 0 being smooth, and larger values denoting sharpness. Each match-\ning half-edge pair must have the same crease values to avoid ambiguity. Note that\na half-edge index determines the corresponding face index, which in turn deter-\nmines a start and end vertex for the directed half-edge.\n• Normals Array NA—Normals can be included in the scheme in numerous ways\nwith varying tradeoffs. In order to handle creases, boundaries, and semi-sharp\nfeatures cleanly, you need one normal per vertex per face, but for many vertices\n(for example, smooth and regular) only a single normal is needed. An array of\nnormals accommodate this; each has a unit vector and a weight 0 ≤w < ∞telling\nhow fast a vertex normal converges to a prescribed normal, with 0 meaning no\nprescribed normal. Normals are referenced by index, thus avoiding redundant\nstores.\nBesides storing the size of each array, the number of edges E (where a matching\npair of half-edges or a boundary edge constitutes a single edge) is stored. This is not\ntoo costly to compute if the mesh has no boundary (E=# half-edges/2 = #faces*3/2),\nand can be computed otherwise by scanning the half-edge array and setting E=(size of\nHA+# of boundary edges in HA)/2.\nInformation about vertex types (smooth, crease, and so on) may also be stored on\na vertex tag for speed. Other items may also be tagged, but the algorithm described\nhere needs to be modified to maintain the invariants across subdivisions.\nUnneeded features can be dropped, such as three normals per face, prescribed\nnormals, or semi-sharp creases, but this loses finer grained control.\nA well-formed mesh requires a few rules. If a half-edge is not paired (it is on a\nboundary), it has pair index –1, and must have infinite crease weight. Otherwise, the\nedge will shrink. Each half-edge of the same edge must have the same weight; other-\nwise the edges will subdivide differently, creating cracks.\nFile Format\nBased on the data structure described here, a file format is defined as an extension to\nthe popular text based Wavefront *.OBJ format. An entry is a line of text, starting\nwith a token denoting the line type followed by space-separated fields. Various pieces\n5.4\nHigh-Performance Subdivision Surfaces\n391\n",
      "content_length": 3074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": null,
      "content": "of data are stored to speed up loading so all items such as paired edges do not need to\nbe recomputed each load. The format in order of file reading/writing is in Table 5.4.3.\nTable 5.4.3\nFile Format Entries\nEntry\nDescription\n#SubdivisionSurfL 0.1\nDenotes a non-standard OBJ file, versioned.\nsi v f e n\nOptional subdivision info giving number of vertices, faces, edges,\nand normals. Allows pre-allocation of arrays.\nv x y z \nOne entry per vertex with floating point position.\nf v1 v2 v3\nOne entry per face with one-based vertex indices, oriented.\nhd j wt \nHalf-edge data, one entry for each half-edge, in the order de-\nscribed by the faces, in half-edge order v0 →v1, v1 →v2, v2 →v0.\nEach entry is a one-based integer edge pair index j (or –1 for a\nboundary) and a floating-point weight wt.\nfc r1 g1 b1 a1 r2 g2 \nOptional face colors, one per vertex, RGBA, [0,1] floats. Not \nb2 a2 r3 g3 b3 a3\nallowed with per-vertex colors vc.\nvc r g b a\nOptional per-vertex color data, RGBA, [0,1] floats. Not allowed\nwith per-face colors fc.\nft u1 v1 u2 v2 u3 v3 texname\nOptional face textures with (u,v) floats in [0,1]. texname is\napplication dependent.\nfn nx ny nz w\nOptional normal data, with weights for prescribed normals. 0 is\ndefault weight.\nfni n1 n2 n3\nOptional face normal indices into the normal table, one normal\nper vertex. Requires fn entries.\nvs wt\nOptional vertex sharpness,[0,∞), with 0 being smooth and\ndefault; one per vertex.\nSubdivision Algorithm Details\nThis is an overview of the Loop subdivision algorithm. Let V = # old vertices, F = # old\nfaces, H = 3F = # of half-edges, and #E = number of edges = (H + # boundary edges)/2.\nOne level of subdivision consists of six steps:\n1. Compute new edge vertices.\n2. Update the original vertices.\n3. Split the faces.\n4. Create new half-edge information.\n5. Update the other features.\n6. Replace the arrays in the data structure with the new ones, and discard,\nstore, or free the old ones as desired.\nThese steps are described in detail in the following sections.\n392\nSection 5\nGraphics \n",
      "content_length": 2040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": null,
      "content": "Computing New Edge Vertices\nFollow these steps to compute the new edge vertices:\n1. Because each existing vertex will soon be modified (and originals need to be\nkept around until all are done), and because new vertices are going to be\nadded per edge, you allocate an array NV for all new vertices of size (# old\nvertices + # edges). When creating new edge vertices, the first V positions in\nthe array are skipped so the original vertices can be placed back in the same\npositions as they are modified.\n2. Allocate an array EM (edge map) of integers of size (# half-edges) to store\nindices mapping half-edges to new vertex indices. Initialize all to –1 to indi-\ncate half-edges not yet mapped.\n3. For each half-edge h, if EM[h] = –1, insert a vertex on the edge using the\nedge split rules. Store the new vertex in an unused slot in NV past the orig-\ninal V, and store the resulting NV index in EM[h]. If h2=E[h] is not –1, h\nhas a paired half-edge h2, so store the NV index in EM[h2] also. \nUpdating the Original Vertices\nNow you must move each original vertex to a new position, placing the new vertex in\nthe new vertex array NV, in the same order and position as before to make splitting\nfaces easy. This is done using the vertex modification rules from before. During updat-\ning, reduce vertex weights by 1, clamping at 0. New vertices have weight 0. Each ver-\ntex stores a half-edge index vh ending on the vertex, which is used to quickly walk\nneighboring vertices and determine edge types, as shown in Figure 5.4.4. Given a half-\nedge index h ending at the vertex, the joined neighbor vertex is VA[FA[Floor[h/3]].ver-\ntexIndex[h mod 3]]. Given eA, the next half-edge of interest is found by eB = EA[eA] and \n. With this information, the edges and neighboring vertices\ncan be queried rapidly.\nThe reason for requiring a boundary vertex to be tagged with an incoming crease\nis so traversal only needs to go in one direction, thus making the code simpler.\nAfter all updates, change all vertices (new and old) to have a half-edge index of\n–1, which denotes no incoming matching half-edge. These will be filled in during the\nface splitting.\nSplitting the Faces\nEach old face will become four new faces, split as shown in Figure 5.4.5. Figure 5.4.5\nshows the original triangle with edge and face orientations, and how this maps to new\nedge and face orientations, along with the order (0, 1, 2, 3) in which the new faces are\nstored.\ne\nFloor e\ne\nC\nB\nB\n=\n⎡\n⎣\n⎢\n⎤\n⎦\n⎥+\n+\n(\n)\n(\n)\n3\n3\n2\n3\nmod\n5.4\nHigh-Performance Subdivision Surfaces\n393\n",
      "content_length": 2530,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": null,
      "content": "1. Allocate an array NF for new faces of size 4F.\n2. For each face f, with vertex indices v0,v1,v2, look up the three edge vertex\nindices as j0=NV[3f+V], j1=NV[3f+1+V], and j2=NV[3f+2+V]. \n3. Split the faces in the order shown in Figure 5.4.5. To NFm add faces\n{j2,v0,j0}, {j0,v1,j1}, {j1,v2,j2}, {j2,j0,j1} at positions 4f, 4f+1, 4f+2, and 4f+3.\nThis order is important! Each parent half-edge ek is conceptually split into\ntwo descendent half-edges ekA, followed by ekB.\n4. During the face split, tag each vertex (which still has a –1 tag from the pre-\nvious steps) with an incident half-edge index ending on the vertex, giving\npreference to an incoming boundary.\n394\nSection 5\nGraphics \nFIGURE 5.4.4\nVertex neighbors.\nFIGURE 5.4.5\nFace splitting.\n",
      "content_length": 749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": null,
      "content": "Creating the New Half-Edge Information\nThis step could be merged with the split-face step, but is separated for clarity. A new list\nof half-edges is needed, correctly paired and weighted. Create a new half-edge array NE\nof size 12*F (three per new face, each old face becomes four new faces, producing 12). \nDefine a function nIndex(j,type) to compute new half-edge pair indices, where\nj is the old half-edge index, and type is 0=A or 1=B, denoting which part of the new\nhalf-edge is being matched. This function is as follows:\nfunction nIndex( j, type)\n/* data table for index offsets - matches new half-edges */\noffsets[] = {3,1,6,4,0,7}\n/* original half-edge pair index */\nop = EV[ei]\nif (op == -1) \nreturn –1   /* boundary edge */\n/* new position of the split-edges for the face with pair op */\nbp = 12*Floor[op/3]\n/* return the matching new index */\nreturn bp + offsets[2*(op mod 3) + type]\nThe {3,1,6,4,0,7} array comes from matching half-edges to neighboring half-\nedges and is dependent on inserting items in arrays as indicated. For each original face\nindex f, do the following:\n1. Let b=12*f be the base half-edge index for a set of new half-edges, which will\nbe stored in NE at the 12 indices b through b+11.\n2. Store the 12 new half-edge pair indices at b,b+1,…,b+11 in the following\norder: {e2B,e0A,b+9,e0B,e1A,b+10,e1B,e2A,b+11,b+2,b+5,b+8}, where eiT is\nnIndex(ei,type) with ei being the edge index. type = 0 for T = A and type\n= 1 for T = B. These are grouped three per face in the order of faces created\nin Figure 5.4.5.\n3. In the previous 12 entries, update the half-edge weights, with descendent\nhalf-edges getting the parent half-edge weights –1, clamped at 0. New half-\nedges with no parent get weight 0.\nUpdating Other Features\nPer-vertex colors and per-vertex texture coordinates can be updated during the edge\nvertex creation and during the vertex re-positioning steps by simple interpolation. Per\nface per vertex colors and textures coordinates can be interpolated in the previous\nsteps also, or can be done as a final step.\nDisplaced subdivision surface modifications can also be applied here by modify-\ning the current vertex positions using a displacement map.\n5.4\nHigh-Performance Subdivision Surfaces\n395\n",
      "content_length": 2235,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": null,
      "content": "If the surface is about to be rendered, temporary normals can be computed using\nstandard averaging techniques or by using the exact normals. Exact normals are more\nappropriate on a surface with vertices at limit points. Normals don’t usually need to be\ncomputed until render time.\nFinal Step\nThe final step is to replace the arrays in the data structure with the new ones, and dis-\ncard, store, or free the old ones as desired.\nFinally, if the mesh is not going to be subdivided further, vertices can be pushed\nto limit positions, and true normals can be computed. In a rendering engine where\nthe object is about to be drawn, this is an appropriate step.\nPerformance Issues\nThis section contains an overview of hardware rendering techniques for this algorithm.\nPerformance Enhancements\nThere are many places to improve the performance of the algorithm itself, especially if\nall the features are not needed. If all you need is a simple, smooth, closed mesh, you\ncan remove all the special cases, making subdivision very fast.\nConsider these implementation tips:\n• Use tables for the b(n) based weights, the tangent weights, normal weights, limit\nposition weights, as well as any other items. A given mesh has a maximum valence\nvertex and all new vertices have valence at most, which makes using tables feasible.\n• Make the half-edge array spaced out by four entries per face instead of three,\nallowing many divide by three and mod three operations to be replaced with\nshifts. This is the traditional space-for-speed tradeoff.\n• Most interior vertices will have valence 6 and be smooth, so make that code fast,\nwith special cases for the other situations. Most boundary vertices will be regular\nwith valence 4. Most edges will be weight 0 and connect valence 6 smooth vertices.\n• Tag edges and vertices for whether they are smooth or need special case code,\nallowing faster decisions, instead of determining vertex and edge types by walking\nneighbors. Once a vertex or edge type is determined it is easy to tag descendents.\n• Pre-compute one level of subdivision to isolate the special case vertices, and then\nat runtime use a simpler version of the algorithm since there are fewer cases. This\nis a minor speed improvement, and is used for some hardware implementations.\n• A pointer-based data structure like a half-edge structure can speed up subdivision\nat the cost of using more (and likely less contiguous) memory and making reading/\nwriting harder. It is not clear which is really faster until you do some tests.\n396\nSection 5\nGraphics \n",
      "content_length": 2540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": null,
      "content": "• Move per-vertex per-face parameters to per-vertex when possible. For example,\ncreases require per-vertex per-face normals because neighboring faces require dif-\nferent normals along the crease, but once a face has been subdivided, the interior\nsmooth new vertices can (and should) use per-vertex normals.\n• Do multiple subdivision steps at once if desired, storing only the resulting triangles,\nand not updating all the connectivity info. This is detailed in the GPU section.\n• Implement adaptive subdivision. Having fewer triangles to split after a few steps\nwill speed things up a lot (but will break the simple algorithm operating on the\ndata structure presented).\nGPU Subdivision and Rendering\nI dropped my original plan to present a state-of-the art GPU subdivision renderer\nonce I reviewed the literature and learned how fast such articles become obsolete.\nInstead the focus here is putting in one place unified rules for a subdivision scheme.\nThis will assist future hardware and software implementations, making this gem use-\nful for a longer period.\nFor GPU rendering, the following is a chronological review of several papers,\nmost of which can be found on the Internet. The papers are roughly evenly divided\nbetween Catmull-Clark methods, Loop methods, and universal methods:\n• [Pulli96] presents an efficient Loop rendering method. It works by grouping tri-\nangles into pairs during a precomputation phase, effectively passing squares and a\n1-neighborhood to a rendering function, which then renders the two triangles to\nan arbitrary subdivision depth.\n• [Bischoff00] presents a very memory efficient and fast Loop rendering solution.\nThe main concept is using multivariate forward differencing to generate triangles\nseveral subdivision levels deep without having to generate the intermediate levels.\nRendering is done patch by patch.\n• [Müller00] presents an extension to [Pulli96], and details a triangle paring algo-\nrithm and a sliding window method. Details are also presented for adaptive sub-\ndivision and crack prevention.\n• [Leeson02] covers a few subdivision methods, and gives an overview of some ren-\ndering tips such as hierarchical backface culling.\n• [Bolz02] implements Catmull-Clark subdivision, using a static array to hold the\nresults. The methods are good for SIMD implementation.\n• [Bolz03] implements Catmull-Clark subdivision on a GPU, with special attention\ngiven to avoiding cracks and pixel dropout caused through floating point errors.\n• [Boubekeur05] presents a general method useful for rendering many types of\nsubdivision surfaces. The main idea is to implement a “refinement pattern” on\nthe GPU. Each triangle or other primitive passed to the GPU is then refined\nusing the pattern. \n• [Bunnell05] and [Shiue05] both implement Catmull-Clark subdivision on a\nGPU, with ample details. \n5.4\nHigh-Performance Subdivision Surfaces\n397\n",
      "content_length": 2874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": null,
      "content": "Fast Subdivision Surface Rendering\nA fast subdivision routine suited for a GPU is based on the following observation. For\neach triangle, upon subdividing, the new items (vertices, edges, faces, colors, and so\non) are a linear combination of a 1-neighborhood of the triangle. Second subdivision\nitems are then linear combinations of first subdivided items, hence a linear combina-\ntion of the original neighborhood. This is exploited in various ways in the preceding\nreferences, and will be explained in a simple case.\nA patch is single triangle T and the surrounding triangles (those that influence\ndescendent triangles from the triangle T). See Figure 5.4.6 for a patch illustration—\non the left, T is shaded and a 1-neighborhood is included. The right side shows T sub-\ndivided once, with a new 1-neighborhood (without all edge lines drawn). \n398\nSection 5\nGraphics \nFIGURE 5.4.6\nSubdividing a patch.\nAssume for the moment there are no creases or boundaries (which can be added\nback in later). All the subdivision levels beneath T can be generated from linear com-\nbinations of existing vertices, so for each level of desired subdivision a mask can be\ncomputed in terms of neighboring vertices that outputs all the triangles descended\nfrom T, without needing to compute intermediate levels. Connectivity information does\nnot need to be computed or stored either—all that is desired are the vertices of the faces,\nwhich naturally fall into a grid arrangement and are suitable for GPU rendering.\nMesh precomputation gathers needed data for each patch, stored per triangle. At\nrender time, a subdivision level is selected, and each patch is passed to a GPU kernel.\nThe GPU kernel then takes the low-resolution triangle, creates subdivided triangles in\none pass, and renders the resulting triangles. In order to incorporate all the features\nfrom the gem, different kernels should be implemented. Alternatively preprocessing\ncould simplify the numbers of cases, resulting in fewer GPU kernel variations.\nA final point is this method might result in pixel dropout or cracks, because\nneighboring triangles may be evaluated using floating point operations in different\norders. This is addressed in [Bolz03] for Catmull-Clark surfaces.\n",
      "content_length": 2229,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": null,
      "content": "Conclusion\nThis article showed details of how to implement Loop subdivision surfaces with addi-\ntional features and provides a starting point for the literature on subdivision surfaces.\nGeometry features such as creases, boundaries, semi-sharp items, and normals were\ncovered, as well as surface tags like colors and textures. Future directions would be to\nadd displaced subdivision surfaces and adaptive subdivision to the algorithm.\nReferences\n[Biermann06] Biermann, Henning, Levin, Adi, and Zorin, Denis. “Piecewise Smooth\nSubdivision Surfaces with Normal Control,” Proceedings of the 27th Annual Con-\nference on Computer Graphics and Interactive Techniques, pp. 113–120, 2006. \n[Bischoff00] Bischoff, Stephan, Kobbelt, Leif, and Seidel, Hans-Peter. “Towards\nHardware Implementation of Loop Subdivision,” Eurographics SIGGRAPH\nGraphics Hardware Workshop, 2000 Proceedings. \n[Bolz02] Bolz, Jeffery, and Schröder, Peter. “Rapid Evaluation of Catmull-Clark \nSubdivision Surfaces,” Web3d 2002 Symposium, available online at http://\nwww.multires.caltech.edu/pubs/fastsubd.pdf.\n[Bolz03] Bolz, Jeffery, and Schröder, Peter. “Evaluation of Subdivision Surfaces on\nProgrammable Graphics hardware,” available online at http://www.multires.cal-\ntech.edu/pubs/GPUSubD.pdf.\n[Boubekeur05] Boubekeur, Tamy, and Schlick, Christophe. “Generic Mesh Refine-\nment on GPU,” ACM SIGGRAPH/Eurographics Graphics Hardware, 2005.\n[Bunnell05] Bunnell, Michael. “Adaptive Tesselation of Subdivision Surfaces with\nDisplacement Mapping,” GPU Gems 2, 2005, pp. 109–122.\n[Catmull78] Catmull, E., and Clark, J. “Recursively Generated B-Spline Surfaces on\nArbitrary Topological Meshes,” Computer Aided Design 10, 6(1978), pp. 350–355.\n[DeRose98] DeRose, Tony, Kass, Michael, and Truong, Tien. “Subdivision Surfaces\nin Character Animation,” International Conference on Computer Graphics and\nInteractive Techniques, SIGGRAPH 1998, pp. 85–94. \n[Hoppe94a] Hoppe, Huges. “Surface Reconstruction from Unorganized Points,”\nPhD Thesis, University of Washington, 1994, http://research.microsoft.com/\n~hoppe/.\n[Hoppe94b] Hoppe, Huges, DeRose, Tony, DuChamp, Tom, et. al. “Piecewise\nSmooth Surface Reconstruction,” Computer Graphics, SIGGRAPH 94 Proceed-\nings, 1994, pp. 295–302.\n[Lee98] Lee, Aaron, Sweldens, Win, et. al. “MAPS: Multiresolution Adaptive Para-\nmeterization of Surfaces,” Proceedings of SIGGRAPH 1998. \n[Lee00] Lee, Aaron, Moreton, Henry, and Hoppe, Huges. “Displaced Subdivision\nSurfaces,” SIGGRAPH 2000, pp. 95–94.\n[Leeson02] Leeson, William. “Subdivision Surfaces for Character Animation,” Game\nProgramming Gems 3, 2003, pp. 372–383.\n5.4\nHigh-Performance Subdivision Surfaces\n399\n",
      "content_length": 2657,
      "extraction_method": "Direct"
    },
    {
      "page_number": 433,
      "chapter": null,
      "content": "[Levin99] Levin, Adi. “Interpolating Nets of Curves by Smooth Subdivision Sur-\nfaces,” Proceedings of SIGGRAPH 99, Computer Graphics Proceedings, Annual\nConference Series, 1999.\n[Loop87] Loop, Charles. “Smooth Subdivision Surfaces Based on Triangles,” Master’s\nThesis, University of Utah, Dept. of Mathematics, 1987, available online at\nhttp://research.microsoft.com/~cloop/thesis.pdf. \n[Müller00] Müeller, Kerstin, and Havemann, Sven. “Subdivision Surface Tesselation\non the Fly Using a Versatile Mesh Data Structure,” Comput. Graph. Forum, Vol.\n19, No. 3, 2000, available online at http://citeseer.ist.psu.edu/. \n[Pulli96] Pulli, Kari, and Segal, Mark. “Fast Rendering of Subdivision Surfaces,” Pro-\nceedings of 7th Eurographics Workshop on Rendering, pp. 61–70, 282, Porto,\nPortugal, June 1996. \n[Schweitzer96] Schweitzer, J.E. “Analysis and Application of Subdivision Surfaces,”\nPhD Thesis, University of Washington, Seattle, 1996, available online at\nhttp://citeseer.ist.psu.edu/.\n[Severn06] Severn, Aaron, and Samavati, Faramarz. “Fast Intersections for Subdivi-\nsion Surfaces,” In International Conference on Computational Science and its\nApplications, 2006. \n[Shiue05] Shiue, L.J., Jones, Ian, and Peters, Jörg. “A Realtime GPU Subdivision Ker-\nnel,” ACM SIGGRAPH Computer Graphics Proceedings, 2005. \n[Stam99] Stam, Jos. “Evaluation of Loop Subdivision Surfaces,” SIGGRAPH 99\nCourse Notes, 1999, http://www.dgp.toronto.edu/people/stam/. \n[Vlachos00] Vlachos, Alex, Peters, Jörg, Boyd, Chas, and Mitchell, Jason. “Curved\nPN Triangles,” ID3G 2001, http://www.cise.ufl.edu/research/SurfLab/papers/. \n[Warren95] Warren, J. “Subdivision Methods for Geometric Design,” Unpublished\nmanuscript, November 1995.\n[Wu04] Wu, Xiaobin, and Jörg Peters, “Interference Detection for Subdivision Sur-\nfaces,” EUROGRAPHICS, 2004. Vol. 23, 3. \n[Zorin96] Zorin, Denis, Schröder, Peter, and Sweldens, Wim. “Interpolating Subdivi-\nsion for Meshes with Arbitrary Topology,” Proceedings of SIGGRAPH 1996,\nACM SIGGRAPH, 1996, pp. 189–192. \n[Zorin97] Zorin, Denis, Peter Schröder, and Wim Sweldens. “Interactive Multi-\nResolution Mesh Editing,” CS-TR-97-06, Department of Computer Science,\nCaltech, January 1997, available online at http://graphics.stanford.edu/~dzorin/\nmultires/meshed/. \n[Zorin00] Zorin, Denis and Schröder, Peter. “Subdivision for Modeling and Anima-\ntion,” Technical Report, ACM SIGGRAPH Course Notes 2000, available online\nat http://mrl.nyu.edu/~dzorin/sig00course/. \n400\nSection 5\nGraphics \n",
      "content_length": 2497,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": null,
      "content": "401\n5.5\nAnimating Relief Impostors\nUsing Radial Basis Functions\nTextures\nVitor Fernando Pamplona, \nInstituto de Informática: UFRGS\nvfpamplona@inf.ufrgs.br\nManuel M. Oliveira, \nInstituto de Informática: UFRGS\noliveira@inf.ufrgs.br\nLuciana Porcher Nedel, \nInstituto de Informática: UFRGS\nnedel@inf.ufrgs.br\nG\names often use simplified representations of scene elements in order to achieve\nreal-time performance. For instance, simple polygonal models extended with\nnormal maps and carefully crafted textures are used to produce impressive scenarios\n[IdSoftware], while billboards and impostors replace distant objects. More recently,\nrelief textures [Oliveira00] (textures containing depth and normal data on a per-texel\nbasis) have been used to create impostors of detailed 3D objects using single quadri-\nlaterals and preserving self-occlusions, self-shadowing, view-motion parallax, and\nobject silhouettes [Policarpo06].\nRelief rendering simulates the appearance of geometric surface detail by using the\ndepth and surface normal information to shade individual fragments. This is obtained\nby performing ray-height-field intersection in 2D texture space, entirely on the GPU\n[Policarpo05]. The mapping of relief details to a polygonal model is done in the con-\nventional way, by assigning a pair of texture coordinates to each vertex of the model.\n",
      "content_length": 1347,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": null,
      "content": "Relief impostors are obtained by mapping relief textures containing multiple layers of\ndepth, normals, and color data per texel onto quadrilaterals [Policarpo06]. \nIntroduction\nTextures in general can be used to represent both static and animated objects, and\ntexture-based animation traditionally uses techniques such as image warping or a set\nof static textures cyclically mapped onto some polygons. Although conventional\nimage warping techniques are limited to some planar deformations, the second\napproach requires as many textures as frames in the animation sequence, which, in\nturn, tends to need a significant amount of artwork. \nThis gem describes a new technique for animating relief impostors based on a sin-\ngle multilayer relief texture using radial basis functions (RBF). The technique preserves\nthe relief-impostor properties, allowing the viewer to observe changes in occlusion and\nparallax during the animation. This is illustrated in Figure 5.5.1, which shows three\nframes of a dog walking animation sequence created from a dog relief impostor. Note\nthe changes in the positions of the dog’s legs. \n402\nSection 5\nGraphics \nFIGURE 5.5.1\nThree frames of a dog walking animation created by warping a relief impos-\ntor. Note the changes in the positions of the legs.\nIn order to produce these animations, during a pre-processing step, the user spec-\nifies a set of control points over the texture of the relief impostor. Moving the control\npoints in 2D warps the texture, thus bringing the represented objects into new poses.\nSuch poses are the key poses to be interpolated during the animation. Note that these\nposes are only implicitly represented by the control points and by a single texture.\nThis situation is illustrated in Figure 5.5.2.\nAs part of the pre-processing, the algorithm also interpolates the positions of\nthese control points for the desired number of frames in the animation and, for each\nof them, solves a linear system to obtain a set of RBF coefficients. The control points\nand their corresponding RBF coefficients define a series of warping functions that\nproduce the actual animation. For efficiency reasons, these control points and coeffi-\ncients are stored in a texture (usually 16 \u0002 16 or 32 \u0002 32 texels). At runtime, this\ndata is used to recreate the animation on the GPU. \n",
      "content_length": 2317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": null,
      "content": "The proposed technique can be used to animate essentially any kind of texture-\nbased representations, such as relief textures [Oliveira00], billboards with normal\nmapping, and displacement maps [Cook84]. Note that it is also possible to replace\nthe RBFs with any other method that describes the desired transformation and that\ncan be evaluated on a GPU. The proposed technique produces real-time realistic ani-\nmations of live and moving objects undergoing repetitive motions.\n5.5\nAnimating Relief Impostors Using Radial Basis Functions Textures\n403\nFIGURE 5.5.2\nControl points (dark dots) placed over the texture of the relief impostor (top\nrow) warp the texture, changing the pose of the rendered dog (bottom row). All poses are\nimplicitly represented by a single texture and the sets of control points. \nImage Warping\nWarping-based texture animation evaluates a function over the source image in order\nto compute each frame of the sequence. Given a source image, a warping function\nproduces an output image by computing new coordinates for each source pixel. Image\nwarping then comprises two steps:\n• A mapping stage that associates source and target pixels’ coordinates.\n• A re-sampling stage.\nThe mapping is usually computed using a global analytic function built from a\nset of correspondences involving control points in the source and the target images.\nMany techniques, such as triangulation based, inverse-distance weighted interpola-\ntion, radial basis functions, and locally bounded radial basis functions, are available to\ngenerate the mapping function from a set of corresponding points [Ruprecht95]. \n",
      "content_length": 1615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": null,
      "content": "Radial Basis Functions\nRadial basis function (RBFs) methods are a mathematical way to produce multivari-\nate approximation and one of the most popular choices when interpolating scattered\ndata [Buhmann03]. In computer graphics, RBFs have been used for surface recon-\nstruction from point clouds [Carr01], for image warping [Ruprecht95], and for ani-\nmation [Noh00]. An RBF is defined in Equation 5.5.1: \n(5.5.1)\nHere, N is the number of centers, φ is a basis function, λi is the i-th coefficient for\nthe RBF representation, ci is the i-th center, and x is a point for which the function\nwill be evaluated. In the case of image warping, ci represents the pixel coordinates of\nthe control points, and x represents the pixel coordinates of any pixel in the image. In\nthis case, a good choice of φ is the multiquadrics, originally proposed by Hardy\n[Hardy71]: \n(5.5.2)\nwhere di = \nand r is a positive arbitrary characteristic radius that can be a con-\nstant or a different value per control point. In Equation 5.5.2, r represents the\nsmoothness of the interpolation and is critical for good image warping results. In our\nexperiments, we used r = 0.5, as suggested in [Ruprecht95].\nThe warping problem can be modeled using the linear system shown in Equation\n5.5.3, where φij is the distance between control points ci and cj expressed in pixel coor-\ndinates, fkx and fky are, respectively, the x and y image coordinates of control point ck.\nλkx and λky are the RBF coefficients that you want to solve for. Once such coefficients\nhave been obtained, you can use RBFs as warping functions.\n(5.5.3)\nInterpolating the Warping Functions\nGiven two sets of control points St and St+k specified by the user for two key poses at\ntimes t and (t+k), respectively, the RBF coefficients for the intermediate poses are\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nφ\n11\n12\n13\n1\n21\n22\n23\n2\n31\n32\n33\n...\n...\n...\nm\nm\n3\n1\n2\n3\nm\nn\nn\nn\nnm\n...\n...\n...\n...\n...\n...\nφ\nφ\nφ\nφ\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\nλ\nλ\nλ\nλ\nλ\nλ\nλ\nλ\n1\n2\n3\n1\n2\n3\nx\nx\nx\nnx\ny\ny\ny\nny\n... ...\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥\n=\nf\nf\nf\nf\nf\nf\nf\nf\nx\nx\nx\nnx\ny\ny\ny\nn\n1\n2\n3\n1\n2\n3\n... ...\ny\n⎡\n⎣\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢\n⎢\n⎤\n⎦\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥\n⎥\nx ci\n−\nφ( )\nd\nr\nd\ni\ni\n=\n+\n2\n2\nf x\nx c\ni\ni\ni\nN\n( ) =\n−\n(\n)\n=∑λ φ\n1\n404\nSection 5\nGraphics \n",
      "content_length": 2227,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": null,
      "content": "obtained by interpolating the coordinates of the corresponding pairs of control points\nin St and St+k, and solving Equation 5.5.3 for the interpolated λs. To achieve some\nsmooth interpolation, we used a cubic Hermite spline, where the end points of the\ntangents are given by the vector 0.5(St + St+k). This is illustrated in Figure 5.5.3.\nWhen using normal maps, the same warping approach has to be applied to the nor-\nmal map as well. Thus, both textures must be evaluated using the same RBF for each\nframe.\n5.5\nAnimating Relief Impostors Using Radial Basis Functions Textures\n405\nFIGURE 5.5.3\nThe light gray frames between time\n0.0 and 0.5 used a cubic Hermite spline to interpolate\nthe control point. \nEvaluating the Warping Function Using Shaders\nModern GPUs can execute programs called shaders. As the warping function needs to\nbe executed for each texel, it is clear that the RBF should be evaluated on a fragment\nshader. But for this, it is necessary to invert the warping functions because, given a\nfragment f, you must be able to obtain the texture coordinates that were mapped to f\nunder the warping transformation. Fortunately, inverting the warping function using\nEquation 5.5.3 only requires two steps:\n• Compute φij using the coordinates of the control points of the current (desired)\npose.\n• Use the x and y coordinates of the unmodified (before moving) control points as\nfkx and fky.\nFor the example shown in Figure 5.5.2, the RBF coefficients used for rendering\nthe image in the bottom center were computed as follows: φij are the distances\nbetween the control points ci and cj shown in the top center, whereas fkx and fky are the\ncoordinates of the k-th control point shown in the top left. Note that the re-sampling\nneeded as the second step of an image warping operation is provided for free by the\ntexture filtering hardware. \n",
      "content_length": 1848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 439,
      "chapter": null,
      "content": "As previously mentioned, you store the RBF data (coordinates of the control\npoints and lambda values) into a texture for access by the shader during runtime. The\nj-th row of this a texture represents the j-th frame of the animation. The RGBA chan-\nnels of the i-th texel store the (x,y) coordinates as well as the λix and λiy coefficients of\nci, respectively (see Figure 5.5.4). We used a float32 non-normalized texture, because\nthe values of λ may not be in [0,1] range.\n406\nSection 5\nGraphics \nFIGURE 5.5.4\nThe animation data is stored in single texture. Each row of\nthe texture represents a frame of the animation. Along a row, the i-th texel\nstores the (x,y) coordinates of the control point ci as well as its λix and λiy\ncoefficients.\nThe shader for evaluating the RBF-based warping function is shown next. It maps\ntexture coordinates of the current fragment (obtained after rendering a texture-mapped\nquadrilateral) into texture coordinates on the original texture. \nListing 5.5.1\nEvaluating the RBF-Based Warping Function\n// Computes the Phi function.\nfloat multiquadric(float r, float h) {\nreturn sqrt(r*r+h*h);\n}\n// Evaluates the RBF for texCoord with a pre-defined number\n// of control points, the actual time and smoothness. \nfloat2 evaluateRBF(float2 i_texCoord, float points, float keyTime,\nfloat smoothness, samplerRECT rbfTexture) {\nfloat2 newTexCoord;\nnewTexCoord.xy = float2(0.0, 0.0);\nfor (int i=0; i<points; i++) {\nfloat2 access = float2(i, keyTime);\nfloat4 rbf = texRECT(rbfTexture, access);\n",
      "content_length": 1512,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": null,
      "content": "float distance = sqrt((pow(rbf.x - i_texCoord.x,2)) \n+ (pow(rbf.y - i_texCoord.y,2)));\nfloat temp = multiquadric(distance, smoothness);\nnewTexCoord.xy += temp * rbf.zw;\n}\nreturn newTexCoord;\n}\nAnimating Relief Maps\nYou can produce RBF-based animations of relief maps by adding a couple of extra\nlines to a relief mapping pixel shader [Policarpo05]. Just before the call to the linear\nsearch, you should clamp the original texture coordinates to the [0,1] range. This is\nrequired if the entire relief map covers only part of the polygon. In this case, the tex-\nture coordinates for some fragments will be out of the [0,1] range needed for the RBF\nevaluation. This clamping does not hurt the animation because there is no depth or\nnormal information outside the region not covered by the texture. You then need to\nadd the code in Listing 5.5.2 to a relief mapping shader, immediately before calling\nthe linear search. \nListing 5.5.2\nActions Required for Relief Warping That Need to Be Executed Before\nCalling the Linear Search\n// s is the texture coordinate used in the relief mapping shader\nfloat2 sZeroOne = clamp (s.xy, 0.0, 1.0);\n// Evaluating RBFs\nfloat2 sRBFEval = evaluateRBF(sZeroOne.xy, points, keyTime,\nsmoothness, rbfTexture);\n// Compensating the clamp. \ns.xy += sRBFEval - sZeroOne;\n... // Call linear search. \nAnimating Relief Impostors\nRelief impostors [Policarpo06] are rendered using multilayer relief representations.\nFigure 5.5.5 (right) shows a dog impostor modeled as a quad-layer relief texture,\nwhose depth values are shown on the left. For the case of relief impostors, the warping\nstrategy described earlier will cause all layers to be subject to the same warping func-\ntion and, consequently, undergo the same motion. Thus, although a single warping\nfunction can be used to animate a running dog, it would not produce a convincing\ndog motion. In this case, for instance, the two front legs would always move together\ninstead of moving in opposite directions.\n5.5\nAnimating Relief Impostors Using Radial Basis Functions Textures\n407\n",
      "content_length": 2055,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": null,
      "content": "Thus, for multilayer relief representations, you might want to animate each indi-\nvidual layer independently. A walking dog motion is illustrated in Figure 5.5.1. In this\nexample, however, the animation was created using a single warping function by\nexploiting the symmetry of the walking motion of bipeds and quadrupeds—for each\nframe f at time t, the first two layers were rendered using time t, while the last two lay-\ners were rendered using time (1–t). Thus, while the right front (back) leg is moving\nforward, the left front (back) leg is moving backward. t is used in the evaluation of the\nfunction evaluateRBF as the parameter keyTime in the code fragment shown in List-\ning 5.5.2.\nIn this case, the linear and binary search calls in Listing 5.5.3 receive two new para-\nmeters: sFront and sBack. These parameters represent the warped texture coordinates\nfor the front and back layers, respectively. These coordinates are used to sample both the\ndepth and normal maps from different layers. This is illustrated in Listing 5.5.4 for the\ncase of the x-component of the normal map, where the retrieved values are combined in\na single RGBA variable (normal_x). (The x and y components of the normal map are\nstored in separate textures, normal_map_x and normal_map_y, respectively. The z compo-\nnent is computed on the fly from the other two components [Policarpo06].)\n408\nSection 5\nGraphics \nFIGURE 5.5.5\nA dog impostor modeled as a quad-layer relief texture.\nThe depth values of the progressing layers are stored in the R, G, B, and\nA channels, respectively (left). A view of the rendered dog impostor is\nshown on the right. See Color Plate 8 for a color version of this image.\nListing 5.5.3\nUsing a Single Warping Function to Produce the Walking Motion Shown\nin Figure 5.5.1\nfloat2 sZeroOne = clamp (s.xy, 0.0, 1.0);\nsFront = evaluateRBF(sZeroOne, points, keyTime, \nsmoothness, rbfTexture);\nsFront = s.xy + (sFront - sZeroOne);\nint keyTimeBack = (int)(keyTime+maxKeyTime/2) % (int)maxKeyTime;\n",
      "content_length": 1998,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": null,
      "content": "sBack  = evaluateRBF(sZeroOne.xy, points, keyTimeBack, \nsmoothness, rbfTexture);\nsBack  = s.xy + (sBack - sZeroOne);\n... // Call the linear search with sBack and sFront. \nListing 5.5.4\nSampling the Multilayer x-Component of the Normal at Two Positions,\nUsing Texture Coordinates sFront and sBack\nfloat4 normal_x;\nnormal_x.xy=tex2D(normal_map_x,sFront.xy).xy;\nnormal_x.zw=tex2D(normal_map_x,sBack.xy).zw;\nA similar operation is performed for the y-component of the normal.\nThe following code fragment uses sFront and sBack to sample the color texture. \nListing 5.5.5\nSampling the Color Texture Using Texture Coordinates sFront and sBack\nand Checking the Relative Position of the Viewing Ray with Respect to Several Layers\n// get color at intersection\nfloat4 c;\nfloat4 cFront = tex2D(texture,sFront.xy);\nfloat4 cBack  = tex2D(texture,sBack.xy);\nfloat4 z=abs(s.z-q); // q is the quad-depth value joined. \nfloat zt=z.x;\nc = cFront;           // hits the first layer.\nif (z.y<zt) c=cFront; // hits the second layer.\nif (z.z<zt) c=cBack;  // hits the third layer.\nif (z.w<zt) c=cBack;  // hits the fourth layer.\nResults\nWe have implemented the described algorithms using C++ and Cg and used them to\nanimate several textures and relief impostors. In all our experiments, the textures had\n400 \u0002 400 texels. On a 2.21GHz PC with 2.0GB of memory and an NVIDIA\nGeForce 8800 GTX with 768MB, our implementation achieves 3,000fps, 710fps,\nand 500fps, when rendering animations of textures with normal maps, relief maps,\nand relief impostors, respectively. \nFigure 5.5.6 depicts the control points (small dark dots) used to define the walk-\ning dog animation shown in Figure 5.5.1. The user defined a set of control points\npositioned on top of the dog image (left). Some of these points were then interactively\nmoved defining the configurations shown in Figure 5.5.6 (center) and (right). As the\nuser moves a control point, the underlying texture is automatically warped, providing\nimmediate visual feedback that allows the user to plan and define the animation (see\nFigure 5.5.2). \n5.5\nAnimating Relief Impostors Using Radial Basis Functions Textures\n409\n",
      "content_length": 2141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 443,
      "chapter": null,
      "content": "Figure 5.5.7 shows a few frames of a horse animation. On the left is the original\nrelief impostor. The images to its right show different poses, seen from the same view-\npoint, obtained with RBF-based warping functions. The accompanying video on the\nCD-ROM shows these animations. \n410\nSection 5\nGraphics \nFIGURE 5.5.6\nControl points (dark dots) used to define the RBF-based warping functions\nused to create the dog walking animation illustrated in Figure 5.5.1. Besides these 12 control\npoints, four extra control points were positioned at the corners of the texture to anchor it.\nFIGURE 5.5.7\nHorse animation. The image on the left shows a view of the original relief\nimpostor. The three images to its right show frames from an animation seen from the same\nviewpoint. Note the changes on the horse’s body and tail. A total of 27 control points were\nused to produce this animation, including 4 anchors at the corners of the texture.\nConclusion\nThis gem presented a technique for animating relief impostors in real-time using RBF-\nbased warping functions. This approach produces realistic animations of live and\nmoving objects undergoing repetitive motions. Given its generality, it can be used to\nanimate essentially any kind of texture representation. During a pre-processing stage,\nthe user specifies a set of control points, which are the centers for an RBF representa-\ntion. By moving such control points around in 2D, the user obtains immediate feed-\nback on the resulting animation. Once the key deformations have been specified, the\n",
      "content_length": 1541,
      "extraction_method": "Direct"
    },
    {
      "page_number": 444,
      "chapter": null,
      "content": "system interpolates the control points for intermediate frames and solves the linear\nsystem defined by Equation 5.5.3 to find a set of RBF coefficients (λs), which are saved\ninto a texture with the 2D coordinates of the control points. The stored information is\nthen read during runtime by a shader that performs the actual animation via texture\nresampling. \nOur technique can be used to define separate warping functions to individual layers\nof a relief texture. As a result, it supports the definition of complex animations using a\nsimple interface, thus reducing the amount of time and artwork usually associated with\ntexture animation. As any other technique, this approach has some limitations: large\ndeformations tend to distort the texture too much, leading to poor results. Also, the use\nof the clamping function shown in Listings 5.5.2 and 5.5.3 may introduce some arti-\nfacts when the polygon used to render the impostor is seen at a grazing angle. Under\nsuch viewing configurations, these artifacts can be avoided by calling the function \nevaluateRBF at each step of both the linear and binary searches, at the cost of some\nperformance penalty.\nThe accompanying CD-ROM contains a video and a demo (including source\ncode and shaders) for animating normal maps and multi-layer relief maps. \nAcknowledgements\nWe would like to thank NVIDIA for donating the GeForce 8800 GTX video card\nused in this work.\nReferences\n[Buhmann03] Buhmann, Martin. Radial Basis Functions, Cambridge University Press,\n2003.\n[Carr01] Carr, Jonathan et al. “Reconstruction and Representation of 3D Objects\nwith Radial Basis Functions,” Proceedings of SIGGRAPH 2001, ACM Press,\nNew York, NY, pp. 67–76.\n[Cook84] Cook, Robert L. “Shade Trees,” In Computer Graphics (SIGGRAPH 84)\n18(3), pp. 223–231, 1984.\n[Donnelly05] Donnelly, William. “Per-Pixel Displacement Mapping with Distance\nFunctions,” GPU Gems 2, 2005.\n[Hardy71] Hardy, Roland L. “Multiquadric Equations of Topography and Other\nIrregular Surfaces,” J. Geophys. Res., 1971, Vol. 73, pp. 1905–1915.\n[IdSoftware] Id Software. DOOM 3, available online at http://www.idsoftware.\ncom/games/doom/doom3/.\n[Litwinowicz94] Litwinowicz, Peter, and Williams, Lance. “Animating Images with\nDrawings,” Proceedings of the 21st Annual Conference on Computer Graphics\nand Interactive Techniques SIGGRAPH, 1994, ACM Press, New York, NY, pp.\n409–412.\n5.5\nAnimating Relief Impostors Using Radial Basis Functions Textures\n411\n",
      "content_length": 2446,
      "extraction_method": "Direct"
    },
    {
      "page_number": 445,
      "chapter": null,
      "content": "[Noh00] Noh, Jun-yong, et al. “Animated Deformations with Radial Basis Func-\ntions,” Proceedings of the ACM Symposium on Virtual Reality Software and\nTechnology, Seoul, Korea, October 22–25, 2000, VRST ’00. ACM Press, New\nYork, NY, pp. 166–174. \n[Oliveira00] Oliveira, Manuel M., Bishop, Gary, and McAllister, David. “Relief\nTexture Mapping,” Proceedings of  SIGGRAPH 2000, New Orleans, LA, July\n23–28, 2000, pp. 359–368.\n[Policarpo05] Policarpo, Fabio, Oliveira, Manuel M., and Comba, João. “Real-Time\nRelief Mapping on Arbitrary Polygonal Surfaces,” ACM SIGGRAPH, 2005,\nSymposium on Interactive 3D Graphics and Games, Washington, DC, April\n3–6, 2005, pp. 155–162.\n[Policarpo06] Policarpo, Fabio, and Oliveira, Manuel M. “Relief Mapping of \nNon-Height-Field Surface Details,” ACM SIGGRAPH 2006 Symposium on\nInteractive 3D Graphics and Games, Redwood City, CA, March 14–17, 2006,\npp. 55–62. \n[Policarpo06b] Policarpo, Fabio, and Oliveira, Manuel M. “Rendering Surface\nDetails in Games with Relief Mapping Using a Minimally Invasive Approach,” In\nWolfgang Engel (Ed.), SHADER X4: Lighting & Rendering. Charles River Media,\nInc., Hingham, Massachusetts, 2006, pp. 109–119.\n[Ruprecht95] Ruprecht, Detlef, and Müller, Heinrich. “Image Warping with Scat-\ntered Data Interpolation,” IEEE Computer Graphics and Applications, Vol. 15,\nNo. 2, 1995, pp. 37–43.\n412\nSection 5\nGraphics \n",
      "content_length": 1375,
      "extraction_method": "Direct"
    },
    {
      "page_number": 446,
      "chapter": null,
      "content": "413\n5.6\nClipmapping on SM1.1 \nand Higher\nBen Garney\nGarageGames\nC\nlipmaps are a fast and robust technique for texturing terrains. This gem provides\na brief introduction to the theory behind clipmaps, and discusses their imple-\nmentation on Shader Model 2.0 hardware. Finally, it explores some advanced topics,\nsuch as support on fixed function, SM1.x, and SM3.0+ hardware, as well as different\nsources for image data.\nBasic Concepts of Clipmaps\nWhen rendering to a 1024 \u0002 768, 32bpp display, only 786,432 pieces of color infor-\nmation are necessary at any given moment to give a fully detailed, unique view. This is\nexactly 3MB of data. If you want to run at 60Hz, you need to transfer only 180MB/sec\nto the display, which is well within the capabilities of most game platforms.\nSuppose you draw a model on-screen. Regardless of how much detail its texture\nmight contain, you cannot display more texels than the screen has pixels. For the case of\na small object, like a character or power-up, you can discard more detailed mip levels of\nthe texture (see [Forsyth07]). However, for environments where the camera spends most\nof its time looking only at a small portion of the mesh, dropping mip levels is impracti-\ncal. If any part of the model requires high detail, you need all or most of the mips, and\nwith a terrain or other environment, you’ll almost always need high detail on some part.\nWhat can be done to deal with terrain textures efficiently? Only load partial\nmipmaps! Ideally, you would only load texels onto the GPU that are needed for the\ncurrent frame—meaning that you could have an arbitrarily detailed terrain that fits in\nonly 3MB of VRAM. Unfortunately, GPU manufacturers haven’t built their hardware\nto support this kind of operation.\nClipmaps are a generalization of mipmapping that allow you to only load subsec-\ntions of each mip level. If a texel that is sampled isn’t loaded, lower-resolution data\nthat’s already loaded is used instead. This means you can upload a relatively small\ndataset, get efficient rendering, and degrade gracefully if the viewpoint manages to\noutrun your texture paging. Although clipmaps aren’t directly supported by current\ngraphics hardware, you can emulate them efficiently using shaders.\n",
      "content_length": 2241,
      "extraction_method": "Direct"
    },
    {
      "page_number": 447,
      "chapter": null,
      "content": "Implementation of Clipmaps\nBuilding on the concept of mipmapping, SGI developed clipmapping for the pur-\npose of virtualizing a single large texture [Tanner96].\n414\nSection 5\nGraphics \nFIGURE 5.6.1\nImage of a clipped mipmap stack.\nRecall that a mipmap is used to reduce aliasing and localize memory accesses.\nConceptually, when a given pixel of a triangle is rendered, the pixel’s bounds are pro-\njected into texture space, and based on its size, a mipmap is selected and from it a\npoint is sampled. The result of this is that as a triangle becomes more distant it selects\nfrom less detailed mip levels and the memory accesses are less scattered than they\nwould otherwise be.\nClipmaps take the same basic idea, but the more detailed levels of the mip pyra-\nmid are clipped to limit memory usage. This means that a 32KB px texture, which\nwould normally have 15 mip levels and consume half a gig of memory, if put into a\nclipmap with a maximum level size of 512px, would only use six 512px textures’\nworth of memory, plus a “cap” 512px texture with a full mip chain. The memory\nfootprint for this is only 7.3MB.\nIn SGI’s InfiniteReality2 hardware platform, the hardware, when accessing mip\nlevels, checked to see if the cached clipmap region in memory covered the area of the\nmip level it wanted to read from. If so, it would sample as normal. If not, it would\nbump up to the next less detailed mip level and try again, with the result that if\ndetailed data was not available for an area, less detailed data would be used instead.\n",
      "content_length": 1529,
      "extraction_method": "Direct"
    },
    {
      "page_number": 448,
      "chapter": null,
      "content": "On the CPU, SGI’s Performer scenegraph was responsible for adjusting the data\nin each layer of the clipmap by purging old data and uploading new data from a data-\nbase on disk.\nAdvantages of Clipmaps\nClipmaps bring several major benefits compared to the other strategies discussed, as\nfollows:\n• They always have smooth transitions between LOD levels, and no specific LOD\nlevel is required to render geometry—worst case, things will just be a bit blurry.\n• They have a fixed memory cost; no dynamic allocation of GPU resources is\nneeded either during rendering or updating. This is important as most GPU dri-\nvers don’t deal well with frequent allocations and deallocations.\n• They are view independent provided you place the focal point for detail at the\ncamera position; data is available with a smooth fall-off in all directions, meaning\nthat spinning in place has no effect on performance.\n• Clipmaps are straightforward to implement on any hardware with programmable\nshaders. Even on fixed-function hardware, it’s possible to emulate them. Imple-\nmenting update region determination is a bit tricky, but the system as a whole is\nstraightforward to work with, with no complex caching logic.\n5.6\nClipmapping on SM1.1 and Higher\n415\nFIGURE 5.6.2\nUpdating clipmaps affect which areas are\ncontained in each detail level.\n",
      "content_length": 1321,
      "extraction_method": "Direct"
    },
    {
      "page_number": 449,
      "chapter": null,
      "content": "• They have well-defined relationships between image quality and resources allocated\nto the clipmap, so it’s easy to tune the visual experience based on user preference.\n• They can receive data from many sources. Unique data from files, CPU synthesis\nof data, or GPU synthesis are all supported.\n• Finally, they have excellent update characteristics, because the quantum of update\nis variable. The minimal amount of update required is usually quite small and\nbounded—just the new texels that need to be uploaded for a single clipmap level,\nwhich is often only a few thousand. The worst case is a full upload of the whole\nclipmap, which is only a dozen megabytes or so.\nDrawbacks of Clipmaps\nThe major drawback of clipmapping is that it cannot deal with varying detail levels.\nDetail simply falls off linearly in texture space from the focal point. This makes them\nunsuitable for dealing with a complex interior environment, where multiple regions\nin texture space may need to be high detail (for instance, the floor and walls may have\ndifferent UV regions that they use). If you can require mid-range SM2 or higher,\nthere are some good options to check out, like [Lefebvre04].\nThe full un-optimized shader for clipmapping is also expensive and requires at\nleast SM2. However, with some geometry conditioning, this can be optimized signif-\nicantly, as you’ll read later on. It might also be possible to use the gradient operators in\nSM3 and higher to write a more efficient clipmapping shader.\nDetails of Clipmaps\nThe following sections explain and describe the details related to clipmaps, including\nclipstack size, the focus point, and methods for updating clipmaps.\nClipstack Size\nThe size of the textures in the clipmap stack is the main variable when working with\nclipmaps, and it can be controlled quite simply—it should be the power of 2 nearest\nto the display resolution. For higher-quality results, bias up, and for lower quality, bias\ndown. The reason for this goes back to the original discussion of the amount of texel\ndata needed for an optimal renderer; the most demanding situation possible, texture-\nwise, is for the view to be looking straight on at a clipmapped surface, zoomed in as\nmuch as possible without magnification of the original texture. In this case, a texture\nequal in size to the screen would be needed to give the illusion of full detail.\nThe Focus Point\nSelecting the focus point, which is the location in UV space of the clipmap where detail\nshould be highest, is another open question when working with clipmaps. There are\nmany possible heuristics, but the one that gives the most consistent results is to simply\nproject down from the camera position onto the plane of the clipmapped geometry\nand use those coordinates as the new focal point.\n416\nSection 5\nGraphics \n",
      "content_length": 2801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 450,
      "chapter": null,
      "content": "Methods for Updating\nThere are three broad paths for getting data into the style of clipmap discussed here.\nThey all do roughly the same thing: updated regions are identified by the clipmap\ncode and data is supplied to fill them.\nFirst, you can blast data into them from files on the disk. In this case, once the\ndata is in system memory, you only need to directly upload it to the GPU. Second,\nyou can synthesize data on the CPU and upload it. I found this to be inferior to the\nnext method, but if you have an existing fast synthesis routine, it might be useful.\nFinally, you can perform synthesis on GPU by doing render to texture operations.\nThis requires allocating the clipstack textures as render targets to begin with, but oth-\nerwise operation is identical to the other modes.\nImplementing Clipmaps\nThe following sections cover the details related to implementing your clipmaps.\nThe SM2.0 Path\nFor simplicity’s sake, this gem discusses only the SM2.0 clipmap path in-depth. Once\nyou have the 2.0 path done, the majority of work is done, so getting the SM1.x and\nSM3.0+ paths going is straightforward.\nThis is the core pixel shader code that drives the clipmap effect in the demo app\non the CD-ROM:\nPS_OUTPUT Output;\n// The base level can always be sampled as there’s nothing behind\n// it... so save some math.\nfloat3 colAccum = tex2D(clipSamplers[0], In.TextureUV[0]);\n// Grab the rest, fading based on distance from each layer’s center.\nfor(int i=1; i<CLIP_LAYER_COUNT; i++)\n{\nfloat fade = smoothstep(0.4, 0.5, distance(In.TextureUV[i],\ng_clipLayerAndCenter[i].xy));\nfloat4 curColor = tex2D(clipSamplers[i], In.TextureUV[i]);\ncolAccum = lerp(curColor, colAccum, fade);\n}\n// Store accumulated result and return.\nOutput.RGBColor = float4(colAccum,1); \nIn the SM2.0 path, you do all the clipmap level selection calculations per-pixel.\nAt each pixel, you must determine the UV coordinate and, using information passed\nvia uniform shader constants, produce a texture coordinate for each clipstack entry by\nscaling and offsetting the original UVs. You also generate a “fade” value for each clip-\nstack entry based on distance in texture space from its focal point. Then, using the\n5.6\nClipmapping on SM1.1 and Higher\n417\n",
      "content_length": 2225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 451,
      "chapter": null,
      "content": "fade values as coefficients, you lerp the colors from each clipstack entry in order from\nleast to most detailed.\nToroidal Updates and the Rectangle Clipper\nA major optimization in the clipmap scheme is to treat the clipstack textures as\ntoroidal buffers. This means that shifting the stack is done as efficiently as possible—\nyou only do work to upload new data. However, determining the regions that need to\nbe updated is a little tricky.\nConsider a level of the clipstack. At any given moment, there’s a rectangle of data\nthat’s contained in the clipstack’s texture. Let’s call this rectangle currentRect. It’s the\ncurrently loaded subset of the full set of data available at some miplevel of the virtual-\nized texture. As you move the focus point, this rectangle shifts around to center on it.\n418\nSection 5\nGraphics \nFIGURE 5.6.3\nThe size of the clipstack texture is only 1/16th the size of the full source level.\nSuppose you’re on the third level of the clipmap from the top. This means that\nthe size of the clipstack texture is only 1/16th the size of the full source level. The sit-\nuation is illustrated in Figure 5.6.3. The grid shows how the texture is mapped to the\ngeometry. You scale unit UV coordinates by four, so the texture is repeated four times\nin each direction. However, the currentRect isn’t aligned to this grid; it’s somewhere in\nthe middle. By uploading the texture data in the pattern shown to the right, you end\nup with every piece of data where you want it on the geometry.\nYou then clip the currentRect against this grid, which is spaced equal to the size of\nthe clipstack textures. You’ll always end up with different pieces (in the common case,\nfour, but if you’re aligned to the grid in various ways, it can be less). This gives you the\nbasic idea of what’s going on and how uploaded data has to map into the texture to be\ndisplayed properly. ClipMap::fillWithTextureData implements this to refill the clip-\nstack entirely.\n",
      "content_length": 1956,
      "extraction_method": "Direct"
    },
    {
      "page_number": 452,
      "chapter": null,
      "content": "What about updating? When you move the rectangle, you tend to get something\nthat looks like Figure 5.6.4.\n5.6\nClipmapping on SM1.1 and Higher\n419\nFIGURE 5.6.4\nThe inverted L-\nshaped region is what needs to be\nuploaded during an update.\nThe inverted L-shaped region is what you need to upload. So when you’re updat-\ning what currentRect contains, you determine what this update region is, and then clip\nand wrap it just as you did above with the currentRect itself, ending up with rectangu-\nlar regions in the texture where data is to be uploaded. This allows very efficient\nclipmap updates—moving the focus point one pixel means a rectangle only one pixel\nwide would be uploaded. See ClipMap::recent for the implementation of this.\nBasic CPU Synthesis\nA helpful aid in debugging is a simple checkerboard synthesizer. A 1px checkerboard\nmakes it easy to spot any sampling issues or other problems. Applying a gradient\nmakes it simple to spot any incorrect updates—the colors won’t match.\nLook in ClipMap::uploadToTexture for an example of this. The #if block can be tog-\ngled to 1 to enable a simple CPU synthesis that chooses a random color for each clipmap\nupload. This is useful for seeing how updates happen and what regions they cover.\nBasic CPU Upload\nBy default, the example app loads data into the clipmap from a large image stored in\nsystem memory. (This allows you to avoid the complexity of a paged loader.) This is a\nstraightforward bitblt operation.\nAdvanced Clipmapping\nThe following sections cover some advanced issues related to clipmaps, including\nadding background paging, budgeting updates for better performance, optimizing fill-\nrate, and more.\n",
      "content_length": 1665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 453,
      "chapter": null,
      "content": "Background Paging\nConceptually this is simple (although implementing a performant pager takes work!).\nBreak your source image and its mip levels into tiles. Maintain a cache in system\nRAM of all the tiles that the clipstack levels overlap, plus a border of data so that\nadjustments to the focal point can be fulfilled rapidly.\nMake sure you have a fast copy from your tiles into the clipstack textures. If data\nisn’t available to update a clipstack level, make sure the data has been requested, and\ndefer the update until a later time. I find that working from the bottom up gives good\nresults—high detail follows the user around, whereas mid-level data sometimes takes\na while to appear.\nBudgeting Updates\nThis is one of the major victories of clipmaps as opposed to other techniques. Most\nsurface caching approaches require a fixed quantum of work to be done—say, synthe-\nsizing a 128 \u0002 128px tile. As texel density increases the quantum does, too, until\nyou’re looking at a minimum of doing a 512px or 1024px tile! No good.\n420\nSection 5\nGraphics \nFIGURE 5.6.5\nAn image from the clipmapping app provided on the CD-ROM. See Color\nPlate 9 for a color version of this image.\n",
      "content_length": 1175,
      "extraction_method": "Direct"
    },
    {
      "page_number": 454,
      "chapter": null,
      "content": "Instead, clipmaps generally require frequent small updates as the focal point moves.\nThe typical update is just a few slices along the horizontal or vertical edges of a clipstack\nlayer—for a 512px clipmap, this might be only a thousand pixels to upload.\nThus, you can set a texel upload budget and, after each level is updated, check to\nsee if you’ve overrun it. If so, just stop updating, and let the next frame’s update take\ncare of it. This is also helpful in cases when the camera is moving—you don’t waste\nmuch time on detail that will only be visible for a frame. It’s also possible to budget\nbased on available time until the next present, using the same methodology. \nAs long as you always update at least one level each time through before aborting,\nall the required data will eventually make it into the clipmap, and you may avoid lots\nof work that would be seen for only a frame or two.\nOptimizing Fillrate/Low-End Support\nBy conditioning your geometry into chunks with known texture coordinate bounds,\nit’s possible to determine efficiently at runtime what clipstack levels are needed to tex-\nture that chunk, thus allowing you to reduce the number of textures that have to be\nbound to the clipmap shader.\nThis also begins to enable SM1.0 support, because you can get down to four or\nfewer active textures, under the four texture sampler limit of SM1.0. By then, moving\nthe level fade calculations into the vertex shader (and ensuring a certain minimal ver-\ntex density!), you can fit the clipmap logic into an SM1.x pixel shader.\nUsing an SM1.x-compatible path is a good idea even on higher end cards because\nit’s much faster than the naive SM2.0 shader. Especially on cards that report high\ncapabilities but can’t do them quickly, like the X300, this can be a huge win.\nBy extending this idea, it’s also possible to target FF cards. You can either bind the\nsmallest clipstack level that contains the chunk’s texcoords, or you can hack up transi-\ntions between two or three levels using register combiners and approximating the\nshaders.\nTaking Advantage of the High End\nIn shader models where the pixel gradient operators are available, you can do the\nmipmap calculations yourself, and look up the exact levels of the clipmap that are\nneeded for the pixel in question. This cuts the fillrate significantly, although the shader\nmay then be costly to evaluate.\nIn higher-end contexts, it’s also feasible to consider maintaining several clipmaps\nfor different attributes. For instance, one for normal maps, another for diffuse, a third\nfor specularity. For “localized” attributes, which tend to average to nothing in the dis-\ntance, like normal maps, it might also be profitable to maintain just the two or three\nmost detailed levels of the clipmap.\nIf You Want To Save Some Time...\nIf you want to just grab an existing implementation off the shelf, Torque Game\nEngine Advanced, which my employer, GarageGames, sells, contains the Atlas terrain\n5.6\nClipmapping on SM1.1 and Higher\n421\n",
      "content_length": 2995,
      "extraction_method": "Direct"
    },
    {
      "page_number": 455,
      "chapter": null,
      "content": "system. TGEA comes with full source and liberal licensing terms, and Atlas has a fully\npaged and optimized clipmapping implementation with support for SM1 and higher.\nTorqueX’s 3D terrain system also includes a comparable implementation in C# on\nXNA. Check them out; they might save you a lot of time and money.\nL3DT, 3d Studio Max, and the Panda DirectX exporter were used to create the\nassets for the demo included on the CD-ROM.\nReferences\n[Forsyth07] Forsyth, Tom. “TomF’s Tech Blog—Knowing Which Mipmap Levels Are\nNeeded,” available online at http://home.comcast.net/~tom_forsyth/blog.wiki.\nhtml, 2007.\n[Lefebvre04] Lefebvre, Sylvain, Darbon, Jerome, and Neyret, Fabrice. “Unified Tex-\nture Management for Arbitrary Meshes,” 2004.\n[Tanner96] Tanner, Migdal, Jones. “The Clipmap: A Virtual Mipmap,” available\nonline at www.cs.virginia.edu/~gfx/Courses/2002/BigData/papers/Texturing/\nClipmap.pdf, 1996.\n422\nSection 5\nGraphics \n",
      "content_length": 930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 456,
      "chapter": null,
      "content": "423\n5.7\nAn Advanced Decal System\nJoris Mans\njoris.mans@10tacle.be\nDmitry Andreev\ndmitry.andreev@10tacle.be\nM\nost games these days use decals in one way or another; for instance, to show bul-\nlet marks on the environment, or to add variation on repetitive geometry. Usu-\nally this is done by rendering a transparent polygon on top of the existing geometry.\nThis technique, however, has some drawbacks, especially if you want to apply bump\nmapping in your decals. When rendering a bump mapped decal on top of existing\nbump mapped geometry, the lighting is not correct, because the pixels underneath \nthe decal should also have been lit using a combination of the decal bump map and the\ngeometry bump map. This gem explains how to render decals that actually replace the\nbump and the diffuse map of the geometry (this can be extended to any kind of tex-\nture map you use), thereby giving correct lighting results and a higher image quality.\nRequirements\nAn implementation of this gem can be done on any platform supporting render tar-\ngets and shader logic capable of sampling and interpolating between values obtained\nfrom at least two different maps (if you only want to use it for diffuse textures), or\nfour (if you want to add bump map support). The best image quality is obtained by\nusing render targets that are the same resolution as the screen, but smaller ones will\nalso work, with a decrease in image quality. The demo provided runs on any PC with\na DirectX 9 compatible graphics card supporting pixel shader version 2.0.\n",
      "content_length": 1529,
      "extraction_method": "Direct"
    },
    {
      "page_number": 457,
      "chapter": null,
      "content": "Normal Decals Method\nIn a traditional engine using a decal system, you first render all the geometry in the\nframe buffer, and on top of that you render polygons containing the decals, usually\nusing some kind of blending. \nAdvanced Decals Method\nIn this example, you’ll do things slightly differently. First, you need to create the neces-\nsary tools in the runtime to accomplish the decal renderer. In this case, we will create\ntwo full-screen render targets. The first one is in 32-bit RGBA format; it’s called the\nDiffuseRenderTarget. The second one will also be in 32-bit RGBA format; it’s called\nBumpRenderTarget. For this second one, you could use a 16-bit per component buffer\nor any other format of render target if your bump maps are stored in higher precision.\nIn our demo, we use DXT5 compressed bump maps so 32-bit RGBA will suffice.\nRendering the scene can be split up into two parts. First, you generate the decal\nbuffers, and next you render the scene with the decals applied. To generate the decal\nbuffers, execute the following steps:\n• Render all depth values of the geometry in the main z buffer (excluding the decals).\nThe depth compare function used is the same as the one you use to do the normal\nscene rendering.\n• Select the DiffuseRenderTarget as the current render target, while still using the\nmain z buffer. The render target is cleared, using black as the clear color. \n• Render all decal geometry into that render target, using the same depth compare\nfunction used previously for the depth pass, but don’t render the complete shader\nas you would in the normal decal case. The rendering uses a special shader that\njust outputs the color of the diffuse texture, pre-multiplied with the opacity tex-\nture (or diffuse alpha depending on your art pipeline). In the alpha component of\nthe render target, output the opacity value used to scale the diffuse value. \n• In the BumpRenderTarget, you do something similar. Render the decal geometry,\nthis time using the bump map texture value in world space as output. This step\ncan be combined with the previous one if your target hardware supports multiple\nrender target rendering. Sample results are shown in Figures 5.7.1 and 5.7.2.\nFinally, you render the scene. The thing you have to do in the shaders used for the\ngeometry is change the code that makes the diffuse texture lookup and the bump tex-\nture lookup. You must actually combine the diffuse value from the texture applied to\nthe geometry and the values found in the DiffuseRenderTarget/BumpRenderTarget\ntextures. This works as follows:\n• Take the screen space position of the pixel you are currently rendering. This will\nbe used as texture coordinates to read out the values of the render targets.\n• Use the texture coordinates to read the diffuse RGB value from the decal diffuse\nmap; call this value drt.\n424\nSection 5\nGraphics \n",
      "content_length": 2861,
      "extraction_method": "Direct"
    },
    {
      "page_number": 458,
      "chapter": null,
      "content": "5.7\nAn Advanced Decal System\n425\nFIGURE 5.7.1\nThe DiffuseRenderTarget used by the decal system.\nFIGURE 5.7.2\nThe BumpRenderTarget used by the decal system.\n",
      "content_length": 156,
      "extraction_method": "Direct"
    },
    {
      "page_number": 459,
      "chapter": null,
      "content": "• Combine it with the RGB value read from the diffuse texture used on the geom-\netry, using the alpha value read from the render target. This gives you the follow-\ning formula, where dt is the diffuse texture of the object, drt is the render target\ntexture containing the decal diffuse value, and d is the resulting diffuse value:\ndrgb = dtrgb *(1 – drta) + drtrgb\n(5.7.1)\n• Read the value stored in the decal bump map; store it in the variable brt.\n• Combine it with the bump map of the object according to the following formula.\nHere bt is the bump texture of the object, wsb is the bump vector in world space,\ndrt is the render target texture containing the decal diffuse value, brt is the render\ntarget texture containing the decal bump value in world space, and b is the result-\ning bump value:\n(5.7.2)\nDecodeBump is the function that converts your RGBA texel into a bump vector,\ndepending on the way you store your bump maps. Of course, interpolating bump\nvectors like this isn’t really mathematically correct, but the visual results in this case\nare fine, so you need not look for a more advanced solution.\nFigures 5.7.3 and 5.7.4 show a comparison of the traditional method and the\ntechnique explained in this gem.\nwsb\nTransformToWorldSpace DecodeBump btrgba\n=\n(\n)\n(\n)\n=\n−\n(\n)+\n(\n)\nb\nwsb\ndrt\nDecodeBump brt\ndr\nxyz\na\nrgba\n*\n*\n1\nt\nb\nnormalize b\na\n=\n( )\n426\nSection 5\nGraphics \nFIGURE 5.7.3\nDecals using the traditional technique.\n",
      "content_length": 1436,
      "extraction_method": "Direct"
    },
    {
      "page_number": 460,
      "chapter": null,
      "content": "DecodeBump\nAs mentioned in the previous paragraph, this example uses a function called Decode-\nBump to decode the bump maps. There are several ways of storing bump maps. The\nchoice of which one to use depends on hardware support, quality, and speed. Explain-\ning in detail the different approaches is beyond the scope of this gem, but it does\ninclude some examples of how this can be done. The easiest solution is to use an RGB\nrender target with 8bits per component and store the bump maps as color values,\nscaled and biased to fit in the 0..255 range of the pixel color.\nEncoding a bump vector into this format would look like this:\ncolor.rgb = (bumpvector xyz + 1)*127.5\n(5.7.3)\nThe corresponding DecodeBump function would be something similar to this:\nbumpvector.xyz = color .rgb* 2 – 1\n(5.7.4)\nRecall, although you wrote byte values in the range of 0..255 when encoding, in\nthe pixel shader, all values are normalized floats, where 0 maps to 0 and 255 maps to\n1.0. The disadvantage of this way of storing is that the bump maps are uncompressed,\nand that compression using DXT1 gives rather bad visual results.\nAnother approach sometimes used is to store the bump values in a DXT5 com-\npressed surface, using the green component to store the x value of the bump vector,\nand the alpha component to store the y value. When reading the bump map, you can\nreconstruct the z value using x and y.\n5.7\nAn Advanced Decal System\n427\nFIGURE 5.7.4\nDecals using this technique.\n",
      "content_length": 1469,
      "extraction_method": "Direct"
    },
    {
      "page_number": 461,
      "chapter": null,
      "content": "Encoding the bump vector would look like this:\n(5.7.5)\nThe DecodeBump function reconstructs the third component:\n(5.7.6)\nThis allows you to store bump maps in a compressed format, while still maintain-\ning a good level of quality by exploiting the fact that the green component in a DXT5\ncompressed texture contains six bits of precision, and that the alpha component is\ncompressed separately. The tradeoff is, of course, that there are more calculations\nneeded to recreate the bump vector—calculations that are not available on all plat-\nforms or that might be too expensive. There is a tradeoff between storage require-\nments and pixel shading speed, but on some platforms the fact that the data is\ncompressed gives you fewer cache misses and actually is faster, even with the calcula-\ntion of the z component, than reading decompressed values directly.\nAdvantages of This Advanced Decal System\nThe main advantage of this system is the increased quality of the image, compared to\nnormal bump maps. It also allows decals to be used in different ways. For instance,\nimagine using decals that only contain bump maps to influence the look of a repetitive\nwall by adding cracks, noise, or other variations. Instead of only using decals in the\nruntime to add bullet and explosion marks, you can also use them in the level editor\nwhen building the scene.\nCreating the same diversity in an engine supporting standard decals would require\nan entirely different approach. Because you cannot replace bump maps with normal\ndecals, you would have to actually create bump maps for each part of the scene where\nyou want variations, replacing the original bump map used with the variant one. Not\nonly does this mean that you will have a lot more bump maps in memory, but it also\nrequires more work from artists to create and place those bump maps on the geometry.\nAs shown in Figure 5.7.5, playing with the opacity of the decals can simulate wear\nand tear on geometry over time. Because you have coherent lighting here, you can per-\nfectly blend in an erosion bump map by playing with the opacity value of the decal.\nIt can also be used to create variations in a scene that uses a lot of instancing. You\ncan instance the same geometry all over, using hardware instancing support if that is\nbumpvector x\ncolor b\nbumpvector y\ncolor\n.\n. *\n.\n.\n=\n+\n=\n2 1\na\nbumpvector z\nbumpvector x\nbumpvect\n*\n.\n.\n2 1\n2\n+\n=\n(\n) +\nor y.\n(\n)\n2\ncolor r\ncolor g\nbumpvector x\ncolo\n.\n.\n.\n*\n.\n=\n=\n+\n(\n)\n0\n1\n127 5\nr b\ncolor a\nbumpvector y\n.\n.\n.\n*\n.\n=\n=\n+\n(\n)\n0\n1\n127 5\n428\nSection 5\nGraphics \n",
      "content_length": 2550,
      "extraction_method": "Direct"
    },
    {
      "page_number": 462,
      "chapter": null,
      "content": "available, but thanks to the decal system, you can add variations on the surface of each\ninstance, without requiring the memory footprint of having each instance separately in\nmemory. Because you use the depth buffer in the decal system, it supports non-planar\ndecals. The only constraint is that the decals have to follow the geometry underneath\nthem as closely as possible. Apart from that, the topology of the decal has no restrictions\nwhatsoever. An example of decals on non-planar geometry is shown in Figure 5.7.6.\n5.7\nAn Advanced Decal System\n429\nFIGURE 5.7.5\nUses of decals for erosion over time. From left to right and top to bottom,\nthe opacity values are 0, 20, 40, 60, 80, and 100 percent, respectively.\n",
      "content_length": 716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 463,
      "chapter": null,
      "content": "It is also quite easy to implement, and integrating it in existing technology does\nusually not require major changes in the rendering pipeline. If the z pre-pass is already\navailable, you can add the render to the two decal render targets right after that pre-\npass, and you just need to change the shader code that samples the diffuse and bump\nmap textures when rendering the scene in order to read from the decal buffers. (See\nthe color insert for color versions of many of the images shown in this gem.)\nPerformance and Experimental Results\nThis section shows the results of our implementation of the technique described here,\nalong with some performance tests and potential issues. The demo on the CD-ROM\nis just another simplified implementation of advanced decals we are using in our\ngaming engine. It shows the main parts of the technique and yields clear performance\ntendencies. All tests were performed on a 3.0GHz P4 with NVIDIA's GeForce 6800\nGT and GeForce 7800 GT.\nWe used four rendering presets of the demo to show different aspects of performance:\n• Original—A standard lighting model that already exists in almost all modern\nengines, without using decals. In this case, it is based on two per-pixel computed\nlight sources using tangent-space normal maps, diffuse maps, and specular maps. \n• Normal decals—Regular decals rendered on top of the geometry rendered using\nthe original shaders. A decal consists of a diffuse texture and a bump texture.\n• Advanced (Original)—Renders the decals into the decal buffers, but uses the shader\nof the original to render the objects on the scene, so no decals are shown. This\nallows you to see the cost of filling the two decal buffers.\n• Advanced—Renders the decals into the decal buffers and applies them to the\nobjects in the scene.\n430\nSection 5\nGraphics \nFIGURE 5.7.6\nDecals on non-planar geometry.\n",
      "content_length": 1857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 464,
      "chapter": null,
      "content": "The primary question is what is the performance difference between normal and\nadvanced decals. Figures 5.7.7 and 5.7.8 show performance in frames per second.\nThere are two 512 \u0002 512 \u0002 32 (diffuse and specular) and one 1024 \u0002 1024 \u0002 32\n(normal map) textures assigned to each object. There are also the same amount of\ntextures of the same sizes assigned to decals.\n5.7\nAn Advanced Decal System\n431\nFor these two tests, we used non-compressed textures with 8x anisotropic filtering\nof normal maps. In the “full scene test” in Figure 5.7.7, the camera was pointed such\nthat almost all decals were visible covering both close and distant scene objects. Whereas\nthe close up test shown in Figure 5.7.8 was completed with the camera pointed only at\none single object covering the full screen, so that all others would be Z culled.\nFIGURE 5.7.7\nFull scene test.\nFIGURE 5.7.8\nClose up test.\n",
      "content_length": 882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 465,
      "chapter": null,
      "content": "No matter what resolution we render in, or whether rendering an entire scene or\nclose up, the “Advanced” technique is about 11% slower than the “Advanced (Origi-\nnal)” technique. The same tests have been done but with compressed textures and\nthey gave us a 23% difference between “Advanced” and “Advanced (Original),” and\nhigher frame rates. So those few additional texture fetches and blending instructions\nin the main shader cost us about 11–23% speed.\nBut how does that difference depend on the complexity of decals? To answer that\nquestion we’ve made two tests showing that dependency. We rendered one full-screen\ndecal multiple times on our scene to see how this would influence performance. As\nthe cost of the lookup in the decal buffers is independent of the number of decals\nused, the test we did previously already showed the performance implications of those\nlookups. The second test shows the cost of actually rendering the decals themselves.\n432\nSection 5\nGraphics \nAs you can see in Figures 5.7.9 and 5.7.10, when only using one decal the stan-\ndard decal technique is faster, but when drawing multiple decals on top of each other,\nthe advanced technique actually renders faster. This is due to the fact that when ren-\ndering multiple standard decals on top of each other, the complex shader, which does\nthe lighting and bump mapping, is executed once for each decal being rendered, while\nin the advanced decal’s case, the complex shader is only executed once, even when\nmultiple decals overlap.\nIf your rendering pipeline is memory or API-call bound, all decal buffers (tex-\ntures) could be filled at once using multiple render targets. In this demo it only shows\nthat it doesn’t cause any additional performance issues. But using it will just minimize\ntexture state changes and API calls, simply because in that case you need to render\ndecals only one time.\nFIGURE 5.7.9\nNon-compressed textures.\n",
      "content_length": 1911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 466,
      "chapter": null,
      "content": "Although we can conclude there is a performance cost of about 12% in our test\nscene, we should not forget that these are test cases and that a rendering engine does\nmuch more rendering than just rendering objects with decals. When taking into\naccount the cost of the other things going on during the rendering (for example, full-\nscreen effects, particle systems, shadowmapping, and so on), the total performance hit\npercentage will be smaller. Something else to consider is that by using these decals,\nyou can build scenes with a smaller amount of different textures, resulting in available\nmemory gains, fewer state changes, and bigger batches, which might actually increase\nrendering performance. So the cost of using those decals ends up being less than 12%,\nwhile gaining available memory. You can even see performance benefits when there\nare lots of overlapping decals.\nDemo\nOn the CD-ROM, you can find a demo of this decal system. There are several param-\neters exposed so you can see and test the differences between this system and tradi-\ntional decals, and tweak certain rendering settings. You can also see how the diffuse\nand bump maps are combined with the decal maps, to get a better understanding of\nthe algorithm. There are some more screenshots found in the “screenshots” subfolder.\nThe demo requires a DirectX 9 compatible video card supporting shader model 2.0.\nConclusion\nThis gem covered a way of rendering decals that has several advantages over the tradi-\ntional approach. It results in better image quality, consistent lighting of the parts\ncovered with decals, and it allows uses of decals that were previously not possible. For\ninstance, decals can be applied that only contain a bump map and no diffuse texture.\nThe method presented here can easily be integrated in existing technology, without\n5.7\nAn Advanced Decal System\n433\nFIGURE 5.7.10\nDXT 1/5 compressed textures.\n",
      "content_length": 1898,
      "extraction_method": "Direct"
    },
    {
      "page_number": 467,
      "chapter": null,
      "content": "requiring massive changes to the production or rendering pipeline, and the perfor-\nmance cost is relatively small. Moreover, when pushing this further, you can use decals\nto replace any texture used on the scene geometry. Another possibility is to add decals\non the scene when building the geometry in the editor, thereby allowing for many\nvariations on top of generic, tiled textures without having to resort to using detail\ntextures.\nReferences\n[Jing06] Jing, YingHui, et. al. “A Post-Processing Decal Texture Mapping Algorithm\non Graphics Hardware,” Proceedings of the 2006 ACM International Confer-\nence on Virtual Reality Continuum and Its Applications, pp. 99–104.\n[Lengyel01] Lengyel, Eric. “Applying Decals to Arbitrary Surfaces,” Game Program-\nming Gems 2, 2001, Charles River Media, pp. 411–415.\n434\nSection 5\nGraphics \n",
      "content_length": 830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 468,
      "chapter": null,
      "content": "435\n5.8\nMapping Large Textures for\nOutdoor Terrain Rendering\nAntonio Seoane, Javier Taibo, \nLuis Hernández, and Alberto Jaspe\nVideaLAB, University of La Coruña\n(antonio.seoane@videalab.udc.es), (jtaibo@udc.es),\n(lhernandez@udc.es ), and (jaspe@videalab.udc.es)\nT\nexturing highly detailed large terrain areas is a requirement in many games, espe-\ncially flight simulators. Fortunately, hardware supports large textures, up to 8192\ntexels square. Classical techniques are based on the tiling of large textures or blending\ndetail textures. The problem with these techniques is that, in one case, geometry must\nbe divided into segments with borders exactly matching the texture tile boundaries\nand, in the other case, the appearance is repetitive, unnatural, and unrealistic. This\ngem explains a method that allows the use of huge textures, based on clipmaps. The\ntechnique can be used with any geometry algorithm without the need to divide tex-\ntures into tiles adapted to the geometry boundaries. Moreover, it allows dynamic\ngeometry deformation.\nIntroduction\nIn the case of online games huge textures can be stored on game servers, so they can\nbe downloaded in real time. This allows textures that would exceed the storage capac-\nity of the user’s computer and also enables easy updates on the game server to add\nmore detail, new features, and so on. Moreover, allowing huge textures is more nat-\nural and easy for game artists, who can use a large canvas to paint with as much detail\nas possible and also eliminate artifacts due to repeating tiled textures.\nIn order to successfully deal with textures that are larger than system and video\nmemory, some specific techniques are needed. On the Virtual Terrain Project Website,\nthere is a large compilation of papers about the problem of mapping large textures over\nterrain [VTerrain07]. One drawback in the vast majority of existing solutions is the\nstrong coupling between texture and geometry databases. This requires subdivision of\nthe texture in order to adapt it to the geometry or vice versa.\n",
      "content_length": 2047,
      "extraction_method": "Direct"
    },
    {
      "page_number": 469,
      "chapter": null,
      "content": "Clipmapping is one of the best approaches to manage large textures that cannot fit\ninto system memory [Tanner98]. This technique decouples the handling of texture\nand geometry, allowing independence between both databases. The first implementa-\ntion of this technique was made in Silicon Graphics systems and required expensive,\nspecific hardware [Montrym97].\nThe main idea of clipmapping is to handle a large size mipmap pyramid (where\nlarge means larger than the texture size limit and/or the available video memory),\nkeeping only a subset of the pyramid in video memory. The portion of each level that\nis kept resident is limited by a user-specified parameter called the clipsize. The levels\nwith size lower than or equal to the clipsize are always in video memory, and the larger\nlevels are clipped to this limit. The area of incomplete levels that is resident is centered\naround a point called the center of detail or the clipcenter. As the camera moves, the\nclipcenter is dynamically updated and the region cached for each level in video mem-\nory is consequently updated. This way, there is always the best possible quality avail-\nable to map the geometry into the region being visualized. They can be large areas\nwith low resolution or small areas with very fine detail.\nThe main advantage of clipmapping is that a huge texture can be handled using a\nlimited portion of memory. For instance, a 65536 \u0002 65536 texel cliptexture (21.3GB\nusing 32 bits depth in a storage device) using a 1024 clipsize requires only 29.34MB\nof video memory. The system can be adjusted to use any clipsize depending on the\namount of video memory that is allocated to it.\nNext, this gem describes a technique that allows the handling of a large amount\nof texture using current PC and console hardware. The technique stores the image in\ntiles that are not used directly as textures. These tiles are combined in a texture stack\nthat caches the region of interest, following the clipmap idea. Although inspired by\nclipmapping, there are important differences in its structure, video memory manage-\nment, and the way the texture is applied. This allows implementation on any graphics\ncard without special hardware requirements—only OpenGL or Direct3D fixed func-\ntion pipeline is required to implement this technique.\nThe technique described in this gem has been successfully used in several projects\nusing texture details of 0.25 m/texel in geographical areas of about 60,000km2[Santi07].\nIt has also been successfully used with different geometry algorithms, based on grids\nas well as TINs. \nThe main advantages of this technique are as follows:\n• It can be implemented using a fixed-function pipeline API such as OpenGL or\nDirect3D.\n• It maintains independence between geometry and texture databases.\n• Texture coordinates can be automatically computed in the GPU, avoiding their\ntransference to the graphics system. This allows modification of the geometry in\nreal-time, while keeping the right texture mapping.\n• Texture aliasing is avoided using trilinear and anisotropic filtering hardware\ncapabilities.\n436\nSection 5\nGraphics \n",
      "content_length": 3114,
      "extraction_method": "Direct"
    },
    {
      "page_number": 470,
      "chapter": null,
      "content": "• It allows the visualization of high-resolution textures with the possibility of\nincluding higher-resolution regions.\n• It allows the use of several independent large textures that can be combined to\nshow different information types simultaneously on the terrain.\nStructure\nThe technique proposed manages a virtually unlimited texture that we call the virtual\ntexture. It is stored using a pyramidal mipmap scheme [Williams83]. The highest\ndetail level of this pyramid is formed by 2l–1 \u0002 2l–1 texels at most (such as in case of a\nsquare texture), with l being the number of levels in the pyramid. Levels are num-\nbered from 0 to 2 i, the largest side size for level i, as illustrated in Figure 5.8.1.\n5.8\nMapping Large Textures for Outdoor Terrain Rendering\n437\nFIGURE 5.8.1\nVirtual texture.\nThe virtual texture is stored complete on persistent storage, either on a local disk\nor remotely requested from a server. This virtual texture is structured in the persistent\nstorage level in square tiles with a side size in texels power of two. An exception to this\nis those levels of the pyramid in which the texture size is smaller than the tile size.\nTiles are addressed with a vector (column, row, and level).\nThe pre-filtered mipmap levels of the virtual texture increase by a third the stor-\nage space required, but this is needed to map the texture in an efficient way that\navoids aliasing artifacts.\nTexture Cache\nFollowing the clipmap concept, a subset of the full pyramid is cached in texture mem-\nory to apply the adequate detail level to the area being visualized. This virtual texture\nis managed through a two-level cache system. The second-level cache is located in\nmain memory and uses a pool of buffers to store the least recently used tiles. \n",
      "content_length": 1755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 471,
      "chapter": null,
      "content": "Tiles are asynchronously loaded on demand. Requests are prioritized by level with\ncoarser levels given higher priority. This way, larger areas are covered as quickly as pos-\nsible and the detail around the center of interest is progressively refined as higher level\ntiles become available. The tile size is a critical parameter, as it can impact the transfer\nrate from persistent storage to main memory.\nThe first-level cache is a subset of the virtual texture levels that is resident in tex-\nture memory. The virtual pyramid is fully stored in texture memory from the apex to\nthe base level. This set of levels is called the pyramid and it will be managed as a reg-\nular mipmapped texture. The size of the base level is called the clipsize. The base level\n(lb) is calculated from the clipsize (c) as lb=log2(c).\nFrom the base level up, only a subregion of the whole level is stored. The set of\nincomplete levels is called the stack. The levels of the stack are all the same size in tex-\nels and, progressively from the coarser to the finer detail level, half the terrain region.\nThe levels that make up the stack are incomplete subsets (with size c \u0002 c texels) of the\ncorresponding virtual texture levels. These levels are centered on a point of interest,\ncalled the center of detail. These concepts are shown in Figure 5.8.1.\nYou’ll use l–lb+1 independent textures in texture memory, as shown in Figure\n5.8.2. The first one (t0) corresponding to the pyramid’s finest level. Subsequent tex-\ntures ti cache the virtual level lb+i.\n438\nSection 5\nGraphics \nFIGURE 5.8.2\nTexture stack.\nTrilinear Filtering\nIn order to allow the graphics system to perform a trilinear filtering to avoid aliasing,\nmipmap levels for every texture are needed. Let tij be the mipmap level j of the texture\ni; it caches level lb+i–j of the virtual texture.\nAs shown in Figure 5.8.3, it is not necessary to have all mipmap levels in the tex-\ntures corresponding to the stack. This can save valuable bandwidth during cache updat-\ning. Our experience proves that about four or five mipmap levels in the textures of the\nstack are enough to achieve good quality without noticeable artifacts with a clipsize of\n1024 \u0002 1024 texels.\nFigure 5.8.4 illustrates the terrain area covered by different levels of the stack.\nThere, you can see the application of those levels to a real terrain, represented by a\ncolor-coded grid (see Color Plate 11 in the color insert of this book for the full-color\nversion of this image).\n",
      "content_length": 2484,
      "extraction_method": "Direct"
    },
    {
      "page_number": 472,
      "chapter": null,
      "content": "Texture Memory Usage\nFor an l level virtual texture with a clipsize c, m mipmap levels for the stack textures\nand a texel depth of b bytes, the usage of texture memory for the cache can be com-\nputed as follows:\n(5.8.1)\ntexture\nmemory\nl\nl\nc\nb\ni\ni\nm\n_\n=\n−\n−\n(\n)⋅\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟+\n=\n−\n∑\n1\n2\n2\n2\n0\n1\n2\n0\ni\ni\nlb\nb\n=∑\n⎛\n⎝\n⎜\n⎜\n⎞\n⎠\n⎟\n⎟⋅\n5.8\nMapping Large Textures for Outdoor Terrain Rendering\n439\nFIGURE 5.8.3\nTexture stack with mipmap levels correspondence.\nFIGURE 5.8.4\nRings of detail and an example of a virtual texture applied to the terrain,\nshowing levels of detail using color codes.\n",
      "content_length": 582,
      "extraction_method": "Direct"
    },
    {
      "page_number": 473,
      "chapter": null,
      "content": "Higher levels in the pyramid can be incomplete, allowing the inclusion of addi-\ntional detail for special interest areas over a constant overall image detail. It is quite\nusual in games such as flight simulators to have a medium detail satellite texture over\nall the terrain and increase detail in some areas in which the plane is likely to fly low\nor approximate, such as an airport.\nUpdating the Contents of the Cache\nData stored on the texture cache corresponds to a zone of the terrain covered by the\nvirtual texture around the center of detail. As this center of detail is moved, contents\nof the cache need to be updated.\nDetail Center Computation\nEvery frame, the application must place the center of detail in the location where the\nmaximum quality is desired. Several strategies can be used. Typically, you’ll use camera\nposition and orientation. The trivial approach is placing the center of detail in the ver-\ntical projection of the camera location over the ground. Better results can be achieved\nby placing it on a point of the visible terrain close to the camera, and then computing\nthe intersection with the terrain of the eye view direction.\nTexture Stack Update\nWhatever strategy is used, once the center of detail is placed, stack texture levels must\nbe updated. Each level is updated sequentially from coarser to finer.\nTextures corresponding to these levels are considered divided in square blocks\nwith side size power of two. These blocks are called subtiles to differentiate them from\nthe tiles stored in the second level cache. The subtile is the texture updating atomic\nunit. Subtile size (s) must be a divisor of the clipsize (c) and the tile size (t), where\ns = 2i , t = 2 j , c = 2k , with i <= j , i < k\n(5.8.2)\nAs the center of detail is moved, some subtiles will become invalid and will have\nto be updated, whereas others will retain useful data. For each texture, there is a sub-\ntile state matrix indicating the validity of each subtile in the texture. Immediately after\nplacing the center of detail, these matrices will need updating for the new position.\nAfter the state matrices are updated, each texture is processed, from coarser to\nfiner detail. For each invalid subtile, you compute the address of the tile containing\nthe subtile data. This tile is requested from the second level cache. If it is resident the\nsubtile data is uploaded to the texture memory; otherwise, the asynchronous load of\nthe tile will be requested by the RAM tile cache and will be available in the next few\nframes. In case of incomplete levels, invalid subtiles absent from persistent storage will\nnever be updated.\n440\nSection 5\nGraphics \n",
      "content_length": 2652,
      "extraction_method": "Direct"
    },
    {
      "page_number": 474,
      "chapter": null,
      "content": "The virtual texture window cached in each stack level is accessed toroidally in the\ncorresponding real texture. This allows partial updates of each level, which drastically\nimproves the efficiency. As seen in Figure 5.8.6, when the center of detail is updated,\nthe window position is changed. Using the wraparound addressing only the new sub-\ntiles not present in the previous window position have to be loaded, while the overlap-\nping area remains in place.\nA subtile update of a texture implies updating the related area in each mipmap\nlevel of this texture. In the mipmap level updating process, consider that subtile size\nfor mipmap level m is s/2 m. Update of levels tij, where j > 0 can be made from coarser\ntextures, because mipmap levels data is replicated, as shown in Figure 5.8.3.\nLoad Control\nTexture upload time is critical for a real-time graphics application like a game to sus-\ntain the frame rate. The render time plus the texture update time must not exceed the\nframe time. For this reason, updating subtiles is limited in duration for each frame.\nThat means that for quick movements of the center of detail, it will not be possible to\nreach the finest detail in one single frame. This usually is not a problem because fast\nmovements do not usually allow the viewer to appreciate details in the image and a\nblurry aspect is normally acceptable.\nWhen deciding the subtile size, it is important to find a tradeoff between an ade-\nquate load control and a high transfer rate. The smaller the subtile size, the higher the\naccuracy to measure the update time. Even though the subtile update time is strongly\ndependent on the hardware used, the smaller sizes typically have a very poor efficiency.\nOur experience has shown that subtile sizes of 128 \u0002 128 give the best performance.\nConcentric Rings Update\nBecause of the previously mentioned update time limit, textures in the stack are not\nalways completely updated. It is necessary to decide when a texture is updated with\nenough data to be applied. The simple approach is to exclude a texture from use in the\nstack until it is completely updated. The problem here is that every time the center of\ndetail is moved the distance of a subtile, it will be invalidated until being completely\nupdated again. This problem is reduced by applying the texture even though only a\npartial area of the full texture is loaded.\nYou update the subtiles of each texture in concentric rings, innermost to outer-\nmost, so the coverage grows as the subtile rings are updated (see Figure 5.8.5). This\nway, the texture is useful from the moment it begins to have some valid subtiles.\nBeginning from the center, the highest interest zone is available sooner. Also, the cen-\nter subtiles are the ones with higher life expectancy.\n5.8\nMapping Large Textures for Outdoor Terrain Rendering\n441\n",
      "content_length": 2831,
      "extraction_method": "Direct"
    },
    {
      "page_number": 475,
      "chapter": null,
      "content": "Pseudocode\nThe virtual texture update is summarized in the following pseudocode:\nCompute the center of detail position\nFor each texture level of the stack\nupdate the subtile validity matrix\nFor each texture level of the stack from coarser to finer detail\nFor each subtile of the level (innermost to outermost)\nand while update time limit is not surpassed\nIf the subtile state is invalid\nCompute the address of the disk tile\nRequest the tile to the RAM tile cache\nIf the tile is cached \nUpdate the subtile in all mipmap levels\nSet subtile state to valid\nRendering Issues\nGeometry management algorithms can be adapted and used with the described tech-\nnique. There are two possible ways to map a virtual texture to a geometry model. The\nfirst way, considering the geometry model is divided into patches, is to apply the\nfinest available texture that covers each geometry patch. In this case, you would follow\nthese steps:\nFor each geometry patch\nApply the finest texture level that covers the patch\nCompute the texture coordinates for the selected texture\nDraw the patch\nThe second way is to select each texture level, asking for its coverage and drawing\nthe geometry covered by the selected level but not for the finer ones. In this case, you\nwould follow these steps:\n442\nSection 5\nGraphics \nFIGURE 5.8.5\nCircular update.\n",
      "content_length": 1322,
      "extraction_method": "Direct"
    },
    {
      "page_number": 476,
      "chapter": null,
      "content": "For each texture level\nSelect and apply the texture\nCompute the texture coordinates for the selected texture\nCompute geometry covered by the level but not finer ones\nDraw the geometry set computed\nNo matter which way is used, texture coordinates must be computed for every\nvertex of the geometry. These coordinates are computed for the finest level of the vir-\ntual texture. Because each texture level from the stack covers half the virtual space of\nthe coarser one, you need to scale the texture coordinates computed to translate it to\nthe virtual texture level applied. The scale factor for level i is 2 l-i-1. The toroidal updat-\ning of the textures in the stack assures that the mapping will be correct if the texture\nrepeats (see Figure 5.8.6).\n5.8\nMapping Large Textures for Outdoor Terrain Rendering\n443\nFIGURE 5.8.6\nToroidal update and mapping example.\nThe texture coordinate computation just described can be done in several ways. For\nstatic geometry, texture coordinates can be precomputed and stored statically in texture\ncoordinate arrays. This way, all the computation can be done with the texture matrix\nscaling and texture repeat mode, so no shaders are needed at all. Only the standard fixed\nfunction graphics pipeline available in both OpenGL and DirectX is needed.\nIn the case of dynamic geometry, texture coordinates must be computed every\ntime the vertices are modified. In both cases, but especially with dynamic geometry, it\nis very helpful to automatically compute the texture coordinates in a vertex shader.\nThis way, you avoid their computation in CPU, the transfer from main memory to\nvideo memory, and the storage for texture coordinate arrays in video memory. The\nfollowing pseudocode shows how to calculate texture coordinates:\nL: left texture limit, R: right texture limit,\nT: top texture limit, B: bottom texture limit,\n(x,y,z): vertex position, i: virtual texture level selected\nscale = 2l-i-1\nu = scale * (x-L)/(R-L)\nv = scale * (y-B)/(T-B)\n",
      "content_length": 1974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 477,
      "chapter": null,
      "content": "Texture coordinate computation can include additional transformations in case\nthere are different coordinate systems for the texture database and the geometry data-\nbase. By using only one GPU texture stage for the mapping of the virtual texture, this\nallows you to easily combine the virtual texture with other virtual or regular textures,\neach one bound to a texture stage.\nResults\nThe presented technique has been tested using a proprietary terrain navigation system\nwith a data set containing a virtual texture of aerial terrain photographs covering an\narea of about 250 \u0002 200Km [Santi07]. The resolution of this image is 0.5m per texel,\nwhich is a stack of 19 levels.\nFigure 5.8.7 shows the results of a stress test executed on a low-end computer\nusing a programmed flight at 3000Km/h over the terrain, using a large clipsize (2048\nsquare texels), and an update time limit of only 1ms.\n444\nSection 5\nGraphics \nFIGURE 5.8.7\nTest results.\nTable 5.8.1\nSystem Configuration for Testing\nGraphics hardware\nAGP 8x NVIDIA GeForce 7800 GS\nClipsize\n2048 texels\nTile size\n512 texels\nSubtile size\n128 texels\nUpdating-time limit\n1ms\nVirtual texture size\n~ 1M  \u0002 1M\nVirtual texture color depth\n24 bits (RGB888)\nFiltering\nTrilinear\nAnisotropic filtering\n4x\nFlight speed\n3000Km/h\n",
      "content_length": 1269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 478,
      "chapter": null,
      "content": "Figure 5.8.7 shows the graphs for a six-second interval, using the configuration\nshown in Table 5.8.1. The first graph shows the time used by the system to upload\ntexture subtiles to VRAM. The system attempts to limit update times to 1ms in order\nto leave time to process the rest of the application.\nThe second graph shows the completeness of the texture stack which can be used\nas a measure of the quality of the texture shown by the system. In spite of the stress\nconditions of the test, it holds a completeness level of about 75%, that means a tex-\nture detail of 1m per texel.\nTable 5.8.2\nStatistics\nMin.\nMax.\nAvg.\nStd. Dev.\nSubtiles per frame\n0\n4\n2.15\n1.55\nSubtiles load (ms)\n0.19\n1.09\n0.33\n0.09\nCompleteness (%)\n72.83\n77.69\n75.53\n0.86\nTable 5.8.2 shows some interesting statistics, such as an average of two subtiles\nuploaded to video memory per frame or an average of 0.33ms used per frame for updat-\ning. Tests with more favorable conditions, such as reducing the clipsize to 1024 texels,\nmaintain averages over 95% quality (0.5m per texel) during all the executions, even\nusing only 1ms as the update time limit, which proves the efficiency of the technique.\nConclusion\nThe technique described in this gem makes it possible to efficiently manage large tex-\ntures beyond hardware limits. They can be used in a variety of real-time applications due\nto configurable load control. The technique stores the image in tiles that are not used\ndirectly as textures. These tiles are combined in a texture stack that caches the region of\ninterest, following the clipmap idea. You do not need to subdivide the geometry to\nmake the patches match the texture tile boundaries, as occurs in many terrain visualiza-\ntion techniques. Whatever the geometry algorithm, there will always be a texture to map\neach patch.\nThe limitation of this technique is more about patch size than geometry struc-\nture, subdivision, or tessellation. The implementation of this texturing technique has\nbeen successfully used with different geometry algorithms, based on grids as well as\nTINs, with some slight level dropping when using large geometry patches for close\nviews, which can be usually avoided.\n5.8\nMapping Large Textures for Outdoor Terrain Rendering\n445\n",
      "content_length": 2240,
      "extraction_method": "Direct"
    },
    {
      "page_number": 479,
      "chapter": null,
      "content": "References\n[Montrym97] Montrym, J.S., Baum, D.R., Dignam, D.L. and Migdal, C.J.\n“InfiniteReality: A Real-Time Graphics System,” SIGGRAPH 97, Proceedings of\nthe 24th Annual Conference on Computer Graphics and Interactive Techniques,\npp. 293–302. ACM Press/Addison-Wesley Publishing Co, 1997.\n[Santi07] The SANTI Project Web Page, available at http://videalab.udc.es/santi.\n[Tanner98] Tanner, C.C., Migdal, C.J., and Jones, M.T. “The Clipmap: A Virtual\nMipmap,” In Proceedings of the 25th Annual Conference on Computer Graph-\nics and Interactive Techniques, pp. 151–158. ACM Press, 1998.\n[VTerrain07] The Virtual Terrain Project Website, http://vterrain.org.\n[Williams83] Williams, L. “Pyramidal Parametrics,” SIGGRAPH 83, Proceedings of\nthe 10th Annual Conference on Computer Graphics and Interactive Techniques,\npp. 1–11. ACM Press, 1983.\n446\nSection 5\nGraphics \n",
      "content_length": 863,
      "extraction_method": "Direct"
    },
    {
      "page_number": 480,
      "chapter": null,
      "content": "447\n5.9\nArt-Based Rendering with\nGraftal Imposters\nJoshua A. Doss, Advanced Visual Computing,\nIntel Corporation\njoshua.a.doss@intel.com\nG\nraftals are used to express the shape and formation of plants in a formal grammar for\nuse in computer graphics. A close relative of fractals, graftals allow for a compact\nrepresentation of foliage [Smith84]. Graftals have also been used in a non-photorealistic\ncartoon rendering implementation at interactive frame rates [Kowalski98]. Graftal\nimposters are used as a real-time method of drawing cartoon-style plants and fur using\nthe geometry shader in modern GPUs. The particular style we aim to produce is inspired\nby Dr. Seuss’s children’s book illustrations [Seuss71].\nAn artist will provide sketches of graftal imposters along with a set of textures that\nplace the foliage in a scene with a high amount of control over the final look and feel.\nThe imposters are placed along the silhouette edges of the object. See Color Plate 12\nfor a full-color example.\nAssets\nCreating graftal imposters requires a set of assets in addition to the geometry—a tex-\nture atlas, control texture, and a vector field texture. The texture atlas contains three\ntypes of graftal imposters along with several variations of each type. A control texture\nprovides information about what type of graftal imposter should be placed at a certain\nlocation on the mesh. The vector field gives a direction and the color texture provides\ninformation on the coloring of the landscape mesh as well as the graftal imposters. \nTexture Atlas\nThe texture atlas contains the graftal imposter itself. It is created by specifying a few\ndifferent types of graftal imposter types, in different rows. The exterior of each graftal\nimposter should have an RGB and alpha value of zero for all components. As you near\nthe soft edge of the graftal imposter, the alpha value should go smoothly from zero to\none in the middle of the stroke, giving you a smooth transition and reducing any\naliasing effects. The red, green, and blue channels should remain zero until the alpha\nchannel is saturated, as shown in Figure 5.9.1.\n",
      "content_length": 2114,
      "extraction_method": "Direct"
    },
    {
      "page_number": 481,
      "chapter": null,
      "content": "Once you reach the middle of the outline, it blends smoothly into solid red, leav-\ning the alpha channel saturated. The inside of graftal imposters will be blended with\nthe color of the underlying geometry while the outside will be blended with the rest of\nthe scene.\nControl Texture\nThe control texture enables the designer to specify where a graftal imposter may \nbe placed and the type of graftal imposter to be drawn. The alpha channel is used to\nindicate areas where no graftal imposters can be drawn. Red should be used where the\ndesigner wants graftal imposters from the first row of the texture atlas, green for \nthe second row, and blue for the third and final row. Using three color channels isn’t\nthe optimal encoding; however, it simplifies the asset-creation process.\nVector Field\nIt is often desirable to indicate a direction, or flow of the graftal imposters. One exam-\nple of this comes when using this technique to create fur (or hair) on a character. You\ncould use the normal as an extrusion direction; however, this limits the amount of\ncontrol the end user has over the final look of the scene. Hair, plants, trees, and so on,\ndon’t always grow at a right angle to the surface from which they protrude. To solve\nthis, you can create the vector field, which gives you the direction. See Figure 5.9.3.\n448\nSection 5\nGraphics \nFIGURE 5.9.1\nThe texture atlas uses red as a color key inside of the graftal\nimposter and the alpha channel to smoothly blend the graftal imposter with\nthe rest of the scene.\n",
      "content_length": 1519,
      "extraction_method": "Direct"
    },
    {
      "page_number": 482,
      "chapter": null,
      "content": "To create the vector field, you leverage existing digital content-creation applica-\ntions and plug-ins. After creating a mesh of the desired resolution, the designer saves it\nand substantially reduces the tessellation of the mesh. You want to be sure to preserve\nthe original normals, as they are used in another step and passed with the mesh. Next,\nthe designer manipulates the normals to indicate which direction the graftal imposters\nshould go; this can be done on a per-face basis or by selecting several normals at the\n5.9\nArt-Based Rendering with Graftal Imposters\n449\nFIGURE 5.9.2\nThe control texture indicates coverage and row selection.\nFIGURE 5.9.3\nThe vector field texture indicates the\ndirection of the graftal imposters.\n",
      "content_length": 734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 483,
      "chapter": null,
      "content": "same time. Once the manipulation is complete, these new values can be saved using a\nnormal map plug-in creating the vector field. The result will look like Figure 5.9.3.\nColor Texture and Mesh\nThe color texture is used to indicate the color of the mesh as well as the internal color-\ning of the graftal imposters. Figure 5.9.4 is the color texture for the scene. This tech-\nnique creates the graftal imposters along the edges. Large differences in edge length\nresult in visible irregularities in the width of the graftal imposters; therefore, the mesh\nshould contain triangles of roughly uniform size. In areas with a high level of detail\nwhere extremely small triangles are required, it might be best to use the control texture\nto omit the creation of graftal imposters. \n450\nSection 5\nGraphics \nFIGURE 5.9.4\nThe color texture is used to color the\nbase mesh and the graftal imposters.\nRuntime\nYou can now use the assets created in the previous step during the runtime compo-\nnent of the algorithm. This implementation of graftal imposters requires the use of a\nprogrammable graphics language with a geometry shader. First, you draw the original\nmesh and apply the color texture. Next, you use the geometry shader to determine\nwhere to place the graftal imposters as well as which type of graftal imposter to place.\nFinally, you use the pixel shader to give the final color to the graftal imposters and\nblend them with the rest of the scene.\n",
      "content_length": 1442,
      "extraction_method": "Direct"
    },
    {
      "page_number": 484,
      "chapter": null,
      "content": "The control texture was created earlier to dictate where you can create graftal\nimposters as well as what type of graftal imposter to draw in a given area. You need to\ntest the triangle to determine whether the primitive is eligible for a graftal imposter\nand assign a type if it is.\ntexCoordCentroid = ( vertex1uv + vertex2uv + vertex3uv ) / 3;\ncontrolSample = controlTexture.sample( sampler, texCoordCentroid );\nif( controlSample.a == 1 )\nif( controlSample.r == 1)\nglyphType = 0;\nelseif( controlSample.g == 1)\nglyphType = 1;\nelse\nglyphType = 2;\nSampling a texture from within the geometry shader is allowed using the unified\ninstruction set provided with Direct3D 10. You need to choose a point at which to sam-\nple the texture, since you have access to multiple vertices. The previous pseudocode\nshows how you can sample the control texture using the centroid of the triangle cur-\nrently being processed.\nNow that you know the triangle is eligible for graftal imposter(s), you need to test\neach edge to see whether it is a silhouette edge. A silhouette edge is an edge that’s shared\nby both a front and a back facing triangle. In order to test an edge to see whether it is \na silhouette edge, calculate the dot product of the face normal N1,N2 with the view\ndirection V for both faces and test to see if the signs differ [Lake00].\n(5.9.1)\nTo create the new geometry at the silhouette edge, you extrude vertices V0 and V1\nin a direction D obtained from the vector field sampled using the texture coordinates\nat the midpoint M of the edge. See Figure 5.9.5.\n(\n) (\n)\n0\nN\nV\nN\nV\n1\n2\n•\n∗\n•\n≤\n5.9\nArt-Based Rendering with Graftal Imposters\n451\nFIGURE 5.9.5\nNew vertices are created by extruding each vertex\nalong the edge where graftal imposters are desired.\n",
      "content_length": 1755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 485,
      "chapter": null,
      "content": "//Recreate the original vertex V0\nPosition = Input.Position;\nOutput.Position = mul(Position, ObjectToProjection);\nOutput.GraftalImposterColor = V0Color = ColorTexture.sample\n(sampler, Input.V0.Texcoord);\n. . .\nAppendVertex();\n//Create a new vertex in the appropriate direction\nPosition = Input.Position * Direction + GraftalHeight;\nOutput.Position = mul(Position, ObjectToProjection);\nOutput.GraftalImposterColor = V0Color;\n. . .\nAppendVertex();\n//Recreate the original vertex V1\nPosition = Input.Position;\nOutput.Position = mul(Position, ObjectToProjection);\nOutput.GraftalImposterColor = V1Color = ColorTexture.sample\n(sampler, Input.V1.Texcoord);\n. . .\nAppendVertex();\n//Create final new vertex, finishing the quad\nPosition = Input.Position * Direction + GraftalHeight;\nOutput.Position = mul(Position, ObjectToProjection);\nOutput.GraftalImposterColor = V1Color;\n. . .\nAppendVertex();\nThis pseudocode shows how to create the graftal imposter surface as well as sam-\npling the color texture in order to shade the graftal imposter. Selecting the color once\nper incoming vertex allows you to have a graftal imposter that crosses a color bound-\nary, because the value is interpolated as it is passed to the pixel shader. \nNext, you assign texture coordinates to the newly created geometry to place the\ngraftal imposter on the newly created surface by indexing into the texture atlas. You\nuse the graftal imposter type G to index into the correct row of the texture and a\npseudorandom value such as a sample into a noise texture to determine the column C.\nNC is the number of variations, or columns, the texture atlas contains (see Equations\n5.9.2–5.9.5).\n(5.9.2)\n(5.9.3)\nuv\nC\nN\nG\nV\nc\nNew\n0\n3\n=\n,\nuv\nC\nN\nG\nV\nc\n0\n3\n1\n3\n=\n+\n,\n452\nSection 5\nGraphics \n",
      "content_length": 1745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 486,
      "chapter": null,
      "content": "(5.9.4)\n(5.9.5)\nThe final step is to sample the texture atlas using the texture coordinates calcu-\nlated in the geometry shader. You want to have a smooth transition from the graftal\nimposter’s black outline to the internal color passed in by the vertex shader. To accom-\nplish this, the red channel of the result is masked off and the sample is linearly inter-\npolated with the color value passed in via the geometry shader. The red channel is\nused as the blending factor in the interpolation.\n//coloring the graftal imposter\nAtlasColor = AtlasTexture.Sample(Sampler, \nGraftalImposterTextureCoords);\nRedZero = float3(0,AtlasColor.gb);\nGraftalImposterColor = lerp(RedZero.rgb, IncomingColor.rgb, \nAtlasColor.rrr);\nGraftalImposterColor.a = AtlasColor.a;\nThe incoming color is interpolated across the two vertices that you sampled within\nthe geometry shader. You blend the graftal imposter’s soft edge by doing a linear inter-\npolation with the red channel masked out. Preserving the alpha value enables you to\nblend the outside of the smooth edge with the underlying landscape color. See Color\nPlate 12 for a full-color example of rendering with graftal imposters.\nAcknowledgements\nThe author would like to thank Jeffery A. Williams, Rahul Sathe, David Bookout,\nNico Galoppo, Adam Lake, and the Advanced Visual Computing team at Intel for\ntheir assistance, support, and contributions. \nConclusion and Future Work\nThis gem has shown how to create a scene in a style similar to that found in Dr. Seuss’s\nchildren’s books. The technique utilizes the new technology capabilities of geometry\nshaders in this GPU-centric technique, leaving more CPU cycles for game logic and\nother tasks. Currently, we are applying graftal imposters only to the silhouette edges. In\nour future work, we would like to automatically generate the vector field used for\nextrusion directions without an artist having to encode it for the entire geometry. \nWhen implementing this technique for production, a couple of additional fea-\ntures may be desired. Adapting the introduction and removal of graftal imposters to\nprovide for inter-frame coherence by scaling or “fading” in the graftal imposters is one\npossible solution. Another important consideration is handling of z-fighting.\nuv\nC\nN\nN\nG\nV\nc\nc\nNew\n1\n1\n3\n=\n+\n,\nuv\nC\nN\nN\nG\nV\nc\nc\n1\n1\n3\n1\n3\n=\n+\n+\n,\n5.9\nArt-Based Rendering with Graftal Imposters\n453\n",
      "content_length": 2374,
      "extraction_method": "Direct"
    },
    {
      "page_number": 487,
      "chapter": null,
      "content": "References\n[Doss07] Doss, Joshua A. “Inking the Cube: Edge Detection with Direct3D 10,”\nGame Developer Magazine, June/July 2007, pp. 13–18.\n[Kowalski98] Kowalski, Michael A., et. al. “Art-Based Rendering of Fur, Grass, and\nTrees,” Proceedings of the 26th Annual Conference on Computer Graphics and\nInteractive Techniques, SIGGRAPH 1998, pp. 433–438.\n[Lake00] Lake, Adam, et. al. “Stylized Rendering Techniques for Real-Time 3D Ani-\nmation and Rendering,” Proceedings of the 1st International Symposium on\nNon-photorealistic Animation and Rendering, NPAR, 2000, pp. 13–20.\n[Rost06] Rost, Randi J. OpenGL Shading Language, Addison Wesley, 2006.\n[Seuss71] Seuss, Dr. The Lorax, Random House, Inc., 1971.\n[Smith84] Smith, Alvy Ray. “Plants, Fractals, and Formal Languages,” Computer\nGraphics, Vol. 18, No. 3, SIGGRAPH 1984, pp. 1–10.\n454\nSection 5\nGraphics \n",
      "content_length": 854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 488,
      "chapter": null,
      "content": "455\n5.10\nCheap Talk: Dynamic \nReal-Time Lipsync\nTimothy E. Roden, Angelo State University\ntroden@angelo.edu\nG\name developers are increasingly using lipsyncing for in-game 3D characters. One\nproblem is that getting lipsyncing up and running can be both time-consuming\nand expensive. A custom solution may involve valuable programmer time while a\nmore expedient method, involving purchased middleware, can have other drawbacks.\nParticularly for developers wanting to experiment with lipsyncing, perhaps as part of\na proof-of-concept demo, a quicker, less expensive solution is desirable. Fortunately,\nyou can incorporate lipsyncing into a game on the cheap and in a minimum amount\nof time. The result is at least adequate for a proof-of-concept and might be sufficient\nfor a packaged game. This gem explains a method for quick and easy lipsyncing.\nRequirements\nIn order to use this method, several general requirements need to be met. First, you need\na 3D character. Because the example animates the lips, you need at least a pair of lips\nand preferably an entire head. This gem’s examples use a head generated with Singular\nInversion’s FaceGen® software. The head model we are using is shown in Figure 5.10.1.\nThe model consists of 7,341 vertices and 12,960 triangles, not including the hair.\nThe head model needs to have some parametric controls for mouth positions that\ncan be manipulated dynamically. A set of morph targets works great. If you are not\nversed in how morph targets work, Lever provides a good explanation [Lever02]. A\nnice thing about the FaceGen® models is that they come with a large set of morph tar-\ngets for both facial expressions and lip positions, which correspond to various basic\nunits of speech. As shown in Figure 5.10.2, the head used here has 16 morph targets\nfor visemes, which are visual representations of speech such as “aah” and “ee.” Watt\nand Policarpo describe visemes as the basic units of visual speech that are described by\nextreme lip shapes, which correspond to basic auditory speech units [Watt03]. A set\nof visemes constitutes a minimally distinct set representing the sounds in a language.\nYou can probably imagine more lip positions than the 16 shown in Figure 5.10.2.\nHowever, this minimal set is actually quite good for the purposes here and will allow\nyou to generate very convincing lipsync animation.\n",
      "content_length": 2353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 489,
      "chapter": null,
      "content": "456\nSection 5\nGraphics \nFIGURE 5.10.1\nA head model generated using Singular Inversion’s\nFaceGen® software.\nFIGURE 5.10.2\nThe 16 visemes used here, each\nshown at its extreme (1.0) morph. See Color Plate 13.\n",
      "content_length": 206,
      "extraction_method": "Direct"
    },
    {
      "page_number": 490,
      "chapter": null,
      "content": "You’ll need the ability in your program to independently adjust each of the 16\nvisemes using a float value that ranges from 0.0 to 1.0. Values of 0.0 effectively turn\noff the viseme, whereas values of 1.0 mean the viseme is at full strength. Figure 5.10.3\nillustrates how a value of 0.0 adds nothing to the mouth position, whereas higher val-\nues cause the mouth to morph into the desired shape. This example allows any set of\ncombinations. So, for example, you could have the mouth change shape by applying\nthe “aah” viseme at a value of 1.0 combined with 0.5 of the “ee” viseme. In fact, this\nability is crucial to enabling you to generate realistic dynamic lipsync.\n5.10\nCheap Talk: Dynamic Real-Time Lipsync\n457\nFIGURE 5.10.3\nThe “aah” viseme at varying values (from left: 0, 0.33, 0.66, and 1.0).\nFor audio, you can use pre-recorded speech or audio generated speech at runtime,\nsuch as the output of text-to-speech engine. You will also need the text of what is\nbeing spoken. Using a text-to-speech engine works nicely because the audio is gener-\nated at runtime based on a text string, so you get the text and audio at the same time. \nGeneral Procedure\nFor each lipsynced audio sample, the general runtime procedure is as follows:\n1. Translate each word of the text into its corresponding set of phonemes.\n2. Translate each phoneme into its corresponding viseme.\n3. Generate animation data based on the set of visemes.\n4. Start playing the audio.\n5. Use the animation data to drive the 3D model during audio playback.\nThe companion CD-ROM contains source code written in C++ for a static\nlibrary that implements Steps 1 through 3 of this procedure.\nWord to Phoneme Mapping\nPhonemes are different from visemes. Phonemes are the basic distinctive units of how\nspeech is heard in a language. Individual words can be broken down into phonemes\nbased on the individual sounds that make up a word. Visemes, on the other hand, are\n",
      "content_length": 1929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 491,
      "chapter": null,
      "content": "the basic visual units of speech. There is a close correspondence between phonemes and\nvisemes. There are typically more phonemes in a language than visemes. That is because\nseveral different sounds may be represented by the same lip position. For example, “s”\nand “z” are audibly different sounds, but the position of the lips can be similar.\nTranslating words into phonemes couldn’t be easier than using the Carnegie Mel-\nlon Pronouncing Dictionary [CMU07]. The CMU dictionary is available online and\ncan be used for any research or commercial purpose without restriction. It is a text file\ncontaining over 118,000 English words and their corresponding phonetic transla-\ntions. For example, the word “hello” translates to the four phonemes HH, AH, L, and\nOW. There are a total of 39 distinct phonemes in the CMU dictionary. Table 5.10.1\nlists each phoneme and an example word found in the dictionary that uses the\nphoneme. Because the dictionary is already in alphabetical order in the text file, it is a\nfairly simple programming task to read the dictionary into an array and perform a\nbinary search to look up words and retrieve their corresponding phonemes.\nTable 5.10.1\nThe 39 CMU Phonemes\nPhoneme\nExample\nTranslation (of the Example)\nAA\nOdd\nAA D\nAE\nAt\nAE T\nAH\nHut\nHH AH T\nAO\nOught\nAO T\nAW\nCow\nK AW\nAY\nHide\nHH AY D\nB\nBe\nB IY\nCH\nCheese\nCH IY Z\nD\nDee\nD IY\nDH\nThee\nDH IY\nEH\nEd\nEH D\nER\nHurt\nHH ER T\nEY\nAte\nEY T\nF\nFee\nF IY\nG\nGreen\nG R IY N\nHH\nHe\nHH IY\nIH\nIt\nIH T\nIY\nEat\nIY T\nJH\nGee\nJH IY\nK\nKey\nK IY\nL\nLee\nL IY\nM\nMe\nM IY\nN\nKnee\nN IY\nNG\nPing\nP IH NG\nOW\nOat\nOW T\n→\n458\nSection 5\nGraphics \n",
      "content_length": 1587,
      "extraction_method": "Direct"
    },
    {
      "page_number": 492,
      "chapter": null,
      "content": "Phoneme\nExample\nTranslation (of the Example)\nOY\nToy\nT OY\nP\nPee\nP IY\nR\nRead\nR IY D\nS\nSea\nS IY\nSH\nShe\nSH IY\nT\nTea\nT IY\nTH\nTheta\nTH EY T AH\nUH\nHood\nHH UH D\nUW\nTwo\nT UW\nV\nVee\nV IY\nW\nWe\nW IY\nY\nYield\nY IY L D\nZ\nZee\nZ IY\nZH\nSeizure\nS IY ZH ER\nPhoneme to Viseme Mapping\nTranslating phonemes to visemes is a direct lookup based on a table you need to cre-\nate beforehand. The dictionary contains 39 separate phonemes and the 3D model\nused here has 16 visemes. A little creativity is required here to determine the correct\nviseme for each of the phonemes. Probably the easiest way to do this is in front of a\nmirror. Using Table 5.10.1, pronounce each example word and notice the position of\nyour lips as you sound out the particular phoneme in the word. Match your lip posi-\ntion with the closest viseme in Figure 5.10.2. For the purposes of this gem, we will use\nthe mapping shown in Table 5.10.2. \nTable 5.10.2\nPhoneme to Viseme Mapping\nPhoneme\nViseme\nPhoneme\nViseme\nPhoneme\nViseme\nAA\nBig aah\nF\nF,V\nP\nB,M,P\nAE\nAah\nG\nCh,J,sh\nR\nR\nAH\nAah\nHH\nEh\nS\nD,S,T\nAO\nBig aah\nIH\nI\nSH\nCh,J,sh\nAW\nBig aah\nIY\nEe\nT\nD,S,T\nAY\nAah\nJH\nCh,J,sh\nTH\nTh\nB\nB,M,P\nK\nCh,J,sh\nUH\nOh\nCH\nCh,J,sh\nL\nTh\nUV\nOoh,Q\nD\nD,S,T\nM\nB,M,P\nV\nF,V\nDH\nTh\nN\nN\nW\nW\nEH\nEh\nNG\nD,S,T\nY\nEe\nER\nR\nOW\nOh\nZ\nW\nEY\nEh\nOY\nOoh,Q\nZH\nCh,J,sh\n5.10\nCheap Talk: Dynamic Real-Time Lipsync\n459\n",
      "content_length": 1311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 493,
      "chapter": null,
      "content": "Real-Time Lipsyncing\nAt runtime, for each audio sample you want lipsynced, you have to convert the string\ncontaining the text into phonemes and then into visemes. Using the code supplied on\nthe companion CD-ROM, this consists of making two functions calls. A third func-\ntion is then called to translate the visemes into lipsync animation data that can be\nused to animate the 3D model during playback of the audio. Let’s first examine how\nthe lipsync data is generated.\nSeveral methods could be used that vary in complexity with more complex meth-\nods providing possibly more accurate data. However, for the purposes of this gem,\nlet’s use an easy approach that gives quite remarkable results given its simplicity. The\nidea is to divide the duration of the spoken audio by the number of visemes and\nassign each viseme a time slot to become active during audio playback.\nFor example, Figure 5.10.4 illustrates the word “hello.” The word consists of four\nvisemes. At time 0, you begin to morph the viseme “eh” from 0 to 1 and then back to\n0. Before “eh” becomes inactive, you must begin to morph the “ahh” viseme, and so on.\nThe idea is to overlap the visemes slightly from one to the next. This results in more\nnatural looking lipsync.\nBy varying the amount of overlap, you can achieve some interesting effects. For\nexample, a long overlap period tends to make the speaker appear to slur words together,\nat least visually. A short overlap period produces very distinct visemes as might be\nexpected when someone is angry. Too short or too long of an overlap produces unnat-\nural looking results. \n460\nSection 5\nGraphics \nFIGURE 5.10.4\nThe word “hello” and its corresponding visemes animated over time.\nA few details of word timing need to be addressed in any solution. For multi-\nsentence audio, the text should contain periods to indicate the end of sentences. Each\nperiod can then be assigned a timeslot so the last viseme at the end of a sentence does\nnot bleed over into the first viseme at the start of the next sentence. The amount of\n",
      "content_length": 2039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 494,
      "chapter": null,
      "content": "time for this end-of-sentence delay will likely need to be discovered by trial and error.\nThe code on the companion CD-ROM uses 500 milliseconds. If using a text-to-\nspeech engine, one trick is to save a few text-to-speech audio files that contain multi-\nple sentences and then review them in a WAV file editor. Looking at the WAV data, it\nis easy to see the duration of the end-of-sentence delay.\nThere are obvious drawbacks to the proposed solution. Perhaps the biggest prob-\nlem is with actual human voice files. Unlike text-to-speech engines, which typically\nspeak at a constant rate, humans often speak at varying rates even within the same\nsentence. This can be problematic with the simple lipsync algorithm described here,\nbecause it relies on a constant rate of speech. Still, the advantage of this method is the\nlipsync data is generated on the fly, which can be very useful in a rapid prototyping\nenvironment where you want to get lipsync up and running quickly. \nConclusion\nCreating a dynamic real-time system for lipsync animation using the method pre-\nsented is likely a few days work, at most, for an experienced programmer. For better\nresults, the method could be enhanced. One idea is to take into account the coarticu-\nlation effect, which refers to changes in audio for a particular sound as a function of\nwhat sounds have come before and what sounds will follow. Implementation ideas are\ngiven in [Watt03]. \nReferences\n[CMU07] The Carnegie Mellon Pronouncing Dictionary, available online at\nhttp://www.speech.cs.cmu.edu/cgi-bin/cmudict, July 2007. \n[Lever02] Lever, Nik. Real-Time 3D Character Animation with Visual C++, Focal\nPress, 2002.\n[Watt03] Watt, Alan, and Policarpo, Fabio. 3D Games: Animation and Advanced Real-\nTime Rendering, Vol. 2, Addison-Wesley, 2003.\n5.10\nCheap Talk: Dynamic Real-Time Lipsync\n461\nON THE CD\n",
      "content_length": 1844,
      "extraction_method": "Direct"
    },
    {
      "page_number": 495,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 496,
      "chapter": null,
      "content": "463\nS E C T I O N\n6\nNETWORKING AND\nMULTIPLAYER\n",
      "content_length": 47,
      "extraction_method": "Direct"
    },
    {
      "page_number": 497,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 498,
      "chapter": null,
      "content": "465\nIntroduction\nDiana Stelmack\nT\nhe number of game genres that are using multiplayer gameplay is growing by\nleaps and bounds. The accessibility of the Internet is reaching more platforms\nthan ever before. The consoles are getting in the act. Console makers are providing\nInternet services to entice their customers to play online with their friends. All this\nnetworking means there is a growing need for network programming, and all these\ngenres and services mean there are more game programmers who need to interface\nwith networking. What does this mean? This means that the complex systems that\ncome together to form games need to have more clearly defined interfaces for non-\nnetworking programmers to use, tools to help find those bugs during crunch time,\nmethodologies in place to deal with security issues as they arise, and so much more.\nThis section contains some gems that just might help you address one or more of\nthese issues.\nThe first gem, by Hyun-jik Baeb, describes a technique called High Level\nAbstraction, or HLA. This technique describes a tool that could be developed to make\nit easier for the non-networking programmer to interact with the networking engine.\nWhether you are a network programmer trying to make it easier, or a non-networking\nprogrammer who wants it easy, take a look at this gem.\nAs we all know, if there is a program running on a machine, there is someone that\nwill try to hack it. Keeping up with network security is a never-ending job. This\nmeans that network programmers need to consider a strategy for keeping the player’s\ninformation safe. It is busy on that “Information Superhighway” and consumers don’t\nknow how many stops there really are between their PCs and the hosts they are con-\nnecting to. The second gem, by Jon Watte, explores the myriad of security approaches\nand presents a well-rounded solution to address most security needs of today.\nTake a game with a lot of simulation. Slow down the frame rate to do lots of cool\ngraphics. Now, for fun, add network latency to data that impacts the simulation, and\nhence the rendering of the scene. By the way, now the multiple human players who exist\nin the networked session are shooting at each other and someone wants credit for that\nkill. All of this involves a lot of network traffic, all of which needs to get from Point A to\nPoint B in a reasonable amount of time with reasonable accuracy. Put in a breakpoint,\nand that can be the end of that testing session, unless you happen to have a smart packet\nsniffer. The third gem, by David Koenig, explores the mechanism to create a game-\nspecific packet sniffer to make finding those network issues easier. Understanding the\ndata is half the battle when you are debugging a gameplay issue on the network.\n",
      "content_length": 2758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 499,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 500,
      "chapter": null,
      "content": "467\n6.1\nHigh-Level Abstraction of\nGame World Synchronization\nHyun-jik Baeb\nO\nne of the important roles of networked gaming hosts is communicating with\nother hosts to maintain game world synchronization, which involves keeping the\ngame worlds in the same states on all hosts around the world. Synchronization of the\ngame world across multiple hosts requires that game programmers write code that:\n• Collects changes occurring on the local host\n• Packs the changes into one or more messages\n• Transmits the messages to remote hosts\n• Applies the messages to the game world states of the remote hosts\nWriting code for these tasks can be simplified by techniques such as the Remote\nProcedure Call (RPC) system [HyunJik04]. RPC sends or receives messages for the\ncost of writing only one line of code for each message type. However, you still have to\nmanually write routines that manage the game world state, gather information to syn-\nchronize, and send and process it. This work grows quickly if your game designer has\ndeveloped hundreds of diverse battle units that cannot be easily generalized within\nyour program architecture. \nThe power of meta-programming [Wikipedia07] increases productivity over\nwriting code manually. RPC is, of course, a kind of meta-programming technique.\nThis gem introduces another meta-programming technique that synchronizes the\ngame worlds using High Level Abstraction (HLA). RPC abstracts source code lines\nthat exchange messages among hosts in a few lines in the lower code layer; however,\nHLA abstracts them in a higher layer, where the messages are exchanged for synchro-\nnizing the game world state, which is why it’s called high-level abstraction.\nRaw memory synchronization techniques also allow game world synchronization.\nHowever, they are lacking in some aspects:\n• Actual working multiplayer gaming requires latency hiding techniques such as dead\nreckoning [Aronson97]. Synchronizing raw memory has no way of doing this.\n• Raw memory synchronization requires game world data to be stored in a block. It\nis difficult in a situation where automatic memory managers or garbage collectors\nare used.\n",
      "content_length": 2135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 501,
      "chapter": null,
      "content": "468\nSection 6\nNetworking and Multiplayer \n• Not every last byte of data has to be synchronized precisely in actual multiplayer\ngaming worlds. For example, a unit located far from the viewport might not\nrequire full precision synchronization.\nIn an HLA world, game world synchronization can be done by declaring object\ntypes and synchronization behavior for each of them, instead of writing code that\nsends or receives messages. The actual code is automatically generated by the source\ncode generator provided in this gem.\nThis gem discusses an HLA usage case and explains the overall system of the\ngame world synchronization, and then constructs an HLA system.\nHLA Usage\nThe goal of HLA is to offer a feasible method for abstracting game world synchro-\nnization. It is composed of object type definitions, their synchronization behaviors,\nand a facility that determines the visibility of each object.\nThe definitions for synchronized objects are stored in a source file in a grammar\nyou define. You can name it the SWD (Synchronized World Definition) file. It will be\ncompiled to several source files and then built within your project files.\nThe facility that determines synchronization range will actually be a function.\nYou will be able to extend it differently, as you wish.\nAnatomy of Game World Synchronization\nBecause this example involves writing your own HLA infrastructure, there’s no limita-\ntion when you adopt the HLA technique to your game project. This gem assumes\nclient/server topology, which can be explained like this:\n• The game hosts are composed of one server and the other clients. The server owns\nall game world objects and takes control of them.\n• One or more messages are sent from the clients to the server when a change of\ngame world occurs in a client. Then they are applied to server’s game world and\nbroadcast to other clients for updating.\n• Messages are sent from a server to the clients when a change of game world occurs\nin a server. The clients receive them and update based on the changes.\nFigure 6.1.1 illustrates this collaboration.\nYou can categorize the changes in the game world state. These are the conditions\nfor sending messages:\n• Value modification of an object\n• Creation of an object\n• Destruction of an object\n• Appearance or disappearance of an object, discussed later\n• Every time interval\n",
      "content_length": 2342,
      "extraction_method": "Direct"
    },
    {
      "page_number": 502,
      "chapter": null,
      "content": "The condition every time interval is needed when data changes are frequent, but\nevery change is not necessarily propagated. A good example of this is a character’s\nposition. These kinds of changes can be announced by way of an unreliable messaging\nprotocol such as User Datagram Protocol (UDP).\nIn many actual game products, not every object is synchronized for every remote\nhost due to suffocation of network traffic bandwidth. This is critical to a massive mul-\ntiplayer game, where every client holds only a very small area of the game world state,\nwhile their server holds all of it. (The server-side game world is even incomplete on\nany single server if the server system consists of distributed processes.) The synchro-\nnization range every host occupies is determined by rules that are unique to every\ngame project.\nFigure 6.1.2 shows an example that culls the synchronization by a circle defined\nby a radius from the center of each observer. One circle reflects a viewport of a host\nand each star represents an object to synchronize. After an object outside two view-\nports goes into a viewport or a viewport approaches it and envelopes it, the host of the\nviewport gets the message “a new object has appeared” and the host creates an object\nin its game world state. In contrast, when the object leaves a viewport by moving the\nviewport or the object, the “disappear” message arrives to the appropriate host.\nChanges that cause corruption of the game world must be prohibited. For exam-\nple, no one wants his or her loving avatar to be unwillingly moved by opposing forces.\nYou can classify kinds of permissions, as shown in Table 6.1.1.\nTable 6.1.1\nPermissions of World State Modification\nChange by Server \nChange by Local \nChange by Remote \nIs Permitted\nHost Is Permitted\nHost Is Permitted\nServer-only\nYes\nNo\nNo\nServer-and-local-only\nYes\nYes\nNo\nEveryone\nYes\nYes\nNo\n6.1\nHigh-Level Abstraction of Game World Synchronization\n469\nFIGURE 6.1.1\nHLA collaboration diagram.\n",
      "content_length": 1976,
      "extraction_method": "Direct"
    },
    {
      "page_number": 503,
      "chapter": null,
      "content": "When the change arrives at the receiver, the game world is not updated with the\nexact data in the change, but rather updated in an interpolated manner. One of the\nfavorite techniques for doing this is dead reckoning [Aronson97].\nNow that you’ve put this world synchronization logic in order, you can imple-\nment the HLA infrastructure that follows it. This is an example of synchronizing\nworld state, so you might want to design your own HLA infrastructure by determin-\ning what synchronization system your game project requires.\nHLA Components\nThe HLA system consists of an SWD compiler and an HLA runtime, as well as the\nSWD files. The grammar of the SWD file depends on which factors are defined as\nimportant for the synchronized objects. The SWD file discussed in this gem has these\nfactors:\n• Object types, AKA classes\n• The classes have, of course, member variables\n• The variables have synchronization behaviors\nNow you can define the major portion of SWD grammar in a simplified BNF\nform, as in Listing 6.1.1. (Note that the symbols and keywords are omitted.)\nListing 6.1.1\nPseudo-Grammar of an SWD File\ncompilation_unit  :=  (first_id,class*)\nclass := (name,member*)\nmember := (behavior,type,name)\nbehavior := (behavior_selection,additional_attribute)\n470\nSection 6\nNetworking and Multiplayer \nFIGURE 6.1.2\nViewports and objects.\n",
      "content_length": 1339,
      "extraction_method": "Direct"
    },
    {
      "page_number": 504,
      "chapter": null,
      "content": "The grammar definition compilation_unit is the entry point of parsing.\nListing 6.1.2 is an example of the SWD file that follows the grammar in Listing\n6.1.1. The keywords conditional, periodic, and so on are explained later. \nListing 6.1.2\nAn Example of an SWD File\nworld MedivalWorld\n{\nsynch_class Knight\n{\nconditional float Life;\nperiodic(interval=0.2,duration=1) int MotionState;\nperiodic(interval=0.2,duration=1) float rotationY;\ndead_reckon Vector3 Position,Velocity;\nconditional int Type;\nstatic ItemList Inventory;\n}\nsynch_class Mountain\n{\nconditional int Type;\n// No mountain moves, of course. \nconditional Vector3 Position;\n}\n}\nThe code generated by the SWD compiler does the following:\n• Manages the synchronized objects and collects any changes to them (creation and\ndestruction of objects or member variable changes)\n• Converts the changes to messages and sends them to the networking layer\n• Receives messages from the networking layer and processes them\nThe code generated by the SWD compiler should do everything for world synchro-\nnization in an ideal situation. However, this is inefficient in the practical programming\nworld, when a small change to the HLA source code is needed. So, let’s drive much of\nHLA infrastructure into a common library.\nNow you might be able to imagine how the HLA system fits into program’s archi-\ntecture. This is shown in Figure 6.1.3.\nThe recommended way of compiling an SWD file is putting it into the custom\nbuild configuration, which was introduced in [HyunJik04].\nThe Synchronized Object\nLet’s call the synchronized object SynchEntity for avoiding ambiguity with the term\nobject. A SynchEntity is one of the classes defined in an SWD file.\nA SynchEntity is an ordinary class in practice; however, it has more attributes and\nbehaviors, which are explained next.\n6.1\nHigh-Level Abstraction of Game World Synchronization\n471\n",
      "content_length": 1874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 505,
      "chapter": null,
      "content": "A SynchEntity exists as the original or the replica, depending on which host has\nthe ownership (and full permission to modify any values) of it. The host that has own-\nership is the subject of the SynchEntity. So SynchEntity has an attribute subject.\nEach object identified across multiple network hosts must be unique. So every\nSynchEntity instance will have a unique identifier value, which is issued by the server.\nEvery member variable in a SynchEntity is actually a property member. The prop-\nerty member consists of a set/get function pair and an alias declaration that binds the\ntwo functions into a virtual member variable. Many contemporary compilers support\nproperty features such as the __declspec(property) keyword in Visual C++. You can\nalso work around this feature’s absence by using a casting operator and an assign oper-\nator even if your compiler doesn’t support the property feature. Listings 6.1.3 and\n6.1.4 show these two cases.\nListing 6.1.3\nUsing the __property Keyword\nclass MyClass\n{\npublic:\n472\nSection 6\nNetworking and Multiplayer \nFIGURE 6.1.3\nHLA activities within a program’s architecture.\n",
      "content_length": 1120,
      "extraction_method": "Direct"
    },
    {
      "page_number": 506,
      "chapter": null,
      "content": "_ _declspec(property(get=getX,put=setX)) int X; \nvoid setX(int);\nint getX();\n};\nListing 6.1.4\nUsing a Casting Operator and an Assign Operator\nclass XType\n{\npublic:\nXType();\nXType(int value); // takes a value into self\nXType& operator=(int value); // takes a value into self\noperator int(); // outputs the internal value\n};\nclass MyClass\n{\npublic:\nXType X;\n};\nThe synchronization behavior for each member variable can be defined in an\nSWD file. The SWD compiler then generates appropriate source code depending on\nwhich behavior is defined for each member variable. Some of the code may monitor\nto see if any changes are made to the variable. You can get better performance by sub-\nstituting it with code similar to Listing 6.1.5, which can help to quickly skip compar-\nisons when there are no changes.\nListing 6.1.5\nFlagging a Variable as Changed While Assigning a Value\nvoid SetXXX(int newVal)\n{\nm_maybeChanged=true;\nm_value=newVal;\n}\nThe synchronization behavior to be bound to a SynchEntity member variable is\ntypically one of static, conditional, periodic, or dead reckoning. \nStatic behavior means it is never synchronized. If there were no the static behavior,\nyou should define a class derived from the SynchEntity just for adding member vari-\nables that don’t have to be synchronized. \nConditional behavior means that the value is synchronized when its value changes.\nThis is the most commonly used behavior; however, it can flood network traffic if the\nvalue changes are too frequent. \nPeriodic behavior resolves the potential problems with conditional behavior by\nsending the value at specified intervals. This behavior needs send interval value and\n6.1\nHigh-Level Abstraction of Game World Synchronization\n473\n",
      "content_length": 1721,
      "extraction_method": "Direct"
    },
    {
      "page_number": 507,
      "chapter": null,
      "content": "send duration. If the value of a periodic behavior member variable changes, it will be\nsent to remote hosts in the interval of send interval value until the send duration time\nelapses. Assuming, for example, that you set the send interval to 0.2 second and the\nduration to 1 second for a periodic behavioral variable, the value will be sent to\nremote hosts five times every 0.2 seconds. Periodic behavior is typically used together\nwith unreliable messaging protocols such as UDP.\nListing 6.1.6 is an example of a conditional behavioral member variable that is\nused in the SWD file, whereas Listing 6.1.7 shows its compiled code. \nListing 6.1.6\nAn Example Conditional Behavioral Member Variable Used in an SWD File\nsynch_class Knight\n{\nconditional int life;\n<...and more...>\n}\nListing 6.1.7\nGenerated Code for the Conditional Behavioral Member Variable\nclass Knight\n{\nprivate:\nint m_private_life;\nbool m_private_life_changed;\ninline void set_life(int value)\n{\nif(value!=m_private_life)\n{\n// A variable whose *_changed \n// is true will be broadcasted soon.\nm_private_life_changed=true;\nm_private_life=value;\n}\n}\ninline int get_life(int value)\n{\nreturn m_private_life;\n}\npublic:\n__declspec(property(get=get_life,put=set_life)) int life;\n<...and more...>\n};\nDead reckoning behavior allows you to hide the jittering values that occur due to\nnetwork latency. A simple dead reckoning model involves three variables to reference:\nthe actual value of the sender, the predicted value of the receiver side, and the interpo-\nlated value. So the SWD compiler should generate these three variables for each dead\nreckoning behavioral variable.\n474\nSection 6\nNetworking and Multiplayer \n",
      "content_length": 1672,
      "extraction_method": "Direct"
    },
    {
      "page_number": 508,
      "chapter": null,
      "content": "The flag that indicates whether the value is changed (m_private_life_changed in\nListing 6.1.7) is then used for collecting change information from the game world.\nOne simple model is to iterate over each SynchEntity and gather the changed ones by\nreading the flag. Because the HLA runtime itself cannot know what the flag values\nare, the iteration routine should be generated by the SWD compiler. Listing 6.1.8\nshows an example for the variable in Listing 6.1.6.\nListing 6.1.8\nGenerated Code That Identifies the Change and Collects It to the Output\nMessage Object\nclass MedivalWorld_Runtime\n{\npublic:\nvoid GatherTheChangeToMessage(SynchEntity* entity,\nCMessage &outputMessage)\n{\n// the identifier SynchEntity_Knight is\n// generated enumeration value from the SWD compiler.\nif(entity->GetType()==SynchEntity_Knight)\n{\nKnight* typedEntity=(Knight*)entity;\nif(typedEntity->m_private_life_changed)\n{\noutputMessage.Write(typedEntity->m_private_life);\ntypedEntity->m_private_life_changed=false;\n}\n<...and more...>\n}\n}\n};\nOne more part to investigate is the routine that receives messages from other\nHLA runtimes and applies them to the local game world. This task, which is called\ndeserialization, is mentioned in [HyunJik04].\nCommunication Between HLA Runtimes\nThe major cases during world synchronization that were classified here are SynchEn-\ntity creation, destruction, appearance, disappearance, and value change. Each of these\ncases corresponds to a messaging sequence.\nAlmost all SynchEntities are created only after the server decides that a creation is\nnecessary (that is, creating the object in the server side at first) and its event is broad-\ncast to the clients. Then the received client creates the replica of the new SynchEntity\nafter receiving the message. The required parameters for creating the SynchEntity are\nits ID and its initial member variable values. These values are serialized to a message\nand then sent to the clients that need to know about the newly created SynchEntity.\nSynchEntities that are trivial in presence but sensitive in performance (machine\ngun projectiles, for example) can be created by the client side even if the server does\n6.1\nHigh-Level Abstraction of Game World Synchronization\n475\n",
      "content_length": 2226,
      "extraction_method": "Direct"
    },
    {
      "page_number": 509,
      "chapter": null,
      "content": "not permit it yet. In this case, the client first creates it and notifies the server, and then\nthe rest is the same as before. The identifier value of a SynchEntity that’s created client\nside always exists in a value range that has been issued by the server when the client\njoined the game world. [Yongha06] shows more details about doing this.\nThe destruction of a SynchEntity is similar to the creation case, except for the fact\nthat the message type has only the ID of the destructed SynchEntity. A SynchEntity is\ndestroyed at the server, the server sends the event to the clients that view the object,\nand the clients also destroy the replica. The additional sequence needed for a trivial\nSynchEntity is a client-side decision to destroy the entity, at which point the server is\nnotified.\nAll changes in the SynchEntity variables are collected and sent to the clients that\npossess replicas. Messages containing these changes have the SynchEntity ID and a list\nof changed values with their variable ID numbers. Then, each of the clients receives\nthese messages and applies the changes to its replicas.\nConsider one more case: the client first decides to change and announces it to the\nserver, but only if it is trivial enough that a client has permissions to call for the mod-\nifications or the subject of the SynchEntity is the client.\nThe visibility of every SynchEntity can be changed as time goes on because its\nposition or the position of each viewer changes. If one SynchEntity enters a viewport,\nthe client that owns the viewport creates the replica of the SynchEntity after the server\nsends the appearance message with the SynchEntity ID and its serialized values. In\ncontrast, the disappearance message with the SynchEntity ID is received at the client\nand then it removes the corresponding replica. \nViewports in HLA Runtime\nThe viewport in HLA runtime maintains the current state (position and such) as well\nas a network host identifier for sending or receiving messages for synchronization.\nTypically a viewport has a camera position (or more, depending on what radar the\nplayer has) and a host identifier value. SynchEntity and the base class of viewport\nSynchViewport are both abstracted classes.\nA simple implementation of the entity-viewport visibility check is calling a func-\ntion that takes two parameters: a SynchEntity and a SynchViewport. This function is\nnormally called N \u0002 M times, where N is the number of all SynchEntity instances\nand M is the number of all SynchViewport instances. You may want to implement the\nfunction to meet your own needs. For example, your method could be based upon\ngeographical range, parent-child relationship of each scene graph node, or portal par-\ntition of BSP/PVS. The prototype of this function is shown in Listing 6.1.9.\nListing 6.1.9\nA Function for Entity-Viewport Visibility Determination\nbool IsOneEntityVisibleToOneViewport(SynchViewport *viewport,\nSynchEntity* SynchEntity);\n476\nSection 6\nNetworking and Multiplayer \n",
      "content_length": 2986,
      "extraction_method": "Direct"
    },
    {
      "page_number": 510,
      "chapter": null,
      "content": "The client/server topology discussed here allows this functionality to be on the\nserver. So this function exists only on the server side. \nHLA Event Handlers\nYou may need to handle something at the exact time when the world state changes.\nExamples of this are the appear and disappear events of a SynchEntity. These cases are\nuseful for loading just-in-time (JIT) resource files for a character type, for example.\nYou can add these event handler interfaces without any limitation because you are\nusing your own HLA system. You just inject these event handler prototypes and the\ninvoker code into the HLA compiler or HLA runtime source lines.\nConstruction of HLA Runtime\nThe HLA runtime fits in with the structure of what you’ve investigated so far. Keep-\ning that in mind, the HLA runtime’s design is shown in Figure 6.1.4.\n6.1\nHigh-Level Abstraction of Game World Synchronization\n477\nFIGURE 6.1.4\nUML class diagram of major classes of HLA system.\n",
      "content_length": 948,
      "extraction_method": "Direct"
    },
    {
      "page_number": 511,
      "chapter": null,
      "content": "HlaServer has these features:\n•\nHlaServer has every instance of SynchEntity_S-derived objects and SynchView-\nport-derived objects as well as an entity-viewport visibility decision maker. (Note\nthat _C and _S postfixes stand for server and client.) \n•\nHlaServer monitors the state of every SynchEntity_S and SynchViewport\ninstance. If a change is detected, HlaServer serializes the changes into several mes-\nsages and sends them to the remote hosts.\n•\nHlaServer interfaces with a networking engine to send or receive messages related\nto world synchronization. \nHlaClient keeps instances of SynchEntity_C replicated from the server. Like\nHlaServer, it also interfaces with the networking engine and has routines for keeping\nthe state of every SynchEntity_C synchronized with the server.\nKnight_S and Knight_C are generated classes from an example class Knight in the\nSWD file.\nClass Knight_C and class Knight_S have members, each of whose type is one of\nthe classes DeadReckonBehavior, ConditionalBehavior, and PeriodicBehavior. These\nclasses help HlaClient and HlaServer determine whether these member variables\nshould be broadcasted. The code in Listing 6.1.7 can become more concise if it uses\nConditionalBehavior class.\nFurther Issues\nThe implementation of the HLA system in this article is just a simple networking\nmodel focused on ease of reading and discussion. These features are worth extending\nbased on the HLA system in this gem:\n• Besides the conditional, periodic, static, and dead reckoning behaviors, there are\nmore models for synchronization. For example, synchronization based on time-\nstamp value. \n• The SynchEntity types discussed so far have no member functions. They could be\nadded to the HLA system by sending event messages to the remote host. There\nare two invocation behaviors—running the member functions only on a host that\nhas the original (this can be in an object-oriented remote procedure call manner),\nor on every host that has the original or replica. This may be specified where the\ninvocation begins or pre-specified in the SWD file.\n• Duplicated definitions in similar classes could be refactored into common\nobjects. This also applies to the SWD files. \n• Optimization of comparison bottlenecks may be helpful for better performance.\nThe HLA in this gem checks visibility for every SynchEntity and every\nSynchViewport, which then results in O(n2) time complexity. You could cull\nsome of them by adding a Boolean variable called “this object is changed” to the\nSynchEntity and SynchViewport classes and use it before the actual comparison.\n478\nSection 6\nNetworking and Multiplayer \n",
      "content_length": 2617,
      "extraction_method": "Direct"
    },
    {
      "page_number": 512,
      "chapter": null,
      "content": "Conclusion\nIf you find yourself writing a lot of similar code to keep your game world synchro-\nnized, implementing your own High Level Abstraction (HLA) system based on this\ndesign can greatly ease your subsequent efforts at game world synchronization. The\nHLA system introduced in this gem can be a guide for the first step as you write your\nown HLA system.\nReferences\n[Aronson97] Aronson. “Dead Reckoning: Latency Hiding for Networked Games,”\navailable online at http://www.gamasutra.com/features/19970919/aronson_01.htm.\n[HyunJik04] Bae, Hyun-jik. “Fast and Efficient Implementation of a Remote Proce-\ndure Call System,” Game Programming Gems 5, edited by Kim Pallister, Charles\nRiver Media, 2005, pp. 627-642.\n[Wikipedia07] “Meta-Programming,” available online at http://en.wikipedia.org/\nwiki/Meta_programming.\n[Yongha06] Kim, Yongha. “Generating Globally Unique Identifiers for Game Objects,”\nGame Programming Gems 6, edited by Michael Dickheiser, Charles River Media,\n2006, pp. 623-628.\n6.1\nHigh-Level Abstraction of Game World Synchronization\n479\n",
      "content_length": 1055,
      "extraction_method": "Direct"
    },
    {
      "page_number": 513,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 514,
      "chapter": null,
      "content": "481\n6.2\nAuthentication for \nOnline Games\nJon Watte\ngpg-7@mindcontrol.org\nA\nuthentication for games, where un-trusted clients connect to one or more trusted\nservers, is an interesting special case of general authentication. This article pre-\nsents some alternatives that you should consider when designing authentication for an\nonline game system, and proposes one particular set of internally cohesive design\nchoices.\nIntroduction\nAuthentication is the process of making sure that someone is who they say they are, and\nby extension, that a given communication comes from a given party. For computer\ngames, this comes in two flavors: \n• Game login—Given credentials (a user name and password) match the informa-\ntion against a database of allowed players.\n• Game session—A network packet was sent by the logged in player it says it came\nfrom.\nNote that authentication, consisting of the ability to determine who sent a spe-\ncific message, does not have much to do with encryption, which is the ability to hide\na message from unintended recipients. The one exception is that one kind of cryp-\ntosystem (public/private key systems) has the ability to provide both functions at\nonce. Unfortunately, these cryptosystems are usually computationally expensive, and\nthus are not a great match for real-time, online services like computer games.\nSecuring Game Logins\nTo secure game logins, you need to worry about a few kinds of problems: \n• Insecure passwords—Players may have a password that is a common word (like\n“secret”), the player’s name, or even a blank. Your password setting mechanism\nshould detect weak passwords and require better ones.\n",
      "content_length": 1641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 515,
      "chapter": null,
      "content": "• Insecure password storage—Are your servers secure? Someone might break into\nthem. If they can read the password in clear text at that point, that’s a problem.\nAlso, are the operators of your system trustworthy? What if you have to lay them\noff or fire one?\n• Sniffed passwords—If you don’t use Secure Sockets Layer or some similar heavy-\nweight encrypted protocol, it’s possible that someone can use a packet sniffer to\nread a password sent in clear text, and then impersonate the user in question.\nAlthough this kind of attack is rare, it has actually happened, typically as part of a\npartial data center compromise.\n• Keyboard sniffers—Some kinds of malware or Trojan programs will install them-\nselves on users’ computers, and then log all the keystrokes that the users make.\nSomeone familiar with the game in question can quickly deduce the login name\nand password used from reading such a log.\n• Uneducated users—In many online games, there are users who will try to get the\naccount name and password directly from communication with other players.\nOnce these “keys” are obtained, the account is typically plundered of any valuable\nvirtual goods, and the password is changed to something random, so the original\nuser can no longer play.\n• Multiple logins—The system should not allow the same user to log in more than\nonce at the same time. Otherwise, a single player will pay for the game and share\nthe login with all his or her friends. Although this is a small bit of lost revenue,\nthe bigger problem comes when you have to ban the account because some of\nthose “friends” didn’t play by the rules. That kind of situation is a customer ser-\nvice nightmare.\nThe main point of this gem is to examine authentication in your client/server\ndesign a little closer.\nOne tradeoff you have to make is whether you want the passwords to be recover-\nable from the database or not. Depending on this choice, you have the following\noptions:\n• Recoverable passwords—If the password is recoverable, you can use the Challenge\nHash Authentication scheme, as described later. However, the passwords are more\nvulnerable when they are recoverable in the database, because an untrustworthy\noperator, or system intruder, might get hold of the list of passwords.\nDon’t store the passwords in clear text in the database. At least scramble them\nusing some key that you build into the code, to make it harder for the casual\ninspector to “accidentally” see the password. There’s still a danger that the pass-\nwords can be compromised, so be vigilant against human factors that can com-\npromise your data.\n• One-way hash passwords—When setting a password, you calculate a one-way hash\nof the password (such as an SHA256 checksum), and store the checksum. When\nusers log in the next time, they give you a password, and you calculate an SHA256\n482\nSection 6\nNetworking and Multiplayer \n",
      "content_length": 2863,
      "extraction_method": "Direct"
    },
    {
      "page_number": 516,
      "chapter": null,
      "content": "checksum of that, and compare to the stored checksum. If they match, you assume\nthe password provided is correct.\nThe main benefit here is security on the system side; reading an SHA256\nchecksum will not let anyone know what the actual password is. Finding another\npassword that generates the same checksum is computationally very hard. How-\never, when doing this, you must use Secret Exchange Authentication (described\nlater), which is more vulnerable than Challenge Hash Authentication.\n• Public key infrastructure—If you have a private/public key cryptosystem, such as\nRSA or SSL, you can publish the public key of the game servers, and even hard-\ncode it into the game client executable. The user’s password is then transmitted\nover this link, safe from eavesdropping. The added benefit is that nobody but the\nauthentic server can decrypt the message, so the client has a reasonable assurance\nthat it is talking to the real server, not an impostor. The drawback is significant\ncomplexity in implementation (the best choice here is probably to go with an\nopen cryptosystem library such as OpenSLL).\nChallenge Hash Authentication\nIn Challenge Hash Authentication, the server issues some random number, called a\nchallenge or nonce, to the client. The client computes a hash of this random number\nand the client-side entered password, and sends the hash value back to the server. The\nserver then computes a hash of the remembered challenge value and the stored (plain-\ntext) password, and compares it to what the client submitted. If the hashes match, the\nright password was supplied.\nThere are three main properties of this system: \n• Passwords are not transmitted—Thus someone sniffing the regular login traffic\ncannot determine what the password is.\n• The challenge is specific to each login attempt—Thus, if you sniff the connection,\nyou cannot remember what the hash is and then just re-supply the same hash\nvalue later to log in, because the random challenge generated by the server for a\nspecific login attempt is different each time.\n• The server has the clear-text password—This is a security problem if the server side\nbecomes compromised, but the clear-text password, which is a secret shared by\nboth sides of the communication, can be used to encrypt any data coming\nto/from that particular client. Care has to be taken to use an encryption algo-\nrithm from which the key cannot be too easily recovered—XOR or ROT-13\nwould not be appropriate!\nA common-sense precaution is to use a hash of the clear-text password as a key\nfor the communication, but not the same hash as used for authentication, or the\nbenefit of an “unsniffable” shared secret is lost.\n6.2\nAuthentication for Online Games\n483\n",
      "content_length": 2705,
      "extraction_method": "Direct"
    },
    {
      "page_number": 517,
      "chapter": null,
      "content": "Secret Exchange Authentication\nIn Secret Exchange Authentication, the server stores a hash of the password. The client\nsubmits a plain-text password, and the server hashes this plain-text password, and com-\npares the hash to the stored hash. If they match, the right password was supplied.\nThere is one strength and two weaknesses in this system: \n• The server doesn’t store the plain-text password—If someone breaks in and steals the\npassword file, it doesn’t matter, because you can’t guess what a password is just by\nknowing its (cryptographic strength) hash. On old UNIX machines, the strength of\nthe cryptography is not that high, so you should still keep your /etc/shadow file\nsecure, but with a 256-bit SHA hash, you should be pretty safe. If you can’t trust\nyour backup operators, or if you get hacked, this is a major benefit!\n• The password is transmitted on each login attempt—If someone can sniff the connec-\ntion, they could recover the password. Thus, you have to secure the login attempt\nusing some kind of encryption—but it’s not clear what you should use as a key to\nachieve good security. The most secure way involves a Diffie-Hellman key exchange,\nwhich is fairly tricky code to implement correctly, but will provide for a secure,\nencrypted channel between two endpoints, without prior exchange of keys. If you\nwanted to protect against a sophisticated attacker inserting himself in the middle of\nthe network, you would additionally have to introduce a public key–based crypto-\ngraphic authentication system, which is a significant additional burden.\n• The server has the clear-text password—Because the client sends the clear-text pass-\nword, the server has at least temporary access to the clear-text password, and can\nuse this as a key for future communication encryption, after the initial login.\nUnfortunately, this means that if someone can impersonate your server, or read\nthe memory of your server process, they can still recover plain-text passwords,\neven if the password storage file itself is secure.\nPublic Key Infrastructure\nIf you have a private/public key cryptosystem, such as RSA or SSL, you can publish the\npublic key of the game servers, or even hard-code it into the game client executable.\nThe user’s credentials are then transmitted over this link, safe from eavesdropping on\nthe wire. An additional option is to generate a private key for the user when setting up\nthe account, storing the matching public key on the server side, and encrypting the pri-\nvate key locally with the user’s password (known as a pass phrase).\nSuch a system has the following properties:\n• The server never sees the pass phrase—Thus, disgruntled employees or server system\nintruders cannot easily steal the credentials through packet sniffing or log skim-\nming. A determined attacker who disassembles the server binary can still get the\ncredentials, but at that point, your entire game is compromised, and you proba-\nbly have bigger problems to worry about.\n484\nSection 6\nNetworking and Multiplayer \n",
      "content_length": 3019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 518,
      "chapter": null,
      "content": "• The user has a good assurance against server impersonation—As long as the server\nprivate key is not compromised, nobody else can pretend to be your server and\nextract user credentials.\n• The user credentials are not portable—If the user accesses your game from more\nthan one location, he or she needs to make a copy of the private key used for his\nor her game account, so that the client can authenticate itself on logon. This is\nnot something users generally expect, and will likely lead to a customer support\nheadache.\nSecuring Game Sessions\nOnce the player has logged in, your troubles are not over. You often need to transfer a\nplayer from one machine to another, or to allow the player to disconnect from the\nserver (perhaps through crashing) and then re-connect, resuming where the player left\noff. You clearly can’t just let the client claim any identity, and have the server blindly\ntrust that, because it would be trivial for one player to suddenly impersonate another\nplayer. Instead, you have to use one of three techniques: identity by IP address, iden-\ntity by authentication token, or identity by cryptography.\nIdentity by IP Address\nIn this method, the server looks at the source IP address and port number of the arriv-\ning packet, and internally has a table that tells which player is connected on which\naddress/port pair. This is secure, as long as you know that the player will keep sending\nfrom the same port, and as long as you trust that the Internet will not accept spoofed\naddresses in packets—or, if a packet is spoofed, that some round-trip confirmation\nwith the real client can take place.\nSuch round-trip confirmation can come in the form of explicit acknowledgement\nof particularly suspect commands (“surrender game,” for example), or implicitly by\nusing a rotating sequence number starting from a random initial starting point.\nSadly, if you use TCP for your connections, or if you need to hand connections\noff between servers, the port part of the client’s address will not necessarily stay the\nsame. TCP allocates a new port for each connection for each machine it connects to,\nand even UDP can suffer port renumbering when you switch destination machines, if\nit’s behind a non-friendly NAT gateway (although most home NAT routers don’t\nimpose this limitation). \nIdentity by Authentication Token\nWhen the player logs in, the server determines the duration for which the connection\nis good—for example, one hour. The server then calculates a hash of a few pieces of\ndata: the client ID, the expiration time of the login session, and a secret number that\nonly the game server knows. The server then sends a token to the client, which con-\ntains the client ID, the expiration time, and the hash of the three pieces of data.\n6.2\nAuthentication for Online Games\n485\n",
      "content_length": 2795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 519,
      "chapter": null,
      "content": "When the client sends data, it precedes the data with this token. The server picks\napart the identity, time, and hash parts, and recomputes the hash with its internal\nsecret number. If the hash matches the hash in the supplied token, the server knows\nthat the packet comes from the player who initially authenticated with the server, or\nat least that the claimed identity and session duration is one that the server has signed\noff on.\nIf the client crashes and then reconnects, it could read the cookie from disk and\nre-supply it, and as long as the session is still valid, no new authentication would be\nnecessary. If game sessions last more than an hour, the server that the player is cur-\nrently talking to would extend the cookie by half an hour each time the cookie is at\nleast half an hour old by re-generating a new token based on client ID, new expiration\ntime, and a hash of those entities and the server secret number. That way, a client can\ncrash and then keep playing as long as it reconnects within half an hour, without hav-\ning to log in again.\nIdentity by Cryptography\nIf you use a shared secret between the server and the client, such as a plain-text pass-\nword, you can use that secret as a key, or perhaps better, a hash of that password and\nsome known salt or nonce different from that used to authenticate the connection ini-\ntially. Each packet sent by the client contains the client ID in plain text, followed by\nthe packet data, encrypted by the shared secret, followed by a checksum of the (unen-\ncrypted) data.\nWhen the server receives a packet, it looks up the client password in an internal\ntable, decrypts the message, and verifies the checksum. If the checksum doesn’t match,\nthe data was not encrypted with the right password, and thus the packet did not come\nfrom the right client.\nBest practice says that part of the encrypted data should be a sequence number, so\nthat successive identical packets will still encrypt differently, and so that capturing and\nreplaying a packet will have a low likelihood of being accepted for real.\nOther Considerations with Game Sessions\nThe other problems mentioned in part two of this gem also bear mentioning, although\nthe solutions aren’t spelled out in as much detail as with the main topic of the article:\n• Insecure passwords—When the player generates or changes a password, you should\nverify that the password contains at least six characters (and allow up to 24). Addi-\ntionally, verify that the password contains at least one character from each of the\nthree groups—letters, digits, and non-alpha-numeric characters.\n• Insecure password storage—To protect server secrets against malicious internal opera-\ntors, follow best IT practices. Don’t let anyone in the company have access to all the\nservers. Store any plain-text password data in a scrambled format, using some key\nthat’s hard-coded into the executable. Store extra sensitive data, such as credit card\n486\nSection 6\nNetworking and Multiplayer \n",
      "content_length": 2978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 520,
      "chapter": null,
      "content": "information or user home addresses, in a server separate from the main game\nservers, with an additional firewall between game and billing information.\n• Keyboard sniffers—If you worry about keyboard sniffers, make the users enter\ntheir passwords using an on-screen keyboard (point-and-click) instead of using\nthe keyboard. Also beware that a malicious piece of software could read out all\ndata in standard text edit controls, so you might want to use a custom GUI con-\ntrol for reading passwords.\n• Uneducated users—Create a comprehensive set of rules for user conduct and safety,\nand require acceptance when users sign up. However, make sure you boil down the\nmost important bits into quick sound bites like “never give out your password, even\nif someone says they are from our company.” Add one of those sound bites to each\nloading screen, perhaps on a rotating basis, to reinforce the message.\n• Multiple logins—When one session ticket or cookie is generated, invalidate all\nprevious such tickets/cookies. This means that a second login on the same\naccount will kick out the first logged-in user. However, if a user disconnects and\nlogs in again, that user will not be affected, because the old session ticket is no\nlonger used.\nConclusion\nIf you are reading this, it’s a good sign—you care about security and want to do it right!\nA good encryption algorithm to use when both sides know the key (such as when using\nsecret exchange authentication and identity by cryptography identification) is the Tiny\nEncryption Algorithm, which is easy to implement, yet cryptographically strong. True\nsticklers for security recommend only using standardized protocols, such as AES, because\nthey undergo more study and publication, and any weaknesses will thus be known\nsooner and wider, giving you early warning when it is time to change cryptosystems.\nSHA256 is a commonly used and standardized hashing (digest) function, and has\nnot yet shown the weaknesses of the older MD5 hashing algorithm. Other alterna-\ntives are available, such as Tiger (see http://www.cs.technion.ac.il/~biham/Reports/\nTiger/tiger/tiger.html).\nA sufficient implementation of authentication and identity for a cluster of collab-\norating trusted servers (such as for an MMORPG or Virtual World) would look\nsomething like this:\n• At setup, all servers in the cluster share a large random number, known as the\ncluster secret.\n• Client connects to login server using unencrypted TCP or UDP.\n• The server issues a challenge to the client, consisting of a 256-bit random num-\nber (nonce).\n• Client calculates a hash of this number concatenated with the password the user\nenters, and supplies the hash to the server.\n6.2\nAuthentication for Online Games\n487\n",
      "content_length": 2716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 521,
      "chapter": null,
      "content": "• Server verifies that the hash of the challenge and stored password matches what\nthe client supplied, and issues an authentication ticket consisting of a user ID,\nticket expiration time, and hash of a cluster secret combined with these two\nitems, and supplies the ticket to the client.\n• Login server also generates a random key for use by this client during this session,\nand supplies it to the client. It also records the key for the user, and the expiry time\nof the ticket. The key is encrypted by a key generated by hashing the user password\nand a known salt (say, the string “abcd”) before sending it to the client.\n• Client connects to any server that is part of the game server cluster.\n• Client starts the connection to a new server within the cluster by sending the\nauthentication ticket previously issued.\n• The new server verifies that the ticket has not expired, and that the hash is cor-\nrect. Using the user ID in the ticket, the new server retrieves the encryption key\nfor the client from the login server.\n• The new server and the client also negotiate sequence numbers for future com-\nmunications at this point.\n• Once authenticated, the new server and client exchange data encrypted with the\nsession key, where the encrypted data includes a hash of the data proper (as\nchecksum) and a sequence number. Each of these packets needs to have only the\nclient ID and ticket identifier (a small integer) as a header, not the full authenti-\ncation ticket.\n• Periodically, the server that the client is currently connected to checks whether\nthe session authentication ticket is about to expire; if this is the case, it contacts\nthe login server to get a new ticket and forwards it to the client.\nThis scheme will protect against the dangers of someone sniffing your passwords\non the open Internet, and against the dangers of someone trying to use sniffed packets\nin a playback attack. For a man-in-the-middle attack, the session being compromised\nwould be insecure, but the man in the middle would not gain the authentication cre-\ndentials to re-authenticate at a later time. To make sure there is no tampering in the\nmiddle, you would have to add public/private key encryption and authentication.\nIt is also worth noting that no technique protects against a user looking at all the\ndata sent to his or her client machine—the user controls the machine running the\nclient, so he or she could always inspect the data in memory. This means your game\ndesign has to be cheat-proof, or you must provide incentives for users not to cheat, to\nget around that problem. Authentication and encryption save you only from third par-\nties getting hold of secret information, not the two first parties (the client and server).\n488\nSection 6\nNetworking and Multiplayer \n",
      "content_length": 2764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 522,
      "chapter": null,
      "content": "References\nAES, the Advanced Encryption Standard algorithm. The successor to the Data Encryp-\ntion Standard, and the current U.S. Federal Information Processing Standard\nencryption algorithm, available online at http://csrc.nist.gov/CryptoToolkit/aes/\naesfact.html.\nOpenSSL. A high-quality Secure Sockets Layer library, using an Apache-style Open\nSource license, available online at http://www.openssl.org/.\n[Schneier95] Schneier, Bruce. Applied Cryptography: Protocols, Algorithms, and Source\nCode in C, 2nd edition, Wiley, 1995.\nSHA, Secure Hash Algorithm. The successor to MD5, and the current U.S. Federal\nInformation Processing Standard hash/digest function, available online at\nhttp://csrc.nist.gov/publications/fips/fips180-2/fips180-2withchangenotice.pdf.\nTiger. A fast hashing (digest) function, with no known patent encumbrance, available\nonline at http://www.cs.technion.ac.il/~biham/Reports/Tiger/tiger/tiger.html and\nhttp://en.wikipedia.org/wiki/Tiger_(hash).\nX-Tea and Corrected Block TEA. A fast, high-strength, simple-to-implement sym-\nmetric encryption algorithm, with no known patent encumbrance, available\nonline at http://en.wikipedia.org/wiki/XXTEA.\n6.2\nAuthentication for Online Games\n489\n",
      "content_length": 1211,
      "extraction_method": "Direct"
    },
    {
      "page_number": 523,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 524,
      "chapter": null,
      "content": "491\n6.3\nGame Network Debugging\nwith Smart Packet Sniffers\nDavid L. Koenig, The Whole Experience, Inc.\nyarnhammer@hotmail.com\nI\nn general, most network traffic in games is very sensitive to long delays in between\ngame packets. This can be problematic when attempting to debug network code in\nreal-time. The standard practice is to use a packet sniffer application that collects net-\nwork traffic on the network during gameplay so that it can be later analyzed. Packet\nsniffers give easy access to information about packet source and destination, and other\nnetwork protocol stack information. What these do not provide are the specifics of\nyour game protocol when that data goes out over the network. \nThe Smart Packet Sniffer Concept\nThe concept of a smart packet sniffer or, perhaps better-named, game message sniffer,\nis the idea that you are not just looking at the raw binary data sent across the wire, or\nTCP/IP protocol data. You are looking at more detailed information in a human\nreadable form. In its most basic description, this is a packet sniffer that has specific\nknowledge of the internals of your game protocol.\nThis smart packet sniffer application was developed while working on Greg Hast-\nings’ Tournament Paintball Max’d for PlayStation 2 (GHTP), which was released in\nlate 2006. An engineer who was no longer with the company wrote the baseline game\nmessage system and network code. With no documentation provided for the network\ncode and message system, it was certainly overwhelming to start working with it. The\nfirst place to start is to look over the code. Doing so will give you a good idea of the\narchitecture of the underlying system. Exactly what game messages are sent across the\nwire and when can be very difficult to grasp initially. This is where a smart packet\nsniffer can come in handy.\nAn Example\nOn the GHTP project, the sniffer showed us that our server was sending a large num-\nber of 100- and sub-100-byte player position packets to the clients. With 42 bytes of\nthat consisting of Ethernet frame, IP, and UDP header information, our packet\n",
      "content_length": 2080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 525,
      "chapter": null,
      "content": "header overhead was around 40 percent. We were able to improve efficiency by coa-\nlescing our packet data and greatly reducing our overall overhead. By using a standard\npacket sniffer, we probably could have examined the binary data of a number of pack-\nets and come to the same conclusion. It only took a quick glance, with our sniffer, to\nsee exactly what network messages were being sent, and how much bandwidth they\nwere consuming. Needless to say, this saved us a great deal of time.\nGotchas with Traditional Debugging Techniques\nYou don’t want to completely abandon your standard debugging functions when work-\ning with network code. However, you should be aware of the artifacts they can intro-\nduce. What you want to avoid is causing bugs that don’t really exist for end users. This\nis usually the result of changing the code path or changing the code timing. The fol-\nlowing are examples that can result in either of these two issues.\nBreakpoints\nThese are generally the developer’s first line of defense when testing a piece of code for\nvalidity. They allow you to see if an operation is following the expected path. They\nallow you to see the values of important variables and register values. This informa-\ntion is invaluable to a developer. The problem that is introduced when it comes to\nnetwork code is that you are only stopping one side of the simulation. The other side,\nwhich is the other host connected to the game, keeps running. At some point this sec-\nondary host will assume that the connection has dropped and will timeout. Now you\nmay not care about this depending on what type of issues you are trying to debug. If\nyou are just trying to find out if a given piece of code is ever executed, this is a quick\nway to obtain that information. However, if you’re trying to figure out how many\nheartbeat packets are being sent to the server, or which messages are coming out of\norder most frequently, breakpoints quickly lose their potency.\nTracepoints\nThe concept of tracepoints was introduced into Visual Studio with version 8.0, also\nknown as Visual Studio .NET 2005. These allow you to place points in the code that,\nwhen hit, do not necessarily cause the game to halt. You can do all sorts of things. You\ncan choose to halt progress. You can also run scripts, print to the debug window, or\nprint a callstack. Although these are great advances in debugging options, they can\nchange the timing of your code.\nDebug Output\nThis is usually in the form of a call to printf, OutputDebugString, or an equivalent\nfunction. These can be useful for obtaining information such as bandwidth usage per\nsecond, or percentage of packets dropped. The problem is that anytime you add\nadditional code to a given operation, the timing is going to change. With additional\n492\nSection 6\nNetworking and Multiplayer \n",
      "content_length": 2817,
      "extraction_method": "Direct"
    },
    {
      "page_number": 526,
      "chapter": null,
      "content": "code comes additional processor instructions. This can cause issues seen in the non-\ninstrumented code to go away, or may introduce other artifacts. Be sure to use caution\nwhen using the debug output pipeline to debug time sensitive code.\nImplementation\nThe base implementation for a smart packet sniffer is simple. The following sections\noutline the basic steps we took when creating our sniffer.\nExpose Network Structures\nThe basics of a smart packet sniffer require that you expose internal game protocol\ninformation. This can usually be done by simply including the same header files for\nboth the game and the sniffer. One suggestion is to set up a shared directory for any\ncode that is common between the game and sniffer. This will help you keep the pro-\ntocol version synchronized between the two applications. \nPacket Acquisition\nYou are going to need a way to pull data off of the network. There are several options\navailable. You can write the code for capturing packets yourself. An alternative, and\nrecommended solution, is to use a third-party library like pcap [Pcap]. This is the\nroute we took with our sniffer. The benefit of using pcap is that the code has been\naround and tested for many years, as well as being Open Source and easy to use.\nPacket Decoding\nOnce you have obtained a group of packets, you are going to want to translate them\ninto game messages. That is where the parsing code comes in. This is basically your\nprotocol codec and what differentiates the smart sniffer from a standard packet sniffing\napplication. In the sample, included on the CD-ROM, we took a plug-in approach.\nThe decoding for our simple example protocol is handled by a DLL, loaded at run-\ntime. This allows you to support as many protocols as you want. It also allows you to\nkeep the specifics of your protocol out of the packet sniffer core code.\nDisplay\nThere are a number of ways you can represent the data. Utilities such as tcpdump\n[Tcpd] use a command-line interface. On the GHTP sniffer, we went with an MFC\nuser interface [Mfc]. The List Control is pretty basic, but lends itself very well to the\ndata we wanted to display. It allows you to set up a simple multi-row, multi-column\nview. There are a number of options out there for building interactive user interfaces.\nDo your homework and find what works best for your project. See Figure 6.3.1.\n6.3\nGame Network Debugging with Smart Packet Sniffers\n493\n",
      "content_length": 2416,
      "extraction_method": "Direct"
    },
    {
      "page_number": 527,
      "chapter": null,
      "content": "Using the WinPcap Library\nThe pcap library is used in the Wireshark Open Source packet sniffer among many\nother network tools [Wireshark]. WinPcap is the Windows version of this library. It\nallows developers to easily capture packets being sent across the network. Developers\nonly need to use a small subset of the full library in order to get started. Make sure to\nlook at the sample code on the CD-ROM for a working example of the functions cov-\nered in this section. To save space, and your sanity, I only list function prototypes here.\nRead over the pcap documentation for more in-depth information on these functions.\nEnumerating Devices\nIn order to start capturing packets, you need to define which local network device you\nwant to listen to. First, you need to know what devices are available on your system.\nPcap provides the following two functions for obtaining device information and for\nflushing the memory used for the query.\nint    pcap_findalldevs(pcap_if_t **, char *);\nvoid   pcap_freealldevs(pcap_if_t *);\n494\nSection 6\nNetworking and Multiplayer \nFIGURE 6.3.1\nA view of the user interface of our smart packet sniffer.\n",
      "content_length": 1137,
      "extraction_method": "Direct"
    },
    {
      "page_number": 528,
      "chapter": null,
      "content": "Initializing Pcap\nBefore you can start capturing packets, you have to initialize pcap. This sets up which\nnetwork device you would like to use for capturing packets. You can set filters for cap-\nture as well. You might, for example, want to filter out everything except UDP pack-\nets. Our packet sniffer sample assumes this to be true.\n//Obtain a handle to the pcap device.\npcap_t\n*pcap_open_live(const char *, int, int, int, char *);\n//Determine the medium for this device. (such as Ethernet)\nint    pcap_datalink(pcap_t *);\n//Compile the packet capture filter from text.\n// (pcap documentation covers the specifics of the filter grammar.)\nint    pcap_compile(pcap_t *, \nstruct bpf_program *, \nchar *, \nint,           bpf_u_int32);\n//Set the packet filter.\nint    pcap_setfilter(pcap_t *, struct bpf_program *);\n//Release the pcap device\nvoid  pcap_close(pcap_t *);\nAcquiring Packets\nThe next step is to set up the pipeline for handling the packets. To do that, we use the\npcap_dispatch function. In the sample code included on the CD-ROM, after initial-\nizing pcap, we set a timer via the SetTimer Windows API function. In the timer han-\ndler, we call the pcap_dispatch function to access the captured packet data.\n//Callback prototype\ntypedef void (*pcap_handler)(u_char *, \nconst struct pcap_pkthdr *, \nconst u_char *);\nint  pcap_dispatch(pcap_t *, int, pcap_handler, u_char *);\nThe packet handler callback is where your code gains access to the packet data.\nThis is where your protocol codec will handle the raw network data and turn it into\nsomething useful.\nSecurity Risk Reduction\nA tool that can decode and display all of the internals of your network code is a great\naid for the engineers working on debugging your protocol. At the same time, it’s also\n6.3\nGame Network Debugging with Smart Packet Sniffers\n495\nON THE CD\n",
      "content_length": 1831,
      "extraction_method": "Direct"
    },
    {
      "page_number": 529,
      "chapter": null,
      "content": "a great tool for those who might want to exploit your protocol. This makes it a poten-\ntially dangerous tool. You should therefore put some thought into how you can limit\nthe potential misuse.\nLimit Deployment\nMake sure that only those who need direct access to the tool can get it. As a network\nengineer, the last thing you want to see is your protocol hacked, on the first day of\nrelease, by a tool you created to make development life easier.\nEncryption\nIf your protocol includes some level of encryption, you may have an inherent\nuntapped line of defense. It is a good idea to provide the ability to disable encryption\nfor ease of debugging. You can do this a number of ways. You can link against an\nunencrypted version for your networking library. Another option is to have a separate\nbuild target that includes a preprocessor define to disable encryption.\nPerhaps your packet sniffer is only able to evaluate packets that are sent out across\nthe wire with encryption disabled. This should help mitigate the risks with developing\na tool like this. Even if it were to make its way to the public channels, users would not\nbe able to use it directly to analyze your network traffic. This solution is not perfect.\nSomeone could analyze the sniffer assembly code to reverse-engineer your internal\nnetwork code structures. This will certainly make it easier for a hacker to find the cor-\nresponding structures in your game binary.\nAn Alternative\nThere are options available if you would rather not write an application from the\nground up. The Wireshark packet sniffer has a plug-in architecture that allows you to\ndefine your protocol specifics. You could, for example, write a packet dissector for your\nprotocol. What this does is expose the internals of your protocol to Wireshark. This\nadds a great deal of extensibility to an already powerful tool. There are a number of\nthird-party packet dissectors that are packaged with Wireshark. For example, there is a\ndissector for the Quake 3 protocol included with the main distribution. As a network\nprogrammer, you should make sure to have a full-featured packet sniffer available.\nSample Code\nThe example on the CD-ROM includes a simple command-line client and server\nsimulation application set, as well as a simple smart packet sniffer application. Full\nsource code is included to all applications. The project files included require Visual\nStudio 2005 and the WinPcap development library.\n496\nSection 6\nNetworking and Multiplayer \n",
      "content_length": 2482,
      "extraction_method": "Direct"
    },
    {
      "page_number": 530,
      "chapter": null,
      "content": "Conclusion\nSometimes small efforts end up with huge wins when you take your time. The core\nsniffer took us about a half a day to write. The end result was a big help in reducing\ndeveloper time when debugging our protocol. Try to make the time early on in your\nproject to think about the information you need to collect from your game in order to\nbest debug it. Put the hooks in as early as possible. It will pay off later in the project.\nReferences\n[Mfc] Microsoft Foundation Classes documentation. Available online at http://\nmsdn2.microsoft.com/en-us/library/d06h2x6e(VS.80).aspx.\n[Pcap] WinPcap Website. Available online at http://www.winpcap.org.\n[Tcpd] tcpdump Website. Avaliable online at http://www.tcpdump.org.\n[Wireshark] Wireshark Website. Available online at http://www.wireshark.org.\n6.3\nGame Network Debugging with Smart Packet Sniffers\n497\n",
      "content_length": 854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 531,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 532,
      "chapter": null,
      "content": "499\nS E C T I O N\n7\nSCRIPTING AND\nDATA-DRIVEN SYSTEMS\n",
      "content_length": 54,
      "extraction_method": "Direct"
    },
    {
      "page_number": 533,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 534,
      "chapter": null,
      "content": "501\nIntroduction\nScott Jacobs\nTom Forsyth\nM\naximizing performance is a perpetual endeavor when developing games. Tradi-\ntionally, the performance focus has been on the game software’s runtime charac-\nteristics. Therefore, compiled languages have been and currently remain the bedrock\nof game programming. But often developers find they need to increase performance\nin other areas: implementing speed and rate of iterative development come immedi-\nately to mind. This is where scripting and data-driven solutions are most frequently\nput to effective use. This section introduces five scripting and data-driven gems, each\none with accompanying code on the CD-ROM.\nFirst, Julien Hamaide provides a method for automatically binding C++ classes to\nthe popular game scripting language Lua. His implementation is particularly focused\non performance, efficient memory usage, and thread safety. Next to interface with\nC++ classes is Joris Mans, who wrote a gem about serializing class instances to and\nfrom relational databases such as PostgreSQL. Storing class instance data in this way\nopens up whole new avenues for data manipulation, sharing, calculating metrics, and\nbalancing.\nMartin Linklater shares a design he calls dataports, which provide a common\ncommunication API for code to manipulate data in other pieces of code. This generic\ninterface can reduce coupling between modules and allow for more flexible interfaces.\nA data-driven approach for managing shaders is presented by Curtiss Murphy.\nThe architecture he introduces is configured by XML files and can conceivably allow\nfor shader iteration and development with little to no graphics programmer involve-\nment after the initial implementation investment.\nFinally, Zou Guangxian explores the idea of directly manipulating Python’s AST\nto create string tables. This gem makes use of powerful functionality inherent in\nPython to hook into Python’s parsing and compiling stages in order to either extract\nuseful information about the code’s structure or to dynamically affect and customize\nthe compilation results, which you will hopefully find interesting and inspiring.\n",
      "content_length": 2127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 535,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 536,
      "chapter": null,
      "content": "503\n7.1\nAutomatic Lua Binding\nSystem\nJulien Hamaide\njulien.hamaide@gmail.com\nW\nith game content growing faster than ever, programmers cannot hand-code all\nthe behavior anymore. They need the help of game and content creators.\nScripting languages have already been used in games for decades, but today’s console\ncan take advantage of them to increase the player’s experience further. This gem\nfocuses on an implementation of a Lua binding. This technique allows programmers\nto expose their C++ classes to Lua without any knowledge about the system. The tools\npresented here can apply to other languages as well. The design has been driven by\nusability, performance, memory footprint, and multithreading.\nIntroduction\nThe binding explained in this gem allows creation, access, and use of C++ objects\ninside a Lua script. As an example, if a list of ENTITY instances is stored inside a single-\nton class WORLD, the following script can be used to set the health of the player:\nlocal entity = WORLD:GetEntity( \"player\" )\nentity:SetHealth( 50 )\nThe binding used in this example is defined by the following declarations:\n// in .h and class definition\nSCRIPTABLE_DefineClass( WORLD )\n// in .cpp\nSCRIPTABLE_Class( WORLD )\n{\nSCRIPTABLE_ResultMethod1( GetEntity, ENTITY, std::string )\n}\nSCRIPTABLE_End()\nBinding a class is as simple as that. No other step is required, allowing the pro-\ngrammers to expose a C++ class and its methods to the Lua binding very simply. \n",
      "content_length": 1457,
      "extraction_method": "Direct"
    },
    {
      "page_number": 537,
      "chapter": null,
      "content": "504\nSection 7\nScripting and Data-Driven Systems \nFeatures\nThe system has been designed with several objectives in mind: \n• Low memory footprint\n• High-performance binding\n• Support of C++ inheritance\n• Ease of use\n• Thread safety between scripts\nBinding of C Functions\nTo bind a function, Lua requires a specific interface. The function must have the type\ndefined in the following code. Lua binding is stack-based, the lua_State contains all\narguments passed to the function. Those arguments must be retrieved with lua_to*\nmethods using stack indexes. In this example, the function accepts a string as the first\nargument, a number as the second argument, and returns another number. More infor-\nmation on binding of C functions can be found in the Lua manual [Ierusalimschy06].\nThe C function binding is the only way you can bind Lua to C/C++, and will be the\nbase of the system.\nint binding_method( lua_State * state )\n{\nconst char * some_string;\ndouble some_number, another_number;\nsome_string = lua_tostring( state, 1 );\nsome_number = lua_tonumber( state, 2 );\n// Do some stuff here, including setting the return value \n// another_number\nlua_pushnumber( state, another_number );\nreturn 1; //Just say we returned one argument.\n}\nObject-Oriented in Lua\nLua is a versatile language that can be used to implement a lot of programming para-\ndigms. This gem explains how Lua can become object-oriented. To help, the Lua\nauthors have defined a set of tools providing “syntactic sugar.” The one we use in this\nsystem is shown in the following code. In it, the_object is an initialized variable, and\nthis code simulates a this_call on the method returned.\nthe_object:Test( 5 ) == the_object[\"Test\"]( the_object, 5 )\nObject-oriented methods can be implemented using this syntactic sugar. The\nobject is accessed as an array with the name of the method and returns the function to\nbe called. Lua has a mechanism that allows any type of variable to react to an array\n",
      "content_length": 1957,
      "extraction_method": "Direct"
    },
    {
      "page_number": 538,
      "chapter": null,
      "content": "access, using a metatable. (This is a feature of Lua 5.1. Lua 5.0 restricted metatables\nto table and userdata objects.) Metatables are Lua tables that are assigned to an object\nthat contains special fields: __index, __newindex, and so on [Ierusalimschy06b]. The\nfunctions set in those special fields are called depending on the situation. When an\nobject is accessed as an array, __index is called. The following code shows how to set\nup a metatable on an object:\nmetatable = {}\nmetatable.__index = function( table, key ) return key end\nsetmetatable( object, metatable )\ntest_return = object[ \"Test\" ] -- call the __index function in\nmetatable\nLua native types are number (double or float), string, table, nil, function (Lua or\nC), thread, and (light) user data. We use the latter to store the object in Lua. Light\nuser data and user data are slightly different. The first is used as a raw pointer, has no\nmetatable, and is not garbage collected. The second is a complete Lua object that can\nhave a metatable.\nBinding C++ Objects in Lua\nThe binding requires several mechanisms: the representation of the C++ object in\nLua, the storage of bound functions, and finally the registering of each C++ object in\nthe binding data. In this gem, the overall technique is given, and special cases are\nexplained later.\nThe Binding Data Structure\nIn existing implementations [Celes05], the binding is directly stored in Lua. Binding\ndata is then stored in each script. But if the number of scripts the system must support\nis high, binding data is duplicated unnecessarily. To avoid this, we decided to store the\nbinding data in C++ in a class called SCRIPTABLE_BINDING_DATA. Each class to be bound\nis assigned an index. SCRIPTABLE_BINDING_DATA contains a map between the class name\nand its index, and this map is stored in ClassIndexTable. Each class then has a map\nbetween a function name and the corresponding binding function. MethodTable is an\narray of these maps indexed by the value in ClassIndexTable. Because the delete oper-\nator has no name, its binding is put in a separate array called DeleteTable. Finally, the\nParentTable contains the index of the parent of each class. When a class has no parent,\nthe ParentTable entry is set to –1.\nA series of helper functions to access these maps can be found on the CD-ROM\nin scriptable_binding_data.h.\nclass SCRIPTABLE_BINDING_DATA\n{\ntypedef int (* BINDING_FUNCTION) (lua_State *);\n7.1\nAutomatic Lua Binding System\n505\n",
      "content_length": 2458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 539,
      "chapter": null,
      "content": "std::map<std:string, int>\nClassIndexTable;\nstd::vector<std::map<std::string, BINDING_FUNCTION>*>\nMethodTable;\nstd::vector<BINDING_FUNCTION>\nDeleteTable;\nstd::vector<int>\nParentTable;\n};\nA pointer to this binding data and the index of the class is stored inside lua_State.\nThe space for this data is allocated by setting the LUAI_EXTRASPACE constant in \nluaconf.h, and the extra memory is allocated before lua_State.\nC++ Objects as Lua Objects\nA bound C++ object needs to store its class index—the result of looking up its class\nname in ClassIndexTable. This class index is used to search for bound functions in\nthe binding data. As previously said, we represent C++ objects as user data in Lua.\nThis user data contains the pointer to the bound object and its class index. The\nSCRIPTABLE_BINDING_HELPER structure helps the readability of the code. \nInside each bound class, an inner class called CLASS_SCRIPT_TYPE is declared. This\nis used in several parts of the binding process, explained individually. The interest\nright now is that it stores the index of the class, making the storage of C++ objects in\na Lua object simpler. _VALUE_::CLASS_SCRIPT_TYPE::GetClassIndex() recovers the\nclass index. \nThe following functions are used by C++ code when reading the arguments of a\ncall made from Lua, and returning the results to Lua.\ntemplate< typename _VALUE_>\nvoid SCRIPTABLE_PushValue( \nlua_State * state, _VALUE_ & object, _VALUE_ * dummy )\n{\nSCRIPTABLE_BINDING_HELPER\n*helper;\nhelper\n= lua_createuserdata( state, sizeof(SCRIPTABLE_BINDING_\nHELPER ) );\nhelper->Object = & object;\nhelper->ClassIndex = _VALUE_::CLASS_SCRIPT_TYPE::GetClassIndex();\n}\ntemplate<typename _VALUE_>\n_VALUE_& SCRIPTABLE_GetValue( lua_State * state, int index, _VALUE_\n*dummy )\n{\n506\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 1803,
      "extraction_method": "Direct"
    },
    {
      "page_number": 540,
      "chapter": null,
      "content": "SCRIPTABLE_BINDING_HELPER\n*helper;\nhelper = lua_touserdata( state, index );\nreturn *(helper->Object);\n}\nBy default, SCRIPTABLE_GetValue returns a reference to the object. But these func-\ntions can be specialized to support specific types, such as string, by value. A version is\ndefined for each C++ type that can convert to a Lua primitive: string, integer, and float. \nstd::string SCRIPTABLE_GetValue( \nlua_State * state, int index, std::string * dummy )\n{\nreturn lua_tostring( state, index );\n}\nThe function signature contains a trick. A dummy pointer is passed in as the\nthird argument. This argument selects the correct overloaded function. If template\nspecialization was used, the return value would always be a reference to the object. By\nusing the dummy pointer trick, the return value can be changed depending on the\ntype—objects can be returned by reference, whereas primitives such as strings and\nfloats can be returned by value.\nThe code is still not complete. A metatable must be assigned to the user data.\nThis metatable defines a method for __index (array access) and __gc (garbage collec-\ntion). As all binding data is stored in SCRIPTABLE_BINDING_DATA, only one instance per\nscript of this metatable is needed and it can be assigned to all C++ objects.\nBinding Function Creation\nThe binding function recovers the arguments from the Lua stack and performs the\nactual call. With the help of SCRIPTABLE_GetValue and SCRIPTABLE_SetValue, the\nbinding function creation is simple. The this pointer is always passed as argument\none. The function arguments are indexed from two.\nint ENTITY_AddHealth( lua_state * state )\n{\nENTITY &entity = SCRIPTABLE_GetValue( state, 1, ( ENTITY*) 0 );\nfloat health_add = SCRIPTABLE_GetValue( state, 2, (float*) 0 );\nfloat new_health = entity.AddHealth ( health_add );\nSCRIPTABLE_PushValue( state, new_health, (float*) 0 );\nreturn 1; // One return value\n}\nAlthough binding code like this is easy to create, the task is repetitive and error-\nprone. A macro-based system is used to generate this function instead. An example of\nsuch a macro is as follows:\n7.1\nAutomatic Lua Binding System\n507\n",
      "content_length": 2133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 541,
      "chapter": null,
      "content": "#define SCRIPTABLE_ResultMethod1( _RETURN_, _METHOD_, _\nPARAMETER_0_ ) \\\nint _RETURN_##_METHOD_##_PARAMETER_0_( lua_State * state ) \\\n{ \\\nCLASS \\\n&self = SCRIPTABLE_GetValue( state, 1, (CLASS *) 0 ); \\\nSCRIPTABLE_PushValue( \\\nstate, \\\nself._METHOD_( SCRIPTABLE_GetValue( state, \n2,(_PARAMETER_0_*)0 )), \\\n(_RETURN_*) 0 \\\n); \\\nreturn 1; \\\n}\nThis macro only creates the binding function. It also needs to be registered into\nthe system. With a little C++ trick, you can do both at the same time. By using a func-\ntion inner class with a static method, you can create and register the method at the\nsame time, as shown in the following code:\nvoid register_ENTITY( BINDING_DATA & binding_data )\n{\nclass float_AddHealth\n{\npublic:\nstatic int Call( lua_State * state )\n{\n// ... ENTITY_AddHealth() code as above...\nreturn 1;\n}\n}\nbinding_data.Register(\n\"ENTITY\", \"AddHealth\", &float_AddHealth::Call );\n}\nThis pattern can be made into a set of general macros, as follows:\n#define SCRIPTABLE_Class( _CLASS_ ) \\\nvoid register_##_CLASS_ ( BINDING_DATA & binding_data )\\\n{\n#define SCRIPTABLE_End( _CLASS_ ) \\\n}\n#define SCRIPTABLE_ResultMethod1( _RETURN_, _METHOD_, \n_PARAMETER_0_ ) \\\nclass _RETURN_##_METHOD_##_PARAMETER_0_ \\\n{ \\\npublic: \\\nstatic int Call( lua_State * state ) \\\n508\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 1313,
      "extraction_method": "Direct"
    },
    {
      "page_number": 542,
      "chapter": null,
      "content": "{ \\\n... SCRIPTABLE_ResultMethod1 () code as above...\nreturn 1; \\\n} \\\n} \\\nbinding_data.Register( class_name, #_METHOD_, \\\n&#_RETURN__#_METHOD__#_PARAMETER_0_::Call );\nThese macros are used in the following way:\nSCIPTABLE_Class(ENTITY)\n{\nSCRIPTABLE_ResultMethod1(float,AddHealth,float)\n}\nSCRIPTABLE_End(ENTITY)\n//...and then at start of day, register the class...\nregister_ENTITY(binding_data);\nAttributes (such as data members of a class) are also handled by the system. Typi-\ncal binding allows the access of attributes by doing object.attribute. To handle such\nan access, the __index and __newindex metamethods must be adapted. To avoid this,\nwhen an attribute is bound with SCRIPTABLE_Attribute, Set and Get functions are\ncreated in Lua. The demo on the CD-ROM contains a definition of this macro and\nan example use of it in the vector_3 class.\nAs shown, the creation of the binding and registration function for each class is\nnow handled by some macros. But you still need a simple way to call these functions. \nAutomatic Type Registering\nTo try to keep the system as transparent to the user as possible, we decided to encap-\nsulate the registration function into a class. Each object that is bound declares an\ninner class derived from SCRIPTABLE_TYPE. This class has a static member whose con-\nstructor adds it to a global table. This table is then walked at program invocation, call-\ning each registration function.\nclass SCRIPTABLE_TYPE\n{\npublic:\nSCRIPTABLE_TYPE()\n{\nSCRIPTABLE_TYPE_TABLE::Add( this );\n}\nvirtual void Register( BINDING_DATA & binding ) = 0;\n}\n7.1\nAutomatic Lua Binding System\n509\n",
      "content_length": 1603,
      "extraction_method": "Direct"
    },
    {
      "page_number": 543,
      "chapter": null,
      "content": "Then, in the bound class, the following code is inserted: \nclass CLASS_SCRIPT_TYPE;\nfriend class CLASS_SCRIPT_TYPE;\nclass CLASS_SCRIPT_TYPE :\npublic SCRIPTABLE_TYPE\n{\npublic :\ntypedef GAME_ENTITY CLASS;\nCLASS_SCRIPT_TYPE( void );\nstatic const char * GetClassName() { return \"GAME_ENTITY\"; }\nstatic int & GetClassIndex()\n{\nstatic int index = -1;\nreturn index;\n}\nstatic int Delete( lua_State * lua_state ); \nvirtual const char * GetName() const { return GetClassName(); } \nvirtual int & GetIndex(){ return GetClassIndex(); } \nvirtual void Register( SCRIPTABLE_BINDING_DATA & binding ); \n};\nThe inner class contains the Register function, the Delete function, the bound\nclass name, and its index. Its definition is hidden inside the SCRIPTABLE_DefineClass\nmacro. The details on this macro can be found in the demo on the CD-ROM.\nclass GAME_ENTITY\n{\npublic:\nSCRIPTABLE_DefineClass( GAME_ENTITY )\n};\nWith the whole system in place, you can call all registration functions by walking\nthe table, as shown in the following code:\nvoid SCRIPTABLE_TYPE_TABLE::Register( BINDING_DATA & binding_data )\n{\nint type_index;\nfor( type_index = 0, type_index < TypeTable.size(); ++type_index )\n{\nTypeTable[ type_index ]->Register( binding_data );\n}\n}\nExtending the Binding System\nThe following sections explain some ways to extend this binding system.\n510\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 1381,
      "extraction_method": "Direct"
    },
    {
      "page_number": 544,
      "chapter": null,
      "content": "Reference Counting and Raw Objects\nThe Lua instance of an object contains its pointer. If an object is destroyed while Lua\nstill has a variable containing the object, bugs can occur. To prevent such situations,\nwe handle the object in two ways: \n• If the object is reference counted, Lua increases the reference count. Even if the\nC++ object is not referenced on the C++ side of the program, the object will not\nbe deleted as long as Lua does not release its reference. The Delete function asso-\nciated on the __gc metamethod will be called when a Lua variable is being col-\nlected, and this function will decrease the reference count.\n• If the object is an uncounted object such as a vector, create a copy of it. In this\ncase, the Delete function just deletes the object.\nThe choice between the two methods is done for each class. Two defines are\navailable—SCRIPTABLE_Class for counted objects and SCRIPTABLE_UncountedClass\nfor uncounted objects. These macros must be placed in the class definition. The demo\nshows both techniques at work.\nInheritance\nInheritance is implemented by storing the class index of the parent in the ParentTable.\nIf a method cannot be found in the current class binding, it searches in its parent bind-\ning. The function BINDING_DATA::GetFunction does this search, and can be found on\nthe CD-ROM. The registration function created by the macros contains a call that sets\nthe parent for the current class. SCRIPTABLE_Class is used for baseless classes, otherwise\nSCRIPTABLE_InheritedClass can be used.\nSupporting inheritance also means that inherited classes can be pushed as argu-\nments to a method. The SCRIPTABLE_PushValue macro has the class index hard-coded.\nTo bypass this issue, the binding function is put in a virtual function of the class. The\ntemplate version of SCRIPTABLE_PushValue is shown next, calling the derived version\nof LuaPushValue. This function is hidden inside SCRIPTABLE_DefineClass (virtual)\nand SCRIPTABLE_DefineRawClass (non-virtual).\ntemplate< typename _VALUE_>\nvoid SCRIPTABLE_PushValue( \nlua_State * state, _VALUE_ & object, _VALUE_ * dummy )\n{\nobject.LuaPushValue( state );\n}\nSingletons, Static Functions, and Attributes\nSingletons, static functions, and attributes share a property: no object is associated\nwith function calls. To follow the C++ syntax as closely as possible, the call to such\nfunctions is done by prefixing the class name with the method name, as if the class\n7.1\nAutomatic Lua Binding System\n511\n",
      "content_length": 2476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 545,
      "chapter": null,
      "content": "was the object. For example, WORLD:GetEntity(). This call triggers a lookup for the\nvariable WORLD in the global table (in Lua, all global variables are stored by name in a\ntable called _G). An easy way to allow this is to create a Lua variable for every C++\nclass. But this solution breaks the memory consumption target. Even if your script is\nnot using a class function, or if a class does not have any static functions, a variable\nwould have been created and inserted into the global table. \nThe chosen solution is to use the __index mechanism on the global table. \nWhen a script accesses a global variable that does not exist in the global table, the\nSCRIPTABLE_LUA_REGISTERER::GlobalIndexEventHandler is called. If the access vari-\nable name matches a class name, a new object with a null pointer is created and\ninserted into the global table with the class name. If it does not match, the handler\nreturns nil as Lua does if no table entry is found. This way, only class variables that are\nused are created, and once the event handler has been called, the variable is available\nin the global scope. This mechanism is transparent to all other variable access.\nTemplate Classes\nThe binding is also able to handle template classes. The template does not need to be \na bound class, but in this case, it should define its name in Lua by using\nSCRIPTABLE_DeclareScriptableTypeName. All the native types are already declared.\nThe name of the class in Lua is the name of the template class suffixed by the name of\nthe parameter. The codebase requires all template class to be of the form CLASS_TO_ or\nCLASS_OF_, therefore RANGE_OF_<VECTOR_3> becomes RANGE_OF_VECTOR_3 in Lua, which\nis quite consistent. If this convention does not suit your needs, the system is easy to\nchange.\nThe macros used are the same as the non-template ones with the word “Template”\nadded. For exampe, SCRIPTABLE_Class becomes SCRIPTABLE_TemplateClass. The\nbinding requires a cpp file to contain all definitions. The binding must also be explic-\nitly instantiated with the help of the macro SCRIPTABLE_InstantiateTemplateClass.\nThe demo shows a dummy template class to show how the binding works.\nThe binding has only been implemented for a single-parameter template class,\nbut it can be easily extended to any number of parameters if needed.\nSupport of Enums\nEnums are supported in our system. SCRIPTABLE_PushValue and GetValue are rede-\nfined to treat them as integers by using SCRIPTABLE_CastValue( ENUM_TYPE, int ).\nTo allow the user to use the name of the enum in the code, a preprocessor pass is done\non the code, replacing all matching enum entries by their values. A macro system is\nalso used to create and register the text as a #define in the preprocessor, but the details\nof this are beyond the scope of this gem.\n512\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 2845,
      "extraction_method": "Direct"
    },
    {
      "page_number": 546,
      "chapter": null,
      "content": "Binding Overloaded Functions\nC++ allows the overloading of a function. In a variant, SetValue can exist for bool,\nint, real, and so on. Lua does not support overloading. Two solutions exist. The \nfirst is that BINDING_DATA::GetFunction can implement an argument type matcher,\nbut this is expensive. The other solution is to rename the function in Lua. The\nSCRIPTABLE_Renamed* macros allow methods to be renamed in Lua so that SetValue\ncan be renamed to SetBoolValue, SetRealValue, and so on.\nDebug Helper\nBecause the binding functions are being created by macro instances, adding debug-\nging functions and asserts to all bound functions is simple. The debugging system is\nenabled by the preprocessor define _LUA_DEBUG_. The debug helper checks the argu-\nment count and the class type. If errors are detected, a Lua error is triggered. This\ndebugging is designed to be usable in a C++ release build, allowing the scripts to be\ndebugged at full speed. The advantage of using Lua errors is that the game does not\ncrash; it just exits the call. The behavior is also compatible with any Lua debugger you\nuse. The binding error can be treated the same way as a Lua error. The debug helpers\ncan be completely deactivated for a retail build, increasing the binding overall speed.\nSummary\nTo bind a class, this macro SCRIPTABLE_DefineClass( MY_CLASS ) must be put in the\nclass definition. Four options are available: \nSCRIPTABLE_DefineClass : LuaPushValue is virtual\nSCRIPTABLE_DefineRawClass : LuaPushValue not virtual\nSCRIPTABLE_DefineTemplateClass : template class, LuaPushValue is \nvirtual\nSCRIPTABLE_DefineRawTemplateClass : template class, LuaPushValue \nnot virtual\nThe template parameter is passed as the second argument of the macro. In the\ncpp file, the class binding implementation must be set up as follows:\nSCRIPTABLE_Class( MY_CLASS )\n{\nSCRIPTABLE_VoidMethod( SetValue, float )\n}\nSCRIPTABLE_End()\nThe options are SCRIPTABLE_(Uncounted)(Inherited)(Template)Class. If the\nobject is not reference counted, the Uncounted version of the macro should be used. If\nthe class inherits from another, use the Inherited version. The demo code covers the\ndefinition of almost all types of object.\n7.1\nAutomatic Lua Binding System\n513\n",
      "content_length": 2225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 547,
      "chapter": null,
      "content": "Future Work\nThe following sections address some possible extensions to this system.\nOverloading of C++ Methods in Lua\nC++ objects exist in Lua and can be used as Lua objects. Overloading the C++ object\nmay be an interesting extension. For example, this would allow hooking of function\ncalls. To intercept each call to GetHealth and print the current health, the GetHealth\nmethod can be overloaded, as shown in the following code:\nObject.GetHealth = \nfunction( self ) \nlocal health = ENTITY.GetHealth( self ) -- Do the call to C/C++\nprint( \"Health of object \" .. self .. \" is \" .. health )\nreturn health;\nend\nWhen a value is set by array access, the metatable’s __newindex entry is called. It\nstores the Lua function into a table as a replacement of the C function. The __index\nfunction is also changed to reflect a new behavior: on array access, it searches the Lua\nfunction table first, and then searches the C++ binding data. Although this solution\nshould work, it causes another problem—the object must be kept by Lua in some\nway. If it is garbage collected, its overloading will be lost. The implementation in the\ndemo does not support overloading, but does support the persistence of objects.\nEvery time a call to SCRIPTABLE_PushValue is made, the Lua version of the object is\nsought in a table called _object. If it exists, it is reused; otherwise, the Lua version of\nthe object is created. This code can be found in the demo.\nSand-Boxing and Type Filtering\nThe presented system allows the binding of objects to Lua. But you may want to\nsand-box some scripts, limiting their access. Low-level scripts can be used as configu-\nration files, accessing features such as file I/O or graphics configuration, whereas user-\nlevel scripts can only access a selected set of classes. The definition of the access level\ncan be set up in the macros and stored in SCRIPT_TYPE. The Register method can be\ncalled with the access level wanted. In the following code, the class WORLD is declared\nas being at user level. The SCRIPT_MANAGER contains the BINDING_DATA. Several man-\nagers can be created with different access levels. When a manager is initialized, it calls\nSCRIPT_TYPE_TABLE::Register with its binding data and its access level. The binding\ndata contains only classes that are available for its access level. Scripts are created by\nand associated with a manager, and use its binding data.\n514\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 2439,
      "extraction_method": "Direct"
    },
    {
      "page_number": 548,
      "chapter": null,
      "content": "SCRIPTABLE_Class( WORLD, ACCESS_LEVEL_User )\nvoid SCRIPT_TYPE_TABLE::Register( \nBINDING_DATA & binding_data, const ACCESS_LEVEL access_level )\n{\nint type_index;\nfor( type_index = 0, type_index < TypeTable.size(); ++type_index )\n{\nif( TypeTable[ type_index ]->GetAccessLevel() >= access_level )\nTypeTable[ type_index ]->Register( binding_data );\n}\n}\nOptimization of Generated Code Size\nThe technique presented here creates a binding function for all bound methods. It\ngenerates lots of code that does the same duty again and again. A solution is to create\nfunction descriptions instead of binding functions. The binding data can be a class\nthat stores this FUNCTION_DESCRIPTION:\nstruct FUNCTION_DESCRIPTION\n{\nconst char * FunctionName;\nconst void * FunctionPointer\nint ArgumentCount;\nARGUMENT_DESCRIPTION * ArgumentDescription;\nRETURN_DESCRIPTION * ReturnValueDescription;\n}\nThe macro system fills an array of these descriptions:\n#define SCRIPTABLE_VoidMethod1( _METHOD_, _PARAMETER_0_ ) \\\n{ #_METHOD_, &CLASS::_METHOD_, 1,\n{ GetArgumentDescription<_PARAMETER_0_>() }, 0 }, \nNow only a single binding function is necessary. Its pseudocode is as follows: \nfor each argument_index  < description.ArgumentCount \ndescription.ArgumentDescription[\nargument_index ].PushArgumentOnCStack();\ncall( decription.FunctionPointer );\nif( description.ReturnValueDescription )\ndescription.ReturnValueDescription->StoreResultValueInLua();\n7.1\nAutomatic Lua Binding System\n515\n",
      "content_length": 1457,
      "extraction_method": "Direct"
    },
    {
      "page_number": 549,
      "chapter": null,
      "content": "This code must be partially written in assembly, because it accesses the C stack.\nThis system, although a little slower, saves some memory, which is useful if your sys-\ntem is memory-bound.\nDemo\nThe code on the CD-ROM provides the binding in full functionality, and it should\nbe simple to plug it into a codebase without any problem. The demo tries to cover all\nthe features explained here. If you launch the demo, you won’t see much except some\ntext. Single-stepping the code is the best way to understand how the system works.\nOnce the broad details are clear, expand the macros manually and step into the\nexpanded code to see the detailed workings.\nConclusion\nThis gem presents an automatic binding system. The user only has to set up some\ndeclarations about a bound class, recompile, and the class is accessible from scripts.\nNo knowledge of either the system or the Lua binding is needed. The system has been\ndesigned to be CPU and memory friendly. The system provides debug helpers such as\nargument checking that can be disabled for retail builds. This gem also presents sev-\neral C++ tricks, such as automatic type registering and dummy pointer function selec-\ntion. With these tools in hand, anybody should be able to write or adapt this binding\nto their engine and script language. \nReferences\n[Celes05] Celes, W., de Figueiredo, L.H., and Ierusalimschy, R. “Binding C/C++\nObjects to Lua,” Game Programming Gems 6, edited by Michael Dickheiser,\nCharles River Media, 2005, pp. 341-356.\n[Ierusalimschy06] Ierusalimschy, R., de Figueiredo, L.H., and Celes, W. “Lua 5.1\nReference Manual,” available online at Lua.org, 2006.\n[Ierusalimschy06b] Ierusalimschy, R. “Programming in Lua (second edition),” avail-\nable online at Lua.org, 2006.\n516\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 1791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 550,
      "chapter": null,
      "content": "517\n7.2\nSerializing C++ Objects \ninto a Database Using\nIntrospection\nJoris Mans\njoris.mans@10tacle.be\nW\nith the ever-increasing amount of assets that need to be managed by content-\ncreation tools, managing those assets becomes more difficult, especially when\nconfronted with quantities that cannot simply all be loaded in memory at the same\ntime. Users of these tools want to be able to navigate through all those assets in order\nto quickly find the one they need. Keyword searches, categories, and hierarchical\nviews are ways of exposing this to the end user. Another issue is that there are many\npeople working on content creation who want to use the same shared assets, and the\nasset creators want to have them exposed to all the users as soon as possible.\nOne of the possible ways to implement this is by using a database backend. This\ngem presents a system that allows storage of C++ objects into an SQL database, and\ntheir retrieval using filters. The implementation and examples were created using\nPostgreSQL 8.2 and Microsoft Visual Studio 2005.\nMetadata\nBefore you can start serializing in the database, you need some introspection tools in the\ncodebase. The implementation of a robust and complete metadata system (hereafter\nreferred to as meta-system) is beyond the scope of this gem, so I will restrict it to a basic\nimplementation that has support for everything necessary for database serialization.\nThe metadata of a class is saved in an instance of a class called MetaType. For the\nsystem to work, you need to be able to retrieve the following information from this\nclass:\n• Classname \n• Parent classname\n• AttributeTable containing an instance of MetaAttribute for each serializable\nattribute\n• Size of an instance of the object in bytes\n",
      "content_length": 1755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 551,
      "chapter": null,
      "content": "This class allows you to manipulate objects of arbitrary types without having to\nresort to RTTI or use polymorphism.\nAttributes\nFor every attribute in the class you want to serialize, you add its information to the\nmetadata. For this, you create a class called MetaAttribute containing the following\ninformation.\n• AttributeName\n• Attribute metatype\n• Memory offset in bytes from the start of the object\n• Whether the attribute is a pointer\n• Whether the attribute is an array\nEach MetaAttribute instance will be added to the AttributeTable list of the corre-\nsponding MetaType instance. Code for these classes can be found on the demo on the\nCD-ROM.\nArrays\nThis gem uses a special array class. This is a template class inheriting from a class called\nArrayBase. This base class contains an interface that allows you to retrieve the number\nof items in an array, to set the number of items, and to retrieve the metatype of the\nitem class used in the array. It also allows you to get the pointer to the data in the array.\nThis way you can manipulate arrays without having to know what type of object is\nstored inside them, an ability that you’ll need when manipulating objects in the data-\nbase system.\nSerializing in Text\nThere is one more thing you need before you can start implementing the database sys-\ntem. You need to be able to serialize simple attributes into text format. This demo uses\nsome overloaded C functions to do this.\nvoid WriteObjectToText( \nconst void * object, \nconst MetaType & meta_type, \nstd::string & output_text \n);\nvoid ReadObjectFromText( \nvoid * object, \nconst MetaType & meta_type, \nconst std::string & input_text \n);\n518\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 1695,
      "extraction_method": "Direct"
    },
    {
      "page_number": 552,
      "chapter": null,
      "content": "These functions have support for reading and writing objects of scalar types and\nstrings. For example:\nint a = 5;\nstd::string output_text;\nWriteObjectToText( &a, META_TYPE_GetStaticMetaType( a ), \noutput_text);\nThis will result in output_text containing the string \"5\".\nThe Database System\nBefore you start serializing into a database, consider that you’ll want to serialize C++\nobjects. What do those objects contain?\nA C++ class consists of a combination of the following:\n• Scalar members (for example, int, float, char, and so on)\n• One or more parent classes, if present\n• Pointers\n• Instances of other C++ classes\nIn this case I will slightly change this list. For the purposes here, a C++ class will\nconsist of the following:\n• Scalar members\n• Strings\n• One parent class, if present\n• Pointers\n• Instances of other C++ classes\n• Arrays of pointers, instances of other C++ classes or scalars, using our own array\ntype\nThe system described in this gem can be extended to support more features of\nC++ classes (multiple inheritance, other collection types, and so on), but it would\nextend the scope too far so I restrict the explanation to classes fitting the previous\ndescription.\nThe Tables\nBecause you’ll store the objects in a relational SQL database, you need to define the\ntables used to store those objects.\nEach table corresponds to one class. The primary key for a table is a field called\n_Identifier and contains an auto-incrementing integer. (You can give this key any\nother name you want as long as it does not conflict with the name of an attribute of\nan object you want to serialize.)\n7.2\nSerializing C++ Objects into a Database Using Introspection\n519\n",
      "content_length": 1671,
      "extraction_method": "Direct"
    },
    {
      "page_number": 553,
      "chapter": null,
      "content": "Using the list of attribute types previously defined, you’ll see how to store each\ntype in a field in the database. For each attribute, the field name corresponds to its\nname.\nScalar Members\nEach scalar member is stored as a scalar field of type integer or real in the database. \nStrings\nEach string is stored in a varchar field.\nThe Parent Class\nYou can use any table in a database as the type of a field of another table. This exam-\nple creates a field called _Parent that will be of the type of the table created to store\nthe parent class. (You could also name this field any other name you want as long as it\ndoesn’t conflict with the name of an attribute in your class.) An example will clarify\nthis class:\nclass Base\n{\nint a;\n};\nclass Subclass : public Base\n{\nint b;\n};\nNow create a table called Base:\nCREATE TABLE \"Base\" \n(\n\"_Identifier\" serial,\n\"a\" integer\n) ;\nand a table called Subclass:\nCREATE TABLE \"Subclass\"\n(\n\"_Identifier\" serial,\n\"_Parent\" \"Base\",\n\"b\" integer\n);\nPointers\nThere are several ways to store a pointer to an object in the database. At first you\nmight think that a pointer could be considered a foreign key, corresponding to the\nprimary key of the table containing the object pointed to. For example:\n520\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 1277,
      "extraction_method": "Direct"
    },
    {
      "page_number": 554,
      "chapter": null,
      "content": "class Base\n{\nint a;\n};\nclass StoreInDatabase\n{\nBase * basePointer;\n};\nYou could create a table called Base like this:\nCREATE TABLE \"Base\" \n(\n\"_Identifier\" serial,\n\"a\" integer\n);\nand a table called StoreInDatabase like this:\nCREATE TABLE \"StoreInDatabase\" \n(\n\"_Identifier\" serial,\n\"basePointer\" integer,\n);\nNow imagine creating some instances:\nBase * base_object = new Base;\nStoreInDatabase * store_object = new StoreInDatabase;\nbase_object->a = 10;\nstore_object->basePointer = base_object;\nYou could store them in the database like so:\nTable Base:\n_Identifier     a\n1                  10\nTable StoreInDatabase:\n_Identifier     basePointer\n1               1\nRetrieving the object from the database seems straightforward. You read out the\ncontents of table StoreInDatabase, use the value found in the field basePointer, and\nget the corresponding row from Base to instantiate the object pointed to. A simple\nsolution...or maybe not? \n7.2\nSerializing C++ Objects into a Database Using Introspection\n521\n",
      "content_length": 999,
      "extraction_method": "Direct"
    },
    {
      "page_number": 555,
      "chapter": null,
      "content": "Check the following example:\nclass Base\n{\nint a;\n};\nclass Subclass : public Base\n{\nint b;\n};\nclass StoreInDatabase\n{\nBase * basePointer;\n};\nSubclass * subclass_object = new Subclass;\nStoreInDatabase * store_object = new StoreInDatabase;\nsubclass_object->a = 10;\nsubclass_object->b = 20;\nstore_object->basePointer = subclass_object;\nIf you were to apply the same system to these objects, you get into trouble. Saving\nthe objects would result in this:\nTable Subclass:\n_Identifier   b       _Parent\n1             20      {10}\nTable StoreInDatabase:\n_Identifier     basePointer\n1               1\nWhen you’re trying to get the object from the database, you have an issue. When\nyou’re retrieving the value stored in basePointer, there is no way of knowing that you\nare not storing a pointer to an object of type Base but an object of type Subclass. You\ndo not know which table corresponds to the key stored in basePointer.\nThere are several solutions to this problem. This gem sticks to one solution, but\nfeel free to experiment with others. Instead of storing an integer that refers to the pri-\nmary key of the object pointed to, let’s try storing a string with the following layout:\n\"( primaryKeyValue, tableName )\"\n522\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 1261,
      "extraction_method": "Direct"
    },
    {
      "page_number": 556,
      "chapter": null,
      "content": "Applying this to the previous example gives this result:\nCREATE TABLE \"Base\" \n(\n\"_Identifier\" serial,\n\"a\" integer\n);\nCREATE TABLE \"Subclass\" \n(\n\"_Identifier\" serial,\n\"_Parent\" \"Base\",\n\"a\" integer\n);\nCREATE TABLE \"StoreInDatabase\" \n(\n\"_Identifier\" serial,\n\"basePointer\" varchar,\n);\nStoring the same objects will result in these table values:\nTable Subclass:\n_Identifier     b       _Parent\n1              20         {10}\nTable StoreInDatabase\n_Identifier      basePointer\n1                \"(1,Subclass)\"\nWhen reading the field values, you can use string manipulation to get the pri-\nmary key part and the tablename part of the field value stored in basePointer, and\nuse the corresponding table to receive the contents of the pointed to object.\nAnother solution is to store a table containing all names of the classes stored in\nthe database, and instead of using a pair containing ( primaryKeyValue, tableName )\nto store a pointer, the field will contain ( primaryKeyValue, classNamePrimary\nKeyValue ), where classNamePrimaryKeyValue contains the primary key value of the\ncorresponding classname stored in the table. Especially with big databases, this would\nbe a more efficient solution, because the amount of data to retrieve from the database\nis smaller, as would be the size of the database, because classnames aren’t replicated in\ndifferent tables.\n7.2\nSerializing C++ Objects into a Database Using Introspection\n523\n",
      "content_length": 1420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 557,
      "chapter": null,
      "content": "Instances of Other C++ Classes\nYou could store instances of classes in the same way that you store the parent class, by\nadding a field with an attribute type corresponding to the table of the associated class.\nFor example:\nclass Base\n{\nint a;\n};\nclass StoreInDatabase\n{\nBase baseInstance;\n};\nCREATE TABLE \"Base\" \n(\n\"_Identifier\" serial,\n\"a\" integer\n);\nCREATE TABLE \"StoreInDatabase\" \n(\n\"_Identifier\" serial,\n\"baseInstance\" \"Base\"\n);\nThis will work, unless you have objects containing pointers to members of other\nobjects. For example:\nclass Base\n{\nint a;\n};\nclass StoreInDatabase\n{\nBase baseInstance;\n};\nclass AnotherToStoreInDatabase\n{\nBase * basePointer;\nint b;\n};\nStoreInDatabase * store_1 = new StoreInDatabase;\nAnotherToStoreInDatabase * store_2 = new AnotherToStoreInDatabase;\nstore_1->baseInstance.a = 10;\nstore_2->basePointer = &store_1->baseInstance;\nstore_2->b = 20;\n524\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 558,
      "chapter": null,
      "content": "There is no way you can store the pointer value in store_2->basePointer,\nbecause it references part of another object and not an entire row in a database table.\nA solution to this issue is to store instances of C++ objects the same way you store\npointers. You store the instance in the table corresponding to its class, make a varchar\nfield in the table corresponding to the object containing the instance, and write the\nstring containing the primary key and the tablename in the field. Here’s an example\nusing the same objects presented previously:\nCREATE TABLE \"Base\" \n(\n\"_Identifier\" serial,\n\"a\" integer\n);\nCREATE TABLE \"StoreInDatabase\" \n(\n\"_Identifier\" serial,\n\"baseInstance\" varchar\n);\nCREATE TABLE \"AnotherToStoreInDatabase\" \n(\n\"_Identifier\" serial,\n\"basePointer\" varchar,\n\"b\" integer\n);\nAnd the result of storing the objects will look like this:\nTable Base:\n_Identifier      a\n1                10\nTable StoreInDatabase:\n_Identifier      baseInstance\n1                \"(1,Base)\"\nTable AnotherToStoreInDatabase:\n_Identifier      basePointer      b\n1               \"(1,Base)\"       20\nArrays\nBecause an array is a data type supported by the database, you can apply the same\nrules used for the previous types, but store them in an array in the field. For a scalar it\nwill simply be an array of scalars; for a pointer, it’s an array of varchar, and so on.\n7.2\nSerializing C++ Objects into a Database Using Introspection\n525\n",
      "content_length": 1427,
      "extraction_method": "Direct"
    },
    {
      "page_number": 559,
      "chapter": null,
      "content": "Creating the Tables\nBy using the MetaType instance of each class, you can generate the tables in the data-\nbase. You generate a string containing the SQL statement to create the table using the\nfollowing pseudocode:\nprocedure AddTypeToDatabase( meta_type )\nbegin\nif meta_type.HasParent()\nif NOT(TypeExistsInDatabase( meta_type.parentClassName )\nAddTypeToDatabase( GetMetaType( meta_type.parentClassName ) )\nendif\nendif\nsql_statement =  \"CREATE TABLE\" + meta_type.className\nsql_statement +=  \"_Identifier serial,\"\nforeach attribute in meta_type.attributeTable \nsql_statement += GenerateCreateAttributeStatement( attribute )\nendfor\nif meta_type.HasParent()\nsql_statement += \"_Parent \" + meta_type.parentClassName \nendif\nExecuteSqlStatement( sql_statement )\nend\nIn this pseudocode, GenerateCreateAttributeStatement generates the part of the\nSQL statement needed to create the field corresponding to the kind of attribute. For\na string attribute called myText, it will generate something along the lines of \"myText\nvarchar\". The actual code will need to take some more things into account, such as\ngenerating the commas between the field declarations and generating the right quotes\nin the SQL statement so there are no case issues.\nOne thing to take into consideration when executing the generated SQL state-\nment is that you need to make sure to create the tables for the base classes before those\nof the subclasses. Otherwise the database will give an error stating that the _Parent\nfield has been declared with an unknown type, hence the TypeExistsInDatabase test\nin the beginning of the procedure.\nThe corresponding code on the CD-ROM demo can be found in the method\ncalled:\nbool DatabaseManager::CreateTable( const MetaType & meta_type )\n526\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 1789,
      "extraction_method": "Direct"
    },
    {
      "page_number": 560,
      "chapter": null,
      "content": "Storing an Object\nStoring an object happens in several phases. First, you get a new primary key value for\nthe corresponding table. Next, you insert the object in the table, which is completely\nempty except for its primary key value. Finally, you update the object in the table, fill-\ning in its attributes. Why all this fuss? Why not just insert the object in the table, have\nthe database autogenerate the primary key value, and be done with it? The reason will\nbecome clear when you go to retrieve objects. \nRemember that each instance of an object in a database table is uniquely identi-\nfied by its primary key value. On the C++ side, each instance of an object of a certain\ntype is uniquely identified by its memory address. Imagine running your application\nand having one object in a table in your database. You ask the database system to give\nyou that object. The program will execute a query, will receive the contents of the\nfields, construct a new instance in memory of the C++ object, and return you its\npointer. So far, so good.\nNow somewhere down the line, you execute exactly the same query. If this sce-\nnario repeats itself, you are in trouble, because the system will actually create a second\ninstance in memory of an object that exists only once in the database. Instead, the\ndatabase manager should return the pointer to the same instance created before.\nA way to solve this issue is by having an instance table inside the database man-\nager. This cache stores the relationship between a primary key value, a tablename, and\nan instance of an object. What will happen the first time you retrieve the object from\nthe database? The system will instantiate the C++ object, fill in its values, and store\nthe pointer to the object, its tablename, and its primary key value in the cache. The\nnext time you ask for a certain object, the database system will take the primary keys\nit receives from the database and match those with the tablename in the cache. If the\nobject already exists in the cache, it will return the stored pointer instead of creating a\nnew instance.\nBut what has this got to do with storing the object? Well, imagine this scenario.\nThe program inserts an object in the database, and somewhere later on tries to\nretrieve it. The original object that was inserted still exists in memory. Here, the data-\nbase system should return the pointer to the original object, and not create a new\ninstance, so at insertion time the object should be added to the cache too. Because you\nneed the primary key value to store the object, you need to retrieve this up front. If\nyou were to insert the object there is no way of retrieving its primary key value by any\nrobust method. Even a SQL SELECT with a WHERE clause of all the attribute values of\nthe object will not be robust, because there could be multiple identical objects in the\ndatabase.\n7.2\nSerializing C++ Objects into a Database Using Introspection\n527\n",
      "content_length": 2931,
      "extraction_method": "Direct"
    },
    {
      "page_number": 561,
      "chapter": null,
      "content": "This is why you must retrieve the primary key value explicitly. But why insert an\nempty object first, and update it afterward? This is purely for code simplicity. Because\nthe SQL syntax for insert and update commands is different, it requires less code if\nyou can use the same codepath for insertion of new objects and updates of existing\nobjects.\nTo store an object in the database, you execute the following pseudocode:\nprocedure StoreObjectInDatabase(object, meta_type )\nbegin\nBeginTransaction();\nInsertObjectAndAttributePrimaryKeys( object, meta_type );\nUpdateObject( object, meta_type );\nEndTransaction();\nend\nSomething important to note are the calls to BeginTransaction and EndTransac-\ntion. Because you execute multiple consecutive SQL statements, it is very important\nto keep the database in a consistent state at all times. Using transactions allows you to\nroll back the database if between a BeginTransaction and EndTransaction something\nwas to happen that crashed the application. Imagine having your application crash\nright after inserting the empty object only containing a primary key value. Next time\nyou use the application there will be corrupt data in the database. Guarding these\nblocks of statements with a transaction block will make sure none of the statements\nexecuted will be permanently stored in the database until EndTransaction is called.\nLet’s take a closer look at the important parts of the two functions used to store\nthe object. The first function is InsertObjectAndAttributePrimaryKeys. Inside this\nfunction, you execute the following steps:\n• If the object already exists in the instance table, or if it is a native database type\n(for example, integer, string, and so on), then return.\n• Iterate over all attributes of the object and its ancestors and call InsertObject\nAndAttributePrimaryKeys on each of them.\n• Check if the table corresponding to the class of the object actually exists in the\ndatabase. If not, create it.\n• Finally, at this point in the function, all the attributes of the object and its ances-\ntors have been processed (recursively), so now you generate a primary key value\nfor the object itself and insert the object in the database with this value.\nThe second function is UpdateObject. Here, you have the following steps:\n• If the object is a native database type, return.\n• Iterate over all attributes of the object and its ancestors and call UpdateObject on\neach of them.\n• Create and execute the SQL statement to update the object’s contents.\n528\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 2554,
      "extraction_method": "Direct"
    },
    {
      "page_number": 562,
      "chapter": null,
      "content": "Updating the Object’s Contents\nGenerating an SQL UPDATE statement consists of four parts. First is the name of the\ntable you want to update, next is the list of the field names you want to update, after\nthat are the field values you want to store, and finally the condition that decides what\nrows are getting updated.\nSelecting the name of the table you are going to update is quite easy; the table has\nthe same name as the classname stored in the metatype of the object. The condition\npart of the statement is also straightforward. You use the primary key value corre-\nsponding to the object pointer. This value can be retrieved from the instance table.\nThe interesting part is generating the field names and values. Let’s start by look-\ning at how to generate those field names.\nThe data in a C++ class consists of a group of attributes found in the class and its\nancestors. When creating the list of field names, start with the attributes in the class\nitself. This is quite simple as the field name is the same as the attribute name. For the\nparent class, you store the complete contents in a field called _Parent. Attributes in\nthat field can be referenced just like accessing a member of a class in C++, by writing\nit in the form _Parent.attributeName. If the parent class has a parent class containing\nattributes, you can access those fields in a similar way, by doing _Parent._Parent.\nattributeName. So, if you want to generate the list of field names for the SQL state-\nment for the ancestors, you iterate over each ancestor’s attributes, write out the names,\nand prefix each name with one or more _Parent. strings. For each level you go up in\nthe hierarchy, you add one extra _Parent. string in front of the attribute name.\nFor example:\nclass Base\n{\nint a;\n};\nclass Subclass : public Base\n{\nint b;\n};\nClass SubSubclass : public Subclass\n{\nint c;\n};\nGenerating the names of the attributes of class SubSubclass results in the following:\nc\n_Parent.b\n_Parent._Parent.a\n7.2\nSerializing C++ Objects into a Database Using Introspection\n529\n",
      "content_length": 2042,
      "extraction_method": "Direct"
    },
    {
      "page_number": 563,
      "chapter": null,
      "content": "Generating the values for each field should of course happen in the same order as\ngenerating the names; otherwise the data will get mangled up. You iterate over each\nattribute and depending on what type of attribute it is, generate the field value in a\ndifferent way. There are three cases, described in the next three sections.\nNative Database Type\nThis is a scalar or a string. Use the WriteObjectToText function to convert the\nattribute to a string, which you can use in the SQL statement.\nObject or Pointer to an Object\nIn this case, you take the memory address of the object (or of the object pointed to in\nthe case of a pointer), get its metatype, and get the primary key value from the\ninstance table. Remember that the instance table is guaranteed to contain those values\nbecause you generated them before generating the UPDATE statement. With this pri-\nmary key, you construct the string containing the primary key-classname pair, as\ndescribed in the “Pointers” section.\nArray\nWhen encountering an array, you construct a string corresponding to an array repre-\nsentation in SQL. The format is \"{ item1, item2, …,itemN}\". Something you need\nto know when retrieving the object later on is the number of items in the array. You\nsimply store the item count as the first element of the array, for easy retrieval later. To\ngenerate the rest of the string, you take the metatype of the object stored in the array,\niterate over each of the items, take a pointer to the item in the array, and execute the\ngeneration of the attribute value for that item.\nIf the array contains pointers to objects, you can iterate over those using the fol-\nlowing code:\nitem_address = *( array_data + sizeof( void * ) * item_index )\nWhere array_data is the start of the array buffer and item_index is the index of\nthe item you want to use in the array.\nIf the array contains instances of objects, the code looks like this:\nitem_address = array_data + item_meta_type.GetByteCount() * item_index\nIn this case item_meta_type is the metatype of the items in the array (in a typical\ntemplated array class it will be the metatype of the template argument type).\nUsing these strings, you can assemble the SQL UPDATE statement and change the\ncontents of the objects in the database.\n530\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 2306,
      "extraction_method": "Direct"
    },
    {
      "page_number": 564,
      "chapter": null,
      "content": "Retrieving an Object\nAs with insertion, object retrieval happens in two steps. First, you execute a SELECT\nstatement, which returns a list of primary key values. Next, for each primary key\nvalue, you check whether the object already exists in the instance table. If it does, you\nreturn its pointer. If it does not exist, you build a SELECT statement to fetch the field\nvalues. The result of this query is used to fill in the attributes of the object you want\nto retrieve. In pseudocode, it looks like this:\nprocedure GetObjectsFromDatabase( object_table, meta_type, predicate )\nbegin\nkey_table = GetAllPrimaryKeysCorrespondingToPredicate( predicate )\nforeach key in key_table do\nif HasObjectInInstanceTable( key, meta_type )\nobject_table.Add( GetObjectFromInstanceTable( key, \nmeta_type ) )\nelse\nobject_table.Add( CreateNewObject( key, meta_type ) )\nendif\nendfor\nend\npredicate contains the filter applied to the query, meta_type is the metatype of\nthe class you want to retrieve instances from, and object_table is the table that will\ncontain the result of the query, a list of pointers to the instances.\nIn the case of creating a new object, you use its metatype to construct a new\ninstance. The MetaType class has a method called CreateNewInstance to accomplish\nthis.\nNext, you generate the SELECT statement. As is the case for the UPDATE statement,\nyou generate a list of the field names of the attributes of the class and its ancestors,\nwhich will be used to specify the field values you want to recover and the order in\nwhich they will be recovered. When iterating through the result of this query, you\nneed to consider the different types of attributes you’ll encounter.\nNative Database Type\nReadObjectFromText is used to convert the string version of the value into the scalar or\nstring value of the attribute.\nPointer to an Object\nYou retrieve a primary key + classname pair in a string. First, check whether an object\ncorresponding to this pair already exists in the instance table. If it does, get its address\nand store it in the pointer attribute. If it does not, create a new instance, execute the\ncode to retrieve that object’s contents from the database, and store the new instance’s\naddress in the pointer.\n7.2\nSerializing C++ Objects into a Database Using Introspection\n531\n",
      "content_length": 2290,
      "extraction_method": "Direct"
    },
    {
      "page_number": 565,
      "chapter": null,
      "content": "Object\nAs in the previous case, you retrieve a primary key + classname pair in a string.\nBecause this is an attribute, you do not have its address in the instance table. You add\nthe address in the instance table, together with its primary key value and metatype,\nexecute a SELECT statement to retrieve its attribute’s values, and fill in its contents.\nArray\nAn array is represented by a string of the format \"{ item_count, item1, item2,\n…,itemN}\". Using some string manipulation, you get the item_count of the array. You\ntake the pointer to the attribute, cast it into a pointer to an object of type BaseArray,\nand set its item_count. Next, you get the pointer to the data and iterate over each of\nits elements. Using the item’s metatype you get from the BaseArray class, you can cat-\negorize the elements in the array (native database type, pointer to an object, or\nobject). For each of those elements, you get its address and its string representation\nfound in the array text received from the database, and use this to fill in the element\nvalue.\nThe Demo\nThe demo is not supposed to represent an industrial-strength full-fledged database\nserialization implementation, but more of a proof of concept, where all the code that’s\nnot directly related to this gem has been kept to a minimum for the sake of clarity. In\norder to use this demo, you first need to do some things, because it requires a running\nPostgreSQL server. There is a version of PostgreSQL included with the demo, so if\nyou do not have one on your machine you can use this. Install it with the default\noptions, and when asked to create a database superuser with the name “postgres,” use\ngem as your password. Once you do this, the database will be ready for use and you\ncan run the test application.\nJust running the application will not show much, but if it runs without errors you\nknow the application is functioning correctly. To understand what is going on in the\ndatabase system, you are encouraged to trace through the code. Just by following the\nsteps executed in the main function of the application, you can see which kinds of\nobjects are created, inserted in, and retrieved from the database. There are some tests\nafter object retrieval that verify that the object returned is correct.\nUsing pgAdmin, you can look at the tables created and the way the objects are\nstored in the database.\nIssues and Future Improvements\nAs with most things in life, the system presented here is far from perfect and there are\na number of improvements that can be made to it. \n532\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 2584,
      "extraction_method": "Direct"
    },
    {
      "page_number": 566,
      "chapter": null,
      "content": "There are things you can do in a C++ object that are not supported by this sys-\ntem. For example, storing a pointer to a string, a pointer to an integer, or any other\nnative database type. These issues can all be solved, but are beyond the scope of this\ngem.\nCurrently, the system supports pointers to objects stored as attributes in a class\n(the demo has an example of this); however, this will not work if the attribute pointed\nto will be processed by the database after encountering the pointer that points to the\nattribute. This issue can be solved by implementing a two-pass approach, where first\nall attribute addresses are stored in a table before beginning the serialization process.\nMemory management has not been touched upon. Imagine having a pointer to\nan object stored in the database instance table, but the application has already deleted\nthis object, creating a dangling pointer. Using smart pointers and reference counting\nis one of the ways to address this issue.\nOne of the strengths of using a database is that you can execute queries with pred-\nicates. In this demo, the database manager supports only simple filters that work\ndirectly on an attribute of the class you are trying to retrieve (for example, \"a =  5\").\nThere is still a lot of room for improvement. You could add functionality that allows\nfor more complicated filters or supports filters in a more human-readable format,\nwhich would then be converted to an SQL statement internally.\nWhen working on a centralized database with many users, one of the challenges\nis to keep every user’s local view synchronized with the database contents. One of the\nimprovements to this system is to implement some kind of notification mechanism\nthat notifies the database manager of user X when user Y modifies some content that’s\nrelevant to user X.\nConclusion\nThis gem presents a way of storing C++ objects in a database using a metadata system.\nThe mechanism is non-intrusive, meaning that there are no changes needed to classes\nwhose objects need to be stored. There are several advantages to this kind of approach.\nYou can store many small objects in a database and quickly retrieve the ones you’re\ninterested in, without having to read a lot of files on disk. Because the database is cen-\ntralized, there can be multiple users accessing the same data, adding new objects, read-\ning them and modifying them, and a database system has all the mechanisms needed\nto work with concurrent access, something that’s much more difficult to accomplish\nusing regular files on a shared network drive, especially when dealing with large\namounts of relatively small data. It also allows any C++ programmer to store objects\nin a database and retrieve them, without needing any knowledge of SQL, because all\nthe implementation details are hidden inside the database manager.\n7.2\nSerializing C++ Objects into a Database Using Introspection\n533\n",
      "content_length": 2901,
      "extraction_method": "Direct"
    },
    {
      "page_number": 567,
      "chapter": null,
      "content": "References\n[Abrahams06] Abrahams, D., and Gurtovoy, A. C++ Template Metaprogramming:\nConcepts, Tools and Techniques from Boost and Beyond, Addison-Wesley Profes-\nsional, 2004.\n[Postgresql06] The PostgreSQL Global Development Group. “PostgreSQL 8.2.4\nDocumentation,” available online at http://www.postgresql.org/docs/8.2/static/\nindex.html, 2006.\n534\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 396,
      "extraction_method": "Direct"
    },
    {
      "page_number": 568,
      "chapter": null,
      "content": "535\n7.3\nDataports\nMartin Linklater\nmslinklater@mac.com\nO\nne of the apparent laws of game programming is that as game projects grow in size\nthey become more complex and more difficult to manage. Because games are\nobviously getting bigger, game programmers have to deal with increasing amounts of\ncomplexity. There are two ways to manage this complexity—you either work harder\nand longer or you create better systems to manage the complexity. I for one would\nrather go for the second option. One aspect of this complexity is managing how data is\nrouted through the various systems present in the game code. Code modules commu-\nnicate by passing data around between themselves and exposing certain parts of their\ninternal data to their modules. This data needs to be stored in a format that each of the\nmodules involved can understand. The need for common knowledge shared between\nmodules creates both runtime and compile-time dependencies and more dependencies\nmeans more complex code structure and longer compilation times. Dataports are a\nway of helping to manage this complexity by reducing compile-time dependencies and\nmaking the runtime behavior more flexible and data-driven.\nThere are two basic ways of controlling communication between code modules—\nyou either code it up, binding pointers to data explicitly in the source code, or you\ncreate a data-driven system and define the data linkage by loading and parsing exter-\nnal linkage definition files. Coding behavior explicitly suffers from two main disad-\nvantages—first you have to rebuild your code whenever you change data connections,\nand second, because the behavior is explicitly encoded in the executable, it can be\nproblematic to extend or alter the behavior at runtime or post release. Dataports are a\ntool to help you create a more dynamic and data-driven flow in your programs.\nConceptual Overview\nConceptually, dataports are very simple. A dataport is a piece of data that has a unique\nglobal identity. This data can be a structure, a class, or a simple C++ data type. Once\nthey are created, dataports register their identity with a manager class. Code elsewhere\nin the program can then get access to the dataport by creating a dataport pointer and\nasking the manager class to bind it to the desired dataport. There are various nuances\nto the implementation, but the basic pattern that you need to visualize is that of data\nstructures and pointers.\n",
      "content_length": 2420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 569,
      "chapter": null,
      "content": "The Dataport\nThe dataport itself is just a template wrapper for a programmer-defined piece of data.\nThere are only a couple of basic methods of a dataport, as follows:\nvoid Register( std::string ID );\nOnce created, the dataport needs to register its identity. Once registered, the data\nis public and available for other pieces of code to interrogate.\nvoid DeRegister( void );\nCalling DeRegister removes the dataport from public view.\nThe Dataport Pointer\nThe dataport pointer is in most respects a traditional pointer. The difference is that\nthe actual binding of pointer to data is done by the Dataport Manager, rather than\nbeing statically defined by source code and bound by the linker. The two methods of\ndataport pointers are as follows:\nDataport<T>* Attach( std::string );\nThis call asks the Dataport Manager to attach your dataport pointer to your\nrequested dataport. This call returns the pointer to the dataport object, so the actual\nline of C++ is as follows:\npDataport = pDataportMgr->Attach( \"Dataport ID\" );\nTo detach a dataport pointer from the data it points to, you need to call the\nfollowing:\nDetach();\nThe return value tells you whether this was successful or whether an error occurred.\nAfter you have attached a dataport pointer to a dataport, you access the data con-\ntained by using the data member.\npDataport->data.<member variable>\nThe Dataport Manager\nThe Dataport Manager is the hidden backbone to the dataport system. At its heart,\nthe Dataport Manager is a storage and retrieval system containing a list of dataports\nthat have been registered. The Dataport Manager deals with pointer binding and\nmanages reference counting. It is worth thinking a little about the implementation of\nthe Dataport Manager because you need to optimize the internal algorithms to suit\nyour application’s usage patterns.\n536\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 1875,
      "extraction_method": "Direct"
    },
    {
      "page_number": 570,
      "chapter": null,
      "content": "If you will be creating and deleting dataports rapidly and binding infrequently,\nyou need a representation that has good create and delete performance, but that does\nnot necessarily have good search performance. On the other hand, if you create and\ndelete dataports infrequently and bind pointers often, you might need a representa-\ntion that has a fast searching performance compared to create and delete performance.\nBecause this example uses C++ for this particular implementation, it capitalizes\non the STL library and uses an STL <list> as the internal storage mechanism. I rec-\nommend this as a general and easy solution unless your profiling later shows that you\nneed a customized solution. The STL library was written with runtime performance\nas a primary aim, and it’s a shame not to use tested and robust code that’s already been\nwritten.\nThe Dataport Manager is a singleton class, meaning that there is always only one\ninstance in memory at runtime. This is because dataports have a universal unique\nidentity and, although it is possible, there is little benefit to be gained by running\nmultiple Dataport Managers in parallel.\nType Safety\nThe first time I implemented a dataport system, I didn’t have any form of type check-\ning in place. It didn’t take long before I refactored the code to include type checking,\nbecause it was entirely possible to bind one type of data to a pointer of a different\ntype. This is certain to introduce some difficult-to-track-down bugs in your code.\nImplementing type safety is definitely a good thing. The code contains a handy C++\ntemplate function (GetID<T>) which, when given a class, returns a unique 32-bit\nnumber identifying that class. It is essentially a pointer to an instance of a static class\nfunction. This ID is used by the Dataport Manager to prevent name collisions\nbetween different types.\nReference Counting\nWhenever you work with data and pointers, there is a danger of a pointer that was\nonce valid becoming invalid for some reason. These hanging pointers can be very diffi-\ncult to track down because the ensuing crash might happen a long time after the data\nhas become invalid. Ideally, you should not be able to make data invalid if there are\nstill pointers pointing to it.\nDataports use reference counting to help debug problems like these. Although\nnot strictly required for functionality, reference counting is a huge help in debugging\npotential problems.\nThe dataport template defines an m_refCount member that all dataports inherit:\n• When the dataport is registered, this reference count is set to zero.\n• When dataport pointers attach to a dataport, its reference count is incremented\nby one.\n7.3\nDataports \n537\n",
      "content_length": 2686,
      "extraction_method": "Direct"
    },
    {
      "page_number": 571,
      "chapter": null,
      "content": "• When dataport pointers detach from a dataport, its reference count is decremented\nby one.\n• When a dataport is de-registered, its reference count is checked. If it does not\nequal zero, there has been a mismatch somewhere and an error is returned \n(kErrorNonZeroRefCount).\nDataport reference counting is a great help with debugging, but it does incur a\nsmall runtime performance penalty. You should consider removing reference counting\nfrom your final release builds to remove this performance penalty. As long as you keep\nreference counting in your debug builds, you will gain the extra debugging informa-\ntion that reference counting gives.\nPractical Examples\nI have been using dataports since 2000 and they have proven to be a very useful addi-\ntion to my programming toolkit. The following sections explain a few examples of\nhow I have used dataports in the past.\nCamera Systems\nIt is very useful to encode a level of abstraction into the camera system in a game. I\nabstract the camera system into tripods and cameras. Tripods are classed as camera\n“attach points” and can be placed in the scene at will. The player controller object has\nmultiple tripods, and things like the debug fly cam have a tripod. These tripods are\ncreated as dataports. The actual cameras that drive the renderer have a tripod dataport\npointer as a member variable. To move a camera to a new location, the camera’s tripod\ndataport pointer was simply detached and reattached to a different tripod.\nOnce this system is in place it is very easy for people on the team to create new\ntripod attach points and attach the cameras to them at runtime. Because all of this can\nbe driven by human readable text identifiers, a great deal of flexibility is added to the\ncode. Once the tripod and camera classes are set up, there is little or no maintenance\nneeded to introduce new camera viewpoints.\nShip Handling Debug Values\nThe handling stats for the ships in both Quantum Redshift and Wipeout Pure were\nheld in human readable XML file format. The filenames for these files included the\nteam names that were associated with the statistics. On program boot these files were\nloaded and given dataport names derived from their filenames. Then, when ships\nwere created in-game, they could bind with their corresponding handling statistics\nvery easily. New teams could be added to the code without having to explicitly add\nextra bindings to their handling statistics. This simple data-driven model simplified\nthe task for both programmer and designer.\n538\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 2566,
      "extraction_method": "Direct"
    },
    {
      "page_number": 572,
      "chapter": null,
      "content": "Broadcasting Positional Information\nOne of the most infuriating things when coding game logic is getting hold of data\nburied inside different classes. Drilling into game class data while maintaining object-\norientated encapsulation and data access rights can become a tricky engineering job in\nitself. Commonly accessed game object data can be wrapped in a dataport and exposed\nto the rest of the game engine in a very simple manner. Rather than trying to memo-\nrize how you navigate through your class hierarchy to get at data, you just need to\nknow its type and its ID, and then let the Dataport Manager find it for you. Things\nlike game object positional information can be wrapped in dataports for easy access by\nother systems.\nIn the past, I have also set up HUD dataports so the in-game HUD can be\ndynamically driven by different game objects very easily. Once you have your dataport\nstructures locked down and you decide on a sensible ID scheme, you can get hold of\ndata very easily.\nProblems\nDataports are certainly not the silver bullet that will make your code easy to use and\nbring about world peace. Sadly, they also have some drawbacks.\nIf you use any form of hashing in your Dataport Manager, it is entirely possible\nthat you will get hash collisions. You can mitigate this problem a little by including\nthe dataport type into your storage, but you will have to deal with hash clashes in a\nsensible manner. In the example code, I don’t use hashing at all, but for performance\nreasons you might want to introduce hashing into your release builds.\nDataports are harder to debug. By their very nature, dataports introduce a level of\ndynamism and freedom into your data binding that you may find makes debugging\nharder. You can mitigate this difficulty by introducing more debug and logging code\ninto the dataport system, but you are going to have to live with the idea that you are\nmaking the code harder to debug.\nHeavy use of a dataport system can introduce some fairly substantial performance\npenalties. Dataports are not meant to be a direct replacement for all your pointer\nusage, and you need to balance the need for flexibility against the added CPU over-\nhead needed to create, delete, and bind dataports with dataport pointers.\nMy personal choice is to use dataports for commonly accessed game elements that\nneed to have global scope and need to be dynamically accessed by multiple code mod-\nules. As long as you are sensible you shouldn’t see dataports hit your frame rate at all.\nIn fact, I have yet to see dataport operations show up during performance measure-\nment with final game code.\n7.3\nDataports \n539\n",
      "content_length": 2631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 573,
      "chapter": null,
      "content": "Conclusion\nThe current implementation does not encode any sort of access behavior into the\ndataport system—all dataports are read/write and have global access. People used to\nconst pointers might feel decidedly uneasy about this freedom, and might want to\nadd “const-ness” into the dataport API. So far, I have not yet felt the need to add this\nto my implementation, but I can appreciate the desire for that extra layer of support\nthat const pointers can give the API.\n540\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 518,
      "extraction_method": "Direct"
    },
    {
      "page_number": 574,
      "chapter": null,
      "content": "541\n7.4\nSupport Your Local Artist:\nAdding Shaders to Your\nEngine\nCurtiss Murphy; Alion Science \nand Technology\ncmmurphy@alionscience.com\nR\necent advances in hardware have made shaders an essential part of visually com-\npelling games. There are now dozens of books, including this one, filled with\nthousands of shader techniques and best practice examples. Your team is raring to go.\nSo what now? How are you going to integrate shaders into your engine? As a devel-\noper, it is tempting to just code the shaders directly into your actors. Unfortunately\nthat approach leads to a dangerous coupling of code, art assets, and shader parameters\nas well as a hard-coded, inflexible solution that is averse to scaling. So, what are you\ngoing to do? This gem is here to help.\nFollowing is a data-driven design to help you incorporate shaders into your\nengine. This design presents good techniques for isolating most shader parameters\nfrom your actor logic. It provides support for simple parameters such as floats and\nintegers as well as more complex parameters such as textures and automatic oscillating\nvalues. The resulting implementation allows artists and level designers to define\nshaders and parameters in XML with little programmer involvement. An example use\ncase shows a blimp that hovers in three dimensions and applies an animated pink\nhighlight; the example is integrated into the engine with zero lines of code. This gem\nincludes a fully working implementation that can be used as the basis of a more com-\nplete solution. \nShader Terminology\nThere are several terms that often confuse newcomers to shader development. The\nfirst is the basic definition of shader itself. Originally, shaders got their name because\nof the way pixels were shaded. Now, there are two kinds of shaders—the vertex shader\ncan manipulate vertices and the geometry shader can manipulate an entire model. \n",
      "content_length": 1884,
      "extraction_method": "Direct"
    },
    {
      "page_number": 575,
      "chapter": null,
      "content": "A better term is processor. After all, shaders can process all sorts of data and are used\nfor much more than just shading. However, the term shader has become the standard\nand will be used by this gem. \nAnother source of confusion is use of the terms fragment shader and pixel shader.\nThey sound like they should be different, but in actuality they are the same. The term\nfragment means that the shader is only computing a “potential” pixel. That is, a pixel\nthat is computed as part of the graphics pipeline but that might not become part of the\nfinal frame buffer. So, for example, “per-pixel-lighting” is really “per-fragment-lighting.”\nIn an ideal world all fragments would become real pixels—there would be no over-\ndraw—so you would only need one term. However, in practice, it is more efficient for\nthe GPU to compute extra fragments than it is to perfectly isolate each pixel. \nThis gem presents general concepts that can be used in both OpenGL and\nDirectX. Because they are both state-driven, the mechanics and concepts are easily\ntransferable. State refers to the entire rendering pipeline, including bound shaders,\nrender states, bound textures, and so on. Mesh refers to any object, geometry, node, or\nprimitive that can be drawn with a single state. For simplicity, this gem generally uses\nOpenGL terms and the OpenGL Shading Language (GLSL). The example applica-\ntion is built using OpenSceneGraph (see http://www.openscenegraph.org) within the\nDelta3D Open Source Game Engine (see http://www.delta3d.org). \nPrograms and Parameters and Managers, Oh My!\nThe primary classes of this solution are the ShaderProgram, the ShaderParameter, and\nthe ShaderManager. This section describes the general purpose of these three classes.\nThe ShaderProgram Class\nThe ShaderProgram class is the heart of this solution. It corresponds directly to the\nconcept of a program (in OpenGL) or effect (in DirectX). The program is what most\npeople mean when they refer to a shader. It is the compiled executable associated with\nthe vertex and fragment shaders. The ShaderProgram class holds onto the actual\nOpenGL program. A program can have a vertex shader, a fragment shader, or both.\nHowever, the most important job of the ShaderProgram class is to hold a list of the\nshader parameters. \nThe ShaderParameter Class\nThe ShaderParameter class holds onto a single uniform variable. Each parameter\nvalue is bound directly to a mesh via its state and is the primary way that an applica-\ntion communicates with shaders. A parameter can be a base color highlight, a gloss\ntexture, a blur weight, an offset point, particle density, alpha strength, or almost any\ntype of value that you want to pass into the shader. The parameters are uniform\nbecause they stay the same for every vertex and pixel drawn by that node during a\nsingle frame. Most parameters are simple data types such as float, int, vec3, and\n542\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 2938,
      "extraction_method": "Direct"
    },
    {
      "page_number": 576,
      "chapter": null,
      "content": "texture2D that are used to affect shader output. This architecture also supports com-\nplex parameters that have their own behavior, as seen in the time-based oscillating\nparameter (explained in a later section).\nThe ShaderManager Class\nShaderManager is the class that holds this architecture together. It is responsible for\nloading shader prototypes from the XML definition file, assigning and unassigning\nthe parameters to a state, tracking the assigned shaders, and managing a cache of com-\npiled programs. The manager is what you use to find a shader program and assign it\nto the meshes in your game engine. This class is based on the Singleton Design Pat-\ntern [Gamma95].\nTo put it all together, the manager holds onto the programs and the programs\nhold onto the parameters. Together, the three classes appear as shown in Figure 7.4.1.\n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine\n543\nFIGURE 7.4.1\nThe ShaderManager class diagram.\nFlexibility Is Key\nWhy go through all this trouble? After all, if you are already coding the behavior of\nyour actor, why not just add the shader code directly? The answer is flexibility. Games\nare now vast, complicated software behemoths [Blow04]. They require huge teams of\nartists, designers, and programmers working in concert. In fact, it is becoming\nincreasingly common to have more artists than programmers. In such an environ-\nment, it is critical to ensure the pipeline is as smooth as possible. Artists need to be\nable to test models and shader effects without having to involve a programmer. Shader\ndevelopers need to be able to edit the parameters of a shader, or even add an entirely\nnew shader without having to edit or recompile code. This design provides a flexible\nsystem that meets those needs and helps remove the dependency between program-\nmers and artists. \n",
      "content_length": 1830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 577,
      "chapter": null,
      "content": "This architecture is especially helpful in allowing artists to visualize their changes\nlive within the real engine. In most studios, the art pipeline involves a suite of tools\nthat is outside of the actual engine. Often, models are created in one tool, textures in\nanother, and shader code in yet a third. This means that what the artist sees while cre-\nating an asset is disjoint from what a player will see in the actual engine. Sometimes,\nthere are additional tools to preview the asset combined with the shader to make it as\nclose to real as possible. However, regardless of how good the tool, it’s never going to\nbe more than just an approximation unless it is viewed live, in the actual game, with\nactual actors, lights, weapons, shadows, and cameras. There is simply no replacement\nfor seeing the final, combined result. Getting this level of realism was a primary moti-\nvation for the data-driven nature of this architecture. Because the shaders are easy to\ndefine and integrate, artists can test their assets in the real engine almost immediately,\nand they can do it with little programmer involvement.\nTest Mode\nAnother aspect of this solution is the ability to dynamically reload shaders at runtime.\nShader development is an art. As such, it can take hundreds of iterations to get one\n“just right.” Maybe the lighting is too bright, the fog decays too quickly, or the gloss\nhighlights are too sharp. Fortunately, the data-driven nature of this architecture makes\nit easy to reload all the shaders in the system at any time. The ShaderManager knows\nwhenever a new shader is created, keeps a list of all active programs, and has access to\nall parameters. It has everything it needs to reload the shader definition XML and\nsystematically replace existing programs and parameters with the updated values. \nThis behavior is provided by the ReloadAndReassignShaderDefinitions() method\non ShaderManager. With a single key press, the new shader is loaded and the artist can\nimmediately visualize their tweaks in the real engine. Whether using this or some\nother shader system, engine programmers should do everything in their power to\nintroduce an in-game test system that will allow artists and designers to reload shaders\nat runtime without restarting. \nPrototypes\nThis architecture is based on the Prototype Design Pattern [Gamma95]. As a refresher,\na prototype is a prototypical instance that is copied to create a new object. In this case,\nthe ShaderProgram that is loaded from the XML file is really just metadata that is not\napplied to an actual mesh. When the manager reads the definition file, it creates new\ninstances of ShaderProgram and adds them to its list of prototypes. The manager then\nloads the shader source code and parameter variables for each instance. Once complete,\nit precompiles the shaders into a program and adds it to a cache.\nThe prototypes should be loaded and compiled when loading a map or at startup\ntime. That way there is no spike in CPU work when applying a shader to a new object\n544\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 3070,
      "extraction_method": "Direct"
    },
    {
      "page_number": 578,
      "chapter": null,
      "content": "in the middle of the game. This design is further optimized so that any unique shader\ncombination (vertex plus fragment) should be compiled only once. To do this, the\nmanager looks for programs that can be shared across prototypes and stores them in\nmCachedPrograms. Essentially, if two prototypes use the same vertex and fragment\nsource code and only differ by the values of the assigned parameters, they both use the\nsame cached program. For instance, this is used to specify unique prototypes that only\ndiffer by a gloss map texture based on the vehicle type. \nNotice that most of the methods on ShaderManager have the word “prototype” in\nthem. That is because there are only two times when the manager is working with an\nactual instance instead of a prototype—first, when assigning a prototype shader to a\nmesh and second, when unassigning (that is, destroying) a shader instance from a\nmesh. In the first case (for example, AssignShaderFromPrototype()), the manager\nclones the prototype to create a unique shader instance. To support this, both Shader-\nProgram and ShaderParameter provide a Clone() method. The program is a fairly\nlight class, so when it is cloned, it simply grabs a few references and sets a few strings.\nThen it makes clones of all the parameters and adds them to its parameter map. For\neach new parameter, it calls AttachToState(). This method binds the parameter to\nthe actual mesh. Finally, the manager adds the new program instance to a list called\nmAssignedNodes.\nThe second case is much simpler. The UnassignShaderFromNode() method\nensures that each parameter is unbound from the state by calling DetachFromState()\nand then removes the shader instance from the active list. Because the implementa-\ntion uses smart pointers, all objects are cleaned up correctly. The whole process of\nassigning and unassigning results in a unique program instance whose parameters are\npart of a specific mesh’s state. The program is precompiled and shared as a prototype,\nbut applied as an instance. \nState Sets and Scene Graphs\nThis architecture has some added benefits if the engine happens to support scene\ngraphs and state sets. A scene graph is just a hierarchical way of storing the meshes in a\nscene. A state set is a mechanism that allows each mesh to have its own unique state\nvalues and provides the ability to switch between them at draw time. When these two\nideas are combined, you get a hierarchy of meshes that can manage their own state.\nThis type of hierarchy typically allows the values of the state to pass down from par-\nent to child. In other words, each child mesh can set some state values of its own and\ninherit the rest from its parent. The total collection of individual and inherited values\nbecomes the mesh’s active state. \nDuring the draw phase, a shader program is composed from both the compiled\nshader code and the associated parameters. This distinction is exactly analogous to the\ncode block and data block used by the operating system. Typically, in a state-based\nscene graph, the shader program and the parameters are tracked as independent state\n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine\n545\n",
      "content_length": 3159,
      "extraction_method": "Direct"
    },
    {
      "page_number": 579,
      "chapter": null,
      "content": "variables. In other words, you can assign the executable shader program to a mesh\nwith or without setting the parameter variables, and vice versa. \nUsing this gem, you can leverage this type of hierarchy to allow a generic, high-\nlevel shader program to cascade down from the top of the scene to any child that\ndoesn’t have its own shader. This could be used to define various default settings such\nas a lighting model. You could also specify “global” shader uniforms without knowing\nwhich program will eventually use them.\nFor example, you could set up global values for an HDR light modifier, custom\nfog variables, beginning and end values for tunnel vision, or the parameters for fish\neye or water blur effect. Because the values cascade down through the scene graph,\nthey become part of each mesh’s state. This should make it easier to tweak global val-\nues for common render effects and result in less overall management of your shader\nparameters. Note that it is possible to achieve a similar effect in the current design by\nusing the mIsShared flag on ShaderParameter (discussed in a later section).\nShader Parameters\nAfter this general overview of the architecture, you should be ready to visit shader\nparameters in more detail. Before you can do that, you need to take a closer look at\nthe way a shader receives data. Generally speaking, there are three types of variables\nthat are sent to the vertex or fragment shader. In OpenGL, they are referred to as uni-\nform, attribute, and varying parameters:\n• Uniform parameters are values that remain the same across an entire piece of\ngeometry. These values do not change during a frame and often they may not\nchange at all.\n• Attributes are like uniforms in that they don’t change very often, but different\nbecause they are unique to each vertex. That is to say, each vertex can have a dif-\nferent value for each attribute used by a shader. Vertex attributes are typically used\nto pass data such as a vertex normal, vertex color, and tangent-space vector. \n• Varying parameters are computed in the vertex shader and sent down to the frag-\nment shader. The GPU interpolates the values across the surface of a triangle\nusing the outputs sent down from each of the three vertices.\nUniforms ’R Us\nWhich of these types of data should be supported by the ShaderParameter class?\nBecause varying parameters are defined entirely in the vertex and fragment shader and\nare generated by the GPU, they are obviously out. That leaves only two types: uni-\nforms and attributes. Consider attributes first. By definition, an attribute parameter\nhas to be set for every vertex. Because each vertex requires its own value for each\nattribute, it is likely that the artist is going to use a 3D modeling tool to set values\n546\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 2806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 580,
      "chapter": null,
      "content": "such as a vertex normal or a vertex color. Alternately, some attributes may be com-\nputed by the engine, such as the tangent and bi-tangent vectors. In either case, the\nattribute parameters are essential elements of the art pipeline that need to be agreed\nupon by the whole team and integrated into both the art tools and game engine. Con-\nsequently, there’s no reason for ShaderParameter to manage them. So, with varying\nand attributes both out of the picture, you just need to support uniforms. \nFortunately, uniforms are what you want to manage anyway. Uniforms are typi-\ncally used to pass down lights, fog conditions, clipping regions, and so on. Addition-\nally, uniforms are also perfect for sending general customizations over to the shader.\nThey are the best way to pass down textures such as a gloss map, detail map, or bump\nmap. They are the obvious choice for control values such as depth for tunnel vision or\nany value that is time-based. So the goal is to design parameters in such a way that the\nartist can easily define their own uniforms. \nAre You My Type?\nThere are many types of parameters supported by shader languages, including inte-\ngers, floats, textures, and vectors of all different sizes. This means you will need to be\nable to support multiple types. Further, you should provide a mechanism to allow for\nmore complex types. After all, the goal is to eliminate the need for programmer\ninvolvement, so it would be nice if the design supported parameters with built-in\nbehavior. Clearly, ShaderParameter cannot be all of these types at once, so it needs to\nbe a base class. Each specific parameter type then becomes a subclass. The class dia-\ngram for this is shown in Figure 7.4.2.\nThe base class provides base data members such as the actual uniform variable,\nparent program, and whether the shader is currently dirty. It exposes behaviors\nrequired by each parameter type such as the ability to clone itself and the ability to\nattach and detach itself from a mesh’s state. Each subclass then has control over how it\nbinds itself to the state and what type of value it manages. \nFigure 7.4.2 shows simple types such as ShaderParamFloat and ShaderParamInt.\nIt shows data-heavy types such as ShaderParamTexture2D, which has to load an image\nfrom file or cache and bind it to the texture unit. It also shows ShaderParamOscilla-\ntor as an example of a complex data type. This oscillating parameter cycles its value\nbetween a minimum and maximum value over some time. The default behavior\ncycles the uniform from 0 up to 1 and back down to 0 every two seconds. The artist\ncan customize the min/max range values, time interval, offset value, and how the\nvalue oscillates. You can easily expand this system by adding your own custom types. \nClone\nIn order to support the prototype design pattern, each parameter type needs to be able\nto clone itself. Simple types such as int and float merely create a new instance and\nassign the value. \n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine\n547\n",
      "content_length": 3021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 581,
      "chapter": null,
      "content": "Data-heavy types should take extreme care to correctly manage a shared reference\nor other cache mechanism. An early version of the ShaderParamTexture2D failed to\ncorrectly manage instance referencing and brought the system to its knees when it\nallocated over 500MB of duplicate texture data. \nThe cloning process enables a very interesting bit of behavior for sharing parame-\nters. Right before the new parameter instance is created, Clone() checks mIsShared to\nsee if the parameter should be shared between cloned instances. A shared parameter\nessentially acts like a global value for all instances of a shader prototype. So, if mIs-\nShared is false, the method performs as expected by creating a new parameter instance\nand copying the values over appropriately. However, if mIsShared is true, the proto-\n548\nSection 7\nScripting and Data-Driven Systems \nFIGURE 7.4.2\nThe ShaderParameter class diagram.\n",
      "content_length": 903,
      "extraction_method": "Direct"
    },
    {
      "page_number": 582,
      "chapter": null,
      "content": "type’s parameter instance is returned instead. The return value is then added to the\nshader program. The result is that two programs will have exactly the same parameter.\nAlthough this can possibly result in weird values, it also sets up the ability to have\nmultiple objects respond to a single parameter. For example, this would allow multi-\nple objects to oscillate in exactly the same way and would allow all instances of a vehi-\ncle to use the same detail texture map. \nUse Case—The Blimp Target\nTo see how the architecture works, let’s examine a simple use case involving a blimp.\nIn this case, the artist wants the blimp to appear to hover in the air with a slight\nbounce in all three dimensions. The artist also wants to apply an animated swirl effect\nto make it look highlighted. To achieve this effect, the artist creates the blimp model\nand defines the following shaders. \nBlimp Vertex Shader\nThe vertex shader is extremely simple. To create the hover, it needs three instances of\nthe ShaderParamOscillator. It processes the X, Y, and Z dilation values and moves\neach vertex in a sinusoidal pattern, as follows:\nuniform float MoveXDilation;\nuniform float MoveYDilation;\nuniform float MoveZDilation;\n// Vertex - Simple blimp shader for 'Hover' and 'Highlight' \n// Lighting was removed for simplicity\nvoid main()\n{\ngl_TexCoord[0] = gl_TextureMatrix[0] * gl_MultiTexCoord0;\ngl_Vertex.x += 1.5 * sin(3.14159 * MoveXDilation);\ngl_Vertex.y += 1.5 * sin(3.14159 * MoveYDilation);\ngl_Vertex.z += 2.0 * sin(3.14159 * MoveZDilation);\ngl_Position = ftransform();\n}\nNow, let’s take a look at the fragment shader. \nBlimp Fragment Shader\nThe fragment shader is a bit more complex. Take a look at the code first: \nuniform sampler2D DiffuseTexture;\nuniform sampler2D HighlightTexture;\nuniform float TimeDilation;\n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine\n549\n",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 583,
      "chapter": null,
      "content": "// FRAGMENT - Provides highlight by blending from a detail texture\nvoid main()\n{\nfloat whackyOffset = sqrt(abs(TimeDilation - 0.5) + 1.0);\nfloat x = gl_TexCoord[0].x;\nfloat y = gl_TexCoord[0].y;\n// look up the three oscillating colors \nvec2 lookup1 = vec2(x + TimeDilation, y + TimeDilation+.25);\nvec2 lookup2 = vec2(x - whackyOffset, y + whackyOffset);\nvec2 lookup3 = vec2(x - (TimeDilation*2.0), y + TimeDilation);\nvec4 color1 = texture2D(HighlightTexture, lookup1);\nvec4 color2 = texture2D(HighlightTexture, lookup2);\nvec4 color3 = texture2D(HighlightTexture, lookup3);\n// Now blend the three colors together to make the highlight\nvec4 highlightColor;\nhighlightColor.a = 1.0;\nhighlightColor.r = color1.r*0.6 + color2.r*0.3 + color3.r*0.3;\nhighlightColor.g = color1.g*0.2 + color2.g*0.7 + color3.g*0.2;\nhighlightColor.b = color1.b*0.2 + color2.b*0.2 + color3.b*0.8;\n// Finally, blend the original color and highlight color\nvec4 diffuseColor = texture2D(diffuseTexture, gl_TexCoord[0].st);\ngl_FragColor = (0.2 * diffuseColor) + (0.8 * highlightColor);\n}\nThis processor takes in one float uniform and two texture uniforms (Shader-\nParamOscillator and ShaderParamTexture2D, respectively). To create the swirling\nhighlight, it does three separate look ups into the detail texture. Each lookup is a per-\nmutation of the TimeDilation uniform variable. Then, it uses the lookup to compute\na final highlight color and blends that color in with the original diffuse texture. \nTo integrate the new shaders into the engine, the artist adds the following snippet\nto the shader definition XML file:\n<shader name=\"Green\">\n<source type=\"Vertex\">Shaders/green_vert.glsl</source>\n<source type=\"Fragment\">Shaders/green_frag.glsl</source>\n<parameter name=\"diffuseTexture\">\n<texture2D textureUnit=\"0\">\n<source type=\"Auto\"/>\n</texture2D>\n</parameter>\n<parameter name=\"TimeDilation\">\n<oscillator cycletimemin=\"2.0\" cycletimemax=\"4.0\"/>\n</parameter>\n<parameter name=\"MoveXDilation\">\n<oscillator cycletimemin=\"5.0\" cycletimemax=\"8.0\"/>\n</parameter>\n550\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 2076,
      "extraction_method": "Direct"
    },
    {
      "page_number": 584,
      "chapter": null,
      "content": "<parameter name=\"MoveYDilation\">\n<oscillator cycletimemin=\"5.0\" cycletimemax=\"8.0\"/>\n</parameter>\n<parameter name=\"MoveZDilation\">\n<oscillator cycletimemin=\"5.0\" cycletimemax=\"8.0\"/>\n</parameter>\n<parameter name=\"HighlightTexture\">\n<texture2D textureUnit=\"1\">\n<source type=\"Image\">Textures/green_detail.png</source>\n<wrap axis=\"S\" mode=\"Repeat\"/>\n<wrap axis=\"T\" mode=\"Repeat\"/>\n</texture2D>\n</parameter>\n</shader>\nThis entry defines a shader program called Green. For that, it specifies the vertex\nand fragment shader files. It also specifies two texture parameters and four oscillating\nfloat values that cycle between 0 and 1. Note that the oscillator uses reasonable\ndefaults, so the artist only had to set the oscillation time. \nThe entire effect is realized with zero lines of code. The artist created the vertex\nand fragment shaders and then added an entry to the definition XML file. The artist\nwas able to see the effect in game and was able to repeatedly tweak the magic numbers\nat runtime without having to repeatedly restart. The significant code snippets are pro-\nvided on the CD-ROM and the complete working example with source can be found\nas part of Delta3D (see the section called “Conclusion”). Color Plate 14 in the color\ninsert shows a few examples of dramatically different results that were generated with-\nout a restart.\nAdvanced Techniques\nThe previous sections define a basic architecture that can be added directly to an\nengine. In addition, there are several advanced techniques that are used in the com-\nplete implementation of this system that might be useful in your environment. \nShader Groups\nShader groups allows several related shaders to be lumped together into one group.\nThis allows the definition of separate shaders for each type of actor, such as damaged\nmode, destroyed mode, and normal mode. Alternatively, you could define a shader\ngroup with a targeted and non-targeted shader, or daytime and nighttime shaders for\nall the actor categories in the system. \nTo implement this, add a new class called ShaderGroup that sits between Shader-\nManager and ShaderProgram. This changes the original design in two ways. First, the\nmanager now holds onto group prototypes instead of program prototypes. Second,\nyou have to look up the group by name before you can find the shader within the\n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine\n551\n",
      "content_length": 2386,
      "extraction_method": "Direct"
    },
    {
      "page_number": 585,
      "chapter": null,
      "content": "group. Note that each group tags one of its shaders as the default; there is always one\nto use. In the full example, the blimp has a group with two shaders—one for the green\nhighlight and one for the normal, untargeted look. The following snippet shows an\nexample of an XML definition with groups:\n<shaderlist>\n<shadergroup name=\"Target Shaders\">\n<shader name=\"Normal\" default=\"yes\">\n...\n</shader>\n<shader name=\"Green\" default=\"no\">\n...\n</shader>\n</shadergroup>\n<shadergroup name=\"Tank Shader\">\n<shader name=\"Normal\" default=\"yes\">\n...\n</shader>\n</shadergroup>\n</shaderlist>\nCombining Shaders with Actors and Properties\nAnother advanced feature leverages actors and actor properties. This feature allows an\nartist or level designer to specify which shader to use for an object by setting the shader\ngroup actor property. Just as the XML definition allows the artists to easily define their\nshaders, the actor property system allows the artists to easily define which shader\nshould be assigned to an actor. The result is an API that is friendly to both the pro-\ngrammer and the artist. For fun, the artist used this feature to add a new shader to the\nterrain. For a complete explanation of actors and actor properties, see [Campbell06].\nThe following snippet shows all the code necessary to change the shader applied\nto an actor. This method is automatically called whenever the string property for the\nshader gets set. The shader property is just a string that can easily be defined in a map\nor received across a network in an actor update message. To apply the shader to the\nmesh, the programmer calls the three important methods on the manager: Find-\nShaderGroupPrototype(), GetDefaultShader(), and AssignShaderFromPrototype().\nNote that all error checking is omitted for brevity and that this method reduces down\nto only two calls if shader groups are not supported. \n// Set the Actor Property for Shader Group\nvoid GameActor::SetShaderGroup(const std::string &groupName)\n{\nShaderManager &sm = ShaderManager::GetInstance();\n552\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 2076,
      "extraction_method": "Direct"
    },
    {
      "page_number": 586,
      "chapter": null,
      "content": "// Make sure any old shaders are cleaned up. Better safe than sorry. \nsm.UnassignShaderFromNode(*GetOSGNode());\n// Get the shader group & the default shader \nconst ShaderGroup *group = sm.FindShaderGroupPrototype(groupName);\nconst Shader *defaultShader = shaderGroup->GetDefaultShader();\n// Make a new cloned instance of the shader from the prototype\n// and assign it to the state set for the mesh\nsm.AssignShaderFromPrototype(*defaultShader, *GetOSGNode());\n}\nFuture Work\nAlthough this version of the design is completely functional, there are many possible\nenhancements. The following list is presented as features to consider for your own\nengine and that may eventually be added to the host game engine, Delta3D (see the\n“Conclusion” section). \n• Geometry shaders—This gem does not support geometry shaders. However, based\non the 4.0 specification, it should be a relatively straightforward addition. \n• XML editor tool—Add a tool that helps the artist create the XML shader defini-\ntion file. This is similar to the level editor described in Game Programming Gems\n6 [Campbell06].\n• Generated shader source—Some engines support the ability to generate shader\nsource code at runtime. This design could be augmented to support such a tech-\nnique by inserting the generated shader code into the program prototype instead\nof loading it from disk. The design would benefit from the optimized runtime\ncode while still allowing the artist to test new uniforms. \n• Actor property parameter—Add a new data type to automatically update a mesh’s\nstate whenever an actor property (such as health or velocity) changes.\n• Enhanced cache—Maintain separate caches for vertex and fragment shaders.\nConclusion\nThis gem presents a ready-to-use shader architecture that can be integrated directly\nwith your engine. It makes a case for building a data-driven shader system that can be\nmanipulated outside of engine code, which gives artists the ability to visualize their\nassets inside the real game. It discusses the three primary classes of ShaderManager,\nShaderProgram, and ShaderParameter. It describes how to use the prototype design\npattern to provide a flexible system with good performance. It explains why uniform\nvariables are critical and how to support many different data types. Finally, this gem\ndemonstrates a real use case allowing an artist to hover and animate a blimp without\ninvolving a developer.\n7.4\nSupport Your Local Artist: Adding Shaders to Your Engine\n553\n",
      "content_length": 2465,
      "extraction_method": "Direct"
    },
    {
      "page_number": 587,
      "chapter": null,
      "content": "For more examples of this concept, see the source snippets available on the CD-\nROM. In addition, a complete and fully working implementation of this design is\navailable in the Tank Target Example provided by the Open Source Delta3D project\n(see www.delta3d.org). \nReferences\n[Blow04] Blow, Jonathan. “Game Development: Harder Than You Think,” available\nonline at http://www.acmqueue.org/modules.php?name=Content&pa=show-\npage&pid=114, 2004.\n[Campbell06] Campbell, Matt and Murphy, Curtiss. “Exposing Actor Properties\nUsing Nonintrusive Proxies,” Game Programming Gems 6, edited by Michael\nDickheiser, Charles River Media, 2006, pp. 383–392. \n[Gamma95] Gamma, Erich, Helm, Richard, Johnson, Ralph, and Vlissides, John.\nDesign Patterns—Elements of Reusable Object Oriented Software, Addison-Wesley,\n1995, pp. 117–126.\n554\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 866,
      "extraction_method": "Direct"
    },
    {
      "page_number": 588,
      "chapter": null,
      "content": "555\n7.5\nDance with Python’s AST\nZou Guangxian\nI\nn any MMORPG, there are plenty of conversations between NPC and player. It\ntakes processor time to encrypt them and costs a lot of bandwidth to transfer them.\nPython is a dynamic object-oriented programming language, and is widely adopted in\nMMORPG development. It is also powerful, and with the Python compiler package,\na developer can manipulate the Abstract Syntax Tree (AST) and the process of analyz-\ning and generating Python bytecode can be controlled at runtime. \nThis gem describes how to replace strings with numbers (ID) by manipulating\nthe AST. This way, bandwidth and runtime costs can be saved. A tool based on this\nidea is also given here. \nIntroduction\nIn computer science, AST means Abstract Syntax Tree. It is generated by the parsing\nphase, and it is used as the source of the bytecode generator. Its internal nodes are\nlabeled with operators such as addition or concatenation, and the leaf nodes represent\nthe operands of the operators. Thus, the grammar rules of the language can be illus-\ntrated with an AST. By visiting the nodes in the AST in order, code generation can be\nperformed and the bytecode will be emitted. \nEach node in the AST has special meaning and information, including whether it\nis a variable or a constant, and if it is a constant, what is its value? By making use of\nthis information or changing it, the programmer can control the bytecode generated,\nthat is, to change the meaning of source code. \nBackground\nThe standard way to handle text in games is to use a translation table. Each string in\nthe game is assigned an ID, and the ID is looked up in a table of strings each time it\nis displayed. The advantages of using IDs instead of raw strings are that they save\nbandwidth and memory, they can refer to any audio speech that goes with them, and\nthe language can be changed easily when translating to different territories without\nchanging the code.\n",
      "content_length": 1946,
      "extraction_method": "Direct"
    },
    {
      "page_number": 589,
      "chapter": null,
      "content": "However, tracking all these IDs takes a lot of time and management during the\ndevelopment of the project. It is much easier and quicker to simply use the strings\ndirectly when trying out concepts. However, this means they then need to be tracked\ndown later and replaced by string ID lookups, and the code needs to be changed to\nperform the string lookup. This is a time-consuming task, and it is easy to make mis-\ntakes or miss instances.\nIt is useful to have a way to examine existing code to find all the strings, enter\nthem in a database, and then track them. As a bonus, once you can inspect code for\nstrings, it is just as easy to edit code and change strings. Instead of changing the code\nto do an indirection through a table each time, it is far simpler to edit the code to\npoint directly at the string. In the rare event that the player changes language or the\nserver updates some text, the edits can be performed again, but otherwise there are no\nextra indirections in the places where the strings are used, reducing code complexity.\nSolution\nPython strings are enclosed in single quotes (' and ') or double quotes (\" and \"). For\nexample:\ncompanyName = 'NetEase.Co'\nprojectName = \"Tang Dynasty\"\naddress = \"\"\"\nGuangZhou,\nChina\n\"\"\"\nFor general programs, it is not easy to extract these strings. Writing a parser to do\nit is tricky and not something to be attempted lightly.\nFurthermore, there can be cases where replacing the text of the string with the\nlookup function in the source text will produce problems, such as being in the wrong\nscope, or when it’s part of a compile-time macro.\nFortunately, Python already has a good mechanism to simplify this job. In the\ncompiler package, there are five functions: \ncompile( source, filename, mode, flags=None, dont_inherit=None ) \ncompileFile( source )\nparse( buf )\nparseFile( path ) \nwalk( ast, visitor[, verbose] )\ncompile and compileFile both compile the source code; however, compileFile\ngenerates a .pyc file and compile returns a code object.\nparse/parseFile returns an AST for the Python source code in the buffer or in\nthe file specified by path. \nThe walk function does an ordered walk over the AST and calls the appropriate\nmethod on the visitor instance for each node encountered. For example, when a Const\n556\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 2321,
      "extraction_method": "Direct"
    },
    {
      "page_number": 590,
      "chapter": null,
      "content": "is encountered, a visitConst function will be called. In general, for a node Type, if the\nvisitType exists, visitType will be called; otherwise ASTVisitor.default will be\ncalled. So, if you can provide a visitor to the walk function with appropriate method,\nthe node information in AST can be extracted. \nTo solve this problem, a visitor that implements visitCallFunc should be pro-\nvided. There are two phases to the solution. First, find all the constant strings in the\ncode, assign an ID to each string const found, and save the relationship between the\nstring and the ID to a file called stringres.txt. In the second phase, the walk is per-\nformed again, the string is replaced with the ID, and the new .pyc file is generated. \nTo help understand the AST, astshow.py is provided on the CD-ROM, and it will\nproduce the formatted output of the AST. Here, I explain what you will get when a\nfunction was called in source code. \nFor example, if a file called sample.py contains this:\nimport game\ngame.msg2player( \"hello\" )\nUse the walk function to walk through the AST of the previous source code, and\nthe node passed to visitCallFunc will be:\nCallFunc(Getattr(Name('game'), 'msg2player'),\n[Const('hello')], None, None)\nIts child node is \"Getattr(Name('game'), 'msg2player')\" and its argments are\n\"[Const('hello')]\".\nThe full name of a function can be extracted with the following function:\ndef getFunctionName( node ):\nif isinstance(node, compiler.ast.Name):\nreturn node.name\nelif isinstance(node, compiler.ast.Getattr):\nreturn getFunctionName(node.getChildNodes()[0])\n+ '.' + node.attrname\nelse:\nreturn ''\nIn the first phase, visitCallFunc can be implemented as:\nimport os\nfrom compiler import ast, pycodegen\nimport utils\nclass Visitor:\ndef visitCallFunc(self, node):\nfunc_name = utils.getFullName( node.node )\nif func_name in utils.helper.functions :\nfor arg in node.args :\nif isinstance( arg, ast.Const ) :\nif isinstance( arg.value, basestring ):\nutils.helper.append( arg.value )\n7.5\nDance with Python’s AST\n557\n",
      "content_length": 2016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 591,
      "chapter": null,
      "content": "# the rest is copied from pycodegen\n# and simply continues to walk the AST.\npos = 0\nkw = 0\nself.visit(node.node)\nfor arg in node.args:\nself.visit(arg)\nif isinstance(arg, ast.Keyword):\nkw = kw + 1\nelse:\npos = pos + 1\nif node.star_args is not None:\nself.visit(node.star_args)\nif node.dstar_args is not None:\nself.visit(node.dstar_args)\nIn this function, the argument will be checked and any const string argument\nwill be added to the ID-string map by the utils.helper.append function.\nIn the second phase, the bytecode will be generated by calling compile/compileFile.\ncompile/compileFile have a strong coupling with CodeGenerator so that you cannot\ndefine a class inherited from CodeGenerator to affect the result. Instead, you can\nassign a custom function as the visitCallFunc to the CodeGenerator, taking control\nof the generation process. The visitCallFunc is given here, and when arg.value is a\nconst string, it is replaced by the ID. \nimport os\nfrom compiler import ast, pycodegen\nimport utils\ndef visitCallFunc(self, node):\nfunc_name = utils.getFullName( node.node )\nif func_name in utils.helper.functions :\nfor arg in node.args :\nif isinstance( arg, ast.Const ) :\nif isinstance( arg.value, basestring ):\narg.value = utils.helper.get( arg.value )\n# the rest is copied from pycodegen\n# and simply continues to walk the AST.\npos = 0\nkw = 0\nself.set_lineno(node)\nself.visit(node.node)\nfor arg in node.args:\nself.visit(arg)\nif isinstance(arg, ast.Keyword):\nkw = kw + 1\nelse:\npos = pos + 1\n558\nSection 7\nScripting and Data-Driven Systems \n",
      "content_length": 1539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 592,
      "chapter": null,
      "content": "if node.star_args is not None:\nself.visit(node.star_args)\nif node.dstar_args is not None:\nself.visit(node.dstar_args)\nhave_star = node.star_args is not None\nhave_dstar = node.dstar_args is not None\nopcode = pycodegen.callfunc_opcode_info[have_star, have_dstar]\nself.emit(opcode, kw << 8 | pos)\nPlease refer to the full source code on the CD-ROM for more details. \nConclusion\nIn this article, with Python’s compiler package, you have the ability to access and mod-\nify the AST. Based on this idea, you get an elegant solution to constructing a string\ntable without programmer help. In addition, the script writer can worry less about later\ntranslations. This process is transparent and can be integrated seamlessly.\nReferences\n[Python] Python language Website, available online at http://www.python.org.\n7.5\nDance with Python’s AST\n559\n",
      "content_length": 835,
      "extraction_method": "Direct"
    },
    {
      "page_number": 593,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 594,
      "chapter": null,
      "content": "561\nAbout the CD-ROM\nAbout the Game Programming Gems 7 CD-ROM\nThe CD-ROM included with this book contains source code, executable demos,\nlibraries, images, and text. All are meant to demonstrate or supplement the gems in\nthis book. Full appreciation of the book requires perusal of the CD-ROM materials.\nEvery effort has been made to ensure the enclosed source code is bug-free and able to\nbe compiled, the executables run trouble-free, and the images and text are freely view-\nable. Please refer to the book’s Website, http://www.gameprogramminggems.com/,\nfor the most recent details regarding the contents of the CD-ROM.\nContents\nFor ease of location, the materials on the CD-ROM are organized into folders that cor-\nrespond to the sections and gems of the book. For your convenience, an auto-run Win-\ndows executable is provided that helps you locate each folder, but the executable is not\nrequired to browse the CD-ROM’s contents. Source code in each folder has been veri-\nfied to compile with Microsoft Visual Studio C++ 7.0 and Visual Studio 2005 solution\nand project files are usually provided. In many cases, precompiled binaries are also\nincluded. When possible, supplemental libraries have also been included but in a few\ninstances additional libraries must be obtained by the user. Examples of these include\nthe Windows version of OpenAL, available from http://developer.creative.com/ and\nthe DirectX SDK, available from http://msdn.microsoft.com/directx/sdk/.\nSystem Requirements for Windows\nWindows 2000, XP, or Vista is required. A document reader capable of displaying\nMicrosoft Word or PDF documents is needed for article supplements. Examples\nusing or demonstrating graphical techniques require a 3D card supporting DirectX 9.\n",
      "content_length": 1744,
      "extraction_method": "Direct"
    },
    {
      "page_number": 595,
      "chapter": null,
      "content": "This page intentionally left blank \n",
      "content_length": 36,
      "extraction_method": "Direct"
    },
    {
      "page_number": 596,
      "chapter": null,
      "content": "563\nNumbers\n2D Gaussian distribution, construction of, 202\n3 \u0002 3 matrix, example of, 180\n10Hz, capturing logs at, 267\n16-bit PCM audio engine file format, pros and cons of,\n308–309\n\" (double quotes), using with Python strings, 556\n' (single quotes), using with Python strings, 556\nSymbols\n< and >, use in formulas, 163\nA\nA&C (age and cost), function in cache replacement, \n12–13\nA* search algorithm\nuse of, 290–292\nweaknesses of, 291–292\nAABB (axis-aligned bounding box), use in scenes, 184\nAABB-trees, considering in collision tests, 187\nabsolute values, impact on behavior cloning, 211\nAbstract Syntax Tree (Abstract), using to replace strings\nwith numbers, 555–559\nAC decision-making algorithms, blocks for, 236–238\nacoustics, raytracing for, 302\naction searching, improving, 286\nActionInfo flow, use in Artificial Contender, 241–242\nActionInfo objects\nretrieving from input blocks, 243\nuse in Artificial Contender, 235, 245\nactions, merging, 284–285\nactors, combining with shaders, 552–553\nAdaptive Replacement Cache (ARC) algorithm, \nuse of, 6\nAddRegion() function, use in optical flow, 31\nAddressingScheme class, using with hexagonal grids, 54\nADPCM audio engine file format, pros and cons of,\n308, 337\naffect, relationship to attitudes, 251\naffine mappings, extracting semantics from, 181\naffine matrices, inverting, 181–182\naffine transforms, use of, 180\nAge algorithm\nbenefits of, 9\nexpansion of, 10–11\nusing in cache replacement, 8–13\nage and cost (A&C), function in cache replacement, \n12–13\nAge Percentage Cost (APC)\ncalculating, 9\nrelationship to RC (relative cost), 12–13\nagents\nattitudes held by, 252–253\ncreating with behavior cloning, 210–216\ntraining, 211–212\ntraining for acceleration, 211\nvision-modeling considerations for, 224\nagent-sensing model\nhearing model for, 219\nvision model for, 217–219\nAI (artificial intelligence). See agents\nAI script, building from trees in behavior cloning, \n215–216\nNumbers with “GPG” proceeding refer to previous editions of the Game Programming Gems Series. \nNumbers without this notation refer to the current volume.\nINDEX\n",
      "content_length": 2080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 597,
      "chapter": null,
      "content": "AIShooter demo, running, 212\nalgorithms\nA* search algorithm, 290–292\nARC (Adaptive Replacement Cache), 6\nblocks for AC decision-making algorithms, 236–238\ncache replacement, 6–8\ncentral limit theorem, 201\nchorus and compression audio processing effects, 301\ncollision detection using MPR, 171–176\ncomponents of, 153\nCSG (constructive solid geometry), 159\nDijkstra’s algorithm and A*, 291\nfarthest feature map, 150\nFringe Search, 293–294\nIDA* (Iterative Deepening A*), 292\nK-medoids clustering, 276–277\nLoop subdivision, 383–390\nLRU (Least Recently Used), 6\nLucas and Kanade, 31–33\nLucas and Kanade algorithm in optical flow, 29–30\npage-replacement, 6\nparticle deposition, 353–354\nplan-merging algorithm, 284–286\npolar-rejection, 200\npolygon cutting, 162–163\nraytracing, 127\nrecursive node learning, 213\nridge structures, 359–360\nRP2 operations, 161–164\nskeletal animation, 367, 370\nsum-of-uniforms, 201\nvictim page determination, 6\nWELL algorithm, 120–121\nwhitening algorithms used with RNGs, 116\nworkflow for Artificial Contender, 230–232\nallocated blocks, resizing, 23\nallocation hooks\nimplementing, 101–102\nusing with memory leaks, 100–101\namplitude envelope, example of, 312\nanimating\nrelief imposters, 407–409\nrelief maps, 407\nanimation data, storing in textures, 406\nanimation systems, overview of, 365–366\nSee also relief imposters\nAPC (Age Percentage Cost)\ncalculating, 9\nrelationship to RC (relative cost), 12–13\nAPC variables, deriving, 10\nAPIs, using in audio processing, 332\napplication crashes, exception handling, 97–98\nApplication Recovery and Restart API, availability in\nWindows Vista, 103\nApplyToModified functor, code for, 243\nARC (Adaptive Replacement Cache) algorithm, \nuse of, 6\narray of pointers, using with heap allocators, 21\narray of vectors, optical flow as, 26\narrays, use with subdivision data structures, 390–391\nArtificial Contender\ndecision-making algorithms for, 232\ndevelopment of, 229\nexecution flow in, 235–236\npartial results in, 232\nusing “Pipes and Filters” design pattern with,\n230–232\nworkflow algorithms for, 230–232\nworkflow diagram for, 234–235\nArtificial Contender implementation\nActionInfo flow, 241–242\nActionInfo type, 245\nalternative block implementations, 245–246\nconstraints, 246–247\nconstructing workflows, 246\nfunction pointers versus functors in, 244–245\ngeneric programming and C++, 238\npartial results in, 244\npolymorphic workflow blocks, 238–241\nartificial intelligence (AI). See agents\nARToolkit\nobtaining marker position with, 74\nretrieving transformation matrix used by, 74\nsample programs in, 72–73\nusing with foot-based navigation, 71–72\nAST (Abstract Syntax Tree), using to replace strings\nwith numbers, 555–559\nasynchronous events, notifications as, 81–83\nasynchronous versus synchronous exceptions, 97\nAtlas terrain system, availability of, 421–422\nattitude components\nduration, 255\npotency, 254–255\nvalence, 253–254\nattitude objects, example of, 252, 256–258\nattitude systems\nexample of, 261–262\nfeatures of, 250–252\n564\nIndex\n",
      "content_length": 2993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 598,
      "chapter": null,
      "content": "model for, 255–256\npersuasion and influence in, 259–260\nsetting half-lives in, 255\nupdating values in, 252\nuse of, 249\nattitudes\naccumulation of, 251\nemotional charges of, 252\nand social exchanges, 260–261\ntoward behavior, 258–259\nuse in Fable, 253\nattributes, serializing into text format, 518–519\naudio\ncalculating room acoustics, 302–303\nkeeping in sync with graphic updates, 314\non PS3, 306\nstreaming with loop markers, 310\nsurround sound, 315–318\nSee also mixing system; next-gen audio engine; \nsounds\naudio channels\nmixing to busses, 318\nprocessing relative to playback frequency, 311\nrequirements for, 307\nsetting volume levels of, 311\nsplitting, 315\naudio compression formats\nADPCM, 337\nMP3, 337\nOGG Vorbis, 337–338\naudio data streaming, prioritizing, 310\naudio effects, using, 301\naudio engines\nconsidering, 306\nfile formats for, 308\nloop markers used with, 309\naudio files, playback of, 307–309\naudio optimization, implementing with GPUs,\n300–301\naudio processing\nAPIs available for, 332\ncompression and streaming, 337–338\neffects and filters in, 336–337\nrank buffers in, 334–336\nsound buffers in, 333–334\naudio samples, clearing out, 334\naudio tools, using, 326–328\nauthentication\nChallenge Hash Authentication, 483\nimplementation of, 487–488\nand password recovery, 482–483\nprocess of, 481\npublic key infrastructure, 484–485\nSecret Exchange Authentication, 484\nAVG usage, finding for pages, 10\naxis-aligned bounding box (AABB), use in scenes, 184\nB\nB-A Minkowski difference, considering as convex shape,\n170\nbackscattering effect, applying for diffuse-light shading,\n377–379\nbalance theory, relationship to attitude systems, 260\nbattlefield, navigating in RTS (real-time strategy) games,\n63, 65\nbehavior, attitudes toward, 258–259\nbehavior cloning\nbuilding AI script from trees for, 215–216\ndemo game for, 210–216\nexplanation of, 209\nbehavior-capture AI technology. See Artificial Contender\nbehaviors, finding in player traces, 272\nBelady’s Min (OPT) algorithm, use of, 6–13\nbest-fitting spheres, relationship to farthest feature map,\n143, 148\nBI (behavioral intention), relationship to attitudes, 259\nbin, selecting based on heap-allocation size, 16–17\nbinding\nC functions, 504\nclasses, 503, 513\nbinding function, creating for Lua, 507–509\nBiquantic subdivision scheme, features of, 382\nbit array, using with heap allocators, 21\nblimp, invoking in shader architecture, 549–551\nblimp fragment shader code sample, 549–550\nblimp vertex shader, creating hover for, 549\nblocks\nfor AC decision-making algorithms, 236–238\nActionInfo objects processed by, 235\nalternative implementations of, 245–246\nconstraints on, 238\nfunction in execution flow, 235\nimplementation examples, 242–243\npolymorphic workflow blocks, 238–241\nIndex\n565\n",
      "content_length": 2736,
      "extraction_method": "Direct"
    },
    {
      "page_number": 599,
      "chapter": null,
      "content": "566\nIndex\nblocks (continued)\nproperties of, 239\nSee also workflow blocks\nBloom, Charles, 184–185\nBlum Blum Shub RNG method, description of, 121\nbones, cumulative error associated with, 366\nbook, use in debugging heap allocation, 22\nBoolean operations, performing on convex polygons,\n161, 163\nbounding boxes, use in scenes, 184\nbounding volume hierarchies, use in narrow phase,\n185–188\nbox, support mapping for, 168\nbox-box overlap test, use in narrow phase, 186–187\nbreakpoints, using with network code, 492\nbroad phase of collision detection, speeding up,\n184–185\nBrownian trees, use in particle deposition, 359–360\nBSP tree, kD-tree as, 129\nbucket organization, parameters for WER, 102\nbump maps, storing for advanced decals method,\n427–428\nbump vectors, encoding, 428\nbusses, mixing audio channels to, 318\nButterfly subdivision scheme, features of, 382\nC\nC functions, binding, 504\nC++, use with Artificial Contender, 238\nC++ classes, components of, 519\nC++ compile-time checking, use with Artificial \nContender, 246\nC++ methods, overloading in Lua, 514\nC++ objects\nand arrays, 530, 532\nbinding in Lua, 505–510\nimproving, 533\nnative database type for, 530–531\nobjects or pointers to objects, 530–531\nretrieving, 531\nstoring, 527–530\nstoring instances of, 525\nupdating contents of, 529–530\nSee also objects\nC++ STL, using with hexagonal grids, 51–52\ncache, direct-mapping main memory to, 43\ncache, function of, 5–6\ncache coherency, use in multithread job and \ndependency system, 90\ncache misses\noccurrence of, 5\ntypes of, 43\ncache replacement\nA&C (age and cost) considerations, 12–13\nAge and Cost metrics, 8–13\ncost of, 11–12\ncache systems, difficulty associated with, 5\ncached memory, reading from, 6\ncache-replacement algorithms\nBelady’s Min (OPT), 6–7\nLRU (Least Recently Used), 7\nMRU (Most Recently Used), 7\nNFU (Not Frequently Used), 7–8\nuse of, 5\ncapsule, support mapping for, 169\ncar race games, using surround sound in, 315\nCatmull-Clark subdivision scheme\nfeatures of, 382, 388–389\nfor GPU rendering, 397\nCD contents\nAddressingScheme for hexagonal grid, 52\nAIShooter demo, 212\nastshow.py file, 557\nC++ object serialization, 532\nclipmap demo, 422\nclipmap effect, 417\ndebugging framework, 103–104\ndecal system, 430, 433\ndeferred function caller, 84\nfoot-based navigation, 70\nheap allocator, 23\nhexagonal tile (grid) example, 47\nhiroPatt.pdf file for foot-based navigation, 72\nhorse animations, 410\nlipsyncing example, 457, 460\nLua binding, 516\nLua binding data structure, 505\nmixing system, 347\nmultithread job and dependency system, 87\noptical flow example, 33\nPlayerViz tool, 268\nprojective space example, 164\nraytracing demo, 139–140\nrelief imposters, 410\nshaders integrated into engine, 551\nsmart packet sniffer, 493, 496\nsound effects, 324\ntables for database backend, 526\n",
      "content_length": 2786,
      "extraction_method": "Direct"
    },
    {
      "page_number": 600,
      "chapter": null,
      "content": "threading system, 36\ncellular automata\nRNG method, 119\nusing hexagonal grids with, 56–57\nusing in RTS (real-time strategy) games, 64\ncentral limit theorem, use with GRNGs, 201–202\ncentroids, connecting for farthest feature map, 149\nChallenge Hash Authentication, properties of, 483\nchorus effects, using, 301\ncircular buffer, role in audio processing, 334\ncities, depicting with square tiles, 50\nclasses, binding, 503, 513\nclient/server topology, considering in game world \nsynchronization, 468\nclipmaps\nadvantages of, 415–416\nbackground paging, 420\nbudgeting updates, 420–421\nand clipstack size, 416\nCPU synthesis and upload, 419\ndrawbacks of, 416\nimplementing, 417–419\nmanaging large textures with, 436\nmethods for updating, 417\noptimizing fillrate/low-end support, 421\npurpose of, 414\nselecting focus points, 416\ntoroidal updates and rectangle clipper, 418–419\nuse of, 413\nclosed lists, eliminating with IDA*, 292–294\nCMU phonemes, 458–459\ncode samples\nApplyToModified function, 243\nattitude system, 255–256\naudio effects, 301\nbackscattering, 378–379\nbinding function for Lua, 507–509\nblimp fragment shader, 549–550\nblimp vertex shader, 549\nBrownian tree, 360\nC++ methods overloaded in Lua, 514\nC++ object update, 529\nC++ objects as Lua objects, 506–507\nC++ objects stored, 528\nclipmap effect, 417\ndataports, 536\ndependency group and links, 93\ndunes created with particle deposition, 362\nfarthest feature map, 148, 150\nfoot-based navigation, 73–75\nforEach function implementation for modifier, 243\nGaussian distribution, 201\nGLRCachePad macro, 43\ngraftal imposters, 451–453\nhalf-edge pair indices in Loop subdivision, 395\njob class in multithread job and dependency system,\n88–89\njob selection in multithread job and dependency\nsystem, 91\nkD-tree EventBoxSide, 135\nkD-tree traversal, 136–137\nKdTreeNode structure, 130\nLFSR113, 119–120\nLua binding, 503\nLua binding and automatic type registering, 509–510\nLua binding data structure, 505–506\nLua binding function, 509\nLua binding metatable for object-oriented method,\n505\nLua binding of C function, 504\nLua binding optimization of generated code size, 515\nLua binding sand-boxing and type filtering, 515\nLucas and Kanade algorithm in optical flow, 29\nmanager class in multithread job and dependency\nsystem, 89\nModifier block for Artificial Contender, 241\nmotion history in OpenCV, 28–29\nmWebCam.AddRegion(), 32\nnon-collinear surface points in XenoCollide, 172\nOpenCV, 26\noverhanging terrain, 363\npacket sniffer, 495\nparticle dynamics, 357\nparticle placement for volcanoes, 358\npathfinding with hexagonal grids, 56\npointers for database backend, 521–523\npolymorphic workflow blocks, 239\nPython’s AST, 556–557\nraytracing, 127\nroom acoustics real-time rendering, 302–303\nscheduler class in multithread job and dependency\nsystem, 90\nserializing attributes into text format, 518–519\nshader groups, 552\nshader in GLSL, 376\nshaders combined with actors and properties,\n552–553\nshaders integrated into engine, 550–551\nIndex\n567\n",
      "content_length": 2971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 601,
      "chapter": null,
      "content": "code samples (continued)\nsource blocks, 242\nspatial search with hexagonal grids, 55\nstatic polymorphism, 241\ntables for database backend, 526\ntexture-coordinate calculation, 443\ntrigonometric functions, 194–195\nvariables allocated memory for OpenCV, 26–27\nwebcamInput class cvAbsDiff function, 27\nwebcamInput class public interface, 31\nWELL algorithm, 120–121\nworker threads in multithread job and dependency\nsystem, 90\nXenoCollide pseudocode, 171\nSee also Listings\ncodecs, requirements of, 309\nCodeGenerator, using with Python’s AST, 558–559\ncognitions, relationship to attitudes, 251\ncollinear cases, detecting, 153\ncollision algorithm. See XenoCollide\ncollision culling, explanation of, 184\ncollision detection\nbroad phase task of, 184–185\nand Loop subdivision, 389–390\nmodeling, 144\nbetween models in scenes, 180\nnarrow phase of, 185–188\nsimplifying using Minkowski differences, 170–171\nusing Minkowsi Portal Refinement (MPR), 171–176\ncollision detection tasks, using semantics for, 184–188\ncollision systems, creating, 165\ncollision tests, considering AABB-trees in, 187\ncollision-detection steps\nchoose_new_candidate(), 173–174\nchoose_new_portal() step of, 175\nfind_candidate_portal(), 172\nfind_origin_ray() step of, 171\nfind_support_in_direction_of_portal(), 174\nif () return hit; step of, 174\nif (origin outside support plane) return miss, 174\nif (support plane close to portal) return miss, 175\nwhile (origin ray does not intersect candidate), 172\ncommand lifetime, use with RTS games, 65–66\ncompression, considering in audio processing, 337–338\ncompression algorithms, using with skeletal animation,\n367\ncomputer vision games, optical flow in, 26\ncomputer vision, using with foot-based navigation,\n71–72\ncones\ncreating, 169\nsupport mapping for, 170\ntesting of model human vision, 219\nconstructive solid geometry (CSG) algorithms, use of,\n159\ncontact information, acquiring with MPR, 176–178\ncontinuous collision detection, approach toward, 144\ncontrol points\ninterpolating for relief imposters, 404–405\nfor walking dog animation, 409–410\nconvex polygon, describing, 161\nconvex shapes, manipulating, 170\ncoplanar cases, detecting, 153\ncosine law, use of, 373\ncrashes, reasons for, 97\nCSG (constructive solid geometry) algorithms, use of,\n159\nCSPRNGs (cryptographically Secure PRNGs)\nBlum Blum Shub, 121\n/dev/random, 121–122\nFortuna, 122\nISAAC, ISAAC+, 121\nMicrosoft’s CryptGenRandom, 122\nYarrow, 122\ncube-map, relationship to farthest feature map,\n144–145\ncustom texture cache, design of, 8–13\ncylinder, support mapping for, 169\nD\nDA (direct-argument) functions, adding deferred calls\nto, 84\ndata loading, considering in streaming audio, 309\ndata structures, designing for subdivision surfaces,\n390–392\ndatabase backend for C++ objects\narray class used in, 518\nmetadata for, 517–518\ndatabase backend tables for C++ objects\narrays, 525\ncreating, 526\ninstances of classes, 523–525\nparent class, 520\npointers, 520–523\n568\nIndex\n",
      "content_length": 2934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 602,
      "chapter": null,
      "content": "scalar members, 520\nstrings, 520\ndataport examples\nbroadcasting positional information, 539\ncamera systems, 538\nproblems with, 539\nship handling debug values, 538\nDataport Manager\nusing, 536–537\nusing hashing in, 539\ndataport pointers, use of, 536\ndataports\nand reference counting, 537–538\nand type safety, 537\nuse of, 535–536\ndebug output, using with network code, 492–493\ndebugging framework\nexception handling, 104\nmemory leak detector, 104\ndebugging support, adding for heap allocation, 22\ndebugging techniques, maintaining for network code,\n492–493\ndecal system, requirements for, 423\ndecals method (advanced)\nadvantages of, 428–430\nDecodeBump function used in, 427–428\nperformance and experimental results, 430–433\nusing, 424–428\ndecision tree implementation, using with agents,\n211–215\ndecision-making algorithms, for goal-oriented planning\nsystems, 281–286\ndecomposition\neffects of, 230\nof worlds into regions, 271\ndeferred functions, use of, 82–85\ndeferred_proch system\nheader file for, 84\nparameters used with, 84\ndemos. See CD contents\nDependency Inversion Principle, applying to workflow\nblocks, 239\ndependency manager system\ndependency graph in, 92\ndependency storage in, 93–94\nentries in, 91\ngroup entry in, 94\njob entry in, 94\ndesign patterns\niterator used with hexagonal grids, 53\nPrototype Design Pattern used with shaders, 544\n/dev/random RNG method, description of, 121–122\ndfpProcessAndClear deferred function caller, use of, \n84\nDIEHARD randomness-testing suite, features of, 116\ndiffuse light\ncomputation of, 373\nmodel for, 374\nproducing, 375\ndiffuse-light shading\nbackscattering, 377–379\nflattening effect of, 375–377\ndiffusion limited aggregation (DLA), creating ridge\nstructures with, 359–360\nDijkstra, modification of A* search algorithm by,\n290–292\ndirect-argument (DA) functions, adding deferred calls\nto, 84\ndirected lines\ncomputing in R2 projective space, 159\ncutting polygons with, 161\noperations on, 157–158\nDirectSound API, features of, 332\ndisc, support mapping for, 168\ndiscrete collision detection, approach toward, 144\ndispositional liking, demonstration by attitudes, 251\nDLA (diffusion limited aggregation), creating ridge\nstructures with, 359–360\nDoo-Sabin subdivision scheme, features of, 382\ndouble quotes (\"), using with Python strings, 556\ndramatic beat, function in attitude systems, 252\nDSP effects, considering in surround sound, 317–318\ndump file, controlling information in, 98\ndunes, improving particle placement of, 361–362\nduration, purpose in attitude systems, 255\nDXT5 compressed surfaces, storing bump values in,\n427\ndynamic geometry, computing texture coordinates in,\n443\nE\nechoes, calculating, 302–303\nedge vertices, computing with Loop subdivision, 393\nedges, manipulating in Loop subdivision, 384–386\neffects versus filters, considering in audio processing,\n336–337\nIndex\n569\n",
      "content_length": 2835,
      "extraction_method": "Direct"
    },
    {
      "page_number": 603,
      "chapter": null,
      "content": "ellipses\naugmenting vision model toolbox with, 219–222\nEquation for, 195\nimplementing for vision model, 221–222\nsupport mapping for, 168\nellipsoid, support mapping for, 168\nentities, roles in game worlds, 55\nEnumerator class, using with hexagonal grids, 53\nepsilon values\nusing in foot-based navigation, 75\nusing with collinear and coplanar cases, 153\nEquations\naffine transform, 180\nattitude objects, 256\nbackscattering, 377–378\nbump-vector encoding, 428\nclustering IPGs, 277\ncost of split, 133\ndecals method (advanced), 426\ndecision tree implementation for agents, 213\nellipse, 195\nellipse implementation for vision model, 221\nflattening effect of diffuse-light shading, 375\ngraftal-imposter texture coordinates, 452–453\nHermite spline, 192–194\ninverse of affine matrix, 181–183\nMA and MB matrices, 186\nmatrix multiplication over vertex, 181\nMinkowski differences, 171\norigin shift, 181\nPhong-Blinn model applied to backscattering,\n378–379\npoints and directed lines in RP2, 155\npolygon cutting, 163\nray intersection with axis-aligned plane, 130\nRBFs (radial basis functions) and relief imposters, 404\nskeletal animation and cumulative error, 366\nskeletal animation rotation computation, 369\nSLERP (spherical linear interpolation), 194\nsphere with support mapping, 167\nsupport mapping for rotated and translated object,\n168\ntexture memory usage for large terrain areas, 439\ntexture stack update for large terrains, 440\nvertex and crease normals in Loop subdivision, 388\nvertices in Loop subdivision, 386\nvertices in skeletal animation, 365\nerosion, simulating effects of, 355–356, 429\nerrors\napplication crashes, 97–100\nmemory leaks, 100–102\nWER (Windows Error Reporting), 102–103\nEventBox, use with kD-tree, 134–135\nexception handling, 97–98, 104\nexceptions, types of, 97\nexplosions\nconsidering as “pops,” 322\nqualities of, 323\nEye Toy: Play, optical flow experiment with, 30–33\neyes. See vision model\nF\nFA (faces array), use with subdivision data structures,\n390–391\nFable\nopinion system in, 260\nuse of attitude in, 252–253\nFaçade, opinion events in, 252\nfade sample amount, specifying for rank buffers, \n336\nfading, use with FFT, 312–313\nfarthest feature map\n2D case of, 145–146\n3D case of, 147\nalgorithm for, 150\nand best-fitting spheres, 143\nand mean curvatures, 143\noversampling, 148\nand preprocessing, 144–148\nand principle curvatures, 143\nand runtime queries, 148–150\nstorage of vertices for, 148\nuse of, 143\nvisualizing, 145\nFast Fourier Transforms (FFT)\nrelationship to audio engines, 312–313\nrelationship to effects and filters, 336\nfeature space output, setting up for behavior cloning,\n210–211\nFFT (Fast Fourier Transforms)\nrelationship to audio engines, 312–313\nrelationship to effects and filters, 336\nwindowing required for, 314\nfield of view\ndetermining entities in, 222\nusing ellipse for, 220\n570\nIndex\n",
      "content_length": 2822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 604,
      "chapter": null,
      "content": "Figures\n2D case of farthest feature map, 145–146\n3D case of farthest feature map, 147\nAABB fitting model, 185\namplitude envelope, 312\nArtificial Contender decision-making workflow, 234\naudio-channel location relative to player, 316\nbackscattering, 378–379\nbest fitting circle in 2D, 149\nbin selection based on heap-allocation size, 16\nbin with single page for small allocator, 17\nBoolean operations on polygons, 163\nbounding box start and end events for kD-tree, 134\nBrownian tree created with DLA, 360\ncellular automata and hexagonal grids, 57\ncellular automaton grid in RTS game, 64\ncircular update for large terrain, 442\nclipmap update, 415\nclipped mipmap stack, 414\nclipstack-texture size, 418\ndecal techniques, 426–427\ndecal-methods tests, 431\ndecals method (advanced), 425\ndecision tree for agents, 214\ndependency graph, 91, 93\ndependency graph after propagation, 92\ndog imposter, 408\nDSP effects in buss, 318\ndunes formed as particles, 362\nDXT 1/5 compressed textures, 433\nedge mask in Loop subdivision, 385\nellipse components for vision model, 220\nEnumerator class used with hexagonal grid, 53\nerosion using decals, 429\nface splitting in Loop subdivision, 394\nfeature space, 210–211\nFFT (Fast Fourier Transforms), 313\nflattening effect of diffuse-light shading, 376–377\nFlock of Birds motion capture device, 70\nFMOD Designer interface, 327\nfoot-based navigation, 71\nFSM (Finite State Machine) for warrior, 257–258\ngame circuit for foot-based navigation, 76\ngame start indicator for foot-based navigation, 76\nGaussian distribution, 200, 202\ngeometry for Loop subdivision, 389\nGLR thread library, 37\ngoal-oriented planning systems, 282\nGPGPU graphics pipeline, 300\nGPU versus CPU computational power, 299\ngraftal imposters, 449–450\ngraftal-imposter vertices, 451\ngrid partitioned into rectangular cells, 54\nhead model for lipsyncing, 456–457\nhexagonal tiles, 48\nhexagonal tiles with axes of symmetry, 50–51\nHIVVE (Highly Interactive Information Value \nVisualization and Evaluation), 278\nHLA activities, 472\nHLA collaboration diagram, 469\nHLA viewports and objects, 470\nHLA-runtime entity-viewport visibility, 476\nHLA-system UML class diagram, 477\nhorse animation, 410\ninteraction feature points, 269\nIPGs (interactive player graphs), 275–276\nkD-tree split plane position, 132\nkD-tree splitting process, 129\nkD-tree traversal cases, 138\nkD-tree with node reduction, 130\nLambert shaded teapot, 374\nlarge allocator memory use, 18\nlava streams, 359\nLCG bias, 123\nMicrosoft XNA XACT audio tool, 327\nmixing layers in mixing system, 345\nmixing system, 342\nmixing system with central mix, 343\nmixing through MIDI control surface, 346\nmountain created with particle deposition, \n361\nmovements of soldier troops in RTS game, 60\nnil node’s place in tree, 20\noptical-flow game, 30, 32\nOren-Nayar shaded teapot, 375\norigin ray, 172\noverhanging terrain, 363\nparticle deposition, 354\nparticles and slope of terrain, 356\nPlay Audio commands relative to surround sound,\n317\nPlayerViz tool, 270\npolygon cut with line, 162\npolygon intersection, 162\nportal for XenoCollide and MPR, 173–174, \n176\npull workflow for Artificial Contender, 237\npull workflow with callback functions, 244\nray components, 128\nraytracing demo application, 140\nIndex\n571\n",
      "content_length": 3232,
      "extraction_method": "Direct"
    },
    {
      "page_number": 605,
      "chapter": null,
      "content": "Figures (continued)\nrectangular domain for hexagonal grid, 52\nred-black-tree nodes for large allocator, 19\nrelief imposters with control points, 403\nrelief-imposter warping, 402\nrotational error removed, 369\nRP2 projective space, 154\nRP2 projective space points and lines, 156\nRTS (real-time strategy) games, 60\nRTS (real-time strategy) sketches, 65\nRTS focus-context interface combination, 61\nRTS games with integrated interfaces, 66\nRTS-game implementation, 63\nsearch grid with search tree, 290\nsensing-model unification, 227\nShaderManager class diagram, 543\nShaderParameter class diagram, 548\nskeletal animation cumulative error, 368\nskeletal animation reduction in cumulative transla-\ntional error, 370\nskeletal animation translation error reduction, 369\nsmart packet sniffer, 494\nsound falloff with zone approach, 225\nsound-layout comparison, 325\nsphere with support mapping, 167\nsquare grid relative to neighborhoods and marching,\n48\nstack overflow, 100\nsticky particles used with overhanging terrain, 363\nsubdividing patch, 398\nsupport mapping as moving plane, 166\nsupport mappings combined, 169\nsupport point, 167\nsupport point in direction of portal, 175\nsurround sound, 315\nsurround sound with channels synced, 316\nterrain composed of angles, 355\nterrain created with search radius, 357\nterrain generated with particle deposition algorithm,\n354\nterrain navigation system results, 444\ntexture atlas for graftal imposters, 448\ntexture stack for large terrain, 438\ntexture stack with mipmap levels, 439\ntextures (non-compressed) for decals method, 432\nthreaded and multithreaded models, 38\ntiles, 50\ntoroidal update and mapping for virtual texture, 443\ntotally-ordered plans, 284\ntransformation semantics, 186\ntrigonometric curve for circle, 196\ntrigonometric curve with constraints, 193\nUCT player trace, 267\nvertex mask in Loop subdivision, 387\nvertex neighbors in Loop subdivision, 394\nview distance check in agent-sensing model, 218\nvirtual texture, 437\nvisemes for “hello,” 460\nvision certainty, 223\nvision model with gradient zones of certainty, 224\nvision model with view angles and circle, 220\nvisual data mining of player traces, 272\nvolcano created with particle deposition, 359\nWalker class used with hexagonal grid, 53\nwalking-dog control points, 410\nworkflow for Artificial Contender, 235\nfilter, use in Artificial Contender, 231\nFilters block, use in AC decision-making algorithms,\n236\nfilters versus effects, considering in audio processing,\n336–337\nFinite State Machine (FSM), use with attitude systems,\n256–257\nFIR (finite impulse response) filters, using in audio\nprocessing, 336–337\nfirst-person shooting (FPS) games, interaction control\nin, 69\nflattening effect, applying for diffuse-light shading,\n375–377\nFloat32 PCM audio engine file format, pros and cons\nof, 308–309\nFlow Regions() function, using in optical flow, 31\nfluster, displaying for player trace, 272\nFMOD Designer interface\nfeatures of, 326–328\ngoals of, 328–329\nSee also sounds\nfolklore algorithm mistakes, occurrence with RNGs,\n123–124\nfoot-based navigation\ncapabilities of, 69–71\nimplementation of, 69–70, 72–75\nrequirements for use with computer vision, 71–72\nsample game, 75–77\ntests with users, 77–78\nfootsteps, randomizing, 328–329\n572\nIndex\n",
      "content_length": 3240,
      "extraction_method": "Direct"
    },
    {
      "page_number": 606,
      "chapter": null,
      "content": "formulas. See Equations\nFortuna RNG method, description of, 122\nFourier series, constructing trigonometric spline from,\n192\nFPS (first-person shooting) games, interaction control\nin, 69\nfragment shaders, use with room acoustics, 302–303\nfragment versus pixel shaders, 542\nframes, storing information per, 10\nfree nodes, managing with large allocator, 18\nfree-list\naccessing with small allocator, 17\nfunction in heap allocation, 16\nfrequency data, using windowing techniques used with,\n312, 314\nFringe Search algorithm, using, 293–294\nfrustum, support mapping for, 170\nFSM (Finite State Machine), use with attitude systems,\n256–257\nfunction calls, categorizing for asynchronous events,\n83–84\nfunction pointers versus functors, use in Artificial \nContender, 244–245\nG\ng() cost, purpose in A* search algorithm, 291, 293\ngain, finding in behavior cloning example, 214\ngame animation systems, overview of, 365–366\ngame architecture, planning for multithreaded pro-\ngrams, 36\ngame logins, securing, 481–485\ngame sessions, securing, 485–487\ngame world state\ncategorizing changes in, 468–469\nsending messages in, 468–469\ngame world synchronization. See synchronizing game\nworlds\ngame worlds, entities in, 55\ngateways, identifying between spatial regions, 271\nGaussian distributions\nexample of, 201\nin nature, 203\nuse of, 199\nusing in RNGs, 115\nGaussian random number generators (GRNGs)\napplication for, 202\nand central limit theorem, 201\npolar-rejection, 200\nuse of, 200–203\nziggurat method, 201\nGaussian randomness, use of, 203\nGeneral Purpose computation on a Graphics Processing\nUnit (GPGPU), overview of, 300\ngeneral purpose registers (GPRs), use with asynchronous\nevents, 84\ngeneric programming\nalternative block implementations in, 245–246\nuse with Artificial Contender, 238\ngeometry creation in Loop subdivision\nedges, 384–386\nlimit positions, 387–388\nvertex and crease normals, 388\nvertices, 386–387\ngeometry models, mapping virtual textures to, \n442–443\nGHTP project, smart packet sniffer used in, 491–492\nGJK (Gilbert, Johnson, Keerthy) algorithm\nversus MPR (Minkowski Portal Refinement),\n177–178\nuse of, 171\nGLRCachePad macro, using in threaded systems, 43–44\nGLRThread interface\ncapabilities of, 39\nsize of, 40\nGLRThreadFoundation singleton, usage of, 38\nGLRThreading library\ncomponents of, 36–38\ncreating cache-aligned data structures with, 43–44\nfeatures of, 36\nthreading capabilities in, 42\nusing, 44–45\nGLRThreading system, submitting objects to, 42\nGLRThreadProperties mechanism, use in threading\nsystems, 39–40\nGNU C library\nhooking functions related to, 102\nreplacing memory functions with, 101\ngoal-oriented planning systems\noverview of, 281–283\npartially-ordered plans in, 282\nplan merging for, 283–286\ntotally-ordered plans in, 282\nGPGPU (General Purpose computation on a Graphics\nProcessing Unit), overview of, 300\nGPRs (general purpose registers), use with asynchronous\nevents, 84\nIndex\n573\n",
      "content_length": 2907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 607,
      "chapter": null,
      "content": "GPU subdivision, considering in Loop subdivision,\n397–398\nGPUs, relationship to audio optimization, 300–301\ngraftal imposters\nassigning texture coordinates to, 452–453\nsampling texture atlas for, 453\nusing assets during runtime, 450–453\ngraftal-imposter assets\ncolor texture and mesh, 450\ncontrol textures, 448, 451\ntexture atlas, 447–448\nvector fields, 448–450\ngraftals, use of, 447\ngranularity, role in next-gen audio engine, 313–314\ngraph edit distance, using, 275–277\ngraph searches, techniques for, 289\ngraph-based data, discovering knowledge in, 278\ngrids\nhexagonal versus square types of, 47\nimpact on visual appearance of games, 49\nrepresenting playing fields with, 49\nuse in games, 47\nGRNGs (Gaussian random number generators)\napplication for, 202\nand central limit theorem, 201\npolar-rejection, 200\nuse of, 200–203\nziggurat method, 201\nH\nh() cost, purpose in A* search algorithm, 291, 293\nHA (half-edge array), use with subdivision data struc-\ntures, 391\nhalf-edge array (HA), use with subdivision data struc-\ntures, 391, 395\nhalf-life, setting in attitude systems, 255\nhandheld gaming systems, relationship to vertical blank-\ning period, 82\nhanging pointers, tracking down, 537\nhanning and hamming window types, use of, 314\nhead model, using for lipsyncing, 455\nheader file, use with asynchronous events, 84\nheap allocation\nadding debugging support for, 22\ncombining allocators, 21\nexample on CD, 23\nextensions of, 23\nhybrid approach toward, 15–23\nwith large allocator, 18–21\nand multithreading, 22\nperception of, 15\nwith per-size template pool-style allocator, 16\nwith small allocator, 16–18\nhearing model\nwith certainty, 224–226\nconsidering in agent-sensing model, 219\nincluding other senses in, 226\nheight fields, traversing with random walkers, 353–354\nHermite spline, Equation for, 192–193\nhexagonal grids\naccess layer of, 53\naddress layer in, 51–53\nimplementing, 54–55\nhexagonal tiles\nadvantage of, 48\naxes of symmetry, 50–51\nequidistant neighbors on, 48\nforming organic shapes with, 50\nisotropy and packing density considerations, 49\nhexagonal-grid applications\ncellular automata, 56–57\npathfinding, 55–56\nspatial search, 55\nhit-test computations, using with hexagonal grids, 54\nHIVVE (Highly Interactive Information Value \nVisualization and Evaluation) tool, features of, \n278\nHLA (High Level Abstraction)\ncollaboration diagram for, 469\ncomponents of, 470–476\nusage of, 461\nHLA event handlers, use of, 477\nHLA runtimes\ncommunication between, 475–476\nconstruction of, 477–478\nviewports in, 476–478\nHLA system, extending, 478\nhook function, use with memory leaks, 101–102\nhorse-animation example, 410\nhuman cognitions, attitudes as basis for, 251\nhuman hearing. See hearing model\nhuman vision. See vision model\nI\nIA (indirect-argument) functions, adding deferred calls\nto, 84\nIDA* (Iterative Deepening A*), eliminating open and\nclosed lists with, 292–293\n574\nIndex\n",
      "content_length": 2882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 608,
      "chapter": null,
      "content": "identity\nby authentication method, 485–486\nby cryptography method, 486\nby IP address method, 485\nIDs, replacing strings with, 555–559\nIIR (infinite impulse response) filters, using in audio\nprocessing, 336–337\nimage warping, using with relief imposters, 403\nindex dispenser, use in dependency storage, 93\nindirect argument data blocks, storage of, 83–84\nindirect-argument (IA) functions, adding deferred calls\nto, 84\nInfiniteReality2 hardware platform, access of miplevels\nin, 414\ninfluence and persuasion, considering in attitude \nsystems, 259–260\ninformation theory, applying to behavior cloning, 214\ninstance-based machine learning\nArtificial Contender example of, 229–230\nfeature space in, 210–211\ninteger representation, finding length in prospective\nspace, 160\ninteractions, capturing, 268\nInversive Congruential Generator RNG method, \ndescription of, 118\nIP address, identity by, 485\nIPGs (interactive player graphs)\nbuilding, 274–278\nclustering, 275–277\nclustering players by, 275\nISAAC, ISAAC+ RNG method, description of, 121\niterator design pattern, using with hexagonal grids, 53\nJ\njittering values, hiding, 474\njob selection, use in multithread job and dependency\nsystem, 91\njobs versus threads, 87\njob-system objects\ncache coherency, 90\njob, 88–89\njob selection, 91\nmanager class, 89\nscheduler, 89–90\nworker threads, 90\njumper, displaying for player trace, 272\nK\nkD-tree\naxis-aligning split planes in, 130\nconstruction of, 132–135\ncost of splits in, 133\ndetermining split plane position, 132\nEventBox used with, 134–135\ntraversal of, 135–138\nuse in raytracing, 128\nuse of, 129\nSee also raytracing\nkD-tree nodes, contents of, 130\nKdTreeNode structure, 130\nkey vertex cell decomposition, applying to worlds, 271\nkeyboard sniffers, concerns about, 482, 487\nK-medoids clustering, using on IPGs, 276–277\nKnuth mistake, occurrence with RNGs, 122–123\nKobbelt subdivision scheme, features of, 382\nL\nLagged Fibonacci Generator (LFG) RNG method,\ndescription of, 118\nLambert’s model\neffects of, 374\nversus Oren-Nayar model, 375\nuse of, 373\nlarge allocator\ncombining with small allocator, 21\nfunction in heap allocation, 18–21\nlatency, relationship to audio engines, 313–314\nlater and now lists, use in Fringe Search algorithm,\n293–294\nLCG (Linear Congruential Generator) RNG method,\ndescription of, 116–117\nleak detector, allocation registry managed by, 101–102\nLeast Recently Used (LRU) algorithm\nefficiency of, 6\nuse of, 7\nleast used pages, identifying, 10\nLFG (Lagged Fibonacci Generator) RNG method,\ndescription of, 118\nLFSR (Linear Feedback Shift Register) RNG method,\ndescription of, 118\nLFSR113, LFSR258, use of, 119–120\nliking/disliking, evaluating in attitude systems, 253–254\nLinear Congruential Generator (LCG) RNG method,\ndescription of, 116–117\nLinear Recurrence Generators (LRGs)\nLFSR113, LFSR258, 119–120\nMersenne Twister, 119\nWELL algorithm, 120–121\nline-of-sight test, doing in agent-sensing model,\n218–219\nIndex\n575\n",
      "content_length": 2936,
      "extraction_method": "Direct"
    },
    {
      "page_number": 609,
      "chapter": null,
      "content": "lines\ncutting polygons with, 162\ndefining relative to projective space, 156\nfinding intersection points of, 157–158\nleading through pair of points, 157\nlinks, creating for dependency manager system, 94\nlipsyncing\nhead model used for, 455\nin real-time, 460–461\nrequirements for, 455–457\nuse of, 455\nword to phoneme mapping for, 457–459\nlistener, role in sound systems, 332\nListings\nGLRThreading library executing test objects, 45\nGLRThreading library test game object, 44\nGLRThreading library threadable function for game\nfunction, 44\nRBF-based warping function, 406–407\nrecursive node learning algorithm, 213\nsFront and sBack for relief imposters, 409\nSWD file, 471\nSWD file for synchronized object, 474\nSWD file pseudo-grammar, 470–471\nsynchronized object _property keyword, 472–473\nsynchronized object casting and assign operators, 473\nwalking motion, 408–409\nSee also code samples\nLOD levels, smooth transitions between, 415\nlogging\nexcluding and oversimplifying, 267\nimplementation of, 268\nusefulness of, 266\nlogins, securing, 481–485\nlogs, capturing at 10Hz, 267\nloop markers\nstreaming audio with, 310\nuse with audio engines, 309\nLoop subdivision algorithm\ncollision detection, 389–390\ncomputing new edge vertices with, 393\ncreating new half-edge information, 395\ndata structure for, 390–392\nedge weights in, 385\nextensions to, 381\nfeature implementation, 388–389\nfeatures of, 382\ngeometry creation, 384–388\nGPU subdivision and rendering, 397–398\nlevels of subdivision in, 392\nperformance enhancements, 396–397\nrelief warping requirements, 407\nsplitting faces with, 393–394\ntoolset for, 383–384\nupdating features, 395–396\nupdating original vertices, 393\nSee also subdivision surfaces\nlossy compression algorithms, using with skeletal \nanimation, 367\nlozenge, support mapping for, 169\nLRGs (Linear Recurrence Generators)\nLFSR113, LFSR258, 119–120\nMersenne Twister, 119\nWELL algorithm, 120–121\nLRU (Least Recently Used) algorithm\nefficiency of, 6\nuse of, 7\nL-systems, use with particle deposition, 361\nLua binding\nattributes, 511–512\nand automatic type registering, 509–510\nof C functions, 504\nof C++ objects, 505–510\ncreating binding function for, 507–509\ndebug helper, 513\nenum support, 512\ninheritance, 511\nmaking object-oriented, 504–505\nnative types for, 505\noptimization of generated code size, 515–516\noverloaded functions, 513\noverloading C++ methods in, 514\nreference counting and raw objects, 511\nsand-boxing and type filtering, 514–515\nsingletons, 511–512\nstatic functions, 511–512\ntemplate classes, 512\nuse of, 503–504\nLua script, turning tree as, 215\nLucas and Kanade algorithm\nturning tree as Lua script, 215\nuse in optical flow, 29–30\nuse with optical flow, 31–33\nM\nMA and MB matrices, use in narrow phase, 185–186\nmain memory, direct-mapping to cache, 43\nSee also memory\n576\nIndex\n",
      "content_length": 2800,
      "extraction_method": "Direct"
    },
    {
      "page_number": 610,
      "chapter": null,
      "content": "malloc/free replacement, heap allocator for, 22\nmanager class, use in multithread job and dependency\nsystem, 89\nmarker position, obtaining with ARToolkit, 74\nmatrices, extracting semantics from, 181–184\nmatrix m, values for, 75\nMAX analysis, using with pages, 10\nmean curvatures, relationship to farthest feature map,\n143\nmemory\nadding to unified sensing model, 227\nassociation with threads, 40\nmanagement by small allocator, 17\nuse in large allocator, 18\nSee also main memory\nmemory leak detector, using, 100–102, 104\nMergers block, use in AC decision-making algorithms,\n237\nmerging plans for agents, 284–286\nMersenne Twister Linear Recurrence Generator, use of,\n119\nmesh, storing for subdivision data structures, 390–391\nMetaAttribute class, use with database backend, 518\nmetals, models for, 374\nMetaType class\nclass metadata saved in, 517–518\nusing with database backend, 526\nMicrosoft CRT, using allocation hooks in, 101\nMicrosoft’s CryptGenRandom RNG method, \ndescription of, 122\nMicrosoft’s XACT audio tool, features of, 326\nMiddle Square RNG method, description of, 116\nMidedge subdivision scheme, features of, 382\nMIDI interface, implementation for Scarface:\nThe World Is Yours, 346\nmini-dump, creating for running process, 98\nMiniDumpCallback function, use in debugging, 104\nMinkowski differences\nfinding points on interiors of, 171\nsimplifying collision detection with, 170–171\nmipmapping, generalizing with clipmaps, 413–414\nmixing system\nfeatures of, 341–342\nimplementation of, 342–345\nperformance of, 346–347\ntuning application for, 345–346\nSee also audio\nModifier block\nimplementing, 241\nuse in AC decision-making algorithms, 236\nmomentary versus dispositional liking, 251\nMost Recently Used (MRU) algorithm, use of, 7\nmountains, improving particle placement of, \n358–361\nMP3 audio compression format, explanation of, 337\nMP3 audio engine file format\nplaying back audio in, 307–308\npros and cons of, 308\nrequirements of, 309\nMPR (Minkowski Portal Refinement)\nversus GJK (Gilbert, Johnson, Keerthy) algorithm,\n177–178\nrelationship to XenoCollide algorithm, 166, \n171–176\nusing for contact information, 176–178\nMRU (Most Recently Used) algorithm, use of, 7\nMultiStream\nbusses for audio channels in, 318\nlatency considerations, 313\nprocessing capabilities of, 306, 309\nrole in SCEE audio engine, 305\nsurround-sound management by, 315\nand volume parameters, 311\nmultithread job and dependency system. See job-system\nobjects\nmultithreaded programs, designing, 36\nmultithreading\nhiding complexity of, 87\nrelationship to heap allocation, 22\nsynchronization problems associated with, 87\nmultivariate normal distribution, example of, 202\nmutex per bin, using in heap allocation, 22\nmWebCam.AddRegion() function, using in optical\nflow, 32\nN\nNA (normals array), use with subdivision data \nstructures, 391\nnarrow phase of collision detection, implementing,\n185–188\nnavigation. See foot-based navigation\nN-by-N attitudes, use in attitude systems, 260\nNeighborhood instance, using with hexagonal grids, 54\nneighborhoods, role in square tiling, 48\nneighbors, coalescing with large allocator, 20\nNetscape mistake, occurrence with RNGs, 123\nnetwork code, maintaining debugging techniques for,\n492–493\nNewton-Raphson method, use in optical flow, 29\nnext-gen, separating from last-gen titles, 317\nIndex\n577\n",
      "content_length": 3303,
      "extraction_method": "Direct"
    },
    {
      "page_number": 611,
      "chapter": null,
      "content": "next-gen audio engine\nchannels for, 307\nand FFT (Fast Fourier Transforms), 312–313\nand frequency domain processing, 312\nand latency, 313–314\npacket smoothing, 314–315\nplayback frequency of, 311\nrouting, 318\nsample formats for, 307–309\nstreaming, 309–310\nvolume parameters for, 311\nSee also audio\nNFU (Not Frequently Used) algorithm, use of, 7–8\nnil node, use with red-black trees, 20\nnodes, managing with large allocators, 19–20\nnoise functions, using with particles, 356–357\nnormal distribution, use of, 199\nnormals array (NA), use with subdivision data\nstructures, 391\nNot Frequently Used (NFU) algorithm, use of, 7–8\nnotifications, considering as asynchronous events,\n81–83\nnow and later lists, use in Fringe Search algorithm,\n293–294\nNPC behavior. See attitude systems\nNPCs, use of totally-ordered plans with, 283\nnumbers, replacing strings with, 555–559\nO\nOBB (oriented bounding box), use in scenes, 184\nobject oriented programming, applying to hexagonal\ngrids, 51–53\nobject threading, implementing, 42\nobjects\ninstantiating in optical flow, 31–33\nin R2 projective space, 155\nuse in R2 projective space, 159\nSee also C++ objects\nOGG Vorbis audio compression format, explanation of,\n337–338\nO(log(N)) search, guaranteeing with large allocator, 18\nopen lists, eliminating with IDA*, 292–294\nOpenAL API, features of, 332\nOpenCV library\nfunctions of webcamInput class in, 26–27\nuse with optical flow, 25–26\nOpenCV methods\ncvAbsDiff function, 27\nimage differences, 27–28\nLucas and Kanade algorithm, 29–30\nmotion history, 28–29\nOpenGL, shader parameters in, 546\nopinion events, function in attitude systems, 252\nopinion system\nexample of, 257\nin Fable, 260\nOPT (Belady’s Min) algorithm, use of, 6–13\noptical flow\nas array of vectors, 26\nin computer vision games, 26\ndefinition of, 25\ngame sample, 30–33\nand image differences, 27–28\ninstantiating objects in, 31–33\nLucas and Kanade algorithm used in, 29–33\nand motion history, 28–29\nand OpenCV library, 25–26\npartitioning queries for, 32\nOren-Nayar model, versus Lambert’s model, 375\noriented bounding box (OBB), use in scenes, 184\norigin shift, example of, 181\noutdoor terrain rendering. See terrain areas\nOutputData type, defining for polymorphic workflow\nblocks, 240\noverhanging terrain, creating with particle deposition,\n362–364\nP\nPA_HARD and PA_SOFT labels, using with threads,\n40\npackets, capturing with WinPcap library, 494–496\npage-replacement algorithms, use of, 6\npages\naccessing relative to Age algorithm, 8–13\nAPC/RC ratios for, 12\ncosts associated with, 11–12\ndetermining for eviction from cache, 10\nplacement with small allocator, 18\nrelationship to cache, 5\nrequesting from OS in heap allocation, 17\nparent side index, use with large allocators, 19–20\npartially-ordered plans, use in plan merging, \n282–283\nparticle deposition\nexplanation of, 353\nimproving, 354–355\nlimitations of, 355\nparticle dynamics, improving, 355–357\n578\nIndex\n",
      "content_length": 2898,
      "extraction_method": "Direct"
    },
    {
      "page_number": 612,
      "chapter": null,
      "content": "particle-placement improvements\ndunes, 361–362\nmountains, 358–361\noverhanging terrain, 362–364\nvolcanoes, 357–358\nparticles\nbehavior of, 355\nsearch radius and elevation threshold of, 356–357\nusing noise functions with, 356–357\npassword recovery, considering in authentication,\n482–483\npasswords\ninsecurity of, 486–487\nprotection in Challenge Hash Authentication, 483\ntransmitting in Secret Exchange Authentication, 484\npathfinding approaches\nA* search algorithm, 290–292\nuse of, 289\nusing hexagonal grids in, 55–56\npcap, initializing, 495\nPCS (potentially colliding set) of triangles, discovering\nat runtime, 143\nper bin marker, using with large allocator, 21\nperspective projection, applying to planes, 156\npersuasion and influence, considering in attitude \nsystems, 259–260\nphase-causing functions, managing for surround sound,\n317\nphonemes\nmapping to visemes, 459\nmapping words to, 457–459\nversus visemes, 457–458\nPhong-Blinn model, applying to backscattering,\n378–379\n“Pipes and Filters” design pattern\nliabilities of, 232–233\nusing with Artificial Contender, 230–232\npixel movement. See optical flow\npixel versus fragment shaders, 542\nplan merging, use with goal-oriented planning systems,\n283–286\nplanes, applying perspective projection to, 156\nplan-merging algorithm, implementing, 284–286\nplants, expressing shape and formation of, 447\nPlay or Pitch functions, managing in surround sound,\n317\nplayback frequency\nconsidering in audio engines, 311\nreducing for audio streams, 310\nplayer traces\nexamining, 272\nfinding emergent behaviors in, 272\nproviding contexts for, 271\nusing visual data mining with, 272\nvisualizing, 270\nplayers, clustering by IPGs, 275\nPlayerViz tool\ncapturing captured information with, 279\ndesign of, 270–271\ngenerating thumbnails of player traces with, 272\ninformation contained in, 268\nworld data in, 271\nplaying fields, representing with grids, 49\nPlaystation 3, next-gen audio engine for, 305\nPN triangles, relationship to subdivision surfaces, 382\npoint, support mapping for, 168\npointers, using with large allocators, 19\npoint-line test, using in projective space, 157\npoints\nconsecutive operations on, 158–159\nfinding in interior of Minkowski difference, 171\noperations on, 157–158\nrepresenting in projective space, 155\npolar coordinates, use with Gaussian distribution, \n202\npolar-rejection, function in GRNGs, 200\npolygon, support mapping for, 170\npolygon meshes, generation of T-intersections in,\n163–164\npolygons\nconvex quality of, 161\ncutting with lines, 162\npolyhedron, support mapping for, 170\npolymorphic workflow blocks, use in Artificial \nContender, 238–241\n“pops”\ndeconstructing, 323–324\nexplosions as, 322\npotency, purpose in attitude systems, 254–255\npotentially colliding set (PCS) of triangles, discovering\nat runtime, 143\npreempting, purpose in threading architecture, 36\nprimary buffer, role in sound systems, 332\nprinciple curvatures, relationship to farthest feature\nmap, 143\nPRNGs (pseudo-random number generators), use of,\n114–115\nprocedural modeling, potential of, 110\nIndex\n579\n",
      "content_length": 3037,
      "extraction_method": "Direct"
    },
    {
      "page_number": 613,
      "chapter": null,
      "content": "programming errors\napplication crashes, 97–100\nmemory leaks, 100–102\nWER (Windows Error Reporting), 102–103\nprojectile paths, adding random variation to, 202\nprojective space\nobjects in RP2, 155\nuse of, 153–154\nPrototype Design Pattern, use with shaders, 544,\n547–548\nPS3, audio on, 306\npseudocode. See code samples\npseudo-random number generators (PRNGs), use of,\n114–115\npublic key infrastructure, using, 484–485\npull model, use in workflow, 236, 239\npush model, use in workflow, 236\nPython’s AST, using to replace strings with numbers,\n555–559\nQ\nQuake, animation of characters in, 365\nQuake 3, conversion mod of, 267\nQueryFlow() function\ncalling in optical flow, 31\npreventing calling in optical flow, 33\nquotes (' and \"), using with Python strings, 556\nR\nR2 projective space\nconverting vectors from, 155\nnumber range limits in, 158–161\nobjects in, 155\noperations in, 157–158, 161–164\npoints and directed lines in, 155–156\nusing integer coordinates in, 158\nradial basis functions (RBFs)\nanimating relief imposters with, 402\nand relief imposters, 404\nSee also relief imposters\nrandom number generators (RNGs)\ndistributions of, 115\nhardware RNGs, 114\nPRNGs (pseudo-random number generators),\n114–115\nand software whitening, 116\nuses of, 113–114\nrandom variation, adding to projectile paths, 202\nrandom walker, use in particle deposition, 353–354\nrandomness testing, conducting, 116\nRANDU mistake, occurrence with RNGs, 123\nrank buffers, role in audio processing, 334–336\nray, components of, 128\nray casting, explanation of, 127\nray queries, support for, 128\nraytracing\nfor acoustics, 302\ndemo application, 139–140\ndynamic scenes, 139\nuse of, 127\nand visibility queries, 128\nSee also kD-tree\nraydir array, use of, 137\nrays, finding intersections of, 130\nRBF coefficients, using with warping function and\nshaders, 405–406\nRBFs (radial basis functions)\nanimating relief imposters with, 402\nand relief imposters, 404\nSee also relief imposters\nRC (relative cost), function in cache replacement,\n12–13\nreal-time strategy (RTS) games\nfocus-context control level in, 61–63\nintegrating sketch- and unit-based interfaces in, 66\nmoving soldiers in, 63–64\nmoving troops through battlefields in, 65\npopularity of, 59\nusing sketch-based approach with, 66\nrectangle, support mapping for, 168\nrecursion, adding to raytracing, 127\nred-black tree\ncombining with book container, 22\nusing with large allocators, 18\nred-black tree node\nsearching for appropriate size, 20\nstoring, 19\nuse in non-default alignment, 21\nReif-Peters subdivision scheme, features of, 382\nrelative cost (RC), function in cache replacement,\n12–13\nrelief imposters\nanimating, 407–409\nand image warping, 403\ninterpolating warping functions for, 404–405\nobtaining, 402\n580\nIndex\n",
      "content_length": 2731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 614,
      "chapter": null,
      "content": "producing, 402\nand RBFs (radial basis functions), 404\nrendering, 407\ntexels for textures used with, 409\nSee also animation systems; RBFs (radial basis \nfunctions); warping functions\nrelief maps, animating, 407\nrelief rendering, explanation of, 401\nrendering\nconsidering in Loop subdivision, 397–398\nof large terrain areas, 442–444\nmethods for, 381\nrelief imposters, 407\nsubdivision surfaces, 398\nRepeaters block, use in AC decision-making algorithms,\n237\nridge structures, creating, 359–360\nRNG methods (cryptographic)\nBlum Blum Shub, 121\n/dev/random, 121–122\nFortuna, 122\nISAAC, ISAAC+, 121\nMicrosoft’s CryptGenRandom, 122\nYarrow, 122\nRNG methods (non-cryptographic)\ncellular automata, 119\nInversive Congruential Generator, 118\nLCG (Linear Congruential Generator), 116–117\nLFG (Lagged Fibonacci Generator), 118\nLFSR (Linear Feedback Shift Register), 118\nLRGs (Linear Recurrence Generators), 119–121\nMiddle Square, 116\nTLCG (Truncated Linear Congruential Generator),\n117–118\nRNGs (random number generators)\ndistributions of, 115\nhardware RNGs, 114\nmistakes associated with, 122–124\nPRNGs (pseudo-random number generators),\n114–115\nand software whitening, 116\nuses of, 113–114\nroom acoustics, calculating, 302–303\nrotated objects, finding support mappings for, 168\nrotation error\neliminating, 367–370\noccurrence in skeletal animation, 367\nrough surfaces, model for, 374–376\nrounded box, support mapping for, 169\nRP2 projective space, use of, 154\nRTS (real-time strategy) games\nfocus-context control level in, 61–62\nintegrating sketch- and unit-based interfaces in, 66\nmoving soldiers in, 63–64\nmoving troops through battlefields in, 65\npath sketching in, 62–63\npopularity of, 59\nusing sketch-based approach with, 66\nruntime queries, using with farthest feature map,\n148–150\nS\nSAH (surface area heuristic), relationship to kD-tree,\n133\nSAT (Separating Axis Theorem), use of box-box text\nwith, 187\nScarface: The World Is Yours, MIDI interface imple-\nmented for, 346\nSCEE audio engine project, goals of, 305\nscheduler class, use in multithread job and dependency\nsystem, 89–90\nSCRIPTABLE_DefineClass( MY_CLASS ) macro,\nbinding classes with, 513\nsearch algorithms, A*, 290–292\nSecret Exchange Authentication, using, 484\nsecuring\ngame logins, 481–485\ngame sessions, 485–487\nsegment, support mapping for, 168\nSelectors block, use in AC decision-making algorithms,\n237\nsensing model\nadding memory to, 227\ncomponents of, 226–227\nSGI’s InfiniteReality2 hardware platform, access of\nmiplevels in, 414\nshader architecture, invoking blimp in, 549–551\nshader groups, using, 551–552\nshader in GLSL code sample, 376\nshader languages, parameters supported by, 547\nshader parameters, use in OpenGL, 546\nshader programs, using with room acoustics, \n302–303\nShaderManager class\nmethods on, 545\nusing, 543\nIndex\n581\n",
      "content_length": 2795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 615,
      "chapter": null,
      "content": "ShaderParameter class\ntypes of data supported by, 546–547\nusing, 542–543\nShaderProgram class, using, 542\nshaders\ncloning of parameter types for, 547–549\ncombining with actors and properties, 552–553\nevaluating warping functions with, 405–407\nfragment versus pixel shaders, 542\nprototypes for, 544–546\nreloading at runtime, 544\nstate sets and scene graphs, 545–546\nterminology for, 541–542\nshadings, producing with Lambert model, 374\nshapes\nfinding support points for, 169\nrepresenting with support mappings, 166–170\nshrink-wrapping, 169\nsupport mappings for, 167–168\nSIMD (Single Instruction Multiple Data), role in audio,\n299\nsingle quotes ('), using with Python strings, 556\nsingletons, use of GLRThreadFoundation in threading\narchitectures, 37\nskeletal animation\nbone and rotation errors in, 366–367\nand cumulative error, 366–370\nfunctionality of, 365–366\nreconstruction errors in, 368\nsketches, creation by users in RTS games, 62\nSLERP (spherical linear interpolation), use with\ntrigonometric splines, 194\nslope of terrain, defining, 355–356\nSM2.0 clipmap path, implementing clipmaps with,\n417–418\nsmall allocator\ncombining with large allocator, 21\nfunction in heap allocation, 16–18\nusing reserved virtual address range for, 21\nsmart packet sniffer\nalternative for, 496\nexample of, 491–492, 496\nfeatures of, 491\nimplementation of, 493\nreducing security risks for, 495–496\nsmooth edges and vertices, determining in Loop subdi-\nvision, 385–386\nsmooth surfaces, representing, 381\nsnapshots, mixing in mixing system, 344\nsniffed passwords, concerns about, 482\nsocial exchanges, relationship to attitudes, 260–261\nsoldiers, moving in RTS (real-time strategy) games,\n63–64\nSorters block, use in AC decision-making algorithms,\n237\nsound buffers, role in audio processing, 333–334\nsound designers, interaction with mixing system, 343\nsound effects\ncreating, 324\nand rank buffers, 335\ntools used in creation of, 326\nsound environment, making changes to, 329\nsound falloff, demonstration of, 225–226\nsound files, comparing layouts of, 324–326\nsound instances, specifying playing of, 335\nsounds\ncomponents of, 328\ncomposition of, 322\nconceptualizing, 322\nconstructing and deconstructing, 323–324\ncreating with FMOD Designer interface, 329\nin-game rendering of, 328\nperception of, 331\nplaying limitations of, 334–335\nproperties of, 224\nSee also audio; FMOD Designer interface\nsound-system overview\nlistener, 332\nprimary buffer, 332\nsound effects, 333\nsound sources, 333\nSources block, use in AC decision-making algorithms,\n236\nSpace War game, use in behavior-cloning example, 210\nspatial movement, tracking, 274\nspatial search, using hexagonal grids in, 55\nsphere, support mapping for, 167–168\nsplines, use of, 192–194\nSplitters block, use in AC decision-making algorithms,\n237\nsqrt(d) (Kobbelt) subdivision scheme, features of, 382\nsquare tiling, neighborhoods in, 48\nstack overflows, handling, 99–100\nstatic polymorphism, implementing, 241\nsticky particles, use with overhanging terrain, 363\nstreaming audio, 309–310, 337–338\n582\nIndex\n",
      "content_length": 3031,
      "extraction_method": "Direct"
    },
    {
      "page_number": 616,
      "chapter": null,
      "content": "strings, replacing with numbers, 555–559\nsubdivision, methods for, 381\nsubdivision data structure, file format for, 391–392\nsubdivision schemes\nproperties of, 381–382\nusage of, 382–383\nsubdivision surfaces\nexplanation of, 381\nfast rendering of, 398\ntoolsets for, 382\nuses of, 382\nSee also Loop subdivision algorithm\nsubdivision type, choosing, 383\nSUBDUE tool, features of, 278\nsum-of-of uniforms algorithm, use with GRNGs,\n201–202\nsupport mappings\ncombining, 169–170\ntranslating and rotating, 168\nusing with shapes, 167–168\nusing with XenoCollide algorithm, 166–170\nsurface area heuristic (SAH), relationship to \nkD-tree, 133\nsurround sound\napproaches toward, 315–316\nDSP effects, 317–318\nsyncing channels, 316–317\nSWD compiler, code generated by, 471\nSWD files\ndefining synchronization behavior in, 473\nuse in HLA systems, 470–471\nSynchEntity class\ncreation of, 475–476\ndestruction and visibility of, 476\nsynchronization bound to, 473–474\nuse in HLA (High Level Abstraction), 471–472\nsynchronization behavior, defining in SWD files, 473\nsynchronizing game worlds\noverview of, 468–470\nsynchronized objects in, 471–475\ntechniques for, 467–468\nsynchronous versus asynchronous exceptions, 97\nT\nTables\naudio engine file formats, 308\nCMU phonemes, 458–459\nedge mask selection in Loop subdivision, 386\nfeature-space calculations, 211\ngame state data for behavior cloning, 212\nLCGs (Linear Congruential Generators) in use, 117\nmatrix inversion methods, 183\nmixing system mixing snapshots, 344\nmotion detection algorithm times for optical flow, 30\nphoneme to viseme mapping, 459\nphonemes, 458–459\nprojective space numerical ranges of results, 160\nshapes with support mappings, 168\nsubdivision data structure file-format entries, 392\nsubdivision schemes, 382\nsupport mappings for compound shapes, 169\nterrain navigation system configuration for testing,\n444\nterrain navigation system statistics, 445\nworld state modification permissions, 469\ntarget victim page, determining via age, 9\ntasks\nin multithread job and dependency system, 95\nsynchronization with dependency manager, 91–94\nterrain\ndefining slope of, 355–356\noverhanging terrain, 362–364\nterrain areas\napplying textures to, 435–437\nconcentric rings update for, 441\nmanaging virtual textures for, 437–440\nrendering issues associated with, 442–444\ntexture cache for, 437–438\ntexture memory usage for, 439–440\ntexture upload time for, 441\nupdating virtual textures for, 442\nusing trilinear filtering with, 438–439\nterrain deformation, simulating with particles, 353\nterrain formed by particles, slope of, 354–355\nterrain navigation system, results of, 444–445\nterrain texturing. See clipmaps\nTestSystem game object, use of GLRThreading library\nwith, 44–45\ntext, handling with translation tables, 555\ntexture atlas, using with graftal imposters, 447–448\ntexture cache\ndesign of, 8–13\nupdating contents of, 440–442\nusing with large terrains, 437–438\ntexture coordinates, computing in dynamic geometry,\n443\ntexture memory usage, considering for large terrain\nareas, 439–440\nIndex\n583\n",
      "content_length": 3029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 617,
      "chapter": null,
      "content": "texture pages, using in cache replacement examples,\n10–11\ntexture stack, updating for large terrains, 440–441\ntexture upload time, considering for terrain areas, 441\ntexture-based representations, animating, 403\ntextures\napplying to large terrain areas, 435\nmanaging with clipmaps, 436\nrepresenting objects with, 402\nstorage of, 435\nstoring animation data in, 406\ntexels for relief imposters, 409\nSee also virtual textures\ntexturing terrains. See clipmaps\nthrash, occurrence of, 5\nthread allocation strategies\nnaive allocation, 41\nthread pools, 41–42\nthread context switching, speed of, 40\nthread handles, storage of, 39–40\nthread local storage, using in heap allocation, 22\nthread pools, use of, 41–42\nthread stack size, altering default for, 39–40\nthreading architecture, designing, 36\nthreading engine, development of, 35\nthreading systems\nengineering, 39\nexecution of, 37\nGLRThreading library sample, 44–45\nthreads\nand cache coherency, 43–44\ndesignating task priority of, 41\nexecution properties of, 40\nversus jobs, 87\nobject threading, 42\npreemption and simultaneous execution of, 39\nand processor affinity, 40\nproperties of, 39–40\nsafety, reentrancy, object synchronicity, and data\naccess, 43\nuse of, 38–39\ntiling\ncreation of, 47\nuse with textures for large terrains, 438\nT-intersections, generation in polygon meshes, 163–164\nTLCG (Truncated Linear Congruential Generator),\ndescription of, 117–118\ntotalistic cellular automaton, use in RTS games, 64\ntotally-ordered plans, use in plan merging, 282–283\ntracepoints, using with network code, 492\ntransformation matrices\nexample of, 182–184\ninverting, 179\nretrieving for ARToolkit, 74\ntransformation semantics\ncoordinate systems used in, 187\nexplanation of, 180\nextracting from matrices, 181–184\nflexibility of, 186\nrequirements for, 183\nuse with box-box test and SAT, 187\nusing for collision detection tasks, 184–188\ntriangles\nfinding intersections of, 144\ntesting for graftal imposters, 451\ntesting for XenoCollide and MPR, 172\ntrigonometric functions, fast evaluation of, 194–195\ntrigonometric splines, use of, 192–194\ntrilinear filtering, using with large terrain areas,\n438–439\ntroops, moving through battlefields in RTS games, 65\nTruncated Linear Congruential Generator (TLCG),\ndescription of, 117–118\ntruncation, effect of, 153\ntuning application, using with mixing system, 345–346\nU\nUCT (Urban Combat Testbed), use of, 267\nunhandled exceptions, reporting, 98–99\nunified sensing model\nadding memory to, 227\ncomponents of, 226–227\nuniform distribution, using in RNGs, 115\nUNIX platforms, creating core dump on, 99\nUpdate() function, using in optical flow, 33\nUrban Combat Testbed (UCT), use of, 267\nuser input, capturing in RTS (real-time strategy) games,\n62–63\nV\nVA (vertices array), use with subdivision data structures,\n390\nvalence, purpose in attitude systems, 253–254\nvalues\nrelationship to attitudes, 250\nupdating in attitude systems, 252\n584\nIndex\n",
      "content_length": 2915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 618,
      "chapter": null,
      "content": "variables, allocation in OpenCV, 26–27\nvector of C++ STL, use with hexagonal grids, 51–52\nvector spaces, mapping between, 180\nvectors\nuse in projective space, 157–158\nuse with GPUs, 300\nvertical blanking period\nwith limited time, 82–83\nrelationship to handheld gaming systems, 82–83\nvertices\nin Loop subdivision, 384, 386–387\nupdating with Loop subdivision algorithm, 393\nvertices array (VA), use with subdivision data structures,\n390\nvictim pages, finding, 10\nview cone check, doing in agent-sensing model, 218\nview distance, computing in agent-sensing model, 218\nvirtual textures\nmanaging for large terrains, 437–440\nmapping to geometry models, 442–443\nupdating for large terrain, 442\nSee also textures\nvisemes\nmapping phonemes to, 459\nversus phonemes, 457–458\nuse with head model for lipsyncing, 455–457\nvision, modeling with certainty, 222–224\nvision model\naugmenting with ellipses, 219–222\nconsidering in agent-sensing model, 217–219\ncreating, 222–224\nvisitCallFunc, using with AST, 557–558\nVista, API for WER, 103\nvisual appearance of games, impact of grids on, 49\nvisual data mining, using with player traces, 272\nvolcanoes, improving particle placement of, 357–358\nvolume levels, setting for audio channels, 311\nvoxels, using with overhanging terrain, 364\nW\nwalk function, using with AST, 557–558\nWalker class, using with hexagonal grids, 53–54\nwalking, controlling in FPS games, 71\nwalking dog animation, control points for, 409–410\nwalking motions, producing, 408–409\nwarping functions\nevaluating using shaders, 405–407\ninterpolating for relief imposters, 404–405\nSee also relief imposters\nwarrior attitudes, FSM for, 257–258\nwaveforms, oscillating values in, 331\nWebcam resolution, considering in optical flow, 33\nwebcamInput class\ncvAbsDiff function in, 27–28\nencapsulation of functionality in, 31\nfunctions in OpenCV, 26–27\nWebsites\nARToolkit, 71\nL’Ecuyer’s papers on RNG algorithms, 124\nrandom noise, 114\nsubdivision surface toolsets, 382\nSUBDUE tool, 278\nUCT (Urban Combat Testbed), 267\nWELL algorithm, use of, 120–121\nWER (Windows Error Reporting), 102–103\nwheel, support mapping for, 170\nwhitening algorithms, using with RNGs, 116\nWin32 model for threading architecture, standard for, 36\nwindowing techniques\nusing with FFT, 312–314\nusing with frequency data, 312, 314\nWindows Error Reporting (WER), 102–103\nWindows Vista, API for WER, 103\nWinPcap library, capturing packets with, 494–496\nWinQual online portal, features of, 102\nwords, mapping to phonemes for lipsyncing, 457–459\nworkflow algorithms, using with Artificial Contender,\n230–232\nworkflow blocks, requirements for, 238\nSee also blocks\nworkflow diagram\nusing with Artificial Contender, 234–235\nvisualizing constraints on, 246\nworkflows\nconstructing, 246\ninterrupting, 244\nprocess of, 235–236\nworld coordinates, transforming model to, 182\nworld geometry\nfinding information values for surfaces in, 278\nvisualizing for player traces, 271\nworlds, decomposing into regions, 271\nWriteCoreDump function, using on UNIX platforms,\n99\nIndex\n585\n",
      "content_length": 3014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 619,
      "chapter": null,
      "content": "X\nX, Y, Z approach, using with surround sound, 315\nXACT audio tool, features of, 326\nXbox 360, implementation in threading systems, 37\nXenoCollide algorithm\nand MPR (Minkowski Portal Refinement), 166\noptimizing, 177\nsupport mappings used with, 166–170\nuse of, 171–176\nY\nYarrow RNG method, description of, 122\nZ\nziggurat method, use with GRNGs, 201\nzone approach, applying to hearing model, 225–226\n586\nIndex\n",
      "content_length": 408,
      "extraction_method": "Direct"
    },
    {
      "page_number": 620,
      "chapter": null,
      "content": "CREATE AMAZING GRAPHICS AND\nCOMPELLING STORYLINES FOR YOUR GAMES!\nAdV4Mfrd VftaJftl Elhftl\n[•HhWr*cl3D\nISBN! 1-9MOO-961-1 •!».«•\n•MlC DttUllrtQ IM Carnal\nISBN! 1.IW2DO--J51.*\" 12? .W\nBe^lrtrtlrto Qarft* Art\nin MttMaxa\nQ.BI4. 1.W2HJ-WS-5 • 129.99\nB«ti)ftnli>Q Oarn* Oraphlei.\nISBN! 1-5920CM3O.)(« S29.99\nHiOv?ri \"or\nQMVM FreGrammtrt arifl Artl*U\nBEN. l.SSZM.[»2-*«S39.99\nCharacter Davelatwnent\nAnd ItoixittllrKi hf Gem**\nCOM. 1.W2DOJ.K1J • 139.99\nColrtt At I F« Tptfll,\nJ*t4nd Edlllsn\nISBN! \\lvVXWf.' 134,99\nO(pfn? Cha*artsf Animallsrt\nAll lrt<M«\nBEN! VS9BJ3.K4-4-I49.99>\nT1W D*rkSMi?\n#1 Gort)* Trlitlirdrt\nEBM. l-i»2DO-i9:i-fl • 139.99\nTo order,\nvisit www.courseptr.com or call 1.800.648.7450\n",
      "content_length": 693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 621,
      "chapter": null,
      "content": "Call 1.800.648.7450 to order\nOrder online at www.courseptr.com\nBeginning Math Concepts\nfor Game Developers\n1-59363-290-6 • S29.99\nGame Design, Second Edition\n1-592 WW93-3 • S39.99\nBeginning\nJava Game Programming,\nSecond Edition\n1-598G3n476-3 • S 2 9.99\nGame Programming All in One,\nThird Edition\n1-59863-289-2 • S49.99\nGOT GAME?\nCOURSE TECHNOLOGY\nCENGAGE Learning-\nProfessional * Technical • Reference\n",
      "content_length": 402,
      "extraction_method": "Direct"
    },
    {
      "page_number": 622,
      "chapter": null,
      "content": "COURSE TtCHNOLQ&Y\nCtNGA-GE LuaminB.-\nProfettiHial • TtDTiLjl - ndoEficz\nOne Force. One Solution.\nFor Your Game Development and Animation Needs)\nfturln Rim tAnix httt |iiiiiLMl tan:-, wilh Thulium <'-:ii[>:f alini Hi pfiniii1 you whh rim ntiic nf Ihr i|i.-ilily ynkln jiiu Imc ccnr- In [HIM.\ntaint; Puj^jmrfilrttl Ghlrtifi\nrt?tt.E3<5flJVLHKfLH\nAl GdrtfttPrt^rartlnitfJ WfadOrtl 3\nI;BM uiK».i=T^.!M«\nEdiiL GMur ftrilgii and CKdlldtt\nfor Fun t L^ninng\n0hl-U4«(M4^>USL5i\nihadBf ^:\nAfhanced Hendem^TKhniquK\nSEh. • ESJM'lJ^-Ha*\nPractical Pinar J\ni!BH 1-5W«HW*M«\nArinuling Facial PnaluKK\n& Expressions\niiw • »iKi:-i!)-y:-w\nMotile ID Game DeuHOfSincin:\nfrom ilnrl tfl M«rk?t\nwtti-swSd'Su-fHas\nIncuduc'tleii ta JD Grdfjliki\nK Anim^tim U^iitg Maqr'\nIbm 14OT>4i>4'U9tf\nBuiintss anJ Legal Frlmer\nfor -Game Oeudopmenl\n,^; ••'^j'/--^:-.'-'^-'^\nCheck out the entire list of Charles River Media guides at www.courseptr.com\nCall 1.800.648.7450 to order\nOrder online at www.courseptr.com\n",
      "content_length": 970,
      "extraction_method": "Direct"
    },
    {
      "page_number": 623,
      "chapter": null,
      "content": "V COURSE TECHNOLOGY\n- Tettrtcal - Reference\nJournal of Game Development\nTh* jQUimtofdme Qewtffm«v-'J<Xi\\>'.'i? B journal todictfBdlQ lha diwamrijcon «f tadmg-riae. anjnal r&warch on g»n**wlmm«nt\ntDpiH jnj ttit noa i«ctry findings in \"alblad i«*min dixiplinpa. Nnit^hw. jotaw0. and twircogv Th« nH^hr^n in lha joiir^'cflinss\n<nem batf^a«WiB qnd dia inJuflnr. ?nd co>w3gini4-rBi]odlQpic&fiMni lha arpescH physitSi mnlhamBlina artificialinwiigsnce. a'jp^cs.\nnewioriung. »ndo. amni3tiof\\ nebe(c3.^is«gle?t«ri nnd nteiigcln'BirittrtainmBnl tiattiegui nMha Jewnsilo unrternea cutting-edge ideas\n1r-:m rhe indiisirv 4th ucadBmc r«sarc.h in orderln advincB ihs luid ni game dauBbpnant snri hi pnamnla rhe Bccaplanca ed lha study of\ngjma JavBiopnant by lha acadamir: communilv\nEKA jimuji :(jh?r.npnon it far DHB lui vtMna. wtnch con??!? oJ 1 quBnertv ^sua?. jni innludBi hnlh- an elBtfranc und nnlra ww> tf wtfi\ntat ACM. ISBA DKU, lid IEEE HTI b\nFar mere Inlermtlti «rf tt wder, plsis* Midi, www.laad.tom\nPar guesbana BDOUI Ilia Jaufiatu vaa ordar. plNia cuniaci tffll Stnlili, ail.Bnttft&mmi.ait\nThtJniir.fm.lE/[7tiii~fl flflL-tf.^~flfif is tm^ accdprinq pnpar siinmisMnns Allpipsnt will bb K:attff&d .according la ththiqhasl stindirch nl Ih*\nFdlnrial Hnnrrl and K^ mlarRRS Airihnrs wil rnrRNn h tr-svoll prinlsjllhnirpiihliihfld pnpnr andwIMrjrisFiar rnn>rignt1n lha pnhlishar. Thnre\njrn no n»nF: rhnrgRS 1nrpi±4ic.ilmii Fill mslnirlinns 1nr marii'irrft prnpanhan and utmin-iinn cnn b* Iniuiri niilinF: nl ww.JDgdjcn The\njnnr.'UL'is pnhlislidd an a qnnrinrly hnns JG nhslrirls am ncrnplad nn nil nugninrj buix\nP^gesubrrii yajr sWractandiuhmissinnimn onlineji^^wM[ofld*«n ar>iseryiihe Mipnpgrto ulpKtafltJj**Rnd*djfnhfc^Mnyntf cam\nFur quDriians Diillm pifliiasiun proc«^ pluasc-cortaLl [mi^inkli iE<H^nirtl9oaBf*fajnHur Michaul Vouici aEi*tDr3JDfld.cnm\nCall far Papers\nwww.jogd.com\n",
      "content_length": 1874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 624,
      "chapter": null,
      "content": "License Agreement/Notice of Limited Warranty\nBy opening the sealed disc container in this book, you agree to the following terms and conditions. If, upon reading the\nfollowing license agreement and notice of limited warranty, you cannot agree to the terms and conditions set forth, return\nthe unused book with unopened disc to the place where you purchased it for a refund.\nLicense:\nThe enclosed software is copyrighted by the copyright holder(s) indicated on the software disc. You are licensed to copy\nthe software onto a single computer for use by a single user and to a backup disc. You may not reproduce, make copies,\nor distribute copies or rent or lease the software in whole or in part, except with written permission of the copyright\nholder(s). You may transfer the enclosed disc only together with this license, and only if you destroy all other copies of the\nsoftware and the transferee agrees to the terms of the license. You may not decompile, reverse assemble, or reverse engi-\nneer the software.\nNotice of Limited Warranty:\nThe enclosed disc is warranted by Course Technology to be free of physical defects in materials and workmanship for \na period of sixty (60) days from end user’s purchase of the book/disc combination. During the sixty-day term of the\nlimited warranty, Course Technology will provide a replacement disc upon the return of a defective disc.\nLimited Liability:\nTHE SOLE REMEDY FOR BREACH OF THIS LIMITED WARRANTY SHALL CONSIST ENTIRELY OF\nREPLACEMENT OF THE DEFECTIVE DISC. IN NO EVENT SHALL COURSE TECHNOLOGY OR THE\nAUTHOR BE LIABLE FOR ANY OTHER DAMAGES, INCLUDING LOSS OR CORRUPTION OF DATA,\nCHANGES IN THE FUNCTIONAL CHARACTERISTICS OF THE HARDWARE OR OPERATING SYSTEM,\nDELETERIOUS INTERACTION WITH OTHER SOFTWARE, OR ANY OTHER SPECIAL, INCIDENTAL,\nOR CONSEQUENTIAL DAMAGES THAT MAY ARISE, EVEN IF COURSE TECHNOLOGY AND/OR THE\nAUTHOR HAS PREVIOUSLY BEEN NOTIFIED THAT THE POSSIBILITY OF SUCH DAMAGES EXISTS.\nDisclaimer of Warranties:\nCOURSE TECHNOLOGY AND THE AUTHOR SPECIFICALLY DISCLAIM ANY AND ALL OTHER\nWARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OF MERCHANTABILITY,\nSUITABILITY TO A PARTICULAR TASK OR PURPOSE, OR FREEDOM FROM ERRORS. SOME STATES\nDO NOT ALLOW FOR EXCLUSION OF IMPLIED WARRANTIES OR LIMITATION OF INCIDENTAL OR\nCONSEQUENTIAL DAMAGES, SO THESE LIMITATIONS MIGHT NOT APPLY TO YOU.\nOther:\nThis Agreement is governed by the laws of the State of Massachusetts without regard to choice of law principles. The\nUnited Convention of Contracts for the International Sale of Goods is specifically disclaimed. This Agreement consti-\ntutes the entire agreement between you and Course Technology regarding use of the software.\n",
      "content_length": 2689,
      "extraction_method": "Direct"
    },
    {
      "page_number": 625,
      "chapter": null,
      "content": "COLOR PLATE 1\nScreenshot of the “path” command from Gem 1.6, where the user’s sketch is highlighted\nin green and the force vectors are illustrated on the battlefield.\nCOLOR PLATE 2\nScreenshot of the “target” command from Gem 1.6, where the command glyph (spiral) is\nhighlighted in red and the force vectors are illustrated on the battlefield.\n",
      "content_length": 343,
      "extraction_method": "Direct"
    },
    {
      "page_number": 626,
      "chapter": null,
      "content": "COLOR PLATE 3\nScreenshot of the “erase” command from Gem 1.6, where the user sketch is highlighted in\nblue and the force vectors are illustrated on the battlefield.\nCOLOR PLATE 4\nA pile of shapes with collisions resolved by the methods in Gem 2.5.\n",
      "content_length": 248,
      "extraction_method": "Direct"
    },
    {
      "page_number": 627,
      "chapter": null,
      "content": "COLOR PLATE 5\nA volcano created using advanced particle deposition, as described in Gem 5.1.\nCOLOR PLATE 6\nMountains created using advanced particle deposition, as described in Gem 5.1.\n",
      "content_length": 186,
      "extraction_method": "Direct"
    },
    {
      "page_number": 628,
      "chapter": null,
      "content": "COLOR PLATE 7\nDunes created using advanced particle deposition as described in Gem 5.1.\nCOLOR PLATE 9\nImage of a clipped mipmap stack from Gem 5.6.\nCOLOR PLATE 8\nA dog impostor from Gem 5.5 mod-\neled as a quad-layer relief texture.\nThe depth values of the progressing\nlayers are stored in the R, G, B and A\nchannels, respectively (left). A view of\nthe rendered dog impostor is shown\non the right. \n",
      "content_length": 398,
      "extraction_method": "Direct"
    },
    {
      "page_number": 629,
      "chapter": null,
      "content": "COLOR PLATE 10\nLeft to right, top to bottom, from Gem 5.7: the example diffuse and bump render targets, traditional decals,\ndecals using the technique in Gem 5.7, erosion over time with opacities of 0, 20, 40, 60, 80, and 100 per-\ncent, and decals applied on non-planar geometry.\n",
      "content_length": 280,
      "extraction_method": "Direct"
    },
    {
      "page_number": 630,
      "chapter": null,
      "content": "COLOR PLATE 11\nRings of detail and an example of a virtual texture applied to terrain as in Gem 5.8, showing levels of detail\nusing color codes.\nCOLOR PLATE 12\nAn example of rendering with graftal imposters from Gem 5.9.\n",
      "content_length": 221,
      "extraction_method": "Direct"
    },
    {
      "page_number": 631,
      "chapter": null,
      "content": "COLOR PLATE 13\nSixteen visemes, each shown at its extreme (1.0) morph, as described in Gem 5.10.\n",
      "content_length": 97,
      "extraction_method": "Direct"
    },
    {
      "page_number": 632,
      "chapter": null,
      "content": "COLOR PLATE 14\nSeveral examples of shaders created with the data-driven shader manager, as described in Gem 7.4.\n",
      "content_length": 113,
      "extraction_method": "Direct"
    }
  ],
  "enrichment": {
    "version": "1.0.0",
    "generated_by": "generate_chapter_metadata.py",
    "contains": [
      "keywords",
      "concepts",
      "summary"
    ]
  }
}